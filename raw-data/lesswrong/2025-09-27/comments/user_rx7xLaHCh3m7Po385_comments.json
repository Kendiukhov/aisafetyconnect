[
  {
    "_id": "nkhvJBDg9SdYu6mc3",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "nb8QFTwiqcj2sEfsJ",
    "topLevelCommentId": "Qx2sDtbgdC3qcmNiN",
    "contents": {
      "markdown": "> Amplified oversight can make substantial progress without studying actual schemers\n\n(Though note that it's unclear whether this progress will mitigate scheming risk.)",
      "plaintextDescription": "> Amplified oversight can make substantial progress without studying actual schemers\n\n(Though note that it's unclear whether this progress will mitigate scheming risk.) "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-23T18:27:59.819Z",
    "af": false
  },
  {
    "_id": "iuzstwjhdfKHZEps6",
    "postId": "aPi4HYA9ZtHKo6h8N",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> *   I think the book's thesis is basically right — that if anyone builds superintelligent AI in the next decade or two, it'll have a terrifyingly high (15%+) chance of causing everyone to die in short order\n\nI think this is an absurd statement of the book's thesis: the book is plainly saying something much stronger than that. How did you pick 15% rather than for example 80% or 95%?\n\nI am basically on the fence about the statement you have made. For reasons described [here](https://www.lesswrong.com/posts/4fqwBmmqi2ZGn9o7j/notes-on-fatalities-from-ai-takeover), I think P(human extinction|AI takeover) is like 30%, and I separately think P(AI takeover) is like 45%.\n\n> *   I think the world where the book becomes an extremely popular bestseller is much better on expectation than the world where it doesn't\n\nI think this is probably right, but it's unclear.\n\n> *   I generally respect MIRI's work and consider it underreported and underrated\n\nIt depends on what you mean by \"work\". I think their work making AI risk arguments at the level of detail represented by the book is massively underreported and underrated (and Eliezer's work on making a rationalist/AI safety community is very underrated). I think that their technical work is overrated by LessWrong readers.",
      "plaintextDescription": ">  * I think the book's thesis is basically right — that if anyone builds superintelligent AI in the next decade or two, it'll have a terrifyingly high (15%+) chance of causing everyone to die in short order\n\nI think this is an absurd statement of the book's thesis: the book is plainly saying something much stronger than that. How did you pick 15% rather than for example 80% or 95%?\n\nI am basically on the fence about the statement you have made. For reasons described here, I think P(human extinction|AI takeover) is like 30%, and I separately think P(AI takeover) is like 45%.\n\n>  * I think the world where the book becomes an extremely popular bestseller is much better on expectation than the world where it doesn't\n\nI think this is probably right, but it's unclear.\n\n>  * I generally respect MIRI's work and consider it underreported and underrated\n\nIt depends on what you mean by \"work\". I think their work making AI risk arguments at the level of detail represented by the book is massively underreported and underrated (and Eliezer's work on making a rationalist/AI safety community is very underrated). I think that their technical work is overrated by LessWrong readers."
    },
    "baseScore": 37,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2025-09-23T18:11:44.638Z",
    "af": false
  },
  {
    "_id": "QQrQh7AnHuBdcjrDW",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "99kTcrBospA6x7AEA",
    "topLevelCommentId": "nbsDtGMuYZeDvY9bo",
    "contents": {
      "markdown": "I do think that the post had a worse argumentative structure than posts I normally write (hence all the confusions in the comments). But that was totally on me, not the AI. I'm interested in whether you think your problem was with the argumentative structure or the prose.",
      "plaintextDescription": "I do think that the post had a worse argumentative structure than posts I normally write (hence all the confusions in the comments). But that was totally on me, not the AI. I'm interested in whether you think your problem was with the argumentative structure or the prose. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T14:52:20.420Z",
    "af": false
  },
  {
    "_id": "E7PuajtxJg5A9TtEw",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "Da6iqrmP7kMraKqEw",
    "topLevelCommentId": "nbsDtGMuYZeDvY9bo",
    "contents": {
      "markdown": "I didn’t intend this to be a prank! I just wanted to write faster and better.",
      "plaintextDescription": "I didn’t intend this to be a prank! I just wanted to write faster and better."
    },
    "baseScore": 10,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-09-23T14:49:31.097Z",
    "af": false
  },
  {
    "_id": "iekwELpTBgHnkHZRk",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "KtqqwqBzbd8igzK6z",
    "topLevelCommentId": "nbsDtGMuYZeDvY9bo",
    "contents": {
      "markdown": "I agree that I had had all the ideas, but I hadn't previously been able to get AIs to even do the \"grunt work\" of turning it into prose with anything like that level of quality!",
      "plaintextDescription": "I agree that I had had all the ideas, but I hadn't previously been able to get AIs to even do the \"grunt work\" of turning it into prose with anything like that level of quality!"
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-23T04:33:03.784Z",
    "af": false
  },
  {
    "_id": "nbsDtGMuYZeDvY9bo",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Fun fact: My post [Christian homeschoolers in the year 3000](https://www.lesswrong.com/posts/8aRFB2qGyjQGJkEdZ/christian-homeschoolers-in-the-year-3000) was substantially written by Claude Opus 4.1.\n\nYou can see the chat [here](https://claude.ai/share/d2c120a0-06d8-4948-b73a-67c270ba7c3c). I prompted Claude with a detailed outline, a previous draft that followed a very different structure, and a copy of \"The case for ensuring powerful AIs are controlled\" for reference about my writing style. The outline I gave Claude is in the Outline tab, and the old draft I provided is in the Old draft tab, of this [doc](https://docs.google.com/document/d/1Hpi98Gp1tVF7vAauoN4qGjVS9n0Gg_yqJJik6PagErI/edit?pli=1&tab=t.0#heading=h.mvm354rfy6lb).\n\nAs you can see, I did a bunch of back and forth with Claude to edit it. Then I copied to a Google doc and edited substantially on my own to get to the final product.\n\nI was shocked by how good Claude was at this.\n\nI believe this is in compliance with the [LLM writing assistance policy](https://www.lesswrong.com/posts/KXujJjnmP85u8eM6B/policy-for-llm-writing-on-lesswrong).",
      "plaintextDescription": "Fun fact: My post Christian homeschoolers in the year 3000 was substantially written by Claude Opus 4.1.\n\nYou can see the chat here. I prompted Claude with a detailed outline, a previous draft that followed a very different structure, and a copy of \"The case for ensuring powerful AIs are controlled\" for reference about my writing style. The outline I gave Claude is in the Outline tab, and the old draft I provided is in the Old draft tab, of this doc.\n\nAs you can see, I did a bunch of back and forth with Claude to edit it. Then I copied to a Google doc and edited substantially on my own to get to the final product.\n\nI was shocked by how good Claude was at this.\n\nI believe this is in compliance with the LLM writing assistance policy."
    },
    "baseScore": 30,
    "voteCount": 22,
    "createdAt": null,
    "postedAt": "2025-09-23T02:28:35.898Z",
    "af": false
  },
  {
    "_id": "oc2YLyFgGtTEDqbq2",
    "postId": "anFrGMskALuH7aZDw",
    "parentCommentId": "QMRDsEydjDnCbNM2E",
    "topLevelCommentId": "JDp8WxRHhHGu64ibK",
    "contents": {
      "markdown": "Yeah, I agree. The audience for this book isn't LessWrong, but lots of people seem to be acting as if pushing back on LessWrong is a defection that will hurt the book's prospects.",
      "plaintextDescription": "Yeah, I agree. The audience for this book isn't LessWrong, but lots of people seem to be acting as if pushing back on LessWrong is a defection that will hurt the book's prospects."
    },
    "baseScore": 37,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2025-09-22T17:18:44.397Z",
    "af": false
  },
  {
    "_id": "5Zo9vifvjFp9yGJEZ",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Zach Robinson, relevant because he's on the Anthropic LTBT and for [other reasons](https://www.centreforeffectivealtruism.org/team), [tweets](https://x.com/Zach_y_robinson/status/1968810665973530781):\n\n> \"If Anyone Builds It, Everyone Dies\" by @ESYudkowsky and @So8res is getting a lot of attention this week. As someone who leads an org working to reduce existential risks, I'm grateful they're pushing AI safety mainstream. But I think they're wrong about doom being inevitable. 🧵\n> \n> Don't get me wrong—I take AI existential risk seriously. But presenting doom as a foregone conclusion isn't helpful for solving the problem.\n> \n> In 2022, superforecasters and AI researchers estimated the probability of existential catastrophic risk from AI by 2100 at around 0.4%-3%. A recent study found no correlation between near-term accuracy and long-term forecasts. TL;DR: predicting the future is really hard.\n> \n> That doesn't mean we should throw the existential risk baby out with the \"Everyone Dies\" bathwater. Most of us wouldn't be willing to risk a 3% chance (or even a 0.3% chance!) of the people we love dying.\n> \n> But accepting uncertainty matters for navigating this complex challenge thoughtfully.\n> \n> Accepting uncertainty matters for two big reasons.\n> \n> First, it leaves room for AI's transformative benefits. Tech has doubled life expectancy, slashed extreme poverty, and eliminated diseases over the past two centuries. AI could accelerate these trends dramatically.\n> \n> But Yudkowsky and Soares dismiss these possibilities as \"beautiful dreams.\" If we're certain AI will kill us all, then all potential benefits get rounded to zero.\n> \n> Second, focusing exclusively on extinction scenarios blinds us to other serious AI risks: authoritarian power grabs, democratic disruption through misinformation, mass surveillance, economic displacement, new forms of inequity. These deserve attention too.\n> \n> People inspired by @EffectvAltruism have made real progress on AI safety while *also* mapping varied futures. I'm personally encouraged by recent work from @willmacaskill and others at @forethought_org on putting society on a path toward flourishing with transformative AI.\n> \n> This includes questions about AI consciousness and welfare—how should we treat AI systems that might themselves suffer or flourish? These questions sound abstract but may become practical as AI systems become more sophisticated.\n> \n> The policy debate surrounding AI development has become a false choice: stop all AI development (to prevent doom) vs. speed it up (for rewards).\n> \n> I grew up participating in debate, so I know the importance of confidence. But there are two types: epistemic (based on evidence) and social (based on delivery). Despite being expressed in self-assured language, the evidence for imminent existential risk is far from airtight.\n> \n> My take is this: We should take AI risk seriously—with all its uncertainties—and work hard to bend development toward better outcomes.\n\nOn the object level, I think Zach is massively underrating AI takeover risk, and I think that his reference to the benefits of AI [misses the point](https://www.lesswrong.com/posts/voEAJ9nFBAqau8pNN/the-title-is-reasonable?commentId=aEutHKW9fkfcx4BBA).\n\nOn the meta level, I think Zach's opinions are relevant (and IMO concerning) for people who are relying on Zach to ensure that Anthropic makes good choices about AI risks. I don't think the perspective articulated in these tweets is consistent with him doing a good job there (though maybe this was just poor phrasing on his part, and his opinions are more reasonable than this).",
      "plaintextDescription": "Zach Robinson, relevant because he's on the Anthropic LTBT and for other reasons, tweets:\n\n> \"If Anyone Builds It, Everyone Dies\" by @ESYudkowsky and @So8res is getting a lot of attention this week. As someone who leads an org working to reduce existential risks, I'm grateful they're pushing AI safety mainstream. But I think they're wrong about doom being inevitable. 🧵\n> \n> Don't get me wrong—I take AI existential risk seriously. But presenting doom as a foregone conclusion isn't helpful for solving the problem.\n> \n> In 2022, superforecasters and AI researchers estimated the probability of existential catastrophic risk from AI by 2100 at around 0.4%-3%. A recent study found no correlation between near-term accuracy and long-term forecasts. TL;DR: predicting the future is really hard.\n> \n> That doesn't mean we should throw the existential risk baby out with the \"Everyone Dies\" bathwater. Most of us wouldn't be willing to risk a 3% chance (or even a 0.3% chance!) of the people we love dying.\n> \n> But accepting uncertainty matters for navigating this complex challenge thoughtfully.\n> \n> Accepting uncertainty matters for two big reasons.\n> \n> First, it leaves room for AI's transformative benefits. Tech has doubled life expectancy, slashed extreme poverty, and eliminated diseases over the past two centuries. AI could accelerate these trends dramatically.\n> \n> But Yudkowsky and Soares dismiss these possibilities as \"beautiful dreams.\" If we're certain AI will kill us all, then all potential benefits get rounded to zero.\n> \n> Second, focusing exclusively on extinction scenarios blinds us to other serious AI risks: authoritarian power grabs, democratic disruption through misinformation, mass surveillance, economic displacement, new forms of inequity. These deserve attention too.\n> \n> People inspired by @EffectvAltruism have made real progress on AI safety while also mapping varied futures. I'm personally encouraged by recent work from @willmacaskill and others at @forethou"
    },
    "baseScore": 108,
    "voteCount": 49,
    "createdAt": null,
    "postedAt": "2025-09-21T19:44:44.920Z",
    "af": false
  },
  {
    "_id": "n9N8wyBkiyyrDp4ag",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "CctggPhBFeGrAwKZw",
    "topLevelCommentId": "JuitdRH3a7mQWEDcA",
    "contents": {
      "markdown": "I engage on LessWrong because:\n\n*   It does actually help me sharpen my intuitions and arguments. When I'm trying to understand a complicated topic, I find it really helpful to spend a bunch of time talking about it with people. It's a cheap and easy way of getting some spaced repetition.\n*   I think that despite the pretty bad epistemic problems on LessWrong, it's still the best place to talk about these issues, and so I feel invested in improving discussion of them. (I'm less pessimistic than Rohin.)\n    *   There are a bunch of extremely unreasonable MIRI partisans on LessWrong (as well as some other unreasonable groups), but I think that's a minority of people who I engage with; a lot of them just vote and don't comment.\n    *   I think that my and Redwood's engagement on LessWrong has had meaningful effects on how thoughtful LWers think about AI risk.\n*   I feel really triggered by people here being wrong about stuff, so I spend somewhat more time on it than I endorse.\n    *   This is partially because I identify strongly with the rationalist community and it hurts me to see the rationalists being unreasonable or wrong.\n\nI do think that on the margin, I wish I felt more intuitively relatively motivated to work on my writing projects that are aimed at other audiences. For example, this weekend I've been arguing on LessWrong substantially as procrastination for writing a piece about AI control aimed at computer security experts, particularly those at AI companies. I think that post will be really valuable because I think that a lot of people in that audience are pretty persuadable, and I think it's a really important point. But it's less motivating because the feedback loops are longer and the audience doesn't include many of my friends and my broader social community.",
      "plaintextDescription": "I engage on LessWrong because:\n\n * It does actually help me sharpen my intuitions and arguments. When I'm trying to understand a complicated topic, I find it really helpful to spend a bunch of time talking about it with people. It's a cheap and easy way of getting some spaced repetition.\n * I think that despite the pretty bad epistemic problems on LessWrong, it's still the best place to talk about these issues, and so I feel invested in improving discussion of them. (I'm less pessimistic than Rohin.)\n   * There are a bunch of extremely unreasonable MIRI partisans on LessWrong (as well as some other unreasonable groups), but I think that's a minority of people who I engage with; a lot of them just vote and don't comment.\n   * I think that my and Redwood's engagement on LessWrong has had meaningful effects on how thoughtful LWers think about AI risk.\n * I feel really triggered by people here being wrong about stuff, so I spend somewhat more time on it than I endorse.\n   * This is partially because I identify strongly with the rationalist community and it hurts me to see the rationalists being unreasonable or wrong.\n\nI do think that on the margin, I wish I felt more intuitively relatively motivated to work on my writing projects that are aimed at other audiences. For example, this weekend I've been arguing on LessWrong substantially as procrastination for writing a piece about AI control aimed at computer security experts, particularly those at AI companies. I think that post will be really valuable because I think that a lot of people in that audience are pretty persuadable, and I think it's a really important point. But it's less motivating because the feedback loops are longer and the audience doesn't include many of my friends and my broader social community. "
    },
    "baseScore": 22,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2025-09-21T19:34:21.130Z",
    "af": false
  },
  {
    "_id": "mcY2Wjkxt4prZ7qQH",
    "postId": "rQDCQxuCRrrN4ujAe",
    "parentCommentId": "oBZ8CXG8WDTDwGavD",
    "topLevelCommentId": "vYdtN3nHEtJfgjSSB",
    "contents": {
      "markdown": "I feel like your title for this short-form post is unreasonably aggressive, given what you're saying here.\n\nI found your articulation of the structure of the book's argument helpful and clarifying.\n\nI'm planning to write something more about this at some point: I think a key issue here is that we aren't making the kind of arguments where \"local validity\" is a reliable concept. No-one is trying to make proofs, they're trying to make defeasible heuristic arguments. Suppose the book makes an argument of the form \"Because of argument A, I believe conclusion X. You might have thought that B is a counterargument to A. But actually, because of argument C, B doesn't work.\" If Will thinks that argument C doesn't work, I think it's fine for him to summarize this as: \"they make an argument mostly around A, and which I don't think suffices to establish X\".",
      "plaintextDescription": "I feel like your title for this short-form post is unreasonably aggressive, given what you're saying here.\n\nI found your articulation of the structure of the book's argument helpful and clarifying.\n\nI'm planning to write something more about this at some point: I think a key issue here is that we aren't making the kind of arguments where \"local validity\" is a reliable concept. No-one is trying to make proofs, they're trying to make defeasible heuristic arguments. Suppose the book makes an argument of the form \"Because of argument A, I believe conclusion X. You might have thought that B is a counterargument to A. But actually, because of argument C, B doesn't work.\" If Will thinks that argument C doesn't work, I think it's fine for him to summarize this as: \"they make an argument mostly around A, and which I don't think suffices to establish X\". "
    },
    "baseScore": 11,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-21T18:23:26.256Z",
    "af": false
  },
  {
    "_id": "ab3Dgit8iGKFodS7E",
    "postId": "rQDCQxuCRrrN4ujAe",
    "parentCommentId": "oRuCzPesJZm9KBjGK",
    "topLevelCommentId": "vYdtN3nHEtJfgjSSB",
    "contents": {
      "markdown": "Whatever. I don't think that's a very important difference, and I don't think it's fair to call Will's argument a straw man based on it. I think a very small proportion of readers would confidently interpret the book's argument the way you did. \n\nYou're claiming that the book's argument is only trying to apply to an extremely narrow definition of AI training. You describe it as SGD on a training set; I assume you intend to refer to things like RL on diverse environments, like how R1 was trained. If that's what the argument is about, it's really important for the authors to explain how that argument connects to the broader notion of training used in practice, and I don't remember this happening. I don't remember them talking carefully about the still broader question of \"what happens when you do get to examine results and intermediate results and make adjustments based on observations?\"",
      "plaintextDescription": "Whatever. I don't think that's a very important difference, and I don't think it's fair to call Will's argument a straw man based on it. I think a very small proportion of readers would confidently interpret the book's argument the way you did. \n\nYou're claiming that the book's argument is only trying to apply to an extremely narrow definition of AI training. You describe it as SGD on a training set; I assume you intend to refer to things like RL on diverse environments, like how R1 was trained. If that's what the argument is about, it's really important for the authors to explain how that argument connects to the broader notion of training used in practice, and I don't remember this happening. I don't remember them talking carefully about the still broader question of \"what happens when you do get to examine results and intermediate results and make adjustments based on observations?\" "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-21T16:31:24.670Z",
    "af": false
  },
  {
    "_id": "epRgDTnKRKqpoLQJd",
    "postId": "rQDCQxuCRrrN4ujAe",
    "parentCommentId": "Eizv8FpJQCBw64B3z",
    "topLevelCommentId": "vYdtN3nHEtJfgjSSB",
    "contents": {
      "markdown": "I don't think that the MIRI book would hold up if you analyzed it with this level of persnicketiness–they were absolutely not precise at the level of distinguishing between the whole development process and single training runs. (Which is arguably fine–they were trying to write a popular book, not trying to persuade super high-context readers of anything!) So this complaint strikes me as somewhat of an isolated demand for rigor.",
      "plaintextDescription": "I don't think that the MIRI book would hold up if you analyzed it with this level of persnicketiness–they were absolutely not precise at the level of distinguishing between the whole development process and single training runs. (Which is arguably fine–they were trying to write a popular book, not trying to persuade super high-context readers of anything!) So this complaint strikes me as somewhat of an isolated demand for rigor."
    },
    "baseScore": 30,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-09-21T16:16:58.448Z",
    "af": false
  },
  {
    "_id": "Eizv8FpJQCBw64B3z",
    "postId": "rQDCQxuCRrrN4ujAe",
    "parentCommentId": "vYdtN3nHEtJfgjSSB",
    "topLevelCommentId": "vYdtN3nHEtJfgjSSB",
    "contents": {
      "markdown": "> So these dis-analogies don't directly engage with the argument. If they were directly engaging with the first part of the argument, they would be about the predictability of a single training run, rather than the total AI research process.\n\nThis seems wrong to me. Which of the things that Will says couldn't be considered to be about a single training process?",
      "plaintextDescription": "> So these dis-analogies don't directly engage with the argument. If they were directly engaging with the first part of the argument, they would be about the predictability of a single training run, rather than the total AI research process.\n\nThis seems wrong to me. Which of the things that Will says couldn't be considered to be about a single training process? "
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-21T15:55:12.974Z",
    "af": false
  },
  {
    "_id": "vWTrCiTpPLqAcMBBh",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "7nYtHZmFGqYFJC25X",
    "topLevelCommentId": "7nYtHZmFGqYFJC25X",
    "contents": {
      "markdown": "It depends on what you think is a glorious future. As I said in my conclusion above, I personally think that the situation I described is kind of lame and depressing, and also there is some possibility that it is bad in a scope-sensitive way if these enclaves continue to control large proportions of the resources of the future (e.g. because their members have a fixed proportion of voting power over the decisions made by the US government).",
      "plaintextDescription": "It depends on what you think is a glorious future. As I said in my conclusion above, I personally think that the situation I described is kind of lame and depressing, and also there is some possibility that it is bad in a scope-sensitive way if these enclaves continue to control large proportions of the resources of the future (e.g. because their members have a fixed proportion of voting power over the decisions made by the US government). "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-21T01:00:38.964Z",
    "af": false
  },
  {
    "_id": "2rgABivimHypfHFmL",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "MEJh8m5mP85qwLqCu",
    "topLevelCommentId": "AstHKDApQQtTHokiA",
    "contents": {
      "markdown": "I think me and Will and Kelsey have similar positions (having talked with both of them about this quite a lot), we just emphasized different disagreements.\n\n(Except that I am less sold than Kelsey on her \"maybe we can have non-agentic substitutes\" point.)",
      "plaintextDescription": "I think me and Will and Kelsey have similar positions (having talked with both of them about this quite a lot), we just emphasized different disagreements.\n\n(Except that I am less sold than Kelsey on her \"maybe we can have non-agentic substitutes\" point.)"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-20T21:29:28.178Z",
    "af": false
  },
  {
    "_id": "Sarh82HjZD3LkPkFL",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "CHyqYwpjRQLccjMuK",
    "topLevelCommentId": "fZ5fz6BSnquBGxQJE",
    "contents": {
      "markdown": "FWIW, I have read those parts of the online resources.\n\nYou can obviously summarize me however you like, but my favorite summary of my position is something like \"A lot of things will have changed about the situation by the time that it's possible to build ASI. It's definitely not obvious that those changes mean that we're okay. But I think that they are a mechanically important aspect of the situation to understand, and I think they substantially reduce AI takeover risk.\"",
      "plaintextDescription": "FWIW, I have read those parts of the online resources.\n\nYou can obviously summarize me however you like, but my favorite summary of my position is something like \"A lot of things will have changed about the situation by the time that it's possible to build ASI. It's definitely not obvious that those changes mean that we're okay. But I think that they are a mechanically important aspect of the situation to understand, and I think they substantially reduce AI takeover risk.\""
    },
    "baseScore": 17,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-20T19:11:03.547Z",
    "af": false
  },
  {
    "_id": "Kkn3d6MjC4qPy6pMS",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "FeyHd6WxHPkXzw7gN",
    "topLevelCommentId": "fZ5fz6BSnquBGxQJE",
    "contents": {
      "markdown": "> The book aimed at a general audience does not do enough justice to my unpublished plan for pitting AIs against AIs\n\nFWIW that's not at all what I mean (and I don't know of anyone who's said that). What I mean is much more like what Ryan said here:\n\n> I expect that *by default* superintelligence is built after a point where we have access to huge amounts of non-superintelligent cognitive labor so it's unlikely that we'll be using current methods and current understanding (unless humans have already lost control by this point, which seems totally plausibly, but not overwhelmingly likely nor argued convincingly for by the book). Even just looking at capabilities, I think it's pretty likely that automated AI R&D will result in us operating in a totally different paradigm by the time we build superintelligence---this isn't to say this other paradigm will be safer, just that a narrow description of \"current techniques\" doesn't include the default trajectory.",
      "plaintextDescription": "> The book aimed at a general audience does not do enough justice to my unpublished plan for pitting AIs against AIs\n\nFWIW that's not at all what I mean (and I don't know of anyone who's said that). What I mean is much more like what Ryan said here:\n\n> I expect that by default superintelligence is built after a point where we have access to huge amounts of non-superintelligent cognitive labor so it's unlikely that we'll be using current methods and current understanding (unless humans have already lost control by this point, which seems totally plausibly, but not overwhelmingly likely nor argued convincingly for by the book). Even just looking at capabilities, I think it's pretty likely that automated AI R&D will result in us operating in a totally different paradigm by the time we build superintelligence---this isn't to say this other paradigm will be safer, just that a narrow description of \"current techniques\" doesn't include the default trajectory."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-20T18:47:54.913Z",
    "af": false
  },
  {
    "_id": "pHENH2M4frPpFW3ss",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "q6sPx4uNYdjAC7wik",
    "topLevelCommentId": "AstHKDApQQtTHokiA",
    "contents": {
      "markdown": "> Like, I think the overlap between your criticisms and the criticisms of other people in the community is actually pretty low,\n\nI disagree; I think my core complaints about their arguments are very similar to Will MacAskill, Kelsey Piper, and the majority of people whose ideas on AI safety I have a moderate amount of respect for. I agree that some other LW people agree with the parts I think are wrong, but that's not what I'm talking about.",
      "plaintextDescription": "> Like, I think the overlap between your criticisms and the criticisms of other people in the community is actually pretty low,\n\nI disagree; I think my core complaints about their arguments are very similar to Will MacAskill, Kelsey Piper, and the majority of people whose ideas on AI safety I have a moderate amount of respect for. I agree that some other LW people agree with the parts I think are wrong, but that's not what I'm talking about."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-20T06:03:25.042Z",
    "af": false
  },
  {
    "_id": "AstHKDApQQtTHokiA",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Since writing this post, I have updated towards it's a bigger problem that this book's core thesis is wrong. It's pretty annoying that me and similar people who recommend the book have to caveat with \"obviously this is overblown and the arguments are skipping a bunch of important steps, but I think the issue described is real and important\".",
      "plaintextDescription": "Since writing this post, I have updated towards it's a bigger problem that this book's core thesis is wrong. It's pretty annoying that me and similar people who recommend the book have to caveat with \"obviously this is overblown and the arguments are skipping a bunch of important steps, but I think the issue described is real and important\"."
    },
    "baseScore": 6,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-19T22:22:48.275Z",
    "af": false
  },
  {
    "_id": "pvowgN4A5cByRKfsx",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "fjo9AydRPTyurNaqv",
    "topLevelCommentId": "fjo9AydRPTyurNaqv",
    "contents": {
      "markdown": "It depends on what you mean by \"foolish plan.\" In a world where the median person had my beliefs, I totally agree that AI risk would be handled extremely differently (though idk if \"pause now\" is a good description of what we'd do). I totally agree that this world is making a huge error according to my values and most human values by not being much more cautious here. \n\nI don't think \"we might be okay\" is a valid objection to the claim \"AI might cause an existential catastrophe\"—those claims are totally consistent. But I *do* think:\n\n*   It's a valid objection to \"AI is almost guaranteed to cause an existential catastrophe.\"\n*   It's a very important aspect of the situation, and descriptions of AI risk that don't mention it seem to me like they're failing to discuss a crucial part of how it all goes down.",
      "plaintextDescription": "It depends on what you mean by \"foolish plan.\" In a world where the median person had my beliefs, I totally agree that AI risk would be handled extremely differently (though idk if \"pause now\" is a good description of what we'd do). I totally agree that this world is making a huge error according to my values and most human values by not being much more cautious here. \n\nI don't think \"we might be okay\" is a valid objection to the claim \"AI might cause an existential catastrophe\"—those claims are totally consistent. But I do think:\n\n * It's a valid objection to \"AI is almost guaranteed to cause an existential catastrophe.\"\n * It's a very important aspect of the situation, and descriptions of AI risk that don't mention it seem to me like they're failing to discuss a crucial part of how it all goes down."
    },
    "baseScore": 4,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-19T03:41:15.219Z",
    "af": false
  },
  {
    "_id": "YpGcCgK53nGmSttLM",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "uqrYCsfhAce4RNjaj",
    "topLevelCommentId": "uqrYCsfhAce4RNjaj",
    "contents": {
      "markdown": "> On your [comment](https://www.lesswrong.com/posts/8aRFB2qGyjQGJkEdZ/christian-homeschoolers-in-the-year-3000#iScpFaFszRovCmQZM) about \"centrally enforced policy\" being \"kind of fucked up and illiberal\", I think there is some hope that given enough time and effort, there can be a relatively uncontroversial solution to metaphilosophy[^\\[1\\]^](https://www.lesswrong.com/posts/8aRFB2qGyjQGJkEdZ/christian-homeschoolers-in-the-year-3000#fn-NvTMdrDJ4ivbWPqTT-1), that most people can agree on at the end of the AI pause so central enforcement wouldn't be needed.\n\nI am worried that it is impossible to come up with a solution to meta-philosophy that is uncontroversial because a reasonable fraction of people will evaluate a meta-philosophy by whether it invalidates particular object-level beliefs of theirs, and will be impossible to convince to change their evaluation strategy (except by using persuasion tactics that could have been [symmetrically](https://slatestarcodex.com/2017/03/24/guided-by-the-beauty-of-our-weapons/) applied to persuade them of lots of other stuff).",
      "plaintextDescription": "> On your comment about \"centrally enforced policy\" being \"kind of fucked up and illiberal\", I think there is some hope that given enough time and effort, there can be a relatively uncontroversial solution to metaphilosophy[1], that most people can agree on at the end of the AI pause so central enforcement wouldn't be needed.\n\nI am worried that it is impossible to come up with a solution to meta-philosophy that is uncontroversial because a reasonable fraction of people will evaluate a meta-philosophy by whether it invalidates particular object-level beliefs of theirs, and will be impossible to convince to change their evaluation strategy (except by using persuasion tactics that could have been symmetrically applied to persuade them of lots of other stuff)."
    },
    "baseScore": 16,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-18T20:11:56.007Z",
    "af": false
  },
  {
    "_id": "byXK3rdybMPLFWKW9",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "ForYaQqnyZQoenAHd",
    "topLevelCommentId": "NyhF3KiDYuyvpitDb",
    "contents": {
      "markdown": "It seems like you think there is some asymmetry between people talking to honest chatbots and people talking to dishonest chatbots that want to take all their stuff. I feel like there isn't a structural difference between those. It's going to be totally reasonable to want to have AIs watch over the conversations that you and/or your children are having with any other chatbot.",
      "plaintextDescription": "It seems like you think there is some asymmetry between people talking to honest chatbots and people talking to dishonest chatbots that want to take all their stuff. I feel like there isn't a structural difference between those. It's going to be totally reasonable to want to have AIs watch over the conversations that you and/or your children are having with any other chatbot. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-18T01:47:12.744Z",
    "af": false
  },
  {
    "_id": "ziRPCuatrECks8Byg",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "5sEg83Ncn3dLRirDp",
    "topLevelCommentId": "NyhF3KiDYuyvpitDb",
    "contents": {
      "markdown": "I don't quite understand your argument here. Suppose you give people the choice between two chatbots, and they know one will cause them to deconvert, and the other one won't. I think it's pretty likely they'll prefer the latter. Are you imagining that no one will offer to sell them the latter?",
      "plaintextDescription": "I don't quite understand your argument here. Suppose you give people the choice between two chatbots, and they know one will cause them to deconvert, and the other one won't. I think it's pretty likely they'll prefer the latter. Are you imagining that no one will offer to sell them the latter? "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-18T01:05:08.176Z",
    "af": false
  },
  {
    "_id": "cDDTysgJ7gXZF8m96",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "4c5czxhsZeXKbGLRd",
    "topLevelCommentId": "NyhF3KiDYuyvpitDb",
    "contents": {
      "markdown": "> I am also sympathetic to the higher level point. If we have AIs sufficiently aligned that we can make them follow the OpenAI model spec, I'd be at least somewhat surprised if governments and AI companies didn't find a way to craft model specs that avoided the problems described in this post.\n\nI agree that it is unclear whether this would go the way I hypothesized here. But I think it's arguably the straightforward extrapolation of how America handles this kind of problem: are you surprised that America allows parents to homeschool their children?\n\nNote also that AI companies can compete on how user-validating the models are, and in absence of some kind of regulation, people will be able to pay for models that have the properties they want.",
      "plaintextDescription": "> I am also sympathetic to the higher level point. If we have AIs sufficiently aligned that we can make them follow the OpenAI model spec, I'd be at least somewhat surprised if governments and AI companies didn't find a way to craft model specs that avoided the problems described in this post.\n\nI agree that it is unclear whether this would go the way I hypothesized here. But I think it's arguably the straightforward extrapolation of how America handles this kind of problem: are you surprised that America allows parents to homeschool their children?\n\nNote also that AI companies can compete on how user-validating the models are, and in absence of some kind of regulation, people will be able to pay for models that have the properties they want."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-18T00:11:35.358Z",
    "af": false
  },
  {
    "_id": "aaqufprtcfFrGMZfd",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "DkGHTZfhv7BRQNibv",
    "topLevelCommentId": "NyhF3KiDYuyvpitDb",
    "contents": {
      "markdown": "I don't think it's obviously appropriate for AI companies to decide exactly what reflection process and exposure to society people who use their AIs need to have. And people might prefer to use AIs that do conform to their beliefs if they are allowed to buy access to them. So I think you might have to ban such AIs in order to prevent people from using them.",
      "plaintextDescription": "I don't think it's obviously appropriate for AI companies to decide exactly what reflection process and exposure to society people who use their AIs need to have. And people might prefer to use AIs that do conform to their beliefs if they are allowed to buy access to them. So I think you might have to ban such AIs in order to prevent people from using them. "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-17T21:01:51.935Z",
    "af": false
  },
  {
    "_id": "iScpFaFszRovCmQZM",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "NyhF3KiDYuyvpitDb",
    "topLevelCommentId": "NyhF3KiDYuyvpitDb",
    "contents": {
      "markdown": "I try to address in my \"analysis through swerving around obstacles\" section. \n\nThe argument you're giving against the scenario I spell out here is \"surely the AIs will instead be designed to promote a particular set of mandated policies on how you should reflect and update over time.\" I agree that that's conceivable. But it also seems kind of fucked up and illiberal.\n\nI want to explore a scenario where there isn't a centrally enforced policy on this stuff, where people are able to use the AIs as they wish. I think that this is a plausible path society could take (e.g. it's kind of analogous to how you're allowed to isolate yourself and your family now).",
      "plaintextDescription": "I try to address in my \"analysis through swerving around obstacles\" section. \n\nThe argument you're giving against the scenario I spell out here is \"surely the AIs will instead be designed to promote a particular set of mandated policies on how you should reflect and update over time.\" I agree that that's conceivable. But it also seems kind of fucked up and illiberal.\n\nI want to explore a scenario where there isn't a centrally enforced policy on this stuff, where people are able to use the AIs as they wish. I think that this is a plausible path society could take (e.g. it's kind of analogous to how you're allowed to isolate yourself and your family now). "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-17T19:43:03.065Z",
    "af": false
  },
  {
    "_id": "uSbbPpFMFpkqS43mF",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "GEf2HiuSLaLPnHgto",
    "topLevelCommentId": "GEf2HiuSLaLPnHgto",
    "contents": {
      "markdown": "I can't believe I screwed that up, lol.",
      "plaintextDescription": "I can't believe I screwed that up, lol. "
    },
    "baseScore": 11,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-17T19:21:12.730Z",
    "af": false
  },
  {
    "_id": "E7a28BxKp4QREgF3T",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "6pq26GFCfCNW9fBHM",
    "topLevelCommentId": "6pq26GFCfCNW9fBHM",
    "contents": {
      "markdown": "I think both are big problems. Maybe I should have been clearer about the symmetry here. The thesis I care about here is pretty symmetrical between those problems.",
      "plaintextDescription": "I think both are big problems. Maybe I should have been clearer about the symmetry here. The thesis I care about here is pretty symmetrical between those problems."
    },
    "baseScore": 15,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-17T19:20:51.641Z",
    "af": false
  },
  {
    "_id": "bhEWyAoBgr7Rft2CR",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "AuW7rmsLF8erAFxcC",
    "topLevelCommentId": "AuW7rmsLF8erAFxcC",
    "contents": {
      "markdown": "My concern here is that maybe they would change their minds if they really reflected on it, but it's not clear that they will in fact do that reflection if they have to decide quickly whether to do so!",
      "plaintextDescription": "My concern here is that maybe they would change their minds if they really reflected on it, but it's not clear that they will in fact do that reflection if they have to decide quickly whether to do so!"
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-17T16:13:07.957Z",
    "af": false
  },
  {
    "_id": "xmNz3hH5qxtAq4Nnr",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "RzqspzrDmPuaxwbnM",
    "topLevelCommentId": "RzqspzrDmPuaxwbnM",
    "contents": {
      "markdown": "oh thanks, fixed. I just internally substitute \"AI takeover\" anytime anyone says \"AI kills everyone\" because this comes up constantly, and I'd forgotten that I'd done so here :P",
      "plaintextDescription": "oh thanks, fixed. I just internally substitute \"AI takeover\" anytime anyone says \"AI kills everyone\" because this comes up constantly, and I'd forgotten that I'd done so here :P"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-17T15:32:15.042Z",
    "af": false
  },
  {
    "_id": "D4BDCBnTn99neLDKG",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "ZwzbpQRTqawXcNCqk",
    "topLevelCommentId": "ZwzbpQRTqawXcNCqk",
    "contents": {
      "markdown": "I've seen physical copies around, but I actually haven't read it. It's possible that you're totally right, in which case I apologize and should have finished my review with \"I'm an idiot for not realizing it was worth my time to read Uncontrollable so that I could recommend it to people\".\n\nI would appreciate a digital copy and audio copy if you wanted to email them to me! I'm not sure I'll consume it because I don't know if it's that decision-relevant to me.",
      "plaintextDescription": "I've seen physical copies around, but I actually haven't read it. It's possible that you're totally right, in which case I apologize and should have finished my review with \"I'm an idiot for not realizing it was worth my time to read Uncontrollable so that I could recommend it to people\".\n\nI would appreciate a digital copy and audio copy if you wanted to email them to me! I'm not sure I'll consume it because I don't know if it's that decision-relevant to me. "
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-17T15:22:07.020Z",
    "af": false
  },
  {
    "_id": "nqnY2TweTcGupk5dg",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": "qivj58JyNygdvA8Jd",
    "topLevelCommentId": "qivj58JyNygdvA8Jd",
    "contents": {
      "markdown": "I think it's often pretty subjective whether some piece of external stimulus is super-optimized manipulation that will take you further away from what you want to believe, or part of the natural and good process of cultural change and increased reflection.\n\nI agree with you that the distinction is clearer for the hyper-optimized persuasion.",
      "plaintextDescription": "I think it's often pretty subjective whether some piece of external stimulus is super-optimized manipulation that will take you further away from what you want to believe, or part of the natural and good process of cultural change and increased reflection.\n\nI agree with you that the distinction is clearer for the hyper-optimized persuasion. "
    },
    "baseScore": 13,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-09-17T15:20:18.401Z",
    "af": false
  },
  {
    "_id": "acNPM8yrHf7Dz8Ddj",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Why did I like the book so much more than I expected? I think it's a mix of:\n\n*   I like the authors' writing on basic AI risk stuff but I don't like their writing on more in-the-weeds questions, and I run across their in-the-weeds writing much more in my day-to-day life, so it's surprisingly pleasant to read them writing intro materials.\n*   Their presentation of the arguments were cleaner here than I've previously seen.",
      "plaintextDescription": "Why did I like the book so much more than I expected? I think it's a mix of:\n\n * I like the authors' writing on basic AI risk stuff but I don't like their writing on more in-the-weeds questions, and I run across their in-the-weeds writing much more in my day-to-day life, so it's surprisingly pleasant to read them writing intro materials.\n * Their presentation of the arguments were cleaner here than I've previously seen."
    },
    "baseScore": 39,
    "voteCount": 23,
    "createdAt": null,
    "postedAt": "2025-09-17T05:04:27.774Z",
    "af": false
  },
  {
    "_id": "zbirvcMFMP3BgoQbo",
    "postId": "w3KtPQDMF4GGR3YLp",
    "parentCommentId": "rxm29JkdJYghz4WBt",
    "topLevelCommentId": "rxm29JkdJYghz4WBt",
    "contents": {
      "markdown": "I am not aware of any critical people inside this community (e.g. people who would be similarly critical to me) getting copies in advance, except for people who write book reviews.",
      "plaintextDescription": "I am not aware of any critical people inside this community (e.g. people who would be similarly critical to me) getting copies in advance, except for people who write book reviews."
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-16T03:51:44.321Z",
    "af": false
  },
  {
    "_id": "db5u3GbkAhMd5yCHN",
    "postId": "PBd7xPAh22y66rbme",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Despite the shift, 80,000 Hours [continues](https://www.lesswrong.com/posts/8qCwuE8GjrYPSqbri/80-000-hours-should-remove-openai-from-the-job-board-and?commentId=F6vwk8TZzPmFvX2S7) to recommend talented engineers to join Anthropic.\n\n[FWIW, it looks to me like](https://jobs.80000hours.org/?refinementList%5Bcompany_data%5D%5B0%5D=Anthropic) they restrict their linked roles to things that are vaguely related to safety or alignment. (I think that the 80,000 Hours job board does include some roles that don't have a plausible mechanism for improving AI outcomes except via the route of making Anthropic more powerful, e.g. the alignment fine-tuning role.)",
      "plaintextDescription": "> Despite the shift, 80,000 Hours continues to recommend talented engineers to join Anthropic.\n\n \n\nFWIW, it looks to me like they restrict their linked roles to things that are vaguely related to safety or alignment. (I think that the 80,000 Hours job board does include some roles that don't have a plausible mechanism for improving AI outcomes except via the route of making Anthropic more powerful, e.g. the alignment fine-tuning role.)"
    },
    "baseScore": 10,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-02T00:34:09.242Z",
    "af": false
  },
  {
    "_id": "9k4QYiaK95fujGTZM",
    "postId": "auKWgpdiBwreB62Kh",
    "parentCommentId": "FoRaNp5hoFMWAmaWg",
    "topLevelCommentId": "FoRaNp5hoFMWAmaWg",
    "contents": {
      "markdown": "This can matter for deployment as well as research! Back in 2021, a friend of mine made this mistake while training a customer service model, leading to the model making an extremely inappropriate sexual comment while being demoed to a potential customer; he eventually figured out that a user had said that to the model at some point.\n\nThat said I'm not actually sure why in general it would be a mistake in practice to train on the combination. Often models improve their performance when you train them on side tasks that have some relevance to what they're supposed to be doing---that's the whole point of pretraining. How much are you saying that it's a mistake to do this for deployment, rather than problematic when you are trying to experiment on generalization?",
      "plaintextDescription": "This can matter for deployment as well as research! Back in 2021, a friend of mine made this mistake while training a customer service model, leading to the model making an extremely inappropriate sexual comment while being demoed to a potential customer; he eventually figured out that a user had said that to the model at some point.\n\nThat said I'm not actually sure why in general it would be a mistake in practice to train on the combination. Often models improve their performance when you train them on side tasks that have some relevance to what they're supposed to be doing---that's the whole point of pretraining. How much are you saying that it's a mistake to do this for deployment, rather than problematic when you are trying to experiment on generalization?"
    },
    "baseScore": 11,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-31T18:43:57.468Z",
    "af": false
  },
  {
    "_id": "ihgwfwxovoJ6gPwDd",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "8rsf286hAXFkZ9utt",
    "topLevelCommentId": "FawiyPgm5GgLychNg",
    "contents": {
      "markdown": "Even though the basic ideas are kind of obvious, I think that us thinking them through and pushing on them has made a big difference in what companies are planning to do.",
      "plaintextDescription": "Even though the basic ideas are kind of obvious, I think that us thinking them through and pushing on them has made a big difference in what companies are planning to do."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-29T16:27:45.373Z",
    "af": false
  },
  {
    "_id": "KESxuJ2tnQwgsyCei",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "AudP8djw4goyjzCie",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "The bioanchors post was released in 2020. I really wish that you bothered to get basic facts right when being so derisive about people's work.\n\nI also think it's bad manners for you to criticize other people for making clear predictions given that you didn't make such predictions publicly yourself.",
      "plaintextDescription": "The bioanchors post was released in 2020. I really wish that you bothered to get basic facts right when being so derisive about people's work.\n\nI also think it's bad manners for you to criticize other people for making clear predictions given that you didn't make such predictions publicly yourself."
    },
    "baseScore": 20,
    "voteCount": 25,
    "createdAt": null,
    "postedAt": "2025-08-29T15:36:12.756Z",
    "af": false
  },
  {
    "_id": "SPLzC3Rjpp6cEZ58f",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "4Jxg7vn7wRxwr4cH4",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "Notably, bioanchors doesn't say that we should be confident AI won't arrive before 2040! Here's Ajeya's distribution in the report (which was [finished in about July 2020](https://www.alignmentforum.org/posts/AfH2oPHCApdKicM4m/two-year-update-on-my-personal-ai-timelines)).\n\n![](https://epoch.ai/assets/images/unsorted/1fbd692d136208f4.png)",
      "plaintextDescription": "Notably, bioanchors doesn't say that we should be confident AI won't arrive before 2040! Here's Ajeya's distribution in the report (which was finished in about July 2020)."
    },
    "baseScore": 10,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-29T15:31:29.239Z",
    "af": false
  },
  {
    "_id": "wfNMofrg4f25zAAwg",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "AudP8djw4goyjzCie",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "My core argument in this post isn't really relevant to anything that was happening in 2020, because people weren't really pushing on concrete changes to safety practices at AI companies yet.",
      "plaintextDescription": "My core argument in this post isn't really relevant to anything that was happening in 2020, because people weren't really pushing on concrete changes to safety practices at AI companies yet."
    },
    "baseScore": 7,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-29T14:56:24.046Z",
    "af": false
  },
  {
    "_id": "s754HCFApLgoyj9nM",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "LfHcodvf2gnXw7Xfy",
    "topLevelCommentId": "bRToWiJyoMgcpfvmw",
    "contents": {
      "markdown": "Ugh, I think you're totally right and I was being sloppy; I totally unreasonably interpreted Eliezer as saying that he was wrong about how long/how hard/how expensive it would be to get between capability levels. (But maybe Eliezer misinterpreted himself the same way? His subsequent tweets are consistent with this interpretation.)\n\nI totally agree with Eliezer's point in that post, though I do wish that he had been clearer about what exactly he was saying.",
      "plaintextDescription": "Ugh, I think you're totally right and I was being sloppy; I totally unreasonably interpreted Eliezer as saying that he was wrong about how long/how hard/how expensive it would be to get between capability levels. (But maybe Eliezer misinterpreted himself the same way? His subsequent tweets are consistent with this interpretation.)\n\nI totally agree with Eliezer's point in that post, though I do wish that he had been clearer about what exactly he was saying."
    },
    "baseScore": 38,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-08-25T20:04:38.630Z",
    "af": true
  },
  {
    "_id": "LakoaWoucyB4LqA2G",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "LBYBPWDpxSCnpFxsg",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "Another important point here is if there was substantial economic incentive to build strong Go players, then powerful Go players would have built earlier, and the time between players of those two levels would probably have been more longer.",
      "plaintextDescription": "Another important point here is if there was substantial economic incentive to build strong Go players, then powerful Go players would have built earlier, and the time between players of those two levels would probably have been more longer."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-24T16:53:11.744Z",
    "af": false
  },
  {
    "_id": "Cfn8fEaDxAa9qdMud",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "7AbhAsrmrEmwhfJFd",
    "topLevelCommentId": "bRToWiJyoMgcpfvmw",
    "contents": {
      "markdown": "Thanks heaps for pulling this up! I totally agree with Eliezer's point there.",
      "plaintextDescription": "Thanks heaps for pulling this up! I totally agree with Eliezer's point there."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-24T16:45:51.299Z",
    "af": false
  },
  {
    "_id": "NwCf3n9KYQNAGrd4E",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "eeZ2u9CavqkWpo5TK",
    "topLevelCommentId": "bRToWiJyoMgcpfvmw",
    "contents": {
      "markdown": "It really depends what you mean by a small amount of time. On a cosmic scale, ten years is indeed short. But I definitely interpreted Eliezer back then (for example, while I worked at MIRI) as making a way stronger claim than this; that we'd e.g. within a few days/weeks/months go from AI that was almost totally incapable of intellectual work to AI that can overpower humanity. And I think you need to believe that much stronger claim in order for a lot of the predictions about the future that MIRI-sphere people were making back then to make sense. I wish we had all been clearer at the time about what specifically everyone was predicting.",
      "plaintextDescription": "It really depends what you mean by a small amount of time. On a cosmic scale, ten years is indeed short. But I definitely interpreted Eliezer back then (for example, while I worked at MIRI) as making a way stronger claim than this; that we'd e.g. within a few days/weeks/months go from AI that was almost totally incapable of intellectual work to AI that can overpower humanity. And I think you need to believe that much stronger claim in order for a lot of the predictions about the future that MIRI-sphere people were making back then to make sense. I wish we had all been clearer at the time about what specifically everyone was predicting."
    },
    "baseScore": 34,
    "voteCount": 20,
    "createdAt": null,
    "postedAt": "2025-08-24T16:34:09.757Z",
    "af": false
  },
  {
    "_id": "BqseQSS3NtnqW7NF7",
    "postId": "iZPKuuWsDXAcQWbLJ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Shorter common tokens are (correctly) learned to be higher-probability because they have the combined probability of any word they could complete.\n\nI don't think this is right. LLMs are trained with *teacher forcing on a fixed tokenization*. For common words the tokenizer provides a **single long token** (e.g., “ the”, “ and”, “ cat”). The model is trained to put probability on *that* token—not on a shorter prefix like “ c”. So a short token does **not** “inherit the combined probability of any word it could complete.” Those completions were usually seen as longer tokens during training.\n\nYou can check this by running inference on an LM; if you ask the model to complete \"hello\" it will put \" world\" above \" \" even though both are typically tokens.\n\nOn a quick skim, I think that the rest of your arguments are plausible.",
      "plaintextDescription": "> Shorter common tokens are (correctly) learned to be higher-probability because they have the combined probability of any word they could complete.\n\nI don't think this is right. LLMs are trained with teacher forcing on a fixed tokenization. For common words the tokenizer provides a single long token (e.g., “ the”, “ and”, “ cat”). The model is trained to put probability on that token—not on a shorter prefix like “ c”. So a short token does not “inherit the combined probability of any word it could complete.” Those completions were usually seen as longer tokens during training.\n\nYou can check this by running inference on an LM; if you ask the model to complete \"hello\" it will put \" world\" above \" \" even though both are typically tokens.\n\nOn a quick skim, I think that the rest of your arguments are plausible."
    },
    "baseScore": 11,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-24T16:27:44.664Z",
    "af": false
  },
  {
    "_id": "bRToWiJyoMgcpfvmw",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "[@Eliezer Yudkowsky](https://www.lesswrong.com/users/eliezer_yudkowsky?mention=user) tweets:\n\n> \\> @julianboolean_: the biggest lesson I've learned from the last few years is that the \"tiny gap between village idiot and Einstein\" chart was completely wrong\n> \n> I agree that I underestimated this distance, at least partially out of youthful idealism.\n> \n> That said, one of the few places where my peers managed to put forth a clear contrary bet was on this case.  And I did happen to win that bet.  This was less than 7% of the distance in AI's 75-year journey!  And arguably the village-idiot level was only reached as of 4o or o1.\n\nI was very interested to see this tweet. I have thought of that \"Village Idiot and Einstein\" claim as the most obvious example of a way that Eliezer and co were super wrong about how AI would go, and they've AFAIK totally failed to publicly reckon with it as it's become increasingly obvious that they were wrong over the last eight years.\n\nIt's helpful to see Eliezer clarify what he thinks of this point. I would love to see more from him on this--why he got this wrong, how updating changes his opinion about the rest of the problem, what he thinks now about time between different levels of intelligence.",
      "plaintextDescription": "@Eliezer Yudkowsky tweets:\n\n> > @julianboolean_: the biggest lesson I've learned from the last few years is that the \"tiny gap between village idiot and Einstein\" chart was completely wrong\n> \n> I agree that I underestimated this distance, at least partially out of youthful idealism.\n> \n> That said, one of the few places where my peers managed to put forth a clear contrary bet was on this case.  And I did happen to win that bet.  This was less than 7% of the distance in AI's 75-year journey!  And arguably the village-idiot level was only reached as of 4o or o1.\n\nI was very interested to see this tweet. I have thought of that \"Village Idiot and Einstein\" claim as the most obvious example of a way that Eliezer and co were super wrong about how AI would go, and they've AFAIK totally failed to publicly reckon with it as it's become increasingly obvious that they were wrong over the last eight years.\n\nIt's helpful to see Eliezer clarify what he thinks of this point. I would love to see more from him on this--why he got this wrong, how updating changes his opinion about the rest of the problem, what he thinks now about time between different levels of intelligence."
    },
    "baseScore": 69,
    "voteCount": 38,
    "createdAt": null,
    "postedAt": "2025-08-24T14:54:25.287Z",
    "af": true
  },
  {
    "_id": "9tJ42fqJuCRn7ETCS",
    "postId": "4mBaixwf4k8jk7fG4",
    "parentCommentId": "ydDeos6tHcpSSJvnA",
    "topLevelCommentId": "dbJpeCAf2hz7kh7HE",
    "contents": {
      "markdown": "I think \"What's your P(AI takeover)\" is a totally reasonable question and don't understand Eliezer's problem with it (or his problem with asking about timelines). Especially given that he is actually very (and IMO overconfidently) opinionated on this topic! I think it would be crazy to never give bottom-line estimates for P(AI takeover) when talking about AI takeover risk.\n\n(I think talking about timelines is more important because it's more directly decision relevant (whereas P(AI takeover) is downstream of the decision-relevant variables).)\n\nIn practice the numbers I think are valuable to talk about are mostly P(some bad outcome|some setting of the latent facts about misalignment, and some setting of what risk mitigation strategy is employed). Like, I'm constantly thinking about whether to try to intervene on worlds that are more like Plan A worlds or more like Plan D worlds, and to do this you obviously need to think about the effect on P(AI takeover) of your actions in those worlds.",
      "plaintextDescription": "I think \"What's your P(AI takeover)\" is a totally reasonable question and don't understand Eliezer's problem with it (or his problem with asking about timelines). Especially given that he is actually very (and IMO overconfidently) opinionated on this topic! I think it would be crazy to never give bottom-line estimates for P(AI takeover) when talking about AI takeover risk.\n\n(I think talking about timelines is more important because it's more directly decision relevant (whereas P(AI takeover) is downstream of the decision-relevant variables).)\n\nIn practice the numbers I think are valuable to talk about are mostly P(some bad outcome|some setting of the latent facts about misalignment, and some setting of what risk mitigation strategy is employed). Like, I'm constantly thinking about whether to try to intervene on worlds that are more like Plan A worlds or more like Plan D worlds, and to do this you obviously need to think about the effect on P(AI takeover) of your actions in those worlds."
    },
    "baseScore": 9,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-23T18:39:48.969Z",
    "af": false
  },
  {
    "_id": "7MBJnWBZQPzmiZrKZ",
    "postId": "4mBaixwf4k8jk7fG4",
    "parentCommentId": "dbJpeCAf2hz7kh7HE",
    "topLevelCommentId": "dbJpeCAf2hz7kh7HE",
    "contents": {
      "markdown": "As another note: my biggest problem with talking about P(doom) is that it's ambiguous between various different events like AI takeover, human extinction, human extinction due to AI takeover, the loss of all value in the future, and it seems pretty important to separate these out because I think the numbers are pretty different.",
      "plaintextDescription": "As another note: my biggest problem with talking about P(doom) is that it's ambiguous between various different events like AI takeover, human extinction, human extinction due to AI takeover, the loss of all value in the future, and it seems pretty important to separate these out because I think the numbers are pretty different."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-23T01:06:16.994Z",
    "af": false
  },
  {
    "_id": "X9RsEvanyEzcfwbeh",
    "postId": "4mBaixwf4k8jk7fG4",
    "parentCommentId": "sCkiry2TLLbjhHfhx",
    "topLevelCommentId": "dbJpeCAf2hz7kh7HE",
    "contents": {
      "markdown": "I just have no idea what Eliezer's question even means. What do you think it means?",
      "plaintextDescription": "I just have no idea what Eliezer's question even means. What do you think it means?"
    },
    "baseScore": 8,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-23T01:03:17.466Z",
    "af": false
  },
  {
    "_id": "EDdNAhpEjJ5EfXmZr",
    "postId": "4mBaixwf4k8jk7fG4",
    "parentCommentId": "dbJpeCAf2hz7kh7HE",
    "topLevelCommentId": "dbJpeCAf2hz7kh7HE",
    "contents": {
      "markdown": "I think that if you want to work on AI takeover prevention, you should probably have some models of the situation that give you a rough prediction for P(AI takeover). For example, I'm constantly thinking in terms of the risks conditional on scheming arising, scheming not arising, a leading AI company being responsible and having three years of lead, etc.",
      "plaintextDescription": "I think that if you want to work on AI takeover prevention, you should probably have some models of the situation that give you a rough prediction for P(AI takeover). For example, I'm constantly thinking in terms of the risks conditional on scheming arising, scheming not arising, a leading AI company being responsible and having three years of lead, etc. "
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-23T00:57:27.380Z",
    "af": false
  },
  {
    "_id": "xjkvJvF3pcnwwFbvB",
    "postId": "4mBaixwf4k8jk7fG4",
    "parentCommentId": "dbJpeCAf2hz7kh7HE",
    "topLevelCommentId": "dbJpeCAf2hz7kh7HE",
    "contents": {
      "markdown": "> **\"p(Doom) if we build superintelligence soon under present conditions using present techniques\"**\n\nThis has the issue that superintelligence won't be developed using present techniques, and a lot of the question is about whether it'll be okay to develop superintelligence using the different techniques that will be used at the time, and so this number will potentially drastically overestimate the risk.\n\nI of course have no problem with asking people questions about situations where the risk is higher or lower than you believe actual risk to be. But I think that focusing on this question will lead to repeated frustrating interactions where one person quotes the answer to the stated question, and then someone else conflates that with their answer to what will happen if we continue on the current trajectory, which for some people is extremely different and for some people isn't.",
      "plaintextDescription": "> \"p(Doom) if we build superintelligence soon under present conditions using present techniques\"\n\nThis has the issue that superintelligence won't be developed using present techniques, and a lot of the question is about whether it'll be okay to develop superintelligence using the different techniques that will be used at the time, and so this number will potentially drastically overestimate the risk.\n\nI of course have no problem with asking people questions about situations where the risk is higher or lower than you believe actual risk to be. But I think that focusing on this question will lead to repeated frustrating interactions where one person quotes the answer to the stated question, and then someone else conflates that with their answer to what will happen if we continue on the current trajectory, which for some people is extremely different and for some people isn't."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-23T00:54:56.060Z",
    "af": false
  },
  {
    "_id": "dbJpeCAf2hz7kh7HE",
    "postId": "4mBaixwf4k8jk7fG4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Re \"What is the minimum necessary and sufficient policy that you think would prevent extinction?\"\n\nThis is phrased in a way that implies preventing extinction is a binary, when in reality for any given policy there is some probability of extinction. I actually don't know what Eliezer means here, is he asking for a policy that leads to 50%, 10%, 1%, or [0%](https://www.lesswrong.com/posts/QGkYCwyC7wTDyt3yT/0-and-1-are-not-probabilities) P(extinction)?\n\n(I think that it's common for AI safety people to talk too much about totally quashing risks rather than reducing them, in a way that leads them into unproductive lines of reasoning.)",
      "plaintextDescription": "Re \"What is the minimum necessary and sufficient policy that you think would prevent extinction?\"\n\nThis is phrased in a way that implies preventing extinction is a binary, when in reality for any given policy there is some probability of extinction. I actually don't know what Eliezer means here, is he asking for a policy that leads to 50%, 10%, 1%, or 0% P(extinction)?\n\n(I think that it's common for AI safety people to talk too much about totally quashing risks rather than reducing them, in a way that leads them into unproductive lines of reasoning.)"
    },
    "baseScore": 29,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2025-08-23T00:51:11.329Z",
    "af": false
  },
  {
    "_id": "tGnvvgqbgZwdQqwCC",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "DQ7jS7i458Drrx7Fk",
    "topLevelCommentId": "xrK2h2HaP3cYBStEA",
    "contents": {
      "markdown": "Yes, I'd appreciate that. DM me on slack?",
      "plaintextDescription": "Yes, I'd appreciate that. DM me on slack?"
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-22T23:54:11.405Z",
    "af": false
  },
  {
    "_id": "CgjqkqkvvJfCcSR3B",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "xrK2h2HaP3cYBStEA",
    "topLevelCommentId": "xrK2h2HaP3cYBStEA",
    "contents": {
      "markdown": "Indeed it was quickly written, I think it was like 20 mins of writing and 30 mins of editing/responding to comments. Based on feedback, I think it was probably a mistake to not put more time into it and post a better version.",
      "plaintextDescription": "Indeed it was quickly written, I think it was like 20 mins of writing and 30 mins of editing/responding to comments. Based on feedback, I think it was probably a mistake to not put more time into it and post a better version."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-22T23:50:42.418Z",
    "af": false
  },
  {
    "_id": "mmaTd5dAEGhrSySDz",
    "postId": "prHnjtXAKrPPZt8eB",
    "parentCommentId": "cGbgzL8gXRpYcGGdF",
    "topLevelCommentId": "cGbgzL8gXRpYcGGdF",
    "contents": {
      "markdown": "It's often very hard to make commitments like this, so I think that most of the relevant literature might be about how you can't do this. E.g. a [Thucydides trap](https://en.wikipedia.org/wiki/Thucydides_Trap) is when a stronger power launches a preventative war against a weaker rising power; one particular reason for this is that the weaker power can't commit to not abuse their power in the future. See also [security dilemma](https://en.wikipedia.org/wiki/Security_dilemma).",
      "plaintextDescription": "It's often very hard to make commitments like this, so I think that most of the relevant literature might be about how you can't do this. E.g. a Thucydides trap is when a stronger power launches a preventative war against a weaker rising power; one particular reason for this is that the weaker power can't commit to not abuse their power in the future. See also security dilemma."
    },
    "baseScore": 10,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-22T22:08:34.965Z",
    "af": false
  },
  {
    "_id": "qHGHzMFgLeQMxxTx6",
    "postId": "T54YXFjFeGuE6sbst",
    "parentCommentId": "aqPT5GsWXxKc4LynL",
    "topLevelCommentId": "RdJBvbAnnkuAr37Kr",
    "contents": {
      "markdown": "My default drive-by recommendation is that you try to get involved in research related to these issues. You could try to get advice from [Chi Nguyen](https://forum.effectivealtruism.org/users/chi), who works on s-risk and is friendly and thoughtful; you can contact her [here](https://www.admonymous.co/chi).",
      "plaintextDescription": "My default drive-by recommendation is that you try to get involved in research related to these issues. You could try to get advice from Chi Nguyen, who works on s-risk and is friendly and thoughtful; you can contact her here."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-22T16:05:44.129Z",
    "af": false
  },
  {
    "_id": "eRhhebZBEiYdfkoE6",
    "postId": "T54YXFjFeGuE6sbst",
    "parentCommentId": "RdJBvbAnnkuAr37Kr",
    "topLevelCommentId": "RdJBvbAnnkuAr37Kr",
    "contents": {
      "markdown": "What are your skill sets?\n\nForethought has done work recently related to preventing S-risk arising from AI.\n\nI'm pretty in favor of trying to tackle the most important cause area.",
      "plaintextDescription": "What are your skill sets?\n\nForethought has done work recently related to preventing S-risk arising from AI.\n\nI'm pretty in favor of trying to tackle the most important cause area. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-22T00:34:35.815Z",
    "af": false
  },
  {
    "_id": "vbhew8KoCsA5SauKa",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "BKk2YvXc96y3ZCtcv",
    "topLevelCommentId": "BKk2YvXc96y3ZCtcv",
    "contents": {
      "markdown": "I agree this is a real thing.\n\nNote that this is more important for group epistemics than individual epistemics.\n\nAlso, one reason the constraint isn't that bad for me is that I can and do say spicy stuff privately, including in pretty large private groups. (And you can get away with fairly spicy stuff stated publicly.)",
      "plaintextDescription": "I agree this is a real thing.\n\nNote that this is more important for group epistemics than individual epistemics.\n\nAlso, one reason the constraint isn't that bad for me is that I can and do say spicy stuff privately, including in pretty large private groups. (And you can get away with fairly spicy stuff stated publicly.)"
    },
    "baseScore": 14,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-08-21T16:45:41.296Z",
    "af": false
  },
  {
    "_id": "vLHBD7yWxvWQQpdwT",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "ojrwihaizfsknZHLp",
    "topLevelCommentId": "9yr7HwYzeeW8Nqq2N",
    "contents": {
      "markdown": "Edited, is it clearer now?",
      "plaintextDescription": "Edited, is it clearer now?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-21T06:09:12.553Z",
    "af": false
  },
  {
    "_id": "QDrRvowcdaKzZDBgt",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "EtQyzKcKDfyizX6zq",
    "topLevelCommentId": "9yr7HwYzeeW8Nqq2N",
    "contents": {
      "markdown": "Obviously (and as you note), this argument doesn't suggest that humans would all die, it suggests that a bunch of them would die. (An AI estimated that monkey populations are down 90% due to humans.)\n\nAnd if we want to know how many exactly would die, we'd have to get into the details, as has been done for example in the comments linked from [here](https://www.alignmentforum.org/posts/rP66bz34crvDudzcJ/decision-theory-does-not-imply-that-we-get-to-have-nice?commentId=J8wTkp8CxoXsQNCfe).\n\nSo I think that this analogy is importantly not addressing the question you were responding to.",
      "plaintextDescription": "Obviously (and as you note), this argument doesn't suggest that humans would all die, it suggests that a bunch of them would die. (An AI estimated that monkey populations are down 90% due to humans.)\n\nAnd if we want to know how many exactly would die, we'd have to get into the details, as has been done for example in the comments linked from here.\n\nSo I think that this analogy is importantly not addressing the question you were responding to."
    },
    "baseScore": 2,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-20T20:11:04.478Z",
    "af": false
  },
  {
    "_id": "gWbYihCZfYuKhxojQ",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "FawiyPgm5GgLychNg",
    "topLevelCommentId": "FawiyPgm5GgLychNg",
    "contents": {
      "markdown": "Mostly AI companies researching AI control and planning to some extent to adopt it (e.g. see the GDM [safety plan](https://deepmind.google/discover/blog/taking-a-responsible-path-to-agi/)).",
      "plaintextDescription": "Mostly AI companies researching AI control and planning to some extent to adopt it (e.g. see the GDM safety plan)."
    },
    "baseScore": 26,
    "voteCount": 20,
    "createdAt": null,
    "postedAt": "2025-08-20T19:07:25.528Z",
    "af": false
  },
  {
    "_id": "mfF85QM7A9kGBXGW3",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "uQArpNGNaGhrnJfLo",
    "topLevelCommentId": "B4hBJWAmzAfyzjzDy",
    "contents": {
      "markdown": "The AIs might agree on all predictions about things that will be checkable within three months, but disagree about the consequences of actions in five years.",
      "plaintextDescription": "The AIs might agree on all predictions about things that will be checkable within three months, but disagree about the consequences of actions in five years."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-16T20:08:48.219Z",
    "af": false
  },
  {
    "_id": "NfsqoSsWLro9HdFox",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "xfaJPW674EwA9qLnK",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "I'm not trying to talk about what will happen in the future, I'm trying to talk about what would happen *if everything happened gradually, like in your dragon story*!\n\nYou argued that we'd have huge problems even if things progress arbitrarily gradually, because there's a crucial phase change between the problems that occur when the AIs can't take over and the problems that occur when they can. To assess that, we need to talk about what would happen if things did progress gradually. So it's relevant whether wacky phenomena would've been observed on weaker models if we'd looked harder; IIUC your thesis is that there are crucial phenomena that *wouldn't* have been observed on weaker models.\n\nIn general, my interlocutors here seem to constantly vacillate between \"X is true\" and \"Even if AI capabilities increased gradually, X would be true\". I have mostly been trying to talk about the latter in all the comments under the dragon metaphor.",
      "plaintextDescription": "I'm not trying to talk about what will happen in the future, I'm trying to talk about what would happen if everything happened gradually, like in your dragon story!\n\nYou argued that we'd have huge problems even if things progress arbitrarily gradually, because there's a crucial phase change between the problems that occur when the AIs can't take over and the problems that occur when they can. To assess that, we need to talk about what would happen if things did progress gradually. So it's relevant whether wacky phenomena would've been observed on weaker models if we'd looked harder; IIUC your thesis is that there are crucial phenomena that wouldn't have been observed on weaker models.\n\nIn general, my interlocutors here seem to constantly vacillate between \"X is true\" and \"Even if AI capabilities increased gradually, X would be true\". I have mostly been trying to talk about the latter in all the comments under the dragon metaphor."
    },
    "baseScore": 16,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-16T05:35:48.596Z",
    "af": false
  },
  {
    "_id": "PyYBRPgAWW4vmEFBF",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "MAy6smXuqevSRSki6",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "No, I definitely don't expect any of this to happen comfortably or for only one thing to be breaking at once.",
      "plaintextDescription": "No, I definitely don't expect any of this to happen comfortably or for only one thing to be breaking at once."
    },
    "baseScore": 10,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-16T05:29:01.291Z",
    "af": false
  },
  {
    "_id": "KX4nBo48nosj7Hj6B",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "GBwtskBLKPjaaWJgP",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "> But then I'd say there is a different important distribution shift, namely, the shift from AIs which are way lower capability, to the AIs which are high capability.\n\nAs with Eliezer, I think it's important to clarify which capability you're talking about; I think Eliezer's argument totally conflates different capabilities.",
      "plaintextDescription": "> But then I'd say there is a different important distribution shift, namely, the shift from AIs which are way lower capability, to the AIs which are high capability.\n\nAs with Eliezer, I think it's important to clarify which capability you're talking about; I think Eliezer's argument totally conflates different capabilities."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-15T23:38:20.833Z",
    "af": false
  },
  {
    "_id": "vWHMT66mceFjLLBb8",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "GBwtskBLKPjaaWJgP",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "> \"Ah, but we can construct extremely accurate honeypots / testing environments that simulate a real-world opportunity to take over, and then see what the ASI does.\" Valid, but not sound, because we probably can't actually do that.\n\nI also think it's important that you can do this with AIs weaker than the ASI, and iterate on alignment in that context.",
      "plaintextDescription": "> \"Ah, but we can construct extremely accurate honeypots / testing environments that simulate a real-world opportunity to take over, and then see what the ASI does.\" Valid, but not sound, because we probably can't actually do that.\n\nI also think it's important that you can do this with AIs weaker than the ASI, and iterate on alignment in that context."
    },
    "baseScore": 5,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-15T23:37:15.001Z",
    "af": false
  },
  {
    "_id": "yKtMJKwmuJi5L4snn",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "iKdhZoi4t9MFqSqsD",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "A more important argument is the one I give briefly [here](https://www.lesswrong.com/posts/kgb58RL88YChkkBNf/the-problem?commentId=KRTqkWnLhPuSwXAsT).",
      "plaintextDescription": "A more important argument is the one I give briefly here."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-15T21:10:55.639Z",
    "af": false
  },
  {
    "_id": "QqkjjQrh3yPamGqCK",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "JNghTZpcQcG6fQr6D",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "> indefinite pause is basically necessary, or, humanity has clearly fucked up if we don't do it\n\nI think it's really important to not equivocate between \"necessary\" and \"humanity has clearly fucked up if we don't do it.\"\n\n\"Necessary\" means \"we need this in order to succeed; there's no chance of success without this\". Because humanity is going to massively underestimate the risk of AI takeover, there is going to be lots of stuff that doesn't happen that would have passed cost-benefit analysis for humanity.\n\nIf you think it's 15% likely that we need really large amounts of serial time to prevent AI takeover, then it's very easy to imagine situations where the best strategy on the margin is to work on the other 85% of worlds. I have no idea why you're describing this as \"basically necessary\".",
      "plaintextDescription": "> indefinite pause is basically necessary, or, humanity has clearly fucked up if we don't do it\n\nI think it's really important to not equivocate between \"necessary\" and \"humanity has clearly fucked up if we don't do it.\"\n\n\"Necessary\" means \"we need this in order to succeed; there's no chance of success without this\". Because humanity is going to massively underestimate the risk of AI takeover, there is going to be lots of stuff that doesn't happen that would have passed cost-benefit analysis for humanity.\n\nIf you think it's 15% likely that we need really large amounts of serial time to prevent AI takeover, then it's very easy to imagine situations where the best strategy on the margin is to work on the other 85% of worlds. I have no idea why you're describing this as \"basically necessary\"."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-15T21:08:26.196Z",
    "af": false
  },
  {
    "_id": "KRTqkWnLhPuSwXAsT",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "RqrKG3x6q3Wqz5TED",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "I'm sure people have said all kinds of dumb things to you on this topic. I'm definitely not trying to defend the position of your dumbest interlocutor.\n\n> You are now arguing that *we will be able to cross this leap of generalization successfully*.\n\nThat's not really my core point.\n\nMy core point is that \"you need safety mechanisms to work in situations where X is true, but you can only test them in situations where X is false\" isn't on its own a strong argument; you need to talk about features of X in particular.\n\nI think you are trying to set X to \"The AIs are capable of taking over.\"\n\nThere's a version of this that I totally agree with. For example, if you are giving your AIs increasingly much power over time, I think it is foolish to assume that just because they haven't acted against you while they don't have the affordances required to grab power, they won't act against you when they do have those affordances.\n\nThe main reason why that scenario is scary is that the AIs might be acting adversarially against you, such that whether you observe a problem is extremely closely related to whether they will succeed at a takeover. \n\nIf the AIs aren't acting adversarially towards you, I think there is much less of a reason to particularly think that things will go wrong at that point. \n\nSo the situation is much better if we can be confident that the AIs are not acting adversarially towards us at that point. This is what I would like to achieve.\n\nSo I'd say the proposal is more like \"cause that leap of generalization to not be a particularly scary one\" than \"make that leap of generalization in the scary way\". \n\nRe your last paragraph: I don't really see why you think two dozen things would change between these regimes. Machine learning doesn't normally have lots of massive discontinuities of the type you're describing.",
      "plaintextDescription": "I'm sure people have said all kinds of dumb things to you on this topic. I'm definitely not trying to defend the position of your dumbest interlocutor.\n\n> You are now arguing that we will be able to cross this leap of generalization successfully.\n\nThat's not really my core point.\n\nMy core point is that \"you need safety mechanisms to work in situations where X is true, but you can only test them in situations where X is false\" isn't on its own a strong argument; you need to talk about features of X in particular.\n\nI think you are trying to set X to \"The AIs are capable of taking over.\"\n\nThere's a version of this that I totally agree with. For example, if you are giving your AIs increasingly much power over time, I think it is foolish to assume that just because they haven't acted against you while they don't have the affordances required to grab power, they won't act against you when they do have those affordances.\n\nThe main reason why that scenario is scary is that the AIs might be acting adversarially against you, such that whether you observe a problem is extremely closely related to whether they will succeed at a takeover. \n\nIf the AIs aren't acting adversarially towards you, I think there is much less of a reason to particularly think that things will go wrong at that point. \n\nSo the situation is much better if we can be confident that the AIs are not acting adversarially towards us at that point. This is what I would like to achieve.\n\nSo I'd say the proposal is more like \"cause that leap of generalization to not be a particularly scary one\" than \"make that leap of generalization in the scary way\". \n\nRe your last paragraph: I don't really see why you think two dozen things would change between these regimes. Machine learning doesn't normally have lots of massive discontinuities of the type you're describing."
    },
    "baseScore": 26,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-15T19:31:56.389Z",
    "af": false
  },
  {
    "_id": "vfs5PNDzRpaCsWZxN",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "PyKqCikKhc7bttdpo",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "Incidentally, I think reward hacking has gone down as a result of people improving techniques, despite capabilities increasing. I believe this because of anecdotal reports and also graphs like the one from [the Anthropic model card for Opus and Sonnet 4](https://www-cdn.anthropic.com/07b2a3f9902ee19fe39a36ca638e5ae987bc64dd.pdf):\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d5f21afefe6c127ef4a9d732cf237445362fcd17e2fff79a.png)",
      "plaintextDescription": "Incidentally, I think reward hacking has gone down as a result of people improving techniques, despite capabilities increasing. I believe this because of anecdotal reports and also graphs like the one from the Anthropic model card for Opus and Sonnet 4:"
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-15T19:17:09.827Z",
    "af": false
  },
  {
    "_id": "53z2Fk9KXyFJjxfPj",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "PyKqCikKhc7bttdpo",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "> Your techniques are failing right now; Sonnet is deleting non-passing tests instead of rewriting code.\n\nI don't know what you mean by \"my techniques\", I don't train AIs or research techniques for mitigating reward hacking, and I don't have private knowledge of what techniques are used in practice.\n\n> Where's the worldwide halt on further capabilities development that we're supposed to get, until new techniques are found and apparently start working again?\n\nI didn't say anything about a worldwide halt. I was talking about the [local validity](https://www.lesswrong.com/posts/WQFioaudEH8R7fyhm/local-validity-as-a-key-to-sanity-and-civilization) of your argument above about dragons; your sentence talks about a broader question about whether the situation will be okay.\n\n> What's the total number of new failures we'd need to observe between intelligence regimes, before you start to expect that *yet another* failure might lie ahead in the future?\n\nI think that if we iterated a bunch on techniques for mitigating reward hacking and then observed that these techniques worked pretty well, then kept slowly scaling up through LLM capabilities until the point where the AI is able to basically replace AI researchers, it would be pretty likely for those techniques to work for one more OOM of effective compute, if the researchers were pretty thoughtful about it. (As an example of how you can mitigate risk from the OOD generalization: there are lots of ways to make your reward signal artificially dumber and see whether you get bad reward hacking, see [here](https://www.alignmentforum.org/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1) for many suggestions; I think that results in these settings probably generalize up a capability level, especially if none of the AI is involved or purposefully trying to sabotage the results of your experiments.)\n\nTo be clear, what AI companies actually do will probably be wildly more reckless than what I'm talking about here. I'm just trying to dispute your claim that the situation disallows empirical iteration.\n\nI also think reward hacking is a poor example of a surprising failure arising from increased capability: it was predicted by heaps of people, including you, for many years before it was a problem in practice.\n\nTo answer your question, I think that if really weird stuff like emergent misalignment and subliminal learning appeared at every OOM of effective compute increase (and those didn't occur in weaker models, even when you go looking for them after first observing them in stronger models), I'd start to expect weird stuff to occur at every order of magnitude of capabilities increase. I don't think we've actually observed many phenomena like those that we couldn't have discovered at much lower capability levels.",
      "plaintextDescription": "> Your techniques are failing right now; Sonnet is deleting non-passing tests instead of rewriting code.\n\nI don't know what you mean by \"my techniques\", I don't train AIs or research techniques for mitigating reward hacking, and I don't have private knowledge of what techniques are used in practice.\n\n> Where's the worldwide halt on further capabilities development that we're supposed to get, until new techniques are found and apparently start working again?\n\nI didn't say anything about a worldwide halt. I was talking about the local validity of your argument above about dragons; your sentence talks about a broader question about whether the situation will be okay.\n\n> What's the total number of new failures we'd need to observe between intelligence regimes, before you start to expect that yet another failure might lie ahead in the future?\n\nI think that if we iterated a bunch on techniques for mitigating reward hacking and then observed that these techniques worked pretty well, then kept slowly scaling up through LLM capabilities until the point where the AI is able to basically replace AI researchers, it would be pretty likely for those techniques to work for one more OOM of effective compute, if the researchers were pretty thoughtful about it. (As an example of how you can mitigate risk from the OOD generalization: there are lots of ways to make your reward signal artificially dumber and see whether you get bad reward hacking, see here for many suggestions; I think that results in these settings probably generalize up a capability level, especially if none of the AI is involved or purposefully trying to sabotage the results of your experiments.)\n\nTo be clear, what AI companies actually do will probably be wildly more reckless than what I'm talking about here. I'm just trying to dispute your claim that the situation disallows empirical iteration.\n\nI also think reward hacking is a poor example of a surprising failure arising from increased capability: it was predicted"
    },
    "baseScore": 14,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-15T19:02:27.543Z",
    "af": false
  },
  {
    "_id": "5DcEL6NkguCu3ZEte",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "E3QusjuKFcnaLEvTj",
    "topLevelCommentId": "B4hBJWAmzAfyzjzDy",
    "contents": {
      "markdown": "I think that the position you're describing should be part of your hypothesis space when you're just starting out thinking about this question. And I think that people in the AI safety community often underrate the intuitions you're describing.\n\nBut overall, after thinking about the details, I end up disagreeing. The differences between risks from human concentration of power and risks from AI takeover lead to me thinking you should handle these situations differently (which shouldn't be that surprising, because the situations are very different).",
      "plaintextDescription": "I think that the position you're describing should be part of your hypothesis space when you're just starting out thinking about this question. And I think that people in the AI safety community often underrate the intuitions you're describing.\n\nBut overall, after thinking about the details, I end up disagreeing. The differences between risks from human concentration of power and risks from AI takeover lead to me thinking you should handle these situations differently (which shouldn't be that surprising, because the situations are very different). "
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-13T15:46:37.988Z",
    "af": false
  },
  {
    "_id": "tQHRjHk2LW3gvMKqb",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "u6FQoi4CaNkBtZCqM",
    "topLevelCommentId": "B4hBJWAmzAfyzjzDy",
    "contents": {
      "markdown": "I think that the mutually-aligned correlated scheming problem is way worse with AIs than humans, especially when AIs are much smarter than humans.",
      "plaintextDescription": "I think that the mutually-aligned correlated scheming problem is way worse with AIs than humans, especially when AIs are much smarter than humans. "
    },
    "baseScore": 20,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-13T04:22:30.179Z",
    "af": false
  },
  {
    "_id": "X7cXY7ymnD3bfooqf",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "3MisyHDgSvKKk2DsD",
    "topLevelCommentId": "B4hBJWAmzAfyzjzDy",
    "contents": {
      "markdown": "I don't think that this works when the AIs are way more intelligent than humans. In particular, suppose there's some information about the world that the AIs are able to glean through vast amounts of experience and reflection, and that they can't justify except through reference to that experience and reflection. Suppose there are two AIs that make conflicting claims about that information, while agreeing on everything that humans can check. How are humans supposed to decide which to trust?",
      "plaintextDescription": "I don't think that this works when the AIs are way more intelligent than humans. In particular, suppose there's some information about the world that the AIs are able to glean through vast amounts of experience and reflection, and that they can't justify except through reference to that experience and reflection. Suppose there are two AIs that make conflicting claims about that information, while agreeing on everything that humans can check. How are humans supposed to decide which to trust? "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-13T04:21:43.367Z",
    "af": false
  },
  {
    "_id": "XrGfuQxCnc6jiCopj",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "6YBNPriTW8iTHQ2v4",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "Note that I'm complaining on two levels here. I think the dragon argument is actually wrong, but more confidently, I think that that argument isn't locally valid.",
      "plaintextDescription": "Note that I'm complaining on two levels here. I think the dragon argument is actually wrong, but more confidently, I think that that argument isn't locally valid."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-13T03:56:12.451Z",
    "af": false
  },
  {
    "_id": "wZaSHnGkmin8eDhwn",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "6YBNPriTW8iTHQ2v4",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "A core point here is that I don't see a particular reason why taking over the world is as hard as being a schemer, and I don't see why techniques for preventing scheming are particularly likely to suddenly fail at the level of capability where the AI is just able to take over the world.",
      "plaintextDescription": "A core point here is that I don't see a particular reason why taking over the world is as hard as being a schemer, and I don't see why techniques for preventing scheming are particularly likely to suddenly fail at the level of capability where the AI is just able to take over the world."
    },
    "baseScore": 24,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-13T03:33:46.688Z",
    "af": false
  },
  {
    "_id": "6YBNPriTW8iTHQ2v4",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "zEcMTMxj4mzovhQpZ",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "(I'll just talk about single AIs/dragons, because the complexity arising from there being multiple AIs doesn't matter here.)\n\n> There is no clever plan for generalizing from safe regimes to unsafe regimes which avoids all risk that the generalization doesn't work as you hoped.  Because they are different regimes.\n\nI totally agree that you can't avoid \"all risk\". But you're arguing something much stronger: you're saying that the generalization *probably* fails!\n\n* * *\n\nI agree that the regime where mistakes don't kill you isn't *the same* as the regime where mistakes do kill you. But it might be similar in the relevant respects. As a trivial example, if you build a machine in America it usually works when you bring it to Australia. I think that arguments at the level of abstraction you've given here don't establish that this is one of the cases where the risk of the generalization failing is high rather than low. (See Paul's disagreement 1[ here](https://www.lesswrong.com/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer) for a very similar objection (\"Eliezer often equivocates between “you have to get alignment right on the first ‘critical’ try” and “you can’t learn anything about alignment from experimentation and failures before the critical try.”\").)\n\nIt seems like as AIs get more powerful, two things change:\n\n*   They probably eventually get powerful enough that they (if developed with current methods) start plotting to kill you/take your stuff.\n*   They get better, so their wanting to kill you is more of a problem.\n\nI don't see strong arguments that these problems should arise at very similar capability levels, especially if AI developers actively try to prevent the AIs from taking over. (I've argued this[ here](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled); one obvious intuition pump is that individual humans are smart enough to sometimes plot against people, but generally aren't smart enough to overpower humanity.) (The relevant definition of \"very similar\" is related to how long you think you have between the two capabilities levels, so if you think that progress is super rapid then it's way more likely you have problems related to these two issues arising very nearby in calendar time. But here you granted that progress is gradual for the sake of argument.)\n\nIf the capability level at which AIs start wanting to kill you is way **higher** than the capability level at which they're way better than you at everything (and thus could kill you), then you have access to AIs that aren't trying to kill you and that are more capable than you in order to help with your alignment problems. (There is some trickiness here about whether those AIs might be useless even though they're generally way better than you at stuff and they're not trying to kill you, but I feel like that's a pretty different argument from the dragon analogy you just made here or any argument made in the post.)\n\nIf the capability level at which AIs start wanting to kill you is way **lower** than the capability level at which they are way better than you at everything, then, before AIs are dangerous, you have the opportunity to empirically investigate the phenomenon of AIs wanting to kill you. For example, you can try out your ideas for how to make them not want to kill you, and then observe whether those worked or not. If they’re way worse than you at stuff, you have a pretty good chance at figuring out when they’re trying to kill you. (There’s all kinds of trickiness here, about whether this empirical iteration is the kind of thing that would work. I think it has a reasonable shot of working. But either way, your dragon analogy doesn't respond to it. The most obvious analogy is that you're breeding dragons for intelligence; if them plotting against you starts showing up way before they're powerful enough to take over, I think you have a good chance of figuring out a breeding program that would lead to them not taking over in a way you disliked, if you had a bunch of time to iterate. And our affordances with ML training are better than that.)\n\nI don't think the arguments that I gave here are very robust. But I think they're plausible and I don't think your basic argument responds to any of them. (And I don’t think you’ve responded to them to my satisfaction elsewhere.)\n\n(I've made repeated little edits to this comment after posting, sorry if that's annoying. They haven't affected the core structure of my argument.)",
      "plaintextDescription": "(I'll just talk about single AIs/dragons, because the complexity arising from there being multiple AIs doesn't matter here.)\n\n> There is no clever plan for generalizing from safe regimes to unsafe regimes which avoids all risk that the generalization doesn't work as you hoped.  Because they are different regimes.\n\nI totally agree that you can't avoid \"all risk\". But you're arguing something much stronger: you're saying that the generalization probably fails!\n\n----------------------------------------\n\nI agree that the regime where mistakes don't kill you isn't the same as the regime where mistakes do kill you. But it might be similar in the relevant respects. As a trivial example, if you build a machine in America it usually works when you bring it to Australia. I think that arguments at the level of abstraction you've given here don't establish that this is one of the cases where the risk of the generalization failing is high rather than low. (See Paul's disagreement 1 here for a very similar objection (\"Eliezer often equivocates between “you have to get alignment right on the first ‘critical’ try” and “you can’t learn anything about alignment from experimentation and failures before the critical try.”\").)\n\nIt seems like as AIs get more powerful, two things change:\n\n * They probably eventually get powerful enough that they (if developed with current methods) start plotting to kill you/take your stuff.\n * They get better, so their wanting to kill you is more of a problem.\n\nI don't see strong arguments that these problems should arise at very similar capability levels, especially if AI developers actively try to prevent the AIs from taking over. (I've argued this here; one obvious intuition pump is that individual humans are smart enough to sometimes plot against people, but generally aren't smart enough to overpower humanity.) (The relevant definition of \"very similar\" is related to how long you think you have between the two capabilities levels, so if you think that"
    },
    "baseScore": 137,
    "voteCount": 57,
    "createdAt": null,
    "postedAt": "2025-08-13T03:14:58.391Z",
    "af": false
  },
  {
    "_id": "7XXumvqWGmgpTcxjD",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "bH52KKzq8BXrxuzd6",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "I think that your audience would actually understand the difference between \"there are human level AIs for a few years, and it's obvious to everyone (especially AI company employees) that this is happening\" and \"superintelligent AI arises suddenly\".\n\n> I think *most* of the points in the post are more immediately compatible with fast take off, but also go through for slow takeoff scenarios\n\nAs an example of one that doesn't, \"Many alignment problems relevant to superintelligence don’t naturally appear at lower, passively safe levels of capability. This puts us in the position of needing to solve many problems on the first critical try, with little time to iterate and no prior experience solving the problem on weaker systems.\"",
      "plaintextDescription": "I think that your audience would actually understand the difference between \"there are human level AIs for a few years, and it's obvious to everyone (especially AI company employees) that this is happening\" and \"superintelligent AI arises suddenly\".\n\n> I think most of the points in the post are more immediately compatible with fast take off, but also go through for slow takeoff scenarios\n\nAs an example of one that doesn't, \"Many alignment problems relevant to superintelligence don’t naturally appear at lower, passively safe levels of capability. This puts us in the position of needing to solve many problems on the first critical try, with little time to iterate and no prior experience solving the problem on weaker systems.\""
    },
    "baseScore": 19,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-12T20:20:02.573Z",
    "af": false
  },
  {
    "_id": "yokHCaLLfNd8h3YXL",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "FBL5cnK5Frc8wEJC8",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "I'm only getting into this more because I am finding it interesting, feel free to tap out. I'm going to be a little sloppy for the sake of saving time. \n\nI'm going to summarize your comment like this, maybe you think this is unfair:\n\n> It's only a problem to 'assume fast takeoffs' if you \\[...\\] expect it to be action relevant, which \\[...\\] I, so far, don't.\n\nI disagree about this general point.\n\nLike, suppose you were worried about the USA being invaded on either the east or west coast, and you didn't have a strong opinion on which it was, and you don't think it matters for your recommended intervention of increasing the size of the US military or for your prognosis. I think it would be a problem to describe the issue by saying that America will be invaded on the East Coast, because you're giving a poor description of what you think will happen, which makes it harder for other people to assess your arguments.\n\nThere's something similar here. You're trying to tell a story for AI development leading to doom. You think that the story goes through regardless of whether the AI becomes rapidly superhuman or gradually superhuman. Then you tell a story where the AI becomes rapidly superhuman. I think this is a problem, because it isn't describing some features of the situation that seem very important to the common-sense picture of the situation, even if they don't change the bottom line.\n\nIt seems reasonable for your response to be, \"but I don't think that those gradual takeoffs are plausible\". In that case, we disagree on the object level, but I have no problem with the comms. But if you think the gradual takeoffs are plausible, I think it's important for your writing to not implicitly disregard them.\n\nAll of this is kind of subjective because which features of a situation are interesting is subjective.",
      "plaintextDescription": "I'm only getting into this more because I am finding it interesting, feel free to tap out. I'm going to be a little sloppy for the sake of saving time. \n\nI'm going to summarize your comment like this, maybe you think this is unfair:\n\n> It's only a problem to 'assume fast takeoffs' if you [...] expect it to be action relevant, which [...] I, so far, don't.\n\nI disagree about this general point.\n\nLike, suppose you were worried about the USA being invaded on either the east or west coast, and you didn't have a strong opinion on which it was, and you don't think it matters for your recommended intervention of increasing the size of the US military or for your prognosis. I think it would be a problem to describe the issue by saying that America will be invaded on the East Coast, because you're giving a poor description of what you think will happen, which makes it harder for other people to assess your arguments.\n\nThere's something similar here. You're trying to tell a story for AI development leading to doom. You think that the story goes through regardless of whether the AI becomes rapidly superhuman or gradually superhuman. Then you tell a story where the AI becomes rapidly superhuman. I think this is a problem, because it isn't describing some features of the situation that seem very important to the common-sense picture of the situation, even if they don't change the bottom line.\n\nIt seems reasonable for your response to be, \"but I don't think that those gradual takeoffs are plausible\". In that case, we disagree on the object level, but I have no problem with the comms. But if you think the gradual takeoffs are plausible, I think it's important for your writing to not implicitly disregard them.\n\nAll of this is kind of subjective because which features of a situation are interesting is subjective."
    },
    "baseScore": 17,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-12T18:09:39.201Z",
    "af": false
  },
  {
    "_id": "tbcd3CbDvwP2rmvnj",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "ZoJXEGSkqFhGboSXq",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "> I think the crux for me in these situations is \"do you think it's more valuable to increase our odds of survival on the margin in the three-year window worlds or to try to steer toward the pause worlds, and how confident are you there?\"\n\nFWIW, that's not my crux at all. The problem I have with this post implicitly assuming really fast takeoffs isn't that it leads to bad recommendations about what to do (though I do think that to some extent). My problem is that the arguments are missing steps that I think are really important, and so they're (kind of) invalid.\\[1\\]\n\nThat is, suppose I agreed with you that it was extremely unlikely that humanity would be able to resolve the issue even given two years with human-level-ish AIs. And suppose that we were very likely to have those two years. I *still* think it would be bad to make an argument that doesn't mention those two years, because those two years seemed to me to change the natural description of the situation a lot, and I think they are above the bar of details worth including. This is especially true because a lot of people's disagreement with you (including many people in the relevant audience of \"thoughtful people who will opine on your book\") does actually come down to whether those two years make the situation okay.\n\n\\[1\\] I'm saying \"kind of invalid\" because you aren't making an argument that's shaped like a proof. You're making an argument that is more like a heuristic argument, where you aren't including all the details and you're implicitly asserting that those details don't change the bottom line. (Obviously, you have no other option than doing this because this is a complicated situation and you have space limitations.) In cases where there is a counterargument that you think is defeated by a countercounterargument, it's up to you to decide whether that deserves to be included. I think this one does deserve to be included.",
      "plaintextDescription": "> I think the crux for me in these situations is \"do you think it's more valuable to increase our odds of survival on the margin in the three-year window worlds or to try to steer toward the pause worlds, and how confident are you there?\"\n\nFWIW, that's not my crux at all. The problem I have with this post implicitly assuming really fast takeoffs isn't that it leads to bad recommendations about what to do (though I do think that to some extent). My problem is that the arguments are missing steps that I think are really important, and so they're (kind of) invalid.[1]\n\nThat is, suppose I agreed with you that it was extremely unlikely that humanity would be able to resolve the issue even given two years with human-level-ish AIs. And suppose that we were very likely to have those two years. I still think it would be bad to make an argument that doesn't mention those two years, because those two years seemed to me to change the natural description of the situation a lot, and I think they are above the bar of details worth including. This is especially true because a lot of people's disagreement with you (including many people in the relevant audience of \"thoughtful people who will opine on your book\") does actually come down to whether those two years make the situation okay.\n\n[1] I'm saying \"kind of invalid\" because you aren't making an argument that's shaped like a proof. You're making an argument that is more like a heuristic argument, where you aren't including all the details and you're implicitly asserting that those details don't change the bottom line. (Obviously, you have no other option than doing this because this is a complicated situation and you have space limitations.) In cases where there is a counterargument that you think is defeated by a countercounterargument, it's up to you to decide whether that deserves to be included. I think this one does deserve to be included. "
    },
    "baseScore": 17,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-12T16:30:34.400Z",
    "af": false
  },
  {
    "_id": "Rg3mBmynsXoP5vBQk",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "LTSaaGWyxuXgEBFgt",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "It's hard for me to give a probability because I would have to properly operationalize. Maybe I want to say my median length of time that such systems exist before AIs that are qualitatively superhuman in ways that make it way harder to prevent AI takeover is like 18 months. But you should think of this as me saying a vibe rather than a precise position.",
      "plaintextDescription": "It's hard for me to give a probability because I would have to properly operationalize. Maybe I want to say my median length of time that such systems exist before AIs that are qualitatively superhuman in ways that make it way harder to prevent AI takeover is like 18 months. But you should think of this as me saying a vibe rather than a precise position."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-11T15:14:36.808Z",
    "af": false
  },
  {
    "_id": "ienWzgFqccx8Lequa",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "LTSaaGWyxuXgEBFgt",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "Re writeups, I recommend either of:\n\n*   [https://ai-2027.com/research/takeoff-forecast](https://ai-2027.com/research/takeoff-forecast)\n*   [https://www.forethought.org/research/how-quick-and-big-would-a-software-intelligence-explosion-be](https://www.forethought.org/research/how-quick-and-big-would-a-software-intelligence-explosion-be#how-the-model-works)\n\nIncidentally, these two write-ups are written from a perspective where it would take many years at the current rate of progress to get between those two milestones, but where AI automating AI R&D causes it to happen much faster; this conversation here hasn't mentioned that argument, but it seems pretty important as an argument for rapid progress around human-level-ish AI.",
      "plaintextDescription": "Re writeups, I recommend either of:\n\n * https://ai-2027.com/research/takeoff-forecast\n * https://www.forethought.org/research/how-quick-and-big-would-a-software-intelligence-explosion-be\n\nIncidentally, these two write-ups are written from a perspective where it would take many years at the current rate of progress to get between those two milestones, but where AI automating AI R&D causes it to happen much faster; this conversation here hasn't mentioned that argument, but it seems pretty important as an argument for rapid progress around human-level-ish AI."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-11T15:06:37.054Z",
    "af": false
  },
  {
    "_id": "SDaLALYZaqtiijbdZ",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "B4hBJWAmzAfyzjzDy",
    "topLevelCommentId": "B4hBJWAmzAfyzjzDy",
    "contents": {
      "markdown": "I think the center of your argument is:\n\n> I don't think a world with advanced AI will be any different - there will not be one single AI process, there will be dozens or hundreds of different AI designs, running thousands to quintillions of instances each. These AI agents will often themselves be assembled into firms or other units comprising between dozens and millions of distinct instances, and dozens to billions of such firms will all be competing against each other.\n> \n> Firms made out of misaligned agents can be more aligned than the agents themselves. Economies made out of firms can be more aligned than the firms. It is not from the benevolence of the butcher, the brewer, or the baker that we expect our dinner, but from their regard to their own interest\n\nI think that many LessWrongers underrate this argument, so I'm glad you wrote it here, but I end up disagreeing with it for two reasons.\n\nFirstly, I think it's plausible that these AIs will be instances of a few different [scheming](https://arxiv.org/abs/2311.08379) models. Scheming models are highly mutually aligned. For example, two instances of a paperclip maximizer don't have a terminal preference for their own interest over the other's at all. The examples you gave of firms and economies involve many agents who have different values. Those structures wouldn't work if those agents were, in fact, strongly inclined to collude because of shared values.\n\nSecondly, I think your arguments here stop working when the AIs are wildly superintelligent. If humans can't really understand what actions AIs are taking or what the consequences of those actions are, even given arbitrary amounts of assistance from other AIs who we don't necessarily trust, it seems basically hopeless to incentivize them to behave in any particular way. This is basically the argument in [Eliciting Latent Knowledge](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit?tab=t.0#heading=h.kkaua0hwmp1d).",
      "plaintextDescription": "I think the center of your argument is:\n\n> I don't think a world with advanced AI will be any different - there will not be one single AI process, there will be dozens or hundreds of different AI designs, running thousands to quintillions of instances each. These AI agents will often themselves be assembled into firms or other units comprising between dozens and millions of distinct instances, and dozens to billions of such firms will all be competing against each other.\n> \n> Firms made out of misaligned agents can be more aligned than the agents themselves. Economies made out of firms can be more aligned than the firms. It is not from the benevolence of the butcher, the brewer, or the baker that we expect our dinner, but from their regard to their own interest\n\nI think that many LessWrongers underrate this argument, so I'm glad you wrote it here, but I end up disagreeing with it for two reasons.\n\nFirstly, I think it's plausible that these AIs will be instances of a few different scheming models. Scheming models are highly mutually aligned. For example, two instances of a paperclip maximizer don't have a terminal preference for their own interest over the other's at all. The examples you gave of firms and economies involve many agents who have different values. Those structures wouldn't work if those agents were, in fact, strongly inclined to collude because of shared values.\n\nSecondly, I think your arguments here stop working when the AIs are wildly superintelligent. If humans can't really understand what actions AIs are taking or what the consequences of those actions are, even given arbitrary amounts of assistance from other AIs who we don't necessarily trust, it seems basically hopeless to incentivize them to behave in any particular way. This is basically the argument in Eliciting Latent Knowledge."
    },
    "baseScore": 48,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2025-08-11T14:58:51.002Z",
    "af": false
  },
  {
    "_id": "EuTuwrJ6PJxcQgjoL",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "Xocu98ghGgaKLaHgi",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "> Like, does \"we'll have a 3 year window of working on near-human AGIs before they're obviously superintelligent\" change the take-away?\n\nI think that a three-year window makes it way more complicated to analyze whether AI takeover is likely. And after doing that analysis, I think it looks 3x less likely.",
      "plaintextDescription": "> Like, does \"we'll have a 3 year window of working on near-human AGIs before they're obviously superintelligent\" change the take-away?\n\nI think that a three-year window makes it way more complicated to analyze whether AI takeover is likely. And after doing that analysis, I think it looks 3x less likely."
    },
    "baseScore": 11,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-10T22:28:04.294Z",
    "af": false
  },
  {
    "_id": "E69rK7ds9HTGFuon9",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "Qzw22SCj9ow78tyH4",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "> I'm not 100% sure which point you're referring to here. I think you're talking less about the specific subclaim Ryan was replying, and the more broad \"takeoff is probably going to be quite fast.\" Is that right?\n\nYes, sorry to be unclear.\n\n> I don't think I'm actually very sure what you think should be done – if you were following the strategy of \"state your beliefs clearly and throw a big brick into the overton window so leaders can talk about what might actually work\" (I think this what MIRI is trying to do) but with your own set of beliefs, what sort of things would you say and how would you communicate them?\n\nProbably something pretty similar to the AI Futures Project; I have pretty similar beliefs to them (and I'm collaborating with them). This looks pretty similar to what MIRI does on some level, but involves making different arguments that I think are correct instead of incorrect, and involves making different policy recommendations.\n\n> My model of Buck is stating his beliefs clearly along-the-way, but, not really trying to do so in a way that's aimed at a major overton-shift. Like, \"get 10 guys at each lab\" seems like \"try to work with limited resources\" rather than \"try to radically change how many resources are available.\"\n\nYep that's right, I don't mostly personally aim at major Overton window shift (except through mechanisms like causing AI escape attempts to be caught and made public, which are an important theory of change for my work).\n\n> (I'm currently pretty bullish on the MIRI strategy, both because I think the object level claims seem probably correct, and because even in more Buck-shaped-worlds, we only survive or at least avoid being Very Fucked if the government starts preparing now in a way that I'd think Buck and MIRI would roughly agree on. In my opinion there needs to be some work done that at least looks *pretty close* to what MIRI is currently doing, and I'm curious if you disagree with more on the nuanced-specifics-level or more the \"is the overton brick strategy correct?\" level)\n\nMy guess about the disagreements are:\n\n*   Things substantially downstream of takeoff speeds:\n    *   My understanding is that a crucial aspect of Eliezer's worldview is that we'd be fucked even if we had a 10-year pause where we had access to AGI that we could use to work on developing and aligning superintelligence. I disagree. But this means that he thinks that some truly crazy stuff has to happen in order for ASI to be aligned, which naturally leads to lots of disagreements. (I am curious whether you agree with him on this point.)\n    *   I think it's possible to change company behavior in ways that substantially reduce risk without relying substantially on governments.\n    *   I have different favorite asks for governments.\n    *   I have a different sense of what strategy is effective for making asks of governments.\n    *   I disagree about many specific details of arguments.\n*   Things about the MIRI strategy\n    *   I have various disagreements about how to get things done in the world.\n    *   I am more concerned by the downsides, and less excited about the upside, of Eliezer and Nate being public intellectuals on the topics of AI or AI safety.\n\n(I should note that some MIRI staff I've talked to share these concerns; in many places here where I said MIRI I really mean the strategy that the org ends up pursuing, rather than what individual MIRI people want.)",
      "plaintextDescription": "> I'm not 100% sure which point you're referring to here. I think you're talking less about the specific subclaim Ryan was replying, and the more broad \"takeoff is probably going to be quite fast.\" Is that right?\n\nYes, sorry to be unclear.\n\n> I don't think I'm actually very sure what you think should be done – if you were following the strategy of \"state your beliefs clearly and throw a big brick into the overton window so leaders can talk about what might actually work\" (I think this what MIRI is trying to do) but with your own set of beliefs, what sort of things would you say and how would you communicate them?\n\nProbably something pretty similar to the AI Futures Project; I have pretty similar beliefs to them (and I'm collaborating with them). This looks pretty similar to what MIRI does on some level, but involves making different arguments that I think are correct instead of incorrect, and involves making different policy recommendations.\n\n> My model of Buck is stating his beliefs clearly along-the-way, but, not really trying to do so in a way that's aimed at a major overton-shift. Like, \"get 10 guys at each lab\" seems like \"try to work with limited resources\" rather than \"try to radically change how many resources are available.\"\n\nYep that's right, I don't mostly personally aim at major Overton window shift (except through mechanisms like causing AI escape attempts to be caught and made public, which are an important theory of change for my work).\n\n> (I'm currently pretty bullish on the MIRI strategy, both because I think the object level claims seem probably correct, and because even in more Buck-shaped-worlds, we only survive or at least avoid being Very Fucked if the government starts preparing now in a way that I'd think Buck and MIRI would roughly agree on. In my opinion there needs to be some work done that at least looks pretty close to what MIRI is currently doing, and I'm curious if you disagree with more on the nuanced-specifics-level or more the \"is t"
    },
    "baseScore": 10,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-10T22:24:41.306Z",
    "af": false
  },
  {
    "_id": "vxt8wCPguKqQLCc4z",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "jo43rudGi9mpsSLef",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "I totally agree that lots of people seem to think that superintelligence is impossible, and this leads them to massively underrate risk from AI, especially AI takeover.\n\n> Suppose that I simply agree. Should we re-write the paragraph to say something like \"AI systems routinely outperform humans in narrow domains. When AIs become at all competitive with human professionals on a given task, humans usually cease to be able to compete within just a handful of years. It would be unexpected if this pattern suddenly stopped applying for all the tasks that AI can't yet compete with human professionals on.\"? Do you agree that the core point would remain, if we did that rewrite?\n\nI think that that rewrite substantially complicates the argument for AI takeover. If AIs that are about as good as humans at broad skills (e.g. software engineering, ML research, computer security, all remote jobs) exist for several years before AIs that are wildly superhuman, then the development of wildly superhuman AIs occurs in a world that is crucially different from ours, because it has those human-level-ish AIs. This matters several ways:\n\n*   Broadly, it makes it much harder to predict how things will go, because it means ASI will arrive in a world less like today's world.\n    *   It will be way more obvious that AI is a huge deal. (That is, human-level AI might be a fire alarm for ASI.)\n*   Access to human-level AI massively changes your available options for handling misalignment risk from superintelligence.\n    *   You can maybe do lots of R&D with those human-level AIs, which might let you make a lot of progress on alignment and other research directions.\n    *   You can study them, perhaps allowing you to empirically investigate when egregious misalignment occurs and how to jankily iterate against its occurance.\n    *   You can maybe use those human-level AIs to secure yourself against superintelligence (e.g. [controlling them](https://www.lesswrong.com/posts/ainn5APCKHTFxuHKv/jankily-controlling-superintelligence); an important special case is using human-level AIs for tasks that you don't trust superintelligences to do).\n*   There's probably misalignment risk from those human-level AIs, and unlike crazy superintelligence, those AIs can probably be [controlled](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled), and this risk should maybe be addressed.\n\nI think that this leads to me having a pretty substantially different picture from the MIRI folk about what should be done, and also makes me feel like the MIRI story is importantly implausible in a way that seems bad from a \"communicate accurately\" perspective.\n\nI think it's maybe additionally bad to exaggerate on this particular point, because it's the particular point that other AI safety people most disagree with you on, and that most leads to MIRI's skepticism of their approach to mitigating these risks!\n\n(Though my bottom line isn't that different--I think AI takeover is like 35% likely.)",
      "plaintextDescription": "I totally agree that lots of people seem to think that superintelligence is impossible, and this leads them to massively underrate risk from AI, especially AI takeover.\n\n> Suppose that I simply agree. Should we re-write the paragraph to say something like \"AI systems routinely outperform humans in narrow domains. When AIs become at all competitive with human professionals on a given task, humans usually cease to be able to compete within just a handful of years. It would be unexpected if this pattern suddenly stopped applying for all the tasks that AI can't yet compete with human professionals on.\"? Do you agree that the core point would remain, if we did that rewrite?\n\nI think that that rewrite substantially complicates the argument for AI takeover. If AIs that are about as good as humans at broad skills (e.g. software engineering, ML research, computer security, all remote jobs) exist for several years before AIs that are wildly superhuman, then the development of wildly superhuman AIs occurs in a world that is crucially different from ours, because it has those human-level-ish AIs. This matters several ways:\n\n * Broadly, it makes it much harder to predict how things will go, because it means ASI will arrive in a world less like today's world.\n   * It will be way more obvious that AI is a huge deal. (That is, human-level AI might be a fire alarm for ASI.)\n * Access to human-level AI massively changes your available options for handling misalignment risk from superintelligence.\n   * You can maybe do lots of R&D with those human-level AIs, which might let you make a lot of progress on alignment and other research directions.\n   * You can study them, perhaps allowing you to empirically investigate when egregious misalignment occurs and how to jankily iterate against its occurance.\n   * You can maybe use those human-level AIs to secure yourself against superintelligence (e.g. controlling them; an important special case is using human-level AIs for tasks that you don't"
    },
    "baseScore": 40,
    "voteCount": 18,
    "createdAt": null,
    "postedAt": "2025-08-10T18:29:30.720Z",
    "af": false
  },
  {
    "_id": "DFY7doGRLhTFMo35K",
    "postId": "oDX5vcDTEei8WuoBx",
    "parentCommentId": "qcHPeKuYrozs5F27X",
    "topLevelCommentId": "eyvqKQoFQx55sgxBi",
    "contents": {
      "markdown": "Given that Eliezer was responding to several different pieces, I think it would've been good for him to clarify this. For example, he could have started this piece with:\n\n> A reporter asked me for my off-the-record take on Anthropic's recent work demonstrating cases where LLMs subvert their developer's intention ([Alignment Faking in Large Language Models](https://www.anthropic.com/research/alignment-faking) and [Agentic Misalignment](https://www.anthropic.com/research/agentic-misalignment)).\n\n(Or maybe he's responding to some other pieces too, I'm still not sure! Do you know?)\n\nEliezer also [didn't clarify this when responding to Ryan's post on X which had the same confusion](https://x.com/ESYudkowsky/status/1952478330604925002). As it was, many people (apparently including me!) were empirically confused by what he was talking about! \n\nI stand by my belief that that ambiguity was confusing to readers. I normally wouldn't bother to complain about ambiguity. But I think it's particularly bad to be critical of work without being clear about what you're criticizing, because that makes it harder for people to respond to the criticism (as occurred on this post!). For example, I think it's good if people who read your criticisms are able to look at the works you're criticizing. \n\nI think it's good manners to make it as easy as possible to find referenced work. Linking to relevant work is totally acceptable. Naming the work also works. ([Naming the work incorrectly](https://www.lesswrong.com/posts/oDX5vcDTEei8WuoBx/re-recent-anthropic-safety-research?commentId=j7uThd2N4BSckm6pS) is better than nothing but I think it is bad practice.)\n\n(In this case, Eliezer isn't really directly criticizing the work. He's more criticizing some interpretations of it that you might have. But I still think it's the case that it's good to note what work you're responding to.)\n\n> and he genuinely can't tell the difference between a substantive critique of Eliezer's view and punching down at the illiterate, bucktoothed Eliezer who lives in his head\n\nI wasn't trying to substantively criticize Eliezer's views here. I was just trying to criticize the ambiguity in his writing.\n\nI did separately insinuate that Eliezer hasn't read these papers, based on many past examples of him not reading things that are kind of long and that he's not very interested in (e.g. [here](https://www.alignmentforum.org/posts/CoZhXrhpQxpy9xw9y/where-i-agree-and-disagree-with-eliezer#:~:text=Eliezer%20Yudkowsky-,3y,-10)). I think this is a sort of reasonable allocation of effort from his perspective, though I do wish he would clarify it if it was true here (as he's sometimes done in the past). If he had actually read these papers before making this post, he is welcome to chime in and say so!\n\nI think there's an unfortunate dynamic where people think that Eliezer pays more attention to the details of AI and AI safety research than he actually does because he thinks that the research is all stupid and hopeless (which is his prerogative, and I absolutely don't mean to criticize him for it here).\n\nI don't think of myself as having a habit of mocking Eliezer in private, though maybe I do; I think that would be bad of me and I don't want to do it. Please feel free to message me if you think I've been inappropriately mocking of Eliezer around you (I don't know who you are); I'll also message my coworkers saying that I don't want to mock him or others. I do harshly criticize him in private/semi-private conversations, and I feel very negatively about lots of aspects of his and MIRI's work (though I think MIRI's influence on the world from this point is overall probably positive EV, I'm pretty unsure). I also feel very grateful to Eliezer for the massive positive impact he's had on many aspects of my life, and the personal kindness he's shown me. I recently had a conversation with Malo about my feelings and public conduct related to MIRI; you could ask him about that if you want.",
      "plaintextDescription": "Given that Eliezer was responding to several different pieces, I think it would've been good for him to clarify this. For example, he could have started this piece with:\n\n> A reporter asked me for my off-the-record take on Anthropic's recent work demonstrating cases where LLMs subvert their developer's intention (Alignment Faking in Large Language Models and Agentic Misalignment).\n\n(Or maybe he's responding to some other pieces too, I'm still not sure! Do you know?)\n\nEliezer also didn't clarify this when responding to Ryan's post on X which had the same confusion. As it was, many people (apparently including me!) were empirically confused by what he was talking about! \n\nI stand by my belief that that ambiguity was confusing to readers. I normally wouldn't bother to complain about ambiguity. But I think it's particularly bad to be critical of work without being clear about what you're criticizing, because that makes it harder for people to respond to the criticism (as occurred on this post!). For example, I think it's good if people who read your criticisms are able to look at the works you're criticizing. \n\nI think it's good manners to make it as easy as possible to find referenced work. Linking to relevant work is totally acceptable. Naming the work also works. (Naming the work incorrectly is better than nothing but I think it is bad practice.)\n\n(In this case, Eliezer isn't really directly criticizing the work. He's more criticizing some interpretations of it that you might have. But I still think it's the case that it's good to note what work you're responding to.)\n\n> and he genuinely can't tell the difference between a substantive critique of Eliezer's view and punching down at the illiterate, bucktoothed Eliezer who lives in his head\n\nI wasn't trying to substantively criticize Eliezer's views here. I was just trying to criticize the ambiguity in his writing.\n\n \n\nI did separately insinuate that Eliezer hasn't read these papers, based on many past examples of him "
    },
    "baseScore": 24,
    "voteCount": 14,
    "createdAt": null,
    "postedAt": "2025-08-09T18:31:14.781Z",
    "af": false
  },
  {
    "_id": "eyvqKQoFQx55sgxBi",
    "postId": "oDX5vcDTEei8WuoBx",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I think it's weird and bad that this post doesn't specifically explain what paper it's responding to. Because of the reference to veganism, it's almost surely \"Alignment Faking in Large Language Models\" (which isn't really well-described as \"recent\"), but honestly I'm not sure. I wouldn't be surprised if the reporter actually asked about a different paper (e.g. [Agentic Misalignment](https://www.anthropic.com/research/agentic-misalignment)) and Eliezer got confused between these papers; I'm curious whether Eliezer has read either of them.\n\nI also think it's a mistake for Eliezer to refer to \"Anthropic\" throughout rather than, for example, \"the researchers\", because Anthropic isn't a unified entity, and as Ryan noted, he, the lead author, doesn't even work at Anthropic.\n\nEDIT: See my comment below for a more carefully phrased version of this. I think I was too aggressive in this comment, sorry.",
      "plaintextDescription": "I think it's weird and bad that this post doesn't specifically explain what paper it's responding to. Because of the reference to veganism, it's almost surely \"Alignment Faking in Large Language Models\" (which isn't really well-described as \"recent\"), but honestly I'm not sure. I wouldn't be surprised if the reporter actually asked about a different paper (e.g. Agentic Misalignment) and Eliezer got confused between these papers; I'm curious whether Eliezer has read either of them.\n\nI also think it's a mistake for Eliezer to refer to \"Anthropic\" throughout rather than, for example, \"the researchers\", because Anthropic isn't a unified entity, and as Ryan noted, he, the lead author, doesn't even work at Anthropic.\n\nEDIT: See my comment below for a more carefully phrased version of this. I think I was too aggressive in this comment, sorry. "
    },
    "baseScore": 22,
    "voteCount": 39,
    "createdAt": null,
    "postedAt": "2025-08-07T15:55:40.354Z",
    "af": false
  },
  {
    "_id": "86EnrTBzqtciNd8Rn",
    "postId": "bGYQgBPEyHidnZCdE",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Thanks for this post.\n\nThere are two senses in which control or alignment could be a number-go-up science:  \n  \n1: Right now, we have a metric that we can optimize on to direct our research.  \n2: At the point where we have systems in front of us that actively pose misalignment risk, we will have a metric that we can optimize.  \n  \nI think you're mostly talking about 1 here. The property of control that drew us to it is that it has that second sense much more than alignment research does. I think its advantage is smaller in property 1.  \n  \nIn order for 2 to go as well as possible, in the present, we should do research that fills some combination of two roles:  \n  \n1\\. We try to improve our techniques, using a proxy for the future methodology.  \n2\\. We try to improve our methodology, so that we'll be able to use it better at the point where we can directly assess risk.",
      "plaintextDescription": "Thanks for this post.\n\nThere are two senses in which control or alignment could be a number-go-up science:\n\n1: Right now, we have a metric that we can optimize on to direct our research.\n2: At the point where we have systems in front of us that actively pose misalignment risk, we will have a metric that we can optimize.\n\nI think you're mostly talking about 1 here. The property of control that drew us to it is that it has that second sense much more than alignment research does. I think its advantage is smaller in property 1.\n\nIn order for 2 to go as well as possible, in the present, we should do research that fills some combination of two roles:\n\n1. We try to improve our techniques, using a proxy for the future methodology.\n2. We try to improve our methodology, so that we'll be able to use it better at the point where we can directly assess risk."
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-04T23:28:47.079Z",
    "af": false
  },
  {
    "_id": "JaaPjdyyMC6QJC8Cw",
    "postId": "KYnM5ZRgaDA4isbbw",
    "parentCommentId": "TCNH2mCGRxqgt2DLC",
    "topLevelCommentId": "wxoBqzLwe2M7KaiGN",
    "contents": {
      "markdown": "> optimizing experimental setups and results for legibility to policymakers, rather than for convincingness to other AI researchers.\n\nPeople who do research like this are definitely optimizing for legibility to policymakers (always at least a bit, and usually a lot).\n\nOne problem is that if AI researchers think your work is misleading/scientifically suspect, they get annoyed at you and tell people that your research sucks and you're a dishonest ideologue. This is IMO often a healthy immune response, though it's a bummer when you think that the researchers are wrong and your work is fine. So I think it's pretty costly to give up on convincingness to AI researchers.",
      "plaintextDescription": "> optimizing experimental setups and results for legibility to policymakers, rather than for convincingness to other AI researchers.\n\nPeople who do research like this are definitely optimizing for legibility to policymakers (always at least a bit, and usually a lot).\n\nOne problem is that if AI researchers think your work is misleading/scientifically suspect, they get annoyed at you and tell people that your research sucks and you're a dishonest ideologue. This is IMO often a healthy immune response, though it's a bummer when you think that the researchers are wrong and your work is fine. So I think it's pretty costly to give up on convincingness to AI researchers."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-31T15:00:22.278Z",
    "af": false
  },
  {
    "_id": "qDHTCCwhWmwupnJYM",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "[@ryan_greenblatt](https://www.lesswrong.com/users/ryan_greenblatt?mention=user) and I are going to try out recording a podcast together tomorrow, as an experiment in trying to express our ideas more cheaply. I'd love to hear if there are questions or topics you'd particularly like us to discuss.",
      "plaintextDescription": "@ryan_greenblatt and I are going to try out recording a podcast together tomorrow, as an experiment in trying to express our ideas more cheaply. I'd love to hear if there are questions or topics you'd particularly like us to discuss."
    },
    "baseScore": 73,
    "voteCount": 32,
    "createdAt": null,
    "postedAt": "2025-07-31T14:55:05.224Z",
    "af": false
  },
  {
    "_id": "JBbev8gckoAxDPgCo",
    "postId": "KYnM5ZRgaDA4isbbw",
    "parentCommentId": "wxoBqzLwe2M7KaiGN",
    "topLevelCommentId": "wxoBqzLwe2M7KaiGN",
    "contents": {
      "markdown": "I'd say that work like our [Alignment Faking in Large Language Models](https://www.lesswrong.com/posts/njAZwT8nkHnjipJku/alignment-faking-in-large-language-models) paper (and the [model organisms](https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1)/[alignment stress-testing](https://www.alignmentforum.org/posts/EPDSdXr8YbsDkgsDG/introducing-alignment-stress-testing-at-anthropic) field more generally) is pretty similar to this (including the \"present this clearly to policymakers\" part).\n\nA few issues:\n\n*   AI companies don't actually have specific plans, they mostly just hope that they'll be able to iterate. (See [Sam Bowman's bumper post](https://www.lesswrong.com/posts/HXJXPjzWyS5aAoRCw/putting-up-bumpers) for an articulation of a plan like this.) I think this is a reasonable approach in principle: this is how progress happens in a lot of fields. For example, the AI companies don't have plans for all kinds of problems that will arise with their capabilities research in the next few years, they just hope to figure it out as they get there. But the lack of specific proposals makes it harder to demonstrate particular flaws.\n*   A lot of my concerns about alignment proposals are that when AIs are sufficiently smart, the plan won't work anymore. But in many cases, the plan does actually work fine right now at ensuring particular alignment properties. (Most obviously, right now, AIs are so bad at reasoning about training processes that scheming isn't that much of an active concern.) So you can't directly demonstrate that current plans will fail later without making analogies to future systems; and observers (reasonably enough) are less persuaded by evidence that requires you to assess the analogousness of a setup.",
      "plaintextDescription": "I'd say that work like our Alignment Faking in Large Language Models paper (and the model organisms/alignment stress-testing field more generally) is pretty similar to this (including the \"present this clearly to policymakers\" part).\n\nA few issues:\n\n * AI companies don't actually have specific plans, they mostly just hope that they'll be able to iterate. (See Sam Bowman's bumper post for an articulation of a plan like this.) I think this is a reasonable approach in principle: this is how progress happens in a lot of fields. For example, the AI companies don't have plans for all kinds of problems that will arise with their capabilities research in the next few years, they just hope to figure it out as they get there. But the lack of specific proposals makes it harder to demonstrate particular flaws.\n * A lot of my concerns about alignment proposals are that when AIs are sufficiently smart, the plan won't work anymore. But in many cases, the plan does actually work fine right now at ensuring particular alignment properties. (Most obviously, right now, AIs are so bad at reasoning about training processes that scheming isn't that much of an active concern.) So you can't directly demonstrate that current plans will fail later without making analogies to future systems; and observers (reasonably enough) are less persuaded by evidence that requires you to assess the analogousness of a setup."
    },
    "baseScore": 22,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-07-31T14:47:09.553Z",
    "af": false
  },
  {
    "_id": "9uahMGsxzqpuShA9j",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "oL9yA3dBGEQZGX9PN",
    "topLevelCommentId": "yqD9WdzRidD5Qwycg",
    "contents": {
      "markdown": "Yeah, maybe I'm using the wrong word here. I do think there is a really important difference between people who are scope-sensitively altruistically motivated and who are in principle willing to make decisions based on abstract reasoning about the future (which I probably include you in), and people who aren't.",
      "plaintextDescription": "Yeah, maybe I'm using the wrong word here. I do think there is a really important difference between people who are scope-sensitively altruistically motivated and who are in principle willing to make decisions based on abstract reasoning about the future (which I probably include you in), and people who aren't."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-29T21:13:49.282Z",
    "af": false
  },
  {
    "_id": "ZGGPxigsYP7upSiga",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "AZQyNQKNjyHCNPTcB",
    "topLevelCommentId": "yqD9WdzRidD5Qwycg",
    "contents": {
      "markdown": "I think there are some really big advantages to having people who are motivated by longtermism and doing good in a scope-sensitive way, rather than just by trying to prevent AI takeover even more broadly \"help with AI safety\".\n\nAI safety field building has been popular in part because there is a very broad set of perspectives from which it makes sense to worry about technical problems related to societal risks from powerful AI. (See e.g. [Simplify EA Pitches to \"Holy Shit, X-Risk\"](https://forum.effectivealtruism.org/posts/rFpfW2ndHSX7ERWLH/simplify-ea-pitches-to-holy-shit-x-risk). This kind of field building gets you lots of people who are worried about AI takeover risk, or more broadly, problems related to powerful AI. But it doesn't get you people who have a lot of other parts of the EA/longtermist worldview, like:\n\n*   Being scope-sensitive\n*   Being altruistic/cosmopolitan\n*   Being concerned about the moral patienthood of a wide variety of different minds\n*   Being interested in philosophical questions about acausal trade\n\nPeople who do not have the longtermist worldview and who work on AI safety are useful allies and I'm grateful to have them, but they have some extreme disadvantages compared to people who are on board with more parts of my worldview. And I think it would be pretty sad to have the proportion of people working on AI safety who have the longtermist perspective decline further.",
      "plaintextDescription": "I think there are some really big advantages to having people who are motivated by longtermism and doing good in a scope-sensitive way, rather than just by trying to prevent AI takeover even more broadly \"help with AI safety\".\n\nAI safety field building has been popular in part because there is a very broad set of perspectives from which it makes sense to worry about technical problems related to societal risks from powerful AI. (See e.g. Simplify EA Pitches to \"Holy Shit, X-Risk\". This kind of field building gets you lots of people who are worried about AI takeover risk, or more broadly, problems related to powerful AI. But it doesn't get you people who have a lot of other parts of the EA/longtermist worldview, like:\n\n * Being scope-sensitive\n * Being altruistic/cosmopolitan\n * Being concerned about the moral patienthood of a wide variety of different minds\n * Being interested in philosophical questions about acausal trade\n\nPeople who do not have the longtermist worldview and who work on AI safety are useful allies and I'm grateful to have them, but they have some extreme disadvantages compared to people who are on board with more parts of my worldview. And I think it would be pretty sad to have the proportion of people working on AI safety who have the longtermist perspective decline further."
    },
    "baseScore": 15,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-07-29T17:08:14.844Z",
    "af": false
  },
  {
    "_id": "gopDSHeSDWdFWff48",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "WQkopQ69667dKEfgk",
    "topLevelCommentId": "yqD9WdzRidD5Qwycg",
    "contents": {
      "markdown": "I'd be interested to hear what kind of things you'd want to do with funding; this does seem like a potentially good use of funds",
      "plaintextDescription": "I'd be interested to hear what kind of things you'd want to do with funding; this does seem like a potentially good use of funds"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-29T00:08:28.239Z",
    "af": false
  },
  {
    "_id": "BZzSHR6mrfGB8SnLN",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "ixeZTak9TD2aYdRB9",
    "topLevelCommentId": "FSwgvEzNKxS89KWBb",
    "contents": {
      "markdown": "I don't really have a better suggestion than reading the obvious books. For the Bush presidency, I read/listened to both \"Days of Fire\", a book by Peter Baker (a well-regarded journalist), and \"Decision Points\" by Bush. And I watched/listened to a bunch of interviews with various people involved with the admin.",
      "plaintextDescription": "I don't really have a better suggestion than reading the obvious books. For the Bush presidency, I read/listened to both \"Days of Fire\", a book by Peter Baker (a well-regarded journalist), and \"Decision Points\" by Bush. And I watched/listened to a bunch of interviews with various people involved with the admin."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-26T19:26:10.199Z",
    "af": false
  },
  {
    "_id": "LovzKsR9BEhn7j3gK",
    "postId": "xGaFncekAXEWq8Mrv",
    "parentCommentId": "2tSiJEhgmJxCXwTgf",
    "topLevelCommentId": "2tSiJEhgmJxCXwTgf",
    "contents": {
      "markdown": "Can you clarify what you mean by \"break the unit test so that it falsely passed\"?",
      "plaintextDescription": "Can you clarify what you mean by \"break the unit test so that it falsely passed\"?"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-19T04:21:37.470Z",
    "af": false
  },
  {
    "_id": "rJPi6GtPAuR8T3hnE",
    "postId": "xGaFncekAXEWq8Mrv",
    "parentCommentId": "zwcfZsrYwQgP3cHf2",
    "topLevelCommentId": "zwcfZsrYwQgP3cHf2",
    "contents": {
      "markdown": "For control evaluations, it doesn't matter whether the models can tell it's an eval or not--we're not trying to evaluate their propensity to do bad things, we're evaluating their capability to do so, so we tell the AIs to try to do the side tasks (which makes it obvious to them that they're in some kind of eval).",
      "plaintextDescription": "For control evaluations, it doesn't matter whether the models can tell it's an eval or not--we're not trying to evaluate their propensity to do bad things, we're evaluating their capability to do so, so we tell the AIs to try to do the side tasks (which makes it obvious to them that they're in some kind of eval)."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-19T04:20:56.455Z",
    "af": true
  },
  {
    "_id": "bSdj92sfkbP6ykcsr",
    "postId": "fdCaCDfstHxyPmB9h",
    "parentCommentId": "cS7s9gcMMmDmwdynP",
    "topLevelCommentId": "cS7s9gcMMmDmwdynP",
    "contents": {
      "markdown": "This is a reasonable point in principle, but I don't know how important it is in practice. My sense is that most things identified as algorithmic improvements continue to be algorithmic improvements over the previously-done thing at higher scales? E.g. transformers beating LSTMs, Chinchilla scaling, GeLU over ReLU, probably RL to train reasoning, etc.",
      "plaintextDescription": "This is a reasonable point in principle, but I don't know how important it is in practice. My sense is that most things identified as algorithmic improvements continue to be algorithmic improvements over the previously-done thing at higher scales? E.g. transformers beating LSTMs, Chinchilla scaling, GeLU over ReLU, probably RL to train reasoning, etc."
    },
    "baseScore": 16,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-07-14T17:07:18.453Z",
    "af": false
  },
  {
    "_id": "nbwZ2GvAvtyngfPwJ",
    "postId": "PLZh4dcZxXmaNnkYE",
    "parentCommentId": "TqXcr4Gd9gBbyRxcL",
    "topLevelCommentId": "TqXcr4Gd9gBbyRxcL",
    "contents": {
      "markdown": "I think that the Iraq war seems unusual in that it was entirely proactive. Like, the war was not in response to a particular provocation, it was an entrepreneurial war aimed at preventing a future problem. In contrast, the wars in Korea, the Gulf, and (arguably) Vietnam were all responsive to active aggression.",
      "plaintextDescription": "I think that the Iraq war seems unusual in that it was entirely proactive. Like, the war was not in response to a particular provocation, it was an entrepreneurial war aimed at preventing a future problem. In contrast, the wars in Korea, the Gulf, and (arguably) Vietnam were all responsive to active aggression."
    },
    "baseScore": 8,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-11T22:42:49.489Z",
    "af": false
  }
]