[
  {
    "_id": "8bgEyK26BxDv3gt4s",
    "postId": "uWTAoisAqnDqDQp6A",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Thanks for making this! For what it's worth, it looks like 22757.12(d) requires developers to be somewhat transparent about risks from internal deployment, which seems quite interesting to me and I'm surprised didn't make the summary.",
      "plaintextDescription": "Thanks for making this! For what it's worth, it looks like 22757.12(d) requires developers to be somewhat transparent about risks from internal deployment, which seems quite interesting to me and I'm surprised didn't make the summary."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-23T21:10:33.720Z",
    "af": false
  },
  {
    "_id": "hvqyxvxrmTebFM58A",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I wonder if \"discourse in such a way that your interlocutor, if they decided to adopt good faith, could easily deal with you (or otherwise leave)\" gets you the benefits of \"assume good faith\" without the drawbacks of asking you to sometimes assume false stuff?",
      "plaintextDescription": "I wonder if \"discourse in such a way that your interlocutor, if they decided to adopt good faith, could easily deal with you (or otherwise leave)\" gets you the benefits of \"assume good faith\" without the drawbacks of asking you to sometimes assume false stuff?"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-26T16:22:15.565Z",
    "af": false
  },
  {
    "_id": "bjW8xuyH7eomEcHSA",
    "postId": "n6nsPzJWurKWKk2pA",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "How do these hold up in backpacks / luggage bags? I'm worried they'd catch on stuff and tear pages more than other bookmarks (that can be pushed totally into the book).",
      "plaintextDescription": "How do these hold up in backpacks / luggage bags? I'm worried they'd catch on stuff and tear pages more than other bookmarks (that can be pushed totally into the book)."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-26T06:49:44.610Z",
    "af": false
  },
  {
    "_id": "BHzkftodjDiXRiPYS",
    "postId": "98sCTsGJZ77WgQ6nE",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> And if the stakes are even higher, you can ultimately try to get me fired from this job. The exact social process for who can fire me is not as clear to me as I would like, but you can convince Eliezer to give head-moderatorship to someone else, or convince the board of Lightcone Infrastructure to replace me as CEO, if you really desperately want LessWrong to be different than it is.\n\nI don't plan on doing this, but who is on the board of Lightcone Infrastructure? This doesn't seem to be on [your website](https://www.lightconeinfrastructure.com/index.html).",
      "plaintextDescription": "> And if the stakes are even higher, you can ultimately try to get me fired from this job. The exact social process for who can fire me is not as clear to me as I would like, but you can convince Eliezer to give head-moderatorship to someone else, or convince the board of Lightcone Infrastructure to replace me as CEO, if you really desperately want LessWrong to be different than it is.\n\nI don't plan on doing this, but who is on the board of Lightcone Infrastructure? This doesn't seem to be on your website."
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-26T04:04:00.313Z",
    "af": false
  },
  {
    "_id": "W6jAoaLXBMbxTNXGa",
    "postId": "98sCTsGJZ77WgQ6nE",
    "parentCommentId": "iPfzSFsEuTvjSEJKt",
    "topLevelCommentId": "6HGKAQJb9cSxBjHiA",
    "contents": {
      "markdown": "> Like, I guess I have never heard the term \"civil justice\" used instead, and I don't know of a better term that clearly spans both\n\nJust realized I never responded to this - I would just use the term \"civil law\" (as I did). For a term that covers both, \"the legal system\" perhaps, altho it's a bit too broad, and you're right that there's not a great option.",
      "plaintextDescription": "> Like, I guess I have never heard the term \"civil justice\" used instead, and I don't know of a better term that clearly spans both\n\nJust realized I never responded to this - I would just use the term \"civil law\" (as I did). For a term that covers both, \"the legal system\" perhaps, altho it's a bit too broad, and you're right that there's not a great option."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-25T20:44:35.824Z",
    "af": false
  },
  {
    "_id": "qky7KTmiAwdupQq4g",
    "postId": "98sCTsGJZ77WgQ6nE",
    "parentCommentId": "ndFkzqafAyRfNxAti",
    "topLevelCommentId": "6HGKAQJb9cSxBjHiA",
    "contents": {
      "markdown": "I can't comment on how things work in Germany, since they have a very different structure of law (that my guess is English-language terms are not well-designed for), but:\n> I agree one could maybe make some argument that it's not \"criminal justice\" until you \"commit a crime by violating a civil court order\"\n\nThis is what I think - in particular, the \"criminal justice system\" is the system that involves dealing with crimes, and the \"civil law system\" is the system that involves dealing with civil wrongs. You're correct that they relate, but there are enough distinctions (who brings cases, proof standards, typical punishments, source of the laws) that I think it makes sense to distinguish them. I further think that most people with enough context to know the difference between civil and criminal law would not guess that a similarly informed person would use the term \"criminal justice system\" to cover civil law.",
      "plaintextDescription": "I can't comment on how things work in Germany, since they have a very different structure of law (that my guess is English-language terms are not well-designed for), but:\n\n> I agree one could maybe make some argument that it's not \"criminal justice\" until you \"commit a crime by violating a civil court order\"\n\nThis is what I think - in particular, the \"criminal justice system\" is the system that involves dealing with crimes, and the \"civil law system\" is the system that involves dealing with civil wrongs. You're correct that they relate, but there are enough distinctions (who brings cases, proof standards, typical punishments, source of the laws) that I think it makes sense to distinguish them. I further think that most people with enough context to know the difference between civil and criminal law would not guess that a similarly informed person would use the term \"criminal justice system\" to cover civil law."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-25T20:42:53.838Z",
    "af": false
  },
  {
    "_id": "8rEmJySb67qHupaq8",
    "postId": "98sCTsGJZ77WgQ6nE",
    "parentCommentId": "dAyJJGod5xNEEKcDM",
    "topLevelCommentId": "6HGKAQJb9cSxBjHiA",
    "contents": {
      "markdown": "Relevant evidence from [the Wikipedia page on Criminal justice](https://en.wikipedia.org/wiki/Criminal_justice):\n> Criminal justice is the delivery of justice to those who have committed crimes[...]\n> \n> The criminal justice system consists of three main parts:\n> 1. Law enforcement agencies, usually the police\n> 2. Courts and accompanying prosecution and defence lawyers\n> 3. Agencies for detaining and supervising offenders, such as prisons and probation agencies.\n\nCivil law lacks parts 1 and 3.",
      "plaintextDescription": "Relevant evidence from the Wikipedia page on Criminal justice:\n\n> Criminal justice is the delivery of justice to those who have committed crimes[...]\n> \n> The criminal justice system consists of three main parts:\n> \n>  1. Law enforcement agencies, usually the police\n>  2. Courts and accompanying prosecution and defence lawyers\n>  3. Agencies for detaining and supervising offenders, such as prisons and probation agencies.\n\nCivil law lacks parts 1 and 3."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-25T20:01:24.355Z",
    "af": false
  },
  {
    "_id": "dAyJJGod5xNEEKcDM",
    "postId": "98sCTsGJZ77WgQ6nE",
    "parentCommentId": "iPfzSFsEuTvjSEJKt",
    "topLevelCommentId": "6HGKAQJb9cSxBjHiA",
    "contents": {
      "markdown": "I would not use the term \"criminal justice\" to describe civil law, since civil law deals with civil wrongs rather than crimes.",
      "plaintextDescription": "I would not use the term \"criminal justice\" to describe civil law, since civil law deals with civil wrongs rather than crimes."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-25T19:59:06.153Z",
    "af": false
  },
  {
    "_id": "6HGKAQJb9cSxBjHiA",
    "postId": "98sCTsGJZ77WgQ6nE",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> A classical example of microeconomics-informed reasoning about criminal justice is the following snippet of logic.\n> \n> If someone can gain in-expectation \\\\(X\\\\) dollars by committing some crime (which has negative externalities of \\\\(Y > X\\\\) dollars), with a probability \\\\(p\\\\) of getting caught, then in order to successfully prevent people from committing the crime you need to make the cost of receiving the punishment (\\\\(Z\\\\)) be greater than \\\\(\\frac{X}{p}\\\\), i.e. \\\\(X < p * Z\\\\). \n\nNote that this is more centrally an example of micro-informed reasoning about the role of punitive damages in civil law, not criminal law, as illustrated by [this classic article making basically this argument](https://heinonline.org/HOL/Page?collection=journals&handle=hein.journals/hlr111&id=891&men_tab=srchresults) about punitive damages.",
      "plaintextDescription": "> A classical example of microeconomics-informed reasoning about criminal justice is the following snippet of logic.\n> \n> If someone can gain in-expectation X dollars by committing some crime (which has negative externalities of Y>X dollars), with a probability p of getting caught, then in order to successfully prevent people from committing the crime you need to make the cost of receiving the punishment (Z) be greater than Xp, i.e. X<p∗Z. \n\nNote that this is more centrally an example of micro-informed reasoning about the role of punitive damages in civil law, not criminal law, as illustrated by this classic article making basically this argument about punitive damages."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-25T19:50:57.572Z",
    "af": false
  },
  {
    "_id": "kW34mvffMPLBfkbpc",
    "postId": "ENCNHyNEgvz9oo9rr",
    "parentCommentId": "8uyrdwg8pLtoHCbBN",
    "topLevelCommentId": "dnBdiujSTLATuwvhD",
    "contents": {
      "markdown": "My understanding as a guy who... watches a bunch of YouTube videos and promises he's right:\n- Within Catholicism, becoming a monk is not normally described as \"ordination\"\n- Within Catholicism, you can definitely be a monk who's ordained to the priesthood \n\nBut you're right that there are different change-of-status ceremonies that denote different kinds of entrance into intense official religious life.",
      "plaintextDescription": "My understanding as a guy who... watches a bunch of YouTube videos and promises he's right:\n\n * Within Catholicism, becoming a monk is not normally described as \"ordination\"\n * Within Catholicism, you can definitely be a monk who's ordained to the priesthood\n\nBut you're right that there are different change-of-status ceremonies that denote different kinds of entrance into intense official religious life."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-23T03:52:48.268Z",
    "af": false
  },
  {
    "_id": "dSykMsNjarsRoQ9E6",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "hr5TztncdNtrvmMzz",
    "topLevelCommentId": "sNPkkFGZmhyiydZoo",
    "contents": {
      "markdown": "Sure, but I bet that's because in fact people are usually attuned to the technical details. I imagine if you were really bad on the technical details, that would become a bigger bottleneck.\n\n[Epistemic status: I have never really worked at a big company and Richard has. I have been a PhD student at UC Berkeley but I don't think that counts]",
      "plaintextDescription": "Sure, but I bet that's because in fact people are usually attuned to the technical details. I imagine if you were really bad on the technical details, that would become a bigger bottleneck.\n\n[Epistemic status: I have never really worked at a big company and Richard has. I have been a PhD student at UC Berkeley but I don't think that counts]"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-22T04:57:32.695Z",
    "af": false
  },
  {
    "_id": "ug7HrH5wLm59r4AdX",
    "postId": "n6nsPzJWurKWKk2pA",
    "parentCommentId": "muQXiqadiAKXAgfcy",
    "topLevelCommentId": "muQXiqadiAKXAgfcy",
    "contents": {
      "markdown": "Thing I did not realize: they cost significantly less if you order from [the Book Darts website](https://www.bookdarts.com/) rather than Field Notes.",
      "plaintextDescription": "Thing I did not realize: they cost significantly less if you order from the Book Darts website rather than Field Notes."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-20T23:39:07.027Z",
    "af": false
  },
  {
    "_id": "NQcqiA4jJiAgqmmWj",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "xuqs26dniD4CnT6nL",
    "topLevelCommentId": "sNPkkFGZmhyiydZoo",
    "contents": {
      "markdown": "I guess there's also \"do you expect to have enough time with your audience for you to develop an argument and them to notice flaws\", which I think correlates with technocratic vs democratic but isn't the same.",
      "plaintextDescription": "I guess there's also \"do you expect to have enough time with your audience for you to develop an argument and them to notice flaws\", which I think correlates with technocratic vs democratic but isn't the same."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-20T22:17:49.027Z",
    "af": false
  },
  {
    "_id": "pJSdrwGG8s8XkGGrC",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> In this post, I mostly conflated \"being a moderate\" with \"working with people at AI companies\". You could in principle be a moderate and work to impose extremely moderate regulations, or push for minor changes to the behavior of governments.\n\nNote that I think something like this describes a lot of people working in AI risk policy, and therefore seems like more than a theoretical possibility.",
      "plaintextDescription": "> In this post, I mostly conflated \"being a moderate\" with \"working with people at AI companies\". You could in principle be a moderate and work to impose extremely moderate regulations, or push for minor changes to the behavior of governments.\n\nNote that I think something like this describes a lot of people working in AI risk policy, and therefore seems like more than a theoretical possibility."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-20T20:56:29.204Z",
    "af": false
  },
  {
    "_id": "xuqs26dniD4CnT6nL",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "sNPkkFGZmhyiydZoo",
    "topLevelCommentId": "sNPkkFGZmhyiydZoo",
    "contents": {
      "markdown": "Interestingly I think some of the points still apply in that setting: people in companies care about technical details so to be persuasive you will have to be familiar with them, and in general the points about the benefits of an informed audience seem like they're going to apply.\n\nMy guess is there are two axes here:\n- technocratic vs democratic approaches (where technocratic approaches mean you're talking to people informed about details, while democratic approaches mean your audience is less informed about details but maybe has a better sense of wider impacts)\n- large-scale vs small-scale bids (where large-scale bids are maybe more likely to require ideologically diverse coalitions, while small-scale bids have less of an aggregate impact)",
      "plaintextDescription": "Interestingly I think some of the points still apply in that setting: people in companies care about technical details so to be persuasive you will have to be familiar with them, and in general the points about the benefits of an informed audience seem like they're going to apply.\n\nMy guess is there are two axes here:\n\n * technocratic vs democratic approaches (where technocratic approaches mean you're talking to people informed about details, while democratic approaches mean your audience is less informed about details but maybe has a better sense of wider impacts)\n * large-scale vs small-scale bids (where large-scale bids are maybe more likely to require ideologically diverse coalitions, while small-scale bids have less of an aggregate impact)"
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-20T20:54:33.875Z",
    "af": false
  },
  {
    "_id": "sJnZPPXDgSAtCsiAM",
    "postId": "n6nsPzJWurKWKk2pA",
    "parentCommentId": "muQXiqadiAKXAgfcy",
    "topLevelCommentId": "muQXiqadiAKXAgfcy",
    "contents": {
      "markdown": "That said I expect to find this very useful for my current reading, which is largely slowly making my way thru big chunks of Latin, a language that is foreign enough to me that I have to think about where I'm up to.",
      "plaintextDescription": "That said I expect to find this very useful for my current reading, which is largely slowly making my way thru big chunks of Latin, a language that is foreign enough to me that I have to think about where I'm up to."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-14T21:42:27.505Z",
    "af": false
  },
  {
    "_id": "muQXiqadiAKXAgfcy",
    "postId": "n6nsPzJWurKWKk2pA",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "One thing I like to do is use postcards as bookmarks. They often have nice art and remind me of cool things.",
      "plaintextDescription": "One thing I like to do is use postcards as bookmarks. They often have nice art and remind me of cool things."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-14T21:36:51.932Z",
    "af": false
  },
  {
    "_id": "oDGhkmLfieWYJDwur",
    "postId": "knxdLtYbPpsd73ZE4",
    "parentCommentId": "nQHrWh6nt2MA3Pmbx",
    "topLevelCommentId": "EbzhMptt3JrZiEfBg",
    "contents": {
      "markdown": "fun fact: more people die of heat in Europe per year than Americans who die of guns.",
      "plaintextDescription": "fun fact: more people die of heat in Europe per year than Americans who die of guns."
    },
    "baseScore": 21,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2025-07-07T00:23:59.909Z",
    "af": false
  },
  {
    "_id": "xbnBbdcysMgJJyeyA",
    "postId": "ZmfxgvtJgcfNCeHwN",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> I think he's using sloppy language.\n> \n> Bengio et al. mix up \"the policy/AI/agent is trained with RL and gets a high (maximal?) score on the training distribution\" and \"the policy/AI/agent is trained such that it wants to maximize reward (or some correlates) even outside of training\".\n\nWait, does the friend elsewhere add \"... and the author is right\" or \"and sloppiness isn't that bad\"? My read of the quote you've provided is a critique and isn't excusing the sloppiness.",
      "plaintextDescription": "> I think he's using sloppy language.\n> \n> Bengio et al. mix up \"the policy/AI/agent is trained with RL and gets a high (maximal?) score on the training distribution\" and \"the policy/AI/agent is trained such that it wants to maximize reward (or some correlates) even outside of training\".\n\nWait, does the friend elsewhere add \"... and the author is right\" or \"and sloppiness isn't that bad\"? My read of the quote you've provided is a critique and isn't excusing the sloppiness."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-01T20:32:35.410Z",
    "af": false
  },
  {
    "_id": "F4WwJoXmgBtHz8Lyz",
    "postId": "5uw26uDdFbFQgKzih",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Ironically, given that it's currently June 11th (two days after my last tweet was posted) my final tweet provides *two* examples of the planning fallacy.\n\n\"Hopefully\" is not a prediction!",
      "plaintextDescription": "> Ironically, given that it's currently June 11th (two days after my last tweet was posted) my final tweet provides two examples of the planning fallacy.\n\n\"Hopefully\" is not a prediction!"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-15T02:15:43.662Z",
    "af": true
  },
  {
    "_id": "H9mNGAnKPtApLToXa",
    "postId": "X2i9dQQK3gETCyqh2",
    "parentCommentId": "ev5sp7jw9cFKyhHL2",
    "topLevelCommentId": "9teKwETHvT2c9Djpw",
    "contents": {
      "markdown": "Sort of? Indeed this is more accessible to smaller groups than training big models, but small groups don't have access to the biggest models, and you can still do a bunch of non-mechinterpy things with the models you do have, so the effect isn't super overwhelming.",
      "plaintextDescription": "Sort of? Indeed this is more accessible to smaller groups than training big models, but small groups don't have access to the biggest models, and you can still do a bunch of non-mechinterpy things with the models you do have, so the effect isn't super overwhelming."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-10T03:55:27.388Z",
    "af": false
  },
  {
    "_id": "GJkeo45AfQhacht5g",
    "postId": "tv6KfHitijSyKCr6v",
    "parentCommentId": "PJxCbbkbswpgDw6fM",
    "topLevelCommentId": "MHbHkkKdJq3GJh6ou",
    "contents": {
      "markdown": "(In case this isn't a joke, Mars Hill church was named after Mars Hill / the Areopagus / Hill of Ares, which in the New Testament is where the apostle Paul gives a speech to a bunch of pagans about Jesus. That hill is named after the Greek god. The church was located on Earth, in particular in Seattle.)",
      "plaintextDescription": "(In case this isn't a joke, Mars Hill church was named after Mars Hill / the Areopagus / Hill of Ares, which in the New Testament is where the apostle Paul gives a speech to a bunch of pagans about Jesus. That hill is named after the Greek god. The church was located on Earth, in particular in Seattle.)"
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-05-20T23:12:27.920Z",
    "af": false
  },
  {
    "_id": "qnuMwjPbXDM8XFyT3",
    "postId": "tz43dmLAchxcqnDRA",
    "parentCommentId": "DBvuLjo62CCziZ7ku",
    "topLevelCommentId": "fDksvb92jZjLdAdBk",
    "contents": {
      "markdown": "Note that this post does not encourage people to withhold being politically active, or to totally refrain from making political donations.",
      "plaintextDescription": "Note that this post does not encourage people to withhold being politically active, or to totally refrain from making political donations."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-20T23:08:25.197Z",
    "af": false
  },
  {
    "_id": "LyFskSY4wY9AX5JoD",
    "postId": "tz43dmLAchxcqnDRA",
    "parentCommentId": "GqaFcNtFBWvhASzvi",
    "topLevelCommentId": "GqaFcNtFBWvhASzvi",
    "contents": {
      "markdown": "I don't know. I would imagine it's more like \"it's bad to donate to the 'wrong' party\" than \"it's good to donate to the 'right' party\".",
      "plaintextDescription": "I don't know. I would imagine it's more like \"it's bad to donate to the 'wrong' party\" than \"it's good to donate to the 'right' party\"."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-05-13T16:30:20.501Z",
    "af": false
  },
  {
    "_id": "FAow9Kvi9tvEedcp6",
    "postId": "LvswJts75fnpdjAEj",
    "parentCommentId": "8mrKoyJmjRwuvxBKo",
    "topLevelCommentId": "8mrKoyJmjRwuvxBKo",
    "contents": {
      "markdown": "In [this comment](https://www.lesswrong.com/posts/LvswJts75fnpdjAEj/mats-mentor-selection?commentId=oyJjteTrczpKrKCz6) we list the names of some of our advisors.",
      "plaintextDescription": "In this comment we list the names of some of our advisors."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-01-28T02:09:01.502Z",
    "af": false
  },
  {
    "_id": "M6qAbgeYDuAZGAite",
    "postId": "LvswJts75fnpdjAEj",
    "parentCommentId": "jWqGeXsLJWfZNYfDY",
    "topLevelCommentId": "jWqGeXsLJWfZNYfDY",
    "contents": {
      "markdown": "In [this comment](https://www.lesswrong.com/posts/LvswJts75fnpdjAEj/mats-mentor-selection?commentId=oyJjteTrczpKrKCz6) we list the names of some of our advisors.",
      "plaintextDescription": "In this comment we list the names of some of our advisors."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-01-28T02:08:50.074Z",
    "af": false
  },
  {
    "_id": "oyJjteTrczpKrKCz6",
    "postId": "LvswJts75fnpdjAEj",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Below is a list of some of the advisors we used for mentor selection. Notes:\n - Two advisors asked not to be named and do not appear here.\n - Advisors by and large focussed their efforts on areas they had some expertise in.\n - Advisors had to flag conflicts of interest, meaning that (for example) we did not take their ratings of themselves into account.\n\nWith that out of the way, here are some advisors who helped us for the Winter 2024-25 cohort:\n - Adam Gleave\n - Alex Lawsen\n - Buck Shlegeris\n - Ethan Perez\n - Lawrence Chan\n - Lee Sharkey\n - Lewis Hammond\n - Marius Hobbhahn\n - Michael Aird\n - Neel Nanda\n\nThe above people also advised us for the Summer 2025 cohort. We also added the below advisors for that cohort:\n - Alexander Gietelink Oldenziel\n - Ben Garfinkel\n - Caspar Oesterheld\n - Jesse Clifton\n - Nate Thomas",
      "plaintextDescription": "Below is a list of some of the advisors we used for mentor selection. Notes:\n\n * Two advisors asked not to be named and do not appear here.\n * Advisors by and large focussed their efforts on areas they had some expertise in.\n * Advisors had to flag conflicts of interest, meaning that (for example) we did not take their ratings of themselves into account.\n\nWith that out of the way, here are some advisors who helped us for the Winter 2024-25 cohort:\n\n * Adam Gleave\n * Alex Lawsen\n * Buck Shlegeris\n * Ethan Perez\n * Lawrence Chan\n * Lee Sharkey\n * Lewis Hammond\n * Marius Hobbhahn\n * Michael Aird\n * Neel Nanda\n\nThe above people also advised us for the Summer 2025 cohort. We also added the below advisors for that cohort:\n\n * Alexander Gietelink Oldenziel\n * Ben Garfinkel\n * Caspar Oesterheld\n * Jesse Clifton\n * Nate Thomas"
    },
    "baseScore": 20,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-01-28T02:07:28.472Z",
    "af": false
  },
  {
    "_id": "A2KhguxsnADkFLjAb",
    "postId": "T5RzkFcNpRdckGauu",
    "parentCommentId": "RBC8JS7ywBffvqJWu",
    "topLevelCommentId": "NvvK7anGFfiWndfbe",
    "contents": {
      "markdown": "OK, my current evidence is that [Jessica Taylor says on Twitter](https://x.com/jessi_cata/status/1882934881245909221) that she knew Ophelia, Ophelia was a Ziz fan, and Ophelia told Jessica that Ophelia was in contact with Somni and Emma prior to the landlord incident.",
      "plaintextDescription": "OK, my current evidence is that Jessica Taylor says on Twitter that she knew Ophelia, Ophelia was a Ziz fan, and Ophelia told Jessica that Ophelia was in contact with Somni and Emma prior to the landlord incident."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-01-25T17:29:11.492Z",
    "af": false
  },
  {
    "_id": "RBC8JS7ywBffvqJWu",
    "postId": "T5RzkFcNpRdckGauu",
    "parentCommentId": "utoY4pMw5FLtHpFvh",
    "topLevelCommentId": "NvvK7anGFfiWndfbe",
    "contents": {
      "markdown": "FWIW I feel like Ophelia's Zizian credentials haven't been that well-established.",
      "plaintextDescription": "FWIW I feel like Ophelia's Zizian credentials haven't been that well-established."
    },
    "baseScore": 5,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-01-24T18:49:33.840Z",
    "af": false
  },
  {
    "_id": "i4xfwaoyrsp9XGQyw",
    "postId": "cuf4oMFHEQNKMXRvr",
    "parentCommentId": "8C9BuzScNfMKbCvd8",
    "topLevelCommentId": "6igMueKbJ6uF4pC27",
    "contents": {
      "markdown": "Clicking on the word \"Here\" in the post works.",
      "plaintextDescription": "Clicking on the word \"Here\" in the post works."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-01-21T17:17:35.036Z",
    "af": false
  },
  {
    "_id": "6igMueKbJ6uF4pC27",
    "postId": "cuf4oMFHEQNKMXRvr",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "The link to the website is still broken.",
      "plaintextDescription": "The link to the website is still broken."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-01-20T18:20:13.637Z",
    "af": false
  },
  {
    "_id": "HCDo94ciBxdGPhwyN",
    "postId": "j2W3zs7KTZXt2Wzah",
    "parentCommentId": "mmHutbJBbntLisL6G",
    "topLevelCommentId": "X7iBYqQzvEgsppcTb",
    "contents": {
      "markdown": "Looking back on this thread, I'm so confused why this comment was highly upvoted. Isn't it kind of obvious that it's reasonable to focus on Eliezer / MIRI because they are super influential (or at least were, with their influence continuing to wane over time)? I think this holds even if there are other problems with shard theory one could address, and even if TurnTrout's complaints about Eliezer/MIRI don't hold water.",
      "plaintextDescription": "Looking back on this thread, I'm so confused why this comment was highly upvoted. Isn't it kind of obvious that it's reasonable to focus on Eliezer / MIRI because they are super influential (or at least were, with their influence continuing to wane over time)? I think this holds even if there are other problems with shard theory one could address, and even if TurnTrout's complaints about Eliezer/MIRI don't hold water."
    },
    "baseScore": 12,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-01-17T00:49:49.096Z",
    "af": false
  },
  {
    "_id": "x7MhmdFMuPddK2JEx",
    "postId": "hgvZn9vqfmPhoe7de",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Would being in a room with people who are vaping have the same benefits as the fog machine? Obviously it has downsides of smell and other additives, but still - I think this should predict that people maybe don't get airborne illnesses at vaping conventions.",
      "plaintextDescription": "Would being in a room with people who are vaping have the same benefits as the fog machine? Obviously it has downsides of smell and other additives, but still - I think this should predict that people maybe don't get airborne illnesses at vaping conventions."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-12-08T17:52:40.594Z",
    "af": false
  },
  {
    "_id": "cWzEaQXSjtiABCT5e",
    "postId": "ekzHLAQGcrMouFYDJ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Typo:\n> We sequencing a typical sample to between one and two billion reads. \n\nShould maybe be \"We will be sequencing...\"?",
      "plaintextDescription": "Typo:\n\n> We sequencing a typical sample to between one and two billion reads.\n\nShould maybe be \"We will be sequencing...\"?"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-12-05T21:51:12.979Z",
    "af": false
  },
  {
    "_id": "F6NnvE7DsWYEbczpJ",
    "postId": "pudQtkre7f9GLmb2b",
    "parentCommentId": "uEfBrgxufKAR9KJhG",
    "topLevelCommentId": "wJyCc8WfLakf6vb6Z",
    "contents": {
      "markdown": "Wait I'm a moron and the thing I checked was actually whether it was an exponential function, sorry.",
      "plaintextDescription": "Wait I'm a moron and the thing I checked was actually whether it was an exponential function, sorry."
    },
    "baseScore": 2,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-12-05T00:42:20.070Z",
    "af": true
  },
  {
    "_id": "wJyCc8WfLakf6vb6Z",
    "postId": "pudQtkre7f9GLmb2b",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Votes cost quadratic points – a vote strength of \"1\" costs 1 point. A vote of strength 4 costs 10 points. A vote of strength 9 costs 45.\n\nFYI this is not a quadratic function.",
      "plaintextDescription": "> Votes cost quadratic points – a vote strength of \"1\" costs 1 point. A vote of strength 4 costs 10 points. A vote of strength 9 costs 45.\n\nFYI this is not a quadratic function."
    },
    "baseScore": 0,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-12-05T00:35:00.177Z",
    "af": true
  },
  {
    "_id": "DmADre7Hw62RfK78Q",
    "postId": "XmbmHRYPMWHTYdaHW",
    "parentCommentId": "sBXbdRgDaYbC7QCvd",
    "topLevelCommentId": "sBXbdRgDaYbC7QCvd",
    "contents": {
      "markdown": "> Dojo Organizations What organizations are you aware of that are providing some kind of rationality dojo format (courses focused on improving the skill of rationality)?\n\nSeems like the stuff after \"Dojo Organizations\" should be on a new line.",
      "plaintextDescription": "> Dojo Organizations What organizations are you aware of that are providing some kind of rationality dojo format (courses focused on improving the skill of rationality)?\n\nSeems like the stuff after \"Dojo Organizations\" should be on a new line."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-12-03T01:51:52.169Z",
    "af": false
  },
  {
    "_id": "sBXbdRgDaYbC7QCvd",
    "postId": "XmbmHRYPMWHTYdaHW",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> About how often do you use LLMs like ChatGPT while active?\n\nWhat does \"while active\" mean in this question?",
      "plaintextDescription": "> About how often do you use LLMs like ChatGPT while active?\n\nWhat does \"while active\" mean in this question?"
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-12-03T01:49:34.390Z",
    "af": false
  },
  {
    "_id": "iAQqtiRbzMNxT6QuS",
    "postId": "9n87is5QsCozxr9fp",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> If one wants to investigate [the Alignment of Complex Systems research group] further, he has an AXRP podcast episode, which I haven’t listened to.\n\nNote that if you want to investigate further but would rather read a transcript than watch a video, [AXRP has you covered](https://www.lesswrong.com/posts/fDiXMnrHc5NLstq5B/axrp-episode-32-understanding-agency-with-jan-kulveit).",
      "plaintextDescription": "> If one wants to investigate [the Alignment of Complex Systems research group] further, he has an AXRP podcast episode, which I haven’t listened to.\n\nNote that if you want to investigate further but would rather read a transcript than watch a video, AXRP has you covered."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-11-29T18:04:26.700Z",
    "af": false
  },
  {
    "_id": "R9ggL32rDCYs5B5oj",
    "postId": "bvYGDbD8ixCqyeCDx",
    "parentCommentId": "qy9r74virrMoyFJqM",
    "topLevelCommentId": "Ar8enHATbo7v6EnWh",
    "contents": {
      "markdown": "Yeah but a bunch of people might actually answer how their neigbours will vote, given that that's what the pollster asked - and if the question is phrased as the post assumes, that's going to be a massive issue.",
      "plaintextDescription": "Yeah but a bunch of people might actually answer how their neigbours will vote, given that that's what the pollster asked - and if the question is phrased as the post assumes, that's going to be a massive issue."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-11-17T22:19:58.763Z",
    "af": false
  },
  {
    "_id": "xqsihgfWzznHFchdL",
    "postId": "bvYGDbD8ixCqyeCDx",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> So I guess 1.5% of Americans have worse judgment than I expected (by my lights, as someone who thinks that Trump is really bad). Those 1.5% were incredibly important for the outcome of the election and for the future of the country, but they are only 1.5% of the population.\n\nNitpick: they are 1.5% of the voting population, making them around 0.7% of the US population.",
      "plaintextDescription": "> So I guess 1.5% of Americans have worse judgment than I expected (by my lights, as someone who thinks that Trump is really bad). Those 1.5% were incredibly important for the outcome of the election and for the future of the country, but they are only 1.5% of the population.\n\nNitpick: they are 1.5% of the voting population, making them around 0.7% of the US population."
    },
    "baseScore": 9,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2024-11-14T19:11:28.078Z",
    "af": false
  },
  {
    "_id": "Ar8enHATbo7v6EnWh",
    "postId": "bvYGDbD8ixCqyeCDx",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> If you ask people who they're voting for, 50% will say they're voting for Harris. But if you ask them who *most of their neighbors* are voting for, only 25% will say Harris and 75% will say Trump!\n\nNote this issue could be fixed if you instead ask people who the neighbour immediately to the right of their house/apartment will vote for, which I think is compatible with what we know about this poll. That said, the critique of \"do people actually know\" stands.",
      "plaintextDescription": "> If you ask people who they're voting for, 50% will say they're voting for Harris. But if you ask them who most of their neighbors are voting for, only 25% will say Harris and 75% will say Trump!\n\nNote this issue could be fixed if you instead ask people who the neighbour immediately to the right of their house/apartment will vote for, which I think is compatible with what we know about this poll. That said, the critique of \"do people actually know\" stands."
    },
    "baseScore": 16,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2024-11-14T19:09:02.721Z",
    "af": false
  },
  {
    "_id": "D89oLbHMt8d2Avk9s",
    "postId": "bvYGDbD8ixCqyeCDx",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> she should have [picked Josh Shapiro](https://thehill.com/homenews/campaign/4869398-harris-shapiro-walz-trump-nate-silver/) as her running mate\n\nNote that [this news story](https://6abc.com/post/josh-shapiro-had-reservations-leaving-job-pennsylvania-governor-after-meeting-harris-abc-news-sources-say/15153613/) makes allegations that, if true, make it sound like the decision was partly Shapiro's:\n\n> Following Harris's interview with Pennsylvania Governor [Josh Shapiro](https://6abc.com/tag/josh-shapiro/), there was a sense among Shapiro's team that the meeting did not go as well as it could have, sources familiar with the matter tell ABC News.\n> \n> Later Sunday, after the interview, Shapiro placed a phone call to Harris' team, indicating he had reservations about leaving his job as governor, sources said.",
      "plaintextDescription": "> she should have picked Josh Shapiro as her running mate\n\nNote that this news story makes allegations that, if true, make it sound like the decision was partly Shapiro's:\n\n> Following Harris's interview with Pennsylvania Governor Josh Shapiro, there was a sense among Shapiro's team that the meeting did not go as well as it could have, sources familiar with the matter tell ABC News.\n> \n> Later Sunday, after the interview, Shapiro placed a phone call to Harris' team, indicating he had reservations about leaving his job as governor, sources said."
    },
    "baseScore": 7,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-11-14T19:02:40.265Z",
    "af": false
  },
  {
    "_id": "bGx9pwgJf2XM3sFay",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "JRZ5hxBSSN6EHC5wc",
    "topLevelCommentId": "A4zqo2EFC4PjQK8HH",
    "contents": {
      "markdown": "Oh except: I did not necessarily mean to claim that any of the things I mentioned were missing from the alignment research scene, or that they were present.",
      "plaintextDescription": "Oh except: I did not necessarily mean to claim that any of the things I mentioned were missing from the alignment research scene, or that they were present."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-11-14T18:04:05.774Z",
    "af": false
  },
  {
    "_id": "w2s3pBpF86xihjdMP",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "hxrQE4xEiHbbwYsBd",
    "topLevelCommentId": "A4zqo2EFC4PjQK8HH",
    "contents": {
      "markdown": "When I wrote that, I wasn't thinking so much about evals / model organisms as stuff like:\n- [putting](https://arxiv.org/abs/1707.01068) a bunch of [agents](https://dspace.mit.edu/handle/1721.1/156591) in a [simulated](https://arxiv.org/abs/2404.16698) world and seeing how they interact\n- [weak-to-strong](https://arxiv.org/abs/2312.09390) / [easy-to-hard](https://arxiv.org/abs/2401.06751) generalization\n\nbasically stuff along the lines of \"when you put agents in X situation, they tend to do Y thing\", rather than trying to understand latent causes / capabilities",
      "plaintextDescription": "When I wrote that, I wasn't thinking so much about evals / model organisms as stuff like:\n\n * putting a bunch of agents in a simulated world and seeing how they interact\n * weak-to-strong / easy-to-hard generalization\n\nbasically stuff along the lines of \"when you put agents in X situation, they tend to do Y thing\", rather than trying to understand latent causes / capabilities"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-11-14T17:40:44.124Z",
    "af": true
  },
  {
    "_id": "ytwPeKx7Gddk9BZFi",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "JRZ5hxBSSN6EHC5wc",
    "topLevelCommentId": "A4zqo2EFC4PjQK8HH",
    "contents": {
      "markdown": "Yeah, that seems right to me.",
      "plaintextDescription": "Yeah, that seems right to me."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-11-14T06:56:53.422Z",
    "af": false
  },
  {
    "_id": "A4zqo2EFC4PjQK8HH",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "**A theory of how alignment research should work**\n\n(cross-posted from [danielfilan.com](https://danielfilan.com/2024/11/13/a-theory-of-how-alignment-research-should-work.html))\n\nEpistemic status:\n- I listened to [the Dwarkesh episode with Gwern](https://www.dwarkeshpatel.com/p/gwern-branwen) and started attempting to think about life, the universe, and everything \n- less than an hour of thought has gone into this post\n- that said, it comes from a background of me [thinking](https://www.lesswrong.com/posts/WgMhovN7Gs6Jpn3PH/danielfilan-s-shortform-feed?commentId=RzdD4JiewyyHeuYBb) for a while about how the field of AI alignment should relate to agent foundations research\n\nMaybe obvious to everyone but me, or totally wrong (this doesn't really grapple with the challenges of working in a domain where an intelligent being might be working against you), but:\n- we currently don't know how to make super-smart computers that do our will\n  - this is not just a problem of having a design that is not feasible to implement: we do not even have a sense of what the design would be\n  - I'm trying to somewhat abstract over intent alignment vs control approaches, but am mostly thinking about intent alignment\n  - I have not thought that much about societal/systemic risks very much, and this post doesn't really address them.\n- ideally we would figure out how to do this\n- the closest traction that we have: deep learning seems to work well in practice, altho our theoretical knowledge of why it works so well or how capabilities are implemented is lagging\n- how should we proceed? Well:\n  - thinking about theory alone has not been practical\n  - probably we need to look at things that exhibit alignment-related phenomena and understand them, and that will help us develop the requisite theory\n    - said things are probably neural networks\n  - there are two ways we can look at neural networks: their behaviour, and their implementation.\n  - looking at behaviour is conceptually straightforward, and valuable, and being done\n  - looking at their implementation is less obvious\n  - what we need is tooling that lets us see relevant things about how neural networks are working\n  - such tools (e.g. SAEs) are not impossible to create, but it is not obvious that their outputs tell us quantities that are actually of interest\n  - in order to discipline the creation of such tools, we should demand that they help us understand models in ways that matter\n    - see Stephen Casper's [engineer's interpretability sequence](https://www.alignmentforum.org/s/a6ne2ve5uturEEQK7), [Jason Gross on compact proofs](https://arxiv.org/abs/2406.11779)\n  - once we get such tools, we should be trying to use them to understand alignment-relevant phenomena, to build up our theory of what we want out of alignment and how it might be implemented\n    - this is also a thing that looking at the external behaviour of models in alignment-relevant contexts should be doing\n- so should we be just doing totally empirical things? No.\n  - firstly, we need to be disciplined along the way by making sure that we are looking at settings that are in fact relevant to the alignment problem, when we do our behavioural analysis and benchmark our interpretability tools. This involves having a model of what situations are in fact alignment-relevant, what problems we will face as models get smarter, etc\n  - secondly, once we have the building blocks for theory, ideally we will put them together and make some actual theorems like \"in such-and-such situations models will never become deceptive\" (where 'deceptive' has been satisfactorily operationalized in a way that suffices to derive good outcomes from no deception and relatively benign humans)\n- I'm imagining the above as being analogous to an imagined history of statistical mechanics (people who know this history or who have read [\"inventing temperature\"](https://global.oup.com/academic/product/inventing-temperature-9780195337389) should let me know if I'm totally wrong about it):\n  - first we have steam engines etc\n  - then we figure out that 'temperature' and 'entropy' are relevant things to track for making the engines run\n  - then we relate temperature, entropy, and pressure\n  - then we get a good theory of thermodynamics \n  - then we develop statistical mechanics\n- exceptions to \"theory without empiricism doesn't work\":\n  - thinking about [deceptive mesa-optimization](https://arxiv.org/abs/1906.01820)\n  - [RLHF failures](https://www.lesswrong.com/posts/DS3TTpCEFKduC8zPy/paper-blogpost-when-your-ais-deceive-you-challenges-with)\n  - [CIRL](https://arxiv.org/abs/1606.03137) analysis\n- lesson of above: theory does seem to help us analyze some issues and raise possibilities ",
      "plaintextDescription": "A theory of how alignment research should work\n\n(cross-posted from danielfilan.com)\n\nEpistemic status:\n\n * I listened to the Dwarkesh episode with Gwern and started attempting to think about life, the universe, and everything\n * less than an hour of thought has gone into this post\n * that said, it comes from a background of me thinking for a while about how the field of AI alignment should relate to agent foundations research\n\nMaybe obvious to everyone but me, or totally wrong (this doesn't really grapple with the challenges of working in a domain where an intelligent being might be working against you), but:\n\n * we currently don't know how to make super-smart computers that do our will\n   * this is not just a problem of having a design that is not feasible to implement: we do not even have a sense of what the design would be\n   * I'm trying to somewhat abstract over intent alignment vs control approaches, but am mostly thinking about intent alignment\n   * I have not thought that much about societal/systemic risks very much, and this post doesn't really address them.\n * ideally we would figure out how to do this\n * the closest traction that we have: deep learning seems to work well in practice, altho our theoretical knowledge of why it works so well or how capabilities are implemented is lagging\n * how should we proceed? Well:\n   * thinking about theory alone has not been practical\n   * probably we need to look at things that exhibit alignment-related phenomena and understand them, and that will help us develop the requisite theory\n     * said things are probably neural networks\n   * there are two ways we can look at neural networks: their behaviour, and their implementation.\n   * looking at behaviour is conceptually straightforward, and valuable, and being done\n   * looking at their implementation is less obvious\n   * what we need is tooling that lets us see relevant things about how neural networks are working\n   * such tools (e.g. SAEs) are not impossible to crea"
    },
    "baseScore": 32,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2024-11-14T04:07:04.103Z",
    "af": true
  },
  {
    "_id": "cfRxhAdo5AoRDzp2f",
    "postId": "XHtygebvHoJSSeNPP",
    "parentCommentId": "K7fghFZ6o2MTRojLu",
    "topLevelCommentId": "K7fghFZ6o2MTRojLu",
    "contents": {
      "markdown": "A way I'd phrase [John's sibling comment](https://www.lesswrong.com/posts/XHtygebvHoJSSeNPP/some-rules-for-an-algebra-of-bayes-nets?commentId=EkLrdrXfJufQe7dq2), at least for the exact case: adding arrows to a DAG increases the set of probability distributions it can represent. This is because the fundamental rule of a Bayes net is that d-separation has to imply conditional independence - but you can have conditional independences in a distribution that aren't represented by a network. When you add arrows, you can remove instances of d-separation, but you can't add any (because nodes are d-separated when all paths between them satisfy some property, and (a) adding arrows can only increase the number of paths you have to worry about and (b) if you look at [the definition of d-separation](https://en.wikipedia.org/wiki/Bayesian_network#d-separation) the relevant properties for paths get harder to satisfy when you have more arrows). Therefore, the more arrows a graph G has, the fewer constraints distribution P has to satisfy for P to be represented by G.",
      "plaintextDescription": "A way I'd phrase John's sibling comment, at least for the exact case: adding arrows to a DAG increases the set of probability distributions it can represent. This is because the fundamental rule of a Bayes net is that d-separation has to imply conditional independence - but you can have conditional independences in a distribution that aren't represented by a network. When you add arrows, you can remove instances of d-separation, but you can't add any (because nodes are d-separated when all paths between them satisfy some property, and (a) adding arrows can only increase the number of paths you have to worry about and (b) if you look at the definition of d-separation the relevant properties for paths get harder to satisfy when you have more arrows). Therefore, the more arrows a graph G has, the fewer constraints distribution P has to satisfy for P to be represented by G."
    },
    "baseScore": 8,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-11-08T00:12:54.610Z",
    "af": false
  },
  {
    "_id": "uSCTRcfwhsJFHWE9B",
    "postId": "sZvMLWrWomN28uWA9",
    "parentCommentId": "8Ph2AwkYYLkGXfZHx",
    "topLevelCommentId": "kC8PZGNm7kRE6gP9j",
    "contents": {
      "markdown": "I enjoyed reading [Nicholas Carlini](https://nicholas.carlini.com/writing/2024/how-i-use-ai.html) and [Jeff Kaufman](https://www.jefftk.com/p/examples-of-how-i-use-llms) write about how they use them, if you're looking for inspiration.",
      "plaintextDescription": "I enjoyed reading Nicholas Carlini and Jeff Kaufman write about how they use them, if you're looking for inspiration."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-11-02T20:01:22.299Z",
    "af": false
  },
  {
    "_id": "K4Hnz8GddpoKMvtsN",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "ZDQrmRjtt85qoQexL",
    "topLevelCommentId": "ZDQrmRjtt85qoQexL",
    "contents": {
      "markdown": "Another way of maintaining Sola Scriptura and Perspicuity in the face of Protestant disagreement about essential doctrines is the possibility that all of this is cleared up in the deuterocanonical books that Catholics believe are scripture but Protestants do not. That said, this will still rule out Protestantism, and it's not clear that the deuterocanon in fact clears everything up.",
      "plaintextDescription": "Another way of maintaining Sola Scriptura and Perspicuity in the face of Protestant disagreement about essential doctrines is the possibility that all of this is cleared up in the deuterocanonical books that Catholics believe are scripture but Protestants do not. That said, this will still rule out Protestantism, and it's not clear that the deuterocanon in fact clears everything up."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-11-02T03:16:46.940Z",
    "af": false
  },
  {
    "_id": "ZDQrmRjtt85qoQexL",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "**A failure of an argument against sola scriptura** (cross-posted from [Superstimulus](https://superstimul.us/display/19e88760-7167-2597-fdd0-c5d974638185))\n\nRecently, Catholic apologist Joe Heschmeyer has produced [a couple](https://www.youtube.com/watch?v=5_SGbUDFQWg) [of videos](url=https://www.youtube.com/watch?v=GlSkKi3chQA) arguing against the Protestant view of the Bible - specifically, the claims of Sola Scriptura and Perspicuity (capitalized because I'll want to refer to them as premises later). \"Sola Scriptura\" has been operationalized a few different ways, but one way that most Protestants would agree on is (taken from the Westminster confession):\n> The whole counsel of God, concerning all things necessary for [...] man’s salvation [...] is either expressly set down in Scripture, or by good and necessary consequence may be deduced from Scripture\n\n\"Perspicuity\" means clarity, and is propounded in the Westminster confession like this:\n> [T]hose things which are necessary to be known, believed, and observed, for salvation, are so clearly propounded and opened in some place of Scripture or other, that not only the learned, but the unlearned, in a due use of the ordinary means, may attain unto a sufficient understanding of them.\n\nSo, in other words, Protestants think that everything you need to know to be saved is in the Bible, and is expressed so obviously that anyone who reads it and thinks about it in a reasonable way will understand it.\n\nI take Heschmeyer's argument to be that if Sola Scriptura and Perspicuity were true, then all reasonable people who have read the Bible and believe it would agree on which doctrines were necessary for salvation - in other words, you wouldn't have a situation where one person thinks P and P is necessary for salvation, while another thinks not-P, or a third thinks that P is not necessary for salvation. But in fact this situation happens a lot, even among seemingly sincere followers of the Bible who believe in Sola Scriptura and Perspicuity. Therefore Sola Scriptura and Perspecuity are false. (For the rest of this post, I'll write Nec(P) for the claim \"P is necessary for salvation\" to save space.)\n\nI think this argument doesn't quite work. Here's why:\n\nIt can be the case that the Bible clearly explains everything that you need to believe, but it doesn't clearly explain which things you need to believe. In other words, Sola Scriptura and Perspicuity say that for all P such that Nec(P), the Bible teaches P clearly - but they don't say that for such P, the Bible teaches P clearly, and also clearly teaches Nec(P). Nor do they say that the only things that are taught clearly in the Bible are things you need to believe (otherwise you could figure out which doctrines you had to believe by just noticing what things the Bible clearly teaches).\n\nFor example, suppose that the Bible clearly teaches that Jesus died for at least some people, and that followers of Jesus should get baptized, and in fact, the only thing you need to believe to be saved is that Jesus died for at least some people. In that world, people of good faith could disagree about whether you need to believe that Jesus died for at least some people, and this would be totally consistent with Sola Scriptura and Perspicuity.\n\nFurthermore, suppose that it's not clear to people of good faith whether or not something is clear to people of good faith. Perhaps something could seem clear to you but not be clear to others of good faith, or also something could be clear but others could fail to understand it because they're not actually of good faith (you need this part otherwise you can tell if something's clear by noticing if anyone disagrees with you). Then, you can have one person who believes P and Nec(P), and another who believes not-P and Nec(not-P), and that be consistent with Sola Scriptura and Perspicuity.\n\nFor example, take the example above, and suppose that some people read the Bible as clearly saying that Jesus died for everyone (aka Unlimited Atonement), and others read the Bible as clearly saying that Jesus only died for his followers (aka Limited Atonement). You could have that disagreement, and if the two groups think the others are being disingenuous, they could both think that you have to agree with them to be saved, while still having Sola Scriptura and Perspicuity being true.\n\nThat said, Heschmeyer's argument is still going to limit the kinds of Protestantism you can adopt. In the above example, if we suppose that you can tell that neither group is in fact being disingenuous, then his argument rules out the combination of Sola Scriptura, Perspicuity, and Nec(Limited Atonement) (as well as Sola Scriptura, Perspicuity, and Nec(Unlimited Atonement)). In this way, applied to the real world, it's going to rule out versions of Protestantism that claim that you have to believe a bunch of things that sincere Christians who are knowledgeable about the Bible don't agree on. That said, it won't rule out Protestantisms that are liberal about what you can believe while being saved.",
      "plaintextDescription": "A failure of an argument against sola scriptura (cross-posted from Superstimulus)\n\nRecently, Catholic apologist Joe Heschmeyer has produced a couple of videos arguing against the Protestant view of the Bible - specifically, the claims of Sola Scriptura and Perspicuity (capitalized because I'll want to refer to them as premises later). \"Sola Scriptura\" has been operationalized a few different ways, but one way that most Protestants would agree on is (taken from the Westminster confession):\n\n> The whole counsel of God, concerning all things necessary for [...] man’s salvation [...] is either expressly set down in Scripture, or by good and necessary consequence may be deduced from Scripture\n\n\"Perspicuity\" means clarity, and is propounded in the Westminster confession like this:\n\n> [T]hose things which are necessary to be known, believed, and observed, for salvation, are so clearly propounded and opened in some place of Scripture or other, that not only the learned, but the unlearned, in a due use of the ordinary means, may attain unto a sufficient understanding of them.\n\nSo, in other words, Protestants think that everything you need to know to be saved is in the Bible, and is expressed so obviously that anyone who reads it and thinks about it in a reasonable way will understand it.\n\nI take Heschmeyer's argument to be that if Sola Scriptura and Perspicuity were true, then all reasonable people who have read the Bible and believe it would agree on which doctrines were necessary for salvation - in other words, you wouldn't have a situation where one person thinks P and P is necessary for salvation, while another thinks not-P, or a third thinks that P is not necessary for salvation. But in fact this situation happens a lot, even among seemingly sincere followers of the Bible who believe in Sola Scriptura and Perspicuity. Therefore Sola Scriptura and Perspecuity are false. (For the rest of this post, I'll write Nec(P) for the claim \"P is necessary for salvation\" to save spa"
    },
    "baseScore": -2,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-11-02T03:12:14.387Z",
    "af": false
  },
  {
    "_id": "avXG5skKSQrHoqW9t",
    "postId": "T9DAHucovCqhLqCdq",
    "parentCommentId": "H4TpEcit24JRbwHqn",
    "topLevelCommentId": "D8PG7FKPh5vnRtRa9",
    "contents": {
      "markdown": "Oh I misread it as \"eighty percent of the effort\" oops.",
      "plaintextDescription": "Oh I misread it as \"eighty percent of the effort\" oops."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-11-02T00:45:57.705Z",
    "af": false
  },
  {
    "_id": "mPZuGzqktemiLK54N",
    "postId": "T9DAHucovCqhLqCdq",
    "parentCommentId": "xbbagvpA8Fc8RNvvJ",
    "topLevelCommentId": "4GqQ2QnEqGJLCnpNT",
    "contents": {
      "markdown": "You say \"higher numbers for polyamorous relationships\" which is contrary to \"If you're polyamorous, but happen to have one partner, you would also put 1 for this question.\"",
      "plaintextDescription": "You say \"higher numbers for polyamorous relationships\" which is contrary to \"If you're polyamorous, but happen to have one partner, you would also put 1 for this question.\""
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-11-01T22:11:16.302Z",
    "af": false
  },
  {
    "_id": "vgacCsoZgR86iovAL",
    "postId": "T9DAHucovCqhLqCdq",
    "parentCommentId": "D8PG7FKPh5vnRtRa9",
    "topLevelCommentId": "D8PG7FKPh5vnRtRa9",
    "contents": {
      "markdown": "> If you've been waiting for an excuse to be done, this is probably the point where twenty percent of the effort has gotten eighty percent of the effect.\n\nShould be \"eighty percent of the benefit\" or similar.",
      "plaintextDescription": "> If you've been waiting for an excuse to be done, this is probably the point where twenty percent of the effort has gotten eighty percent of the effect.\n\nShould be \"eighty percent of the benefit\" or similar."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-11-01T21:43:42.918Z",
    "af": false
  },
  {
    "_id": "Ceg7anBLN6e3XtWdz",
    "postId": "T9DAHucovCqhLqCdq",
    "parentCommentId": "D8PG7FKPh5vnRtRa9",
    "topLevelCommentId": "D8PG7FKPh5vnRtRa9",
    "contents": {
      "markdown": "I'd be interested in a Q about whether people voted in the last national election for their country (maybe with an option for \"my country does not hold national elections\") and if so how they voted (if you can find a schema that works for most countries, which I guess is hard).",
      "plaintextDescription": "I'd be interested in a Q about whether people voted in the last national election for their country (maybe with an option for \"my country does not hold national elections\") and if so how they voted (if you can find a schema that works for most countries, which I guess is hard)."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-11-01T21:42:02.243Z",
    "af": false
  },
  {
    "_id": "CG7CavCRexNEGgFwo",
    "postId": "T9DAHucovCqhLqCdq",
    "parentCommentId": "D8PG7FKPh5vnRtRa9",
    "topLevelCommentId": "D8PG7FKPh5vnRtRa9",
    "contents": {
      "markdown": "In the highest degree question, one option is \"Ph D.\". This should be \"PhD\", no spaces, no periods.",
      "plaintextDescription": "In the highest degree question, one option is \"Ph D.\". This should be \"PhD\", no spaces, no periods."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-11-01T21:40:48.159Z",
    "af": false
  },
  {
    "_id": "D8PG7FKPh5vnRtRa9",
    "postId": "T9DAHucovCqhLqCdq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Are you planning on having more children? Answer yes if you don't have children but want some, or if you do have children but want more.\n\nWhether I want to have children and whether I plan to have children are different questions. There are lots of things I want but don't have plans to get, and one sometimes finds oneself with plans to achieve things that one doesn't actually want.",
      "plaintextDescription": "> Are you planning on having more children? Answer yes if you don't have children but want some, or if you do have children but want more.\n\nWhether I want to have children and whether I plan to have children are different questions. There are lots of things I want but don't have plans to get, and one sometimes finds oneself with plans to achieve things that one doesn't actually want."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-11-01T21:39:52.935Z",
    "af": false
  },
  {
    "_id": "EmcD4Bsbv8TPbruif",
    "postId": "EQJfdqSaMcJyR5k73",
    "parentCommentId": "Ao6rHRyEbTQ4nqCwu",
    "topLevelCommentId": "SsyXtAhBtyFHgoSHz",
    "contents": {
      "markdown": "Sure, I'm just surprised it could work without me having Calibri installed.",
      "plaintextDescription": "Sure, I'm just surprised it could work without me having Calibri installed."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-11-01T21:28:22.293Z",
    "af": false
  },
  {
    "_id": "KLWeLuuYZg7A5eLJy",
    "postId": "sZvMLWrWomN28uWA9",
    "parentCommentId": "QCwiSGfiALhjvho3v",
    "topLevelCommentId": "5xKfHRyajrXcW4Cri",
    "contents": {
      "markdown": "Could be a thing where people can opt into getting the vibes or the vibes and the definitions.",
      "plaintextDescription": "Could be a thing where people can opt into getting the vibes or the vibes and the definitions."
    },
    "baseScore": 2,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-11-01T20:28:58.693Z",
    "af": false
  },
  {
    "_id": "kkHwvm9uTFJAxYEpG",
    "postId": "sZvMLWrWomN28uWA9",
    "parentCommentId": "5xKfHRyajrXcW4Cri",
    "topLevelCommentId": "5xKfHRyajrXcW4Cri",
    "contents": {
      "markdown": "Also, my feedback is that some of the definitions seem kind of vague. Like, apparently an ultracontribution is \"a mathematical object representing uncertainty over probability\" - this tells me what it's supposed to be, but doesn't actually tell me what it is. The ones that actually show up in the text don't seem too vague, partially because they're not terms that are super precise.",
      "plaintextDescription": "Also, my feedback is that some of the definitions seem kind of vague. Like, apparently an ultracontribution is \"a mathematical object representing uncertainty over probability\" - this tells me what it's supposed to be, but doesn't actually tell me what it is. The ones that actually show up in the text don't seem too vague, partially because they're not terms that are super precise."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-11-01T20:22:12.087Z",
    "af": false
  },
  {
    "_id": "5xKfHRyajrXcW4Cri",
    "postId": "sZvMLWrWomN28uWA9",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "How are you currently determining which words to highlight? You say \"terms that readers might not know\" but this varies a lot based on the reader (as you mention in the long-term vision section).",
      "plaintextDescription": "How are you currently determining which words to highlight? You say \"terms that readers might not know\" but this varies a lot based on the reader (as you mention in the long-term vision section)."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-11-01T20:19:20.262Z",
    "af": false
  },
  {
    "_id": "H8q2BJbRCgDfppMpa",
    "postId": "sZvMLWrWomN28uWA9",
    "parentCommentId": "hjxrdYgaiysJTrihK",
    "topLevelCommentId": "kC8PZGNm7kRE6gP9j",
    "contents": {
      "markdown": "FWIW I think it's not uncommon for people to not use LLMs daily (e.g. I don't).",
      "plaintextDescription": "FWIW I think it's not uncommon for people to not use LLMs daily (e.g. I don't)."
    },
    "baseScore": 13,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2024-11-01T20:14:57.435Z",
    "af": false
  },
  {
    "_id": "EinMn254P2QxrrDZp",
    "postId": "sZvMLWrWomN28uWA9",
    "parentCommentId": "EeygrKCRT5ZNkBSPJ",
    "topLevelCommentId": "kC8PZGNm7kRE6gP9j",
    "contents": {
      "markdown": "FWIW I think the actual person with responsibility is the author if the author approves it, and you if the author doesn't.",
      "plaintextDescription": "FWIW I think the actual person with responsibility is the author if the author approves it, and you if the author doesn't."
    },
    "baseScore": 11,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2024-11-01T20:13:39.770Z",
    "af": false
  },
  {
    "_id": "Cr6JcdsKYAymiq3Ex",
    "postId": "EQJfdqSaMcJyR5k73",
    "parentCommentId": "DpK5TMmx4uiqgxYTs",
    "topLevelCommentId": "SsyXtAhBtyFHgoSHz",
    "contents": {
      "markdown": "I believe I'm seeing Gill Sans? But when I google \"Calibri\" I see text that looks like it's in Calibri, so that's confusing.",
      "plaintextDescription": "I believe I'm seeing Gill Sans? But when I google \"Calibri\" I see text that looks like it's in Calibri, so that's confusing."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-11-01T20:07:34.400Z",
    "af": false
  },
  {
    "_id": "BEt6JxpGuDjyvbf8R",
    "postId": "HrtyZm2zPBtAmZFEs",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Since people have reported not being able to see the tweet thread, I will reproduce it in this comment (with pictures replaced by my descriptions of them):\n\n\n> If developers had to prove to regulators that powerful AI systems are safe to deploy, what are the best arguments they could use?\n> \n> Our new report tackles the (very big!) question of how to make a ‘safety case’ for AI.\n> \n> [image of the start of the paper]\n> \n> We define a safety case as a rationale developers provide to regulators to show that their AI systems are unlikely to cause a catastrophe.\n> \n> The term ‘safety case’ is not new. In many industries (e.g. aviation), products are ‘put on trial’ before they are released.\n> \n> [cartoon of a trial: regulator is a judge, one side is a developer advancing a safety case, other side is a red team advancing a risk case]\n> \n> We simplify the process of making a safety case by breaking it into six steps.\n> \n> 1. Specify the macrosystem (all AI systems) and the deployment setting.\n> 2. Concretize 'AI systems cause a catastrophe' into specific unacceptable outcomes (e.g. the AI systems build a bioweapon)\n> 3. Justify claims about the deployment setting.\n> 4. Carve up the collection of AI systems into smaller groups (subsystems) that can be analyzed in isolation.\n> 5. Assess risk from subsystems acting unilaterally.\n> 6. Assess risk from subsystems cooperating together.\n>\n> [The above, but in picture format]\n> \n> We first focus on step 5: how would developers argue that individual subsystems are safe? These arguments can be grouped into four categories.\n> \n> Arguments could assert AI systems are\n> - too weak to be dangerous even if open-sourced (inability)\n> - unable to cause disaster due to restrictions (control)\n> - safe despite ablility to cause harm (trustworthiness)\n> - judged to be safe by credible AI advisors (deference)\n> \n> [The above, but in picture format and slightly reworded]\n> \n> Consider the analogy of a robber and a diamond. How might one justify that the robber won’t steal the diamond?\n> \n> [Image with heading: \"Why won't the robber steal the diamond?\".\n> \n> Row 1 says \"**Inability** The robber is an infant. _Analogy: e.g. GPT-3 is clearly too weak to be dangerous_\", next to an image of a baby near a diamond on a pedestal.\n>\n> Row 2 says \"**Control** The robber is in jail. _Analogy: e.g. AI systems are sandboxed and can't escape._\", next to an image of a prisoner in a box with bars, near a diamond on a pedestal.\n>\n> Row 3 says \"**Trustworthiness** The robber does not steal. _Analogy: e.g. AI systems follow developer instructions._\", next to an image of a person sitting down cross-legged and wearing an orange robe, next to a diamond on a pedestal.\n> \n> Row 4 says \"**Deference** The security service CEO says the diamond is secure. _Analogy: AI advisors say that AI systems are safe._\", next to an image of a figure with grey hair saying \"All good\" next to a diamond on a pedestal with lines coming out of it, connecting it to small grey boxes (sensors?).]\n>\n> The ordering of these categories is intentional. As AI systems become more powerful, developers will likely rely mostly on inability, then control, then trustworthiness, and finally, deference to AI advisors.\n> \n> [Image of graph where the horizontal axis is \"Increasingly powerful AI\" and the vertical axis is \"Primary safety argument\". Inability, Control, Trustworthiness, and Deference are shown in order from bottom-left to top-right. An arrow connects the words \"We are here\" to Inability.]\n>\n> Next, we give examples of arguments in each category. Arguments are ranked on three axes:\n> 1. Practicality\n> 2. Strength\n> 3. Scalability\n> \n> No argument received full marks! Research will be needed to justify the safety of advanced AI systems.\n>\n> [A complicated diagram showing a variety of arguments under the Inability, Control, Trustworthiness, and Deference categories, together with ratings for their Practicality, Maximum Strength, and Scalability.]\n> \n> The arguments in the previous step pertain to small groups of AI systems. It would be difficult to directly apply them to large groups. We also explain how to justify that the actions of *many* AI systems won’t cause a catastrophe (step 6 in our framework).\n> \n> [Image titled \"Large-scale AI misbehavior\". Below are 3 rows, with 2 columns. The left column is labelled \"Causes\" and the right is labelled \"Strategies\".\n> \n> Row 1: Cause: Widespread alignment faking. Strategy: Blitzkrieg: overwhelm controls\n> \n> Row 2: Cause: Infectious jailbreaks. Strategy: Strike: disable infrastructure.\n> \n> Row 3: Cause: Rapid memetic value drift. Strategy: Hivemind: combine intelligence.\n> \n> dots are shown below, likely to indicate that there are more causes and strategies not shown.]\n> \n> We are hoping this report will:\n> \n> 1. Motivate research that further clarifies the assumptions behind safety arguments.\n> 2. Inform the design of hard safety standards.\n> \n> More in the paper: https://bit.ly/3IJ5N95\n> Many thanks to my coauthors! @NickGabs01, @DavidSKrueger, and @thlarsen.\n> \n> Might be of interest to @bshlgrs, @RogerGrosse, @DavidDuvenaud, @EvanHub, @aleks_madry, @ancadianadragan, @rohinmshah, @jackclarkSF, @Manderljung, @RichardMCNgo",
      "plaintextDescription": "Since people have reported not being able to see the tweet thread, I will reproduce it in this comment (with pictures replaced by my descriptions of them):\n\n> If developers had to prove to regulators that powerful AI systems are safe to deploy, what are the best arguments they could use?\n> \n> Our new report tackles the (very big!) question of how to make a ‘safety case’ for AI.\n> \n> [image of the start of the paper]\n> \n> We define a safety case as a rationale developers provide to regulators to show that their AI systems are unlikely to cause a catastrophe.\n> \n> The term ‘safety case’ is not new. In many industries (e.g. aviation), products are ‘put on trial’ before they are released.\n> \n> [cartoon of a trial: regulator is a judge, one side is a developer advancing a safety case, other side is a red team advancing a risk case]\n> \n> We simplify the process of making a safety case by breaking it into six steps.\n> \n>  1. Specify the macrosystem (all AI systems) and the deployment setting.\n>  2. Concretize 'AI systems cause a catastrophe' into specific unacceptable outcomes (e.g. the AI systems build a bioweapon)\n>  3. Justify claims about the deployment setting.\n>  4. Carve up the collection of AI systems into smaller groups (subsystems) that can be analyzed in isolation.\n>  5. Assess risk from subsystems acting unilaterally.\n>  6. Assess risk from subsystems cooperating together.\n> \n> [The above, but in picture format]\n> \n> We first focus on step 5: how would developers argue that individual subsystems are safe? These arguments can be grouped into four categories.\n> \n> Arguments could assert AI systems are\n> \n>  * too weak to be dangerous even if open-sourced (inability)\n>  * unable to cause disaster due to restrictions (control)\n>  * safe despite ablility to cause harm (trustworthiness)\n>  * judged to be safe by credible AI advisors (deference)\n> \n> [The above, but in picture format and slightly reworded]\n> \n> Consider the analogy of a robber and a diamond. How might"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-10-31T21:52:19.728Z",
    "af": true
  },
  {
    "_id": "dvTkdWWx3fgbcyqog",
    "postId": "EQJfdqSaMcJyR5k73",
    "parentCommentId": "jEkrCP2Lhb78F4iKG",
    "topLevelCommentId": "SsyXtAhBtyFHgoSHz",
    "contents": {
      "markdown": "Update: I have already gotten over it.",
      "plaintextDescription": "Update: I have already gotten over it."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-31T21:35:47.550Z",
    "af": false
  },
  {
    "_id": "jEkrCP2Lhb78F4iKG",
    "postId": "EQJfdqSaMcJyR5k73",
    "parentCommentId": "oGWHKcqC9J7PjfRTA",
    "topLevelCommentId": "SsyXtAhBtyFHgoSHz",
    "contents": {
      "markdown": "It looks kinda small to me, someone who uses Firefox on Ubuntu.",
      "plaintextDescription": "It looks kinda small to me, someone who uses Firefox on Ubuntu."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-30T00:02:22.326Z",
    "af": false
  },
  {
    "_id": "9u7tG9p5iWzrxuhms",
    "postId": "rhEXTkDmssrHBNrfS",
    "parentCommentId": "WXmFgaowgjvgzDFQt",
    "topLevelCommentId": "WXmFgaowgjvgzDFQt",
    "contents": {
      "markdown": "A thing you are maybe missing is that the discussion groups are now in the past.\n\n> You should be sure to point out that many of the readings are dumb and wrong\n\nThe hope is that the scholars notice this on their own.\n\n> Week 3 title should maybe say “How could we safely train AIs…”? I think there are other training options if you don’t care about safety.\n\nLol nice catch.",
      "plaintextDescription": "A thing you are maybe missing is that the discussion groups are now in the past.\n\n> You should be sure to point out that many of the readings are dumb and wrong\n\nThe hope is that the scholars notice this on their own.\n\n> Week 3 title should maybe say “How could we safely train AIs…”? I think there are other training options if you don’t care about safety.\n\nLol nice catch."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-15T21:07:01.872Z",
    "af": false
  },
  {
    "_id": "n4FRvc7Wo5kqcFeXh",
    "postId": "rhEXTkDmssrHBNrfS",
    "parentCommentId": "5AEqyPcxv5aREExjK",
    "topLevelCommentId": "5AEqyPcxv5aREExjK",
    "contents": {
      "markdown": "We included a summary of Situational Awareness as an optional reading! I guess I thought the full thing was a bit too long to ask people to read. Thanks for the other recs!",
      "plaintextDescription": "We included a summary of Situational Awareness as an optional reading! I guess I thought the full thing was a bit too long to ask people to read. Thanks for the other recs!"
    },
    "baseScore": 2,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-15T21:04:27.485Z",
    "af": false
  },
  {
    "_id": "xDZ5LRscuMyzmGwHq",
    "postId": "QA3cmgNtNriMpxQgo",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> to simplify, we ask that $\\mathbb{G}(Y - \\mathbb{G}(Y | \\Pi) | \\Pi) = 0$ for every expression $Y$ and set of arguments $\\Pi$\n\nHere and in the next dot point, should the inner heuristic estimate be conditioning on a larger set of arguments (perhaps chosen by an unknown method)? Otherwise it seems like you're just expressing some sort of self-knowledge.",
      "plaintextDescription": "> to simplify, we ask that G(Y−G(Y|Π)|Π)=0 for every expression Y and set of arguments Π\n\nHere and in the next dot point, should the inner heuristic estimate be conditioning on a larger set of arguments (perhaps chosen by an unknown method)? Otherwise it seems like you're just expressing some sort of self-knowledge."
    },
    "baseScore": 2,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-07T22:36:07.474Z",
    "af": false
  },
  {
    "_id": "cta6XwZCd4ztzBibW",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "ApQYDjgwiBE6ZqiAj",
    "topLevelCommentId": "DBkhhAdkkbqWkMJgv",
    "contents": {
      "markdown": "OP doesn't emphasize liability insurance enough but part of the hope is that you can mandate that companies be insured up to $X00 billion, which costs them less than $X00 billion assuming that they're not likely to be held liable for that much. Then the hope is the insurance company can say \"please don't do extremely risky stuff or your premium goes up\".",
      "plaintextDescription": "OP doesn't emphasize liability insurance enough but part of the hope is that you can mandate that companies be insured up to $X00 billion, which costs them less than $X00 billion assuming that they're not likely to be held liable for that much. Then the hope is the insurance company can say \"please don't do extremely risky stuff or your premium goes up\"."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-10-07T16:59:30.529Z",
    "af": false
  },
  {
    "_id": "ejgRq8rpqxScsMHHX",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "LYessvKYhcmFSAKes",
    "topLevelCommentId": "icp8b559N65uudhye",
    "contents": {
      "markdown": "> On the other hand, there's not a clear criteria for when we would pause again after, say, a six month pause in scaling.\n\nRealized that I didn't respond to this - [PauseAI's proposal](https://pauseai.info/proposal) is for a pause until safety can be guaranteed, rather than just for 6 months.",
      "plaintextDescription": "> On the other hand, there's not a clear criteria for when we would pause again after, say, a six month pause in scaling.\n\nRealized that I didn't respond to this - PauseAI's proposal is for a pause until safety can be guaranteed, rather than just for 6 months."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-10-05T23:11:25.838Z",
    "af": false
  },
  {
    "_id": "uqq2SQP4rojNrNowj",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "8t9Wuhno7Y2EQx8Gu",
    "topLevelCommentId": "icp8b559N65uudhye",
    "contents": {
      "markdown": "> I believe AI pauses by governments would absolutely be more serious and longer, preventing overhangs from building up too much.\n\nAre you saying that overhangs wouldn't build up too much under pauses because the government wouldn't let it happen, or that RSPs would have less overhang because they'd pause for less long so less overhang would build up? I can't quite tell.",
      "plaintextDescription": "> I believe AI pauses by governments would absolutely be more serious and longer, preventing overhangs from building up too much.\n\nAre you saying that overhangs wouldn't build up too much under pauses because the government wouldn't let it happen, or that RSPs would have less overhang because they'd pause for less long so less overhang would build up? I can't quite tell."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-10-05T23:10:03.408Z",
    "af": false
  },
  {
    "_id": "M9ytpxqpgw8S6BJd8",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "2xpz6cBpDeqfCYdAc",
    "topLevelCommentId": "icp8b559N65uudhye",
    "contents": {
      "markdown": "I'm not saying there's no reason to think that RSPs are better or worse than pause, just that if overhang is a relevant consideration for pause, it's also a relevant consideration for RSPs.",
      "plaintextDescription": "I'm not saying there's no reason to think that RSPs are better or worse than pause, just that if overhang is a relevant consideration for pause, it's also a relevant consideration for RSPs."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-05T23:05:06.431Z",
    "af": false
  },
  {
    "_id": "LYessvKYhcmFSAKes",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "Rm2hXWy9eRRusjcC6",
    "topLevelCommentId": "icp8b559N65uudhye",
    "contents": {
      "markdown": "> I'd imagine that RSP proponents think that if we execute them properly, we will simply not build dangerous models beyond our control, period.\n\nI think pause proponents think similarly!",
      "plaintextDescription": "> I'd imagine that RSP proponents think that if we execute them properly, we will simply not build dangerous models beyond our control, period.\n\nI think pause proponents think similarly!"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-05T23:04:18.643Z",
    "af": false
  },
  {
    "_id": "Stotm8LCnrBq7GzrG",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "ThGwmd2DLbye9jxGm",
    "topLevelCommentId": "icp8b559N65uudhye",
    "contents": {
      "markdown": "> Do you see the same people invoking overhang as an argument against pauses and also talking about RSPs as though they are not also impacted?\n\nI guess I'm not tracking this closely enough. I'm not really that focussed on any one arguer's individual priorities, but more about the discourse in general. Basically, I think that overhang is a consideration for unconditional pauses if and only if it's a consideration for RSPs, so it's a bad thing if overhang is brought up as an argument against unconditional pauses and not against RSPs, because this will distort the world's ability to figure out the costs and benefits of each kind of policy.\n\nAlso, to be clear, it's not impossible that RSPs are all things considered better than unconditional pauses, and better than nothing, despite overhang. But if so, I'd hope someone somewhere would have written a piece saying \"RSPs have the cost of causing overhang, but on net are worth it\".",
      "plaintextDescription": "> Do you see the same people invoking overhang as an argument against pauses and also talking about RSPs as though they are not also impacted?\n\nI guess I'm not tracking this closely enough. I'm not really that focussed on any one arguer's individual priorities, but more about the discourse in general. Basically, I think that overhang is a consideration for unconditional pauses if and only if it's a consideration for RSPs, so it's a bad thing if overhang is brought up as an argument against unconditional pauses and not against RSPs, because this will distort the world's ability to figure out the costs and benefits of each kind of policy.\n\nAlso, to be clear, it's not impossible that RSPs are all things considered better than unconditional pauses, and better than nothing, despite overhang. But if so, I'd hope someone somewhere would have written a piece saying \"RSPs have the cost of causing overhang, but on net are worth it\"."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-10-05T23:03:21.283Z",
    "af": false
  },
  {
    "_id": "X8XBfBkMh332XdWNh",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": "xB6hWgSGr8jo96TE6",
    "topLevelCommentId": "icp8b559N65uudhye",
    "contents": {
      "markdown": "I'm not saying that RSPs are or aren't better than a pause. But I would think that if overhang is a relevant consideration for pauses, it's also a relevant consideration for RSPs.",
      "plaintextDescription": "I'm not saying that RSPs are or aren't better than a pause. But I would think that if overhang is a relevant consideration for pauses, it's also a relevant consideration for RSPs."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-04T18:01:24.955Z",
    "af": false
  },
  {
    "_id": "icp8b559N65uudhye",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "A complaint about AI pause: if we pause AI and then unpause, progress will then be really quick, because there's a backlog of improvements in compute and algorithmic efficiency that can be immediately applied.\n\nOne definition of what an RSP is: if a lab makes observation O, then they pause scaling until they implement protection P.\n\nDoesn't this sort of RSP have the same problem with fast progress after pausing? Why have I never heard anyone make this complaint about RSPs? Possibilities:\n- They do and I just haven't seen it\n- People expect \"AI pause\" to produce longer / more serious pauses than RSPs (but this seems incidental to the core structure of RSPs)",
      "plaintextDescription": "A complaint about AI pause: if we pause AI and then unpause, progress will then be really quick, because there's a backlog of improvements in compute and algorithmic efficiency that can be immediately applied.\n\nOne definition of what an RSP is: if a lab makes observation O, then they pause scaling until they implement protection P.\n\nDoesn't this sort of RSP have the same problem with fast progress after pausing? Why have I never heard anyone make this complaint about RSPs? Possibilities:\n\n * They do and I just haven't seen it\n * People expect \"AI pause\" to produce longer / more serious pauses than RSPs (but this seems incidental to the core structure of RSPs)"
    },
    "baseScore": 33,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2024-10-03T20:49:52.571Z",
    "af": false
  },
  {
    "_id": "RzdD4JiewyyHeuYBb",
    "postId": "WgMhovN7Gs6Jpn3PH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I continue to think that agent foundations research is kind of underrated. Like, we're supposed to do mechinterp to understand the algorithm models implement - but how do we know what algorithms are good?",
      "plaintextDescription": "I continue to think that agent foundations research is kind of underrated. Like, we're supposed to do mechinterp to understand the algorithm models implement - but how do we know what algorithms are good?"
    },
    "baseScore": 29,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2024-10-03T20:46:59.069Z",
    "af": false
  },
  {
    "_id": "jDuq8ubjEF299MbXq",
    "postId": "NsxFcB2EAxcY6cdGh",
    "parentCommentId": "o8X29G6YYDRdQbtrq",
    "topLevelCommentId": "i6Hufq3sNzQ94d6z9",
    "contents": {
      "markdown": "Ah, thanks for clarifying that.",
      "plaintextDescription": "Ah, thanks for clarifying that."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-10-01T22:37:40.964Z",
    "af": false
  },
  {
    "_id": "i6Hufq3sNzQ94d6z9",
    "postId": "NsxFcB2EAxcY6cdGh",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'm kind of confused which unilateralist got to design the game. You say:\n> The first person to click the Unilateral Virtue Link was a proponent of \"Avoiding actions that noticeably increase the chance that the world will end.\" But, this virtue was actually in the majority. The first unilateralist of a Virtue Minority was a proponent of \"Accurately reporting your epistemic state.\" \n> \n> A year later, as we decided what to do for Petrov Day, we decided to lure the _first_ unilateralist into a surprise meeting, where I then said \"Here's a reminder of what happened in Petrov Day last year. You now have one hour to design this year's Petrov game. Go.\"\n\nSo it sounds like the unilateralist who wanted to avoid actions that noticeably increase the chance the world will end got picked. But then it sounds like the winner made a game that was supposed to be about accurately reporting epistemic state:\n> The designer had (I think?) initially noticed the \"focus on accurately reporting epistemic state\" aspect, but said that during the stressful hour of designing the game had eventually forgotten that. The version they handed off wasn't particularly optimized for that, but the framework of a social deception game seemed to me to be a good substrate for \"accurate epistemic reporting.\" [...] It seemed important to me that Petrov's payoff specifically be about reporting his beliefs\n",
      "plaintextDescription": "I'm kind of confused which unilateralist got to design the game. You say:\n\n> The first person to click the Unilateral Virtue Link was a proponent of \"Avoiding actions that noticeably increase the chance that the world will end.\" But, this virtue was actually in the majority. The first unilateralist of a Virtue Minority was a proponent of \"Accurately reporting your epistemic state.\"\n> \n> A year later, as we decided what to do for Petrov Day, we decided to lure the first unilateralist into a surprise meeting, where I then said \"Here's a reminder of what happened in Petrov Day last year. You now have one hour to design this year's Petrov game. Go.\"\n\nSo it sounds like the unilateralist who wanted to avoid actions that noticeably increase the chance the world will end got picked. But then it sounds like the winner made a game that was supposed to be about accurately reporting epistemic state:\n\n> The designer had (I think?) initially noticed the \"focus on accurately reporting epistemic state\" aspect, but said that during the stressful hour of designing the game had eventually forgotten that. The version they handed off wasn't particularly optimized for that, but the framework of a social deception game seemed to me to be a good substrate for \"accurate epistemic reporting.\" [...] It seemed important to me that Petrov's payoff specifically be about reporting his beliefs"
    },
    "baseScore": 5,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-09-29T08:21:51.729Z",
    "af": false
  },
  {
    "_id": "D2fN8rhxmNi9CTXme",
    "postId": "DALNiKBjMGMF8grme",
    "parentCommentId": "ngfkm8iYHNpyuqDjT",
    "topLevelCommentId": "7jo857aCun9z6rbAC",
    "contents": {
      "markdown": "I'd now make this bet if you were down. Offer expires in 48 hours.",
      "plaintextDescription": "I'd now make this bet if you were down. Offer expires in 48 hours."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-30T16:57:08.309Z",
    "af": false
  },
  {
    "_id": "cXBPxA7bBqvk8Momj",
    "postId": "DALNiKBjMGMF8grme",
    "parentCommentId": "ngfkm8iYHNpyuqDjT",
    "topLevelCommentId": "7jo857aCun9z6rbAC",
    "contents": {
      "markdown": "I'd like to wait and see what various models say.",
      "plaintextDescription": "I'd like to wait and see what various models say."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-28T03:27:45.495Z",
    "af": false
  },
  {
    "_id": "K6c5nzqmqy6Xxzjcw",
    "postId": "5uffoui3axcK7gQXG",
    "parentCommentId": "LKB7KfxxH8wgkHeEE",
    "topLevelCommentId": "HWPeQCFZ5zdygR9hK",
    "contents": {
      "markdown": "[this comment is irrelevant to the point you actually care about and is just nit-picking about the analogy]\n\nThere is a pretty big divide between \"liberal\" and \"conservative\" Christianity that is in some ways bigger than the divide between different denominations. In the US, people who think of themselves as \"Episcopalians\" tend to be more liberal than people who call themselves \"Baptists\". In the rest of this comment, I'm going to assume we're talking about conservative Anglicans rather than Episcopalians (those terms referring to the same denominational family), and also about conservative Baptists, since they're more likely to be up to stuff / doing meaningful advocacy, and more likely to care about denominational distinctions. That said, liberal Episcopalians and liberal Baptists are much more likely to get along, and also openly talk about how they're in cooperation.\n\nMy guess is that conservative Anglicans and Baptists don't spend much time at each other's churches, at least during worship, given that they tend to have very different types of services and very different views about the point of worship (specifically about the role of the eucharist). Also there's a decent chance they don't allow each other to commune at their church (more likely on the Baptist end). Similarly, I don't think they are going to have that much social overlap, altho I could be wrong here. There's a good chance they read many of the same blogs tho.\n\nIn terms of policy advocacy, on the current margin they are going to mostly agree - common goals are going to be stuff like banning abortion, banning gay marriage, and ending the practice of gender transition. Anglican groups are going to be more comfortable with forms of state Christianity than Baptists are, altho this is lower-priority for both, I think. They are going to advocate for their preferred policies in part by denominational policy bodies, but also by joining common-cause advocacy organizations.\n\nBoth Anglican and Baptist churches are largely going to be funded by members, and their members are going to be disjoint. That said it's possible that their policy bodies will share large donor bases.\n\nThey are also organized pretty differently internally: Anglicans have a very hierarchical structure, and Baptists having a very decentralized structure (each congregation is its own democratic policy, and is able to e.g. vote to remove the pastor and hire a new one)\n\nAnyway: I'm pretty sympathetic to the claim of conservative Anglicans and Baptists being meaningfully distinct power bases, altho it would be misleading to not acknowledge that they're both part of a broader conservative Christian ecosystem with shared media sources, fashions, etc.\n\nPart of the reason this analogy didn't vibe for me is that Anglicans and Baptists are about as dissimilar as Protestants can get. If it were Anglicans and Presbyterians or Baptists and Pentecostals that would make more sense, as those denominations are much more similar to each other.",
      "plaintextDescription": "[this comment is irrelevant to the point you actually care about and is just nit-picking about the analogy]\n\nThere is a pretty big divide between \"liberal\" and \"conservative\" Christianity that is in some ways bigger than the divide between different denominations. In the US, people who think of themselves as \"Episcopalians\" tend to be more liberal than people who call themselves \"Baptists\". In the rest of this comment, I'm going to assume we're talking about conservative Anglicans rather than Episcopalians (those terms referring to the same denominational family), and also about conservative Baptists, since they're more likely to be up to stuff / doing meaningful advocacy, and more likely to care about denominational distinctions. That said, liberal Episcopalians and liberal Baptists are much more likely to get along, and also openly talk about how they're in cooperation.\n\nMy guess is that conservative Anglicans and Baptists don't spend much time at each other's churches, at least during worship, given that they tend to have very different types of services and very different views about the point of worship (specifically about the role of the eucharist). Also there's a decent chance they don't allow each other to commune at their church (more likely on the Baptist end). Similarly, I don't think they are going to have that much social overlap, altho I could be wrong here. There's a good chance they read many of the same blogs tho.\n\nIn terms of policy advocacy, on the current margin they are going to mostly agree - common goals are going to be stuff like banning abortion, banning gay marriage, and ending the practice of gender transition. Anglican groups are going to be more comfortable with forms of state Christianity than Baptists are, altho this is lower-priority for both, I think. They are going to advocate for their preferred policies in part by denominational policy bodies, but also by joining common-cause advocacy organizations.\n\nBoth Anglican and Baptist chur"
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-07-22T23:51:25.611Z",
    "af": false
  },
  {
    "_id": "JHAcR5XDDnFLfSC6f",
    "postId": "DALNiKBjMGMF8grme",
    "parentCommentId": "Cagm3Fk2Lk6fdPowS",
    "topLevelCommentId": "7jo857aCun9z6rbAC",
    "contents": {
      "markdown": "Further updates:\n- On the one hand, Nate Silver's model now gives Trump a ~30% chance of winning in Virginia, making my side of the bet look good again.\n- On the other hand, the Economist model gives Trump a 10% chance of winning Delaware and a 20% chance of winning Illinois, which suggests that there's something going wrong with the model and that it was untrustworthy a month ago.\n- That said, betting markets [currently](https://web.archive.org/web/20240720003118/https://electionbettingodds.com/DEMPrimary2024.html) think there's only a one in four chance that Biden is the nominee, so this bet probably won't resolve.",
      "plaintextDescription": "Further updates:\n\n * On the one hand, Nate Silver's model now gives Trump a ~30% chance of winning in Virginia, making my side of the bet look good again.\n * On the other hand, the Economist model gives Trump a 10% chance of winning Delaware and a 20% chance of winning Illinois, which suggests that there's something going wrong with the model and that it was untrustworthy a month ago.\n * That said, betting markets currently think there's only a one in four chance that Biden is the nominee, so this bet probably won't resolve."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-20T00:32:48.788Z",
    "af": false
  },
  {
    "_id": "mmYaatppwNmzLbap4",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> I will spend time reading posts and papers, improving coding skills as needed to run and interpret experiments, learning math as needed for writing up proofs, talking with concept-based interpretability researchers as well as other conceptual alignment researchers\n\nI feel like this is missing the bit where you write proofs, run and interpret experiments, etc.",
      "plaintextDescription": "> I will spend time reading posts and papers, improving coding skills as needed to run and interpret experiments, learning math as needed for writing up proofs, talking with concept-based interpretability researchers as well as other conceptual alignment researchers\n\nI feel like this is missing the bit where you write proofs, run and interpret experiments, etc."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:50:07.609Z",
    "af": false
  },
  {
    "_id": "jmQNk4kN3vKXzBetJ",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> As a maximal goal, I might seek to test my theories about the detection of generalizable human values (like reciprocity and benevolence) by programming an alife simulation meant to test a toy-model version of agentic interaction and world-model agreement/interoperability through the fine-structure of the simulated agents.\n\nDo you think you will be able to do this in the next 6 weeks? Might be worth scaling this down to \"start a framework to test my theories\" or something like that",
      "plaintextDescription": "> As a maximal goal, I might seek to test my theories about the detection of generalizable human values (like reciprocity and benevolence) by programming an alife simulation meant to test a toy-model version of agentic interaction and world-model agreement/interoperability through the fine-structure of the simulated agents.\n\nDo you think you will be able to do this in the next 6 weeks? Might be worth scaling this down to \"start a framework to test my theories\" or something like that"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:49:31.191Z",
    "af": false
  },
  {
    "_id": "rS4fpeFxu4nGAtSux",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> the fine-structure of the simulated agents.\n\nwhat does this mean?",
      "plaintextDescription": "> the fine-structure of the simulated agents.\n\nwhat does this mean?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:48:28.138Z",
    "af": false
  },
  {
    "_id": "pco8yFw8LiaQyzXuN",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> alife\n\nI think most people won't know what this word means",
      "plaintextDescription": "> alife\n\nI think most people won't know what this word means"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:48:07.886Z",
    "af": false
  },
  {
    "_id": "rxLiDXPzsGjmmaJso",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> I plan to stress-test and further flesh out the theory, with a minimal goal of producing a writeup presenting results I've found and examining whether the assumptions of the toy models of the original post hold up as a way of examining Natural Abstractions as an alignment plan.\n\nI feel like this doesn't give me quite enough of an idea of what you'd be doing - like, what does \"stress-testing\" involve? What parts need fleshing out?",
      "plaintextDescription": "> I plan to stress-test and further flesh out the theory, with a minimal goal of producing a writeup presenting results I've found and examining whether the assumptions of the toy models of the original post hold up as a way of examining Natural Abstractions as an alignment plan.\n\nI feel like this doesn't give me quite enough of an idea of what you'd be doing - like, what does \"stress-testing\" involve? What parts need fleshing out? "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:47:49.686Z",
    "af": false
  },
  {
    "_id": "LcGyGruNKneexiX8z",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> **Time-bounded: Are research activities and outputs time-bounded?**\n> \n> *   Does the proposal include a tentative timeline of planned activities (e.g., LTFF grant, scholar symposium talk, paper submission)?\n> *   How might the timeline change if planned research activities are unsuccessful?\n\nThis part is kind of missing - I'm seeing a big list of stuff you could do, but not an indication of how much of it you might reasonably expect to do in the next 5 weeks. A better approach here would be to give a list of things you could do in those 5 weeks together estimates for how much time each thing could take, possibly with a side section of \"here are other things I could do depending on how stuff goes\"",
      "plaintextDescription": "> Time-bounded: Are research activities and outputs time-bounded?\n> \n>  * Does the proposal include a tentative timeline of planned activities (e.g., LTFF grant, scholar symposium talk, paper submission)?\n>  * How might the timeline change if planned research activities are unsuccessful?\n\nThis part is kind of missing - I'm seeing a big list of stuff you could do, but not an indication of how much of it you might reasonably expect to do in the next 5 weeks. A better approach here would be to give a list of things you could do in those 5 weeks together estimates for how much time each thing could take, possibly with a side section of \"here are other things I could do depending on how stuff goes\""
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:44:40.370Z",
    "af": false
  },
  {
    "_id": "Fiy2im9QytMQxNqvQ",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Theory of Change\n\nI feel like this section is missing a sentence like \"OK here's the thing that would be the output of my project, and here's how it would cause these good effects\"",
      "plaintextDescription": "> Theory of Change\n\nI feel like this section is missing a sentence like \"OK here's the thing that would be the output of my project, and here's how it would cause these good effects\""
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:41:51.293Z",
    "af": false
  },
  {
    "_id": "cnnzwgpMHbQBXMsWw",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> \\[probably I also put a timeline here of stuff I have done so far?\\]\n\nThis is valuable for you to do so that you can get a feel for what you can do in a week, but I'm not sure it's actually that valuable to plop into the RP",
      "plaintextDescription": "> [probably I also put a timeline here of stuff I have done so far?]\n\nThis is valuable for you to do so that you can get a feel for what you can do in a week, but I'm not sure it's actually that valuable to plop into the RP"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:40:50.745Z",
    "af": false
  },
  {
    "_id": "GFGkPME68SjAGaEam",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Less prosaically, it's not impossible that a stronger or more solidly grounded theory of semantics or of interoperable world-models might prove to be the \"last missing piece\" between us and AGI; that said, given that my research path primarily involves things like finding and constructing conceptual tools, writing mathematical proofs, and reasoning about bounds on accumulating errors - and not things like training new frontier models - I think the risk/dual-use-hazard of my proposed work is minimal.\n\nI don't really understand this argument. Why wouldn't having a better theory of semantics and concepts help people build better AIs, but still do a good job of describing what's going on in smart AIs? Like, you might think the more things you know about smart AIs, the easier it would be to build them - where does this argument break?  \n  \nThe thing you imply here is that it's pretty different from stuff people currently do to train frontier models, but you already told me that scaling frontier models was really unlikely to lead to AGI, so why should that give me any comfort?",
      "plaintextDescription": "> Less prosaically, it's not impossible that a stronger or more solidly grounded theory of semantics or of interoperable world-models might prove to be the \"last missing piece\" between us and AGI; that said, given that my research path primarily involves things like finding and constructing conceptual tools, writing mathematical proofs, and reasoning about bounds on accumulating errors - and not things like training new frontier models - I think the risk/dual-use-hazard of my proposed work is minimal.\n\nI don't really understand this argument. Why wouldn't having a better theory of semantics and concepts help people build better AIs, but still do a good job of describing what's going on in smart AIs? Like, you might think the more things you know about smart AIs, the easier it would be to build them - where does this argument break?\n\nThe thing you imply here is that it's pretty different from stuff people currently do to train frontier models, but you already told me that scaling frontier models was really unlikely to lead to AGI, so why should that give me any comfort?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:39:58.897Z",
    "af": false
  },
  {
    "_id": "58szgjNbJ2wysA9tx",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Not only would a better theory of semantics help researchers detect objects and features which are natural to the AI, it would also help them check whether a given AI treats some feature of its environment or class of object as a natural cluster, and help researchers agree within provable bounds on what concept precisely they are targeting.\n\nThis part isn't so clear to me. Why can't I just look at what features of the world an AI represents without a theory of semantics?",
      "plaintextDescription": "> Not only would a better theory of semantics help researchers detect objects and features which are natural to the AI, it would also help them check whether a given AI treats some feature of its environment or class of object as a natural cluster, and help researchers agree within provable bounds on what concept precisely they are targeting.\n\nThis part isn't so clear to me. Why can't I just look at what features of the world an AI represents without a theory of semantics?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:37:34.059Z",
    "af": false
  },
  {
    "_id": "KCSskyDjjQastpuhN",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": "PRA9gSSMAkdDBTgdE",
    "topLevelCommentId": "PRA9gSSMAkdDBTgdE",
    "contents": {
      "markdown": "the next paragraph is kind of like that but making a sort of novel point so maybe they're necessary? I'd try to focus them on saying things you haven't yet said",
      "plaintextDescription": "the next paragraph is kind of like that but making a sort of novel point so maybe they're necessary? I'd try to focus them on saying things you haven't yet said"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:36:27.230Z",
    "af": false
  },
  {
    "_id": "vob6XZvr3jARDc2Qg",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> (admittedly unlikely)\n\nwhy do you think it's unlikely?",
      "plaintextDescription": "> (admittedly unlikely)\n\nwhy do you think it's unlikely?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:32:31.368Z",
    "af": false
  },
  {
    "_id": "PRA9gSSMAkdDBTgdE",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> On one hand, arbitrary agents - or at least a large class of agents, or at least (proto-)AGIs that humans make - might turn out to simply already naturally agree with us on the features we abstract from our surroundings; a better-grounded and better-developed theory of semantics would allow us to confirm this and become more optimistic about the feasibility of alignment.\n> \n> On the other, such agents might prove in general to have inner ontologies totally unrelated to our own, or perhaps only somewhat different, but in enduring and hazardous ways; a better theory of semantics would warn us of this in advance and suggest other routes to AGI or perhaps drive a total halt to development.\n\nI feel like these two paragraphs are just fleshing out the thing you said earlier and aren't really needed",
      "plaintextDescription": "> On one hand, arbitrary agents - or at least a large class of agents, or at least (proto-)AGIs that humans make - might turn out to simply already naturally agree with us on the features we abstract from our surroundings; a better-grounded and better-developed theory of semantics would allow us to confirm this and become more optimistic about the feasibility of alignment.\n> \n> On the other, such agents might prove in general to have inner ontologies totally unrelated to our own, or perhaps only somewhat different, but in enduring and hazardous ways; a better theory of semantics would warn us of this in advance and suggest other routes to AGI or perhaps drive a total halt to development.\n\nI feel like these two paragraphs are just fleshing out the thing you said earlier and aren't really needed"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:32:08.267Z",
    "af": false
  },
  {
    "_id": "tQeCALKcKhaAEinqu",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> inner\n\nis this word needed?",
      "plaintextDescription": "> inner\n\nis this word needed?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:31:34.295Z",
    "af": false
  },
  {
    "_id": "HyJENnxW3WeXaPFXY",
    "postId": "fQAjgi9Khtx2jyGQt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> those that rely on arbitrary AGIs detecting and \\[settling on as natural\\] the same features of the world that humans do, including values and qualities important to humanity\n\ncan you give examples of such strategies, and argue that they rely on this?",
      "plaintextDescription": "> those that rely on arbitrary AGIs detecting and [settling on as natural] the same features of the world that humans do, including values and qualities important to humanity\n\ncan you give examples of such strategies, and argue that they rely on this?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-12T22:31:10.986Z",
    "af": false
  }
]