[
  {
    "_id": "mbuRXYRwMktexg8Fm",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "i recently ran into to a vegan advocate tabling in a public space, and spoke briefly to them for the explicit purpose of better understanding what it feels like to be the target of advocacy on something i feel moderately sympathetic towards but not fully bought in on. (i find this kind of thing very valuable for noticing flaws in myself and improving; it's much harder to be perceptive of one's own actions otherwise). the part where i am genuinely quite plausibly persuadable of his position in theory is important; i think if i had talked to e.g flat earthers one might say my reaction is just because i'd already decided not to be persuaded. several interesting things i noticed (none of which should be surprising or novel, especially for someone less autistic than me, but as they say, intellectually knowing things is not the same as actual experience):\n\n*   this guy certainly knew more about e.g health impacts of veganism than i did, and i would not have been able to hold my own in an actual debate.\n    *   in particular, it's really easy for actually-good-in-practice heuristics to come out as logical fallacies, especially when arguing with someone much more familiar with the object level details than you are.\n    *   interestingly, since i was pushing the conversation in a pretty meta direction, he actually explicitly said something to the effect that he's had thousands of conversations like this and has a response to basically every argument i could make, do i really think i have something he hasn't heard before, etc. in that moment i realized this was probably true, and that this nonetheless did not necessarily mean that he was correct in his claim. and in addition it certainly didn't make me feel any more emotionally willing to accept his argument\n    *   in the past, i've personally had the exact experience of arguing for something where i had enough of a dialogue tree that other people couldn't easily find any holes, where the other people were unconvinced, and felt really confused why people weren't seeing the very straightforward argument, and then later it turned out i was actually just wrong and the other people were applying correct heuristics\n        *   my guess is at the extreme, with sufficient prep and motivation, you can get in this position for arbitrarily wrong beliefs. like probably if i talked to flat earthers for a while i'd get deep enough in their dialogue tree that i'd stop being able to refute them on the object level and would (for the purposes of my own epistemics, not to convince an external audience) have to appeal to cognitive heuristics that are isomorphic to some cognitive fallacies.\n    *   of course we shouldn't always appeal to the cognitive heuristics. [doing so is almost always reasonable and yet you will miss out on the one thing that actually does matter](https://www.astralcodexten.com/p/heuristics-that-almost-always-work). to do anything interesting you do have to eventually dig into some particular spicy claims and truly resolve things at the object level. but there are so many things in the world and resolving them takes so much time that you need some heuristics to reject a whole bunch of things out of hand and focus your energy on the things that matter.\n        *   like, i could invest energy until i can actually refute flat earthers completely on the object level, and i'd almost certainly succeed. but this would be a huge waste of time. on the other hand, i could also just never look into anything and say \"nothing ever happens\". but every important thing to ever happen did, in fact, happen at some point \\[citation needed\\].\n*   it's really really irritating to be cut off mid sentence. this is hard to admit because i also have an unconscious tendency to do this (currently working on fixing this) and my guess is other people get very annoyed when i do this to them.\n    *   sometimes i do enjoy being cut off in conversations, but on reflection this is only when i feel like (a) the conversation is cooperative enough that i feel like we're trying to discover the truth together, (b) the other person actually understands what i'm saying before i finish saying it. but since these conditions are much rarer and requires high levels of social awareness to detect, it's a good first order heuristic that interrupting people is bad.\n*   i found it completely unhelpful to be told that he was also in my shoes X years ago with similar uncertainties when he was deciding to become vegan; or to be told that he had successfully convinced Y other people to become vegan; or to be subject to what i want to call \"therapy speak\". i only want to therapyspeak with people i feel relatively close to, and otherwise it comes off as very patronizing.\n    *   i think there's a closely related thing, which is genuine curiosity about people's views. it uses similar phrases like \"what makes you believe that?\" but has a very different tone and vibe.\n    *   his achievements mean a lot more to himself than to me. i don't really care that much what he's accomplished for the purposes of deciding whether his argument is correct. any credibility points conferred are more than cancelled out by it being kind of annoying. *even if it is true*, there's nothing more annoying than hearing say \"i've thought about this more than you / accomplished more than you have because of my phd/experience/etc so you should listen to me\" unless you really really really trust this person\n        *   the calculus changes when there is an audience.\n    *   therapyspeak is still probably better than nothing, and can be a useful stepping stone for the socially incompetent\n\none possible take is that i'm just really weird and these modes of interaction work well for normal people more because they're less independently thinking or need to be argued out of having poorly thought out bad takes or something like that, idk. i can't rule this out but my guess is normal people probably are even more this than i am. also, for the purposes of analogy to the AI safety movement, presumably we want to select for people who are independent thinkers who have especially well thought out takes more than just normal people.\n\nalso my guess is this particular interaction was probably extremely out of distribution from the perspective of those tabling. my guess is activists generally have a pretty polished pitch for most common situations which includes a bunch of concrete ways of talking they've empirically found to cause people to engage, learned through years of RL against a general audience, but the polishedness of this pitch doesn't generalize out of distribution when poked at in weird ways. my interlocutor even noted at some point that his conversations when tabling generally don't go the way ours went.",
      "plaintextDescription": "i recently ran into to a vegan advocate tabling in a public space, and spoke briefly to them for the explicit purpose of better understanding what it feels like to be the target of advocacy on something i feel moderately sympathetic towards but not fully bought in on. (i find this kind of thing very valuable for noticing flaws in myself and improving; it's much harder to be perceptive of one's own actions otherwise). the part where i am genuinely quite plausibly persuadable of his position in theory is important; i think if i had talked to e.g flat earthers one might say my reaction is just because i'd already decided not to be persuaded. several interesting things i noticed (none of which should be surprising or novel, especially for someone less autistic than me, but as they say, intellectually knowing things is not the same as actual experience):\n\n * this guy certainly knew more about e.g health impacts of veganism than i did, and i would not have been able to hold my own in an actual debate.\n   * in particular, it's really easy for actually-good-in-practice heuristics to come out as logical fallacies, especially when arguing with someone much more familiar with the object level details than you are.\n   * interestingly, since i was pushing the conversation in a pretty meta direction, he actually explicitly said something to the effect that he's had thousands of conversations like this and has a response to basically every argument i could make, do i really think i have something he hasn't heard before, etc. in that moment i realized this was probably true, and that this nonetheless did not necessarily mean that he was correct in his claim. and in addition it certainly didn't make me feel any more emotionally willing to accept his argument\n   * in the past, i've personally had the exact experience of arguing for something where i had enough of a dialogue tree that other people couldn't easily find any holes, where the other people were unconvinced, and felt really"
    },
    "baseScore": 114,
    "voteCount": 50,
    "createdAt": null,
    "postedAt": "2025-09-25T21:28:56.845Z",
    "af": false
  },
  {
    "_id": "8WEtD49kR3SNHAdTF",
    "postId": "tv6KfHitijSyKCr6v",
    "parentCommentId": "qbYD9a56wgAy9qWkF",
    "topLevelCommentId": "MtW94mEeFLqpPPB5B",
    "contents": {
      "markdown": "yeah, but there would be a lot of worlds where the merger was totally fine and beneficial where it fell through because people had unfounded fears",
      "plaintextDescription": "yeah, but there would be a lot of worlds where the merger was totally fine and beneficial where it fell through because people had unfounded fears"
    },
    "baseScore": 3,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-21T18:51:00.919Z",
    "af": false
  },
  {
    "_id": "tcb7X7j3iAafR8iap",
    "postId": "tv6KfHitijSyKCr6v",
    "parentCommentId": "MtW94mEeFLqpPPB5B",
    "topLevelCommentId": "MtW94mEeFLqpPPB5B",
    "contents": {
      "markdown": "i mean, in general, it's a lot easier to tell plausible-seeming stories of things going really poorly than actually high-likelihood stories of things going poorly. so the anecdata of it actually happening is worth a lot",
      "plaintextDescription": "i mean, in general, it's a lot easier to tell plausible-seeming stories of things going really poorly than actually high-likelihood stories of things going poorly. so the anecdata of it actually happening is worth a lot"
    },
    "baseScore": 6,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-09-21T04:47:27.278Z",
    "af": false
  },
  {
    "_id": "HZLkX5fZcSudgFT2c",
    "postId": "fF8pvsn3AGQhYsbjp",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I've been repeatedly loud and explicit about this but an happy to state again that racing to build superintelligence before we know how to make it not kill everyone (or cause other catastrophic outcomes) seems really bad and I wish we could coordinate to not do that.",
      "plaintextDescription": "I've been repeatedly loud and explicit about this but an happy to state again that racing to build superintelligence before we know how to make it not kill everyone (or cause other catastrophic outcomes) seems really bad and I wish we could coordinate to not do that."
    },
    "baseScore": 167,
    "voteCount": 63,
    "createdAt": null,
    "postedAt": "2025-09-20T14:34:12.922Z",
    "af": false
  },
  {
    "_id": "HjbywEbehHP3vurrC",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "LqdgoQF9KYv3awHu4",
    "topLevelCommentId": "yDYxwzWpyfzGS34Lq",
    "contents": {
      "markdown": "there's an exogenous factor, which is that the entire country was shifting leftward during the 50s and 60s. it's plausible that the 1964 bill would have passed anyways without the 1957 bill, possibly even earlier",
      "plaintextDescription": "there's an exogenous factor, which is that the entire country was shifting leftward during the 50s and 60s. it's plausible that the 1964 bill would have passed anyways without the 1957 bill, possibly even earlier"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-19T22:03:02.937Z",
    "af": false
  },
  {
    "_id": "yDYxwzWpyfzGS34Lq",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "what's the current state of analysis on whether the civil rights act of 1957 was actually net positive or negative for civil rights in hindsight? there are two possible stories one can tell, and at the time people were arguing about which is correct:\n\n1.  passing even a useless civil rights bill is a lot better than nothing because it sets a precedent that getting civil rights bills through the Senate is possible / makes the southern coalition no longer look invincible. this serves a useful coordination mechanism because people only want to support things that they think other people will support.\n2.  passing a useless civil rights bill is worse than no bill because it creates a false sense of progress and makes it feel like something was done even when nothing was. to the extent that the bill signals to people that getting civil rights bills through the Senate is possible, this is a false impression because the only reason the bill could get through was that it was watered down to uselessness.\n\nthis feels directly analogous to the question of whether we should accept very weak AI safety regulations today.",
      "plaintextDescription": "what's the current state of analysis on whether the civil rights act of 1957 was actually net positive or negative for civil rights in hindsight? there are two possible stories one can tell, and at the time people were arguing about which is correct:\n\n 1. passing even a useless civil rights bill is a lot better than nothing because it sets a precedent that getting civil rights bills through the Senate is possible / makes the southern coalition no longer look invincible. this serves a useful coordination mechanism because people only want to support things that they think other people will support.\n 2. passing a useless civil rights bill is worse than no bill because it creates a false sense of progress and makes it feel like something was done even when nothing was. to the extent that the bill signals to people that getting civil rights bills through the Senate is possible, this is a false impression because the only reason the bill could get through was that it was watered down to uselessness.\n\nthis feels directly analogous to the question of whether we should accept very weak AI safety regulations today. "
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-19T21:43:04.404Z",
    "af": false
  },
  {
    "_id": "xiwXNktcbbQcf7vJ8",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "a thing i've noticed rat/autistic people do (including myself): one very easy way to trick our own calibration sensors is to add a bunch of caveats or considerations that make it feel like we've modeled all the uncertainty (or at least, more than other people who haven't). so one thing i see a lot is that people are self-aware that they have limitations, but then over-update on how much this awareness makes them calibrated. one telltale hint that i'm doing this myself is if i catch myself saying something because i want to demo my rigor and prove that i've considered some caveat that one might think i forgot to consider\n\ni've heard others make a similar critique about this as a communication style which can mislead non-rats who are not familiar with the style, but i'm making a different claim here that one can trick oneself.\n\nit seems that one often believes being self aware of a certain limitation is enough to correct for it sufficiently to at least be calibrated about how limited one is. a concrete example: part of being socially incompetent is not just being bad at taking social actions, but being bad at detecting social feedback on those actions. of course, many people are not even aware of the latter. but many are aware of and acknowledge the latter, and then act as if because they've acknowledged a potential failure mode and will try to be careful towards avoiding it, that they are much less susceptible to the failure mode than other people in an otherwise similar reference class.\n\none variant of this deals with hypotheticals - because hypotheticals often can/will never be evaluated, this allows one to get the feeling that one is being epistemically virtuous and making falsifiable predictions, without ever actually getting falsified. for example, a statement \"if X had happened, then i bet we would see Y now\" has prediction vibes but is not actually a prediction. this is especially pernicious when one fails but says \"i failed but i was close, so i should still update positively on what i did.\" while not always a bad idea, there's a bias-variance tradeoff here, where doing this more often reduces variance but increases bias. i find that cases where i thought i was close but later realized i was actually far off the mark are sufficiently common that this isn't an imaginary concern.\n\nanother variant is i think we are much less susceptible to some forms of brainworms/ideology, and are also much better at understanding the mechanisms behind brainworms and identifying them in others, so we over-update on our own insusceptibility to brainworms (despite evidence from the reference class of rationalists that seems to suggest at least as much as genpop if not higher levels of obvious-cult-forming). however, it's just that we are suscpetible to different types of brainworks as normies.\n\nanother variant is introspective ability. i think we are probably better in some sense at self-introspection, in the sense that we are better at noticing certain kinds of patterns in our own behavior and developing models for those patterns. but i've also come to believe that this kind of modeling has huge blind spots, and leads many to believe they have a much greater degree of mastery over their own minds than they actually do. however, the feeling that one is aware of the possibility of one having blind spots and being aware of what they often look like in other people can lead to overconfidence about whether one would notice these blindspots in themself.\n\ni feel like the main way i notice these things is by noticing them in other people over long periods of knowing them, and then noticing that my actions are actually deeply analogous to theirs in some way. it also helps to notice non-rats not falling into the same pitfalls sometimes.\n\ni'm not sure how to fix this. merely being aware of it probably is not sufficient. probably the solution is not to stop thinking about one's own limitations, but rather to add some additional cogtech on top. my guess is there is probably valuable memetic technology out there that especially wise people use but which most people, rat or not, don't use. also, difficult-to-fake feedback from reality seems important.",
      "plaintextDescription": "a thing i've noticed rat/autistic people do (including myself): one very easy way to trick our own calibration sensors is to add a bunch of caveats or considerations that make it feel like we've modeled all the uncertainty (or at least, more than other people who haven't). so one thing i see a lot is that people are self-aware that they have limitations, but then over-update on how much this awareness makes them calibrated. one telltale hint that i'm doing this myself is if i catch myself saying something because i want to demo my rigor and prove that i've considered some caveat that one might think i forgot to consider\n\ni've heard others make a similar critique about this as a communication style which can mislead non-rats who are not familiar with the style, but i'm making a different claim here that one can trick oneself.\n\nit seems that one often believes being self aware of a certain limitation is enough to correct for it sufficiently to at least be calibrated about how limited one is. a concrete example: part of being socially incompetent is not just being bad at taking social actions, but being bad at detecting social feedback on those actions. of course, many people are not even aware of the latter. but many are aware of and acknowledge the latter, and then act as if because they've acknowledged a potential failure mode and will try to be careful towards avoiding it, that they are much less susceptible to the failure mode than other people in an otherwise similar reference class.\n\none variant of this deals with hypotheticals - because hypotheticals often can/will never be evaluated, this allows one to get the feeling that one is being epistemically virtuous and making falsifiable predictions, without ever actually getting falsified. for example, a statement \"if X had happened, then i bet we would see Y now\" has prediction vibes but is not actually a prediction. this is especially pernicious when one fails but says \"i failed but i was close, so i should still "
    },
    "baseScore": 64,
    "voteCount": 28,
    "createdAt": null,
    "postedAt": "2025-09-18T02:28:05.422Z",
    "af": false
  },
  {
    "_id": "AuW7rmsLF8erAFxcC",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "my guess is that lots of people would change their minds if they really reflected on it with full wisdom and the assistance of an aligned and emotionally intelligent assistant. but if truly deep down some/many people value their beliefs over truth, and would never change their minds even if they reflected deeply on it, who are we to tell them not to do that? the best we can ask for is that they leave us alone to do what we believe is good, and vice versa.",
      "plaintextDescription": "my guess is that lots of people would change their minds if they really reflected on it with full wisdom and the assistance of an aligned and emotionally intelligent assistant. but if truly deep down some/many people value their beliefs over truth, and would never change their minds even if they reflected deeply on it, who are we to tell them not to do that? the best we can ask for is that they leave us alone to do what we believe is good, and vice versa."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-17T16:04:07.076Z",
    "af": false
  },
  {
    "_id": "MbAok8KhkGuNvLRMN",
    "postId": "fdCaCDfstHxyPmB9h",
    "parentCommentId": "tzQwCncreh72wJJy5",
    "topLevelCommentId": "tzQwCncreh72wJJy5",
    "contents": {
      "markdown": "in general publicly known training techniques are behind sota, so this should be taken into account.",
      "plaintextDescription": "in general publicly known training techniques are behind sota, so this should be taken into account."
    },
    "baseScore": 11,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-17T15:49:28.257Z",
    "af": false
  },
  {
    "_id": "tZscD9iz8fXJbzw38",
    "postId": "uGzhvohms374znKF3",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I like the spirit of this work but it would benefit a lot from a more in depth review of the existing literature and methodologies. some examples (non exhaustive)\n\n*   the piecewise approximation thing is a pretty widely accepted opinion in ML\n*   visualizing the loss landscape as a plane between three points in model space is pretty common in the field and often the landscape is a lot more nontrivial.\n*   a factor of 3 loss difference is huge! if you want to claim that smooth actfn is better beyond what's explained by the loss, you need to compare two models with the same loss but different actfn.\n*   the post just hand waves away the difference between SGD and Adam. this is an important difference! Adam tries to take ~constant sized steps along each axis direction.\n*   local approximation of the loss landscape as approximately quadratic is pretty widely accepted; generally people look at the eigenvalues of the Hessian to try to understand the local shape of the loss landscape.\n*   scaling the gradient 500x is less impactful than it sounds like because the changes to the gradient scale are way less important than you'd expect because they get multiplied out by (1-beta2), this is unlike SGD where gradient scaling is equivalent to LR scaling.\n*   learning rate decay is an important part of real training that substantially affects many conclusions\n*   to compare models, if possible generally you want to train to the L(D) regime (loss has stopped improving at all), or pick some principled criterion for stopping early compute-optinally (L(C))",
      "plaintextDescription": "I like the spirit of this work but it would benefit a lot from a more in depth review of the existing literature and methodologies. some examples (non exhaustive)\n\n * the piecewise approximation thing is a pretty widely accepted opinion in ML\n * visualizing the loss landscape as a plane between three points in model space is pretty common in the field and often the landscape is a lot more nontrivial.\n * a factor of 3 loss difference is huge! if you want to claim that smooth actfn is better beyond what's explained by the loss, you need to compare two models with the same loss but different actfn.\n * the post just hand waves away the difference between SGD and Adam. this is an important difference! Adam tries to take ~constant sized steps along each axis direction.\n * local approximation of the loss landscape as approximately quadratic is pretty widely accepted; generally people look at the eigenvalues of the Hessian to try to understand the local shape of the loss landscape.\n * scaling the gradient 500x is less impactful than it sounds like because the changes to the gradient scale are way less important than you'd expect because they get multiplied out by (1-beta2), this is unlike SGD where gradient scaling is equivalent to LR scaling.\n * learning rate decay is an important part of real training that substantially affects many conclusions\n * to compare models, if possible generally you want to train to the L(D) regime (loss has stopped improving at all), or pick some principled criterion for stopping early compute-optinally (L(C))"
    },
    "baseScore": 16,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-17T15:31:16.574Z",
    "af": false
  },
  {
    "_id": "mexQgkSoYe4yGuP4Y",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "crazy how x dot com is literally more addictive than actual amphetamines",
      "plaintextDescription": "crazy how x dot com is literally more addictive than actual amphetamines "
    },
    "baseScore": 7,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-16T01:16:10.864Z",
    "af": false
  },
  {
    "_id": "cHjfqF2iCBgihw6rp",
    "postId": "LtT24cCAazQp4NYc5",
    "parentCommentId": "9i6hEbipSWwyzsFAA",
    "topLevelCommentId": "9i6hEbipSWwyzsFAA",
    "contents": {
      "markdown": "it seems fine to invest and then publicly state your views, including that it should not be interpreted as an endorsement. your investment (and that of other people who decide similarly) is trivial in size compared to the other sources of funding, such that it's not counterfactual. you're not going to cause the founders of anthropic to get any less of a windfall. the decision process for the vast majority of possible investors does not take into account whether or not you invested.\n\ni think you've already sufficiently signaled your genuineness, for all practical purposes. i don't think it's healthy to have a purity spiral.",
      "plaintextDescription": "it seems fine to invest and then publicly state your views, including that it should not be interpreted as an endorsement. your investment (and that of other people who decide similarly) is trivial in size compared to the other sources of funding, such that it's not counterfactual. you're not going to cause the founders of anthropic to get any less of a windfall. the decision process for the vast majority of possible investors does not take into account whether or not you invested.\n\ni think you've already sufficiently signaled your genuineness, for all practical purposes. i don't think it's healthy to have a purity spiral."
    },
    "baseScore": 6,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-08-28T08:36:41.319Z",
    "af": false
  },
  {
    "_id": "wnXMyzbojXCDAek87",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "only a fool is easily parted from his money. but even the most wise, intelligent, and savvy are routinely parted from his power and influence",
      "plaintextDescription": "only a fool is easily parted from his money. but even the most wise, intelligent, and savvy are routinely parted from his power and influence "
    },
    "baseScore": 3,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-23T22:35:20.381Z",
    "af": false
  },
  {
    "_id": "6hJHa958vqWTCshku",
    "postId": "zEB4xeTHxneHKtqSd",
    "parentCommentId": "BHvcCP6AW88wqptXh",
    "topLevelCommentId": "BHvcCP6AW88wqptXh",
    "contents": {
      "markdown": "lots of very important people spend all day being pestered by people due to their power/importance. at least some of them appreciate occasional interactions with people who just want to chat about something random like their bike seat",
      "plaintextDescription": "lots of very important people spend all day being pestered by people due to their power/importance. at least some of them appreciate occasional interactions with people who just want to chat about something random like their bike seat"
    },
    "baseScore": 62,
    "voteCount": 37,
    "createdAt": null,
    "postedAt": "2025-08-20T08:03:56.772Z",
    "af": false
  },
  {
    "_id": "BbFvefnwRydh3vvxT",
    "postId": "CqmTsC6AHqrNgoS3i",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "once, due to a huge traffic accident on the 101 that turned a sub-half-hour uber into a 1 hour 15 minute uber, i arrived at the airport about 20 minutes before departure (yes, departure, not \"gate closes\"). because the uber app operates on \"uber time\", which is entirely unmoored from reality, this meant that the arrival time estimate slipped out 5 minutes at a time. so i went through the whole range of emotions: whew, thankfully i have an hour buffer; hmm, i've never arrived 40 minutes before a flight it's going to be tight; holy shit i'm going to miss this flight with only 30 mins; holy shit even if i wasn't going to miss the flight before i definitely am now.\n\n(to make matters worse, not only was the flight not delayed, the departure time was changed to be 5 minutes earlier than originally planned. i didn't even know they could do that.)\n\ni somehow still managed to get on the flight, which departed as scheduled. still not sure that happened, but i think i learned something about how perseverance is important in the face of almost certain defeat, or something like that idk. 2/10 would not do again.",
      "plaintextDescription": "once, due to a huge traffic accident on the 101 that turned a sub-half-hour uber into a 1 hour 15 minute uber, i arrived at the airport about 20 minutes before departure (yes, departure, not \"gate closes\"). because the uber app operates on \"uber time\", which is entirely unmoored from reality, this meant that the arrival time estimate slipped out 5 minutes at a time. so i went through the whole range of emotions: whew, thankfully i have an hour buffer; hmm, i've never arrived 40 minutes before a flight it's going to be tight; holy shit i'm going to miss this flight with only 30 mins; holy shit even if i wasn't going to miss the flight before i definitely am now.\n\n(to make matters worse, not only was the flight not delayed, the departure time was changed to be 5 minutes earlier than originally planned. i didn't even know they could do that.)\n\ni somehow still managed to get on the flight, which departed as scheduled. still not sure that happened, but i think i learned something about how perseverance is important in the face of almost certain defeat, or something like that idk. 2/10 would not do again."
    },
    "baseScore": 11,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-15T21:42:53.411Z",
    "af": false
  },
  {
    "_id": "wpQJgbGeogqMtSHv3",
    "postId": "n6nsPzJWurKWKk2pA",
    "parentCommentId": "qQCYKWY7ckPZbxvSa",
    "topLevelCommentId": "qQCYKWY7ckPZbxvSa",
    "contents": {
      "markdown": "you can even indicate an exact line by making a larger or smaller dogear",
      "plaintextDescription": "you can even indicate an exact line by making a larger or smaller dogear"
    },
    "baseScore": 15,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-08-15T12:47:41.859Z",
    "af": false
  },
  {
    "_id": "5KjjBLrLYtMkp2NYT",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "in defense of putting your python imports in the middle of your file (in global scope, not inside functions)\n\n*   i have never in my life wanted to know the list of all the things a file imports before seeing any of the actual code in the file. if i see something i dont recognize, i would appreciate it more if the import were right above the usage; otherwise, i have to ctrl+f for it anyways. what's\n*   it's more annoying to have to add it to the top of the file. auto import things in ides are often broken.\n*   there's absolutely nothing wrong with importing something multiple times. it costs absolutely nothing; it's just a no-op the second time. even in C you can do `#pragma once` to get python-like behavior\n*   the only reason not to do this is that if you put an import in the middle of a function it's weird (if you import both blobally and locally, then using the thing locally but before the local import errors). so just don't put the import inside a function",
      "plaintextDescription": "in defense of putting your python imports in the middle of your file (in global scope, not inside functions)\n\n * i have never in my life wanted to know the list of all the things a file imports before seeing any of the actual code in the file. if i see something i dont recognize, i would appreciate it more if the import were right above the usage; otherwise, i have to ctrl+f for it anyways. what's\n * it's more annoying to have to add it to the top of the file. auto import things in ides are often broken.\n * there's absolutely nothing wrong with importing something multiple times. it costs absolutely nothing; it's just a no-op the second time. even in C you can do #pragma once to get python-like behavior\n * the only reason not to do this is that if you put an import in the middle of a function it's weird (if you import both blobally and locally, then using the thing locally but before the local import errors). so just don't put the import inside a function"
    },
    "baseScore": 11,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-14T06:08:01.775Z",
    "af": false
  },
  {
    "_id": "FsfNdWsXQdD92hSy6",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "kW2JehYDr6ChwQKuX",
    "topLevelCommentId": "DRkujxhixzjBaPG49",
    "contents": {
      "markdown": "my guess is sth like 1; I think some people are a lot more sensitive to some flavors than others. also the extent to which you pay attention can affect flavor a lot. and fwiw I frequently notice that some restaurants make the same dish a lot better than other restaurants, and the major ingredients must be about the same (or at least the quality/quantity difference is small enough that it's not the first thing I noticed), so it must be in the minor ingredients. but often my friends won't notice a big difference and conversely I don't notice a big difference in the foods they draw strong distinctions for\n\n(there are some really subtle flavors that I like, that are very subtle and which most places get wrong in ways that I can't put my finger on - e.g hainan chicken rice, gyu-don, edmonton style donair)",
      "plaintextDescription": "my guess is sth like 1; I think some people are a lot more sensitive to some flavors than others. also the extent to which you pay attention can affect flavor a lot. and fwiw I frequently notice that some restaurants make the same dish a lot better than other restaurants, and the major ingredients must be about the same (or at least the quality/quantity difference is small enough that it's not the first thing I noticed), so it must be in the minor ingredients. but often my friends won't notice a big difference and conversely I don't notice a big difference in the foods they draw strong distinctions for\n\n(there are some really subtle flavors that I like, that are very subtle and which most places get wrong in ways that I can't put my finger on - e.g hainan chicken rice, gyu-don, edmonton style donair)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-07T21:45:07.454Z",
    "af": false
  },
  {
    "_id": "XGGRPq29KuTGtJ2yJ",
    "postId": "oPqgSrfkTt2sCKM3e",
    "parentCommentId": "vu646uwk4ZrgMSSBN",
    "topLevelCommentId": "mD9fsEXXpdCKxw9h5",
    "contents": {
      "markdown": "am not at all involved in naming so not sure, but i'd guess none of it is intentional and more just the emergent result of compromise between a bunch of people / relatively little effort being expended on naming",
      "plaintextDescription": "am not at all involved in naming so not sure, but i'd guess none of it is intentional and more just the emergent result of compromise between a bunch of people / relatively little effort being expended on naming"
    },
    "baseScore": 4,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-05T19:19:14.261Z",
    "af": false
  },
  {
    "_id": "6i5vgtfkovraw8mu8",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "BoxpkAdBxX25vLWaF",
    "topLevelCommentId": "zCQS6mjS53CSiXYgM",
    "contents": {
      "markdown": "solution is very simple. administer alcohol before the test until everyone is exactly as clumsy as the NIST Standard Reference Clumsy Human for Accessibility Testing\n\n(or, rig the game, make a ledge that automatically pops out of the ground a split second before your foot hits it)",
      "plaintextDescription": "solution is very simple. administer alcohol before the test until everyone is exactly as clumsy as the NIST Standard Reference Clumsy Human for Accessibility Testing\n\n(or, rig the game, make a ledge that automatically pops out of the ground a split second before your foot hits it)"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-03T19:11:55.550Z",
    "af": false
  },
  {
    "_id": "zCQS6mjS53CSiXYgM",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "idea: survey people about whether 3^^^3 toe stubbings can be worse than torture, except with a twist: with 50% probability, arrange the furniture in the room such that people actually accidentally stub their toe right before answering the survey",
      "plaintextDescription": "idea: survey people about whether 3^^^3 toe stubbings can be worse than torture, except with a twist: with 50% probability, arrange the furniture in the room such that people actually accidentally stub their toe right before answering the survey"
    },
    "baseScore": 10,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-03T05:55:58.045Z",
    "af": false
  },
  {
    "_id": "DRkujxhixzjBaPG49",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "obviously there's also a lot of consumer demand, but I wonder how much of the trend towards food with less complicated ingredients being marketed with that as a major pro is because it's more technically impressive to accomplish (my layman understanding is that the easy way to make viable commercial food is to just toss in a bunch of preservatives and emulsifiers and stabilizers and you have a lot of margin for error, and avoiding them requires a lot of creativity in leveraging the specific properties of the food you're dealing with / modifying the packaging strategy to create a more elegant solution)",
      "plaintextDescription": "obviously there's also a lot of consumer demand, but I wonder how much of the trend towards food with less complicated ingredients being marketed with that as a major pro is because it's more technically impressive to accomplish (my layman understanding is that the easy way to make viable commercial food is to just toss in a bunch of preservatives and emulsifiers and stabilizers and you have a lot of margin for error, and avoiding them requires a lot of creativity in leveraging the specific properties of the food you're dealing with / modifying the packaging strategy to create a more elegant solution)"
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-01T20:40:54.979Z",
    "af": false
  },
  {
    "_id": "6MoeDQL4TNQWkPEn9",
    "postId": "xPrL2xF9iYWpPmu6B",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> If I were in her shoes, behaving the way she behaves, I would feel disgust toward *myself*.\n\n(tl;dr: you should be kind to yourself)\n\nsuppose you have some major flaw that you should fix; something you feel kind of vaguely bad about it, but somehow you just haven't ever managed to fix it. given that you being aware of it hasn't already just fixed it, it's probably not trivial to fix. now imagine someone (whose opinion you care a lot about) comes in and berates you for it. they call you an idiot, a fool, the entire circus--the whole works. they argue that you should obviously just do X (you've already tried to do X, but they don't know that, because they're so convinced that you're wrong that you can't get a word in edgewise).\n\nmaybe after thinking about it for a while you'd agree, but in that moment? you'd probably feel quite upset and defensive and annoyed. if you respect this person's opinion a lot, you might even feel ashamed or afraid. and however hard it usually is to make progress on improving, it is probably even harder in that moment.\n\nbeing disgusted at people for having a nail in their head is counterproductive to them taking the nail out. being disgusted at *yourself* for having a nail in your head is counterproductive to you taking the nail out of your own head.\n\nyou'd probably feel a lot better if someone sat down with you and first listened to all the things you had tried, and understood your confusion at why you find it so hard to fix something that seems easy to fix, etc. it's possible to still convey a sense of urgency and importance without meanness.\n\njust as you can treat other people this way, so too can you treat yourself this way; just as this is more effective to get other people to do what you want so too is this a better way to get yourself to do what you want.",
      "plaintextDescription": "> If I were in her shoes, behaving the way she behaves, I would feel disgust toward myself.\n\n(tl;dr: you should be kind to yourself)\n\nsuppose you have some major flaw that you should fix; something you feel kind of vaguely bad about it, but somehow you just haven't ever managed to fix it. given that you being aware of it hasn't already just fixed it, it's probably not trivial to fix. now imagine someone (whose opinion you care a lot about) comes in and berates you for it. they call you an idiot, a fool, the entire circus--the whole works. they argue that you should obviously just do X (you've already tried to do X, but they don't know that, because they're so convinced that you're wrong that you can't get a word in edgewise).\n\nmaybe after thinking about it for a while you'd agree, but in that moment? you'd probably feel quite upset and defensive and annoyed. if you respect this person's opinion a lot, you might even feel ashamed or afraid. and however hard it usually is to make progress on improving, it is probably even harder in that moment.\n\nbeing disgusted at people for having a nail in their head is counterproductive to them taking the nail out. being disgusted at yourself for having a nail in your head is counterproductive to you taking the nail out of your own head.\n\nyou'd probably feel a lot better if someone sat down with you and first listened to all the things you had tried, and understood your confusion at why you find it so hard to fix something that seems easy to fix, etc. it's possible to still convey a sense of urgency and importance without meanness.\n\njust as you can treat other people this way, so too can you treat yourself this way; just as this is more effective to get other people to do what you want so too is this a better way to get yourself to do what you want."
    },
    "baseScore": 16,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-07-31T08:32:14.176Z",
    "af": false
  },
  {
    "_id": "oL9yA3dBGEQZGX9PN",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "ZGGPxigsYP7upSiga",
    "topLevelCommentId": "yqD9WdzRidD5Qwycg",
    "contents": {
      "markdown": "It feels weird to me to treat longtermism as an ingroup/outgroup divider. I guess I think of myself as not really EA/longtermist. I mostly care about the medium-term glorious transhumanist future. I don't really base my actions on the core longtermist axiom; I only care about the unimaginably vast number of future moral patients indirectly, through caring about humanity being able to make and implement good moral decisions a hundred years from now. \n\nThe main thing I look at to determine whether someone is value-aligned with me is whether they care about making the future go well (in a vaguely ambitious transhumanist coded way), as opposed to personal wealth or degrowth whatever.",
      "plaintextDescription": "It feels weird to me to treat longtermism as an ingroup/outgroup divider. I guess I think of myself as not really EA/longtermist. I mostly care about the medium-term glorious transhumanist future. I don't really base my actions on the core longtermist axiom; I only care about the unimaginably vast number of future moral patients indirectly, through caring about humanity being able to make and implement good moral decisions a hundred years from now. \n\nThe main thing I look at to determine whether someone is value-aligned with me is whether they care about making the future go well (in a vaguely ambitious transhumanist coded way), as opposed to personal wealth or degrowth whatever."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-29T21:01:45.385Z",
    "af": false
  },
  {
    "_id": "AZQyNQKNjyHCNPTcB",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "yqD9WdzRidD5Qwycg",
    "topLevelCommentId": "yqD9WdzRidD5Qwycg",
    "contents": {
      "markdown": "why longtermist, as opposed to AI safety?",
      "plaintextDescription": "why longtermist, as opposed to AI safety?"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-29T07:11:17.705Z",
    "af": false
  },
  {
    "_id": "xw8yYemduX6BwLsfj",
    "postId": "KXr8ys8PYppKXgGWj",
    "parentCommentId": "fhsznRwG476rbA5SK",
    "topLevelCommentId": "fhsznRwG476rbA5SK",
    "contents": {
      "markdown": "this is not exactly what you're asking for, but german swaps the ones and tens digits in spoken language",
      "plaintextDescription": "this is not exactly what you're asking for, but german swaps the ones and tens digits in spoken language "
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-28T02:46:47.112Z",
    "af": false
  },
  {
    "_id": "hkpwGbfMhoGRpNiay",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "Kyh7wPFnaDAbCJMQz",
    "topLevelCommentId": "QeHXNuBRHmrbHEfFM",
    "contents": {
      "markdown": "yeah, that was an intentional feature of the example chosen. i'd guess most people who are aware of this fact do still think of helicopter as a single unit in their head unless they choose to decompose it, because you hear each of these words often enough that you don't really need to work out the meaning from etymology. and so if pter were pronounced the way it currently is when in helicopter but pronounced as \"peter\" in pterodactyl it would not actually be that much more confusing than it currently is.",
      "plaintextDescription": "yeah, that was an intentional feature of the example chosen. i'd guess most people who are aware of this fact do still think of helicopter as a single unit in their head unless they choose to decompose it, because you hear each of these words often enough that you don't really need to work out the meaning from etymology. and so if pter were pronounced the way it currently is when in helicopter but pronounced as \"peter\" in pterodactyl it would not actually be that much more confusing than it currently is."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-27T21:57:17.795Z",
    "af": false
  },
  {
    "_id": "QeHXNuBRHmrbHEfFM",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "(this is based on / expanded from a response I wrote to a tweet that was talking about how autistic people struggle in the world because the world follows unwritten rules that are more important than the written ones.)\n\nI think most autistic people should invest more in understanding the unwritten rules. it can be cruel and unfair, but it's important to know how to interact with it. and it's actually a really interesting system to map out, with its own rhyme and reason.\n\nit's entirely understandable that people feel burned by bad past experiences, and to have learned helplessness from bullying or other unfair treatment. this kind of thing leaves a scar and can make it feel viscerally hopeless.\n\nbut it still feels defeatist to just throw up one's hands and say \"it's too complicated.\" yes, it's complicated and fuzzy and initially unintuitive and takes years to master. so is ML research. the point of being intelligent is that you are good at finding patterns and learning things, and there's nothing truly fundamentally different about the unwritten rules of social interaction.\n\nI see people taking examples of weird unintuitive social rules all the time and, tbh, none of them are truly that complicated compared to like, language learning, or ML stuff? like, memorizing lots of arbitrary rules is just part of learning literally any intellectual discipline. not to mention that there often is actually underlying structure that you can reason about.\n\nimo one other reason this happens is that complaining is fun! a bit of a digression: to build on the language learning analogy, I love to complain about e.g how deranged the kanji system is in Japanese. it makes for an entertaining story to explain that each kanji maps onto between two and a zillion very distinct possible pronunciations, and that it's entirely context dependent on the phrase, with lots of strange exceptions and so on that you just kind of have to memorize. but honestly, when I reflect on my experience as a Chinese speaker, I notice that that even though Chinese characters, unlike Japanese, map to a unique pronunciation 99% of the time, I still think of the conceptual atomic unit of Chinese as being groups of characters, and it's actually really easy for me to forget that two different words actually contain the same character. (like imagine if \"pter\" were a single character in words like helicopter and pterodactyl both contain \"pter\", but you'd probably think of \"helicopter\" as an atomic unit with its own unique identity). \n\nso while it's fun to complain about Japanese, it actually isn't as bad as it sounds to learn it, and it actually does fit the shape of the human brain quite well. i think this is basically the right way to think about learning unwritten rules in social interaction as well. it takes lots of effort and practice, some things like accents are a ton of work to fix, and it's obviously hard to become as fluent in it as a native speaker, but you can get pretty damn good even if you get started later in life if you truly care a lot.",
      "plaintextDescription": "(this is based on / expanded from a response I wrote to a tweet that was talking about how autistic people struggle in the world because the world follows unwritten rules that are more important than the written ones.)\n\nI think most autistic people should invest more in understanding the unwritten rules. it can be cruel and unfair, but it's important to know how to interact with it. and it's actually a really interesting system to map out, with its own rhyme and reason.\n\nit's entirely understandable that people feel burned by bad past experiences, and to have learned helplessness from bullying or other unfair treatment. this kind of thing leaves a scar and can make it feel viscerally hopeless.\n\nbut it still feels defeatist to just throw up one's hands and say \"it's too complicated.\" yes, it's complicated and fuzzy and initially unintuitive and takes years to master. so is ML research. the point of being intelligent is that you are good at finding patterns and learning things, and there's nothing truly fundamentally different about the unwritten rules of social interaction.\n\nI see people taking examples of weird unintuitive social rules all the time and, tbh, none of them are truly that complicated compared to like, language learning, or ML stuff? like, memorizing lots of arbitrary rules is just part of learning literally any intellectual discipline. not to mention that there often is actually underlying structure that you can reason about.\n\nimo one other reason this happens is that complaining is fun! a bit of a digression: to build on the language learning analogy, I love to complain about e.g how deranged the kanji system is in Japanese. it makes for an entertaining story to explain that each kanji maps onto between two and a zillion very distinct possible pronunciations, and that it's entirely context dependent on the phrase, with lots of strange exceptions and so on that you just kind of have to memorize. but honestly, when I reflect on my experience as a Chines"
    },
    "baseScore": 32,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2025-07-27T11:48:58.455Z",
    "af": false
  },
  {
    "_id": "isFP9TrFSKAN9LsrT",
    "postId": "4oCh3x6EPHomEbcDJ",
    "parentCommentId": "r8nGLRS2j4dENsxsA",
    "topLevelCommentId": "hnqb7CwkasbGnssY2",
    "contents": {
      "markdown": "I want to defend interp as a reasonable thing for one to do for superintelligence alignment, to the extent that one believes there is any object level work of value to do right now. (maybe there isn't, and everyone should go do field building or something. no strong takes rn.) I've become more pessimistic about the weird alignment theory over time and I think it's doomed just like how most theory work in ML is doomed (and at least ML theorists can test their theories against real NNs, if they so choose! alignment theory has no AGI to test against.)\n\nI don't really buy that interp (specifically ambitious mechinterp, the project of fully understanding exactly how neural networks work down to the last gear) has been that useful for capabilities insights to date. fmpov, the process that produces useful capabilities insights generally operates at a different level of abstraction than mechinterp operates at. I can't talk about current examples for obvious reasons but I can talk about historical ones. with Chinchilla, it fixes a mistake in the Kaplan paper token budget methodology that's obvious in hindsight; momentum and LR decay, which have been around for decades, are based on intuitive arguments from classic convex optimization; transformers came about by reasoning about the shape and trajectory of computers and trying to parallelize things as much as possible. also, a lot of stuff Just Works and nobody knows why.\n\none analogy that comes to mind is if your goal is to make your country's economy go well, it certainly can't hurt to become really good friends with a random subset of the population to understand everything they do. you'll learn things about how they respond to price changes or whether they'd be more efficient with better healthcare or whatever. but it's probably a much much higher priority for you to understand how economies respond to the interest rate, or tariffs, or job programs, or so on, and you want to think of people as crowds of *homo economicus* with preference curves modeled as a few simple splines or something.\n\nas interp starts actually working it might generate actual capabilities insights. I don't feel confident claiming this will never happen. but it feels marginal and likely substantially less efficient than just directly working on capabilities. (it's hard to accidentally advance a field if you're not even trying to advance it and a hundred incredibly smart and well resources people are poking at it!)",
      "plaintextDescription": "I want to defend interp as a reasonable thing for one to do for superintelligence alignment, to the extent that one believes there is any object level work of value to do right now. (maybe there isn't, and everyone should go do field building or something. no strong takes rn.) I've become more pessimistic about the weird alignment theory over time and I think it's doomed just like how most theory work in ML is doomed (and at least ML theorists can test their theories against real NNs, if they so choose! alignment theory has no AGI to test against.)\n\nI don't really buy that interp (specifically ambitious mechinterp, the project of fully understanding exactly how neural networks work down to the last gear) has been that useful for capabilities insights to date. fmpov, the process that produces useful capabilities insights generally operates at a different level of abstraction than mechinterp operates at. I can't talk about current examples for obvious reasons but I can talk about historical ones. with Chinchilla, it fixes a mistake in the Kaplan paper token budget methodology that's obvious in hindsight; momentum and LR decay, which have been around for decades, are based on intuitive arguments from classic convex optimization; transformers came about by reasoning about the shape and trajectory of computers and trying to parallelize things as much as possible. also, a lot of stuff Just Works and nobody knows why.\n\none analogy that comes to mind is if your goal is to make your country's economy go well, it certainly can't hurt to become really good friends with a random subset of the population to understand everything they do. you'll learn things about how they respond to price changes or whether they'd be more efficient with better healthcare or whatever. but it's probably a much much higher priority for you to understand how economies respond to the interest rate, or tariffs, or job programs, or so on, and you want to think of people as crowds of homo economicus wit"
    },
    "baseScore": 14,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-07-26T05:47:39.025Z",
    "af": false
  },
  {
    "_id": "HHK5a94S5Pp83BzEQ",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "everyone is a few hops away from everyone else. this applies in both directions: when I meet random people they always have some weak connection to other people I know, but also when I think of a collection of people as a cluster, most specific pairs of people within that cluster barely know each other except through other people in the cluster.",
      "plaintextDescription": "everyone is a few hops away from everyone else. this applies in both directions: when I meet random people they always have some weak connection to other people I know, but also when I think of a collection of people as a cluster, most specific pairs of people within that cluster barely know each other except through other people in the cluster."
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-07-26T05:23:40.620Z",
    "af": false
  },
  {
    "_id": "6cp64cgNzNYBxvFRu",
    "postId": "pzChyAuu7ggmLd75B",
    "parentCommentId": "WZbrmW9bQaQCd3qSm",
    "topLevelCommentId": "WZbrmW9bQaQCd3qSm",
    "contents": {
      "markdown": "i think \"build out infrastructure\" is hugely overrated in research. for example, the existing codebases for SAEs (training, activation caching, autointerp) are often actively worse than useless, such that i would rather spend a weekend rewriting it from scratch than work within them. in general i think people should throw out and rewrite research infra much more often than they do. not saying truly good research infrastructure can't exist, in theory, just that empirically people really suck at making good reusable infrastructure.",
      "plaintextDescription": "i think \"build out infrastructure\" is hugely overrated in research. for example, the existing codebases for SAEs (training, activation caching, autointerp) are often actively worse than useless, such that i would rather spend a weekend rewriting it from scratch than work within them. in general i think people should throw out and rewrite research infra much more often than they do. not saying truly good research infrastructure can't exist, in theory, just that empirically people really suck at making good reusable infrastructure."
    },
    "baseScore": 13,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-07-23T23:04:00.783Z",
    "af": false
  },
  {
    "_id": "yzJXYwpS9jjLxTFa6",
    "postId": "s58hDHX2GkFDbpGKD",
    "parentCommentId": "FPdmg2sMLGkQsEdFt",
    "topLevelCommentId": "vSFdLhaacaYGYL5uK",
    "contents": {
      "markdown": "if only his friend were the great scientist Hamming, who would blame him for not working on the most important problem in the field",
      "plaintextDescription": "if only his friend were the great scientist Hamming, who would blame him for not working on the most important problem in the field"
    },
    "baseScore": 14,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-07-21T23:37:16.309Z",
    "af": false
  },
  {
    "_id": "Rq5FvYxePcxvteKPE",
    "postId": "s58hDHX2GkFDbpGKD",
    "parentCommentId": "vSFdLhaacaYGYL5uK",
    "topLevelCommentId": "vSFdLhaacaYGYL5uK",
    "contents": {
      "markdown": "not igniting the atmosphere was probably the most important problem in the field at that moment. why wasn't he working on it?",
      "plaintextDescription": "not igniting the atmosphere was probably the most important problem in the field at that moment. why wasn't he working on it?"
    },
    "baseScore": 57,
    "voteCount": 31,
    "createdAt": null,
    "postedAt": "2025-07-21T05:24:00.066Z",
    "af": false
  },
  {
    "_id": "Mojmzmcztagx4WSJz",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "random thoughts on analytical and emotional intelligence \n\none thing that I think the world needs more of is analyses into the nature of the mind by people who are both rigorous/analytically inclined, and also emotionally intelligent/integrated. much writing from the former fails to model large parts of the human mind, and much writing from the latter fails to create models of sufficient clarity and validity. \n\nI think this underlies a lot of my instinctive dislike of humanities work. people who are emotionally perceptive but not rigorous and analytical tend to notice interesting things about the human experience, but then come up with very poor models that set off all of my bullshit sensors that are attuned to rigorous arguments. but I think it should be possible to have humanities work that is not like this.\n\n(for clarity, from here out I will say analytical and emotional to refer to the axes which are independent of each other, and ABNE (analytically but not emotionally intelligent) and EBNA for the converse)\n\n(I also want to clarify that I don't think of analytical as being in opposition to intuition, at least in the context of this post. something something Terence Tao's post about how the best mathematicians start out thinking in rigor before developing the intuitions to think without applying rigor all the time, but their intuitions check out rigorously when needed)\n\nbecause there's a strong anti correlation between analytical inclination and emotional integration, it's easy to round this off to a single axis. but I think this is too oversimplifying.\n\nanalytical people like to construct typologies and categorizations that cleanly describe the world. edge cases are very important because in a lawful world, thinking about the edge cases teaches you a lot about the laws of the world, which in turn gives you deep understanding that is surprising but robust (physics is the poster child for this worldview). analytical people are very aware that it's easy to make theories that sound nice but aren't actually good; it's important to have rigorous standards for testing theories; indeed, it's important to have rigorous meta-theory (epistemology) for what counts as an actual rigorous standard and not just Rigorous Vibes. analytical people care a lot about subtle but important distinctions - for example, correlation vs causality; or thinking of high dimensional data as having large principal components vs fundamentally low dimensional data being embedded in high dimensional space; or true anticorrelations vs collider bias; or the distinction between unmodelability and noise; or biased vs high variance estimators. in some sense the Sequences is a compendium of examples from the analytical worldview.\n\nemotionally intelligent people tend to be good at modelling themselves and the people around them. they can sense subtle social cues. they can notice that certain attributes or actions of people reveal much about their internal state, that ABNE people would think of as noise. they tend towards the arts and forms of creative expression that communicate and allow empathizing with emotional state. they are attuned to narratives and worldviews as a thing that shapes the actions of individuals and groups. they have fewer unresolved internal conflicts between their parts. they can spot emotional/motivational contradictions in the minds of other people that end up driving actions; this is inferred not just from what others say, but from what they don't say, or what they hesitate to say.\n\nABNE people tend to think of EBNA people as dumb or irrational. they think of EBNA people as snobbish about the humanities and their ideas as insight porn, the result of an ingroup signalling game gone awry. ABNE people tend to construct sophisticated models of themselves and other people that lack a huge chunk of important inputs, making certain kinds of things really hard to model, and requiring way more intellectual capability to achieve the same accuracy as emotionally intelligent people. this frequently leads to large swathes of human behavior being changed chalked up to noise, or irrationality, or even completely giving up on modelling entire facets of humanity. ABNE people tend to have major issues with internal coherency between their parts.\n\nEBNA people like to think of ABNE people as boring nerdy nitpickers. they spot the internal incoherency of ABNE people but undervalue the analytical component. oftentimes they phrase beliefs in forms that contain some important true observation, but can't put it in the form of something rigorous and testable, and thereby gets lost in the oceans of insight slop sloshing about in the world. EBNA people notice lots of signals but fail to put them together into a coherent, effective worldview, and instead coarsely clusters observations together into simple heuristics.\n\nthis post is unapologetically written as an analytical way of trying to get the best of both worlds, because this is how I think natively. perhaps in the future I'll try to make a more balanced / broadly digestible version.",
      "plaintextDescription": "random thoughts on analytical and emotional intelligence \n\none thing that I think the world needs more of is analyses into the nature of the mind by people who are both rigorous/analytically inclined, and also emotionally intelligent/integrated. much writing from the former fails to model large parts of the human mind, and much writing from the latter fails to create models of sufficient clarity and validity. \n\nI think this underlies a lot of my instinctive dislike of humanities work. people who are emotionally perceptive but not rigorous and analytical tend to notice interesting things about the human experience, but then come up with very poor models that set off all of my bullshit sensors that are attuned to rigorous arguments. but I think it should be possible to have humanities work that is not like this.\n\n(for clarity, from here out I will say analytical and emotional to refer to the axes which are independent of each other, and ABNE (analytically but not emotionally intelligent) and EBNA for the converse)\n\n(I also want to clarify that I don't think of analytical as being in opposition to intuition, at least in the context of this post. something something Terence Tao's post about how the best mathematicians start out thinking in rigor before developing the intuitions to think without applying rigor all the time, but their intuitions check out rigorously when needed)\n\nbecause there's a strong anti correlation between analytical inclination and emotional integration, it's easy to round this off to a single axis. but I think this is too oversimplifying.\n\nanalytical people like to construct typologies and categorizations that cleanly describe the world. edge cases are very important because in a lawful world, thinking about the edge cases teaches you a lot about the laws of the world, which in turn gives you deep understanding that is surprising but robust (physics is the poster child for this worldview). analytical people are very aware that it's easy to make th"
    },
    "baseScore": 72,
    "voteCount": 29,
    "createdAt": null,
    "postedAt": "2025-07-20T01:48:55.988Z",
    "af": false
  },
  {
    "_id": "jkFHuDwZee7Frt6wp",
    "postId": "3xs67kNbeYgytgETb",
    "parentCommentId": "f59nujqkzXP5kEHEu",
    "topLevelCommentId": "wSWAsKbKTkGurdRNW",
    "contents": {
      "markdown": "it's also worth noting that I am far in the tail ends of the distribution of people willing to ignore incentive gradients if I believe it's correct not to follow them. (I've gotten somewhat more pragmatic about this over time, because sometimes not following the gradient is just dumb. and as a human being it's impossible not to care a little bit about status and money and such. but I still have a very strong tendency to ignore local incentives if I believe something is right in the long run.) like I'm aware I'll get promoed less and be viewed as less cool and not get as much respect and so on if I do the alignment work I think is genuinely important in the long run. \n\nI'd guess for most people, the disincentives for working on xrisk alignment make openai a vastly less pleasant place. so whenever I say I don't feel like I'm pressured not to do what I'm doing, this does not necessarily mean the average person at openai would agree if they tried to work on my stuff.",
      "plaintextDescription": "it's also worth noting that I am far in the tail ends of the distribution of people willing to ignore incentive gradients if I believe it's correct not to follow them. (I've gotten somewhat more pragmatic about this over time, because sometimes not following the gradient is just dumb. and as a human being it's impossible not to care a little bit about status and money and such. but I still have a very strong tendency to ignore local incentives if I believe something is right in the long run.) like I'm aware I'll get promoed less and be viewed as less cool and not get as much respect and so on if I do the alignment work I think is genuinely important in the long run. \n\nI'd guess for most people, the disincentives for working on xrisk alignment make openai a vastly less pleasant place. so whenever I say I don't feel like I'm pressured not to do what I'm doing, this does not necessarily mean the average person at openai would agree if they tried to work on my stuff."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-18T08:09:36.735Z",
    "af": false
  },
  {
    "_id": "af5iumcRHSkGZGCoT",
    "postId": "3xs67kNbeYgytgETb",
    "parentCommentId": "x9eaBcuxv5kCunvvi",
    "topLevelCommentId": "wSWAsKbKTkGurdRNW",
    "contents": {
      "markdown": "I acknowledge that I probably have an unusual experience among people working on xrisk things at openai. From what I've heard from other people I trust, there probably have been a bunch of cases where someone was genuinely blocked from publishing something about xrisk, and I just happen to have gotten lucky so far.",
      "plaintextDescription": "I acknowledge that I probably have an unusual experience among people working on xrisk things at openai. From what I've heard from other people I trust, there probably have been a bunch of cases where someone was genuinely blocked from publishing something about xrisk, and I just happen to have gotten lucky so far."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-18T08:00:48.732Z",
    "af": false
  },
  {
    "_id": "yLwws5BYzg54LMRGR",
    "postId": "3xs67kNbeYgytgETb",
    "parentCommentId": "tK2dTCX4BpQY2aZ3G",
    "topLevelCommentId": "wSWAsKbKTkGurdRNW",
    "contents": {
      "markdown": "i wouldn't comment this confidently if i didn't",
      "plaintextDescription": "i wouldn't comment this confidently if i didn't"
    },
    "baseScore": 15,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-07-18T01:05:11.233Z",
    "af": false
  },
  {
    "_id": "QEHbWGmeGofgX5kME",
    "postId": "3xs67kNbeYgytgETb",
    "parentCommentId": "L2uG2AebGZ5dDSRRD",
    "topLevelCommentId": "wSWAsKbKTkGurdRNW",
    "contents": {
      "markdown": "openai explicitly encourages safety work that also is useful for capabilities. people at oai think of it as a positive attribute when safety work also helps with capabilities, and are generally confused when i express the view that not advancing capabilities is a desirable attribute of doing safety.\n\ni think we as a community has a definition of the word safety that diverges more from the layperson definition than the openai definition does. i think our definition is more *useful* to focus on for making the future go well, but i wouldn't say it's the most accepted one.\n\ni think openai deeply believes that doing things in the real world is more important than publishing academic things. so people get rewarded for putting interventions in the world than putting papers in the hands of academics.",
      "plaintextDescription": "openai explicitly encourages safety work that also is useful for capabilities. people at oai think of it as a positive attribute when safety work also helps with capabilities, and are generally confused when i express the view that not advancing capabilities is a desirable attribute of doing safety.\n\ni think we as a community has a definition of the word safety that diverges more from the layperson definition than the openai definition does. i think our definition is more useful to focus on for making the future go well, but i wouldn't say it's the most accepted one.\n\ni think openai deeply believes that doing things in the real world is more important than publishing academic things. so people get rewarded for putting interventions in the world than putting papers in the hands of academics."
    },
    "baseScore": 22,
    "voteCount": 14,
    "createdAt": null,
    "postedAt": "2025-07-17T23:41:11.124Z",
    "af": false
  },
  {
    "_id": "f59nujqkzXP5kEHEu",
    "postId": "3xs67kNbeYgytgETb",
    "parentCommentId": "3GbyFCpmfsySAbkiw",
    "topLevelCommentId": "wSWAsKbKTkGurdRNW",
    "contents": {
      "markdown": "i have never experienced pushback when publishing research that draws attention to xrisk. it's more that people are not incentivized to work on xrisk research in the first place. also, for mundane safety work, my guess is that modern openai just values shipping things into prod a lot more than writing papers.",
      "plaintextDescription": "i have never experienced pushback when publishing research that draws attention to xrisk. it's more that people are not incentivized to work on xrisk research in the first place. also, for mundane safety work, my guess is that modern openai just values shipping things into prod a lot more than writing papers."
    },
    "baseScore": 21,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-07-17T23:35:52.942Z",
    "af": false
  },
  {
    "_id": "cHRvnLNQvSLc5eJPd",
    "postId": "3xs67kNbeYgytgETb",
    "parentCommentId": "wSWAsKbKTkGurdRNW",
    "topLevelCommentId": "wSWAsKbKTkGurdRNW",
    "contents": {
      "markdown": "most of the x-risk relevant research done at openai is published? the stuff that's not published is usually more on the practical risks side. there just isn't that much xrisk stuff, period.",
      "plaintextDescription": "most of the x-risk relevant research done at openai is published? the stuff that's not published is usually more on the practical risks side. there just isn't that much xrisk stuff, period."
    },
    "baseScore": 49,
    "voteCount": 21,
    "createdAt": null,
    "postedAt": "2025-07-17T23:33:43.422Z",
    "af": false
  },
  {
    "_id": "tkbCmbb7EuhQaQKzq",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "wGLyFKpMD8vFgDmsC",
    "topLevelCommentId": "6wqrugJKDvta9yceh",
    "contents": {
      "markdown": "well, in academia, if you do quality work anyways and ignore incentives, you'll get a lot less funding to do that quality work, and possibly perish.\n\nunfortunately, academia is not a sufficiently well designed system to extract useful work out of grifters.",
      "plaintextDescription": "well, in academia, if you do quality work anyways and ignore incentives, you'll get a lot less funding to do that quality work, and possibly perish.\n\nunfortunately, academia is not a sufficiently well designed system to extract useful work out of grifters."
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-07-17T23:00:38.071Z",
    "af": false
  },
  {
    "_id": "6wqrugJKDvta9yceh",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "i think of the idealized platonic researcher as the person who has chosen ultimate (intellectual) freedom over all else. someone who really cares about some particular thing that nobody else does - maybe because they see the future before anyone else does, or maybe because they just really like understanding everything about ants or abstract mathematical objects or something. in exchange for the ultimate intellectual freedom, they give up vast amounts of money, status, power, etc.\n\none thing that makes me sad is that modern academia is, as far as I can tell, not this. when you opt out of the game of the Economy, in exchange for giving up real money, status, and power, what you get from Academia is another game of money, status, and power, with different rules, and much lower stakes, and also everyone is more petty about everything.\n\nat the end of the day, what's even the point of all this? to me, it feels like sacrificing everything for nothing if you eschew money, status, and power, and then just write a terrible irreplicable p-hacked paper that reduces the net amount of human knowledge by adding noise and advances your career so you can do more terrible useless papers. at that point, why not just leave academia and go to industry and do something equally useless for human knowledge but get paid stacks of cash for it?\n\nofc there are people in academia who do good work but it often feels like the incentives force most work to be this kind of horrible slop.",
      "plaintextDescription": "i think of the idealized platonic researcher as the person who has chosen ultimate (intellectual) freedom over all else. someone who really cares about some particular thing that nobody else does - maybe because they see the future before anyone else does, or maybe because they just really like understanding everything about ants or abstract mathematical objects or something. in exchange for the ultimate intellectual freedom, they give up vast amounts of money, status, power, etc.\n\none thing that makes me sad is that modern academia is, as far as I can tell, not this. when you opt out of the game of the Economy, in exchange for giving up real money, status, and power, what you get from Academia is another game of money, status, and power, with different rules, and much lower stakes, and also everyone is more petty about everything.\n\nat the end of the day, what's even the point of all this? to me, it feels like sacrificing everything for nothing if you eschew money, status, and power, and then just write a terrible irreplicable p-hacked paper that reduces the net amount of human knowledge by adding noise and advances your career so you can do more terrible useless papers. at that point, why not just leave academia and go to industry and do something equally useless for human knowledge but get paid stacks of cash for it?\n\nofc there are people in academia who do good work but it often feels like the incentives force most work to be this kind of horrible slop."
    },
    "baseScore": 44,
    "voteCount": 31,
    "createdAt": null,
    "postedAt": "2025-07-17T05:24:41.564Z",
    "af": true
  },
  {
    "_id": "AsHLT4eEAnshBtYMx",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "JvfbqhYbRvWi7yrdv",
    "topLevelCommentId": "pogcgTjhTKxbiErDj",
    "contents": {
      "markdown": "in my worldview this is very easily explained. if you do Jungian therapy your self model starts incorporating Jungian concepts for explaining your own brain. You didn't change the way your brain works fundamentally, you just changed your own model of your brain. The same way that if you read a book on the biology of plants you'll start viewing them in the lens of cells, and if you read a book on the ancient spirits associated with each plant you'll start thinking of plants as being animated by the ghosts of our ancestors.\n\nThe big mistake happens when people think of their self model as actually genuinely introspection. Then, you might think that you've changed the shape of your mind instead of only changing your understanding of your mind.\n\nInstead, I think the right way to figure out if your self model is correct is to make predictions about your future behavior and see if they come true; act based on your self model and see if you become more successful at life, or whether you mysteriously repeatedly fail in some way.",
      "plaintextDescription": "in my worldview this is very easily explained. if you do Jungian therapy your self model starts incorporating Jungian concepts for explaining your own brain. You didn't change the way your brain works fundamentally, you just changed your own model of your brain. The same way that if you read a book on the biology of plants you'll start viewing them in the lens of cells, and if you read a book on the ancient spirits associated with each plant you'll start thinking of plants as being animated by the ghosts of our ancestors.\n\nThe big mistake happens when people think of their self model as actually genuinely introspection. Then, you might think that you've changed the shape of your mind instead of only changing your understanding of your mind.\n\nInstead, I think the right way to figure out if your self model is correct is to make predictions about your future behavior and see if they come true; act based on your self model and see if you become more successful at life, or whether you mysteriously repeatedly fail in some way."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-17T05:07:01.631Z",
    "af": false
  },
  {
    "_id": "dHJtetLnLAitraHug",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "8i9wccjEXsgQNsuvs",
    "topLevelCommentId": "2AbFmRkGF9TzXrrAt",
    "contents": {
      "markdown": "i haven't looked into this deeply but how strong is the evidence for (lack of) oxidative damage? the SSC post is somewhat unsatisfying in that it doesn't really consider outcomes other than literal Parkinson's, and just kind of says the animal model results are confusion.",
      "plaintextDescription": "i haven't looked into this deeply but how strong is the evidence for (lack of) oxidative damage? the SSC post is somewhat unsatisfying in that it doesn't really consider outcomes other than literal Parkinson's, and just kind of says the animal model results are confusion."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-16T01:04:57.217Z",
    "af": false
  },
  {
    "_id": "NLJCjhchAsfZXAqGY",
    "postId": "EYc9sFizBaEgAsFNW",
    "parentCommentId": "AFmN8KSD2PKhgJooa",
    "topLevelCommentId": "AFmN8KSD2PKhgJooa",
    "contents": {
      "markdown": "i agree hiring lots of people is rarely the solution, especially for this type of problem. adding more people to a team actually slows it down at some point. generally, you want a small team of extremely competent people.",
      "plaintextDescription": "i agree hiring lots of people is rarely the solution, especially for this type of problem. adding more people to a team actually slows it down at some point. generally, you want a small team of extremely competent people."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-14T21:14:10.601Z",
    "af": false
  },
  {
    "_id": "2AbFmRkGF9TzXrrAt",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "when people say that (prescription) amphetamines \"borrow from the future\", is there strong evidence on this? with Ozempic we've observed that people are heavily biased against things that feel like a free win, so the tradeoff narrative is memetically fit. distribution shift from ancestral environment means algernon need not apply",
      "plaintextDescription": "when people say that (prescription) amphetamines \"borrow from the future\", is there strong evidence on this? with Ozempic we've observed that people are heavily biased against things that feel like a free win, so the tradeoff narrative is memetically fit. distribution shift from ancestral environment means algernon need not apply"
    },
    "baseScore": 25,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2025-07-14T20:51:37.469Z",
    "af": false
  },
  {
    "_id": "nai6aeiGjCqpoEFRA",
    "postId": "7Q7DPSk4iGFJd8DRk",
    "parentCommentId": "XBcxPdbpFihLrWDYD",
    "topLevelCommentId": "DKGcHGHPtCkQ2jA6C",
    "contents": {
      "markdown": "glad it was helpful!",
      "plaintextDescription": "glad it was helpful! "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-14T20:40:48.897Z",
    "af": false
  },
  {
    "_id": "a9AH7iXWe5zQ4vdmC",
    "postId": "7Q7DPSk4iGFJd8DRk",
    "parentCommentId": "wRBuaEmBjYG5apsDi",
    "topLevelCommentId": "FKvd7Ai6uLQMfJGBz",
    "contents": {
      "markdown": "319,892",
      "plaintextDescription": "319,892"
    },
    "baseScore": 11,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-14T08:59:38.995Z",
    "af": false
  },
  {
    "_id": "oyxPCdxGY4SmpBEyj",
    "postId": "DwqxPmNL3aXGZDPkT",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "by coincidence, i independently happened to test something very similar on chatgpt 4.5 earlier today and got similar results\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/056041afcf90f8a3871679514df1ed157dd72df0cfa07019.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/96d1da22bb1f51fddddf03a234a2f81c49323561c3f77477.png)",
      "plaintextDescription": "by coincidence, i independently happened to test something very similar on chatgpt 4.5 earlier today and got similar results"
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-07-14T06:31:27.598Z",
    "af": false
  },
  {
    "_id": "GxbwqxjDmN4xEGTJe",
    "postId": "cxuzALcmucCndYv4a",
    "parentCommentId": "LHD69yQk7JAjcW4Yd",
    "topLevelCommentId": "LHD69yQk7JAjcW4Yd",
    "contents": {
      "markdown": "i think the exodus was not literally inevitable, but it would have required a heroic effort to prevent. imo the two biggest causes of the exodus were the board coup and the implosion of superalignment (which was indirectly caused by the coup).\n\nmy guess is there will be some people who take alignment people less seriously in long timelines because of AI 2027. i would not measure this by how loudly political opponents dunk on alignment people, because they will always find something to dunk on. i think the best way to counteract this is to emphasize the principle component that this whole AI thing is really big deal, and that there is a very wide range of beliefs in the field, but even \"long\" timeline worlds are insane as hell compared to what everyone else expects. i'm biased, though, because i think sth like 2035 is a more realistic median world; if i believed AGI was 50% likely to happen by 2029 or something then i might behave very diffrently",
      "plaintextDescription": "i think the exodus was not literally inevitable, but it would have required a heroic effort to prevent. imo the two biggest causes of the exodus were the board coup and the implosion of superalignment (which was indirectly caused by the coup).\n\nmy guess is there will be some people who take alignment people less seriously in long timelines because of AI 2027. i would not measure this by how loudly political opponents dunk on alignment people, because they will always find something to dunk on. i think the best way to counteract this is to emphasize the principle component that this whole AI thing is really big deal, and that there is a very wide range of beliefs in the field, but even \"long\" timeline worlds are insane as hell compared to what everyone else expects. i'm biased, though, because i think sth like 2035 is a more realistic median world; if i believed AGI was 50% likely to happen by 2029 or something then i might behave very diffrently"
    },
    "baseScore": 6,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-07-12T23:21:11.174Z",
    "af": false
  },
  {
    "_id": "SGfLmrDvCtaWfZwat",
    "postId": "5NF7DRvcLLGHn78bT",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "also, for posterity / future historians, here is some (public, non-exhaustive, from my recollection) info on somewhat more obscure historical openai model naming around the 2020-2022 era:\n\n*   a very useful (non exhaustive) resource is the \"Model index for researchers\" post which no longer exists but which is thankfully archived: [https://archive.is/XDCN0](https://archive.is/XDCN0)\n*   the GPT-3 series was launched as ada, babbage, curie, davinci. davinci was the full size 175B and the others were smaller models (see [https://blog.eleuther.ai/gpt3-model-sizes/](https://blog.eleuther.ai/gpt3-model-sizes/)) )\n*   a cushman model was very briefly made available but then disappeared forever. my memory is a bit fuzzy but i believe this is separate from the code-cushman-001 mentioned in the model index\n*   the first GPT-3 instruction following model was launched as davinci-instruct-beta, subsequent IF models were text-davinci-001 (GPT-3), text-davinci-002 (GPT-3.5), text-davinci-003 (GPT-3.5)\n*   the name GPT-3.5 was not public until the existence of this index. so nobody really knew that text-davinci-002 was no longer GPT-3 until then",
      "plaintextDescription": "also, for posterity / future historians, here is some (public, non-exhaustive, from my recollection) info on somewhat more obscure historical openai model naming around the 2020-2022 era:\n\n * a very useful (non exhaustive) resource is the \"Model index for researchers\" post which no longer exists but which is thankfully archived: https://archive.is/XDCN0\n * the GPT-3 series was launched as ada, babbage, curie, davinci. davinci was the full size 175B and the others were smaller models (see https://blog.eleuther.ai/gpt3-model-sizes/ )\n * a cushman model was very briefly made available but then disappeared forever. my memory is a bit fuzzy but i believe this is separate from the code-cushman-001 mentioned in the model index\n * the first GPT-3 instruction following model was launched as davinci-instruct-beta, subsequent IF models were text-davinci-001 (GPT-3), text-davinci-002 (GPT-3.5), text-davinci-003 (GPT-3.5)\n * the name GPT-3.5 was not public until the existence of this index. so nobody really knew that text-davinci-002 was no longer GPT-3 until then"
    },
    "baseScore": 12,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-07-11T21:04:27.973Z",
    "af": false
  },
  {
    "_id": "r7hWmbasZxQo6YcXJ",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "YbwaksgYyiGDWdupf",
    "topLevelCommentId": "pogcgTjhTKxbiErDj",
    "contents": {
      "markdown": "the experience that led to calling it a hot take is i was arguing against someone who disagreed with this right before i wrote it up",
      "plaintextDescription": "the experience that led to calling it a hot take is i was arguing against someone who disagreed with this right before i wrote it up"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-11T08:52:47.993Z",
    "af": false
  },
  {
    "_id": "pogcgTjhTKxbiErDj",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "hot take: introspection isn't really real. you can't access your internal state in any meaningful sense beyond what your brain chooses to present to you (e.g visual stimuli, emotions, etc), for reasons outside of your direct control. when you think you're introspecting, what's really going on when you think you're introspecting is you have a model of yourself inside your brain, which you learn gradually by seeing yourself do certain things, experience certain stimuli or emotions, etc.\n\nyour self-model is not fundamentally special compared to any other models you have. it works the same way as your model of anyone or anything else, except you have way more data on yourself, and also you directly experience your own emotions and sensori stimuli, as opposed to having to infer them for other people. often your emotional brain sabotages your ability to understand yourself, but also it sometimes sabotages your ability to understand other people too (e.g groupthink, tribalism).\n\nyour self-model can diverge arbitrarily far from reality. when you're emotionally unintegrated, you have a model of yourself that fails to understand how your emotions truly work, so you will systematically mispredict how you will actually behave, or try to fix yourself in ways that don't work because you are misunderstanding the causes of your actions. further evidence comes from some of the split brain experiments which show people confidently hallucinating causes for their actions which are demonstrably untrue.\n\n(for emotional integration in particular, misprediction is only half of the thing; the other part of the thing is sometimes you can get detached from stimuli and stop perceiving them entirely. this other part is not neatly explained by this hypothesis)\n\nyour self-model contains a nested self-model, but this isn't special in any way. your models of other people have a model of yourself contained within (what does this person think about me?). your nested self model can also be arbitrarily wrong - it is very common to fail to understand the ways in which your top level self model is wrong. you probably don't have a third level nested self model because that's not very useful and very costly to maintain.\n\nyour inner monologue is not most of your thinking, and doesn't give you faithful representation of your true thoughts. people without internal monologues do just fine. also, it's very very common for people to lie to themselves.\n\nI dont think this is just speculative/unfalsifiable. I claim that thinking of your \"introspection\" as self modelling will lead you to make better decisions irl. you can apply techniques for learning to model other kinds of knowledge. you can realize that cognitive biases that apply to other kinds of modeling also apply to modeling yourself.\n\nthere's no clean boundary between your self model and your model of other things. if it's useful to model e.g your phone, or gut bacteria, or glasses, or partner/close friends, in close interaction with your stimuli and actions, there is no sharp self boundary.\n\na regular computer program can truly introspect on itself in ways that humans cannot, but this is fundamentally not that interesting. it definitely doesn't mean computers are conscious.\n\ncorollary: \"introspection\" is not in any way related to consciousness or moral patienthood, and it is uninteresting to ask whether AIs or nonhuman animals are capable of introspection for the purposes of determining things about consciousness and moral patienthood.",
      "plaintextDescription": "hot take: introspection isn't really real. you can't access your internal state in any meaningful sense beyond what your brain chooses to present to you (e.g visual stimuli, emotions, etc), for reasons outside of your direct control. when you think you're introspecting, what's really going on when you think you're introspecting is you have a model of yourself inside your brain, which you learn gradually by seeing yourself do certain things, experience certain stimuli or emotions, etc.\n\nyour self-model is not fundamentally special compared to any other models you have. it works the same way as your model of anyone or anything else, except you have way more data on yourself, and also you directly experience your own emotions and sensori stimuli, as opposed to having to infer them for other people. often your emotional brain sabotages your ability to understand yourself, but also it sometimes sabotages your ability to understand other people too (e.g groupthink, tribalism).\n\nyour self-model can diverge arbitrarily far from reality. when you're emotionally unintegrated, you have a model of yourself that fails to understand how your emotions truly work, so you will systematically mispredict how you will actually behave, or try to fix yourself in ways that don't work because you are misunderstanding the causes of your actions. further evidence comes from some of the split brain experiments which show people confidently hallucinating causes for their actions which are demonstrably untrue.\n\n(for emotional integration in particular, misprediction is only half of the thing; the other part of the thing is sometimes you can get detached from stimuli and stop perceiving them entirely. this other part is not neatly explained by this hypothesis)\n\nyour self-model contains a nested self-model, but this isn't special in any way. your models of other people have a model of yourself contained within (what does this person think about me?). your nested self model can also be arbitrarily"
    },
    "baseScore": 14,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-07-11T02:37:09.686Z",
    "af": false
  },
  {
    "_id": "FmBwndnyqHiRaZ8Sk",
    "postId": "BqwXYFtpetFxqkxip",
    "parentCommentId": "rcumShKAkHfgZm2uk",
    "topLevelCommentId": "YK5MvKDo8KRoK4P8i",
    "contents": {
      "markdown": "random thought, not related to GP comment: i agree identifying expertise in a domain you don't know is really hard, but from my experience, identifying generalizable intelligence/agency/competence is less hard. generally it seems like a useful signal to see how fast they can understand and be effective at a new thing that's related to what they've done before but that they've not thought much specifically about before. this isn't perfectly correlated with competence at their primary field, but it's probably still very useful.\n\ne.g it's generally pretty obvious if someone is flailing on an ML/CS interview Q because they aren't very smart, or just not familiar with the tooling. people who are smart will very quickly and systematically figure out how to use the tooling, and people who aren't will get stuck and sit there being confused. I bet if you took e.g a really smart mathematician with no CS experience and dropped them in a CS interview, it would be very fascinating to watch them figure out things from scratch\n\ndisclaimer that my impressions here are not necessarily strictly tied to feedback from reality on e.g job performance (i can see whether people pass the rest of the interview after making a guess at the 10 minute mark, but it's not like i follow up with managers a year after they get hired to see how well they're doing)",
      "plaintextDescription": "random thought, not related to GP comment: i agree identifying expertise in a domain you don't know is really hard, but from my experience, identifying generalizable intelligence/agency/competence is less hard. generally it seems like a useful signal to see how fast they can understand and be effective at a new thing that's related to what they've done before but that they've not thought much specifically about before. this isn't perfectly correlated with competence at their primary field, but it's probably still very useful.\n\ne.g it's generally pretty obvious if someone is flailing on an ML/CS interview Q because they aren't very smart, or just not familiar with the tooling. people who are smart will very quickly and systematically figure out how to use the tooling, and people who aren't will get stuck and sit there being confused. I bet if you took e.g a really smart mathematician with no CS experience and dropped them in a CS interview, it would be very fascinating to watch them figure out things from scratch\n\ndisclaimer that my impressions here are not necessarily strictly tied to feedback from reality on e.g job performance (i can see whether people pass the rest of the interview after making a guess at the 10 minute mark, but it's not like i follow up with managers a year after they get hired to see how well they're doing)"
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-09T21:01:34.752Z",
    "af": false
  },
  {
    "_id": "DKGcHGHPtCkQ2jA6C",
    "postId": "7Q7DPSk4iGFJd8DRk",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "another useful tip: you can put arbitrary javascript on cards, do you can do stuff like randomize the font size or make minor wording tweaks or whatever to avoid overfitting",
      "plaintextDescription": "another useful tip: you can put arbitrary javascript on cards, do you can do stuff like randomize the font size or make minor wording tweaks or whatever to avoid overfitting"
    },
    "baseScore": 19,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2025-07-09T19:31:46.940Z",
    "af": false
  },
  {
    "_id": "cy7wWFBpxdKwLoAAv",
    "postId": "7Q7DPSk4iGFJd8DRk",
    "parentCommentId": "9HJsChXSstv6wLCYL",
    "topLevelCommentId": "9HJsChXSstv6wLCYL",
    "contents": {
      "markdown": "you can use subdecks and get best of both worlds",
      "plaintextDescription": "you can use subdecks and get best of both worlds "
    },
    "baseScore": 11,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-07-09T19:30:34.298Z",
    "af": false
  },
  {
    "_id": "NLtNfDAYkT2YoZdXC",
    "postId": "7Q7DPSk4iGFJd8DRk",
    "parentCommentId": "QjiFdTWu6yjSWZzjD",
    "topLevelCommentId": "FKvd7Ai6uLQMfJGBz",
    "contents": {
      "markdown": "languages, math, cs, music, miscellaneous knowledge",
      "plaintextDescription": "languages, math, cs, music, miscellaneous knowledge"
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-07-09T19:29:32.516Z",
    "af": false
  },
  {
    "_id": "FKvd7Ai6uLQMfJGBz",
    "postId": "7Q7DPSk4iGFJd8DRk",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "as a datapoint: I do an average of 60 reviews per day these days, down from my peak of 400/day or so (which lasted about a year), and 200/day when I was first starting. I don't generally form habits easily, and I am generally extremely procrastinatory, but I've basically never missed more than a few days of Anki in a row since I started nearly 7 years ago. So I think this habit is especially robust once instilled, and once you can reliably do 20/day you should slowly ramp up to more and more.",
      "plaintextDescription": "as a datapoint: I do an average of 60 reviews per day these days, down from my peak of 400/day or so (which lasted about a year), and 200/day when I was first starting. I don't generally form habits easily, and I am generally extremely procrastinatory, but I've basically never missed more than a few days of Anki in a row since I started nearly 7 years ago. So I think this habit is especially robust once instilled, and once you can reliably do 20/day you should slowly ramp up to more and more."
    },
    "baseScore": 25,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2025-07-08T23:56:44.019Z",
    "af": false
  },
  {
    "_id": "QZxEFiQNMfFfJ6Btq",
    "postId": "PDLhQ3dJsTyApJZ6j",
    "parentCommentId": "a9r7Ldf9WPpYxkccN",
    "topLevelCommentId": "a9r7Ldf9WPpYxkccN",
    "contents": {
      "markdown": "I think the solution is to become more emotionally integrated. take the time to understand your emotional mind and the reasons behind why it believes the things it does. some say therapy helps with this; your mileage may vary. I've found introspection + living life more fully helps a lot.",
      "plaintextDescription": "I think the solution is to become more emotionally integrated. take the time to understand your emotional mind and the reasons behind why it believes the things it does. some say therapy helps with this; your mileage may vary. I've found introspection + living life more fully helps a lot."
    },
    "baseScore": 12,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-07-04T19:48:57.339Z",
    "af": false
  },
  {
    "_id": "Gatt23ANiThSvJaPX",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "WeQ3Xt3PxGsb7faxG",
    "topLevelCommentId": "iKjeNL3B3Lk6pTeJM",
    "contents": {
      "markdown": "the LLM cost should not be too bad. it would mostly be looking at vague vibes rather than requiring lots of reasoning about the thing. I trust e.g AI summaries vastly less because they can require actual intelligence.\n\nI'm happy to fund this a moderate amount for the MVP. I think it would be cool if this existed.\n\nI don't really want to deal with all the problems that come with modifying something that already works for other people, at least not before we're confident the ideas are good. this points towards building a new thing. fwiw I think if building a new thing, the chat part would be most interesting/valuable standalone (and I think it's good to have platforms grow out of a simple core rather than to do everything at once)",
      "plaintextDescription": "the LLM cost should not be too bad. it would mostly be looking at vague vibes rather than requiring lots of reasoning about the thing. I trust e.g AI summaries vastly less because they can require actual intelligence.\n\nI'm happy to fund this a moderate amount for the MVP. I think it would be cool if this existed.\n\nI don't really want to deal with all the problems that come with modifying something that already works for other people, at least not before we're confident the ideas are good. this points towards building a new thing. fwiw I think if building a new thing, the chat part would be most interesting/valuable standalone (and I think it's good to have platforms grow out of a simple core rather than to do everything at once)"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-01T19:22:27.765Z",
    "af": false
  },
  {
    "_id": "qF9iTkCHLG6BmxHmT",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "d3jfWmANdtRz8CdAv",
    "topLevelCommentId": "BmseZcWToGbYkEn6F",
    "contents": {
      "markdown": "it's kind of haphazard and I have no reason to believe I'm better at prompting than anyone else. the broad strokes are I tell it to:\n\n*   use lowercase\n*   not use emojis\n*   be concise, explain at bird's eye level\n*   don't sugar cost things\n*   not be too professional/formal; use some IRC/twitter slang without overdoing it\n*   speak as if it's a conversation over a dinner table between two close friends who are also technical experts\n*   don't dumb things down but also don't use unnecessary jargon\n\nI've also been trying to get it to use CS/ML analogies when it would make things clearer, much the same way people on LW would do, but it's been hard to get the model to do it in a natural, non cringe way. rn it overdoes it and makes lots of very forced and not insightful analogies despite my attempts to explain to it",
      "plaintextDescription": "it's kind of haphazard and I have no reason to believe I'm better at prompting than anyone else. the broad strokes are I tell it to:\n\n * use lowercase\n * not use emojis\n * be concise, explain at bird's eye level\n * don't sugar cost things\n * not be too professional/formal; use some IRC/twitter slang without overdoing it\n * speak as if it's a conversation over a dinner table between two close friends who are also technical experts\n * don't dumb things down but also don't use unnecessary jargon\n\nI've also been trying to get it to use CS/ML analogies when it would make things clearer, much the same way people on LW would do, but it's been hard to get the model to do it in a natural, non cringe way. rn it overdoes it and makes lots of very forced and not insightful analogies despite my attempts to explain to it"
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-07-01T07:48:21.347Z",
    "af": false
  },
  {
    "_id": "P8BnpYxpNyB2FayjT",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "JnQPTjr6geAjfpEyf",
    "topLevelCommentId": "iKjeNL3B3Lk6pTeJM",
    "contents": {
      "markdown": "there's a broader category of things which are not literally scrolling but still time wasting / consuming info not to enrich oneself, but to push the dopamine button, and I think even removing the scroll doesn't fix this (my phone is intentionally quite high friction to use and I still fail to stay off of it)",
      "plaintextDescription": "there's a broader category of things which are not literally scrolling but still time wasting / consuming info not to enrich oneself, but to push the dopamine button, and I think even removing the scroll doesn't fix this (my phone is intentionally quite high friction to use and I still fail to stay off of it)"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-01T05:02:37.940Z",
    "af": false
  },
  {
    "_id": "AZ6byRjCLrGCvNEPh",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "odRD7DCLYbGADRNsP",
    "topLevelCommentId": "iKjeNL3B3Lk6pTeJM",
    "contents": {
      "markdown": "to be clear I explicitly decided not to think too hard about this kind of issue when brainstorming. I think the long run solution is probably something like an elected governance scheme that lets the users control what model to use. maybe make it bicameral to split power between users and funders. but my main motivation for this brainstorming was to think of ideas I could implement in a weekend for shits and giggles to see how well they work irl",
      "plaintextDescription": "to be clear I explicitly decided not to think too hard about this kind of issue when brainstorming. I think the long run solution is probably something like an elected governance scheme that lets the users control what model to use. maybe make it bicameral to split power between users and funders. but my main motivation for this brainstorming was to think of ideas I could implement in a weekend for shits and giggles to see how well they work irl"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-01T02:30:40.930Z",
    "af": false
  },
  {
    "_id": "BmseZcWToGbYkEn6F",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "one big problem with using LMs too much imo is that they are dumb and catastrophically wrong about things a lot, but they are very pleasant to talk to, project confidence and knowledgeability, and reply to messages faster than 99.99% of people. these things are more easily noticeable than subtle falsehood, and reinforce a reflex of asking the model more and more. it's very analogous to twitter soundbites vs reading long form writing and how that eroded epistemics.\n\nhotter take: the extent to which one finds current LMs smart is probably correlated with how much one is swayed by good vibes from their interlocutor as opposed to the substance of the argument (ofc conditional on the model actually giving good vibes, which varies from person to person. I personally never liked chatgpt vibes until I wrote a big system prompt)",
      "plaintextDescription": "one big problem with using LMs too much imo is that they are dumb and catastrophically wrong about things a lot, but they are very pleasant to talk to, project confidence and knowledgeability, and reply to messages faster than 99.99% of people. these things are more easily noticeable than subtle falsehood, and reinforce a reflex of asking the model more and more. it's very analogous to twitter soundbites vs reading long form writing and how that eroded epistemics.\n\nhotter take: the extent to which one finds current LMs smart is probably correlated with how much one is swayed by good vibes from their interlocutor as opposed to the substance of the argument (ofc conditional on the model actually giving good vibes, which varies from person to person. I personally never liked chatgpt vibes until I wrote a big system prompt)"
    },
    "baseScore": 28,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-06-30T23:37:52.970Z",
    "af": false
  },
  {
    "_id": "iKjeNL3B3Lk6pTeJM",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "random brainstorming ideas for things the ideal sane discourse encouraging social media platform would have:\n\n*   have an LM look at the comment you're writing and real time give feedback on things like \"are you sure you want to say that? people will interpret that as an attack and become more defensive, so your point will not be heard\". addendum: if it notices you're really fuming and flame warring, literally gray out the text box for 2 minutes with a message like \"take a deep breath. go for a walk. yelling never changes minds\"\n*   have some threaded chat component bolted on (I have takes on best threading system). big problem is posts are fundamentally too high effort to be a way to think; people want to talk over chat (see success of discord). dialogues were ok but still too high effort and nobody wants to read the transcript. one stupid idea is have an LM look at the transcript and gently nudge people to write things up if the convo is interesting and to have UI affordances to make it low friction (eg a single button that instantly creates a new post and automatically invites everyone from the convo to edit, and auto populates the headers)\n*   inspired by the court system, the most autistically rule following part of the US government: have explicit trusted judges who can be summoned to adjudicate claims or meta level \"is this valid arguing\" claims. top level judges are selected for fixed terms by a weighted sortition scheme that uses some game theoretic / schelling point stuff to discourage partisanship\n*   recommendation system where you can say what kind of stuff you want to be recommended in some text box in the settings. also when people click \"good/bad rec\" buttons on the home page, try to notice patterns and occasionally ask the user whether a specific noticed pattern is correct and ask whether they want it appended to their rec preferences\n*   opt in anti scrolling pop up that asks you every few days what the highest value interaction you had recently on the site was, or whether you're just mindlessly scrolling. gently reminds you to take a break if you can't come up with a good example of a good interaction.\n*   argument mapping is really cool imo but I think most attempts fail because they try to make arguments super structured and legible. I think a less structured version that lets you vote on how much you think various posts respond to other posts and how well you think it addresses the key points and which posts overlap in arguments would be valuable. like you'd see clusters with (human written and vote selected) summaries of various clusters, and then links of various strengths inter cluster. I think this would greatly help epistemics by avoiding infinite argument retreading",
      "plaintextDescription": "random brainstorming ideas for things the ideal sane discourse encouraging social media platform would have:\n\n * have an LM look at the comment you're writing and real time give feedback on things like \"are you sure you want to say that? people will interpret that as an attack and become more defensive, so your point will not be heard\". addendum: if it notices you're really fuming and flame warring, literally gray out the text box for 2 minutes with a message like \"take a deep breath. go for a walk. yelling never changes minds\"\n * have some threaded chat component bolted on (I have takes on best threading system). big problem is posts are fundamentally too high effort to be a way to think; people want to talk over chat (see success of discord). dialogues were ok but still too high effort and nobody wants to read the transcript. one stupid idea is have an LM look at the transcript and gently nudge people to write things up if the convo is interesting and to have UI affordances to make it low friction (eg a single button that instantly creates a new post and automatically invites everyone from the convo to edit, and auto populates the headers)\n * inspired by the court system, the most autistically rule following part of the US government: have explicit trusted judges who can be summoned to adjudicate claims or meta level \"is this valid arguing\" claims. top level judges are selected for fixed terms by a weighted sortition scheme that uses some game theoretic / schelling point stuff to discourage partisanship\n * recommendation system where you can say what kind of stuff you want to be recommended in some text box in the settings. also when people click \"good/bad rec\" buttons on the home page, try to notice patterns and occasionally ask the user whether a specific noticed pattern is correct and ask whether they want it appended to their rec preferences\n * opt in anti scrolling pop up that asks you every few days what the highest value interaction you had recently on the "
    },
    "baseScore": 71,
    "voteCount": 29,
    "createdAt": null,
    "postedAt": "2025-06-30T21:56:20.012Z",
    "af": false
  },
  {
    "_id": "GuGFe5X78SdkMvRRB",
    "postId": "D4eZF6FAZhrW4KaGG",
    "parentCommentId": "LWLF3FzPPSkAPtM2S",
    "topLevelCommentId": "BKJyH9mBtxFXfjq6t",
    "contents": {
      "markdown": "to be clear, I am not intending to claim that you wrote this post believing that it was wrong. I believe that you are trying your best to improve the epistemics and I commend the effort. \n\nI had interpreted your third sentence as still defending the policy of the post even despite now agreeing with Oliver, but I understand now that this is not what you meant, and that you are no longer in favor of the policy advocated in the post. my apologies for the misunderstanding.\n\nI don't think you should just declare that people's beliefs are unfalsifiable. certainly some people's views will be. but finding a crux is always difficult and imo should be done through high bandwidth talking to many people directly to understand their views first (in every group of people, especially one that encourages free thinking among its members, there will be a great diversity of views!). it is *not* effective to put people on blast publicly and then backtrack when people push back saying you misunderstood their position.\n\nI realize this would be a lot of work to ask of you. unfortunately, coordination is hard. it's one of the hardest things in the world. I don't think you have any moral obligation to do this beyond any obligation you feel to making AI go well / improving this community. I'm mostly saying this to lay out my view of why I think this post did not accomplish its goals, and what I think would be the most effective course of action to find a set of cruxes that truly captures the disagreement. I think this would be very valuable if accomplished and it would be great if someone did it.",
      "plaintextDescription": "to be clear, I am not intending to claim that you wrote this post believing that it was wrong. I believe that you are trying your best to improve the epistemics and I commend the effort. \n\nI had interpreted your third sentence as still defending the policy of the post even despite now agreeing with Oliver, but I understand now that this is not what you meant, and that you are no longer in favor of the policy advocated in the post. my apologies for the misunderstanding.\n\nI don't think you should just declare that people's beliefs are unfalsifiable. certainly some people's views will be. but finding a crux is always difficult and imo should be done through high bandwidth talking to many people directly to understand their views first (in every group of people, especially one that encourages free thinking among its members, there will be a great diversity of views!). it is not effective to put people on blast publicly and then backtrack when people push back saying you misunderstood their position.\n\nI realize this would be a lot of work to ask of you. unfortunately, coordination is hard. it's one of the hardest things in the world. I don't think you have any moral obligation to do this beyond any obligation you feel to making AI go well / improving this community. I'm mostly saying this to lay out my view of why I think this post did not accomplish its goals, and what I think would be the most effective course of action to find a set of cruxes that truly captures the disagreement. I think this would be very valuable if accomplished and it would be great if someone did it."
    },
    "baseScore": 11,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-30T21:10:07.023Z",
    "af": false
  },
  {
    "_id": "foMshoQ3MpSHaxLcf",
    "postId": "p28GHSYskzsGKvABH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "my hot take is i agree that human researchers spend a ridiculous amount doing stupid stuff (see my shortform on this), but also I don't think it's very easy to automate the stupid stuff.\n\nI've optimized my research setup to get quite tight feedback loops. if I had more slack I could probably make things even better, but it would look more like developing better infrastructure and hpopt techniques myself, than handing work off to agents.\n\nI disagree that you have to use grid search and not anything more clever in theory. I currently use grid searches too for simplicity, and it's definitely nontrivial to get the clever thing to tell you about interactions, but it doesn't seem fundamentally impossible.",
      "plaintextDescription": "my hot take is i agree that human researchers spend a ridiculous amount doing stupid stuff (see my shortform on this), but also I don't think it's very easy to automate the stupid stuff.\n\nI've optimized my research setup to get quite tight feedback loops. if I had more slack I could probably make things even better, but it would look more like developing better infrastructure and hpopt techniques myself, than handing work off to agents.\n\nI disagree that you have to use grid search and not anything more clever in theory. I currently use grid searches too for simplicity, and it's definitely nontrivial to get the clever thing to tell you about interactions, but it doesn't seem fundamentally impossible."
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-06-29T20:16:03.439Z",
    "af": false
  },
  {
    "_id": "jod47F9F8ikrkBLJB",
    "postId": "D4eZF6FAZhrW4KaGG",
    "parentCommentId": "aED68Pt7hnzm4Hptc",
    "topLevelCommentId": "BKJyH9mBtxFXfjq6t",
    "contents": {
      "markdown": "i think there's a lot of variance. i personally can only work in unpredictable short intense bursts, during which i get my best work done; then i have to go and chill for a while. if i were 1 year away from the singularity i'd try to push myself past my normal limits and push chilling to a minimum, but doing so now seems like a bad idea. i'm currently trying to fix this more durably in the long run but this is highly nontrival",
      "plaintextDescription": "i think there's a lot of variance. i personally can only work in unpredictable short intense bursts, during which i get my best work done; then i have to go and chill for a while. if i were 1 year away from the singularity i'd try to push myself past my normal limits and push chilling to a minimum, but doing so now seems like a bad idea. i'm currently trying to fix this more durably in the long run but this is highly nontrival"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-25T22:22:14.878Z",
    "af": false
  },
  {
    "_id": "5xSFFegDDqfyiFofy",
    "postId": "D4eZF6FAZhrW4KaGG",
    "parentCommentId": "eD7bxBZcxgdf7cXdt",
    "topLevelCommentId": "BKJyH9mBtxFXfjq6t",
    "contents": {
      "markdown": "it still seems bad to advocate for the exactly wrong policy, especially one that doesn't make sense even if you turn out to be correct (as habryka points out in the original comment, many think 2028 is not really when most people expect agi to have happened). it seems very predictable that people will just (correctly) not listen to the advice, and in 2028 both sides on this issue will believe that their view has been vindicated - you will think of course rationalists will never change their minds and emotions on agi doom, and most rationalists will think obviously it was right not to follow the advice because they never expected agi to definitely happen before 2028.\n\ni think you would have much more luck advocating for chilling today and citing past evidence to make your case..",
      "plaintextDescription": "it still seems bad to advocate for the exactly wrong policy, especially one that doesn't make sense even if you turn out to be correct (as habryka points out in the original comment, many think 2028 is not really when most people expect agi to have happened). it seems very predictable that people will just (correctly) not listen to the advice, and in 2028 both sides on this issue will believe that their view has been vindicated - you will think of course rationalists will never change their minds and emotions on agi doom, and most rationalists will think obviously it was right not to follow the advice because they never expected agi to definitely happen before 2028.\n\ni think you would have much more luck advocating for chilling today and citing past evidence to make your case.."
    },
    "baseScore": 7,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-25T22:18:45.819Z",
    "af": false
  },
  {
    "_id": "9cgrMDteKcMdpMESo",
    "postId": "D4eZF6FAZhrW4KaGG",
    "parentCommentId": "BKJyH9mBtxFXfjq6t",
    "topLevelCommentId": "BKJyH9mBtxFXfjq6t",
    "contents": {
      "markdown": "my synthesis is I think people should chill out more today and sprint harder as the end gets near (ofc, some fraction of people should always be sprinting as if the end is near, as a hedge, but I think it should be less than now. also, if you believe the end really is <2 years away then disregard this). the burnout thing is real and it's a big reason for me deciding to be more chill. and there's definitely some weird fear driven action / negative vision thing going on. but also, sprinting now and chilling in 2028 seems like exactly the wrong policy",
      "plaintextDescription": "my synthesis is I think people should chill out more today and sprint harder as the end gets near (ofc, some fraction of people should always be sprinting as if the end is near, as a hedge, but I think it should be less than now. also, if you believe the end really is <2 years away then disregard this). the burnout thing is real and it's a big reason for me deciding to be more chill. and there's definitely some weird fear driven action / negative vision thing going on. but also, sprinting now and chilling in 2028 seems like exactly the wrong policy "
    },
    "baseScore": 11,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-25T18:47:30.651Z",
    "af": false
  },
  {
    "_id": "FBNnKe7i5imhFfJxh",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "some lessons from ml research:\n\n*   any shocking or surprising result in your own experiment is 80% likely to be a bug until proven otherwise. your first thought should always be to comb for bugs.\n*   only after you have ruled out bugs do you get to actually think about how to fit your theory to the data, and even then, there might still be a hidden bug.\n*   most papers are terrible and don't replicate.\n*   most techniques that sound intuitively plausible don't work.\n*   most techniques only look good if you don't pick a strong enough baseline.\n*   an actually good idea can take many tries before it works.\n*   once you have good research intuitions, the most productive state to be in is to literally not think about what will go into the paper and just do experiments that satisfy your curiosity and. convince yourself that the thing is true. once you have that, running the final sweeps is really easy\n*   most people have no intuition whatsoever about their hardware and so will write code that is horribly inefficient. even learning a little bit about hardware fundamentals so you don't do anything obviously dumb is super valuable\n*   in a long and complex enough project, you will almost certainly have a bug that invalidates weeks (or months) of work. being really careful and testing helps but slows down velocity a lot. unclear what the right equilibrium is.\n*   feedback loop time is incredibly important, if you can get rapid feedback, you will make so much more progress.\n*   implementing something that is already known to work is always vastly easier than inventing/researching something new.\n*   you will inevitably spend a lot of time doing things that have no impact on the final published work whatsoever. like not even contributing that much useful intuition. this is unfortunate but unavoidable\n*   oftentimes you will spend a lot of time being fundamentally philosophically confused about what to do, and only really figure out halfway through the project. this is normal.\n*   direction is really important. most well executed research is still useless because it was the wrong direction.\n*   research impact is super super long tailed. i think it's really not worth doing research if you aren't aiming for the long tail. if you're early career, you should probably focus on doing things that enable you to aim at the long tail eventually, instead of trying to have lots of impact early on (for example, probablly better to do something you feel motivated by and learn a lot from than something that is \"maximally important\" but which you don't have the skills to execute adequately on yet)",
      "plaintextDescription": "some lessons from ml research:\n\n * any shocking or surprising result in your own experiment is 80% likely to be a bug until proven otherwise. your first thought should always be to comb for bugs.\n * only after you have ruled out bugs do you get to actually think about how to fit your theory to the data, and even then, there might still be a hidden bug.\n * most papers are terrible and don't replicate.\n * most techniques that sound intuitively plausible don't work.\n * most techniques only look good if you don't pick a strong enough baseline.\n * an actually good idea can take many tries before it works.\n * once you have good research intuitions, the most productive state to be in is to literally not think about what will go into the paper and just do experiments that satisfy your curiosity and. convince yourself that the thing is true. once you have that, running the final sweeps is really easy\n * most people have no intuition whatsoever about their hardware and so will write code that is horribly inefficient. even learning a little bit about hardware fundamentals so you don't do anything obviously dumb is super valuable\n * in a long and complex enough project, you will almost certainly have a bug that invalidates weeks (or months) of work. being really careful and testing helps but slows down velocity a lot. unclear what the right equilibrium is.\n * feedback loop time is incredibly important, if you can get rapid feedback, you will make so much more progress.\n * implementing something that is already known to work is always vastly easier than inventing/researching something new.\n * you will inevitably spend a lot of time doing things that have no impact on the final published work whatsoever. like not even contributing that much useful intuition. this is unfortunate but unavoidable\n * oftentimes you will spend a lot of time being fundamentally philosophically confused about what to do, and only really figure out halfway through the project. this is normal.\n * directio"
    },
    "baseScore": 195,
    "voteCount": 109,
    "createdAt": null,
    "postedAt": "2025-06-21T23:25:40.999Z",
    "af": false
  },
  {
    "_id": "g2KdZhfnvmE2NXXmD",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "a simple elegant intuition for the relationship between SVD and eigendecomposition that I haven't heard before: \n\nthe eigendecomposition of A tells us which directions A stretches along without rotating. but sometimes we want to know all the directions things get stretched along, even if there is rotation.\n\nwhy does taking the eigendecomposition of \\\\(A^TA\\\\) help us? suppose we rewrite \\\\(A = RS\\\\), where S just scales (i.e is normal matrix), and R is just a rotation matrix. then, \\\\(A^TA = S^TR^TRS\\\\), and the R's cancel out because transpose of rotation matrix is also its inverse.\n\nintuitively, imagine thinking of A as first scaling in place, and then rotating. then, ATA would first scale, then rotate, then rotate again in the opposite direction, then scale again. so all the rotations cancel out and the resulting eigenvalues of ATA are the squares of the scaling factors.",
      "plaintextDescription": "a simple elegant intuition for the relationship between SVD and eigendecomposition that I haven't heard before: \n\nthe eigendecomposition of A tells us which directions A stretches along without rotating. but sometimes we want to know all the directions things get stretched along, even if there is rotation.\n\nwhy does taking the eigendecomposition of ATA help us? suppose we rewrite A=RS, where S just scales (i.e is normal matrix), and R is just a rotation matrix. then, ATA=STRTRS, and the R's cancel out because transpose of rotation matrix is also its inverse.\n\nintuitively, imagine thinking of A as first scaling in place, and then rotating. then, ATA would first scale, then rotate, then rotate again in the opposite direction, then scale again. so all the rotations cancel out and the resulting eigenvalues of ATA are the squares of the scaling factors."
    },
    "baseScore": 11,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-06-07T18:15:09.278Z",
    "af": false
  },
  {
    "_id": "LDpXyB2xaxpi38YWd",
    "postId": "mtgbgepheaQsAyXmG",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> \"didn't really trust Sam even back then, they just thought the option was signing the letter or the company dying, and they really didn't want the latter.\"\n\nI personally knew a bunch of people at the time who didn't fully support the letter but signed it anyways because they felt overwhelming pressure to do so. I'd guess these people are a minority of signatories, but it's hard to know for sure. (The kinds of people who would sign out of fear of retaliation are probably unlikely to reveal this to people they don't know super well.)\n\nAlso, yeah, financial/livelihood motivation were not the only factor (the board had just taken this huge drastic action that made no sense and then refused to explain why! that comes off pretty shady!) but obviously a huge factor. Few people in the world are indifferent to suddenly losing their job, let alone seeing millions of their own dollars evaporate before their eyes.",
      "plaintextDescription": "> \"didn't really trust Sam even back then, they just thought the option was signing the letter or the company dying, and they really didn't want the latter.\"\n\n \n\nI personally knew a bunch of people at the time who didn't fully support the letter but signed it anyways because they felt overwhelming pressure to do so. I'd guess these people are a minority of signatories, but it's hard to know for sure. (The kinds of people who would sign out of fear of retaliation are probably unlikely to reveal this to people they don't know super well.)\n\nAlso, yeah, financial/livelihood motivation were not the only factor (the board had just taken this huge drastic action that made no sense and then refused to explain why! that comes off pretty shady!) but obviously a huge factor. Few people in the world are indifferent to suddenly losing their job, let alone seeing millions of their own dollars evaporate before their eyes."
    },
    "baseScore": 31,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-05-20T17:02:56.866Z",
    "af": false
  },
  {
    "_id": "HLksqRCbEcupejHmn",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "L7yPGFAxzxDKjocsb",
    "topLevelCommentId": "5E4Smr6tPrshzhHKH",
    "contents": {
      "markdown": "(I would never make knowingly false statements about epistemology to try to win an object level point; I still disagree with your claims about epistemology and believe that my epistemology arguments are in good faith and capture truth in some way. This disagreement might be because I've not communicated myself well. I originally wasn't going to reply but I felt the need to say this because your comment can be viewed as accusing me of intellectual/epistemic dishonesty, even if that wasn't your intention.)",
      "plaintextDescription": "(I would never make knowingly false statements about epistemology to try to win an object level point; I still disagree with your claims about epistemology and believe that my epistemology arguments are in good faith and capture truth in some way. This disagreement might be because I've not communicated myself well. I originally wasn't going to reply but I felt the need to say this because your comment can be viewed as accusing me of intellectual/epistemic dishonesty, even if that wasn't your intention.)"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-20T15:50:18.816Z",
    "af": false
  },
  {
    "_id": "JEyPt7e2Bx2ygdGn4",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "vbiZe5qHn68nyv646",
    "topLevelCommentId": "5E4Smr6tPrshzhHKH",
    "contents": {
      "markdown": "I am confused why you think my claims are only semi related. to me my claim is very straightforward, and the things i'm saying are straightforwardly converying a world model that seems to me to explain why i believe my claim. i'm trying to explain in good faith, not trying to say random things. i'm claiming a theory of how people parse information, to justify my opening statement, which i can clarify as:\n\n*   sometimes, people use the rhetorical move of saying something like \"people think 95% doom is overconfident, yet 5% isn't. but that's also being 95% confident in not-doom, and yet they don't consider *that* overconfident. curious.\" followed by \"well actually, it's only a big claim under *your* reference class. under mine, i.e the set of all instances of a more intelligent thing emerging, actually, 95% doom is *less* overconfident than 5% doom\" this post was inspired by seeing one such tweet, but i see such claims like this every once in a while that play reference class tennis.\n*   i think this kind of argument is really bad at persuading people who don't already agree (from empirical observation). my opening statement is saying \"please stop doing this, if you do it, and thank you for not doing this, if you dont already do it\" the rest of my paragraphs provide an explanation of my theory for why this is bad for changing people's minds. this seems pretty obviously relevant for justifying why we should stop doing the thing. i sometimes see people out there talk like this (including my past self at some point), and then fail to convince people, and then feel very confused about why people don't see the error of their ways when presented with an alternative reference class. if my theory is correct (maybe it isn't, this isn't a super well thought out take, it's more a shower thought), then it would explain this, and people who are failing to convince people would probably want to know why they're failing. i did not spell this out in my opening statement because i thought it was clear but in retrospect this was not clear from the opening statement\n*   i don't think the root cause is people being irrational epistemically. i think there is a fundamental reason why people do this that is very reasonable. i think you disagree with this on the object level and many of my paragraphs are attempting to respond to what i view as the reason you disagree. this does not explicitly show up in the opening statement, but since you disagree with this, i thought it would make sense to respond to that too\n*   i am not saying you should explicitly say \"yeah i think you should treat me as a scammer until i prove otherwise\"! i am also not saying you should try to argue with people who have already stopped listening to you because they think you're a scammer! i am merely saying we should be aware that people might be entertaining that as a hypothesis, and if you try to argue by using this particular class of rhetorical move, you will only trigger their defenses further, and that you should instead just directly provide the evidence for why you should be taken seriously, in a socially appropriate manner. if i understand correctly, i think the thing you are saying one should do is the same as the thing i'm saying one should do, but phrased in a different way; i'm saying not to do a thing that you seem to already not be doing.\n\ni think i have not communicated myself well in this conversation, and my mental model is that we aren't really making progress, and therefore this conversation has not brought value and joy into the world in the way i intended. so this will probably be my last reply, unless you think doing so would be a grave error.",
      "plaintextDescription": "I am confused why you think my claims are only semi related. to me my claim is very straightforward, and the things i'm saying are straightforwardly converying a world model that seems to me to explain why i believe my claim. i'm trying to explain in good faith, not trying to say random things. i'm claiming a theory of how people parse information, to justify my opening statement, which i can clarify as:\n\n * sometimes, people use the rhetorical move of saying something like \"people think 95% doom is overconfident, yet 5% isn't. but that's also being 95% confident in not-doom, and yet they don't consider that overconfident. curious.\" followed by \"well actually, it's only a big claim under your reference class. under mine, i.e the set of all instances of a more intelligent thing emerging, actually, 95% doom is less overconfident than 5% doom\" this post was inspired by seeing one such tweet, but i see such claims like this every once in a while that play reference class tennis.\n * i think this kind of argument is really bad at persuading people who don't already agree (from empirical observation). my opening statement is saying \"please stop doing this, if you do it, and thank you for not doing this, if you dont already do it\" the rest of my paragraphs provide an explanation of my theory for why this is bad for changing people's minds. this seems pretty obviously relevant for justifying why we should stop doing the thing. i sometimes see people out there talk like this (including my past self at some point), and then fail to convince people, and then feel very confused about why people don't see the error of their ways when presented with an alternative reference class. if my theory is correct (maybe it isn't, this isn't a super well thought out take, it's more a shower thought), then it would explain this, and people who are failing to convince people would probably want to know why they're failing. i did not spell this out in my opening statement because i thought it "
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-05-19T08:04:41.937Z",
    "af": false
  },
  {
    "_id": "JSzzgFGyipMkNpSxa",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "yvC9LYqJsCuBrFckz",
    "topLevelCommentId": "5E4Smr6tPrshzhHKH",
    "contents": {
      "markdown": "i'm not even saying people should not evaluate evidence for and against a proposition in general! it's just that this is expensive, and so it is perfectly reasonable to have heuristics to decide which things to evaluate, and so you should first prove with costly signals that you are not pwning them, and then they can weigh the evidence. and until you can provide enough evidence that you're not pwning them for it to be worth their time to evaluate your claims in detail, that it should not be surprising that many people won't listen to the evidence; and that even if they do listen, if there is still lingering suspicion that they are being pwned, you need to provide the type of evidence that could persuade someone that they aren't getting pwned (for which being credibly very honest and truth seeking is necessary but not sufficient), which is sometimes different from mere compellingness of argument",
      "plaintextDescription": "i'm not even saying people should not evaluate evidence for and against a proposition in general! it's just that this is expensive, and so it is perfectly reasonable to have heuristics to decide which things to evaluate, and so you should first prove with costly signals that you are not pwning them, and then they can weigh the evidence. and until you can provide enough evidence that you're not pwning them for it to be worth their time to evaluate your claims in detail, that it should not be surprising that many people won't listen to the evidence; and that even if they do listen, if there is still lingering suspicion that they are being pwned, you need to provide the type of evidence that could persuade someone that they aren't getting pwned (for which being credibly very honest and truth seeking is necessary but not sufficient), which is sometimes different from mere compellingness of argument"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-05-19T01:07:29.381Z",
    "af": false
  },
  {
    "_id": "d4m7KKwba8jx7RNo6",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "ycKeRtqqEASwEA4v3",
    "topLevelCommentId": "5E4Smr6tPrshzhHKH",
    "contents": {
      "markdown": "in practice many of the claims you hear will be optimized for memetic fitness, even if the people making the claims are genuine. well intentioned people can still be naive, or have blind spots, or be ideologically captured.\n\nalso, presumably the people you are trying to convince are on average less surrounded by truth seeking people than you are (because being in the alignment community is strongly correlated with caring about seeking truth).\n\ni don't think this gives up your ability to communicate with people. you simply have to signal in some credible way that you are not only well intentioned but also not merely the carrier of some very memetic idea that slipped past your antibodies. there are many ways to accomplish this. for example, you can build up a reputation of being very scrupulous and unmindkilled. this lets you convey ideas freely to other people in your circles that are also very scrupulous and unmindkilled. when interacting with people outside this circle, for whom this form of reputation is illegible, you need to find something else. depending on who you're talking to and what kinds of things they take seriously, this could be leaning on the credibility of someone like geoff hinton, or of sam/demis/dario, or the UK government, or whatever.\n\nthis might already be what you're doing, in which case there's no disagreement between us.",
      "plaintextDescription": "in practice many of the claims you hear will be optimized for memetic fitness, even if the people making the claims are genuine. well intentioned people can still be naive, or have blind spots, or be ideologically captured.\n\nalso, presumably the people you are trying to convince are on average less surrounded by truth seeking people than you are (because being in the alignment community is strongly correlated with caring about seeking truth).\n\ni don't think this gives up your ability to communicate with people. you simply have to signal in some credible way that you are not only well intentioned but also not merely the carrier of some very memetic idea that slipped past your antibodies. there are many ways to accomplish this. for example, you can build up a reputation of being very scrupulous and unmindkilled. this lets you convey ideas freely to other people in your circles that are also very scrupulous and unmindkilled. when interacting with people outside this circle, for whom this form of reputation is illegible, you need to find something else. depending on who you're talking to and what kinds of things they take seriously, this could be leaning on the credibility of someone like geoff hinton, or of sam/demis/dario, or the UK government, or whatever.\n\nthis might already be what you're doing, in which case there's no disagreement between us."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-19T01:01:56.147Z",
    "af": false
  },
  {
    "_id": "spFi9eFtJ7dwH2pJE",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "8dFYTGkTty6FhvwFj",
    "topLevelCommentId": "5E4Smr6tPrshzhHKH",
    "contents": {
      "markdown": "i am also trying to accurately describe reality. what i'm saying is, even from the perspective of someone smart and truth-seeking but who doesn't know much about the object-level, it is very reasonable to use bigness of claim as a heuristic for how much evidence you need before you're satisfied, and that if you don't do this, you will be worse at finding the truth in practice. my guess is this applies even more so to the average person.\n\ni think this is very analogous to occam's razor / trust region optimization. clearly, we need to discount theories based on complexity because there are exponentially more complex theories compared to simple ones, many of which have no easily observable difference to the simpler ones, opening you up to being pwned. and empirically it seems a good heuristic to live life by. complex theories can still be true! but given two theories that both accurately describe reality, you want the simpler one. similarly, given two equally complex claims that accurately describe the evidence, you want the one that is less far fetched from your current understanding of the world / requires changing less of your worldview.\n\nalso, it doesn't have to be something you literally personally experienced. it's totally valid to read the wikipedia page on the branch davidians or whatever and feel slightly less inclined to take things that have similar vibes seriously, or even to absorb the vibe from your environs (your aversion to scammers and cranks surely did not come ex nihilo, right?)\n\nfor most of the examples i raised, i didn't necessarily mean the claim was literally 100% human extinction, and i don't think it matters that it wasn't. first, because the important thing is the vibe of the claim (catastrophic) - since we're talking about heuristics on how seriously to take things that you don't have time to deep dive on, the rule has to be relatively cheap to implement. i think most people, even quite smart people, genuinely don't feel much of an emotional difference between literal human extinction vs collapse of society vs half of people dying painfully, unless they first spend a half hour carefully thinking about the implications of extinction. (and even then depending on their values they may still not feel a huge difference)\n\nalso, it would be really bad if you could weasel your way out of a reference class that easily; it would be rife for abuse by bad actors - \"see, *our* weird sect of christianity claims that after armageddon, not only will all actual sinners' souls be tortured forever, but that the devil will create every *possible* sinner's soul to torture forever! this is actually fundamentally different from all existing christian theories, and it would be unfathomably worse, so it really shouldn't be thought of as the same kind of claim\"\n\neven if most people are trying to describe the world accurately (which i think is not true and we only get this impression because we live in a strange bubble of very truth seeking people + are above-average capable at understanding things object level and therefore quickly detecting scams), ideas are still selected for memeticness. i'm sure that 90% of conspiracy theorists genuinely believe that humanity is controlled by lizards and are trying their best to spread what they believe to be true. many (not all) of the worst atrocities in history have been committed by people who genuinely thought they were on the side of truth and good.\n\n(actually, i think people *do* get pwned all the time, even in our circles. rationalists are probably more likely than average (controlling for intelligence) to get sucked into obviously culty things (e.g zizians), largely because they *don't* have the memetic antibodies needed to not get pwned, for one reason or another. so probably many rationalists would benefit from evaluating things a little bit more on vibes/bigness and a little bit less on object level)",
      "plaintextDescription": "i am also trying to accurately describe reality. what i'm saying is, even from the perspective of someone smart and truth-seeking but who doesn't know much about the object-level, it is very reasonable to use bigness of claim as a heuristic for how much evidence you need before you're satisfied, and that if you don't do this, you will be worse at finding the truth in practice. my guess is this applies even more so to the average person.\n\ni think this is very analogous to occam's razor / trust region optimization. clearly, we need to discount theories based on complexity because there are exponentially more complex theories compared to simple ones, many of which have no easily observable difference to the simpler ones, opening you up to being pwned. and empirically it seems a good heuristic to live life by. complex theories can still be true! but given two theories that both accurately describe reality, you want the simpler one. similarly, given two equally complex claims that accurately describe the evidence, you want the one that is less far fetched from your current understanding of the world / requires changing less of your worldview.\n\nalso, it doesn't have to be something you literally personally experienced. it's totally valid to read the wikipedia page on the branch davidians or whatever and feel slightly less inclined to take things that have similar vibes seriously, or even to absorb the vibe from your environs (your aversion to scammers and cranks surely did not come ex nihilo, right?)\n\nfor most of the examples i raised, i didn't necessarily mean the claim was literally 100% human extinction, and i don't think it matters that it wasn't. first, because the important thing is the vibe of the claim (catastrophic) - since we're talking about heuristics on how seriously to take things that you don't have time to deep dive on, the rule has to be relatively cheap to implement. i think most people, even quite smart people, genuinely don't feel much of an emotional "
    },
    "baseScore": 9,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-05-18T21:20:23.585Z",
    "af": false
  },
  {
    "_id": "zRTXK5fP7FvoRo2jv",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "vNSyqKyyxordib6mT",
    "topLevelCommentId": "5E4Smr6tPrshzhHKH",
    "contents": {
      "markdown": "I agree this reference class is better, and implies a higher prior, but I think it's reasonable for the prior over \"arbitrary credentialed people warning about something\" to be still relatively low in an absolute sense- lots of people have impressive sounding credentials that are not actually good evidence of competence (consider: it's basically a meme at this point that whenever you see a book where the author puts \"PhD\" after their name, they probably are a grifter / their phd was probably kinda bs), and also there is a real negativity bias where fearmongering is amplified by both legacy and social media. Also, for the purposes of understanding normal people, it's useful to keep in mind that trust in credentials and institutions is not very high right now in the US among genpop.",
      "plaintextDescription": "I agree this reference class is better, and implies a higher prior, but I think it's reasonable for the prior over \"arbitrary credentialed people warning about something\" to be still relatively low in an absolute sense- lots of people have impressive sounding credentials that are not actually good evidence of competence (consider: it's basically a meme at this point that whenever you see a book where the author puts \"PhD\" after their name, they probably are a grifter / their phd was probably kinda bs), and also there is a real negativity bias where fearmongering is amplified by both legacy and social media. Also, for the purposes of understanding normal people, it's useful to keep in mind that trust in credentials and institutions is not very high right now in the US among genpop."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-18T20:34:22.153Z",
    "af": false
  },
  {
    "_id": "5E4Smr6tPrshzhHKH",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I claim it is a lot more reasonable to use the reference class of \"people claiming the end of the world\" than \"more powerful intelligences emerging and competing with less intelligent beings\" when thinking about AI x-risk. further, we should not try to convince people to adopt the latter reference class - this sets off alarm bells, and rightly so (as I will argue in short order) - but rather to bite the bullet, start from the former reference class, and provide arguments and evidence for why this case is different from all the other cases.\n\nthis raises the question: how should you pick which reference class to use, in general? how do you prevent reference class tennis, where you argue back and forth about what is the right reference class to use? I claim the solution is you want to use reference classes that have consistently made good decisions irl. the point of reference classes is to provide a heuristic to quickly apply judgement to large swathes of situations that you don't have time to carefully examine. this is important because otherwise it's easy to get tied up by bad actors who avoid being refuted by making their beliefs very complex and therefore hard to argue against.\n\nthe big problem with the latter reference class is it's not like anyone has had many experiences using it to make decisions ex ante, and if you squint really hard to find day to day examples, they don't all work out the same way. smarter humans do mostly tend to win over less smart humans. but if you work at a zoo, you will almost always be more worried about physical strength and aggressiveness when putting different species in the same enclosure. if you run a farm (or live in Australia), you're very worried about relatively dumb invasive animals like locusts and rabbits.\n\non the other hand, everyone has personally experienced a dozen different doomsday predictions. whether that's your local church or faraway cult warning about Armageddon, or Y2K, or global financial collapse in 2008, or the maximally alarmist climate people, or nuclear winter, or peak oil. for basically all of them, the right action empirically in retrospect was to not think too much about it. there are many concrete instances of people saying \"but this is different\" and then getting burned.\n\nand if you allow any reference class to be on as strong a footing as very well established reference classes, then you open yourself up to getting pwned ideologically. \"all complex intricate objects we have seen created have been created by something intelligent, therefore the universe must also have an intelligent creator.\" it's a very important memetic defense mechanism.\n\n(to be clear this doesn't mean you can only believe things others believe, or that humans taking over earth is not important evidence, or that doomsday is impossible!! I personally think AGI will probably kill everyone. but this is a big claim and should be treated as such. if we don't accept this, then we will forever fail to communicate with people who don't already agree with us on AGI x-risk.)",
      "plaintextDescription": "I claim it is a lot more reasonable to use the reference class of \"people claiming the end of the world\" than \"more powerful intelligences emerging and competing with less intelligent beings\" when thinking about AI x-risk. further, we should not try to convince people to adopt the latter reference class - this sets off alarm bells, and rightly so (as I will argue in short order) - but rather to bite the bullet, start from the former reference class, and provide arguments and evidence for why this case is different from all the other cases.\n\nthis raises the question: how should you pick which reference class to use, in general? how do you prevent reference class tennis, where you argue back and forth about what is the right reference class to use? I claim the solution is you want to use reference classes that have consistently made good decisions irl. the point of reference classes is to provide a heuristic to quickly apply judgement to large swathes of situations that you don't have time to carefully examine. this is important because otherwise it's easy to get tied up by bad actors who avoid being refuted by making their beliefs very complex and therefore hard to argue against.\n\nthe big problem with the latter reference class is it's not like anyone has had many experiences using it to make decisions ex ante, and if you squint really hard to find day to day examples, they don't all work out the same way. smarter humans do mostly tend to win over less smart humans. but if you work at a zoo, you will almost always be more worried about physical strength and aggressiveness when putting different species in the same enclosure. if you run a farm (or live in Australia), you're very worried about relatively dumb invasive animals like locusts and rabbits.\n\non the other hand, everyone has personally experienced a dozen different doomsday predictions. whether that's your local church or faraway cult warning about Armageddon, or Y2K, or global financial collapse in 2008, or t"
    },
    "baseScore": 23,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2025-05-18T00:29:46.325Z",
    "af": false
  },
  {
    "_id": "Cmo2ok3zWcw95ptFu",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "my summary of these two papers: [https://arxiv.org/pdf/1805.12152 ](https://arxiv.org/pdf/1805.12152) [https://arxiv.org/pdf/1905.02175](https://arxiv.org/pdf/1905.02175)\n\nthe first paper observes a phenomenon where adversarial accuracy and normal accuracy are at odds with each other. the authors present a toy example to explain this. \n\nthe construction involves giving each input one channel that is 90% accurate for predicting the binary label, and a bazillion iid gaussian channels that are as noisy as possible individually, so that when you take the average across all of them you get ~100% accuracy. they show that when you do \\\\(\\ell_\\infty\\\\)-adversarial training on the input you learn to only use the 90% accurate feature, whereas normal training uses all bazillion weak channels.\n\nthe key to this construction is that they consider an \\\\(\\ell_\\infty\\\\)-ball on the input (distance is the max across all coordinates). so this means by adding more and more features, you can move further and further in \\\\(\\ell_2\\\\) space (specifically, \\\\(\\sqrt n\\\\) in terms of the number of features). but the \\\\(\\ell_2\\\\) distance between the means of the two high dimensional gaussians stays constant, so no matter what your \\\\(\\varepsilon\\\\) is, at some point with enough channels you can perturb anything from one class into the other class and vice versa.\n\nin the second paper, the authors do further experiments on real models to show that you can separate out the robust features and the unrobust ones, and recombine them into frankenstein images that look like dogs to humans but cats to the unrobust model and dogs to the robust model.\n\nthey also generalize the toy example in the previous paper. they argue that in general, adversarial examples arise exactly when the adversarial attack metric and the loss metric differ. in other words, the loss function (and downstream part of the model, in a multilayer model) implies some loss surface around any data point, and some directions on that surface will be a lot more important for the loss than some other directions. but your \\\\(\\varepsilon\\\\) ball (in, say, \\\\(\\ell_2\\\\)) that you do your attack in will treat all those directions equally importantly. so you can pick the direction that maximizes the amount of loss change.\n\ntheir new example is a classification task on two features, where the two classes are very stretched out gaussians placed diagonally from each other, so that a \\\\(\\ell_2\\\\) ball from each mean reaches into the distribution of the other gaussian. during normal training, the classification boundary learned falls right along the line where the mahalanobis distance from the two means is the same (intuitively, the classification boundary falls along exactly those points where a data point is equally likely to be sampled from either distribution.) but this is different from \\\\(\\ell_2\\\\) norm! it treats distances along the low-variance axis of the gaussian as being much larger, so it doesn't mind putting the boundary close (in \\\\(\\ell_2\\\\) norm) to the mean. this lets the \\\\(\\ell_2\\\\) perturbation step over the boundary.",
      "plaintextDescription": "my summary of these two papers: https://arxiv.org/pdf/1805.12152  https://arxiv.org/pdf/1905.02175\n\nthe first paper observes a phenomenon where adversarial accuracy and normal accuracy are at odds with each other. the authors present a toy example to explain this. \n\nthe construction involves giving each input one channel that is 90% accurate for predicting the binary label, and a bazillion iid gaussian channels that are as noisy as possible individually, so that when you take the average across all of them you get ~100% accuracy. they show that when you do ℓ∞-adversarial training on the input you learn to only use the 90% accurate feature, whereas normal training uses all bazillion weak channels.\n\nthe key to this construction is that they consider an ℓ∞-ball on the input (distance is the max across all coordinates). so this means by adding more and more features, you can move further and further in ℓ2 space (specifically, √n in terms of the number of features). but the ℓ2 distance between the means of the two high dimensional gaussians stays constant, so no matter what your ε is, at some point with enough channels you can perturb anything from one class into the other class and vice versa.\n\nin the second paper, the authors do further experiments on real models to show that you can separate out the robust features and the unrobust ones, and recombine them into frankenstein images that look like dogs to humans but cats to the unrobust model and dogs to the robust model.\n\nthey also generalize the toy example in the previous paper. they argue that in general, adversarial examples arise exactly when the adversarial attack metric and the loss metric differ. in other words, the loss function (and downstream part of the model, in a multilayer model) implies some loss surface around any data point, and some directions on that surface will be a lot more important for the loss than some other directions. but your ε ball (in, say, ℓ2) that you do your attack in will treat all t"
    },
    "baseScore": 16,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-05-16T21:45:07.586Z",
    "af": false
  },
  {
    "_id": "MTh9mmLPLfW2HDykX",
    "postId": "A5PJYKw8tw38JpbJC",
    "parentCommentId": "xKgyTZ5FGcjFHjRW4",
    "topLevelCommentId": "xKgyTZ5FGcjFHjRW4",
    "contents": {
      "markdown": "BB grows so fast that in practice it doesn't seem worth distinguishing any of these cases for programs of nontrivial size.",
      "plaintextDescription": "BB grows so fast that in practice it doesn't seem worth distinguishing any of these cases for programs of nontrivial size."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-16T20:29:13.851Z",
    "af": false
  },
  {
    "_id": "yBdszz4dsotyj8ird",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "the modern world has many flaws, but I'm still deeply grateful for the modern era of unprecedented peace, prosperity, and freedom in the developed world. 99% of people reading these words have never had to worry about dying in a cholera epidemic, or malaria or smallpox or the plague, or childbirth, or in war, or from a famine, or due to a political purge. this is not true for other times in history, or other places in the world today.\n\n(extremely unoriginal thought, but still important to acknowledge periodically because it's easy to take for granted. especially because it's much more common to complain about ways the world is broken than to acknowledge what has improved over time.)",
      "plaintextDescription": "the modern world has many flaws, but I'm still deeply grateful for the modern era of unprecedented peace, prosperity, and freedom in the developed world. 99% of people reading these words have never had to worry about dying in a cholera epidemic, or malaria or smallpox or the plague, or childbirth, or in war, or from a famine, or due to a political purge. this is not true for other times in history, or other places in the world today.\n\n(extremely unoriginal thought, but still important to acknowledge periodically because it's easy to take for granted. especially because it's much more common to complain about ways the world is broken than to acknowledge what has improved over time.)"
    },
    "baseScore": 84,
    "voteCount": 51,
    "createdAt": null,
    "postedAt": "2025-05-12T06:00:31.564Z",
    "af": false
  },
  {
    "_id": "XFkYFBEXfu4BHHmez",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "why is it convergent to invent foods which consist of some form of carbs enveloping meat?",
      "plaintextDescription": "why is it convergent to invent foods which consist of some form of carbs enveloping meat?"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-05-05T08:19:20.180Z",
    "af": false
  },
  {
    "_id": "9c44rkDArwWHYTK9p",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "HcpvhtADg2kPKxuJ8",
    "topLevelCommentId": "GkL3QQqr6wkAoANHy",
    "contents": {
      "markdown": "why do stimulants help ADHD? well, they short circuit the part where your brain figures out what priorities to trust based on whether they achieve your true motives. if your brain has already learned that your self model is bad at picking actions that eventually pay off towards its true motives, it won't put its full effort behind those actions. if you can trick it by making every action feel like it's paying off, you can get it to go along.\n\nhonestly unclear whether this is good or bad. on the one hand, if your self model has fallen out of sync, this is pretty necessary to get things done, and could get you out of a bad feedback loop (ADHD is really bad for noticing that your self model has fallen horribly out of sync and acting effectively on it!). some would argue on naturalistic grounds that ideally the true long term solution is to use your brain's machinery the way it was always intended, by deeply understanding and accepting (and possibly modifying) your actual motives/priorities and having them steer your actions. the other option is to permanently circumvent your motivation system, to turn it into a rubber stamp for whatever decrees are handed down from the self model, which, forever unmoored from needing to model the self, is no longer an understanding of the self but rather an aspirational endpoint towards which the self is molded. I genuinely don't know which is better as an end goal.",
      "plaintextDescription": "why do stimulants help ADHD? well, they short circuit the part where your brain figures out what priorities to trust based on whether they achieve your true motives. if your brain has already learned that your self model is bad at picking actions that eventually pay off towards its true motives, it won't put its full effort behind those actions. if you can trick it by making every action feel like it's paying off, you can get it to go along.\n\nhonestly unclear whether this is good or bad. on the one hand, if your self model has fallen out of sync, this is pretty necessary to get things done, and could get you out of a bad feedback loop (ADHD is really bad for noticing that your self model has fallen horribly out of sync and acting effectively on it!). some would argue on naturalistic grounds that ideally the true long term solution is to use your brain's machinery the way it was always intended, by deeply understanding and accepting (and possibly modifying) your actual motives/priorities and having them steer your actions. the other option is to permanently circumvent your motivation system, to turn it into a rubber stamp for whatever decrees are handed down from the self model, which, forever unmoored from needing to model the self, is no longer an understanding of the self but rather an aspirational endpoint towards which the self is molded. I genuinely don't know which is better as an end goal. "
    },
    "baseScore": 17,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-05-04T16:19:17.298Z",
    "af": false
  },
  {
    "_id": "HcpvhtADg2kPKxuJ8",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "GkL3QQqr6wkAoANHy",
    "topLevelCommentId": "GkL3QQqr6wkAoANHy",
    "contents": {
      "markdown": "why is ADHD also strongly correlated with systematization? it could just be worse self modelling - ADHD happens when your brain's model of its own priorities and motivations falls out of sync from your brain's actual priorities and motivations. if you're bad at understanding yourself, you will misunderstand your priorities, and also you will not be able to control your priorities, because you won't know what kinds of evidence will really persuade your brain to adopt a specific priority, and your brain will learn that it can't really trust you to assign it priorities to satisfy its motives (burnout).",
      "plaintextDescription": "why is ADHD also strongly correlated with systematization? it could just be worse self modelling - ADHD happens when your brain's model of its own priorities and motivations falls out of sync from your brain's actual priorities and motivations. if you're bad at understanding yourself, you will misunderstand your priorities, and also you will not be able to control your priorities, because you won't know what kinds of evidence will really persuade your brain to adopt a specific priority, and your brain will learn that it can't really trust you to assign it priorities to satisfy its motives (burnout)."
    },
    "baseScore": 16,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-05-04T16:05:51.563Z",
    "af": false
  },
  {
    "_id": "yoDojNrY46tTAH2M9",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "GkL3QQqr6wkAoANHy",
    "topLevelCommentId": "GkL3QQqr6wkAoANHy",
    "contents": {
      "markdown": "it is kind of funny that caring a lot about reflective stability of alignment proposals and paradoxes arising from self modelling (e.g in action counterfactuals) is most common in the people who are the worst at modelling themselves",
      "plaintextDescription": "it is kind of funny that caring a lot about reflective stability of alignment proposals and paradoxes arising from self modelling (e.g in action counterfactuals) is most common in the people who are the worst at modelling themselves"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-04T15:59:27.053Z",
    "af": false
  },
  {
    "_id": "GkL3QQqr6wkAoANHy",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "random half baked thoughts from a sleep deprived jet lagged mind: my guess is that the few largest principal components of variance of human intelligence are something like:\n\n*   a general factor that affects all cognitive abilities uniformly (this is a sum of a bazillion things. they could be something physiological like better cardiovascular function, or more efficient mitochondria or something; or maybe there's some pretty general learning/architecture hyperparameter akin to lr or aspect ratio that simply has better or worse configurations. each small change helps/hurts a little bit). having a better general factor makes you better at pattern recognition and prediction, which is the foundation of all intelligence. whether this is learning a policy or a world model, you need to be able to spot regularities in the world to exploit to have any hope of making good predictions.may\n*   a systematization factor (how much to be inclined towards using the machinery of pattern recognition towards finding and operating using explicit rules about the world, vs using that machinery implicitly and relying on intuition). this is the autist vs normie axis. importantly, it's not like normies are born with hard coded social skills modules, for the same reason that humans aren't born with language modules (sorry chomsky). we learn these things by being general reasoners placed in an environment where we are exposed to language and social interactions, etc. it just turns out that systematizing is generally really good for certain kinds of domains (math, CS) and pretty bad for other domains (social interaction). I think this explains why some people of decent general intelligence just cannot grasp basic CS concepts, or vice versa for social interaction. this is because as the number of layers of abstraction increases, it becomes increasingly difficult to model the system from the bottom up, until at some point there's a phase transition where it becomes better to model the system top down and give up on any hope of ever understanding it mechanistically. this is exacerbated through life because people generally accumulate knowledge (both explicit and tacit) faster in the domains in which they already excel, and because of declining neuroplasticity with age. modern society is increasingly constructed to be amenable to systematization (via laws, contracts, standards, etc), because systematization is necessary to govern and scale a civilization. (I think this axis also explains a correlation with embodiment/integratedness, though it's unclear under this theory how exactly the causality should work there. maybe only being able to systematize makes it harder to model the self? intuitively, it feels like intervening on integratedness causes one to become better at non-systematizing reasoning as a whole, though that could just be because we're actually intervening on the common cause and using self-modelling as a tight feedback loop)\n\nthere are other big components (exploration vs exploitation, risk tolerance, creativity, memory) that aren't explained here.\n\nI might clean up my thinking and write something more comprehensible later. none of these ideas are novel, but I think a lot can be gained through pinning them down exactly. unfortunately, on priors, this kind of theoretical speculation is rarely useful. though it might be possible to test parts of theories like this experimentally.",
      "plaintextDescription": "random half baked thoughts from a sleep deprived jet lagged mind: my guess is that the few largest principal components of variance of human intelligence are something like:\n\n * a general factor that affects all cognitive abilities uniformly (this is a sum of a bazillion things. they could be something physiological like better cardiovascular function, or more efficient mitochondria or something; or maybe there's some pretty general learning/architecture hyperparameter akin to lr or aspect ratio that simply has better or worse configurations. each small change helps/hurts a little bit). having a better general factor makes you better at pattern recognition and prediction, which is the foundation of all intelligence. whether this is learning a policy or a world model, you need to be able to spot regularities in the world to exploit to have any hope of making good predictions.may\n * a systematization factor (how much to be inclined towards using the machinery of pattern recognition towards finding and operating using explicit rules about the world, vs using that machinery implicitly and relying on intuition). this is the autist vs normie axis. importantly, it's not like normies are born with hard coded social skills modules, for the same reason that humans aren't born with language modules (sorry chomsky). we learn these things by being general reasoners placed in an environment where we are exposed to language and social interactions, etc. it just turns out that systematizing is generally really good for certain kinds of domains (math, CS) and pretty bad for other domains (social interaction). I think this explains why some people of decent general intelligence just cannot grasp basic CS concepts, or vice versa for social interaction. this is because as the number of layers of abstraction increases, it becomes increasingly difficult to model the system from the bottom up, until at some point there's a phase transition where it becomes better to model the system top d"
    },
    "baseScore": 48,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2025-05-04T15:56:31.129Z",
    "af": false
  },
  {
    "_id": "vcz3aAEpejiWS5rtt",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "execution is necessary for success, but direction is what sets apart merely impressive and truly great accomplishment. though being better at execution can make you better at direction, because it enables you to work on directions that others discard as impossible.",
      "plaintextDescription": "execution is necessary for success, but direction is what sets apart merely impressive and truly great accomplishment. though being better at execution can make you better at direction, because it enables you to work on directions that others discard as impossible."
    },
    "baseScore": 47,
    "voteCount": 20,
    "createdAt": null,
    "postedAt": "2025-05-02T13:51:49.425Z",
    "af": false
  },
  {
    "_id": "avQTSari7BqWrQNsf",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "conference talks aren't worth going to irl because they're recorded anyways. ofc, you're not actually going to remember to watch the recording, but it's not like anyone pays attention at the irl talk anyways",
      "plaintextDescription": "conference talks aren't worth going to irl because they're recorded anyways. ofc, you're not actually going to remember to watch the recording, but it's not like anyone pays attention at the irl talk anyways"
    },
    "baseScore": 8,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-04-29T00:34:07.704Z",
    "af": false
  },
  {
    "_id": "g4nhoRMNFvqytiHRB",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "6D6si9eZhLxzBC3xk",
    "topLevelCommentId": "eejkQsL8qzbcv68tH",
    "contents": {
      "markdown": "are you saying something like: you can't actually do more of everything except one thing, because you'll never do everything. so there's a lot of variance that comes from exploration that *multiplies* with your \\\\(\\mathcal O(k^2)\\\\) variance from having a suboptimal zero point. so in practice your \\\\(k\\\\) needs to be very close to optimal. so my thing is true but not useful in practice.\n\ni feel people do empirically shift \\\\(k\\\\) quite a lot throughout life and it does seem to change how effectively they learn. if you're mildly depressed your \\\\(k\\\\) is slightly too low and you learn a little bit slower. if you're mildly manic your \\\\(k\\\\) is too high and you also learn a little bit slower. therapy, medications, and meditations shift \\\\(k\\\\) mildly.",
      "plaintextDescription": "are you saying something like: you can't actually do more of everything except one thing, because you'll never do everything. so there's a lot of variance that comes from exploration that multiplies with your O(k2) variance from having a suboptimal zero point. so in practice your k needs to be very close to optimal. so my thing is true but not useful in practice.\n\ni feel people do empirically shift k quite a lot throughout life and it does seem to change how effectively they learn. if you're mildly depressed your k is slightly too low and you learn a little bit slower. if you're mildly manic your k is too high and you also learn a little bit slower. therapy, medications, and meditations shift k mildly."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-04-23T21:15:05.420Z",
    "af": false
  },
  {
    "_id": "KdxGxYYkj47PkajPx",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "t8B6fAHc5zskadf92",
    "topLevelCommentId": "eejkQsL8qzbcv68tH",
    "contents": {
      "markdown": "is this for a reason other than the variance thing I mention?\n\nI think the thing I mention is still important is because it means there is no fundamental difference between positive and negative motivation. I agree that if everything was different degrees of extreme bliss then the variance would be so high that you never learn anything in practice. but if you shift everything slightly such that some mildly unpleasant things are now mildly pleasant, I claim this will make learning a bit faster or slower but still converge to the same thing.",
      "plaintextDescription": "is this for a reason other than the variance thing I mention?\n\nI think the thing I mention is still important is because it means there is no fundamental difference between positive and negative motivation. I agree that if everything was different degrees of extreme bliss then the variance would be so high that you never learn anything in practice. but if you shift everything slightly such that some mildly unpleasant things are now mildly pleasant, I claim this will make learning a bit faster or slower but still converge to the same thing."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-04-21T06:45:39.489Z",
    "af": false
  },
  {
    "_id": "HPq5zT43DCYup2fFj",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "eejkQsL8qzbcv68tH",
    "topLevelCommentId": "eejkQsL8qzbcv68tH",
    "contents": {
      "markdown": "i don't think this is unique to world models. you can also think of rewards as things you move towards or away from. this is compatible with translation/scaling-invariance because if you move towards everything but move towards X even more, then in the long run you will do more of X on net, because you only have so much probability mass to go around.\n\ni have an alternative hypothesis for why positive and negative motivation feel distinct in humans.\n\nalthough the *expectation* of the reward gradient doesn't change if you translate the reward, it hugely affects the variance of the gradient.[^wzgx2dgkqx] in other words, if you always move towards everything, you will still eventually learn the right thing, but it will take a lot longer.\n\nmy hypothesis is that humans have some hard coded baseline for variance reduction. in the ancestral environment, the expectation of perceived reward was centered around where zero feels to be. our minds do try to adjust to changes in distribution (e.g hedonic adaptation), but it's not perfect, and so in the current world, our baseline may be suboptimal.\n\n[^wzgx2dgkqx]: Quick proof sketch (this is a very standard result in RL and is the motivation for advantage estimation, but still good practice to check things).The REINFORCE estimator is \\(\\nabla_\\theta \\text{R} = \\mathbb E_{\\tau \\sim \\pi(\\cdot)} [R(\\tau) \\nabla_\\theta \\log \\pi(\\tau)]\\). WLOG, suppose we define a new reward \\(R'(\\tau) = R(\\tau) + k\\) where \\(k > 0\\) (and assume that \\(\\mathbb E [R] = 0\\), so \\(R'\\) is moving away from the mean).Then we can verify the expectation of the gradient is still the same:\\(\\nabla_\\theta \\text{R'} - \\nabla_\\theta \\text{R} = \\mathbb E_{\\tau \\sim \\pi(\\cdot)} [\\nabla_\\theta \\log \\pi(\\tau)] = \\int \\pi(\\tau) \\frac{\\nabla_\\theta \\pi(\\tau)}{\\pi(\\tau)} \\mathrm d\\tau = 0\\).But the variance increases:\\(\\mathbb V_{\\tau \\sim \\pi(\\cdot)} [R(\\tau) \\nabla_\\theta \\log \\pi(\\tau)] = \\int R(\\tau)^2 (\\nabla_\\theta \\log \\pi(\\tau))^2 \\pi(\\tau) \\mathrm d\\tau - (\\nabla_\\theta R(\\tau))^2\\)\\(\\mathbb V_{\\tau \\sim \\pi(\\cdot)} [R'(\\tau) \\nabla_\\theta \\log \\pi(\\tau)] = \\int (R(\\tau) + k)^2 (\\nabla_\\theta \\log \\pi(\\tau))^2 \\pi(\\tau) \\mathrm d\\tau - (\\nabla_\\theta R(\\tau))^2\\)So:\\(\\mathbb V_{\\tau \\sim \\pi(\\cdot)} [R'(\\tau) \\nabla_\\theta \\log \\pi(\\tau)] - \\mathbb V_{\\tau \\sim \\pi(\\cdot)} [R(\\tau) \\nabla_\\theta \\log \\pi(\\tau)] = 2 k \\int R(\\tau) (\\nabla_\\theta \\log \\pi(\\tau))^2 \\pi(\\tau) \\mathrm d\\tau + k^2 \\int (\\nabla_\\theta \\log \\pi(\\tau))^2 \\pi(\\tau) \\mathrm d\\tau\\)Obviously, both terms on the right have to be non-negative. More generally, if \\(\\mathbb E[R] = k\\), the variance increases with \\(\\mathcal O(k^2)\\). So having your rewards be uncentered hurts a ton.",
      "plaintextDescription": "i don't think this is unique to world models. you can also think of rewards as things you move towards or away from. this is compatible with translation/scaling-invariance because if you move towards everything but move towards X even more, then in the long run you will do more of X on net, because you only have so much probability mass to go around.\n\ni have an alternative hypothesis for why positive and negative motivation feel distinct in humans.\n\nalthough the expectation of the reward gradient doesn't change if you translate the reward, it hugely affects the variance of the gradient.[1] in other words, if you always move towards everything, you will still eventually learn the right thing, but it will take a lot longer.\n\nmy hypothesis is that humans have some hard coded baseline for variance reduction. in the ancestral environment, the expectation of perceived reward was centered around where zero feels to be. our minds do try to adjust to changes in distribution (e.g hedonic adaptation), but it's not perfect, and so in the current world, our baseline may be suboptimal.\n\n \n\n 1. ^\n    Quick proof sketch (this is a very standard result in RL and is the motivation for advantage estimation, but still good practice to check things).\n    \n    The REINFORCE estimator is ∇θR=Eτ∼π(⋅)[R(τ)∇θlogπ(τ)].\n    \n     \n    \n    WLOG, suppose we define a new reward R′(τ)=R(τ)+k where k>0 (and assume that E[R]=0, so R′ is moving away from the mean).\n    \n    Then we can verify the expectation of the gradient is still the same:∇θR'−∇θR=Eτ∼π(⋅)[∇θlogπ(τ)]=∫π(τ)∇θπ(τ)π(τ)dτ=0.\n    \n    But the variance increases:\n    \n    Vτ∼π(⋅)[R(τ)∇θlogπ(τ)]=∫R(τ)2(∇θlogπ(τ))2π(τ)dτ−(∇θR(τ))2\n    \n    Vτ∼π(⋅)[R′(τ)∇θlogπ(τ)]=∫(R(τ)+k)2(∇θlogπ(τ))2π(τ)dτ−(∇θR(τ))2\n    \n    So:\n    \n    Vτ∼π(⋅)[R′(τ)∇θlogπ(τ)]−Vτ∼π(⋅)[R(τ)∇θlogπ(τ)]=2k∫R(τ)(∇θlogπ(τ))2π(τ)dτ+k2∫(∇θlogπ(τ))2π(τ)dτ\n    \n    Obviously, both terms on the right have to be non-negative. More generally, if E[R]=k, the variance increases with "
    },
    "baseScore": 12,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-04-14T06:23:43.512Z",
    "af": false
  },
  {
    "_id": "vpLirKzKjqNKccYZz",
    "postId": "LpYWG67J7XwtZdFwu",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "seems like an interesting idea. I had never heard of it before, and I'm generally decently aware of weird stuff like this, so they probably need to put more effort into publicity. I don't know if truly broad public appeal will ever happen but I could imagine this being pretty popular among the kinds of people who would e.g use manifold.\n\none perverse incentive this scheme creates is that if you think other charity is better than political donations, you are incentivized to donate to the party with less in its pool, and you get a 1:1 match for free, at the expense of people who wanted to support a candidate.\n\nalso, in the grand scheme of things, the amount of money in politics isn't that big, but it's still a solid chunk. but the TAM is inherently quite limited.\n\nhttps://slatestarcodex.com/2019/09/18/too-much-dark-money-in-almonds/",
      "plaintextDescription": "seems like an interesting idea. I had never heard of it before, and I'm generally decently aware of weird stuff like this, so they probably need to put more effort into publicity. I don't know if truly broad public appeal will ever happen but I could imagine this being pretty popular among the kinds of people who would e.g use manifold.\n\none perverse incentive this scheme creates is that if you think other charity is better than political donations, you are incentivized to donate to the party with less in its pool, and you get a 1:1 match for free, at the expense of people who wanted to support a candidate.\n\nalso, in the grand scheme of things, the amount of money in politics isn't that big, but it's still a solid chunk. but the TAM is inherently quite limited.\n\nhttps://slatestarcodex.com/2019/09/18/too-much-dark-money-in-almonds/"
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-04-13T19:18:30.967Z",
    "af": false
  },
  {
    "_id": "Xbt65D6hoMfC99rJt",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "i find it disappointing that a lot of people believe things about trading that are obviously crazy even if you only believe in a very weak form of the EMH. for example, technical analysis is obviously tea leaf reading - if it were predictive whatsoever, you could make a lot of money by exploiting it until it is no longer predictive.",
      "plaintextDescription": "i find it disappointing that a lot of people believe things about trading that are obviously crazy even if you only believe in a very weak form of the EMH. for example, technical analysis is obviously tea leaf reading - if it were predictive whatsoever, you could make a lot of money by exploiting it until it is no longer predictive. "
    },
    "baseScore": 10,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-04-09T08:31:48.148Z",
    "af": false
  },
  {
    "_id": "tokPb5i2YjDxKNhB9",
    "postId": "A5PJYKw8tw38JpbJC",
    "parentCommentId": "h63fyaGCyeqS7HaGg",
    "topLevelCommentId": "MNKnJvjEfRGqAu5Qc",
    "contents": {
      "markdown": "Not necessarily. Returns could be long tailed, so hypersuccess or bust could be higher EV. though you might be risk averse enough that this isn't worth it to you.",
      "plaintextDescription": "Not necessarily. Returns could be long tailed, so hypersuccess or bust could be higher EV. though you might be risk averse enough that this isn't worth it to you."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-04-07T16:13:12.078Z",
    "af": false
  },
  {
    "_id": "sh7km9danaCjnuZr6",
    "postId": "A5PJYKw8tw38JpbJC",
    "parentCommentId": "MNKnJvjEfRGqAu5Qc",
    "topLevelCommentId": "MNKnJvjEfRGqAu5Qc",
    "contents": {
      "markdown": "unless your goal is hypersuccess or bust",
      "plaintextDescription": "unless your goal is hypersuccess or bust"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-04-07T08:04:13.889Z",
    "af": false
  },
  {
    "_id": "BLfcnJ2GYQgW5tM7n",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I wonder how many supposedly consistently successful retail traders are actually just picking up pennies in front of the steamroller, and would eventually lose it all if they kept at it long enough.\n\nalso I wonder how many people have runs of very good performance interspersed by big losses, such that the overall net gains are relatively modest, but psychologically they only remember/recount the runs of good performance, whereas the losses were just bad luck and will be avoided next time.",
      "plaintextDescription": "I wonder how many supposedly consistently successful retail traders are actually just picking up pennies in front of the steamroller, and would eventually lose it all if they kept at it long enough.\n\nalso I wonder how many people have runs of very good performance interspersed by big losses, such that the overall net gains are relatively modest, but psychologically they only remember/recount the runs of good performance, whereas the losses were just bad luck and will be avoided next time."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-04-07T07:44:44.389Z",
    "af": false
  },
  {
    "_id": "CMgKovvLzSTzuJP9J",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "ZiEG2yEqnTb88h9ph",
    "topLevelCommentId": "cZADfF3wbZqbaCFCB",
    "contents": {
      "markdown": "I mean, the proximate cause of the 1989 protests was the death of the quite reformist general secretary Hu Yaobang. The new general secretary, Zhao Ziyang, was very sympathetic towards the protesters and wanted to negotiate with them, but then he lost a power struggle against Li Peng and Deng Xiaoping (who was in semi retirement but still held onto control of the military). Immediately afterwards, he was removed as general secretary and martial law was declared, leading to the massacre.",
      "plaintextDescription": "I mean, the proximate cause of the 1989 protests was the death of the quite reformist general secretary Hu Yaobang. The new general secretary, Zhao Ziyang, was very sympathetic towards the protesters and wanted to negotiate with them, but then he lost a power struggle against Li Peng and Deng Xiaoping (who was in semi retirement but still held onto control of the military). Immediately afterwards, he was removed as general secretary and martial law was declared, leading to the massacre."
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-04-05T00:20:05.837Z",
    "af": false
  },
  {
    "_id": "FJWzepwTkTkf7uDxu",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "6reSQdPbf6e7qJ6pF",
    "topLevelCommentId": "cZADfF3wbZqbaCFCB",
    "contents": {
      "markdown": "It's often very costly to do so - for example, ending the zero covid policy was very politically costly even though it was the right thing to do. Also, most major reconfigurations even for autocratic countries probably mostly happen right after there is a transition of power (for China, Mao is kind of an exception, but thats because he had so much power that it was impossible to challenge his authority even when he messed up).",
      "plaintextDescription": "It's often very costly to do so - for example, ending the zero covid policy was very politically costly even though it was the right thing to do. Also, most major reconfigurations even for autocratic countries probably mostly happen right after there is a transition of power (for China, Mao is kind of an exception, but thats because he had so much power that it was impossible to challenge his authority even when he messed up)."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-04-04T18:24:57.365Z",
    "af": false
  }
]