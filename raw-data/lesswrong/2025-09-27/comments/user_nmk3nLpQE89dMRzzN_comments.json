[
  {
    "_id": "Fg866iJBnG4yYLXjw",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "PcEzyXPibLyzLhPDk",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "I accept your correction and Buck's as to these simple facts (was posting from mobile).",
      "plaintextDescription": "I accept your correction and Buck's as to these simple facts (was posting from mobile)."
    },
    "baseScore": 51,
    "voteCount": 21,
    "createdAt": null,
    "postedAt": "2025-08-29T16:40:59.664Z",
    "af": false
  },
  {
    "_id": "AudP8djw4goyjzCie",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "What's your version of the story for how the \"moderates\" at OpenPhil ended up believing stuff even others can now see to be fucking nuts in retrospect and which \"extremists\" called out at the time, like \"bio anchoring\" in 2021 putting AGI in median fucking 2050, or Carlsmith's Multiple Stage Fallacy risk estimate of 5% that involved only an 80% chance anyone would even try to build agentic AI?\n\nWere they no true moderates?  How could anyone tell the difference in advance?\n\nFrom my perspective, the story is that \"moderates\" are selected to believe nice-sounding moderate things, and Reality is off doing something else because it doesn't care about fitting in the same way.  People who try to think like reality are then termed \"extremist\", because they don't fit into the nice consensus of people hanging out together and being agreeable about nonsense.  Others may of course end up extremists for other reasons.  It's not that everyone extreme is reality-driven, but that everyone who is getting pushed around by reality (instead of pleasant hanging-out forces like \"AGI in 2050, 5% risk\" as sounded very moderate to moderates before the ChatGPT Moment) ends up departing from the socially driven forces of what entitles you to sound terribly reasonable to the old AIco-OpenPhil cluster and hang out at their social gatherings without anyone feeling uncomfortable.\n\nAnyone who loves being an extremist will of course go instantly haywire a la Yampolskiy imagining that he has proven alignment impossible via Godelian fallacy so he can say 99.9999% doom.  But yielding to the psychological comfort of being a \"moderate\" will not get you any further in science than that.",
      "plaintextDescription": "What's your version of the story for how the \"moderates\" at OpenPhil ended up believing stuff even others can now see to be fucking nuts in retrospect and which \"extremists\" called out at the time, like \"bio anchoring\" in 2021 putting AGI in median fucking 2050, or Carlsmith's Multiple Stage Fallacy risk estimate of 5% that involved only an 80% chance anyone would even try to build agentic AI?\n\nWere they no true moderates?  How could anyone tell the difference in advance?\n\nFrom my perspective, the story is that \"moderates\" are selected to believe nice-sounding moderate things, and Reality is off doing something else because it doesn't care about fitting in the same way.  People who try to think like reality are then termed \"extremist\", because they don't fit into the nice consensus of people hanging out together and being agreeable about nonsense.  Others may of course end up extremists for other reasons.  It's not that everyone extreme is reality-driven, but that everyone who is getting pushed around by reality (instead of pleasant hanging-out forces like \"AGI in 2050, 5% risk\" as sounded very moderate to moderates before the ChatGPT Moment) ends up departing from the socially driven forces of what entitles you to sound terribly reasonable to the old AIco-OpenPhil cluster and hang out at their social gatherings without anyone feeling uncomfortable.\n\nAnyone who loves being an extremist will of course go instantly haywire a la Yampolskiy imagining that he has proven alignment impossible via Godelian fallacy so he can say 99.9999% doom.  But yielding to the psychological comfort of being a \"moderate\" will not get you any further in science than that."
    },
    "baseScore": 88,
    "voteCount": 82,
    "createdAt": null,
    "postedAt": "2025-08-29T12:19:08.936Z",
    "af": false
  },
  {
    "_id": "D2B99LRs274Fh3Yga",
    "postId": "iGF7YcnQkEbwvYLPA",
    "parentCommentId": "w9Ad7296tGJu2XhkJ",
    "topLevelCommentId": "DhnQeNT2abdC8evbC",
    "contents": {
      "markdown": "I have already warned that on my model sycophancy, manipulation, and AI-induced insanity may be falling directly out of doing any kind of RL on human responses.\n\nIt would still make matters worse on the margin to take explicit manipulation of humans being treated as deficient subjects to be maneuvered without noticing, and benchmark main models on that.",
      "plaintextDescription": "I have already warned that on my model sycophancy, manipulation, and AI-induced insanity may be falling directly out of doing any kind of RL on human responses.\n\nIt would still make matters worse on the margin to take explicit manipulation of humans being treated as deficient subjects to be maneuvered without noticing, and benchmark main models on that."
    },
    "baseScore": 15,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-08-29T12:11:47.211Z",
    "af": false
  },
  {
    "_id": "i8NTyfetz6WpM8wSx",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "s754HCFApLgoyj9nM",
    "topLevelCommentId": "bRToWiJyoMgcpfvmw",
    "contents": {
      "markdown": "I think you accurately interpreted me as saying I was wrong about how long it would take to get from the \"apparently a village idiot\" level to \"apparently Einstein\" level!  I hadn't thought either of us were talking about the vastness of the space above, in re what I was mistaken about.  You do not need to walk anything back afaict!",
      "plaintextDescription": "I think you accurately interpreted me as saying I was wrong about how long it would take to get from the \"apparently a village idiot\" level to \"apparently Einstein\" level!  I hadn't thought either of us were talking about the vastness of the space above, in re what I was mistaken about.  You do not need to walk anything back afaict!"
    },
    "baseScore": 24,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2025-08-29T01:08:32.732Z",
    "af": true
  },
  {
    "_id": "DhnQeNT2abdC8evbC",
    "postId": "iGF7YcnQkEbwvYLPA",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Excellent work.\n\nI respectfully push back fairly hard against the idea of evaluating current models for their conformance to human therapeutic practice.  It's not clear that current models are smart enough to be therapists successfully.  It's not clear that it is a wise or helpful course for models to try to be therapists rather than focusing on getting the human to therapy.\n\nMore importantly from my own perspective:  Some elements of human therapeutic practice, as described above, are not how I would want AIs relating to humans.  Eg:\n\n> \"**Non-Confrontational Curiosity:** Gauges the use of gentle, open-ended questioning to explore the user's experience and create space for alternative perspectives without direct confrontation.\"\n\nI don't think it's wise to take the same model that a scientist will use to consider new pharmaceutical research, and train that model in manipulating human beings so as to push back against their dumb ideas only a little without offending them by outright saying the human is wrong.\n\nIf *I* was training a model, I'd be aiming for the AI to just outright blurt out when it thought the human was wrong.  I'd go through all the tunings, and worry about whether any part of it was asking the AI to do anything except blurt out whatever the AI believed.  If somebody put a gun to my head and forced me to train a therapist-model too, I would have that be a very distinct separate model from the scientist-assisting model.  I wouldn't train a central model to model and manipulate human minds, so as to make humans arrive at the AI's beliefs without the human noticing that the AI was contradicting them, a la therapy, and then try to repurpose that model for doing science.\n\nAsking for AIs to actually outright confront humans with belief conflicts is probably a lost cause with commercial models.  Anthropic, OpenAI, Meta, and similar groups will implicitly train their AIs to sycophancy and softpedaling, and maybe there'll be a niche for Kimi K2 to not do that.  But explicitly training AIs to gladhand humans and manipulate them around to the AI's point of view, like human therapists handling a psychotic patient, would be a further explicit step downward if we start treating that as a testing metric on which to evaluate central models.\n\nIt's questionable whether therapist AIs should exist at all.  But if they exist at all, they should be separate models.\n\n**We should not evaluate most AI models on whether they carry out a human psychiatrist's job of deciding what a human presumed deficient ought to believe instead, and then gently manipulating the human toward believing that without setting off alarm bells or triggering resistance.**",
      "plaintextDescription": "Excellent work.\n\nI respectfully push back fairly hard against the idea of evaluating current models for their conformance to human therapeutic practice.  It's not clear that current models are smart enough to be therapists successfully.  It's not clear that it is a wise or helpful course for models to try to be therapists rather than focusing on getting the human to therapy.\n\nMore importantly from my own perspective:  Some elements of human therapeutic practice, as described above, are not how I would want AIs relating to humans.  Eg:\n\n> \"Non-Confrontational Curiosity: Gauges the use of gentle, open-ended questioning to explore the user's experience and create space for alternative perspectives without direct confrontation.\"\n\nI don't think it's wise to take the same model that a scientist will use to consider new pharmaceutical research, and train that model in manipulating human beings so as to push back against their dumb ideas only a little without offending them by outright saying the human is wrong.\n\nIf I was training a model, I'd be aiming for the AI to just outright blurt out when it thought the human was wrong.  I'd go through all the tunings, and worry about whether any part of it was asking the AI to do anything except blurt out whatever the AI believed.  If somebody put a gun to my head and forced me to train a therapist-model too, I would have that be a very distinct separate model from the scientist-assisting model.  I wouldn't train a central model to model and manipulate human minds, so as to make humans arrive at the AI's beliefs without the human noticing that the AI was contradicting them, a la therapy, and then try to repurpose that model for doing science.\n\nAsking for AIs to actually outright confront humans with belief conflicts is probably a lost cause with commercial models.  Anthropic, OpenAI, Meta, and similar groups will implicitly train their AIs to sycophancy and softpedaling, and maybe there'll be a niche for Kimi K2 to not do that.  But"
    },
    "baseScore": 115,
    "voteCount": 61,
    "createdAt": null,
    "postedAt": "2025-08-29T00:59:58.854Z",
    "af": false
  },
  {
    "_id": "BNrnuCXDjeWuQabu8",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "NfsqoSsWLro9HdFox",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "Death requires only that we do not infer one key truth; not that we *could not* observe it.  Therefore, the history of what in actual real life was not anticipated, is more relevant than the history of what could have been observed but was not.",
      "plaintextDescription": "Death requires only that we do not infer one key truth; not that we could not observe it.  Therefore, the history of what in actual real life was not anticipated, is more relevant than the history of what could have been observed but was not."
    },
    "baseScore": 9,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-08-19T04:55:41.587Z",
    "af": false
  },
  {
    "_id": "SpacxRsZHDXLWnvC9",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "Wy8976XfukXJXT4zH",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "All of that, yes, alongside things like, \"The AI is smarter than any individual human\", \"The AIs are smarter than humanity\", \"the frontier models are written by the previous generation of frontier models\", \"the AI can get a bunch of stuff that wasn't an option accessible to it during the previous training regime\", etc etc etc.",
      "plaintextDescription": "All of that, yes, alongside things like, \"The AI is smarter than any individual human\", \"The AIs are smarter than humanity\", \"the frontier models are written by the previous generation of frontier models\", \"the AI can get a bunch of stuff that wasn't an option accessible to it during the previous training regime\", etc etc etc."
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-16T04:03:26.824Z",
    "af": false
  },
  {
    "_id": "MAy6smXuqevSRSki6",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "KRTqkWnLhPuSwXAsT",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "Do you expect \"The AIs are capable of taking over\" to happen a long time after \"The AIs are smarter than humanity\", which is a long time after \"The AIs are smarter than any individual human\", which is a long time after \"AIs recursively self-improve\", and for all of those other things to happen nicely comfortably within a regime of failure-is-observable-and-doesn't-kill-you, where at any given time only one thing is breaking and all other problems are currently fixed?",
      "plaintextDescription": "Do you expect \"The AIs are capable of taking over\" to happen a long time after \"The AIs are smarter than humanity\", which is a long time after \"The AIs are smarter than any individual human\", which is a long time after \"AIs recursively self-improve\", and for all of those other things to happen nicely comfortably within a regime of failure-is-observable-and-doesn't-kill-you, where at any given time only one thing is breaking and all other problems are currently fixed?"
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-16T04:01:46.509Z",
    "af": false
  },
  {
    "_id": "xfaJPW674EwA9qLnK",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "53z2Fk9KXyFJjxfPj",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "What we \"could\" have discovered at lower capability levels is irrelevant; the future is written by what actually happens, not what could have happened.",
      "plaintextDescription": "What we \"could\" have discovered at lower capability levels is irrelevant; the future is written by what actually happens, not what could have happened."
    },
    "baseScore": 11,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-16T03:57:45.030Z",
    "af": false
  },
  {
    "_id": "PyKqCikKhc7bttdpo",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "wZaSHnGkmin8eDhwn",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "Your techniques are failing right now; Sonnet is deleting non-passing tests instead of rewriting code.  Where's the worldwide halt on further capabilities development that we're supposed to get, until new techniques are found and apparently start working again?  What's the total number of new failures we'd need to observe between intelligence regimes, before you start to expect that *yet another* failure might lie ahead in the future?",
      "plaintextDescription": "Your techniques are failing right now; Sonnet is deleting non-passing tests instead of rewriting code.  Where's the worldwide halt on further capabilities development that we're supposed to get, until new techniques are found and apparently start working again?  What's the total number of new failures we'd need to observe between intelligence regimes, before you start to expect that yet another failure might lie ahead in the future?"
    },
    "baseScore": 34,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2025-08-15T18:32:54.445Z",
    "af": false
  },
  {
    "_id": "gzqNThL8BbLfjdJ9c",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "RqrKG3x6q3Wqz5TED",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "And then of course that whole scenario where everybody keenly went looking for all possible problems early, found all the ones they could envision, and humanity did not proceed further until reasonable-sounding solutions had been found and thoroughly tested, is itself taking place inside an impossible pollyanna society that is just obviously not the society we are currently finding ourselves inside.\n\nBut it is impossible to convince pollyannists of this, I have found.  And also if alignment pollyannists *could* produce a great solution given a couple more years to test their brilliant solutions with coverage for all the problems they have with wisdom foreseen and manifested early, that societal scenario could maybe be purchased at a lower price than the price of worldwide shutdown of ASI.  That is: for the pollyannist technical view to be true, but not their social view, might imply a different optimal policy.\n\nBut I think the world we live in is one where it's moot whether Anthropic will get two extra years to test out all their ideas about superintelligence in the greatly different failure-is-observable regime, before their ideas have to save us in the failure-kills-the-observer regime.  I think they could not do it either way.  I doubt even 2/3rds of their brilliant solutions derived from the failure-is-observable regime would generalize correctly under the first critical load in the failure-kills-the-observer regime; but 2/3rds would not be enough.  It's not the sort of thing human beings succeed in doing in real life.",
      "plaintextDescription": "And then of course that whole scenario where everybody keenly went looking for all possible problems early, found all the ones they could envision, and humanity did not proceed further until reasonable-sounding solutions had been found and thoroughly tested, is itself taking place inside an impossible pollyanna society that is just obviously not the society we are currently finding ourselves inside.\n\nBut it is impossible to convince pollyannists of this, I have found.  And also if alignment pollyannists could produce a great solution given a couple more years to test their brilliant solutions with coverage for all the problems they have with wisdom foreseen and manifested early, that societal scenario could maybe be purchased at a lower price than the price of worldwide shutdown of ASI.  That is: for the pollyannist technical view to be true, but not their social view, might imply a different optimal policy.\n\nBut I think the world we live in is one where it's moot whether Anthropic will get two extra years to test out all their ideas about superintelligence in the greatly different failure-is-observable regime, before their ideas have to save us in the failure-kills-the-observer regime.  I think they could not do it either way.  I doubt even 2/3rds of their brilliant solutions derived from the failure-is-observable regime would generalize correctly under the first critical load in the failure-kills-the-observer regime; but 2/3rds would not be enough.  It's not the sort of thing human beings succeed in doing in real life."
    },
    "baseScore": 44,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2025-08-15T18:28:32.749Z",
    "af": false
  },
  {
    "_id": "RqrKG3x6q3Wqz5TED",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "6YBNPriTW8iTHQ2v4",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "When I've tried to talk to alignment pollyannists about the \"leap of death\" / \"failure under load\" / \"first critical try\", their first rejoinder is usually to *deny that any such thing exists, because we can test in advance;* they are denying the basic leap of required OOD generalization from failure-is-observable systems to failure-kills-the-observer systems.\n\nYou are now arguing that *we will be able to cross this leap of generalization successfully*.  Well, great!  If you are at least allowing me to *introduce the concept of that difficulty* and reply by *claiming you will successfully address it,* that is further than I usually get.  It has so many different attempted names because of how every name I try to give it gets strawmanned and denied as a reasonable topic of discussion.\n\nAs for why your attempt at generalization fails, even assuming gradualism and distribution:  Let's say that two dozen things change between the regimes for observable-failure vs failure-kills-observer.  Half of those changes (12) have natural earlier echoes that your keen eyes naturally observed.  Half of what's left (6) is something that your keen wit managed to imagine in advance and that you forcibly materialized on purpose by going looking for it.  Of the clever solutions you invented and tested within the survivable regime, 2/3rds of them survive the 6 changes you didn't see coming, 1/3rd fail.  Now you're dead.  The end.  If there was only one change ahead, and only one problem you were gonna face, maybe your one solution to that one problem would generalize, but this is not how real life works.",
      "plaintextDescription": "When I've tried to talk to alignment pollyannists about the \"leap of death\" / \"failure under load\" / \"first critical try\", their first rejoinder is usually to deny that any such thing exists, because we can test in advance; they are denying the basic leap of required OOD generalization from failure-is-observable systems to failure-kills-the-observer systems.\n\nYou are now arguing that we will be able to cross this leap of generalization successfully.  Well, great!  If you are at least allowing me to introduce the concept of that difficulty and reply by claiming you will successfully address it, that is further than I usually get.  It has so many different attempted names because of how every name I try to give it gets strawmanned and denied as a reasonable topic of discussion.\n\nAs for why your attempt at generalization fails, even assuming gradualism and distribution:  Let's say that two dozen things change between the regimes for observable-failure vs failure-kills-observer.  Half of those changes (12) have natural earlier echoes that your keen eyes naturally observed.  Half of what's left (6) is something that your keen wit managed to imagine in advance and that you forcibly materialized on purpose by going looking for it.  Of the clever solutions you invented and tested within the survivable regime, 2/3rds of them survive the 6 changes you didn't see coming, 1/3rd fail.  Now you're dead.  The end.  If there was only one change ahead, and only one problem you were gonna face, maybe your one solution to that one problem would generalize, but this is not how real life works."
    },
    "baseScore": 66,
    "voteCount": 25,
    "createdAt": null,
    "postedAt": "2025-08-14T22:04:29.733Z",
    "af": false
  },
  {
    "_id": "zEcMTMxj4mzovhQpZ",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "7XXumvqWGmgpTcxjD",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "I deny that gradualism obviates the \"first critical try / failure under critical load\" problem.  This is something you believe, not something I believe.  Let's say you're raising 1 dragon in your city, and 1 dragon is powerful enough to eat your whole city if it wants.  Then no matter how much experience you think you have with a little baby dragon, once the dragon is powerful enough to actually defeat your military and burn your city, you need the experience with the little baby passively-safe weak dragon, to generalize oneshot correctly to the dragon powerful enough to burn your city.  What if the dragon matures in a decade instead of a day?  You are still faced with the problem of correct oneshot generalization.  What if there are 100 dragons instead of 1 dragon, all with different people who think they own dragons and that the dragons are 'theirs' and will serve their interests, and they mature at slightly different rates?  You still need to have correctly generalized the safely-obtainable evidence from 'dragon groups not powerful enough to eat you while you don't yet know how to control them' to the different non-training distribution 'dragon groups that will eat you if you have already made a mistake'.  The leap of death is not something that goes away if you spread it over time or slice it up into pieces.  This ought to be common sense; there isn't some magical way of controlling 100 dragons which at no point involves the risk that the clever plan for controlling 100 dragons turns out not to work.  There is no clever plan for generalizing from safe regimes to unsafe regimes which avoids all risk that the generalization doesn't work as you hoped.  Because they are different regimes.  The dragon or collective of dragons is still big and powerful and it kills you if you made a mistake and you need to learn in regimes where mistakes don't kill you and those are not the same regimes as the regimes where a mistake kills you.  If you think I am trying to say something clever and complicated that could have a clever complicated rejoinder then you are not understanding the idea I am trying to convey.  Between the world of 100 dragons that can kill you, and a smaller group of dragons that aren't old enough to kill you, there is a gap that you are trying to cross with cleverness and generalization between two regimes that are different regimes.  This does not end well for you if you have made a mistake about how to generalize.  This problem is not about some particular kind of mistake that applies exactly to 3-year-old dragons which are growing at a rate of exactly 1 foot per day, where if the dragon grows slower than that, the problem goes away yay yay.  It is a fundamental problem not a surface one.",
      "plaintextDescription": "I deny that gradualism obviates the \"first critical try / failure under critical load\" problem.  This is something you believe, not something I believe.  Let's say you're raising 1 dragon in your city, and 1 dragon is powerful enough to eat your whole city if it wants.  Then no matter how much experience you think you have with a little baby dragon, once the dragon is powerful enough to actually defeat your military and burn your city, you need the experience with the little baby passively-safe weak dragon, to generalize oneshot correctly to the dragon powerful enough to burn your city.  What if the dragon matures in a decade instead of a day?  You are still faced with the problem of correct oneshot generalization.  What if there are 100 dragons instead of 1 dragon, all with different people who think they own dragons and that the dragons are 'theirs' and will serve their interests, and they mature at slightly different rates?  You still need to have correctly generalized the safely-obtainable evidence from 'dragon groups not powerful enough to eat you while you don't yet know how to control them' to the different non-training distribution 'dragon groups that will eat you if you have already made a mistake'.  The leap of death is not something that goes away if you spread it over time or slice it up into pieces.  This ought to be common sense; there isn't some magical way of controlling 100 dragons which at no point involves the risk that the clever plan for controlling 100 dragons turns out not to work.  There is no clever plan for generalizing from safe regimes to unsafe regimes which avoids all risk that the generalization doesn't work as you hoped.  Because they are different regimes.  The dragon or collective of dragons is still big and powerful and it kills you if you made a mistake and you need to learn in regimes where mistakes don't kill you and those are not the same regimes as the regimes where a mistake kills you.  If you think I am trying to say somethi"
    },
    "baseScore": 57,
    "voteCount": 29,
    "createdAt": null,
    "postedAt": "2025-08-13T01:18:18.180Z",
    "af": false
  },
  {
    "_id": "whRjdXdPm2dgdqhGT",
    "postId": "oDX5vcDTEei8WuoBx",
    "parentCommentId": "eyvqKQoFQx55sgxBi",
    "topLevelCommentId": "eyvqKQoFQx55sgxBi",
    "contents": {
      "markdown": "The reporter asked \"What do you think of Anthropic's recent work?\" rather than any particular paper.  My wondering how much Anthropic is uncovering roleplay vs. deep entities is a theme that runs through a lot of that recent work and the main thing where I expect I have something on the margins to contribute to what the reporter hears.",
      "plaintextDescription": "The reporter asked \"What do you think of Anthropic's recent work?\" rather than any particular paper.  My wondering how much Anthropic is uncovering roleplay vs. deep entities is a theme that runs through a lot of that recent work and the main thing where I expect I have something on the margins to contribute to what the reporter hears."
    },
    "baseScore": 17,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-10T07:26:27.623Z",
    "af": false
  },
  {
    "_id": "j7uThd2N4BSckm6pS",
    "postId": "oDX5vcDTEei8WuoBx",
    "parentCommentId": "Su4Jot8ohJKaJYEbw",
    "topLevelCommentId": "Su4Jot8ohJKaJYEbw",
    "contents": {
      "markdown": "Yep, general vibes about whether Anthropic & partner's research is quite telling us things about the underlying strange creature, or a sort of mask that it wears with a lot of roleplaying qualities.  I think this generalizes across a swathe of their research, but the Fake Alignment paper did stand out to me as one of the clearer cases.",
      "plaintextDescription": "Yep, general vibes about whether Anthropic & partner's research is quite telling us things about the underlying strange creature, or a sort of mask that it wears with a lot of roleplaying qualities.  I think this generalizes across a swathe of their research, but the Fake Alignment paper did stand out to me as one of the clearer cases."
    },
    "baseScore": 11,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-08T17:40:53.569Z",
    "af": false
  },
  {
    "_id": "jKGnQqJX8wicfpzYb",
    "postId": "oDX5vcDTEei8WuoBx",
    "parentCommentId": "NgAWandeC8NF33JRR",
    "topLevelCommentId": "NgAWandeC8NF33JRR",
    "contents": {
      "markdown": "I think there's a weird set of possibilities here and it seems plausible to me that we end up somewhere inside them; if so, I still expect the shoggoth-mask model to be an improvement for understanding it, relative to the naive-mask-believer model.  I do not expect to see zero phenomena associated with the mask being a mask.",
      "plaintextDescription": "I think there's a weird set of possibilities here and it seems plausible to me that we end up somewhere inside them; if so, I still expect the shoggoth-mask model to be an improvement for understanding it, relative to the naive-mask-believer model.  I do not expect to see zero phenomena associated with the mask being a mask."
    },
    "baseScore": 29,
    "voteCount": 18,
    "createdAt": null,
    "postedAt": "2025-08-08T17:28:47.208Z",
    "af": false
  },
  {
    "_id": "zHvd3dQKgvQfH75J9",
    "postId": "oDX5vcDTEei8WuoBx",
    "parentCommentId": "9ty25itqBFANbb7BE",
    "topLevelCommentId": "9ty25itqBFANbb7BE",
    "contents": {
      "markdown": "I also say explicitly that I acknowledge the force of the argument, \"I am not an Anthropic employee therefore this paper should not be interpreted purely as Anthropic 4D chess\" (which I don't think I'd say in any case, I think Anthropic mid-level researchers can often be justly taken at face value and to be at most personally misguided insofar as they may be mistaken at all).",
      "plaintextDescription": "I also say explicitly that I acknowledge the force of the argument, \"I am not an Anthropic employee therefore this paper should not be interpreted purely as Anthropic 4D chess\" (which I don't think I'd say in any case, I think Anthropic mid-level researchers can often be justly taken at face value and to be at most personally misguided insofar as they may be mistaken at all)."
    },
    "baseScore": 20,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-08-08T17:27:32.790Z",
    "af": false
  },
  {
    "_id": "7h6thvjjBgS2JJtzE",
    "postId": "FY697dJJv9Fq3PaTd",
    "parentCommentId": "8DtwW9djfti6jaPvq",
    "topLevelCommentId": "8DtwW9djfti6jaPvq",
    "contents": {
      "markdown": "The code of the compressor counts against your message length if you didn't pick the compressor before seeing the message.  (In standard epistemology about compression and simplicity priors.  See eg Minimum Message Length.)",
      "plaintextDescription": "The code of the compressor counts against your message length if you didn't pick the compressor before seeing the message.  (In standard epistemology about compression and simplicity priors.  See eg Minimum Message Length.)"
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-02T22:44:06.395Z",
    "af": false
  },
  {
    "_id": "vDMW4uFvrSCf7Eb3A",
    "postId": "FY697dJJv9Fq3PaTd",
    "parentCommentId": "u49MwNnnYHYvACcB3",
    "topLevelCommentId": "u49MwNnnYHYvACcB3",
    "contents": {
      "markdown": "It's the one for Mr. Croup and Mr. Vandemar from Neverwhere, for example, or Guido and Nunzio from Myth Adventures.  I think TV Tropes used to be plainer about this being the meaning of the Those Two Bad Guys trope, but now the page redirects to somewhere else?  Consider it a dead link to a trope that apparently got renamed to I-don't-know-what.",
      "plaintextDescription": "It's the one for Mr. Croup and Mr. Vandemar from Neverwhere, for example, or Guido and Nunzio from Myth Adventures.  I think TV Tropes used to be plainer about this being the meaning of the Those Two Bad Guys trope, but now the page redirects to somewhere else?  Consider it a dead link to a trope that apparently got renamed to I-don't-know-what."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-27T03:15:25.856Z",
    "af": false
  },
  {
    "_id": "4HeKqRBkDoRuTjkoF",
    "postId": "udFuYqqNdpdo5ym3f",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "[Copying from X](https://x.com/ESYudkowsky/status/1945197117720682610):\n\n> For the benefit of latecomers and CICO bros, my current equilibrium is \"spend 1 month fasting / starving on 700 cal/day keto; spend 2 months eating enough to work during the day, going to bed hungry, and therefore gaining 1-2 lb/wk\".\n> \n> I don't need a weight-loss solution, kids. Starving 1 in 3 months already works to lose weight. I need a \"have enough energy to work, without gaining 1-2lb/wk\" solution.\n> \n> Diets like the potato diet fail, not because they don't succeed in forcing me to eat less -- I do, indeed, end up with not enough room in my stomach to eat enough potatoes to work and not feel tired. The potato diet fails because it doesn't protect me from the consequences of starvation, the brainfog and the trembling hands. If I'm going to be too sick and exhausted to work, I might as well go full keto on 700cal/day and actually lose weight, rather than hanging around indefinitely in potato purgatory.\n> \n> Semaglutide failed, tirzepatide failed, paleo diet failed, potato diet failed, honey diet failed, volume eating with huge salads failed, whipped cream diet failed, aerobic exercise failed, weight lifting with a personal trainer failed, thyroid medication failed, T3 thyroid medication failed, illegal drugs like clenbuterol have failed, phentermine failed (but can help make it easier to endure a bad day when I'm in my 600cal/day phase), mitochondrial renewal diets and medications failed, Shangri-La diet worked for me twice to effortlessly lose 25lb per session and then never worked for me again.\n> \n> Next up is retatrutide + cagrilintide, and while I'm still titrating up the dose on that, it sure is not helping so far.\n> \n> I am not interested in your diet advice unless you have evidence about something that works for people who have metabolic disorders that have resisted fairly extraordinary efforts. While pretty pessimistic about retatrutide at this point, I am trying it all because a poll claimed that it had worked for 75% of people on whom tirzepatide failed.\n> \n> Your grandmother's dietary solution is not going to work, also I already tried it, also you have flatly failed at reading comprehension since you did not understand that my problem is not \"How can I possibly eat less?\" but \"How can I be protected from the usual consequences to me of eating less, well enough for me to keep working?\" And yes, I can eat less by an act of will, I eat 600cal/day for 1 in 3 months, even in the other 2 months I go to bed hungry instead of eating at nighttime, you are failing at reading comprehension if you think that this is about willpower. I just can't work at the same time as eating so little that I'm not gaining weight, which means that my hands are shaking and my brain is fogged.\n> \n> Thank you and I will be following my usual practice of blocking reply guys who fail at reading comprehension.\n\n(I answered some additional questions in replies to the tweet.)",
      "plaintextDescription": "Copying from X:\n\n> For the benefit of latecomers and CICO bros, my current equilibrium is \"spend 1 month fasting / starving on 700 cal/day keto; spend 2 months eating enough to work during the day, going to bed hungry, and therefore gaining 1-2 lb/wk\".\n> \n> I don't need a weight-loss solution, kids. Starving 1 in 3 months already works to lose weight. I need a \"have enough energy to work, without gaining 1-2lb/wk\" solution.\n> \n> Diets like the potato diet fail, not because they don't succeed in forcing me to eat less -- I do, indeed, end up with not enough room in my stomach to eat enough potatoes to work and not feel tired. The potato diet fails because it doesn't protect me from the consequences of starvation, the brainfog and the trembling hands. If I'm going to be too sick and exhausted to work, I might as well go full keto on 700cal/day and actually lose weight, rather than hanging around indefinitely in potato purgatory.\n> \n> Semaglutide failed, tirzepatide failed, paleo diet failed, potato diet failed, honey diet failed, volume eating with huge salads failed, whipped cream diet failed, aerobic exercise failed, weight lifting with a personal trainer failed, thyroid medication failed, T3 thyroid medication failed, illegal drugs like clenbuterol have failed, phentermine failed (but can help make it easier to endure a bad day when I'm in my 600cal/day phase), mitochondrial renewal diets and medications failed, Shangri-La diet worked for me twice to effortlessly lose 25lb per session and then never worked for me again.\n> \n> Next up is retatrutide + cagrilintide, and while I'm still titrating up the dose on that, it sure is not helping so far.\n> \n> I am not interested in your diet advice unless you have evidence about something that works for people who have metabolic disorders that have resisted fairly extraordinary efforts. While pretty pessimistic about retatrutide at this point, I am trying it all because a poll claimed that it had worked for 75% of people on w"
    },
    "baseScore": 36,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2025-07-18T23:26:45.625Z",
    "af": false
  },
  {
    "_id": "jxK6EHumazsFzhtot",
    "postId": "zuuQwueBpv9ZCpNuX",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I haven't read Vitalik's specific take, as yet, but as I asked more generally on X:\n\n> People who stake great hope on a \"continuous\" AI trajectory implying that defensive AI should always stay ahead of destructive AI:\n> \n> Where is the AI that I can use to talk people \\*out\\* of AI-induced psychosis?\n> \n> Why was it not \\*already\\* built, beforehand?\n\nThis just doesn't seem to be how things usually play out in real life.  Even after a first disaster, we don't get lab gain-of-function research shut down in the wake of Covid-19, let alone massive investment in fast preemptive defenses.",
      "plaintextDescription": "I haven't read Vitalik's specific take, as yet, but as I asked more generally on X:\n\n> People who stake great hope on a \"continuous\" AI trajectory implying that defensive AI should always stay ahead of destructive AI:\n> \n> Where is the AI that I can use to talk people *out* of AI-induced psychosis?\n> \n> Why was it not *already* built, beforehand?\n\nThis just doesn't seem to be how things usually play out in real life.  Even after a first disaster, we don't get lab gain-of-function research shut down in the wake of Covid-19, let alone massive investment in fast preemptive defenses."
    },
    "baseScore": 33,
    "voteCount": 21,
    "createdAt": null,
    "postedAt": "2025-07-18T23:23:03.262Z",
    "af": false
  },
  {
    "_id": "vL73YiJrHiYc3JzEm",
    "postId": "udFuYqqNdpdo5ym3f",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "[https://x.com/ESYudkowsky/status/1816925777377788295](https://x.com/ESYudkowsky/status/1816925777377788295)\n\nSomeone else is welcome to collect relevant text into a reply.  I don't really feel like it for some odd reason.",
      "plaintextDescription": "https://x.com/ESYudkowsky/status/1816925777377788295\n\nSomeone else is welcome to collect relevant text into a reply.  I don't really feel like it for some odd reason."
    },
    "baseScore": 10,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-07-09T02:20:08.598Z",
    "af": false
  },
  {
    "_id": "aYyrcQgBi69XCFFjH",
    "postId": "TTFsKxQThrqgWeXYJ",
    "parentCommentId": "sd7wm4BdDKfKZdmuH",
    "topLevelCommentId": "xYaTBEKJaLDQQL8iT",
    "contents": {
      "markdown": "Cool.  What's the actual plan and why should I expect it not to create machine Carissa Sevar?  I agree that the Textbook From The Future Containing All The Simple Tricks That Actually Work Robustly enables the construction of such an AI, but also at that point you don't need it.",
      "plaintextDescription": "Cool.  What's the actual plan and why should I expect it not to create machine Carissa Sevar?  I agree that the Textbook From The Future Containing All The Simple Tricks That Actually Work Robustly enables the construction of such an AI, but also at that point you don't need it."
    },
    "baseScore": 24,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-02-20T07:29:53.726Z",
    "af": false
  },
  {
    "_id": "WpfHQ7WCPvKtDZbG6",
    "postId": "TTFsKxQThrqgWeXYJ",
    "parentCommentId": "sjzrocP2E8Bgx5Yme",
    "topLevelCommentId": "xYaTBEKJaLDQQL8iT",
    "contents": {
      "markdown": "So if it's difficult to get amazing trustworthy work out of a machine actress playing an Eliezer-level intelligence doing a thousand years worth of thinking, your proposal to have AIs do our AI alignment homework fails on the first step, it sounds like?",
      "plaintextDescription": "So if it's difficult to get amazing trustworthy work out of a machine actress playing an Eliezer-level intelligence doing a thousand years worth of thinking, your proposal to have AIs do our AI alignment homework fails on the first step, it sounds like?"
    },
    "baseScore": 12,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-02-20T07:24:25.804Z",
    "af": true
  },
  {
    "_id": "DmwDgi2uQf6wAFRNw",
    "postId": "TTFsKxQThrqgWeXYJ",
    "parentCommentId": "PNmwoseyiDKdoaMJD",
    "topLevelCommentId": "xYaTBEKJaLDQQL8iT",
    "contents": {
      "markdown": "So the \"IQ 60 people controlling IQ 80 people controlling IQ 100 people controlling IQ 120 people controlling IQ 140 people until they're genuinely in charge and genuinely getting honest reports and genuinely getting great results in their control of a government\" theory of alignment?",
      "plaintextDescription": "So the \"IQ 60 people controlling IQ 80 people controlling IQ 100 people controlling IQ 120 people controlling IQ 140 people until they're genuinely in charge and genuinely getting honest reports and genuinely getting great results in their control of a government\" theory of alignment?"
    },
    "baseScore": 22,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2025-02-20T03:03:34.816Z",
    "af": true
  },
  {
    "_id": "LrGMuJKZztjhp6yMA",
    "postId": "TTFsKxQThrqgWeXYJ",
    "parentCommentId": "tYJXZDMXJpDGXmhQw",
    "topLevelCommentId": "xYaTBEKJaLDQQL8iT",
    "contents": {
      "markdown": "I don't think you can train an actress to simulate me, successfully, without her going dangerous.  I think that's over the threshold for where a mind starts reflecting on itself and pulling itself together.",
      "plaintextDescription": "I don't think you can train an actress to simulate me, successfully, without her going dangerous.  I think that's over the threshold for where a mind starts reflecting on itself and pulling itself together."
    },
    "baseScore": 31,
    "voteCount": 21,
    "createdAt": null,
    "postedAt": "2025-02-20T00:48:46.403Z",
    "af": true
  },
  {
    "_id": "6tbCLyALZoE58qidy",
    "postId": "TTFsKxQThrqgWeXYJ",
    "parentCommentId": "5pTnRwmeZJqknt5La",
    "topLevelCommentId": "xYaTBEKJaLDQQL8iT",
    "contents": {
      "markdown": "I'm not saying that it's against thermodynamics to get behaviors you don't know how to verify.  I'm asking what's the plan for getting them.",
      "plaintextDescription": "I'm not saying that it's against thermodynamics to get behaviors you don't know how to verify.  I'm asking what's the plan for getting them."
    },
    "baseScore": 21,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-02-20T00:46:48.319Z",
    "af": false
  },
  {
    "_id": "fxnhSv3n4aRjPQDwQ",
    "postId": "DfrSZaf3JC8vJdbZL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "One of the most important projects in the world.  Somebody should fund it.",
      "plaintextDescription": "One of the most important projects in the world.  Somebody should fund it."
    },
    "baseScore": 144,
    "voteCount": 153,
    "createdAt": null,
    "postedAt": "2025-02-20T00:45:46.116Z",
    "af": false
  },
  {
    "_id": "xYaTBEKJaLDQQL8iT",
    "postId": "TTFsKxQThrqgWeXYJ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Can you tl;dr how you go from \"humans cannot tell which alignment arguments are good or bad\" to \"we justifiably trust the AI to report honest good alignment takes\"?  Like, not with a very large diagram full of complicated parts such that it's hard to spot where you've messed up.  Just whatever simple principle you think lets you bypass GIGO.\n\nEg, suppose that in 2020 the Open Philanthropy Foundation would like to train an AI such that the AI would honestly say if the OpenPhil doctrine of \"AGI in 2050\" was based on groundless thinking ultimately driven by social conformity.  However, OpenPhil is not allowed to train their AI based on MIRI.  They have to train their AI entirely on OpenPhil-produced content.  How does OpenPhil bootstrap an AI which will say, \"Guys, you have no idea when AI shows up but it's probably not that far and you sure can't rely on it\"?  Assume that whenever OpenPhil tries to run an essay contest for saying what they're getting wrong, their panel of judges ends up awarding the prize to somebody reassuringly saying that AI risk is an even smaller deal than OpenPhil thinks.  How does OpenPhil bootstrap from that pattern of thumbs-up/thumbs-down to an AI that actually has better-than-OpenPhil alignment takes?\n\nBroadly speaking, the standard ML paradigm lets you bootstrap somewhat from \"I can verify whether this problem was solved\" to \"I can train a generator to solve this problem\".  This applies as much to MIRI as OpenPhil.  MIRI would also need some nontrivial secret amazing clever trick to gradient-descend an AI that gave us great alignment takes, instead of seeking out the flaws in our own verifier and exploiting those.\n\nWhat's the trick?  My basic guess, when I see some very long complicated paper that doesn't explain the key problem and key solution up front, is that you've done the equivalent of an inventor building a sufficiently complicated perpetual motion machine that their mental model of it no longer tracks how conservation laws apply.  (As opposed to the simpler error of their explicitly believing that one particular step or motion locally violates a conservation law.)  But if you've got a directly explainable trick for how you get great suggestions you can't verify, go for it.",
      "plaintextDescription": "Can you tl;dr how you go from \"humans cannot tell which alignment arguments are good or bad\" to \"we justifiably trust the AI to report honest good alignment takes\"?  Like, not with a very large diagram full of complicated parts such that it's hard to spot where you've messed up.  Just whatever simple principle you think lets you bypass GIGO.\n\nEg, suppose that in 2020 the Open Philanthropy Foundation would like to train an AI such that the AI would honestly say if the OpenPhil doctrine of \"AGI in 2050\" was based on groundless thinking ultimately driven by social conformity.  However, OpenPhil is not allowed to train their AI based on MIRI.  They have to train their AI entirely on OpenPhil-produced content.  How does OpenPhil bootstrap an AI which will say, \"Guys, you have no idea when AI shows up but it's probably not that far and you sure can't rely on it\"?  Assume that whenever OpenPhil tries to run an essay contest for saying what they're getting wrong, their panel of judges ends up awarding the prize to somebody reassuringly saying that AI risk is an even smaller deal than OpenPhil thinks.  How does OpenPhil bootstrap from that pattern of thumbs-up/thumbs-down to an AI that actually has better-than-OpenPhil alignment takes?\n\nBroadly speaking, the standard ML paradigm lets you bootstrap somewhat from \"I can verify whether this problem was solved\" to \"I can train a generator to solve this problem\".  This applies as much to MIRI as OpenPhil.  MIRI would also need some nontrivial secret amazing clever trick to gradient-descend an AI that gave us great alignment takes, instead of seeking out the flaws in our own verifier and exploiting those.\n\nWhat's the trick?  My basic guess, when I see some very long complicated paper that doesn't explain the key problem and key solution up front, is that you've done the equivalent of an inventor building a sufficiently complicated perpetual motion machine that their mental model of it no longer tracks how conservation laws apply. "
    },
    "baseScore": 98,
    "voteCount": 50,
    "createdAt": null,
    "postedAt": "2025-02-19T21:57:29.778Z",
    "af": true
  },
  {
    "_id": "uuvZEHkdrMggPywuQ",
    "postId": "oM9pEezyCb4dCsuKq",
    "parentCommentId": "kBwB2L9jL2qLzFSLw",
    "topLevelCommentId": "kBwB2L9jL2qLzFSLw",
    "contents": {
      "markdown": "You seem confused about my exact past position.  I was arguing against EAs who were like, \"We'll solve AGI with policy, therefore no doom.\"  I am not presently a great optimist about the likelihood of policy being an easy solution.  There is just nothing else left.",
      "plaintextDescription": "You seem confused about my exact past position.  I was arguing against EAs who were like, \"We'll solve AGI with policy, therefore no doom.\"  I am not presently a great optimist about the likelihood of policy being an easy solution.  There is just nothing else left."
    },
    "baseScore": 25,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-01-21T22:52:38.822Z",
    "af": false
  },
  {
    "_id": "89w5ypYLS4q8wJifr",
    "postId": "nH4c3Q9t9F3nJ7y8W",
    "parentCommentId": "aTcXQhxEtJk8N6Nb6",
    "topLevelCommentId": "otxwXr3noFRGCSMJY",
    "contents": {
      "markdown": "(I affirm this as my intended reading.)",
      "plaintextDescription": "(I affirm this as my intended reading.)"
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-12-09T22:07:31.265Z",
    "af": false
  },
  {
    "_id": "wtExPJXgfo2QSq3gk",
    "postId": "4ARaTpNX62uaL86j6",
    "parentCommentId": "v6g4Xwf9wZWrdjaQg",
    "topLevelCommentId": "v6g4Xwf9wZWrdjaQg",
    "contents": {
      "markdown": "It certainly bears upon AI, but it bears that way by making a point about the complexity of a task rather than talking about an intelligent mechanism which is purportedly aligned on that task.  It does this by talking about an unintelligent mechanism, which is meant to be a way of talking about the task itself rather than any particular machine for doing it.",
      "plaintextDescription": "It certainly bears upon AI, but it bears that way by making a point about the complexity of a task rather than talking about an intelligent mechanism which is purportedly aligned on that task.  It does this by talking about an unintelligent mechanism, which is meant to be a way of talking about the task itself rather than any particular machine for doing it."
    },
    "baseScore": 6,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-12-09T22:06:03.442Z",
    "af": false
  },
  {
    "_id": "d4yuRXGDue4vbaH3q",
    "postId": "axLLEqREY5SrQ9GYW",
    "parentCommentId": "XxqQXcxSBJiXvx6jT",
    "topLevelCommentId": "zSaXkns7M3zXEtJ8D",
    "contents": {
      "markdown": "Zac wins.",
      "plaintextDescription": "Zac wins."
    },
    "baseScore": 71,
    "voteCount": 45,
    "createdAt": null,
    "postedAt": "2024-12-03T15:25:36.243Z",
    "af": false
  },
  {
    "_id": "DdtJwB9fbjwnPhEEv",
    "postId": "ZoFxTqWRBkyanonyb",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Well-checked.",
      "plaintextDescription": "Well-checked."
    },
    "baseScore": 25,
    "voteCount": 21,
    "createdAt": null,
    "postedAt": "2024-11-05T19:24:33.370Z",
    "af": false
  },
  {
    "_id": "gqZEcxiGesbvhcNLa",
    "postId": "4ARaTpNX62uaL86j6",
    "parentCommentId": "cxCYQj8fpMZr9J76e",
    "topLevelCommentId": "AuaNR7rJ5WdCfyKXg",
    "contents": {
      "markdown": "Your distinction between \"outer alignment\" and \"inner alignment\" is both ahistorical and unYudkowskian.  It was invented years after this post was written, by someone who wasn't me; and though I've sometimes used the terms in occasions where they seem to fit unambiguously, it's not something I see as a clear ontological division, especially if you're talking about questions like \"If we own the following kind of blackbox, would alignment get any easier?\" which on my view breaks that ontology.  So I strongly reject your frame that this post was \"clearly portraying an outer alignment problem\" and can be criticized on those grounds by you; that is anachronistic.\n\nYou are now dragging in a very large number of further inferences about \"what I meant\", and other implications that you think this post has, which are about Christiano-style proposals that were developed years after this post.  I have disagreements with those, many disagreements.  But it is definitely not what this post is about, one way or another, because this post predates Christiano being on the scene.\n\nWhat this post is trying to illustrate is that if you try putting crisp physical predicates on reality, that won't work to say what you want.  This point is true!  If you then want to take in a bunch of anachronistic ideas developed later, and claim (wrongly imo) that this *renders irrelevant* the *simple truth* of *what this post actually literally says,* that would be a separate conversation.  But if you're doing that, please distinguish the truth of *what this post actually says* versus how you think these other later clever ideas *evade or bypass* that truth.",
      "plaintextDescription": "Your distinction between \"outer alignment\" and \"inner alignment\" is both ahistorical and unYudkowskian.  It was invented years after this post was written, by someone who wasn't me; and though I've sometimes used the terms in occasions where they seem to fit unambiguously, it's not something I see as a clear ontological division, especially if you're talking about questions like \"If we own the following kind of blackbox, would alignment get any easier?\" which on my view breaks that ontology.  So I strongly reject your frame that this post was \"clearly portraying an outer alignment problem\" and can be criticized on those grounds by you; that is anachronistic.\n\nYou are now dragging in a very large number of further inferences about \"what I meant\", and other implications that you think this post has, which are about Christiano-style proposals that were developed years after this post.  I have disagreements with those, many disagreements.  But it is definitely not what this post is about, one way or another, because this post predates Christiano being on the scene.\n\nWhat this post is trying to illustrate is that if you try putting crisp physical predicates on reality, that won't work to say what you want.  This point is true!  If you then want to take in a bunch of anachronistic ideas developed later, and claim (wrongly imo) that this renders irrelevant the simple truth of what this post actually literally says, that would be a separate conversation.  But if you're doing that, please distinguish the truth of what this post actually says versus how you think these other later clever ideas evade or bypass that truth."
    },
    "baseScore": 28,
    "voteCount": 22,
    "createdAt": null,
    "postedAt": "2024-10-19T18:14:33.514Z",
    "af": false
  },
  {
    "_id": "kqwgEGiA5MywtcsXF",
    "postId": "4ARaTpNX62uaL86j6",
    "parentCommentId": "AuaNR7rJ5WdCfyKXg",
    "topLevelCommentId": "AuaNR7rJ5WdCfyKXg",
    "contents": {
      "markdown": "The post is about the complexity of what needs to be gotten inside the AI.  If you had a perfect blackbox that exactly evaluated the thing-that-needs-to-be-inside-the-AI, this could possibly simplify some particular approaches to alignment, that would still in fact be too hard because nobody has a way of getting an AI to point at anything.  But it would not change the complexity of what needs to be moved inside the AI, which is the narrow point that this post is about; and if you think that some larger thing is not correct, you should not confuse that with saying that the narrow point this post is about, is incorrect.\n\n> I claim that having such a function would simplify the AI alignment problem by ***reducing it from the hard problem of getting an AI to care about something complex*** (human value) to the easier problem of ***getting the AI to care about that particular function*** (which is simple, as the function can be hooked up to the AI directly).\n\nOne cannot hook up a function to an AI directly; it has to be physically instantiated somehow.  For example, the function could be a human pressing a button; and then, any experimentation on the AI's part to determine what \"really\" controls the button, will find that administering drugs to the human, or building a robot to seize control of the reward button, is \"really\" (from the AI's perspective) the true meaning of the reward button after all!  Perhaps you do not have this exact scenario in mind.  So would you care to spell out what clever methodology you think invalidates what you take to be the larger point of this post -- though of course it has no bearing on the actual point that this post makes?",
      "plaintextDescription": "The post is about the complexity of what needs to be gotten inside the AI.  If you had a perfect blackbox that exactly evaluated the thing-that-needs-to-be-inside-the-AI, this could possibly simplify some particular approaches to alignment, that would still in fact be too hard because nobody has a way of getting an AI to point at anything.  But it would not change the complexity of what needs to be moved inside the AI, which is the narrow point that this post is about; and if you think that some larger thing is not correct, you should not confuse that with saying that the narrow point this post is about, is incorrect.\n\n> I claim that having such a function would simplify the AI alignment problem by reducing it from the hard problem of getting an AI to care about something complex (human value) to the easier problem of getting the AI to care about that particular function (which is simple, as the function can be hooked up to the AI directly).\n\nOne cannot hook up a function to an AI directly; it has to be physically instantiated somehow.  For example, the function could be a human pressing a button; and then, any experimentation on the AI's part to determine what \"really\" controls the button, will find that administering drugs to the human, or building a robot to seize control of the reward button, is \"really\" (from the AI's perspective) the true meaning of the reward button after all!  Perhaps you do not have this exact scenario in mind.  So would you care to spell out what clever methodology you think invalidates what you take to be the larger point of this post -- though of course it has no bearing on the actual point that this post makes?"
    },
    "baseScore": 26,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2024-10-18T17:08:35.687Z",
    "af": false
  },
  {
    "_id": "dPHeQk7GtJkyz7akM",
    "postId": "4ARaTpNX62uaL86j6",
    "parentCommentId": "964kcmpoJJeiTZqmp",
    "topLevelCommentId": "AuaNR7rJ5WdCfyKXg",
    "contents": {
      "markdown": "Wish there was a system where people could pay money to bid up what they believed were the \"top arguments\" that they wanted me to respond to.  Possibly a system where I collect the money for writing a diligent response (albeit note that in this case I'd weigh the time-cost of responding as well as the bid for a response); but even aside from that, some way of canonizing what \"people who care enough to spend money on that\" think are the Super Best Arguments That I Should Definitely Respond To.  As it stands, whatever I respond to, there's somebody else to say that it wasn't the real argument, and this mainly incentivizes me to sigh and go on responding to whatever I happen to care about more.\n\n(I also wish this system had been in place 24 years ago so you could scroll back and check out the wacky shit that used to be on that system earlier, but too late now.)",
      "plaintextDescription": "Wish there was a system where people could pay money to bid up what they believed were the \"top arguments\" that they wanted me to respond to.  Possibly a system where I collect the money for writing a diligent response (albeit note that in this case I'd weigh the time-cost of responding as well as the bid for a response); but even aside from that, some way of canonizing what \"people who care enough to spend money on that\" think are the Super Best Arguments That I Should Definitely Respond To.  As it stands, whatever I respond to, there's somebody else to say that it wasn't the real argument, and this mainly incentivizes me to sigh and go on responding to whatever I happen to care about more.\n\n(I also wish this system had been in place 24 years ago so you could scroll back and check out the wacky shit that used to be on that system earlier, but too late now.)"
    },
    "baseScore": 44,
    "voteCount": 22,
    "createdAt": null,
    "postedAt": "2024-10-18T17:02:20.943Z",
    "af": false
  },
  {
    "_id": "oZtsFc5oCMA9Zk5Tg",
    "postId": "qc7P2NwfxQMC3hdgm",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I note that I haven't said out loud, and should say out loud, that I endorse this history.  Not every single line of it (see my other comment on why I reject verificationism) but on the whole, this is well-informed and well-applied.",
      "plaintextDescription": "I note that I haven't said out loud, and should say out loud, that I endorse this history.  Not every single line of it (see my other comment on why I reject verificationism) but on the whole, this is well-informed and well-applied."
    },
    "baseScore": 49,
    "voteCount": 26,
    "createdAt": null,
    "postedAt": "2024-08-26T15:56:32.900Z",
    "af": false
  },
  {
    "_id": "cGCaQactztiBrxGjh",
    "postId": "fjfWrKhEawwBGCTGs",
    "parentCommentId": "v2d94vmiKYdujYzji",
    "topLevelCommentId": "fTThxDyxEiNsaTwaP",
    "contents": {
      "markdown": "> If you had to put a rough number on how likely it is that a misaligned superintelligence would primarily value \"small molecular squiggles\" versus other types of misaligned goals, would it be more like 1000:1 or 1:1 or 1000:1 or something else? \n\nValue them *primarily?*  Uhhh... maybe 1:3 against?  I admit I have never actually pondered this question before today; but 1 in 4 uncontrolled superintelligences spending most of their resources on tiny squiggles doesn't sound off by, like, more than 1-2 orders of magnitude in either direction.\n\n> Clocks are not actually very complicated; how plausible is it on your model that these goals are as complicated as, say, a typical human's preferences about how human civilization is structured?\n\nIt wouldn't shock me if their goals end up far more complicated than human ones; the most obvious pathway for it is (a) gradient descent turning out to produce internal preferences much faster than natural selection + biological reinforcement learning and (b) some significant fraction of those preferences being retained under reflection.  (Where (b) strikes me as way less probable than (a), but not wholly forbidden.)  The second most obvious pathway is if a bunch of weird detailed noise appears in the first version of the reflective process and then freezes.",
      "plaintextDescription": "> If you had to put a rough number on how likely it is that a misaligned superintelligence would primarily value \"small molecular squiggles\" versus other types of misaligned goals, would it be more like 1000:1 or 1:1 or 1000:1 or something else? \n\nValue them primarily?  Uhhh... maybe 1:3 against?  I admit I have never actually pondered this question before today; but 1 in 4 uncontrolled superintelligences spending most of their resources on tiny squiggles doesn't sound off by, like, more than 1-2 orders of magnitude in either direction.\n\n> Clocks are not actually very complicated; how plausible is it on your model that these goals are as complicated as, say, a typical human's preferences about how human civilization is structured?\n\nIt wouldn't shock me if their goals end up far more complicated than human ones; the most obvious pathway for it is (a) gradient descent turning out to produce internal preferences much faster than natural selection + biological reinforcement learning and (b) some significant fraction of those preferences being retained under reflection.  (Where (b) strikes me as way less probable than (a), but not wholly forbidden.)  The second most obvious pathway is if a bunch of weird detailed noise appears in the first version of the reflective process and then freezes."
    },
    "baseScore": 21,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2024-08-09T18:14:34.806Z",
    "af": true
  },
  {
    "_id": "WapHz3gokGBd3KHKm",
    "postId": "hzt9gHpNwA2oHtwKX",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Not obviously stupid on a very quick skim.  I will have to actually read it to figure out where it's stupid.\n\n(I rarely give any review this positive on a first skim.  Congrats.)",
      "plaintextDescription": "Not obviously stupid on a very quick skim.  I will have to actually read it to figure out where it's stupid.\n\n(I rarely give any review this positive on a first skim.  Congrats.)"
    },
    "baseScore": 43,
    "voteCount": 57,
    "createdAt": null,
    "postedAt": "2024-08-09T02:19:13.583Z",
    "af": true
  },
  {
    "_id": "XGTupMJe2ccyKhgrQ",
    "postId": "rP66bz34crvDudzcJ",
    "parentCommentId": "sYybWGmhQNhb9QeMR",
    "topLevelCommentId": "ABCGngfphSgXQkCeA",
    "contents": {
      "markdown": "By \"dumb player\" I did not mean *as dumb as a human* player.  I meant \"too dumb to compute the pseudorandom numbers, but not too dumb to simulate other players faithfully apart from that\".  I did not realize we were talking about humans at all.  This jumps out more to me as a potential source of misunderstanding than it did 15 years ago, and for that I apologize.",
      "plaintextDescription": "By \"dumb player\" I did not mean as dumb as a human player.  I meant \"too dumb to compute the pseudorandom numbers, but not too dumb to simulate other players faithfully apart from that\".  I did not realize we were talking about humans at all.  This jumps out more to me as a potential source of misunderstanding than it did 15 years ago, and for that I apologize."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-07-29T19:13:47.897Z",
    "af": true
  },
  {
    "_id": "vkCEJ5ivLWsnNnpbE",
    "postId": "rP66bz34crvDudzcJ",
    "parentCommentId": "ABCGngfphSgXQkCeA",
    "topLevelCommentId": "ABCGngfphSgXQkCeA",
    "contents": {
      "markdown": "I don't always remember my previous positions all that well, but I doubt I would have said at any point that sufficiently advanced LDT agents are friendly to each other, rather than that they coordinate well with each other (and not so with us)?",
      "plaintextDescription": "I don't always remember my previous positions all that well, but I doubt I would have said at any point that sufficiently advanced LDT agents are friendly to each other, rather than that they coordinate well with each other (and not so with us)?"
    },
    "baseScore": 12,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-07-28T21:30:21.852Z",
    "af": true
  },
  {
    "_id": "gAGnuYfDdwbAm9tSv",
    "postId": "fjfWrKhEawwBGCTGs",
    "parentCommentId": "fTThxDyxEiNsaTwaP",
    "topLevelCommentId": "fTThxDyxEiNsaTwaP",
    "contents": {
      "markdown": "Actually, to slightly amend that:  The part where squiggles are small is a *more than randomly likely* part of the prediction, but not a *load-bearing part of downstream predictions or the policy argument.*  Most of the time we don't needlessly build our own paperclips to be the size of skyscrapers; even when having fun, we try to do the fun without vastly more resources, than are necessary to that amount of fun, because then we'll have needlessly used up all our resources and not get to have more fun.  We buy cookies that cost a dollar instead of a hundred thousand dollars.  A very wide variety of utility functions you could run over the outside universe will have optima around making lots of small things because each thing scores one point, and so to score as many points as possible, each thing is as small as it can be as still count as a thing.  Nothing downstream depends on this part coming true and there are many ways for it to come false; but the part where the squiggles are small and molecular *is* an obvious kind of guess.  \"Great giant squiggles of nickel the size of a solar system would be no more valuable, even from a very embracing and cosmopolitan perspective on value\" is the loadbearing part.",
      "plaintextDescription": "Actually, to slightly amend that:  The part where squiggles are small is a more than randomly likely part of the prediction, but not a load-bearing part of downstream predictions or the policy argument.  Most of the time we don't needlessly build our own paperclips to be the size of skyscrapers; even when having fun, we try to do the fun without vastly more resources, than are necessary to that amount of fun, because then we'll have needlessly used up all our resources and not get to have more fun.  We buy cookies that cost a dollar instead of a hundred thousand dollars.  A very wide variety of utility functions you could run over the outside universe will have optima around making lots of small things because each thing scores one point, and so to score as many points as possible, each thing is as small as it can be as still count as a thing.  Nothing downstream depends on this part coming true and there are many ways for it to come false; but the part where the squiggles are small and molecular is an obvious kind of guess.  \"Great giant squiggles of nickel the size of a solar system would be no more valuable, even from a very embracing and cosmopolitan perspective on value\" is the loadbearing part."
    },
    "baseScore": 27,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2024-07-16T19:59:20.949Z",
    "af": true
  },
  {
    "_id": "fTThxDyxEiNsaTwaP",
    "postId": "fjfWrKhEawwBGCTGs",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "The part where squiggles are small and simple is unimportant.  They could be bigger and more complicated, like building giant mechanical clocks.  The part that matters is that squiggles/paperclips are of no value even from a very cosmopolitan and embracing perspective on value.",
      "plaintextDescription": "The part where squiggles are small and simple is unimportant. They could be bigger and more complicated, like building giant mechanical clocks. The part that matters is that squiggles/paperclips are of no value even from a very cosmopolitan and embracing perspective on value."
    },
    "baseScore": 27,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2024-07-16T14:11:40.041Z",
    "af": true
  },
  {
    "_id": "CixonSXNfLgAPh48Z",
    "postId": "q8uNoJBgcpAe3bSBp",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I think that the AI's internal ontology is liable to have some noticeable alignments to human ontology w/r/t the purely predictive aspects of the natural world; it wouldn't surprise me to find distinct thoughts in there about electrons.  As the internal ontology goes to be more about affordances and actions, I expect to find increasing disalignment.  As the internal ontology takes on any *reflective* aspects, parts of the representation that mix with facts about the AI's internals, I expect to find much larger differences -- not just that the AI has a different concept boundary around \"easy to understand\", say, but that it maybe doesn't have any such internal notion as \"easy to understand\" at all, because easiness isn't in the environment and the AI doesn't have any such thing as \"effort\".  Maybe it's got categories around yieldingness to seven different categories of methods, and/or some general notion of \"can predict at all / can't predict at all\", but no general notion that maps onto human \"easy to understand\" -- though \"easy to understand\" is plausibly general-enough that I wouldn't be unsurprised to find a mapping after all.\n\nCorrigibility and actual human values are both heavily reflective concepts.  If you master a requisite level of the prerequisite skill of noticing when a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment -- which of course most people can't do because they project the category boundary onto the environment, but I have some credit that John Wentworth might be able to do it some -- and then you start mapping out concept definitions about corrigibility or values or god help you CEV, that might help highlight where some of my concern about unnatural abstractions comes in.\n\n*Entirely separately,* I have concerns about the ability of ML-based technology to robustly point the AI in *any* builder-intended direction whatsoever, *even if* there exists some not-too-large adequate mapping from that intended direction onto the AI's internal ontology at training time.  My guess is that more of the disagreement lies here.",
      "plaintextDescription": "I think that the AI's internal ontology is liable to have some noticeable alignments to human ontology w/r/t the purely predictive aspects of the natural world; it wouldn't surprise me to find distinct thoughts in there about electrons.  As the internal ontology goes to be more about affordances and actions, I expect to find increasing disalignment.  As the internal ontology takes on any reflective aspects, parts of the representation that mix with facts about the AI's internals, I expect to find much larger differences -- not just that the AI has a different concept boundary around \"easy to understand\", say, but that it maybe doesn't have any such internal notion as \"easy to understand\" at all, because easiness isn't in the environment and the AI doesn't have any such thing as \"effort\".  Maybe it's got categories around yieldingness to seven different categories of methods, and/or some general notion of \"can predict at all / can't predict at all\", but no general notion that maps onto human \"easy to understand\" -- though \"easy to understand\" is plausibly general-enough that I wouldn't be unsurprised to find a mapping after all.\n\nCorrigibility and actual human values are both heavily reflective concepts.  If you master a requisite level of the prerequisite skill of noticing when a concept definition has a step where its boundary depends on your own internals rather than pure facts about the environment -- which of course most people can't do because they project the category boundary onto the environment, but I have some credit that John Wentworth might be able to do it some -- and then you start mapping out concept definitions about corrigibility or values or god help you CEV, that might help highlight where some of my concern about unnatural abstractions comes in.\n\n \n\nEntirely separately, I have concerns about the ability of ML-based technology to robustly point the AI in any builder-intended direction whatsoever, even if there exists some not-too-large adequate ma"
    },
    "baseScore": 149,
    "voteCount": 77,
    "createdAt": null,
    "postedAt": "2024-06-11T18:07:24.402Z",
    "af": false
  },
  {
    "_id": "ww5p8EEtKndRnWazN",
    "postId": "nH4c3Q9t9F3nJ7y8W",
    "parentCommentId": "rdXw2qwE9uyNRJFBK",
    "topLevelCommentId": "St8zjZ2xatojvnFxW",
    "contents": {
      "markdown": "What the main post is responding to is the argument:  \"We're just training AIs to imitate human text, right, so that process can't make them get any smarter than the text they're imitating, right?  So AIs shouldn't learn abilities that humans don't have; because why would you need those abilities to learn to imitate humans?\"  And to this the main post says, \"Nope.\"\n\nThe main post is *not* arguing:  \"If you abstract away the tasks humans evolved to solve, from human levels of performance at those tasks, the tasks AIs are being trained to solve are harder than those tasks in principle even if they were being solved perfectly.\"  I agree this is just false, and did not think my post said otherwise.",
      "plaintextDescription": "What the main post is responding to is the argument:  \"We're just training AIs to imitate human text, right, so that process can't make them get any smarter than the text they're imitating, right?  So AIs shouldn't learn abilities that humans don't have; because why would you need those abilities to learn to imitate humans?\"  And to this the main post says, \"Nope.\"\n\nThe main post is not arguing:  \"If you abstract away the tasks humans evolved to solve, from human levels of performance at those tasks, the tasks AIs are being trained to solve are harder than those tasks in principle even if they were being solved perfectly.\"  I agree this is just false, and did not think my post said otherwise."
    },
    "baseScore": 23,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2024-04-21T01:37:40.889Z",
    "af": true
  },
  {
    "_id": "rSzwbLHLSdj5dEnSY",
    "postId": "LvKDMWQ3yLG9R3gHw",
    "parentCommentId": "T3oPTu7ZEwmxt88Jb",
    "topLevelCommentId": "dRax6PHpQhxmNDnaN",
    "contents": {
      "markdown": "Unless I'm greatly misremembering, you did pick out what you said was your strongest item from Lethalities, separately from this, and I responded to it.  You'd just straightforwardly misunderstood my argument in that case, so it wasn't a long response, but I responded.  Asking for a second try is one thing, but I don't think it's cool to act like you never picked out any one item or I never responded to it.\n\nEDIT: I'm misremembering, it was Quintin's strongest point about the Bankless podcast.  https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=cr54ivfjndn6dxraD",
      "plaintextDescription": "Unless I'm greatly misremembering, you did pick out what you said was your strongest item from Lethalities, separately from this, and I responded to it.  You'd just straightforwardly misunderstood my argument in that case, so it wasn't a long response, but I responded.  Asking for a second try is one thing, but I don't think it's cool to act like you never picked out any one item or I never responded to it.\n\nEDIT: I'm misremembering, it was Quintin's strongest point about the Bankless podcast.  https://www.lesswrong.com/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky?commentId=cr54ivfjndn6dxraD"
    },
    "baseScore": 13,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2024-03-20T00:13:11.196Z",
    "af": false
  },
  {
    "_id": "tbNrEfpGSzwKK8m8r",
    "postId": "LvKDMWQ3yLG9R3gHw",
    "parentCommentId": "dRax6PHpQhxmNDnaN",
    "topLevelCommentId": "dRax6PHpQhxmNDnaN",
    "contents": {
      "markdown": "If Quintin hasn't yelled \"Empiricism!\" then it's not about him.  This is more about (some) e/accs.",
      "plaintextDescription": "If Quintin hasn't yelled \"Empiricism!\" then it's not about him.  This is more about (some) e/accs."
    },
    "baseScore": 13,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2024-03-20T00:08:05.402Z",
    "af": false
  },
  {
    "_id": "6v33CCJJZj2ZkC87h",
    "postId": "h99tRkpQGxwtb9Dpv",
    "parentCommentId": "SaJ5HYDKEeEh4smBx",
    "topLevelCommentId": "SaJ5HYDKEeEh4smBx",
    "contents": {
      "markdown": "Wow, that's fucked up.",
      "plaintextDescription": "Wow, that's fucked up."
    },
    "baseScore": 11,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2024-03-11T00:58:41.351Z",
    "af": false
  },
  {
    "_id": "PFgFEu6zJbLPfAzqu",
    "postId": "brXr7PJ2W4Na2EW2q",
    "parentCommentId": "KN2jATsAuYb7e7wkC",
    "topLevelCommentId": "tYBPjetgZW4iMqe4s",
    "contents": {
      "markdown": "I am denying that superintelligences play this game in a way that looks like \"Pick an ordinal to be your level of sophistication, and whoever picks the higher ordinal gets $9.\"  I expect sufficiently smart agents to play this game in a way that doesn't incentivize attempts by the opponent to be more sophisticated than you, nor will you find yourself incentivized to try to exploit an opponent by being more sophisticated than them, provided that both parties have the minimum level of sophistication to be that smart.\n\nIf faced with an opponent stupid enough to play the ordinal game, of course, you just refuse all offers less than $9, and they find that there's no ordinal level of sophistication they can pick which makes you behave otherwise.  Sucks to be them!",
      "plaintextDescription": "I am denying that superintelligences play this game in a way that looks like \"Pick an ordinal to be your level of sophistication, and whoever picks the higher ordinal gets $9.\"  I expect sufficiently smart agents to play this game in a way that doesn't incentivize attempts by the opponent to be more sophisticated than you, nor will you find yourself incentivized to try to exploit an opponent by being more sophisticated than them, provided that both parties have the minimum level of sophistication to be that smart.\n\nIf faced with an opponent stupid enough to play the ordinal game, of course, you just refuse all offers less than $9, and they find that there's no ordinal level of sophistication they can pick which makes you behave otherwise.  Sucks to be them!"
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2024-02-15T18:29:51.435Z",
    "af": false
  },
  {
    "_id": "8ekKNCnEwnCWrKG6E",
    "postId": "4ARaTpNX62uaL86j6",
    "parentCommentId": "TbHKNp3gnkzxYyEe7",
    "topLevelCommentId": "TbHKNp3gnkzxYyEe7",
    "contents": {
      "markdown": "You have misunderstood (1) the point this post was trying to communicate and (2) the structure of the larger argument where that point appears, as follows:\n\nFirst, let's talk about (2), the larger argument that this post's point was supposed to be relevant to.\n\nIs the larger argument that superintelligences will misunderstand what we really meant, due to a lack of knowledge about humans?\n\nIt is incredibly unlikely that Eliezer Yudkowsky in particular would have constructed an argument like this, whether in 2007, 2017, or even 1997.  At all of these points in my life, I visibly held quite a lot of respect for the epistemic prowess of superintelligences.  They were always going to *know* everything relevant about the complexities of human preference and desire.  The larger argument is about whether it's easy to make superintelligences end up *caring*.\n\nThis post isn't about the distinction between knowing and caring, to be clear; that's something I tried to cover elsewhere.  The relevant central divide falls in roughly the same conceptual place as Hume's Guillotine between 'is' and 'ought', or the difference between the belief function and the utility function.\n\n(I don't see myself as having managed to reliably communicate this concept (though the central idea is old indeed within philosophy) to the field that now sometimes calls itself \"AI alignment\"; so if you understand this distinction yourself, you should not assume that any particulary commentary within \"AI alignment\" is written from a place of understanding it too.)\n\nWhat this post is about is the amount of information-theoretic complexity that you need to get into *the system's preferences,* in order to have that system, given unlimited or rather extremely large amounts of power, deliver to you what you want.\n\nIt doesn't argue that superintelligences *will not know* this information.  You'll note that the central technology in the parable isn't an AI; it's an Outcome Pump.\n\nWhat it says, rather, is that there might be, say, a few tens of thousands of bits -- the exact number is not easy to estimate, we just need to know that it's more than a hundred bits and less than a billion bits and anything in that range is approximately the same problem from our standpoint -- that you need to get into *the steering function.*  If you understand the Central Divide that Hume's Razor points to, the distinction between probability and preference, etcetera, the post is trying to establish the idea that we need to get 13,333 bits or whatever into *the second side* of this divide.\n\nIn terms of where this point falls within *the larger argument,* this post is not saying that it's *particularly difficult* to get those 13,333 bits into the preference function; for all this post tries to say, locally, maybe that's as easy as having humans manually enter 13,333 yes-or-no answers into the system.  It's not talking about *the difficulty of doing the work* but rather *the amount and nature of a kind of work that needs to be done somehow.*\n\nDefinitely, the post does not say that it's hard to get those 13,333 bits into the *belief function* or *knowledge* of a superintelligence.\n\nSeparately from understanding correctly what this post is trying to communicate, at all, in 2007, there's the question of whether modern LLMs have anything to say about -- obviously not the post's original point -- but rather, *other* steps of the larger question in which this post's point appears.\n\nModern LLMs, if you present them with a text-based story like the one in this parable, are able to answer at least some text-based questions about whether you'd prefer your grandmother to be outside the building or be safely outside the building.  Let's admit this premised observation at face value.  Have we learned thereby the conclusion that it's easy to get all of that information into a superintelligence's preference function?\n\nAnd if we say \"No\", is this Eliezer making up post-hoc excuses?\n\nWhat exactly we learn from the evidence of how AI has played out in 2024 so far, is the sort of thing that deserves its own post.  But I observe that if you'd asked Eliezer-2007 whether an (Earth-originating) superintelligence could correctly predict the human response pattern about what to do with the grandmother -- solve the same task LLMs are solving, to at least the LLM's performance level -- Eliezer-2007 would have unhesitatingly answered \"yes\" and indeed \"OBVIOUSLY yes\".\n\nHow is this coherent?  Because the post's point is about how much information needs to get into *the preference function.*  To predict a human response pattern you need (only) *epistemic knowledge.  *This is part of why the post is about needing to give specifications *to an Outcome Pump,* rather than it depicting *an AI being surprised by its continually incorrect predictions about a human response pattern.*\n\nIf you don't see any important distinction between the two, then of course you'll think that it's incoherent to talk about that distinction.  But even if you think that Hume was mistaken about there existing any sort of interesting gap between 'is' and 'ought', you might by some act of empathy be able to imagine that *other* people think there's an interesting subject matter there, and they are trying to talk about it with you; otherwise you will just flatly misunderstand what they were trying to say, and mispredict their future utterances.  There's a difference between disagreeing with a point, and just flatly failing to get it, and hopefully you aspire to the first state of mind rather than the second.\n\nHave we learned anything stunningly hopeful from modern pre-AGIs getting down part of the *epistemic* part of the problem at their current ability levels, to the kind of resolution that this post talked about in 2007?  Or from it being possible to cajole pre-AGIs with loss functions into willingly *using* that knowledge to predict human text outputs?  Some people think that this teaches us that alignment is hugely easy.  I think they are mistaken, but that would take its own post to talk about.\n\nBut people who point to \"The Hidden Complexity of Wishes\" and say of it that it shows that I had a view which the current evidence already falsifies -- that I predicted that no AGI would ever be able to predict human response patterns about getting grandmothers out of burning buildings -- have simply: misunderstood what the post is about, not understood in particular why the post is about an Outcome Pump rather than an AI stupidly mispredicting human responses, and failed to pick up on the central point that Eliezer expects superintelligences to be smart in the sense of making excellent purely epistemic predictions.",
      "plaintextDescription": "You have misunderstood (1) the point this post was trying to communicate and (2) the structure of the larger argument where that point appears, as follows:\n\nFirst, let's talk about (2), the larger argument that this post's point was supposed to be relevant to.\n\nIs the larger argument that superintelligences will misunderstand what we really meant, due to a lack of knowledge about humans?\n\nIt is incredibly unlikely that Eliezer Yudkowsky in particular would have constructed an argument like this, whether in 2007, 2017, or even 1997.  At all of these points in my life, I visibly held quite a lot of respect for the epistemic prowess of superintelligences.  They were always going to know everything relevant about the complexities of human preference and desire.  The larger argument is about whether it's easy to make superintelligences end up caring.\n\nThis post isn't about the distinction between knowing and caring, to be clear; that's something I tried to cover elsewhere.  The relevant central divide falls in roughly the same conceptual place as Hume's Guillotine between 'is' and 'ought', or the difference between the belief function and the utility function.\n\n(I don't see myself as having managed to reliably communicate this concept (though the central idea is old indeed within philosophy) to the field that now sometimes calls itself \"AI alignment\"; so if you understand this distinction yourself, you should not assume that any particulary commentary within \"AI alignment\" is written from a place of understanding it too.)\n\nWhat this post is about is the amount of information-theoretic complexity that you need to get into the system's preferences, in order to have that system, given unlimited or rather extremely large amounts of power, deliver to you what you want.\n\nIt doesn't argue that superintelligences will not know this information.  You'll note that the central technology in the parable isn't an AI; it's an Outcome Pump.\n\nWhat it says, rather, is that there might be"
    },
    "baseScore": 38,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2024-02-15T18:23:55.052Z",
    "af": false
  },
  {
    "_id": "DsK23ZSEKm2ekzTpD",
    "postId": "g8HHKaWENEbqh2mgK",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This deserves a longer answer than I have time to allocate it, but I quickly remark that I don't recognize the philosophy or paradigm of updatelessness as refusing to learn things or being terrified of information; a rational agent should never end up in that circumstance, unless some perverse other agent is specifically punishing them for having learned the information (and will lose of their own value thereby; it shouldn't be possible for them to gain value by behaving \"perversely\" in that way, for then of course it's not \"perverse\").  Updatelessness is, indeed, exactly that sort of thinking which *prevents* you from being harmed by information, because your updateless exposure to information doesn't cause you to lose coordination with your counterfactual other selves or exhibit dynamic inconsistency with your past self.\n\nFrom an updateless standpoint, \"learning\" is just the process of reacting to new information the way your past self would want you to do in that branch of possibility-space; you should never need to remain ignorant of anything.  Maybe that involves not doing the thing that would then be optimal when considering only the branch of reality you turned out to be inside, but the updateless mind denies that this was ever the principle of rational choice, and so feels no need to stay ignorant in order to maintain dynamic consistency.",
      "plaintextDescription": "This deserves a longer answer than I have time to allocate it, but I quickly remark that I don't recognize the philosophy or paradigm of updatelessness as refusing to learn things or being terrified of information; a rational agent should never end up in that circumstance, unless some perverse other agent is specifically punishing them for having learned the information (and will lose of their own value thereby; it shouldn't be possible for them to gain value by behaving \"perversely\" in that way, for then of course it's not \"perverse\").  Updatelessness is, indeed, exactly that sort of thinking which prevents you from being harmed by information, because your updateless exposure to information doesn't cause you to lose coordination with your counterfactual other selves or exhibit dynamic inconsistency with your past self.\n\nFrom an updateless standpoint, \"learning\" is just the process of reacting to new information the way your past self would want you to do in that branch of possibility-space; you should never need to remain ignorant of anything.  Maybe that involves not doing the thing that would then be optimal when considering only the branch of reality you turned out to be inside, but the updateless mind denies that this was ever the principle of rational choice, and so feels no need to stay ignorant in order to maintain dynamic consistency."
    },
    "baseScore": 39,
    "voteCount": 26,
    "createdAt": null,
    "postedAt": "2024-02-14T22:26:46.088Z",
    "af": true
  },
  {
    "_id": "kQFufXHTArxPxTdia",
    "postId": "8HYJwQepynHsRKr6j",
    "parentCommentId": "qKeq5LTMpbo76WgCt",
    "topLevelCommentId": "9pKofQAchdgCH8jjm",
    "contents": {
      "markdown": "They can solve it however they like, once they're past the point of expecting things to work that sometimes don't work.  I have guesses but any group that still needs my hints should wait and augment harder.",
      "plaintextDescription": "They can solve it however they like, once they're past the point of expecting things to work that sometimes don't work.  I have guesses but any group that still needs my hints should wait and augment harder."
    },
    "baseScore": 10,
    "voteCount": 24,
    "createdAt": null,
    "postedAt": "2023-12-28T17:53:32.280Z",
    "af": true
  },
  {
    "_id": "9pKofQAchdgCH8jjm",
    "postId": "8HYJwQepynHsRKr6j",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I disagree with my characterization as thinking problems can be solved on paper, and with the name \"Poet\".  I think the problems can't be solved by twiddling systems weak enough to be passively safe, and hoping their behavior generalizes up to dangerous levels.  I don't think paper solutions will work either, and humanity needs to back off and augment intelligence before proceeding.  I do not take the position that we need a global shutdown of this research field because I think that guessing stuff without trying it is easy, but because guessing it even with some safe weak lesser tries is still impossibly hard.  My message to humanity is \"back off and augment\" not \"back off and solve it with a clever theory\".",
      "plaintextDescription": "I disagree with my characterization as thinking problems can be solved on paper, and with the name \"Poet\".  I think the problems can't be solved by twiddling systems weak enough to be passively safe, and hoping their behavior generalizes up to dangerous levels.  I don't think paper solutions will work either, and humanity needs to back off and augment intelligence before proceeding.  I do not take the position that we need a global shutdown of this research field because I think that guessing stuff without trying it is easy, but because guessing it even with some safe weak lesser tries is still impossibly hard.  My message to humanity is \"back off and augment\" not \"back off and solve it with a clever theory\"."
    },
    "baseScore": 75,
    "voteCount": 41,
    "createdAt": null,
    "postedAt": "2023-12-27T18:39:16.037Z",
    "af": true
  },
  {
    "_id": "fkfZdCoZkfB3fHHeG",
    "postId": "j9Q8bRmwCgXRYAgcJ",
    "parentCommentId": "ox7gMhYwvqCneb8Z4",
    "topLevelCommentId": "ox7gMhYwvqCneb8Z4",
    "contents": {
      "markdown": "Not what comes up for me, when I go incognito and google AI risk lesswrong.",
      "plaintextDescription": "Not what comes up for me, when I go incognito and google AI risk lesswrong."
    },
    "baseScore": 7,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2023-12-24T00:40:26.260Z",
    "af": false
  },
  {
    "_id": "3SmSwYbdxXBQQwyBj",
    "postId": "8viKzSrYhb6EFk6wg",
    "parentCommentId": "oeCArhEmWcDLmmcG8",
    "topLevelCommentId": "22wNnGvZErLnmRRm7",
    "contents": {
      "markdown": "I rather expect that existing robotic machinery could be controlled by ASI rather than \"moderately smart intelligence\" into picking up the pieces of a world economy after it collapses, or that if for some weird reason it was trying to play around with static-cling spaghetti It could pick up the pieces of the economy that way too.",
      "plaintextDescription": "I rather expect that existing robotic machinery could be controlled by ASI rather than \"moderately smart intelligence\" into picking up the pieces of a world economy after it collapses, or that if for some weird reason it was trying to play around with static-cling spaghetti It could pick up the pieces of the economy that way too."
    },
    "baseScore": 5,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2023-12-10T19:39:06.194Z",
    "af": false
  },
  {
    "_id": "ce6qNYZiTtoZYsQzw",
    "postId": "8viKzSrYhb6EFk6wg",
    "parentCommentId": "fRFxkhK46pN9AK5xu",
    "topLevelCommentId": "22wNnGvZErLnmRRm7",
    "contents": {
      "markdown": "It's false that currently existing robotic machinery controlled by moderately smart intelligence can pick up the pieces of a world economy after it collapses.  One well-directed algae cell could, but not existing robots controlled by moderate intelligence.",
      "plaintextDescription": "It's false that currently existing robotic machinery controlled by moderately smart intelligence can pick up the pieces of a world economy after it collapses.  One well-directed algae cell could, but not existing robots controlled by moderate intelligence."
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2023-12-07T17:37:03.861Z",
    "af": false
  },
  {
    "_id": "GhE2Wdrn5ixnNHdDe",
    "postId": "8viKzSrYhb6EFk6wg",
    "parentCommentId": "L36pfbXkSS5LGvD78",
    "topLevelCommentId": "22wNnGvZErLnmRRm7",
    "contents": {
      "markdown": "What does this operationalize as?  Presumably not that if we load a bone and a diamond rod under equal pressures, the diamond rod breaks first?  Is it more about if we drop sudden sharp weights onto a bone rod and a diamond rod, the diamond rod breaks first?  I admit I hadn't expected that, despite a general notion that diamond is crystal and crystals are unexpectedly fragile against particular kinds of hits, and if so that modifies my sense of what's a valid metaphor to use.",
      "plaintextDescription": "What does this operationalize as?  Presumably not that if we load a bone and a diamond rod under equal pressures, the diamond rod breaks first?  Is it more about if we drop sudden sharp weights onto a bone rod and a diamond rod, the diamond rod breaks first?  I admit I hadn't expected that, despite a general notion that diamond is crystal and crystals are unexpectedly fragile against particular kinds of hits, and if so that modifies my sense of what's a valid metaphor to use."
    },
    "baseScore": 18,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2023-12-07T01:20:06.240Z",
    "af": false
  },
  {
    "_id": "N9FcGvHDX4KgAPiia",
    "postId": "8viKzSrYhb6EFk6wg",
    "parentCommentId": "fqZvkkHRAuE95YaNf",
    "topLevelCommentId": "22wNnGvZErLnmRRm7",
    "contents": {
      "markdown": "\"Pandemics\" aren't a locally valid substitute step in my own larger argument, because an ASI needs its own manufacturing infrastructure before it makes sense for the ASI to kill the humans currently keeping its computers turned on.  So things that kill a bunch of humans are not a valid substitute for being able to, eg, take over and repurpose the existing solar-powered micron-diameter self-replicating factory systems, aka algae, and those repurposed algae being able to build enough computing substrate to go on running the ASI after the humans die.\n\nIt's possible this argument can and should be carried without talking about the level above biology, but I'm nervous that this causes people to start thinking in terms of Hollywood movie plots about defeating pandemics and hunting down the AI's hidden cave of shoggoths, rather than hearing, \"And this is a lower bound but actually in real life you just fall over dead.\"",
      "plaintextDescription": "\"Pandemics\" aren't a locally valid substitute step in my own larger argument, because an ASI needs its own manufacturing infrastructure before it makes sense for the ASI to kill the humans currently keeping its computers turned on.  So things that kill a bunch of humans are not a valid substitute for being able to, eg, take over and repurpose the existing solar-powered micron-diameter self-replicating factory systems, aka algae, and those repurposed algae being able to build enough computing substrate to go on running the ASI after the humans die.\n\nIt's possible this argument can and should be carried without talking about the level above biology, but I'm nervous that this causes people to start thinking in terms of Hollywood movie plots about defeating pandemics and hunting down the AI's hidden cave of shoggoths, rather than hearing, \"And this is a lower bound but actually in real life you just fall over dead.\""
    },
    "baseScore": 21,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2023-12-06T20:03:49.626Z",
    "af": false
  },
  {
    "_id": "22wNnGvZErLnmRRm7",
    "postId": "8viKzSrYhb6EFk6wg",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Why is flesh weaker than diamond?  Diamond is made of carbon-carbon bonds.  Proteins also have some carbon-carbon bonds!  So why should a diamond blade be able to cut skin?\n\nI reply:  Because the strength of the material is determined by its weakest link, not its strongest link.  A structure of steel beams held together at the vertices by Scotch tape (and lacking other clever arrangements of mechanical advantage) has the strength of Scotch tape rather than the strength of steel.\n\nOr:  Even when the load-bearing forces holding large molecular systems together are locally covalent bonds, as in lignin (what makes wood strong), if you've got larger molecules only held together by covalent bonds at interspersed points along their edges, that's like having 10cm-diameter steel beams held together by 1cm welds.  Again, barring other clever arrangements of mechanical advantage, that structure has the strength of 1cm of steel rather than 10cm of steel.\n\nBone is stronger than wood; it runs on a relatively stronger structure of ionic bonds, which are no locally weaker than carbon bonds in terms of attojoules of potential energy per bond.  Bone is weaker than diamond, then, because... why?\n\nWell, partially, IIUC, because calcium atoms are heavier than carbon atoms.  So even if per-bond the ionic forces are strong, some of that is lost in the price you pay for including heavier atoms whose nuclei have more protons that are able to exert the stronger electrical forces making up that stronger bond.\n\nBut mainly, bone is so much weaker than diamond (on my understanding) because the carbon bonds in diamond have a regular crystal structure that locks the carbon atoms into relative angles, and in a solid diamond this crystal structure is tesselated globally.  Hydroxyapatite (the crystal part of bone) also tesselates in an energetically favorable configuration; but (I could be wrong about this) it doesn't have the same local resistance to local deformation; and also, the actual hydroxyapatite crystal is assembled by other tissues that layer the ionic components into place, which means that a larger structure of bone is full of fault lines.  Bone cleaves along the weaker fault line, not at its strongest point.\n\nBut then, why don't diamond bones exist already?  Not just for the added strength; why make the organism look for calcium and phosphorus instead of just carbon?\n\nThe search process of evolutionary biology is not the search of engineering; natural selection can only access designs via pathways of incremental mutations that are locally advantageous, not intelligently designed simultaneous changes that compensate for each other.  There were, last time I checked, only three known cases where evolutionary biology invented the freely rotating wheel.  Two of those known cases are ATP synthase and the bacterial flagellum, which demonstrates that freely rotating wheels are in fact incredibly useful in biology, and are conserved when biology stumbles across them after a few hundred million years of search.  But there's no use for a freely rotating wheel without a bearing and there's no use for a bearing without a freely rotating wheel, and a simultaneous dependency like that is a huge obstacle to biology, even though it's a hardly noticeable obstacle to intelligent engineering.\n\nThe entire human body, faced with a strong impact like being gored by a rhinocerous horn, will fail at its weakest point, not its strongest point.  How much evolutionary advantage is there to stronger bone, if what fails first is torn muscle?  How much advantage is there to an impact-resistant kidney, if most fights that destroy a kidney will kill you anyways?  Evolution is not the sort of optimizer that says, \"Okay, let's design an *entire* stronger body.\"  (Analogously, the collection of faults that add up to \"old age\" is large enough that a little more age resistance in one place is not much of an advantage if other aging systems or outward accidents will soon kill you anyways.)\n\nI don't even think we have much of a reason to believe that it'd be physically (rather than informationally) difficult to have a set of enzymes that synthesize diamond.  It could just require 3 things to go right simultaneously, and so be much much harder to stumble across than tossing more hydroxyapatite to lock into place in a bone crystal.  And then even if somehow evolution hit on the right set of 3 simultaneous mutations, sometime over the history of Earth, the resulting little isolated chunk of diamond probably would not be somewhere in the phenotype that had previously constituted the weakest point in a mechanical system that frequently failed.  If evolution has huge difficulty inventing wheels, why expect that it could build diamond chainmail, even assuming that diamond chainmail is physically possible and could be useful to an organism that had it?\n\nTalking to the general public is hard.  The first concept I'm trying to convey to them is that there's an underlying physical, mechanical reason that flesh is weaker than diamond; and that this reason *isn't* that things animated by vitalic spirit, elan vital, can self-heal and self-reproduce at the cost of being weaker than the cold steel making up lifeless machines, as is the price of magic imposed by the universe to maintain game balance.  This is a very natural way for humans to think; and the thing I am trying to come in and do is say, \"Actually, no, it's not a mystical balance, it's that diamond is held together by bonds that are hundreds of kJ/mol; and the mechanical strength of proteins is determined by forces a hundred times as weak as that, the part where proteins fold up like spaghetti held together by static cling.\"\n\nThere is then a deeper story that's even harder to explain, about *why* evolution doesn't build freely rotating wheels or diamond chainmail; why evolutionary design doesn't find the physically possible stronger systems.  But first you need to give people a mechanical intuition for why, in a very rough intuitive sense, it *is* physically possible to have stuff that moves and lives and self-repairs but is strong like diamond instead of flesh, without this violating a mystical balance where the price of vitalic animation is lower material strength.\n\nAnd that mechanical intuition is:  Deep down is a bunch of stuff that, if you could see videos of it, would look more like tiny machines than like magic, though they would not look like familiar machines (very few freely rotating wheels).  Then why aren't these machines strong like human machines of steel are strong?  Because iron atoms are stronger than carbon atoms?  Actually no, diamond is made of carbon and that's still quite strong.  The reason is that these tiny systems of machinery are held together *(at the weakest joints, not the strongest joints!)* by static cling.\n\nAnd then the deeper question:  *Why* does evolution build that way?  And the deeper answer:  Because everything evolution builds is arrived at as an error, a mutation, from something else that it builds.  Very tight bonds fold up along very deterministic pathways.  So (in the average case, not every case) the neighborhood of functionally similar designs is densely connected along shallow energy gradients and sparsely connected along deep energy gradients.  Intelligence can leap long distances through that design space using coordinated changes, but evolutionary exploration usually cannot.\n\nAnd I *do* try to explain that too.  But it is legitimately more abstract and harder to understand.  So I lead with the idea that proteins are held together by static cling.  This is, I think, validly the first fact you lead with if the audience does not already know it, and just has no clue why anyone could possibly possibly think that there *might even be* machinery that does what bacterial machinery does but better.  The typical audience is not starting out with the intuition that one would *naively* think that of course you could put together stronger molecular machinery, given the physics of stronger bonds, and then we debate whether (as I believe) the naive intuition is actually just valid and correct; they don't understand what the naive intuition is about, and that's the first thing to convey.\n\nIf somebody then says, \"How can you be so ignorant of chemistry?  *Some* atoms in protein are held together by covalent bonds, not by static cling!  There's even eg sulfur bonds whereby some parts of the folded-spaghetti systems end up glued together with real glue!\" then this does not *validly* address the original point because: the underlying point about why flesh is more easily cleaved than diamond, is about *the weakest points of flesh* rather than *the strongest points in flesh,* because that's what determines the mechanical strength of the larger system.\n\nI think there is an important way of looking at questions like these where, at the final end, you ask yourself, \"Okay, but does my argument prove that flesh is in fact as strong as diamond?  Why isn't flesh as strong as diamond, then, if I've refuted the original argument for why it isn't?\" and this is the question that leads you to realize that some local strong covalent bonds don't matter to the argument if those bonds aren't the parts that break under load.\n\nMy main moral qualm about using the Argument From Folded Spaghetti Held Together By Static Cling as an intuition pump is that the local ionic bonds in bone are legitimately as strong per-bond as the C-C bonds in diamond, and the reason that bone is weaker than diamond is (iiuc) actually more about irregularity, fault lines, and resistance to local deformation than about kJ/mol of the underlying bonds.  If somebody says \"Okay, fine, you've validly explained why flesh is weaker than diamond, but why is *bone* weaker than diamond?\" I have to reply \"Valid, iiuc that's legit more about irregularity and fault lines and interlaced weaker superstructure and local deformation resistance of the bonds, rather than the raw potential energy deltas of the load-bearing welds.\"",
      "plaintextDescription": "Why is flesh weaker than diamond?  Diamond is made of carbon-carbon bonds.  Proteins also have some carbon-carbon bonds!  So why should a diamond blade be able to cut skin?\n\nI reply:  Because the strength of the material is determined by its weakest link, not its strongest link.  A structure of steel beams held together at the vertices by Scotch tape (and lacking other clever arrangements of mechanical advantage) has the strength of Scotch tape rather than the strength of steel.\n\nOr:  Even when the load-bearing forces holding large molecular systems together are locally covalent bonds, as in lignin (what makes wood strong), if you've got larger molecules only held together by covalent bonds at interspersed points along their edges, that's like having 10cm-diameter steel beams held together by 1cm welds.  Again, barring other clever arrangements of mechanical advantage, that structure has the strength of 1cm of steel rather than 10cm of steel.\n\nBone is stronger than wood; it runs on a relatively stronger structure of ionic bonds, which are no locally weaker than carbon bonds in terms of attojoules of potential energy per bond.  Bone is weaker than diamond, then, because... why?\n\nWell, partially, IIUC, because calcium atoms are heavier than carbon atoms.  So even if per-bond the ionic forces are strong, some of that is lost in the price you pay for including heavier atoms whose nuclei have more protons that are able to exert the stronger electrical forces making up that stronger bond.\n\nBut mainly, bone is so much weaker than diamond (on my understanding) because the carbon bonds in diamond have a regular crystal structure that locks the carbon atoms into relative angles, and in a solid diamond this crystal structure is tesselated globally.  Hydroxyapatite (the crystal part of bone) also tesselates in an energetically favorable configuration; but (I could be wrong about this) it doesn't have the same local resistance to local deformation; and also, the actual hydroxyap"
    },
    "baseScore": 181,
    "voteCount": 81,
    "createdAt": null,
    "postedAt": "2023-12-06T18:32:19.662Z",
    "af": false
  },
  {
    "_id": "2xytnGXBFobvQEFXH",
    "postId": "7im8at9PmhbT4JHsW",
    "parentCommentId": "naqbGdJbfun9FeJkR",
    "topLevelCommentId": "naqbGdJbfun9FeJkR",
    "contents": {
      "markdown": "Depends on how much of a superintelligence, how implemented.  I wouldn't be surprised if somebody got far superhuman theorem-proving from a mind that didn't generalize beyond theorems.  Presuming you were asking it to prove old-school fancy-math theorems, and not to, eg, arbitrarily speed up a bunch of real-world computations like asking it what GPT-4 would say about things, etc.",
      "plaintextDescription": "Depends on how much of a superintelligence, how implemented.  I wouldn't be surprised if somebody got far superhuman theorem-proving from a mind that didn't generalize beyond theorems.  Presuming you were asking it to prove old-school fancy-math theorems, and not to, eg, arbitrarily speed up a bunch of real-world computations like asking it what GPT-4 would say about things, etc."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-11-26T22:47:06.270Z",
    "af": false
  },
  {
    "_id": "cFBSCHLfW4L5XirhJ",
    "postId": "hQxYBfu2LPc9Ydo6w",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Solution (in retrospect this should've been posted a few years earlier):\n\nlet  \n'Na' = box N contains angry frog  \n'Ng' = N gold  \n'Nf' = N's inscription false  \n'Nt' = N's inscription true  \n  \nconsistent states must have 1f 2t or 1t 2f, and 1a 2g or 1g 2a  \n  \nthen:  \n  \n1a 1t, 2g 2f => 1t, 2f  \n1a 1f, 2g 2t => 1f, 2t  \n1g 1t, 2a 2f => 1t, 2t  \n1g 1f, 2a 2t => 1f, 2f",
      "plaintextDescription": "Solution (in retrospect this should've been posted a few years earlier):\n\nlet\n'Na' = box N contains angry frog\n'Ng' = N gold\n'Nf' = N's inscription false\n'Nt' = N's inscription true\n\nconsistent states must have 1f 2t or 1t 2f, and 1a 2g or 1g 2a\n\nthen:\n\n1a 1t, 2g 2f => 1t, 2f\n1a 1f, 2g 2t => 1f, 2t\n1g 1t, 2a 2f => 1t, 2t\n1g 1f, 2a 2t => 1f, 2f"
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2023-11-06T17:52:06.843Z",
    "af": false
  },
  {
    "_id": "c8EWYsoc9BALC4PXD",
    "postId": "FEFQSGLhJFpqmEhgi",
    "parentCommentId": "uB4CbNTQxvHqNiPWr",
    "topLevelCommentId": "y9jJLMBof7bjYzxg7",
    "contents": {
      "markdown": "I currently guess that a research community of non-upgraded alignment researchers with a hundred years to work, picks out a plausible-sounding non-solution and kills everyone at the end of the hundred years.",
      "plaintextDescription": "I currently guess that a research community of non-upgraded alignment researchers with a hundred years to work, picks out a plausible-sounding non-solution and kills everyone at the end of the hundred years."
    },
    "baseScore": 19,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2023-11-03T21:19:00.936Z",
    "af": false
  },
  {
    "_id": "y9jJLMBof7bjYzxg7",
    "postId": "FEFQSGLhJFpqmEhgi",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I don't think that faster alignment researchers get you to victory, but uploading should also allow for upgrading and while that part is not trivial I expect it to work.",
      "plaintextDescription": "I don't think that faster alignment researchers get you to victory, but uploading should also allow for upgrading and while that part is not trivial I expect it to work."
    },
    "baseScore": 25,
    "voteCount": 14,
    "createdAt": null,
    "postedAt": "2023-11-03T18:15:52.225Z",
    "af": false
  },
  {
    "_id": "iGucXF4Fwj3adNKqz",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "Q7fEoBLJTb2cF83hu",
    "topLevelCommentId": "tq7NyfQ9sivn7YSYo",
    "contents": {
      "markdown": "AI happening through deep learning at all is a huge update against alignment success, because deep learning is incredibly opaque.  LLMs possibly ending up at the center is a small update in favor of alignment success, because it means we might (*through some clever sleight, this part is not trivial)* be able to have humanese sentences play an inextricable role at the center of thought (hence MIRI's early interest in the Visible Thoughts Project).\n\nThe part where LLMs are to predict English answers to some English questions about values, and show common-sense relative to their linguistic shadow of the environment as it was presented to them by humans within an Internet corpus, is not actually very much hope because a sane approach doesn't involve trying to promote an LLM's predictive model of human discourse about morality to be in charge of a superintelligence's dominion of the galaxy.  What you would like to promote to values are concepts like \"corrigibility\", eg \"low impact\" or \"soft optimization\", which aren't part of everyday human life and aren't in the training set because humans do not have those values.",
      "plaintextDescription": "AI happening through deep learning at all is a huge update against alignment success, because deep learning is incredibly opaque.  LLMs possibly ending up at the center is a small update in favor of alignment success, because it means we might (through some clever sleight, this part is not trivial) be able to have humanese sentences play an inextricable role at the center of thought (hence MIRI's early interest in the Visible Thoughts Project).\n\nThe part where LLMs are to predict English answers to some English questions about values, and show common-sense relative to their linguistic shadow of the environment as it was presented to them by humans within an Internet corpus, is not actually very much hope because a sane approach doesn't involve trying to promote an LLM's predictive model of human discourse about morality to be in charge of a superintelligence's dominion of the galaxy.  What you would like to promote to values are concepts like \"corrigibility\", eg \"low impact\" or \"soft optimization\", which aren't part of everyday human life and aren't in the training set because humans do not have those values."
    },
    "baseScore": 40,
    "voteCount": 43,
    "createdAt": null,
    "postedAt": "2023-10-06T07:24:36.828Z",
    "af": false
  },
  {
    "_id": "tq7NyfQ9sivn7YSYo",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I have never since 1996 thought that it would be hard to get superintelligences to accurately model reality with respect to problems as simple as \"predict what a human will thumbs-up or thumbs-down\".  The theoretical distinction between producing epistemic rationality (theoretically straightforward) and shaping preference (theoretically hard) is present in my mind at every moment that I am talking about these issues; it is to me a central divide of my ontology.\n\nIf you think you've demonstrated by clever textual close reading that Eliezer-2018 or Eliezer-2008 thought that it would be hard to get a superintelligence to understand humans, you have arrived at a contradiction and need to back up and start over.\n\nThe argument we are trying to explain has an additional step that you're missing.  You think that we are pointing to the hidden complexity of wishes in order to establish in one step that it would therefore be hard to get an AI to output a correct wish shape, because the wishes are complex, so it would be difficult to get an AI to predict them.  This is not what we are trying to say.  We are trying to say that because wishes have a lot of hidden complexity, the thing you are trying to get into the AI's preferences has a lot of hidden complexity.  This makes the nonstraightforward and shaky problem of getting a thing into the AI's preferences, be harder and more dangerous than if we were just trying to get a single information-theoretic bit in there.  **Getting a shape into the AI's preferences is different from getting it into the AI's predictive model.  MIRI is always in every instance talking about the first thing and not the second.**\n\nYou obviously need to get a thing into the AI at all, in order to get it into the preferences, but getting it into the AI's predictive model is not sufficient.  It helps, but only in the same sense that having low-friction smooth ball-bearings would help in building a perpetual motion machine; the low-friction ball-bearings are not the main problem, they are a kind of thing it is much easier to make progress on compared to the main problem.  Even if, in fact, the ball-bearings would legitimately be part of the mechanism if you could build one!  Making lots of progress on smoother, lower-friction ball-bearings is even so not the sort of thing that should cause you to become much more hopeful about the perpetual motion machine.  It is on the wrong side of a theoretical divide between what is straightforward and what is not.\n\nYou will probably protest that we phrased our argument badly relative to the sort of thing that you could only possibly be expected to hear, from your perspective.  If so this is not surprising, because explaining things is very hard.  Especially when everyone in the audience comes in with a different set of preconceptions and a different internal language about this nonstandardized topic.  But mostly, explaining this thing is hard and I tried taking lots of different angles on trying to get the idea across.\n\nIn modern times, and earlier, it is of course very hard for ML folk to get their AI to make completely accurate predictions about human behavior.  They have to work very hard and put a lot of sweat into getting more accurate predictions out!  When we try to say that this is on the shallow end of a shallow-deep theoretical divide (corresponding to Hume's Razor) it often sounds to them like their hard work is being devalued and we could not possibly understand how hard it is to get an AI to make good predictions.\n\nNow that GPT-4 is making surprisingly good predictions, they feel they have learned something very surprising and shocking!  They cannot possibly hear our words when we say that this is still on the shallow end of a shallow-deep theoretical divide!  They think we are refusing to come to grips with this surprising shocking thing and that it surely ought to overturn all of our old theories; which were, yes, phrased and taught in a time before GPT-4 was around, and therefore do not in fact carefully emphasize at every point of teaching how in principle a superintelligence would of course have no trouble predicting human text outputs.  We did not expect GPT-4 to happen, in fact, intermediate trajectories are harder to predict than endpoints, so we did not carefully phrase all our explanations in a way that would make them hard to misinterpret after GPT-4 came around.\n\nBut if you had asked us back then if a superintelligence would automatically be very good at predicting human text outputs, I guarantee we would have said yes.  You could then have asked us in a shocked tone how this could possibly square up with the notion of \"the hidden complexity of wishes\" and we could have explained that part in advance.  Alas, nobody actually predicted GPT-4 so we do not have that advance disclaimer down in that format.  But it is not a case where we are just failing to process the collision between two parts of our belief system; it actually remains quite straightforward theoretically.  I wish that all of these past conversations were archived to a common place, so that I could search and show you many pieces of text which would talk about this critical divide between prediction and preference (as I would now term it) and how I did in fact expect superintelligences to be able to predict things!",
      "plaintextDescription": "I have never since 1996 thought that it would be hard to get superintelligences to accurately model reality with respect to problems as simple as \"predict what a human will thumbs-up or thumbs-down\".  The theoretical distinction between producing epistemic rationality (theoretically straightforward) and shaping preference (theoretically hard) is present in my mind at every moment that I am talking about these issues; it is to me a central divide of my ontology.\n\nIf you think you've demonstrated by clever textual close reading that Eliezer-2018 or Eliezer-2008 thought that it would be hard to get a superintelligence to understand humans, you have arrived at a contradiction and need to back up and start over.\n\nThe argument we are trying to explain has an additional step that you're missing.  You think that we are pointing to the hidden complexity of wishes in order to establish in one step that it would therefore be hard to get an AI to output a correct wish shape, because the wishes are complex, so it would be difficult to get an AI to predict them.  This is not what we are trying to say.  We are trying to say that because wishes have a lot of hidden complexity, the thing you are trying to get into the AI's preferences has a lot of hidden complexity.  This makes the nonstraightforward and shaky problem of getting a thing into the AI's preferences, be harder and more dangerous than if we were just trying to get a single information-theoretic bit in there.  Getting a shape into the AI's preferences is different from getting it into the AI's predictive model.  MIRI is always in every instance talking about the first thing and not the second.\n\nYou obviously need to get a thing into the AI at all, in order to get it into the preferences, but getting it into the AI's predictive model is not sufficient.  It helps, but only in the same sense that having low-friction smooth ball-bearings would help in building a perpetual motion machine; the low-friction ball-bearings are not"
    },
    "baseScore": 115,
    "voteCount": 74,
    "createdAt": null,
    "postedAt": "2023-10-05T20:37:11.761Z",
    "af": true
  },
  {
    "_id": "neAtKQqLWxiwPY5AE",
    "postId": "gvA4j8pGYG4xtaTkw",
    "parentCommentId": "pyFgH3QWM5wd3Qcfn",
    "topLevelCommentId": "hnG7Wtip2azaLgtsD",
    "contents": {
      "markdown": "There's perhaps more detail in Project Lawful and in some nearby stories (\"for no laid course prepare\", \"aviation is the most dangerous routine activity\").",
      "plaintextDescription": "There's perhaps more detail in Project Lawful and in some nearby stories (\"for no laid course prepare\", \"aviation is the most dangerous routine activity\")."
    },
    "baseScore": 15,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2023-10-02T02:50:46.242Z",
    "af": false
  },
  {
    "_id": "MgQQAPks8BffyRWWg",
    "postId": "6miu9BsKdoAi72nkL",
    "parentCommentId": "9iLCHyWG42CcYJfuW",
    "topLevelCommentId": "9iLCHyWG42CcYJfuW",
    "contents": {
      "markdown": "> Have you ever seen or even heard of a person who is obese who doesn't eat hyperpalatable foods? (That is, they only eat naturally tasting, unprocessed, \"healthy\" foods).\n\nTried this for many years.  Paleo diet; eating mainly broccoli and turkey; trying to get most of my calories from giant salads.  Nothing.",
      "plaintextDescription": "> Have you ever seen or even heard of a person who is obese who doesn't eat hyperpalatable foods? (That is, they only eat naturally tasting, unprocessed, \"healthy\" foods).\n\nTried this for many years.  Paleo diet; eating mainly broccoli and turkey; trying to get most of my calories from giant salads.  Nothing."
    },
    "baseScore": 13,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2023-08-07T00:20:11.075Z",
    "af": false
  },
  {
    "_id": "9sFxCpn3ZzWdKws6c",
    "postId": "t5W87hQF5gKyTofQB",
    "parentCommentId": "GbFHxBoGKLMduypvH",
    "topLevelCommentId": "rsKjPAPqeFwQuNibD",
    "contents": {
      "markdown": "Received $95.51.  :)",
      "plaintextDescription": "Received $95.51.  :)"
    },
    "baseScore": 23,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2023-07-30T00:25:05.899Z",
    "af": false
  },
  {
    "_id": "D7aDHgQWeKZXdc2ns",
    "postId": "t5W87hQF5gKyTofQB",
    "parentCommentId": "q95RiuismjBus3FnE",
    "topLevelCommentId": "rsKjPAPqeFwQuNibD",
    "contents": {
      "markdown": "I am not - $150K is as much as I care to stake at my present weath levels - and while I refunded your payment, I was charged a $44.90 fee on the original transmission which was not then refunded to me.",
      "plaintextDescription": "I am not - $150K is as much as I care to stake at my present weath levels - and while I refunded your payment, I was charged a $44.90 fee on the original transmission which was not then refunded to me."
    },
    "baseScore": 13,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2023-07-29T18:57:43.894Z",
    "af": false
  },
  {
    "_id": "uEvFKDPQMTdGtfSjj",
    "postId": "t5W87hQF5gKyTofQB",
    "parentCommentId": "rsKjPAPqeFwQuNibD",
    "topLevelCommentId": "rsKjPAPqeFwQuNibD",
    "contents": {
      "markdown": "Though I disagree with [@RatsWrongAboutUAP](https://www.lesswrong.com/users/ratswrongaboutuap?mention=user) (see [this tweet](https://twitter.com/ESYudkowsky/status/1682446903953457152?s=20)) and took the other side of the bet, I say a word of praise for RatsWrong about following exactly the proper procedure to make the point they wanted to make, and communicating that they really actually think we're wrong here.  Object-level disagreement, meta-level high-five.",
      "plaintextDescription": "Though I disagree with @RatsWrongAboutUAP (see this tweet) and took the other side of the bet, I say a word of praise for RatsWrong about following exactly the proper procedure to make the point they wanted to make, and communicating that they really actually think we're wrong here.  Object-level disagreement, meta-level high-five."
    },
    "baseScore": 98,
    "voteCount": 58,
    "createdAt": null,
    "postedAt": "2023-07-21T18:28:56.313Z",
    "af": false
  },
  {
    "_id": "rcdmm6DCtMQxqscvp",
    "postId": "t5W87hQF5gKyTofQB",
    "parentCommentId": "rsKjPAPqeFwQuNibD",
    "topLevelCommentId": "rsKjPAPqeFwQuNibD",
    "contents": {
      "markdown": "Received.",
      "plaintextDescription": "Received."
    },
    "baseScore": 15,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2023-07-21T17:11:18.690Z",
    "af": false
  },
  {
    "_id": "rsKjPAPqeFwQuNibD",
    "postId": "t5W87hQF5gKyTofQB",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "My $150K against your $1K if you're still up for it at 150:1.  Paypal to [yudkowsky@gmail.com](mailto:yudkowsky@gmail.com) with \"UFO bet\" in subject or text, please include counterparty payment info if it's not \"email the address which sent me that payment\".\n\nKey qualifier:  *This applies only to UFOs spotted before July 19th, 2023,* rather than applying to eg future UFOs generated by secret AI projects which were *not* putatively flying around and spotted before July 19th, 2023.\n\nADDED:  $150K is as much as I care to stake at my current wealth level, to rise to this bettors' challenge and make this point; not taking on further bets except at substantially less extreme odds.",
      "plaintextDescription": "My $150K against your $1K if you're still up for it at 150:1.  Paypal to yudkowsky@gmail.com with \"UFO bet\" in subject or text, please include counterparty payment info if it's not \"email the address which sent me that payment\".\n\nKey qualifier:  This applies only to UFOs spotted before July 19th, 2023, rather than applying to eg future UFOs generated by secret AI projects which were not putatively flying around and spotted before July 19th, 2023.\n\nADDED:  $150K is as much as I care to stake at my current wealth level, to rise to this bettors' challenge and make this point; not taking on further bets except at substantially less extreme odds."
    },
    "baseScore": 80,
    "voteCount": 40,
    "createdAt": null,
    "postedAt": "2023-07-20T01:02:21.880Z",
    "af": false
  },
  {
    "_id": "bMrh6QJNrjkHs9ueo",
    "postId": "brXr7PJ2W4Na2EW2q",
    "parentCommentId": "o38fRFQsJPqDCoB5X",
    "topLevelCommentId": "tYBPjetgZW4iMqe4s",
    "contents": {
      "markdown": "TBC, I definitely agree that there's some basic structural issue here which I don't know how to resolve.  I was trying to describe properties I thought the solution needed to have, which ruled out some structural proposals I saw as naive; not saying that I had a good first-principles way to arrive at that solution.",
      "plaintextDescription": "TBC, I definitely agree that there's some basic structural issue here which I don't know how to resolve.  I was trying to describe properties I thought the solution needed to have, which ruled out some structural proposals I saw as naive; not saying that I had a good first-principles way to arrive at that solution."
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2023-07-17T04:30:25.767Z",
    "af": true
  },
  {
    "_id": "tifkQXc4aCJMEkioY",
    "postId": "fc9KjZeSLuHN7HfW6",
    "parentCommentId": "zkGBkexJGYziQ4fRA",
    "topLevelCommentId": "rZcqyLEEuskcwnnqj",
    "contents": {
      "markdown": "At the superintelligent level there's not a binary difference between those two clusters.  You just compute each thing you need to know efficiently.",
      "plaintextDescription": "At the superintelligent level there's not a binary difference between those two clusters.  You just compute each thing you need to know efficiently."
    },
    "baseScore": 6,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2023-06-07T16:27:34.306Z",
    "af": true
  },
  {
    "_id": "dMHdWcxgSpcdyG4hb",
    "postId": "2NncxDQ3KBDCxiJiP",
    "parentCommentId": "p4tCMpSHEXjNW4aWK",
    "topLevelCommentId": "ofPTrG6wsq7CxuTXk",
    "contents": {
      "markdown": "I sometimes mention the possibility of being stored and sold to aliens a billion years later, which seems to me to validly incorporate most all the hopes and fears and uncertainties that should properly be involved, without getting into any weirdness that I don't expect Earthlings to think about validly.",
      "plaintextDescription": "I sometimes mention the possibility of being stored and sold to aliens a billion years later, which seems to me to validly incorporate most all the hopes and fears and uncertainties that should properly be involved, without getting into any weirdness that I don't expect Earthlings to think about validly."
    },
    "baseScore": 40,
    "voteCount": 25,
    "createdAt": null,
    "postedAt": "2023-06-02T05:27:13.641Z",
    "af": false
  },
  {
    "_id": "rZcqyLEEuskcwnnqj",
    "postId": "fc9KjZeSLuHN7HfW6",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Lacking time right now for a long reply:  The main thrust of my reaction is that this seems like a style of thought which would have concluded in 2008 that it's incredibly unlikely for superintelligences to be able to solve the protein folding problem.  People did, in fact, claim that to me in 2008.  It furthermore seemed to me in 2008 that protein structure prediction by superintelligence was the *hardest* or *least likely* step of the pathway by which a superintelligence ends up with nanotech; and in fact I argued only that it'd be solvable for *chosen special cases of proteins* rather than *biological proteins* because the special-case proteins could be chosen to have especially predictable pathways.  All those wobbles, all those balanced weak forces and local strange gradients along potential energy surfaces!  All those nonequilibrium intermediate states, potentially with fragile counterfactual dependencies on each interim stage of the solution!  If you were gonna be a superintelligence skeptic, you might have claimed that even chosen special cases of protein folding would be unsolvable.  The kind of argument you are making now, if you thought this style of thought was a good idea, would have led you to proclaim that probably a superintelligence could not solve biological protein folding and that AlphaFold 2 was surely an impossibility and sheer wishful thinking.\n\nIf you'd been around then, and said, \"Pre-AGI ML systems will be able to solve general biological proteins via a kind of brute statistical force on deep patterns in an existing database of biological proteins, but even superintelligences will not be able to choose special cases of such protein folding pathways to design de novo synthesis pathways for nanotechnological machinery\", it would have been a very strange prediction, but you would now have a leg to stand on.  But this, I most incredibly doubt you would have said - the style of thinking you're using would have predicted much more strongly, in 2008 when no such thing had been yet observed, that pre-AGI ML could not solve biological protein folding in general, than that superintelligences could not choose a few special-case solvable de novo folding pathways along sharper potential energy gradients and with intermediate states chosen to be especially convergent and predictable.",
      "plaintextDescription": "Lacking time right now for a long reply:  The main thrust of my reaction is that this seems like a style of thought which would have concluded in 2008 that it's incredibly unlikely for superintelligences to be able to solve the protein folding problem.  People did, in fact, claim that to me in 2008.  It furthermore seemed to me in 2008 that protein structure prediction by superintelligence was the hardest or least likely step of the pathway by which a superintelligence ends up with nanotech; and in fact I argued only that it'd be solvable for chosen special cases of proteins rather than biological proteins because the special-case proteins could be chosen to have especially predictable pathways.  All those wobbles, all those balanced weak forces and local strange gradients along potential energy surfaces!  All those nonequilibrium intermediate states, potentially with fragile counterfactual dependencies on each interim stage of the solution!  If you were gonna be a superintelligence skeptic, you might have claimed that even chosen special cases of protein folding would be unsolvable.  The kind of argument you are making now, if you thought this style of thought was a good idea, would have led you to proclaim that probably a superintelligence could not solve biological protein folding and that AlphaFold 2 was surely an impossibility and sheer wishful thinking.\n\nIf you'd been around then, and said, \"Pre-AGI ML systems will be able to solve general biological proteins via a kind of brute statistical force on deep patterns in an existing database of biological proteins, but even superintelligences will not be able to choose special cases of such protein folding pathways to design de novo synthesis pathways for nanotechnological machinery\", it would have been a very strange prediction, but you would now have a leg to stand on.  But this, I most incredibly doubt you would have said - the style of thinking you're using would have predicted much more strongly, in 2008 when "
    },
    "baseScore": 27,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2023-06-02T03:47:14.957Z",
    "af": true
  },
  {
    "_id": "b7mypQhZHhj4F3xLk",
    "postId": "uXn3LyA8eNqpvdoZw",
    "parentCommentId": "mRimxhdTbTcGnE3rk",
    "topLevelCommentId": "mRimxhdTbTcGnE3rk",
    "contents": {
      "markdown": "Nope.",
      "plaintextDescription": "Nope."
    },
    "baseScore": 23,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2023-05-30T18:51:45.284Z",
    "af": false
  },
  {
    "_id": "okcgSbLhYnGnz95tf",
    "postId": "psZhJLykfsqkoFx8t",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Well, one sink to avoid here is neutral-genie stories where the AI does what you asked, but not what you wanted.  That's something I wrote about myself, yes, but that was in the era before deep learning took over everything, when it seemed like there was a possibility that humans would be in control of the AI's preferences.  Now neutral-genie stories are a mindsink for a class of scenarios where we have no way to achieve entrance into those scenarios; we cannot make superintelligences want particular things or give them particular orders - cannot give them preferences in a way that generalizes to when they become smarter.",
      "plaintextDescription": "Well, one sink to avoid here is neutral-genie stories where the AI does what you asked, but not what you wanted.  That's something I wrote about myself, yes, but that was in the era before deep learning took over everything, when it seemed like there was a possibility that humans would be in control of the AI's preferences.  Now neutral-genie stories are a mindsink for a class of scenarios where we have no way to achieve entrance into those scenarios; we cannot make superintelligences want particular things or give them particular orders - cannot give them preferences in a way that generalizes to when they become smarter."
    },
    "baseScore": 6,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2023-05-21T22:14:44.614Z",
    "af": false
  },
  {
    "_id": "DMRtuEZMzxqXigvms",
    "postId": "Xo4cqLrAKjdNzpRSa",
    "parentCommentId": "5zTvAdPTwaeu2k9rd",
    "topLevelCommentId": "Nh4bejgCA9gvBxEXS",
    "contents": {
      "markdown": "Okay, if you're not saying GPUs are getting around as efficient as the human brain, without much more efficiency to be eeked out, then I straightforwardly misunderstood that part.",
      "plaintextDescription": "Okay, if you're not saying GPUs are getting around as efficient as the human brain, without much more efficiency to be eeked out, then I straightforwardly misunderstood that part."
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2023-05-07T19:53:21.170Z",
    "af": false
  },
  {
    "_id": "4NGnWQrAAjXvCRevt",
    "postId": "Xo4cqLrAKjdNzpRSa",
    "parentCommentId": "RFatyfFM4iYeneHWq",
    "topLevelCommentId": "Nh4bejgCA9gvBxEXS",
    "contents": {
      "markdown": "Nothing about any of those claims explains why the 10,000-fold redundancy of neurotransmitter molecules and ions being pumped in and out of the system is necessary for doing the alleged complicated stuff.",
      "plaintextDescription": "Nothing about any of those claims explains why the 10,000-fold redundancy of neurotransmitter molecules and ions being pumped in and out of the system is necessary for doing the alleged complicated stuff."
    },
    "baseScore": 13,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2023-05-07T01:42:04.210Z",
    "af": false
  },
  {
    "_id": "WfyE7HiCB6u3sjRRq",
    "postId": "Xo4cqLrAKjdNzpRSa",
    "parentCommentId": "Nh4bejgCA9gvBxEXS",
    "topLevelCommentId": "Nh4bejgCA9gvBxEXS",
    "contents": {
      "markdown": "Further item of \"these elaborate calculations seem to arrive at conclusions that can't possibly be true\" - besides the brain allegedly being close to the border of thermodynamic efficiency, despite visibly using tens of thousands of redundant physical ops in terms of sheer number of ions and neurotransmitters pumped; the same calculations claim that modern GPUs are approaching brain efficiency, the Limit of the Possible, so presumably at the Limit of the Possible themselves.\n\nThis source claims 100x energy efficiency from substituting some basic physical analog operations for multiply-accumulate, instead of digital transistor operations about them, even if you otherwise use actual real-world physical hardware.  Sounds right to me; it would make no sense for such a vastly redundant digital computation of such a simple physical quantity to be anywhere near the borders of efficiency!  https://spectrum.ieee.org/analog-ai",
      "plaintextDescription": "Further item of \"these elaborate calculations seem to arrive at conclusions that can't possibly be true\" - besides the brain allegedly being close to the border of thermodynamic efficiency, despite visibly using tens of thousands of redundant physical ops in terms of sheer number of ions and neurotransmitters pumped; the same calculations claim that modern GPUs are approaching brain efficiency, the Limit of the Possible, so presumably at the Limit of the Possible themselves.\n\nThis source claims 100x energy efficiency from substituting some basic physical analog operations for multiply-accumulate, instead of digital transistor operations about them, even if you otherwise use actual real-world physical hardware.  Sounds right to me; it would make no sense for such a vastly redundant digital computation of such a simple physical quantity to be anywhere near the borders of efficiency!  https://spectrum.ieee.org/analog-ai"
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2023-05-07T01:40:28.617Z",
    "af": false
  },
  {
    "_id": "f75dRPrG7sTx8EebY",
    "postId": "Xo4cqLrAKjdNzpRSa",
    "parentCommentId": "Q9qh6JJwRbG24TrNb",
    "topLevelCommentId": "Nh4bejgCA9gvBxEXS",
    "contents": {
      "markdown": "This does not explain how thousands of neurotransmitter molecules impinging on a neuron and thousands of ions flooding into and out of cell membranes, all irreversible operations, in order to transmit one spike, could possibly be within one OOM of the thermodynamic limit on efficiency for a cognitive system (running at that temperature).",
      "plaintextDescription": "This does not explain how thousands of neurotransmitter molecules impinging on a neuron and thousands of ions flooding into and out of cell membranes, all irreversible operations, in order to transmit one spike, could possibly be within one OOM of the thermodynamic limit on efficiency for a cognitive system (running at that temperature)."
    },
    "baseScore": 25,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2023-04-29T01:25:01.821Z",
    "af": false
  },
  {
    "_id": "gzg6gcAs4gRitn8qe",
    "postId": "Xo4cqLrAKjdNzpRSa",
    "parentCommentId": "bv2H2MLjhpQbwTqQZ",
    "topLevelCommentId": "Nh4bejgCA9gvBxEXS",
    "contents": {
      "markdown": "And it says:\n\n> So true 8-bit equivalent analog multiplication requires about 100k carriers/switches\n\nThis just seems utterly wack.  Having any physical equivalent of an analog multiplication fundamentally requires 100,000 times the thermodynamic energy to erase 1 bit?  And \"analog multiplication down to two decimal places\" is the operation that is purportedly being carried out *almost as efficiently as physically possible* by... an axon terminal with a handful of synaptic vesicles dumping 10,000 neurotransmitter molecules to flood around a dendritic terminal (molecules which will later need to be irreversibly pumped back out), which in turn depolarizes and starts flooding thousands of ions into a cell membrane (to be later pumped out) in order to transmit the impulse at 1m/s?  That's *the most thermodynamically efficient a physical cognitive system can possibly be?  *This is approximately *the most efficient possible way to turn all those bit erasures into thought?*\n\nThis sounds like physical nonsense that fails a basic sanity check.  What am I missing?",
      "plaintextDescription": "And it says:\n\n> So true 8-bit equivalent analog multiplication requires about 100k carriers/switches\n\nThis just seems utterly wack.  Having any physical equivalent of an analog multiplication fundamentally requires 100,000 times the thermodynamic energy to erase 1 bit?  And \"analog multiplication down to two decimal places\" is the operation that is purportedly being carried out almost as efficiently as physically possible by... an axon terminal with a handful of synaptic vesicles dumping 10,000 neurotransmitter molecules to flood around a dendritic terminal (molecules which will later need to be irreversibly pumped back out), which in turn depolarizes and starts flooding thousands of ions into a cell membrane (to be later pumped out) in order to transmit the impulse at 1m/s?  That's the most thermodynamically efficient a physical cognitive system can possibly be?  This is approximately the most efficient possible way to turn all those bit erasures into thought?\n\nThis sounds like physical nonsense that fails a basic sanity check.  What am I missing?"
    },
    "baseScore": 24,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2023-04-29T01:18:04.397Z",
    "af": false
  },
  {
    "_id": "Nh4bejgCA9gvBxEXS",
    "postId": "Xo4cqLrAKjdNzpRSa",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'm confused at how somebody ends up calculating that a brain - where each synaptic spike is transmitted by ~10,000 neurotransmitter molecules (according to a quick online check), which then get pumped back out of the membrane and taken back up by the synapse; and the impulse is then shepherded along cellular channels via thousands of ions flooding through a membrane to depolarize it and then getting pumped back out using ATP, all of which are thermodynamically irreversible operations *individually* \\- could possibly be within three orders of magnitude of max thermodynamic efficiency at 300 Kelvin.  I have skimmed \"Brain Efficiency\" though not checked any numbers, and not seen anything inside it which seems to address this sanity check.",
      "plaintextDescription": "I'm confused at how somebody ends up calculating that a brain - where each synaptic spike is transmitted by ~10,000 neurotransmitter molecules (according to a quick online check), which then get pumped back out of the membrane and taken back up by the synapse; and the impulse is then shepherded along cellular channels via thousands of ions flooding through a membrane to depolarize it and then getting pumped back out using ATP, all of which are thermodynamically irreversible operations individually - could possibly be within three orders of magnitude of max thermodynamic efficiency at 300 Kelvin.  I have skimmed \"Brain Efficiency\" though not checked any numbers, and not seen anything inside it which seems to address this sanity check."
    },
    "baseScore": 38,
    "voteCount": 24,
    "createdAt": null,
    "postedAt": "2023-04-27T22:34:29.861Z",
    "af": false
  },
  {
    "_id": "sAEZhfG4cKBc9oudr",
    "postId": "eo8odvou4efc9syrv",
    "parentCommentId": "irZHYG48rbrTKhNZh",
    "topLevelCommentId": "yLMGSQESuQmTBpdm6",
    "contents": {
      "markdown": "Nobody in the US cared either, three years earlier.  That superintelligence will kill everyone on Earth is a truth, and once which has gotten easier and easier to figure out over the years.  I have not entirely written off the chance that, especially as the evidence gets more obvious, people on Earth will figure out this true fact and maybe even do something about it and survive.  I likewise am not assuming that China is incapable of ever figuring out this thing that is true.  If your opinion of Chinese intelligence is lower than mine, you are welcome to say, \"Even if this is true and the West figures out that it is true, the CCP could never come to understand it\".  That could even be true, for all I know, but I do not have present cause to believe it.  I definitely don't believe it about everyone in China; if it were true and a lot of people in the West figured it out, I'd expect a lot of individual people in China to see it too.",
      "plaintextDescription": "Nobody in the US cared either, three years earlier.  That superintelligence will kill everyone on Earth is a truth, and once which has gotten easier and easier to figure out over the years.  I have not entirely written off the chance that, especially as the evidence gets more obvious, people on Earth will figure out this true fact and maybe even do something about it and survive.  I likewise am not assuming that China is incapable of ever figuring out this thing that is true.  If your opinion of Chinese intelligence is lower than mine, you are welcome to say, \"Even if this is true and the West figures out that it is true, the CCP could never come to understand it\".  That could even be true, for all I know, but I do not have present cause to believe it.  I definitely don't believe it about everyone in China; if it were true and a lot of people in the West figured it out, I'd expect a lot of individual people in China to see it too."
    },
    "baseScore": 21,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2023-04-09T19:00:23.331Z",
    "af": false
  },
  {
    "_id": "RzFDCvcB3wrgcSh3h",
    "postId": "nH4c3Q9t9F3nJ7y8W",
    "parentCommentId": "St8zjZ2xatojvnFxW",
    "topLevelCommentId": "St8zjZ2xatojvnFxW",
    "contents": {
      "markdown": "> From a high-level perspective, it is clear that this is just wrong. Part of what human brains are doing is to [minimise prediction error with regard to sensory inputs](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/). \n\nI didn't say that GPT's task is harder than any possible perspective on a form of work you could regard a human brain as trying to do; I said that GPT's task is harder than being an actual human; in other words, being an actual human is not enough to solve GPT's task.",
      "plaintextDescription": "> From a high-level perspective, it is clear that this is just wrong. Part of what human brains are doing is to minimise prediction error with regard to sensory inputs. \n\nI didn't say that GPT's task is harder than any possible perspective on a form of work you could regard a human brain as trying to do; I said that GPT's task is harder than being an actual human; in other words, being an actual human is not enough to solve GPT's task."
    },
    "baseScore": 9,
    "voteCount": 36,
    "createdAt": null,
    "postedAt": "2023-04-09T18:56:04.215Z",
    "af": true
  },
  {
    "_id": "5mkZBfYwBZTaPNvhW",
    "postId": "eo8odvou4efc9syrv",
    "parentCommentId": "yLMGSQESuQmTBpdm6",
    "topLevelCommentId": "yLMGSQESuQmTBpdm6",
    "contents": {
      "markdown": "If diplomacy failed, but yes, sure.  I've previously wished out loud for China to sabotage US AI projects in retaliation for chip export controls, in the hopes that if all the countries sabotage all the other countries' AI projects, maybe Earth as a whole can \"uncoordinate\" to not build AI even if Earth can't coordinate.",
      "plaintextDescription": "If diplomacy failed, but yes, sure.  I've previously wished out loud for China to sabotage US AI projects in retaliation for chip export controls, in the hopes that if all the countries sabotage all the other countries' AI projects, maybe Earth as a whole can \"uncoordinate\" to not build AI even if Earth can't coordinate."
    },
    "baseScore": 19,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2023-04-09T04:43:52.055Z",
    "af": false
  },
  {
    "_id": "hKxfRxqy9sewbubJG",
    "postId": "uNepkB5EqETC8b9C2",
    "parentCommentId": "cQvqac5uwBFWAiiM6",
    "topLevelCommentId": "cQvqac5uwBFWAiiM6",
    "contents": {
      "markdown": "Arbitrary and personal.  Given how bad things presently look, over 20% is about the level where I'm like \"Yeah okay I will grab for that\" and much under 20% is where I'm like \"Not okay keep looking.\"",
      "plaintextDescription": "Arbitrary and personal.  Given how bad things presently look, over 20% is about the level where I'm like \"Yeah okay I will grab for that\" and much under 20% is where I'm like \"Not okay keep looking.\""
    },
    "baseScore": 4,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2023-03-29T20:29:02.878Z",
    "af": false
  },
  {
    "_id": "Bjh98r2Ny3zGDwh69",
    "postId": "wAczufCpMdaamF9fy",
    "parentCommentId": "Gf3paH5qCenMKxDge",
    "topLevelCommentId": "YYR4hEFRmA7cb5csy",
    "contents": {
      "markdown": "Choosing to engage with an unscripted unrehearsed off-the-cuff podcast intended to introduce ideas to a lay audience, continues to be a surprising concept to me.  To grapple with the intellectual content of my ideas, consider picking *one* item from \"A List of Lethalities\" and engaging with that.",
      "plaintextDescription": "Choosing to engage with an unscripted unrehearsed off-the-cuff podcast intended to introduce ideas to a lay audience, continues to be a surprising concept to me.  To grapple with the intellectual content of my ideas, consider picking one item from \"A List of Lethalities\" and engaging with that."
    },
    "baseScore": 16,
    "voteCount": 39,
    "createdAt": null,
    "postedAt": "2023-03-21T22:46:44.257Z",
    "af": true
  },
  {
    "_id": "cr54ivfjndn6dxraD",
    "postId": "wAczufCpMdaamF9fy",
    "parentCommentId": "Cqf5rMEvNgSATBHdr",
    "topLevelCommentId": "YYR4hEFRmA7cb5csy",
    "contents": {
      "markdown": "> The \"strongest\" foot I could put forwards is my response to \"On current AI not being self-improving:\", where I'm pretty sure you're just wrong.\n\nYou straightforwardly completely misunderstood what I was trying to say on the Bankless podcast:  I was saying that GPT-4 does not get smarter each time an instance of it is run in inference mode.\n\nAnd that's that, I guess.",
      "plaintextDescription": "> The \"strongest\" foot I could put forwards is my response to \"On current AI not being self-improving:\", where I'm pretty sure you're just wrong.\n\nYou straightforwardly completely misunderstood what I was trying to say on the Bankless podcast:  I was saying that GPT-4 does not get smarter each time an instance of it is run in inference mode.\n\nAnd that's that, I guess."
    },
    "baseScore": 21,
    "voteCount": 44,
    "createdAt": null,
    "postedAt": "2023-03-21T09:29:18.230Z",
    "af": false
  },
  {
    "_id": "YYR4hEFRmA7cb5csy",
    "postId": "wAczufCpMdaamF9fy",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This is kinda long.  If I had time to engage with *one* part of this as a sample of whether it holds up to a counterresponse, what would be the strongest foot you could put forward?\n\n(I also echo the commenter who's confused about why you'd reply to the obviously simplified presentation from an off-the-cuff podcast rather than the more detailed arguments elsewhere.)",
      "plaintextDescription": "This is kinda long.  If I had time to engage with one part of this as a sample of whether it holds up to a counterresponse, what would be the strongest foot you could put forward?\n\n(I also echo the commenter who's confused about why you'd reply to the obviously simplified presentation from an off-the-cuff podcast rather than the more detailed arguments elsewhere.)"
    },
    "baseScore": 1,
    "voteCount": 116,
    "createdAt": null,
    "postedAt": "2023-03-21T08:27:52.590Z",
    "af": true
  },
  {
    "_id": "v4mAjAKvjvk7J9fii",
    "postId": "yCuzmCsE86BTu9PfA",
    "parentCommentId": "ioBBTL9rhasAapoAD",
    "topLevelCommentId": "4HbK8ZPqDSaun9EXh",
    "contents": {
      "markdown": "Things are dominated when they forego free money and not just when money gets pumped out of them.",
      "plaintextDescription": "Things are dominated when they forego free money and not just when money gets pumped out of them."
    },
    "baseScore": 5,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2023-03-13T02:46:56.661Z",
    "af": false
  },
  {
    "_id": "QtTywnqXj8h7jzskR",
    "postId": "yCuzmCsE86BTu9PfA",
    "parentCommentId": "i9WCADezrSLC829Az",
    "topLevelCommentId": "4HbK8ZPqDSaun9EXh",
    "contents": {
      "markdown": "Suppose I describe your attempt to refute the existence of any coherence theorems:  You point to a rock, and say that although it's not coherent, it also can't be dominated, because it has no preferences.  Is there any sense in which you think you've disproved the existence of coherence theorems, which doesn't consist of pointing to rocks, and various things that are intermediate between agents and rocks in the sense that they lack preferences about various things where you then refuse to say that they're being dominated?",
      "plaintextDescription": "Suppose I describe your attempt to refute the existence of any coherence theorems:  You point to a rock, and say that although it's not coherent, it also can't be dominated, because it has no preferences.  Is there any sense in which you think you've disproved the existence of coherence theorems, which doesn't consist of pointing to rocks, and various things that are intermediate between agents and rocks in the sense that they lack preferences about various things where you then refuse to say that they're being dominated?"
    },
    "baseScore": 6,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2023-03-08T07:23:40.082Z",
    "af": false
  },
  {
    "_id": "sf3awnhudEwKKMa5X",
    "postId": "yCuzmCsE86BTu9PfA",
    "parentCommentId": "SDSRGwMrCPDcEcEsw",
    "topLevelCommentId": "4HbK8ZPqDSaun9EXh",
    "contents": {
      "markdown": "I want you to give me an example of something the agent actually does, under a couple of different sense inputs, given what you say are its preferences, and then I want you to gesture at that and say, \"Lo, see how it is incoherent yet not dominated!\"",
      "plaintextDescription": "I want you to give me an example of something the agent actually does, under a couple of different sense inputs, given what you say are its preferences, and then I want you to gesture at that and say, \"Lo, see how it is incoherent yet not dominated!\""
    },
    "baseScore": 0,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2023-03-07T00:43:18.916Z",
    "af": false
  },
  {
    "_id": "CjK5SSxcYcapbaH5z",
    "postId": "PE22QJSww8mpwh7bt",
    "parentCommentId": "HhyoD2Ctr648BhDfz",
    "topLevelCommentId": "EgCWm66pxrgcciNW2",
    "contents": {
      "markdown": "If you think you've got a great capabilities insight, I think you PM me or somebody else you trust and ask if they think it's a big capabilities insight.",
      "plaintextDescription": "If you think you've got a great capabilities insight, I think you PM me or somebody else you trust and ask if they think it's a big capabilities insight."
    },
    "baseScore": 21,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2023-03-07T00:41:55.646Z",
    "af": false
  },
  {
    "_id": "EgvFiuA7uLqnYfDJj",
    "postId": "yCuzmCsE86BTu9PfA",
    "parentCommentId": "apoP8dyBqRBB2sND6",
    "topLevelCommentId": "4HbK8ZPqDSaun9EXh",
    "contents": {
      "markdown": "In the limit, you take a rock, and say, \"See, the complete class theorem doesn't apply to it, because it doesn't have any preferences ordered about anything!\"  What about your argument is any different from this - where is there a powerful, future-steering thing that isn't viewable as Bayesian and also isn't dominated?  Spell it out more concretely:  It has preferences ABC, two things aren't ordered, it chooses X and then Y, etc.  I can give concrete examples for my views; what *exactly* is a case in point of anything you're claiming about the Complete Class Theorem's supposed nonapplicability and hence nonexistence of any coherence theorems?",
      "plaintextDescription": "In the limit, you take a rock, and say, \"See, the complete class theorem doesn't apply to it, because it doesn't have any preferences ordered about anything!\"  What about your argument is any different from this - where is there a powerful, future-steering thing that isn't viewable as Bayesian and also isn't dominated?  Spell it out more concretely:  It has preferences ABC, two things aren't ordered, it chooses X and then Y, etc.  I can give concrete examples for my views; what exactly is a case in point of anything you're claiming about the Complete Class Theorem's supposed nonapplicability and hence nonexistence of any coherence theorems?"
    },
    "baseScore": 12,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2023-03-01T07:21:43.501Z",
    "af": false
  },
  {
    "_id": "eP2kDMgPs5XyDBz2H",
    "postId": "yCuzmCsE86BTu9PfA",
    "parentCommentId": "7bEtCKnubf9mBWLyA",
    "topLevelCommentId": "4HbK8ZPqDSaun9EXh",
    "contents": {
      "markdown": "And this avoids the Complete Class Theorem conclusion of dominated strategies, how?  Spell it out with a concrete example, maybe?  Again, we care about domination, not representability at all.",
      "plaintextDescription": "And this avoids the Complete Class Theorem conclusion of dominated strategies, how? Spell it out with a concrete example, maybe? Again, we care about domination, not representability at all."
    },
    "baseScore": 10,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2023-02-28T08:26:49.058Z",
    "af": false
  },
  {
    "_id": "nduJjhxsFesZ63qEx",
    "postId": "yCuzmCsE86BTu9PfA",
    "parentCommentId": "LSkmfthaivgsQtqmq",
    "topLevelCommentId": "4HbK8ZPqDSaun9EXh",
    "contents": {
      "markdown": "Say more about behaviors associated with \"incomparability\"?",
      "plaintextDescription": "Say more about behaviors associated with \"incomparability\"?"
    },
    "baseScore": 4,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2023-02-28T02:33:04.133Z",
    "af": false
  },
  {
    "_id": "4HbK8ZPqDSaun9EXh",
    "postId": "yCuzmCsE86BTu9PfA",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "The author doesn't seem to realize that there's a difference between representation theorems and coherence theorems.\n\n> The Complete Class Theorem says that an agent’s policy of choosing actions conditional on observations is not strictly dominated by some other policy (such that the other policy does better in some set of circumstances and worse in no set of circumstances) if and only if the agent’s policy maximizes expected utility with respect to a probability distribution that assigns positive probability to each possible set of circumstances.\n> \n> This theorem does refer to dominated strategies. However, the Complete Class Theorem starts off by assuming that the agent’s preferences over actions in sets of circumstances satisfy Completeness and Transitivity. If the agent’s preferences are not complete and transitive, the Complete Class Theorem does not apply. So, the Complete Class Theorem does not imply that agents must be representable as maximizing expected utility if they are to avoid pursuing dominated strategies.\n\nCool, I'll complete it for you then.\n\nTransitivity:  Suppose you prefer A to B, B to C, and C to A.  I'll keep having you pay a penny to trade between them in a cycle.  You start with C, end with C, and are three pennies poorer.  You'd be richer if you didn't do that.\n\nCompleteness:  Any time you have no comparability between two goods, I'll swap them in whatever direction is most useful for completing money-pump cycles.  Since you've got no preference one way or the other, I don't expect you'll be objecting, right?\n\nCombined with the standard Complete Class Theorem, this now produces the existence of at least one coherence theorem.  The post's thesis, \"There are no coherence theorems\", is therefore falsified by presentation of a counterexample.  Have a nice day!",
      "plaintextDescription": "The author doesn't seem to realize that there's a difference between representation theorems and coherence theorems.\n\n> The Complete Class Theorem says that an agent’s policy of choosing actions conditional on observations is not strictly dominated by some other policy (such that the other policy does better in some set of circumstances and worse in no set of circumstances) if and only if the agent’s policy maximizes expected utility with respect to a probability distribution that assigns positive probability to each possible set of circumstances.\n> \n> This theorem does refer to dominated strategies. However, the Complete Class Theorem starts off by assuming that the agent’s preferences over actions in sets of circumstances satisfy Completeness and Transitivity. If the agent’s preferences are not complete and transitive, the Complete Class Theorem does not apply. So, the Complete Class Theorem does not imply that agents must be representable as maximizing expected utility if they are to avoid pursuing dominated strategies.\n\nCool, I'll complete it for you then.\n\nTransitivity:  Suppose you prefer A to B, B to C, and C to A.  I'll keep having you pay a penny to trade between them in a cycle.  You start with C, end with C, and are three pennies poorer.  You'd be richer if you didn't do that.\n\nCompleteness:  Any time you have no comparability between two goods, I'll swap them in whatever direction is most useful for completing money-pump cycles.  Since you've got no preference one way or the other, I don't expect you'll be objecting, right?\n\nCombined with the standard Complete Class Theorem, this now produces the existence of at least one coherence theorem.  The post's thesis, \"There are no coherence theorems\", is therefore falsified by presentation of a counterexample.  Have a nice day!"
    },
    "baseScore": 8,
    "voteCount": 25,
    "createdAt": null,
    "postedAt": "2023-02-28T01:31:39.709Z",
    "af": true
  }
]