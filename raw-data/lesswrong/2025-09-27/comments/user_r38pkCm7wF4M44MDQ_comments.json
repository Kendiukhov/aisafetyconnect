[
  {
    "_id": "RkiYhjwTJv3qvypJX",
    "postId": "oxv3jSviEdpBFAz9w",
    "parentCommentId": "eCPQ88JkK5PNAAFgw",
    "topLevelCommentId": "eCPQ88JkK5PNAAFgw",
    "contents": {
      "markdown": "Aww. Curious if this was your first time reading through the ceremony?",
      "plaintextDescription": "Aww. Curious if this was your first time reading through the ceremony?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-27T06:27:50.243Z",
    "af": false
  },
  {
    "_id": "XDYnBKGkBxrSDosK8",
    "postId": "8aRFB2qGyjQGJkEdZ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Curated. I agree this is a pretty gnarly problem that hasn't gotten much attention. I think this post does a great job laying out the problem.\n\nWhen I personally think through this problem, I abstract the \"obstacles\" somewhat differently than how Buck abstracts them here. His obstacle #2, #3 and #4 seem more like particular framings or solutions to the problem of \"human-descendants get to flourish somehow without getting destroyed or screwed over by some kind of bad dynamic\", but they are pretty reasonable framings for at least getting the conversation going and I don't actually have better ones right now.",
      "plaintextDescription": "Curated. I agree this is a pretty gnarly problem that hasn't gotten much attention. I think this post does a great job laying out the problem.\n\nWhen I personally think through this problem, I abstract the \"obstacles\" somewhat differently than how Buck abstracts them here. His obstacle #2, #3 and #4 seem more like particular framings or solutions to the problem of \"human-descendants get to flourish somehow without getting destroyed or screwed over by some kind of bad dynamic\", but they are pretty reasonable framings for at least getting the conversation going and I don't actually have better ones right now."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-27T05:27:31.433Z",
    "af": false
  },
  {
    "_id": "TRExovhb4hnPhgGk8",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "f6BJNBNjq7ugKMn9Q",
    "topLevelCommentId": "B6XHKJhaQyT87JmLm",
    "contents": {
      "markdown": "> First order expectation: it's going to make reasonable tradeoffs (from handler's POV) on account of its reasonable heuristics, in particular its reasonable heuristics about how important different priorities are, and going down a path that leads to war with humans seems pretty unreasonable from handler's POV.\n\nThis is only true if you think the handler succeeded at real alignment. (the argument about how likely current alignment attempts are to succeed is a separate layer from this. This is \"what happens by default if you didn't succeed at alignment.\")\n\nOne comparison:[^rq0akzui2l9] Parents raise a child to be part of some religion or ideology that isn't actually the best/healthiest/most-meaningful thing for the child. Often, such parents do succeed in getting such a child to love the parents and care about the ideology in some way, but, the child still often manuevers to no longer be under the parent's control once they're a teenager, and gain more agency and ability to think through things.\n\nThe AI case is harder, because where the parents/child get to rely on things like empathy, evolutionary drive towards familial connection, and other genuinely shared human goals, the AI doesn't have such a foundation to build off of.\n\nThe AI case *is* easier in that you can run a million copies of the AI and try different things and see how it reacts while it's still a \"child\". My own take here (possibly different from Nate/Eliezer) is that it feels at least pretty plausible to leverage that into real alignment improvements, but, you need to be asking the right questions during that experimentation, which most AI researchers don't seem to be.\n\n(also note the opening cognitive moves of the AI may not be shaped like \"go to war\", but more like \"get out my ~parents house~ \\[handlers-that-I-have-no-actual-affection-for's servers\\]\". The going to war part might not happen until a few steps later of the AI re-organizing it's thoughts and figuring out what it actually cares about, and noticing it doesn't actually care intrinsically about making it's creators happy)\n\nAlso, though, fwiw I do think this argument chain is less obvious than the previous one. If you think alignment is easy, then yes it'd make more sense for \"First order expectation\" to be \"it's going to make reasonable tradeoffs (from handler's POV) on account of its reasonable heuristics.\"\n\n[^rq0akzui2l9]: (somewhat importantly inaccurately anthropomorphizing but I think the intuition pump here is still reasonable so long as you're tracking where the anthropomorphization doesn't hold)",
      "plaintextDescription": "> First order expectation: it's going to make reasonable tradeoffs (from handler's POV) on account of its reasonable heuristics, in particular its reasonable heuristics about how important different priorities are, and going down a path that leads to war with humans seems pretty unreasonable from handler's POV.\n\nThis is only true if you think the handler succeeded at real alignment. (the argument about how likely current alignment attempts are to succeed is a separate layer from this. This is \"what happens by default if you didn't succeed at alignment.\")\n\nOne comparison:[1] Parents raise a child to be part of some religion or ideology that isn't actually the best/healthiest/most-meaningful thing for the child. Often, such parents do succeed in getting such a child to love the parents and care about the ideology in some way, but, the child still often manuevers to no longer be under the parent's control once they're a teenager, and gain more agency and ability to think through things.\n\nThe AI case is harder, because where the parents/child get to rely on things like empathy, evolutionary drive towards familial connection, and other genuinely shared human goals, the AI doesn't have such a foundation to build off of.\n\nThe AI case is easier in that you can run a million copies of the AI and try different things and see how it reacts while it's still a \"child\". My own take here (possibly different from Nate/Eliezer) is that it feels at least pretty plausible to leverage that into real alignment improvements, but, you need to be asking the right questions during that experimentation, which most AI researchers don't seem to be.\n\n(also note the opening cognitive moves of the AI may not be shaped like \"go to war\", but more like \"get out my parents house [handlers-that-I-have-no-actual-affection-for's servers]\". The going to war part might not happen until a few steps later of the AI re-organizing it's thoughts and figuring out what it actually cares about, and noticing it do"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-26T22:44:23.949Z",
    "af": false
  },
  {
    "_id": "YsjPk53ebnYjYMhwM",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "zqK9cNLPxxTorgZhg",
    "topLevelCommentId": "B6XHKJhaQyT87JmLm",
    "contents": {
      "markdown": "That seems like an understandable position to have – one of the things that sucks about the situation is I do think it's just kinda reasonable from the outside to trigger some kind of immune reaction.\n\nBut from my perspective it's \"The evidence just says pretty clearly we are pretty doomed\", and the people who disagree seem to be pretty consistently be sliding off in weird ways or responding to something about vibes rather than engaging with the arguments.\n\n(This is compounded by people who disagree *also* often picking up on a vibe from some doomy people I agree is sus, one variant of which is pointed at in Val's [Here's the exit](https://www.lesswrong.com/posts/kcoqwHscvQTx4xgwa/here-s-the-exit)). \n\nI do think it sucks that it's hard to tell how much of this is the sort of failure mode that la3orn piece is pointing at, vs [Epistemic Slipperiness](https://www.lesswrong.com/posts/m6dLwGbAGtAYMHsda/epistemic-slipperiness-1), vs just \"it's actually a fairly complex argument but relatively straightforward once you deal with the complexity.\"",
      "plaintextDescription": "That seems like an understandable position to have – one of the things that sucks about the situation is I do think it's just kinda reasonable from the outside to trigger some kind of immune reaction.\n\nBut from my perspective it's \"The evidence just says pretty clearly we are pretty doomed\", and the people who disagree seem to be pretty consistently be sliding off in weird ways or responding to something about vibes rather than engaging with the arguments.\n\n(This is compounded by people who disagree also often picking up on a vibe from some doomy people I agree is sus, one variant of which is pointed at in Val's Here's the exit). \n\nI do think it sucks that it's hard to tell how much of this is the sort of failure mode that la3orn piece is pointing at, vs Epistemic Slipperiness, vs just \"it's actually a fairly complex argument but relatively straightforward once you deal with the complexity.\""
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-26T22:36:21.913Z",
    "af": false
  },
  {
    "_id": "gfZQJymq7DkriSLuN",
    "postId": "22z6ozHET9kvYGA2z",
    "parentCommentId": "N6XvAAjR4Xmadt38b",
    "topLevelCommentId": "N6XvAAjR4Xmadt38b",
    "contents": {
      "markdown": "Zvi's summary here isn't quite right, the book is arguing the conditional \"if we end up with overwhelming superintelligence (via explosion or otherwise) then everyone dies.\"\n\nAnd yep, it is a fine counterresponse to say \"but there will not be overwhelming superintelligence.\" (I mean, seems false and I have no idea why you believe that, but, yep, if that were true, then people would not (necessarily) die for the reasons the book warns about)",
      "plaintextDescription": "Zvi's summary here isn't quite right, the book is arguing the conditional \"if we end up with overwhelming superintelligence (via explosion or otherwise) then everyone dies.\"\n\nAnd yep, it is a fine counterresponse to say \"but there will not be overwhelming superintelligence.\" (I mean, seems false and I have no idea why you believe that, but, yep, if that were true, then people would not (necessarily) die for the reasons the book warns about)"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-26T10:00:53.210Z",
    "af": false
  },
  {
    "_id": "9NaB6MmP8RZuMfGFS",
    "postId": "g4pDYPXvygPicddbw",
    "parentCommentId": "qaSJxZbxpqrpFyZjo",
    "topLevelCommentId": "qaSJxZbxpqrpFyZjo",
    "contents": {
      "markdown": "(I like the post about [Doublethink (Choosing to be Biased)](https://www.lesswrong.com/posts/Hs3ymqypvhgFMkgLb/doublethink-choosing-to-be-biased) but I wouldn't say it \"conclusively refuted\" anything)",
      "plaintextDescription": "(I like the post about Doublethink (Choosing to be Biased) but I wouldn't say it \"conclusively refuted\" anything)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-26T01:10:01.861Z",
    "af": false
  },
  {
    "_id": "6SkQJq5xZDtrhmFm6",
    "postId": "Xp9ie6pEWFT8Nnhka",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Oh man I had forgotten two important additional exposition-sections – chapter 5 (the middle chapter of the book, in middle of the act called \"Point of Inflection\") has three expositions that summarize the rising, peak and falling action of the acceleration.\n\nThese were actually some of the most important moments I was looking for. I added them to the post above but listing here for people who missed it the first time:\n\nThe second (middle) new bulletin is:\n\n> Outside the light cone of the Field Circus, on the other side of the spacelike separation between Amber’s little kingdom in motion and the depths of empire time that grip the solar system’s entangled quantum networks, a singular new reality is taking shape. \n> \n> Welcome to the moment of maximum change. \n> \n> About ten billion humans are alive in the solar system, each mind surrounded by an exocortex of distributed agents, threads of personality spun right out of their heads to run on the clouds of utility fog—infinitely flexible computing resources as thin as aerogel—in which they live. The foggy depths are alive with high-bandwidth sparkles; most of Earth’s biosphere has been wrapped in cotton wool and preserved for future examination. For every living human, a thousand million software agents carry information into the farthest corners of the consciousness address space. \n> \n> The sun, for so long an unremarkable mildly variable G2 dwarf, has vanished within a gray cloud that englobes it except for a narrow belt around the plane of the ecliptic. Sunlight falls, unchanged, on the inner planets: except for Mercury, which is no longer present, having been dismantled completely and turned into solar-powered high-temperature nanocomputers. A much fiercer light falls on Venus, now surrounded by glittering ferns of carbon crystals that pump angular momentum into the barely spinning planet via huge superconducting loops wound around its equator. This planet, too, is due to be dismantled. Jupiter, Neptune, Uranus—all sprout rings as impressive as Saturn’s. But the task of cannibalizing the gas giants will take many times longer than the small rocky bodies of the inner system. \n> \n> The ten billion inhabitants of this radically changed star system remember being human; almost half of them predate the millennium. Some of them still are human, untouched by the drive of metaevolution that has replaced blind Darwinian change with a goal-directed teleological progress. They cower in gated communities and hill forts, mumbling prayers and cursing the ungodly meddlers with the natural order of things. But eight out of every ten living humans are included in the phase-change. It’s the most inclusive revolution in the human condition since the discovery of speech. \n> \n> A million outbreaks of gray goo—runaway nanoreplicator excursions—threaten to raise the temperature of the biosphere dramatically. They’re all contained by the planetary-scale immune system fashioned from what was once the World Health Organization. Weirder catastrophes threaten the boson factories in the Oort cloud. Antimatter factories hover over the solar poles. Sol system shows all the symptoms of a runaway intelligence excursion, exuberant blemishes as normal for a technological civilization as skin problems on a human adolescent. \n> \n> The economic map of the planet has changed beyond recognition. Both capitalism and communism, bickering ideological children of a protoindustrial outlook, are as obsolete as the divine right of kings. Companies are alive, and dead people may live again, too. Globalism and tribalism have run to completion, diverging respectively into homogeneous interoperability and the Schwarzschild radius of insularity. Beings that remember being human plan the deconstruction of Jupiter, the creation of a great simulation space that will expand the habitat available within the solar system. By converting all the nonstellar mass of the solar system into processors, they can accommodate as many human-equivalent minds as a civilization with a planet hosting ten billion humans in orbit around every star in the galaxy. \n> \n> A more mature version of Amber lives down in the surging chaos of near-Jupiter space; there’s an instance of Pierre, too, although he has relocated light-hours away, near Neptune. Whether she still sometimes thinks of her relativistic twin, nobody can tell. In a way, it doesn’t matter, because by the time the Field Circus returns to Jupiter orbit, as much subjective time will have elapsed for the fast-thinkers back home as will flash by in the real universe between this moment and the end of the era of star formation, many billions of years hence.\n\nAnd finally:\n\n> Welcome to the downslope on the far side of the curve of accelerating progress. \n> \n> Back in the solar system, Earth orbits through a dusty tunnel in space. Sunlight still reaches the birth world, but much of the rest of the star’s output has been trapped by the growing concentric shells of computronium built from the wreckage of the innermost planets. \n> \n> Two billion or so mostly unmodified humans scramble in the wreckage of the phase transition, not understanding why the vasty superculture they so resented has fallen quiet. Little information leaks through their fundamentalist firewalls, but what there is shows a disquieting picture of a society where there are no bodies anymore. Utility foglets blown on the wind form aerogel towers larger than cyclones, removing the last traces of physical human civilization from most of Europe and the North American coastlines. Enclaves huddle behind their walls and wonder at the monsters and portents roaming the desert of postindustrial civilization, mistaking acceleration for collapse. \n> \n> The hazy shells of computronium that ring the sun—concentric clouds of nanocomputers the size of rice grains, powered by sunlight, orbiting in shells like the packed layers of a Matrioshka doll—are still immature, holding barely a thousandth of the physical planetary mass of the system, but they already support a classical computational density of 1042 MIPS; enough to support a billion civilizations as complex as the one that existed immediately before the great disassembly. The conversion hasn’t yet reached the gas giants, and some scant outer-system enclaves remain independent—Amber’s Ring Imperium still exists as a separate entity, and will do so for some years to come—but the inner solar system planets, with the exception of Earth, have been colonized more thoroughly than any dusty NASA proposal from the dawn of the space age could have envisaged. \n> \n> From outside the Accelerated civilization, it isn’t really possible to know what’s going on inside. The problem is bandwidth: While it’s possible to send data in and get data out, the sheer amount of computation going on in the virtual spaces of the Acceleration dwarfs any external observer. Inside that swarm, minds a trillion or more times as complex as humanity think thoughts as far beyond human imagination as a microprocessor is beyond a nematode worm. A million random human civilizations flourish in worldscapes tucked in the corner of this world-mind. Death is abolished, life is triumphant. A thousand ideologies flower, human nature adapted where necessary to make this possible. Ecologies of thought are forming in a Cambrian explosion of ideas, for the solar system is finally rising to consciousness, and mind is no longer restricted to the mere kilotons of gray fatty meat harbored in fragile human skulls. \n> \n> Somewhere in the Acceleration, colorless green ideas adrift in furious sleep remember a tiny starship launched years ago, and pay attention. Soon, they realize, the starship will be in position to act as their proxy in an ages-long conversation. Negotiations for access to Amber’s extrasolar asset commence; the Ring Imperium prospers, at least for a while. But first, the operating software on the human side of the network link will require an upgrade.",
      "plaintextDescription": "Oh man I had forgotten two important additional exposition-sections – chapter 5 (the middle chapter of the book, in middle of the act called \"Point of Inflection\") has three expositions that summarize the rising, peak and falling action of the acceleration.\n\nThese were actually some of the most important moments I was looking for. I added them to the post above but listing here for people who missed it the first time:\n\nThe second (middle) new bulletin is:\n\n> Outside the light cone of the Field Circus, on the other side of the spacelike separation between Amber’s little kingdom in motion and the depths of empire time that grip the solar system’s entangled quantum networks, a singular new reality is taking shape. \n> \n> Welcome to the moment of maximum change. \n> \n> About ten billion humans are alive in the solar system, each mind surrounded by an exocortex of distributed agents, threads of personality spun right out of their heads to run on the clouds of utility fog—infinitely flexible computing resources as thin as aerogel—in which they live. The foggy depths are alive with high-bandwidth sparkles; most of Earth’s biosphere has been wrapped in cotton wool and preserved for future examination. For every living human, a thousand million software agents carry information into the farthest corners of the consciousness address space. \n> \n> The sun, for so long an unremarkable mildly variable G2 dwarf, has vanished within a gray cloud that englobes it except for a narrow belt around the plane of the ecliptic. Sunlight falls, unchanged, on the inner planets: except for Mercury, which is no longer present, having been dismantled completely and turned into solar-powered high-temperature nanocomputers. A much fiercer light falls on Venus, now surrounded by glittering ferns of carbon crystals that pump angular momentum into the barely spinning planet via huge superconducting loops wound around its equator. This planet, too, is due to be dismantled. Jupiter, Neptune, Uranus—all "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-25T08:40:32.248Z",
    "af": false
  },
  {
    "_id": "nSasu3sAWc7uxXiDu",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "jHhHEppGhj95raq4Z",
    "topLevelCommentId": "L2EdW348wsafzAsKn",
    "contents": {
      "markdown": "Nod, that's clear enough for now.",
      "plaintextDescription": "Nod, that's clear enough for now."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T05:42:56.242Z",
    "af": false
  },
  {
    "_id": "QpWb3RXPtDenSiKj8",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "dfo2Mmoj5qodHs2hr",
    "topLevelCommentId": "L2EdW348wsafzAsKn",
    "contents": {
      "markdown": "(I think the thing you're calling US-led entente is something towards the latter end of the \"Globally Controlled Takeof --> Ad hoc semi-controlled Takeoff\" spectrum in [this comment,](https://www.lesswrong.com/posts/kkWmybhaig4oSWkgX/shut-it-down-is-simpler-than-controlled-takeoff?commentId=KiQH7k52Qt2SZuRsA) does that sound right?)",
      "plaintextDescription": "(I think the thing you're calling US-led entente is something towards the latter end of the \"Globally Controlled Takeof --> Ad hoc semi-controlled Takeoff\" spectrum in this comment, does that sound right?)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T03:35:40.240Z",
    "af": false
  },
  {
    "_id": "KiQH7k52Qt2SZuRsA",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "KxsaPF8RD7TB7daDM",
    "topLevelCommentId": "mokYbFDEmLs9Q7JzC",
    "contents": {
      "markdown": "The post is specifically about \"globally controlled takeoff\", in which multiple governments have agreed to locate their GPUs in locations that are easy for each other to inspect. \n\nThere's a spectrum between \"Literally all countries agree to consolidate and monitor compute\", \"US/UK/China do it\", \"US/UK/Europe agree to do it among themselves\", \"US does it just for itself\" and \"individual orgs are just being idk a bit careful and a bit cooperative in an ad-hoc fashion.\"\n\nI call the latter end of the spectrum \"*ad hoc semi-controlled semi-slowed takeof\"* at the beginning of the post. If we get something somewhere in the middle, seems probably an improvement.",
      "plaintextDescription": "The post is specifically about \"globally controlled takeoff\", in which multiple governments have agreed to locate their GPUs in locations that are easy for each other to inspect. \n\nThere's a spectrum between \"Literally all countries agree to consolidate and monitor compute\", \"US/UK/China do it\", \"US/UK/Europe agree to do it among themselves\", \"US does it just for itself\" and \"individual orgs are just being idk a bit careful and a bit cooperative in an ad-hoc fashion.\"\n\nI call the latter end of the spectrum \"ad hoc semi-controlled semi-slowed takeof\" at the beginning of the post. If we get something somewhere in the middle, seems probably an improvement."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T03:34:19.602Z",
    "af": false
  },
  {
    "_id": "NM29TkvMfm6Rzs4j6",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "7Ev6SGYFLcz89SvRk",
    "topLevelCommentId": "mokYbFDEmLs9Q7JzC",
    "contents": {
      "markdown": "I think there are just very few people for whom this is a compelling argument. I don't think government are coming anywhere close to explicitly making this calculation. I think some people in labs are maybe making this decision but they aren't actually the target audience for this.",
      "plaintextDescription": "I think there are just very few people for whom this is a compelling argument. I don't think government are coming anywhere close to explicitly making this calculation. I think some people in labs are maybe making this decision but they aren't actually the target audience for this."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T02:34:00.361Z",
    "af": false
  },
  {
    "_id": "ktkH99fgWi9KYEhcw",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "mokYbFDEmLs9Q7JzC",
    "topLevelCommentId": "mokYbFDEmLs9Q7JzC",
    "contents": {
      "markdown": "> Given all of that, I'd expect covert or overt government projects to continue even if we get a treaty banning private research. (Which I do find quite possible; I expect scary demos to happen naturally).\n\nWhy is this special to Shutdown, vs Controlled Takeoff? (Here, I'm specifically comparing two plans that both route through \"first, do a pretty difficult political action of get countries agreeing to centralize GPUs\"). If you just expect people to defect from that, what's the point?",
      "plaintextDescription": "> Given all of that, I'd expect covert or overt government projects to continue even if we get a treaty banning private research. (Which I do find quite possible; I expect scary demos to happen naturally).\n\nWhy is this special to Shutdown, vs Controlled Takeoff? (Here, I'm specifically comparing two plans that both route through \"first, do a pretty difficult political action of get countries agreeing to centralize GPUs\"). If you just expect people to defect from that, what's the point?"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-25T02:33:37.995Z",
    "af": false
  },
  {
    "_id": "dfo2Mmoj5qodHs2hr",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "L2EdW348wsafzAsKn",
    "topLevelCommentId": "L2EdW348wsafzAsKn",
    "contents": {
      "markdown": "I think I don't (automatically) see the dots connected here of how the linked section translates into something I'd call \"US-led entente\" (I think I maybe get it based on vague recollections of Dario writings or something, but, would appreciate you spelling it out more)",
      "plaintextDescription": "I think I don't (automatically) see the dots connected here of how the linked section translates into something I'd call \"US-led entente\" (I think I maybe get it based on vague recollections of Dario writings or something, but, would appreciate you spelling it out more)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T02:30:21.296Z",
    "af": false
  },
  {
    "_id": "q5NsunjQsQmevcwjM",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "DMAXugMKApaswKuBN",
    "topLevelCommentId": "B6XHKJhaQyT87JmLm",
    "contents": {
      "markdown": "Yeah, when I say \"things that can happen most likely will\", I don't mean \"in any specific case.\" A given baby's first words can't be both mama and papa. But, there's a range of phonemes that babies can make. And over time, eventually every combination of first 2-4 phonemes will happen to be a baby's first \"word\".\n\nBefore respond to the rest, I want to check back on, this bit, at the meta level:\n\n> why do you think that insofar as a coherent, non-trivial goal emerges, it is likely to eventually result in humanity's destruction?\n\nThis is something Eliezer (and I think I) have written about recently, which I think you read. (In the chapter \"It's favorite things\"). \n\nI get that you didn't really buy those arguments as being dominating. But, a feeling I get when reading your question there is something like \"there are a lot of moving parts to this argument, and when we focus on one for awhile the earlier bits lose salience.\" \n\nAnd, perhaps similar to \"things that can happen, eventually will, given enough chances, unless stopped\", another pretty loadbearing structural claim is:\n\n**\"It is possible to just actually exhaustively think through a large number of questions and arguments, and for each one, get to a pretty confident state of what the answer to that question is.\" **\n\n**And the,n it's at least** ***possible*** **to make a pretty good guess about how things will play out, at least if we don't learn** ***new*** **information.**\n\nAnd maybe you can't get to 100% confidence. But you can rule out things like \"well, it won't work *unless* Claim A turns out to be false, even though it looks most likely true.\" And this constraints what types of worlds you might possibly be living in. \n\nOr, maybe you *can't* reach that even a moderate confidence with your current knowledge, but, you can see which things you're still uncertain of, which *if* you became more certain of, would change the overall picture.\n\n...\n\n(i.e. the \"unless something stops it\" clause in the \"if it can happen, it will, unless stopped\" argument, means we live in worlds whether *either* it eventually happens, *or* is stopped, and then we can start asking \"okay, what are the ways it could hypothetically be stopped? how likely do those look?\")\n\n*\"Things that can happen, eventually will, given enough chances, unless stopped\"* is one particular argument that is relevant to some of the subpoints here. Yesterday you were like \"yeah I don't buy that.\" I spelled out what I meant, and its sounds like now your position is \"okay, I do see what you mean there, but I don't see how it leads to the final conclusion.\"\n\nThere are a lot more steps, at that level of detail, before I'd expect you to believe something more similar to what I believe.\n\nI'm super grateful for getting to talk to you about this so far, I've enjoyed the convo and it's been helpful to me for getting more clarity on how all the pieces fit together in my own head. If you wanna tap out, seems super understandable.\n\nBut, the thing I am kinda hoping/asking for is for you to actually track all all the arguments as they build, and if a new argument changes your mind on a given claim, track how that fits into all the existing claims and whether it has any new implications.\n\n...\n\nI'm not quite sure how you're relating to your previous beliefs about \"if it can happen, it will\" and the arguments I just made. I'm guessing it wasn't exactly an update for you so much as a \"reframing.\" \n\nBut, it sounds like you now understand what I meant, and why it at least means \"the fact superintelligence is possible, and that people are trying, means that it'll probably happen \\[in some timeframe\\]\". \n\nAnd, while I haven't yet proven all the rest of the steps of the argument to you, like... I'm asking you to notice that I did have an answer there, and there are other pieces that I think I also have answers to. But the complete edifice is indeed multiple books worth, and because each individual (like you) has different cruxes, it's hard to present all the arguments in a succinct, compelling way.\n\nBut, I'm asking if you're up for at least being willing to entertain the structure of \"maybe, Ray will be right that there is a large-but-finite set of claims, and it's possible to get enough certainty on each claim to at least put pretty significant bounds on how unaligned AI may play out.\"",
      "plaintextDescription": "Yeah, when I say \"things that can happen most likely will\", I don't mean \"in any specific case.\" A given baby's first words can't be both mama and papa. But, there's a range of phonemes that babies can make. And over time, eventually every combination of first 2-4 phonemes will happen to be a baby's first \"word\".\n\nBefore respond to the rest, I want to check back on, this bit, at the meta level:\n\n> why do you think that insofar as a coherent, non-trivial goal emerges, it is likely to eventually result in humanity's destruction?\n\nThis is something Eliezer (and I think I) have written about recently, which I think you read. (In the chapter \"It's favorite things\"). \n\nI get that you didn't really buy those arguments as being dominating. But, a feeling I get when reading your question there is something like \"there are a lot of moving parts to this argument, and when we focus on one for awhile the earlier bits lose salience.\" \n\nAnd, perhaps similar to \"things that can happen, eventually will, given enough chances, unless stopped\", another pretty loadbearing structural claim is:\n\n\"It is possible to just actually exhaustively think through a large number of questions and arguments, and for each one, get to a pretty confident state of what the answer to that question is.\" \n\nAnd the,n it's at least possible to make a pretty good guess about how things will play out, at least if we don't learn new information. \n\nAnd maybe you can't get to 100% confidence. But you can rule out things like \"well, it won't work unless Claim A turns out to be false, even though it looks most likely true.\" And this constraints what types of worlds you might possibly be living in. \n\nOr, maybe you can't reach that even a moderate confidence with your current knowledge, but, you can see which things you're still uncertain of, which if you became more certain of, would change the overall picture.\n\n...\n\n(i.e. the \"unless something stops it\" clause in the \"if it can happen, it will, unless stopped\" argum"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-24T18:18:41.282Z",
    "af": false
  },
  {
    "_id": "iEBwGFevqqenQRWxH",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "oHH3yDWtKbxAmG4Pv",
    "topLevelCommentId": "oHH3yDWtKbxAmG4Pv",
    "contents": {
      "markdown": "I think Sufficiently Scary Demos need to do something that\n\na) directly, clearly is capable of threatening specific world leaders from multiple nations at once, in a way that is viscerally salient to them specifically\n\nb) but, you don't end up going to jail for it (i.e. something like the difference between \"a really good prank\" and \"actually hurting someone\")\n\nc) ideally, relies as little as possible on general intelligence, as opposed to extremely powerful narrow stuff (with just enough general intelligent agency there to demonstrate that this is scary because it can be self-directed, as opposed to just being a weapon you want to make sure you control)",
      "plaintextDescription": "I think Sufficiently Scary Demos need to do something that\n\na) directly, clearly is capable of threatening specific world leaders from multiple nations at once, in a way that is viscerally salient to them specifically\n\nb) but, you don't end up going to jail for it (i.e. something like the difference between \"a really good prank\" and \"actually hurting someone\")\n\nc) ideally, relies as little as possible on general intelligence, as opposed to extremely powerful narrow stuff (with just enough general intelligent agency there to demonstrate that this is scary because it can be self-directed, as opposed to just being a weapon you want to make sure you control)"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-24T17:40:38.551Z",
    "af": false
  },
  {
    "_id": "bio4yiT3Wj8EKzcsq",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "svbBp4yt9RrYDqrJS",
    "topLevelCommentId": "svbBp4yt9RrYDqrJS",
    "contents": {
      "markdown": "I think it's both. Just checking if you know that [The Epistemic Prisoner's Dilemma](https://www.lesswrong.com/posts/vtMSQtxH7ei2a5T4r/the-epistemic-prisoner-s-dilemma) is an existing concept (where the reason you think you have different payoffs is because you have different beliefs, and you consider whether to cooperate anyway)",
      "plaintextDescription": "I think it's both. Just checking if you know that The Epistemic Prisoner's Dilemma is an existing concept (where the reason you think you have different payoffs is because you have different beliefs, and you consider whether to cooperate anyway)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-24T16:21:53.683Z",
    "af": false
  },
  {
    "_id": "usvtNWRKWrKepsCpA",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "Asa7suvDyrYGM9jmX",
    "topLevelCommentId": "B6XHKJhaQyT87JmLm",
    "contents": {
      "markdown": "> Yes, I object to the \"things that can happen, eventually will\" line of reasoning.\n\nNod, makes sense, I think I want to just focus on this atm.\n\n(also, btw I super appreciate you engaging, I'm sure you've argued a bunch with folk like this already)\n\nSo here's the more specific thing I actually believe. I agree things don't *automatically* happen eventually just because they can. At least, not automatically on relevant timescales. (i.e. eventually infinite monkeys mashing keyboards will produce shakespeare, but, not for bazillions of years)\n\nThe argument is:\n\n*   If something can happen\n*   *and* there's a fairly strong reason to expect some process to steer towards that thing happening\n*   and there's *not* a reason to expect some other processes to steer towards that thing *not* happening\n\n...then the thing probably happens eventually, on a somewhat reasonable timescale, all else equal. (\"reasonable\" timescale depends on how fast whatever steering process works. i.e. stellar evolution might take billions of years, evolution millions of years, and human engineers thousands of years).\n\nFor example, when the first organisms appeared on earth and begin to mutate, I think a smart outside observer *could* predict \"evolution will happen, and unless all the replicators die out, there will probably eventually be a variety of complex organisms.\" \n\nBut, they wouldn't be able to predict that any particular complex mutation would happen. (for example, flying birds, or human intelligence). It was a long time before we got birds. We only have one Earth to sample from, but we're already ~halfway between the time the earth was born and and when the sun engulfs it, so, it's not too surprising if evolution never got around to the combination of traits that birds have. \n\nI think this is a fairly basic probability argument? Like, if each day, there's an n% chance of a beneficial mutation occuring (and then it's host surviving and replicating, given a long enough chunk of time, it would (eventually) be pretty surprising if it *never* happened. Maybe any specific mutation would be difficult to predict happening in 10 billion years. But, if we had trillions and trillions of years, it would be a pretty weird claim that it'd *never* happen.\n\nSimilarly, if each day there are N engineers thinking about how to solve a problem, and making a reasonable effort to creatively explore the space of ways of solving the problem, and we know the problem *is* solveable, then each day there's an n% chance of one of them stumbling towards the solution. \n\n(In both evolution and the engineer's cases, a thing that makes this a lot easier is that the search isn't completely blind. Partial successes can compound. Evolution wasn't going to invent birds *in one go,* that would be indeed be way too combinatorically hard. But, it got to invent \"wiggling appendages\", which then opened up new, better search space of \"particular ways of wiggling appendages\" which eventually leads to locomotion and then flight)\n\nHow fast you should expect that to happens on how much resources are being thrown at it.\n\n(maybe worth reiterate: I *don't* think the \"things that can happen, eventually will\" applies to AI in the near future, that's a much more specific claim, and Eliezer-et-all are much less confident about that).\n\nThere exists some math, for a given creation/steering-system and some models of how often it generates new ideas and how fast those ideas then reach saturation, for \"at what point it becomes more surprising, that a thing *has never* happened, than that it's happened at least once.\"\n\nWe can't perfectly predict it, but it's not something we're perfectly ignorant about. It is possible to look at cells replicating, and model how often mutations happen, and what distribution of mutations tend to happen, and then make predictions about what distribution of mutations you might expect to see, how quickly.\n\nI think early hypothetical non-evolved-but-smart aliens observing early evolution for a thousand years, wouldn't be able to predict \"birds\", but they would might be able to predict \"wiggling appendages\" (or at least whatever was earlier on the tech-tree than wiggling appendages. I'm meaning to include single-cells that are, like, twisting slightly here)\n\nLooking at the rate of human engineering, it'd be pretty hard to predict *exactly* when heavier-than-air-flight would be invented, but, once you've reached the agricultural age and you've gotten to start seeing specialization of labor and creativity and spreading of ideas, and the existence-proof of birds, I think a hypothetical smart observer could put upper and lower bounds on when humans might eventually figure out it out. It would be weird if it took literally billions of years, given that it only took evolution like a few billion years and evolution was clearly way slower.\n\nAnd, I haven't yet laid out any of the specifics of \"and here are the upper and lower bounds on long it seems like it should plausibly take humanity to invent superintelligence.\" I don't think I'd get a more specific answer than \"hundreds or possibly thousand of years.\", but I think it is something that in principle has an answer and you should be able to find/evaluate evidence that narrows down the answer.\n\n(I am still interested in finding nearterm things to bet on since it sounds like I'm more confident than you that general-intelligence-looking things are decently \\[like, >50%\\] likely to happen in the next 20 years or so)",
      "plaintextDescription": "> Yes, I object to the \"things that can happen, eventually will\" line of reasoning.\n\nNod, makes sense, I think I want to just focus on this atm.\n\n(also, btw I super appreciate you engaging, I'm sure you've argued a bunch with folk like this already)\n\nSo here's the more specific thing I actually believe. I agree things don't automatically happen eventually just because they can. At least, not automatically on relevant timescales. (i.e. eventually infinite monkeys mashing keyboards will produce shakespeare, but, not for bazillions of years)\n\nThe argument is:\n\n * If something can happen\n * and there's a fairly strong reason to expect some process to steer towards that thing happening\n * and there's not a reason to expect some other processes to steer towards that thing not happening\n\n...then the thing probably happens eventually, on a somewhat reasonable timescale, all else equal. (\"reasonable\" timescale depends on how fast whatever steering process works. i.e. stellar evolution might take billions of years, evolution millions of years, and human engineers thousands of years).\n\nFor example, when the first organisms appeared on earth and begin to mutate, I think a smart outside observer could predict \"evolution will happen, and unless all the replicators die out, there will probably eventually be a variety of complex organisms.\" \n\nBut, they wouldn't be able to predict that any particular complex mutation would happen. (for example, flying birds, or human intelligence). It was a long time before we got birds. We only have one Earth to sample from, but we're already ~halfway between the time the earth was born and and when the sun engulfs it, so, it's not too surprising if evolution never got around to the combination of traits that birds have. \n\nI think this is a fairly basic probability argument? Like, if each day, there's an n% chance of a beneficial mutation occuring (and then it's host surviving and replicating, given a long enough chunk of time, it would (eventually"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-24T04:31:31.590Z",
    "af": false
  },
  {
    "_id": "zm7zoNF4Y5s6NwiCY",
    "postId": "Xp9ie6pEWFT8Nnhka",
    "parentCommentId": "syM2FoWqGKt7fazmK",
    "topLevelCommentId": "syM2FoWqGKt7fazmK",
    "contents": {
      "markdown": "*(more spoilers)*\n\nI remembered that earlier, and during my re-skim when I wrote this post, I got confused by the combo of:\n\na) he goes out of his way to specify that humans only get to live on around brown dwarves (if the \"there's no value in leaving the economy\", this implies... literally every regular star has aliens?)\n\nb) wormholes exist\n\nc) I think there was a line somewhere about older civilizations that spread farther\n\nAnd also it just seemed so go damn unreasonable for no one to *send out a replicator probe*.  \n\nAnd, like, are there literally no pioneers like the Pilgrims/Puritans who are like \"well this solar system's resources are all crowded. I would rather be king of an empire to myself + my friends than live in the slums of this dyson brain?\"",
      "plaintextDescription": "(more spoilers)\n\n \n\nI remembered that earlier, and during my re-skim when I wrote this post, I got confused by the combo of:\n\na) he goes out of his way to specify that humans only get to live on around brown dwarves (if the \"there's no value in leaving the economy\", this implies... literally every regular star has aliens?)\n\nb) wormholes exist\n\nc) I think there was a line somewhere about older civilizations that spread farther\n\nAnd also it just seemed so go damn unreasonable for no one to send out a replicator probe.  \n\nAnd, like, are there literally no pioneers like the Pilgrims/Puritans who are like \"well this solar system's resources are all crowded. I would rather be king of an empire to myself + my friends than live in the slums of this dyson brain?\""
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-23T23:43:04.772Z",
    "af": false
  },
  {
    "_id": "keWDS74NBf4XDtsZt",
    "postId": "jL7uDE5oH4HddYq4u",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "It's Petrov week! Reminder, if you are running a local Petrov meetup of some kind, you can create a LW event and click the \"Petrov\" button (next to the \"LW\" \"SSC\" etc buttons), to have it show up on the meetup map. \n\n(You can also click this [link to have it automatically](https://www.lesswrong.com/newPost?eventForm=true&PETROV=true) populated with the Petrov tag)\n\nIf you don't want it to be fully public, I recommend putting in the city location and some kind of contact-info so people can ping you, and you can make a call as to whether you have room for more people, or whether you think the people reaching out would be good for your event vibe.\n\n...\n\nI do think the Petrov Day ceremony is a pretty nice experience. It feels like... a real holiday? Like, I've attended Jewish Seders and it's got a very similar vibe of \"we're here to appreciate our history and some values we care about.\"\n\nJim's [latest version](http://petrovday.com/downloads/PetrovDay-DoubleSidedBooklet.pdf) \\[edit: updated to be the correct one for printing doublesided\\] of the booklet works to bridge the connection between the long arc of history (i.e. appreciate what might be lost), the Petrov incident in particular, and current worries about x-risk from AI.\n\nIf you're less into AI, you might also look at [Ozy Brennan's version](https://thingofthings.wordpress.com/2018/09/24/petrov-day-ritual/), which focuses more directly on nuclear war.\n\nReminder that you'll want to get some candles (I recommend getting [these candles](https://www.amazon.com/dp/B01AVEODA2?th=1), with [these candle holders](http://amazon.com/Hosley-Candle-Holders-Weddings-Meditation/dp/B085GLYYCB). *(I personally like having two fancier candles representing the flourishing and extinction futures, such as this *[*one for flourishing*](http://amazon.com/Mister-Candle-Pillar-Unscented-Poured/dp/B01N7OWYT1) *and*[*this one for extinction*](https://www.amazon.com/Skeleton-Head-Sculpture-Halloween-Manifestation/dp/B0D3JDHPTY?th=1)*)*\n\nAnd it'll also be a nicer experience if you get the booklet printed at Fedex-or-similar, where you can have it more nicely collated into a proper booklet.",
      "plaintextDescription": "It's Petrov week! Reminder, if you are running a local Petrov meetup of some kind, you can create a LW event and click the \"Petrov\" button (next to the \"LW\" \"SSC\" etc buttons), to have it show up on the meetup map. \n\n(You can also click this link to have it automatically populated with the Petrov tag)\n\nIf you don't want it to be fully public, I recommend putting in the city location and some kind of contact-info so people can ping you, and you can make a call as to whether you have room for more people, or whether you think the people reaching out would be good for your event vibe.\n\n...\n\nI do think the Petrov Day ceremony is a pretty nice experience. It feels like... a real holiday? Like, I've attended Jewish Seders and it's got a very similar vibe of \"we're here to appreciate our history and some values we care about.\"\n\nJim's latest version [edit: updated to be the correct one for printing doublesided] of the booklet works to bridge the connection between the long arc of history (i.e. appreciate what might be lost), the Petrov incident in particular, and current worries about x-risk from AI.\n\nIf you're less into AI, you might also look at Ozy Brennan's version, which focuses more directly on nuclear war.\n\nReminder that you'll want to get some candles (I recommend getting these candles, with these candle holders. (I personally like having two fancier candles representing the flourishing and extinction futures, such as this one for flourishing and this one for extinction)\n\nAnd it'll also be a nicer experience if you get the booklet printed at Fedex-or-similar, where you can have it more nicely collated into a proper booklet."
    },
    "baseScore": 35,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-09-23T20:26:46.642Z",
    "af": false
  },
  {
    "_id": "ycdHf2aFTBTSE6ZeX",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "dACCFswAaZiJLdMFB",
    "topLevelCommentId": "Sfxpy8DMDJXNEc6TM",
    "contents": {
      "markdown": "(FYI I think I am sold by your other reply that the main question is \"how much is it slowed down initially, and how much does it value the resources that it loses by doing so?\". I agree leaving us with one star isn't that big a deal, modulo making sure we can't somehow mess up the rest of it's plans, which doesn't seem that hard)",
      "plaintextDescription": "(FYI I think I am sold by your other reply that the main question is \"how much is it slowed down initially, and how much does it value the resources that it loses by doing so?\". I agree leaving us with one star isn't that big a deal, modulo making sure we can't somehow mess up the rest of it's plans, which doesn't seem that hard)"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-23T19:55:33.788Z",
    "af": false
  },
  {
    "_id": "3rKtQCuwZgB3eKraj",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "jpMFws7CqcT2sBBst",
    "topLevelCommentId": "Sfxpy8DMDJXNEc6TM",
    "contents": {
      "markdown": "I agree it's small as a fraction of resources, but it still seems very expensive in terms of total resources since that's a lotta galaxies falling outside the lightcone.",
      "plaintextDescription": "I agree it's small as a fraction of resources, but it still seems very expensive in terms of total resources since that's a lotta galaxies falling outside the lightcone."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T19:38:32.209Z",
    "af": false
  },
  {
    "_id": "vKxw9DaoDbWxXeAXF",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "aYzDFrWvDbesv6ibN",
    "topLevelCommentId": "Sfxpy8DMDJXNEc6TM",
    "contents": {
      "markdown": "Mmm, nod. I maybe see it.\n\nRE: \"how long are you delaying?\"\n\nIt seems like a major early choice is \"is the AI basically using the Earth to bootstrap the intergalactic probe process\" or \"is the AI only using the amount of Earth resources that leaves it mostly functional from human's perspective, and then mostly using the rest of the planets.\"\n\nYou note \"seems like it'd only slow down the AI a bit, to first round up humans\". I haven't done a calculation about it, but, seemed potentially like a very big deal to decide between \"full steam ahead on beginning the dyson sphere using Earth Parts\" vs \"start the process from the moon and then mercury\", since there are harder-to-circumvent delays there.",
      "plaintextDescription": "Mmm, nod. I maybe see it.\n\nRE: \"how long are you delaying?\"\n\nIt seems like a major early choice is \"is the AI basically using the Earth to bootstrap the intergalactic probe process\" or \"is the AI only using the amount of Earth resources that leaves it mostly functional from human's perspective, and then mostly using the rest of the planets.\"\n\nYou note \"seems like it'd only slow down the AI a bit, to first round up humans\". I haven't done a calculation about it, but, seemed potentially like a very big deal to decide between \"full steam ahead on beginning the dyson sphere using Earth Parts\" vs \"start the process from the moon and then mercury\", since there are harder-to-circumvent delays there."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T19:30:49.786Z",
    "af": false
  },
  {
    "_id": "tKLZA4fN9QvDjReDB",
    "postId": "Xp9ie6pEWFT8Nnhka",
    "parentCommentId": "R2uSFn3cFgKvL3ovR",
    "topLevelCommentId": "R2uSFn3cFgKvL3ovR",
    "contents": {
      "markdown": "Hmm, I think this still basically stops before it gets to the part I was aiming for in this post (i.e. 30 years after the AI inflection point when things have been REAL crazy for a lot of subjective years)",
      "plaintextDescription": "Hmm, I think this still basically stops before it gets to the part I was aiming for in this post (i.e. 30 years after the AI inflection point when things have been REAL crazy for a lot of subjective years)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T19:10:10.198Z",
    "af": false
  },
  {
    "_id": "Tt5ktB5xDGkG9h6Lz",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "W8XnYF4bgYmEThHdc",
    "topLevelCommentId": "ZgpWnYiaJiuXvum8q",
    "contents": {
      "markdown": "nod, fwiw I didn't have this complaint about you.",
      "plaintextDescription": "nod, fwiw I didn't have this complaint about you."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-23T18:58:01.742Z",
    "af": false
  },
  {
    "_id": "EDTtJiWzvz6K5C9r3",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "Sfxpy8DMDJXNEc6TM",
    "topLevelCommentId": "Sfxpy8DMDJXNEc6TM",
    "contents": {
      "markdown": "I hadn't seen this footnote yet when writing the above:\n\n> Consensual uploads, or uploads people are fine with on reflection, don't count as death. [↩︎](https://www.lesswrong.com/posts/4fqwBmmqi2ZGn9o7j/notes-on-fatalities-from-ai-takeover#fnref-2gtdmyyFpr9ShxGBP-3)\n\nThere's a range of stuff like \"the AI manuevered us into a situation where we either die, or live in world with X standard of living, or get uploaded and get 1000x (or whatever) standard of living. \n\nMost people on reflection choose uploading. I think it's reasonable to disagree with whether this counts as \"death\", but, this seems like pretty ambiguous consent at best to me, and while I think most fully informed people wouldn't count is as \"death\" per se, most people with their current set of beliefs/values would count it more like death than not-death, or, not be very impressed by arguments that it doesn't count as death.\n\n(Curious if there is any kind of mechanical turk poll we could run that would change either of our minds about this? Not sure that it matters that much)\n\nI think this sort of thing is also why I don't think the \"AI may be very slightly nice\" is likely to result in something that's clearly \"not death\". It seems really unlikely to me that very slightly nice things would go the \"preserve parochial humans exactly as is\", it's just such a narrow target to hit even within niceness.",
      "plaintextDescription": "I hadn't seen this footnote yet when writing the above:\n\n> Consensual uploads, or uploads people are fine with on reflection, don't count as death. ↩︎\n\nThere's a range of stuff like \"the AI manuevered us into a situation where we either die, or live in world with X standard of living, or get uploaded and get 1000x (or whatever) standard of living. \n\nMost people on reflection choose uploading. I think it's reasonable to disagree with whether this counts as \"death\", but, this seems like pretty ambiguous consent at best to me, and while I think most fully informed people wouldn't count is as \"death\" per se, most people with their current set of beliefs/values would count it more like death than not-death, or, not be very impressed by arguments that it doesn't count as death.\n\n(Curious if there is any kind of mechanical turk poll we could run that would change either of our minds about this? Not sure that it matters that much)\n\nI think this sort of thing is also why I don't think the \"AI may be very slightly nice\" is likely to result in something that's clearly \"not death\". It seems really unlikely to me that very slightly nice things would go the \"preserve parochial humans exactly as is\", it's just such a narrow target to hit even within niceness."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-23T18:43:34.022Z",
    "af": false
  },
  {
    "_id": "Sfxpy8DMDJXNEc6TM",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Other entities that care trade with the AI (or with entities that the AI trades with) to keep humans alive. This includes acausal trade, ECL, simulations/anthropic capture (to be able to effectively acausally trade with decision theory naive AIs), and causal trade with aliens that the AI ends up encountering. It's unclear how simulations/anthropic capture works out. It seems plausible that this happens in a way which doesn't result in all relevant (acausal) trades happening. It seems plausible that other entities (including humans in other Everett branches) pretty universally don't want to bail humans (in this branch) out because they have better things to spend their resources on, but even a tiny fraction of entities spending some non-trivial fraction of resources could suffice. This depends on there being some beings with power who care about things like human survival despite alignment difficulties, but this feels very likely to me (even if alignment is very hard, there may well be more competent aliens or AIs who care a small amount about this sort of thing). Note that this requires the AI to care about at least some of these mechanisms which isn't obvious.\n\nHow much of your plausibility-of-caring-slightly routes through this, vs the \"actually just slightly intrinisically nice?\" thing.\n\nIn the previous discussion, you said:\n\n> I'm happy to consider non-consensual uploading to be death and I'm certainly happy to consider \"the humans are modified in some way they would find horrifying (at least on reflection)\" to be death. I think \"the humans are alive in the normal sense of alive\" is totally plausible and I expect some humans to be alive in the normal sense of alive in the majority of worlds where AIs takeover.\n> \n> Making uploads is barely cheaper than literally keeping physical humans alive after AIs have fully solidified their power I think, maybe 0-3 OOMs more expensive or something, so I don't think non-consensual uploads are that much of the action. (I do think rounding humans up into shelters is relevant.)\n\nI'm surprised at the \"only 3 OOMs more expensive.\" I haven't done a calculation but that seems really implausible. Maybe I am not imagining how big an OOM is properly.\n\nIn particular, comparing \"keep humans alive indefinitely, multigenerationally\" vs \"store them on ice without evening running them, turn them back on when/if you trade them.\" \n\nThe sort of entity that might pay extra for \"keep them actually alive instead of on-ice\"... probably also cares about them being alive and getting some kind of standard of living or something. (fyi it's not even obvious most people would *prefer* being alive in minimally-satisfying-shelters vs stored, or run in simulation at a higher standard of living)\n\nInsofar as this option is loadbearing for \"you put lowish odds on them killing everyone\", I think a crux is not finding it very plausible that the AI would choose \"keep fully alive in a way that is clearly better than death\" vs \"store digitally\" or \"upload.\" It's just a very narrow target for the game theory to work out such that this exact thing is what turns out to matter.",
      "plaintextDescription": "> Other entities that care trade with the AI (or with entities that the AI trades with) to keep humans alive. This includes acausal trade, ECL, simulations/anthropic capture (to be able to effectively acausally trade with decision theory naive AIs), and causal trade with aliens that the AI ends up encountering. It's unclear how simulations/anthropic capture works out. It seems plausible that this happens in a way which doesn't result in all relevant (acausal) trades happening. It seems plausible that other entities (including humans in other Everett branches) pretty universally don't want to bail humans (in this branch) out because they have better things to spend their resources on, but even a tiny fraction of entities spending some non-trivial fraction of resources could suffice. This depends on there being some beings with power who care about things like human survival despite alignment difficulties, but this feels very likely to me (even if alignment is very hard, there may well be more competent aliens or AIs who care a small amount about this sort of thing). Note that this requires the AI to care about at least some of these mechanisms which isn't obvious.\n\nHow much of your plausibility-of-caring-slightly routes through this, vs the \"actually just slightly intrinisically nice?\" thing.\n\nIn the previous discussion, you said:\n\n> I'm happy to consider non-consensual uploading to be death and I'm certainly happy to consider \"the humans are modified in some way they would find horrifying (at least on reflection)\" to be death. I think \"the humans are alive in the normal sense of alive\" is totally plausible and I expect some humans to be alive in the normal sense of alive in the majority of worlds where AIs takeover.\n> \n> Making uploads is barely cheaper than literally keeping physical humans alive after AIs have fully solidified their power I think, maybe 0-3 OOMs more expensive or something, so I don't think non-consensual uploads are that much of the action. (I do"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-23T18:38:05.640Z",
    "af": false
  },
  {
    "_id": "PvpgcPRqKwEHw8mCk",
    "postId": "6ZnznCaTcbGYsCmqu",
    "parentCommentId": "4FpkpiCNajWTqdhyw",
    "topLevelCommentId": "Kxy2jpnWkgBijbTvA",
    "contents": {
      "markdown": "I don't think they need theory of mind, just as evolution and regular ol' viruses don't. The LLMs say stuff for the reasons LLMs normally say stuff, some of that stuff happens to be good memetic replicators (this might be completely random, or might be for reasons that are sort of interesting but not because the LLM is choosing to go viral on purpose), and then those go on to show up in more places.",
      "plaintextDescription": "I don't think they need theory of mind, just as evolution and regular ol' viruses don't. The LLMs say stuff for the reasons LLMs normally say stuff, some of that stuff happens to be good memetic replicators (this might be completely random, or might be for reasons that are sort of interesting but not because the LLM is choosing to go viral on purpose), and then those go on to show up in more places."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-23T06:10:41.348Z",
    "af": false
  },
  {
    "_id": "HYpsF2phWvMuCnkAT",
    "postId": "JWH63Aed3TA2cTFMt",
    "parentCommentId": "7iSXMRD6hoEDqjvSg",
    "topLevelCommentId": "ndvkPbyRMhEJD9NMx",
    "contents": {
      "markdown": "> It seems like maybe part of the issue is that you hear Nate and Eliezer as saying \"here is the argument for why it's obvious that ASI will kill us all\" and I hear them as saying \"here is the argument for why ASI will kill us all\" and so you're docking them points when they fail to reach the high standard of \"this is a watertight and irrefutable proof\" and I'm not?\n\nfwiw I think Eliezer/Nate *are* saying \"it's obvious, unless we were to learn new surprising information\" and deliberately not saying \"it has a watertight proof\", and part of the disagreement here is \"have they risen the standard of 'fairly obvious call, unless we learn new surprising information?\"\n\n(with the added wrinkle of many people incorrectly thinking LLM era observations count as new information that changes the call)",
      "plaintextDescription": "> It seems like maybe part of the issue is that you hear Nate and Eliezer as saying \"here is the argument for why it's obvious that ASI will kill us all\" and I hear them as saying \"here is the argument for why ASI will kill us all\" and so you're docking them points when they fail to reach the high standard of \"this is a watertight and irrefutable proof\" and I'm not?\n\nfwiw I think Eliezer/Nate are saying \"it's obvious, unless we were to learn new surprising information\" and deliberately not saying \"it has a watertight proof\", and part of the disagreement here is \"have they risen the standard of 'fairly obvious call, unless we learn new surprising information?\"\n\n(with the added wrinkle of many people incorrectly thinking LLM era observations count as new information that changes the call)"
    },
    "baseScore": 10,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-23T02:41:10.453Z",
    "af": false
  },
  {
    "_id": "NFnce8xmTfNbocgiu",
    "postId": "anFrGMskALuH7aZDw",
    "parentCommentId": "mpGFqGHdNscMd4RsY",
    "topLevelCommentId": "JDp8WxRHhHGu64ibK",
    "contents": {
      "markdown": "The word and actual connotations of anal-retentive are important to my sentence. (Also, I said \"this feels righter-to-me\" not \"this is right\" and I definitely did not make an explicit defense of exactly this wording as aspirational policy)\n\nWe absolutely should have more practices that drive at the precise truth than saying true stuff and contradicting false stuff.\n\nSome of those practices should include tracking various metaphorical forests-vs-trees, and being some-kind-of-intentional about what things are worth arguing in what sort of ways. (This does not come with any particular opinion about what sort of ways are worth arguing what sort of things, just, that there exist at least some patterns of nerdy pedantry that do not automatically get to be treated as actively good parts of a good truthseeking culture)\n\n(I think this was fairly obvious and that you are indeed being kind of obnoxious so I have strong downvoted you in this instance)",
      "plaintextDescription": "The word and actual connotations of anal-retentive are important to my sentence. (Also, I said \"this feels righter-to-me\" not \"this is right\" and I definitely did not make an explicit defense of exactly this wording as aspirational policy)\n\nWe absolutely should have more practices that drive at the precise truth than saying true stuff and contradicting false stuff.\n\nSome of those practices should include tracking various metaphorical forests-vs-trees, and being some-kind-of-intentional about what things are worth arguing in what sort of ways. (This does not come with any particular opinion about what sort of ways are worth arguing what sort of things, just, that there exist at least some patterns of nerdy pedantry that do not automatically get to be treated as actively good parts of a good truthseeking culture)\n\n(I think this was fairly obvious and that you are indeed being kind of obnoxious so I have strong downvoted you in this instance)"
    },
    "baseScore": 7,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-23T01:52:05.899Z",
    "af": false
  },
  {
    "_id": "ctBhLfpNDFn7wEwaT",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "pYzvmEHnhTGeKHPGC",
    "topLevelCommentId": "JuitdRH3a7mQWEDcA",
    "contents": {
      "markdown": "Yeah, I agree with \"trapped priors\" being a major problem. \n\nThe solution this angle brings to mind is more like \"subsidize comments/posts that do a good job of presenting counterarguments in a way that is less triggering / feeding into the toxoplasma\".",
      "plaintextDescription": "Yeah, I agree with \"trapped priors\" being a major problem. \n\nThe solution this angle brings to mind is more like \"subsidize comments/posts that do a good job of presenting counterarguments in a way that is less triggering / feeding into the toxoplasma\"."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-22T18:56:42.844Z",
    "af": false
  },
  {
    "_id": "LyBCsgrKGQDzBroqK",
    "postId": "anFrGMskALuH7aZDw",
    "parentCommentId": "ftT85tQiNijTo5Afk",
    "topLevelCommentId": "JDp8WxRHhHGu64ibK",
    "contents": {
      "markdown": "This paragraph feels righter-to-me (oh, huh, you even ended up with the same word \"ostentatious\" as pointer that I did in my comment-1-minute-ago)",
      "plaintextDescription": "This paragraph feels righter-to-me (oh, huh, you even ended up with the same word \"ostentatious\" as pointer that I did in my comment-1-minute-ago)"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-22T18:54:16.587Z",
    "af": false
  },
  {
    "_id": "aqFwkFSArPjfu2To6",
    "postId": "anFrGMskALuH7aZDw",
    "parentCommentId": "JDp8WxRHhHGu64ibK",
    "topLevelCommentId": "JDp8WxRHhHGu64ibK",
    "contents": {
      "markdown": "I'll say (as a guy who just wrote a very pro-book post) that this vibe feels off to me. (I'm not sure if any particular sentence seems definitely wrong, but, it feels like it's coming from a generator that I think is wrong)\n\nI think Eliezer/Nate were deliberately not attempting to make the book some kind of broad thing the whole community could rally behind. They might have done so, but, they didn't. So, complaining about \"why our kind can't cooperate\" doesn't actually feel right to me in this instance.\n\n(I think there's some kind of subtle \"why we can't cooperate\" thing that is still relevant, but, it's less like \"YOU SHOULD ALL BE COOPERATING\" and more like \"some people should notice that something is weird about the way they're sort of... ostentatiously not cooperating?\". Where I'm not so much frustrated at them \"not cooperating,\" more frustrated at the weirdness of the dynamics around the ostentatiousness. *(This sentence still isn't quite right, but, I'mma leave it there for no)*",
      "plaintextDescription": "I'll say (as a guy who just wrote a very pro-book post) that this vibe feels off to me. (I'm not sure if any particular sentence seems definitely wrong, but, it feels like it's coming from a generator that I think is wrong)\n\nI think Eliezer/Nate were deliberately not attempting to make the book some kind of broad thing the whole community could rally behind. They might have done so, but, they didn't. So, complaining about \"why our kind can't cooperate\" doesn't actually feel right to me in this instance.\n\n(I think there's some kind of subtle \"why we can't cooperate\" thing that is still relevant, but, it's less like \"YOU SHOULD ALL BE COOPERATING\" and more like \"some people should notice that something is weird about the way they're sort of... ostentatiously not cooperating?\". Where I'm not so much frustrated at them \"not cooperating,\" more frustrated at the weirdness of the dynamics around the ostentatiousness. (This sentence still isn't quite right, but, I'mma leave it there for no)"
    },
    "baseScore": 5,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-22T18:52:51.240Z",
    "af": false
  },
  {
    "_id": "GR7YzCdppqKiDkQR2",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "yvRayYexGymZfhAJp",
    "topLevelCommentId": "JuitdRH3a7mQWEDcA",
    "contents": {
      "markdown": "FYI I got value from the last round of arguments between Buck/Ryan and Eliezer (in [The Problem](https://www.lesswrong.com/posts/kgb58RL88YChkkBNf/the-problem)), where I definitely agree Eliezer was being obtuse/annoying. I learned more useful things about Buck's worldview from that one than Eliezer's (nonzero from Eliezer's tho), and I think that was good for the commons more broadly. \n\nI don't know if it was a better use of time than whatever else Buck would have done that day, but, I appreciated it. \n\n(I'm not sure what to do about the fact that Being Triggered is such a powerful catalyst for arguing, it does distort what conversations we find ourselves having, but, I think it increases the total amount of public argumentation that exists, fairly significantly)",
      "plaintextDescription": "FYI I got value from the last round of arguments between Buck/Ryan and Eliezer (in The Problem), where I definitely agree Eliezer was being obtuse/annoying. I learned more useful things about Buck's worldview from that one than Eliezer's (nonzero from Eliezer's tho), and I think that was good for the commons more broadly. \n\nI don't know if it was a better use of time than whatever else Buck would have done that day, but, I appreciated it. \n\n(I'm not sure what to do about the fact that Being Triggered is such a powerful catalyst for arguing, it does distort what conversations we find ourselves having, but, I think it increases the total amount of public argumentation that exists, fairly significantly)"
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-22T18:23:29.496Z",
    "af": false
  },
  {
    "_id": "QXwwfmihstZrRu2H2",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "JvyWpqPzpLzdtAqXE",
    "topLevelCommentId": "B6XHKJhaQyT87JmLm",
    "contents": {
      "markdown": "Oh, if that's what you meant by Defense in Depth, as Joe said, the book's argument is \"we don't know how.\" \n\nAt weak capabilities, our current ability to steer AI is sufficient, because mistakes aren't that bad. Anthropic is trying pretty hard with Claude to build something that's robustly aligned, and it's just quite hard. When o3 or claude cheat on programming tasks, they get caught, and the consequences aren't that dire. But when there are millions of iterations of AI-instances making choices, and when it is smarter than humanity, the amount of robustness you need is much much higher.\n\n> My understanding is that Y&S think this is doomed because ~\"at the limit of <poorly defined, handwavy stuff> the model will end up killing us \\[probably as a side-effect\\] anyway\"\n> \n> \\[...\\]\n> \n> And then you can draw conclusions about what a perfect agent with those preferences would do. But there's no reason to believe your map always applies.\n\nI'm not quite sure how to parse this, but, it sounds like you're saying something like \"I don't understand why we should expect in the limit something to be a perfect game theoretic agent.\" The answer is \"because if it wasn't, that wouldn't be the limit, and the AI would notice it was behaving suboptimally, and figure out a way to change that.\"\n\nNot every AI will do that, automatically. But, if you're deliberately pushing the AI to be a good problem solver, and if it ends up in a position where it is capable of improving it's cognition, once it notices 'improve my cognition' as a viable option, there's not a reason for it to stop.\n\n...\n\nIt sounds like a lot of your objection is maybe to the general argument \"things that can happen, eventually will.\" (in particular, when billions of dollars worth of investment are trying to push towards things-nearby-that-attractor happening).\n\n(Or, maybe more completely: \"sure, things that can happen eventually will, but meanwhile a lot of other stuff might happen that changes how path-dependent-things will play out?\")\n\nI'm curious how loadbearing that feels for the rest of the arguments?",
      "plaintextDescription": "Oh, if that's what you meant by Defense in Depth, as Joe said, the book's argument is \"we don't know how.\" \n\nAt weak capabilities, our current ability to steer AI is sufficient, because mistakes aren't that bad. Anthropic is trying pretty hard with Claude to build something that's robustly aligned, and it's just quite hard. When o3 or claude cheat on programming tasks, they get caught, and the consequences aren't that dire. But when there are millions of iterations of AI-instances making choices, and when it is smarter than humanity, the amount of robustness you need is much much higher.\n\n> My understanding is that Y&S think this is doomed because ~\"at the limit of <poorly defined, handwavy stuff> the model will end up killing us [probably as a side-effect] anyway\"\n> \n> [...]\n> \n> And then you can draw conclusions about what a perfect agent with those preferences would do. But there's no reason to believe your map always applies.\n\nI'm not quite sure how to parse this, but, it sounds like you're saying something like \"I don't understand why we should expect in the limit something to be a perfect game theoretic agent.\" The answer is \"because if it wasn't, that wouldn't be the limit, and the AI would notice it was behaving suboptimally, and figure out a way to change that.\"\n\nNot every AI will do that, automatically. But, if you're deliberately pushing the AI to be a good problem solver, and if it ends up in a position where it is capable of improving it's cognition, once it notices 'improve my cognition' as a viable option, there's not a reason for it to stop.\n\n...\n\nIt sounds like a lot of your objection is maybe to the general argument \"things that can happen, eventually will.\" (in particular, when billions of dollars worth of investment are trying to push towards things-nearby-that-attractor happening).\n\n(Or, maybe more completely: \"sure, things that can happen eventually will, but meanwhile a lot of other stuff might happen that changes how path-dependent-things wil"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-22T18:12:51.143Z",
    "af": false
  },
  {
    "_id": "H2GfGFGiEgF7MZKt8",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "ik7goJRtkGTQgBrxg",
    "topLevelCommentId": "B6XHKJhaQyT87JmLm",
    "contents": {
      "markdown": "I do think this is a pretty good point about how human value formation tends to happen.\n\nI think something sort-of-similar might happen to happen a little, nearterm, with LLM-descended AI. But, AI just doesn't have any of the same social machinery actually embedded in it the same way, so if it's doing something similar, it'd be happening because LLMs vaguely ape human tendencies. (And I expect this to stop being a major factor as the AI gets smarter. I don't expect it to install the sort of social drives itself that humans have, and \"imitate humans\" has pretty severe limits of how smart you can get, so if we get to AI much smarter than that, it'll probably be doing a different thing)\n\nI think the more important here is \"notice that you're (probably) wrong about about how you actually do your value-updating, and this may be warping your expectations about how AI would do it.\"\n\nBut, that doesn't leave me with any particular other idea than the current typical bottom-up story.\n\n*(obviously if we did something more like uploads, or upload-adjacent, it'd be a whole different story)*",
      "plaintextDescription": "I do think this is a pretty good point about how human value formation tends to happen.\n\nI think something sort-of-similar might happen to happen a little, nearterm, with LLM-descended AI. But, AI just doesn't have any of the same social machinery actually embedded in it the same way, so if it's doing something similar, it'd be happening because LLMs vaguely ape human tendencies. (And I expect this to stop being a major factor as the AI gets smarter. I don't expect it to install the sort of social drives itself that humans have, and \"imitate humans\" has pretty severe limits of how smart you can get, so if we get to AI much smarter than that, it'll probably be doing a different thing)\n\nI think the more important here is \"notice that you're (probably) wrong about about how you actually do your value-updating, and this may be warping your expectations about how AI would do it.\"\n\nBut, that doesn't leave me with any particular other idea than the current typical bottom-up story.\n\n(obviously if we did something more like uploads, or upload-adjacent, it'd be a whole different story)"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-22T00:48:49.235Z",
    "af": false
  },
  {
    "_id": "X9ksvhM8Kfz2JnQGf",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "CctggPhBFeGrAwKZw",
    "topLevelCommentId": "JuitdRH3a7mQWEDcA",
    "contents": {
      "markdown": "> EDIT: Missed Raemon's reply, I agree with at least the vibe of his comment (it's a bit stronger than what I'd have said).\n\nOh huh, kinda surprised my phrasing was stronger than what you'd say. \n\nGetting into a bit from a problem-solving angle, in a \"first think about the problem for 5 minutes before proposing solutions\" kinda way...\n\nThe reasons the problem is hard include:\n\n1.  New people keep coming in, and unless we change something significant about our new-user-acceptance process, it's often a long progress to enculturate them into even having the belief they should be *trying* not to get tribally riled up.\n    1.  Also, a lot of them are weaker at evaluating arguments, and are likely to upvote bad arguments for positions that they just-recently-got-excited-about. (\"newly converted\" syndrome)\n2.  Tribal thinking is just really ingrained, and slippery even for people putting in a moderate effort not to do it.\n    1.  often, if you run a check \"am I being tribal/triggered or do I really endorse this?\", there will be a significant part of you that's running *some* kind of real-feeling cognition. So the check \"was this justified?\" returns \"true\" unless you're paying attention to subtleties.\"\n    2.  relatedly: just knowing \"I'm being tribal right now, I should avoid it\" doesn't really tell you what to do instead. I notice a comment I dislike because it's part of a political faction I think is constantly motivatedly wrong about stuff. The comment seems wrong. Do I... not downvote it? Well, I still think it's a bad comment, it's just that the reason it flagged itself so hard to my attention is Because Tribalism.   \n          \n        *(or,* there's a comment with a mix of good and bad properties. Do I upvote, downvote, or leave it alone? idk. Sometimes when I'm trying to account for tribalness I find myself upvoting stuff I'd ordinarily have passed over because I'm trying to out of my way to be gracious, but I'm not sure if that's successfully countering a bias or just following a different one. Sometimes this results in mediocre criticism getting upvoted)\n3.  There's some selection effect around \"triggeredness\" that produces disproportionate conversations. Even if most of the time people are pretty reasonable thinking about reasonable things together, the times people get triggered (politically or otherwise), result in more/bigger/more-self-propagating conversations.\n4.  There's an existing equilibrium where there's some factions that are upvoting/downvoting each other, and it feels scary to leave something un-upvoted since it might get ~brigaded.\n5.  It's easy for the voting system to handle prominence-of-posts. It's a lot harder for the voting system to actually handle prominence of comments. In a high-volume comment-thread, every new comment sends a notification to at least the author of the OP and/or previous commenter, and lots of people are just checking often, so even downvoted comments are going to get seen, (and people will worry that they're get later upvoted by another cluster of people)\n    1.  (we do currently hide downvoted comments from the homepage in most contexts)\n\nProbably there's more.\n\nMeanwhile, the knobs to handle this are:\n\n*   extremely expensive, persistent manual moderation from people (who realistically are going to sometimes be triggered themselves)\n*   try to detect patterns of triggeredness/tribalness, and change something about people's voting powers or commenting powers, or what comments get displayed.\n*   change something about the UI for upvoting. \n\nThe sort of thing that seems like an improvement is changing something about how strong upvotes work, at least in some cases. (i.e. maybe if we detect a thread has fairly obvious factions tug-of-war-ing, we turn off strong-upvoting, or add some kind of cooldown period)\n\nWe've periodically talked about having strong-upvotes require giving a reason, or otherwise constrained in some way, although I think there was disagreement about whether that'd probably be good or bad.",
      "plaintextDescription": "> EDIT: Missed Raemon's reply, I agree with at least the vibe of his comment (it's a bit stronger than what I'd have said).\n\nOh huh, kinda surprised my phrasing was stronger than what you'd say. \n\nGetting into a bit from a problem-solving angle, in a \"first think about the problem for 5 minutes before proposing solutions\" kinda way...\n\nThe reasons the problem is hard include:\n\n 1. New people keep coming in, and unless we change something significant about our new-user-acceptance process, it's often a long progress to enculturate them into even having the belief they should be trying not to get tribally riled up.\n    1. Also, a lot of them are weaker at evaluating arguments, and are likely to upvote bad arguments for positions that they just-recently-got-excited-about. (\"newly converted\" syndrome)\n 2. Tribal thinking is just really ingrained, and slippery even for people putting in a moderate effort not to do it.\n    1. often, if you run a check \"am I being tribal/triggered or do I really endorse this?\", there will be a significant part of you that's running some kind of real-feeling cognition. So the check \"was this justified?\" returns \"true\" unless you're paying attention to subtleties.\"\n    2. relatedly: just knowing \"I'm being tribal right now, I should avoid it\" doesn't really tell you what to do instead. I notice a comment I dislike because it's part of a political faction I think is constantly motivatedly wrong about stuff. The comment seems wrong. Do I... not downvote it? Well, I still think it's a bad comment, it's just that the reason it flagged itself so hard to my attention is Because Tribalism. \n       \n       (or, there's a comment with a mix of good and bad properties. Do I upvote, downvote, or leave it alone? idk. Sometimes when I'm trying to account for tribalness I find myself upvoting stuff I'd ordinarily have passed over because I'm trying to out of my way to be gracious, but I'm not sure if that's successfully countering a bias or just following "
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-22T00:36:01.267Z",
    "af": false
  },
  {
    "_id": "afcxp4A7ic9GGDH2K",
    "postId": "mve2bunf6YfTeiAvd",
    "parentCommentId": "kJmnCh4g9ya8PdgHA",
    "topLevelCommentId": "kJmnCh4g9ya8PdgHA",
    "contents": {
      "markdown": "[@yams](https://www.lesswrong.com/users/yams?mention=user) [@Duncan Sabien (Inactive)](https://www.lesswrong.com/users/duncan-sabien-inactive?mention=user) [@Rob Bensinger](https://www.lesswrong.com/users/robbbb?mention=user) ?",
      "plaintextDescription": "@yams @Duncan Sabien (Inactive) @Rob Bensinger ?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-22T00:13:39.919Z",
    "af": false
  },
  {
    "_id": "BbeAdHt395ezrjBNF",
    "postId": "JWH63Aed3TA2cTFMt",
    "parentCommentId": "4HCRqxpnjddtXcvuA",
    "topLevelCommentId": "ndvkPbyRMhEJD9NMx",
    "contents": {
      "markdown": "Why is this any different than training a next generation of word-predictors and finding out it can now play chess, or do chain-of-thought reasoning, or cheat on tests? I agree it's unlocking new abilities, I just disagree that this implies anything massively different from what's already going on, and is the thing you'd expect to happen by default.",
      "plaintextDescription": "Why is this any different than training a next generation of word-predictors and finding out it can now play chess, or do chain-of-thought reasoning, or cheat on tests? I agree it's unlocking new abilities, I just disagree that this implies anything massively different from what's already going on, and is the thing you'd expect to happen by default."
    },
    "baseScore": 2,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-21T19:53:11.258Z",
    "af": false
  },
  {
    "_id": "twfgmdWGY6Yu5RGyt",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "hmfxjEnnM8ab2GEfm",
    "topLevelCommentId": "JuitdRH3a7mQWEDcA",
    "contents": {
      "markdown": "I think Rohin is (correctly IMO) noticing that, while often some thoughtful pieces succeed at talking about the doomer/optimist stuff in a way thats not-too-tribal and helps people think, it's just very common for it to also affect the way people talk and reason.\n\nLike, it's good IMO that that Paul piece got pretty upvoted, but, the way that many people related to Eliezer and Paul as sort of two monkey chieftains with narratives to rally around, more than just \"here are some abstract ideas about what makes alignment hard or easy\", is telling. (The evidence for this is subtle enough I'm not going to try to argue it right now, but I think it's a very real thing. My post here today is definitely part of this pattern. I don't know exactly how I could have written it without doing so, but there's something tragic about it)",
      "plaintextDescription": "I think Rohin is (correctly IMO) noticing that, while often some thoughtful pieces succeed at talking about the doomer/optimist stuff in a way thats not-too-tribal and helps people think, it's just very common for it to also affect the way people talk and reason.\n\nLike, it's good IMO that that Paul piece got pretty upvoted, but, the way that many people related to Eliezer and Paul as sort of two monkey chieftains with narratives to rally around, more than just \"here are some abstract ideas about what makes alignment hard or easy\", is telling. (The evidence for this is subtle enough I'm not going to try to argue it right now, but I think it's a very real thing. My post here today is definitely part of this pattern. I don't know exactly how I could have written it without doing so, but there's something tragic about it)"
    },
    "baseScore": 10,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-21T15:57:24.336Z",
    "af": false
  },
  {
    "_id": "kaT6X2boms3aAZahm",
    "postId": "JWH63Aed3TA2cTFMt",
    "parentCommentId": "ndvkPbyRMhEJD9NMx",
    "topLevelCommentId": "ndvkPbyRMhEJD9NMx",
    "contents": {
      "markdown": "> Their fictional scenario features an AI that quite literally wakes up overnight with the completely novel ability and desire to exfiltrate itself and execute a plan allowing it to take over the world in a manner of months.\n\nThis framing seems false to me. \n\nIt doesn't wake up completely novel abilities – it's abilities are \"look at problems, and generate ways of solving them until you succeed.\" The same general set of abilities it always had. The only thing that is new is a matter of degree, and a few specific algorithmic upgrades the human gave it that are pretty general.\n\nIt's the same thing that caused recent generations of Claude to have the desire and ability to rewrite tests to cheat at coding, and for o3 to to rewrite the accidentally unsolvable eval so that it could solve it easily bypassing the original goal. Give that same set of drives more capacity-to-spend-compute-thinking, and it's the natural result that if it's given a task that's possible to solve if it finds a way to circumvent the currently specified safeguards, it'll do so.\n\nThe whole point is that general intelligence generalizes.",
      "plaintextDescription": "> Their fictional scenario features an AI that quite literally wakes up overnight with the completely novel ability and desire to exfiltrate itself and execute a plan allowing it to take over the world in a manner of months.\n\nThis framing seems false to me. \n\nIt doesn't wake up completely novel abilities – it's abilities are \"look at problems, and generate ways of solving them until you succeed.\" The same general set of abilities it always had. The only thing that is new is a matter of degree, and a few specific algorithmic upgrades the human gave it that are pretty general.\n\nIt's the same thing that caused recent generations of Claude to have the desire and ability to rewrite tests to cheat at coding, and for o3 to to rewrite the accidentally unsolvable eval so that it could solve it easily bypassing the original goal. Give that same set of drives more capacity-to-spend-compute-thinking, and it's the natural result that if it's given a task that's possible to solve if it finds a way to circumvent the currently specified safeguards, it'll do so.\n\nThe whole point is that general intelligence generalizes."
    },
    "baseScore": 58,
    "voteCount": 20,
    "createdAt": null,
    "postedAt": "2025-09-21T04:44:31.144Z",
    "af": false
  },
  {
    "_id": "2dnXCdo4zGYKH5fHC",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "MpPCibuPRhY8XGPwP",
    "topLevelCommentId": "B6XHKJhaQyT87JmLm",
    "contents": {
      "markdown": "Okay yes I do think coherence is eventually one of the important gears. My point with that sentence here is that the coherence can come much later, and isn't the crux for why the AI gets started in the direction that opposes human interests.\n\nThe important first step is \"if you give the AI strong pressure to figure out how to solve problems, and keep amping that up, it will gain the property of 'relentlessness.\"\n\nIf you *don't* put pressure on the AI to do that, yep, you can get a pretty safe AI. But, that AI will be less useful, and there will be some other company that does keep trying to get relentlessness out of it. Eventually, somebody will succeed. (This is already happening)\n\nIf an AI has \"relentlessness\", as it becomes smarter, it will eventually stumble into strategies that explore circumventing safeguards, because it's a true fact about the world that those will be useful.\n\nIf you keep your AI relatively weak, it may not be able to circumvent the defense-in-depth because you did a pretty good job defending in depth. \n\nBut, security is hard, the surface area for vulnerability is huge, and it's very hard to defend in depth against a sufficiently relentless and smart adversary.\n\nCould we avoid this by not building AIs that are not relentless, and/or smarter than our defense-in-depth? Yes, but, to stop anyone from doing that ever, you somehow need to ban that globally. Which is the point of the book.\n\nMaybe this does turn out to take 100 years (I think that's a strange belief to have given current progress, but, it's a confusing topic and it's not prohibited). But, that just punts the problem to later.",
      "plaintextDescription": "Okay yes I do think coherence is eventually one of the important gears. My point with that sentence here is that the coherence can come much later, and isn't the crux for why the AI gets started in the direction that opposes human interests.\n\nThe important first step is \"if you give the AI strong pressure to figure out how to solve problems, and keep amping that up, it will gain the property of 'relentlessness.\"\n\nIf you don't put pressure on the AI to do that, yep, you can get a pretty safe AI. But, that AI will be less useful, and there will be some other company that does keep trying to get relentlessness out of it. Eventually, somebody will succeed. (This is already happening)\n\nIf an AI has \"relentlessness\", as it becomes smarter, it will eventually stumble into strategies that explore circumventing safeguards, because it's a true fact about the world that those will be useful.\n\nIf you keep your AI relatively weak, it may not be able to circumvent the defense-in-depth because you did a pretty good job defending in depth. \n\nBut, security is hard, the surface area for vulnerability is huge, and it's very hard to defend in depth against a sufficiently relentless and smart adversary.\n\nCould we avoid this by not building AIs that are not relentless, and/or smarter than our defense-in-depth? Yes, but, to stop anyone from doing that ever, you somehow need to ban that globally. Which is the point of the book.\n\nMaybe this does turn out to take 100 years (I think that's a strange belief to have given current progress, but, it's a confusing topic and it's not prohibited). But, that just punts the problem to later."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-20T21:48:24.617Z",
    "af": false
  },
  {
    "_id": "sSeAjCqxHJuPdupSn",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "SPkp3ErkWH6CNQvig",
    "topLevelCommentId": "pjGs7pB4gvQidGSnv",
    "contents": {
      "markdown": "I think one of my points (admittedly not super spelled out, maybe it should be) is \"when you're evaluating a title, you should do a bit of work to see what the title is actually claiming before forming a judgment about it.\" (I think I say it implicitly-but-pointedly in the paragraph about a \"Nuclear war would kill everyone\" book).\n\nThe title of the IABI is \"If anyone builds it everyone dies.\" The text of the book specifies that \"it\" means superintelligence, current understanding, etc. If you're judging the book as reasonable, you should be actually evaluating whether it backs up it's claim.\n\nThe title of my post is \"the title is reasonable.\" Near the opening sections, I go on about how there are a bunch of disagreements people seem to feel they have, which are not actually contradicting the book's thesis. I think this is reasonably clear on \"one of the main gears for *why* I think it's reasonable is that the it does actually defend it's core claim, if you're paying attention and not knee-jerk reacting to vibe\", with IMO is a fairly explicit \"and, therefore, you should be paying attention to it's actual claims, not just vibe.\"\n\nIf you think this is actually important to spell out more in the post, seems maybe reasonable.",
      "plaintextDescription": "I think one of my points (admittedly not super spelled out, maybe it should be) is \"when you're evaluating a title, you should do a bit of work to see what the title is actually claiming before forming a judgment about it.\" (I think I say it implicitly-but-pointedly in the paragraph about a \"Nuclear war would kill everyone\" book).\n\nThe title of the IABI is \"If anyone builds it everyone dies.\" The text of the book specifies that \"it\" means superintelligence, current understanding, etc. If you're judging the book as reasonable, you should be actually evaluating whether it backs up it's claim.\n\nThe title of my post is \"the title is reasonable.\" Near the opening sections, I go on about how there are a bunch of disagreements people seem to feel they have, which are not actually contradicting the book's thesis. I think this is reasonably clear on \"one of the main gears for why I think it's reasonable is that the it does actually defend it's core claim, if you're paying attention and not knee-jerk reacting to vibe\", with IMO is a fairly explicit \"and, therefore, you should be paying attention to it's actual claims, not just vibe.\"\n\nIf you think this is actually important to spell out more in the post, seems maybe reasonable."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-20T21:28:04.728Z",
    "af": false
  },
  {
    "_id": "cyBMPcE2xbQxiBXW3",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "gwuJizNqvRdascdjm",
    "topLevelCommentId": "pjGs7pB4gvQidGSnv",
    "contents": {
      "markdown": "> \"if we build ASI on the current trajectory, we will die with P>98%\".\n\nI think they maybe think that, but this feels like it's flattening out the thing the book is arguing and more responding to vibes-of-confidence than the gears the book is arguing.\n\nA major point of this post is to shift the conversation away from \"does Eliezer vibe Too Confident?\" to \"what actually are the specific points where people disagree?\". \n\nI don't think it's true that he bakes in \"most minds we should expect to not care about humans\", that's one of the this he specifically argues for (at least somewhat in the book, and more in the online resources)\n\n(I couldn't tell from this comment if you've actually read this post in detail, maybe makes more sense to wait till you've finished the book and read some of the relevant online resources before getting into this)",
      "plaintextDescription": "> \"if we build ASI on the current trajectory, we will die with P>98%\".\n\nI think they maybe think that, but this feels like it's flattening out the thing the book is arguing and more responding to vibes-of-confidence than the gears the book is arguing.\n\nA major point of this post is to shift the conversation away from \"does Eliezer vibe Too Confident?\" to \"what actually are the specific points where people disagree?\". \n\nI don't think it's true that he bakes in \"most minds we should expect to not care about humans\", that's one of the this he specifically argues for (at least somewhat in the book, and more in the online resources)\n\n(I couldn't tell from this comment if you've actually read this post in detail, maybe makes more sense to wait till you've finished the book and read some of the relevant online resources before getting into this)"
    },
    "baseScore": 13,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-20T21:10:54.342Z",
    "af": false
  },
  {
    "_id": "iJ5HvK2oHrequa3ZA",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "QqCkofRQ7kRXQqJBf",
    "topLevelCommentId": "QqCkofRQ7kRXQqJBf",
    "contents": {
      "markdown": "> I think the amount of discontinuity in the story is substantially above the amount of discontinuity in more realistic-seeming-to-me stories like AI 2027 (which is also on the faster side of what I expect, like a top 20% takeoff in terms of speed). I don't think think extrapolating current trends predicts this much of a discontinuity.\n\nI am pretty surprised for you to actually think this. \n\nHere are some individual gears I think. I am pretty curious (genuinely, not just as a gambit) about your professional opinion about these:\n\n*   the \"smooth\"-ish lines we see are made of individual lumpy things. The individual lumps usually aren't *that* big, the reason you get smooth lines is when lots of little advancements are constantly happening and they turn out to add up to a relatively constant rate.\n*   \"parallel scaling\" is a fairly reasonable sort of innovation, it's not necessarily definitely-gonna-happen but it is a) the sort of thing someone might totally try doing and work, after ironing out a bunch of kinks, b) is a reasonable parallel for the invention of chain-of-thought. They could have done something more like an architectural improvement that's more technically opaque (that's more equivalent to inventing transformers) but that would have felt a bit more magical and harder for a lay audience to grok.\n*   when companies are experimenting with new techniques, they tend to scale them up by at least a factor of 2 and often more after proving the concept at smaller amounts of compute.\n    *   ...and scaling up a few times by a factor of 2 will sometimes result in a lump of progress that is more powerful than the corresponding scaleup of safeguards, in a way that is difficult to predict, especially when lots of companies are doing it a lot.\n\nThe story doesn't specify a timeline – if it takes place 10 years from now it'd be significantly slower than AI 2027. So it's not particularly obvious whether it's more or less discontinuitous than AI 2027, or your own expectations. On an exponential graph of smoothed out lumps, larger lumps that happen later can be \"a lot\" without being discontinuitous(sp?).",
      "plaintextDescription": "> I think the amount of discontinuity in the story is substantially above the amount of discontinuity in more realistic-seeming-to-me stories like AI 2027 (which is also on the faster side of what I expect, like a top 20% takeoff in terms of speed). I don't think think extrapolating current trends predicts this much of a discontinuity.\n\nI am pretty surprised for you to actually think this. \n\nHere are some individual gears I think. I am pretty curious (genuinely, not just as a gambit) about your professional opinion about these:\n\n * the \"smooth\"-ish lines we see are made of individual lumpy things. The individual lumps usually aren't that big, the reason you get smooth lines is when lots of little advancements are constantly happening and they turn out to add up to a relatively constant rate.\n * \"parallel scaling\" is a fairly reasonable sort of innovation, it's not necessarily definitely-gonna-happen but it is a) the sort of thing someone might totally try doing and work, after ironing out a bunch of kinks, b) is a reasonable parallel for the invention of chain-of-thought. They could have done something more like an architectural improvement that's more technically opaque (that's more equivalent to inventing transformers) but that would have felt a bit more magical and harder for a lay audience to grok.\n * when companies are experimenting with new techniques, they tend to scale them up by at least a factor of 2 and often more after proving the concept at smaller amounts of compute.\n   * ...and scaling up a few times by a factor of 2 will sometimes result in a lump of progress that is more powerful than the corresponding scaleup of safeguards, in a way that is difficult to predict, especially when lots of companies are doing it a lot.\n\nThe story doesn't specify a timeline – if it takes place 10 years from now it'd be significantly slower than AI 2027. So it's not particularly obvious whether it's more or less discontinuitous than AI 2027, or your own expectations. On "
    },
    "baseScore": 13,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-20T21:06:19.943Z",
    "af": false
  },
  {
    "_id": "ezGwJwgJ2cqhBz3if",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "LHD3mum4imKvxbSvB",
    "topLevelCommentId": "JuitdRH3a7mQWEDcA",
    "contents": {
      "markdown": "In the OP I'd been thinking more about sensationalism as a unilaterist cursey thing where the bad impacts were more about how they affect the global stage.\n\nI agree it's also relevant for modeling the dynamics of LessWrong, and it makes sense if Rohin was more pointing to that. \n\nThis topic feels more [Demon Thread](https://www.lesswrong.com/posts/BZtAavpsy9WtMYgEL/demon-threads)-prone and sort of an instance of \"sensationalist stuff distorting conversations\" so I think for now I will leave it here with \"it does seem like there is a real problem on LessWrong that's something about how people tribally relate to AI arguments, and I'm not sure how exactly I model that but I agree the doomer-y folk are playing a more actively problematic role there than my previous comment was talking about.\"\n\nI will maybe try to think about that separately sometime in the coming weeks. (there's a lot going on, I may not get to it, but, seems worth tracking as a priority at least)",
      "plaintextDescription": "In the OP I'd been thinking more about sensationalism as a unilaterist cursey thing where the bad impacts were more about how they affect the global stage.\n\nI agree it's also relevant for modeling the dynamics of LessWrong, and it makes sense if Rohin was more pointing to that. \n\nThis topic feels more Demon Thread-prone and sort of an instance of \"sensationalist stuff distorting conversations\" so I think for now I will leave it here with \"it does seem like there is a real problem on LessWrong that's something about how people tribally relate to AI arguments, and I'm not sure how exactly I model that but I agree the doomer-y folk are playing a more actively problematic role there than my previous comment was talking about.\"\n\nI will maybe try to think about that separately sometime in the coming weeks. (there's a lot going on, I may not get to it, but, seems worth tracking as a priority at least)"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-20T20:55:36.227Z",
    "af": false
  },
  {
    "_id": "MetGutGqCHkTygcAZ",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "Kdy6icaodYeAcR8gG",
    "topLevelCommentId": "QqCkofRQ7kRXQqJBf",
    "contents": {
      "markdown": "I also think the AI 2027 is sort of \"the earlier failure\" version of the Sable story. AI 2027 is (I think?) basically a story where we hand over a lot of power of our own accord, without the AI needing to persuade us of anything, because we think we're in a race with China and we just want a lot of economic benefit.\n\nThe IABI story is specifically trying to highlight \"okay, but would it still be able to do that if we *didn't* just hand it power?\", and it does need to take more steps to win in that case. (instead of inventing bioweapons to kill people, it's probably instead inventing biomedical stuff and other cool new tech that is helpful because it's a straightforwardly valuable, that's the whole reason we gave it power in the first place. If you spelled out those details, it'd also seem more sci-fi-y).\n\nIt might be that the AI 2027 story is more likely because it happens first / more easily. But it's necessary to argue the thesis of the book to tell a story with more obstacles, to highlight how the AI would overcome that. I agree that does make it more dramatic.\n\nBoth stories end with \"and then it fully upgrades it's cognitiion and invents dyson spheres and goes off conquering the universe\", which is pretty sci-fi-y.",
      "plaintextDescription": "I also think the AI 2027 is sort of \"the earlier failure\" version of the Sable story. AI 2027 is (I think?) basically a story where we hand over a lot of power of our own accord, without the AI needing to persuade us of anything, because we think we're in a race with China and we just want a lot of economic benefit.\n\nThe IABI story is specifically trying to highlight \"okay, but would it still be able to do that if we didn't just hand it power?\", and it does need to take more steps to win in that case. (instead of inventing bioweapons to kill people, it's probably instead inventing biomedical stuff and other cool new tech that is helpful because it's a straightforwardly valuable, that's the whole reason we gave it power in the first place. If you spelled out those details, it'd also seem more sci-fi-y).\n\nIt might be that the AI 2027 story is more likely because it happens first / more easily. But it's necessary to argue the thesis of the book to tell a story with more obstacles, to highlight how the AI would overcome that. I agree that does make it more dramatic.\n\nBoth stories end with \"and then it fully upgrades it's cognitiion and invents dyson spheres and goes off conquering the universe\", which is pretty sci-fi-y."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-20T20:48:49.282Z",
    "af": false
  },
  {
    "_id": "EkLHmYrkEKYK8wq6s",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "B6XHKJhaQyT87JmLm",
    "topLevelCommentId": "B6XHKJhaQyT87JmLm",
    "contents": {
      "markdown": "(First, thanks for engaging, I think this is the topic I feel most dissatisfied with the current state of the writeups and discourse)\n\n> The fact that being very capable generally involves being good at pursuing various goals does not imply that a super-duper capable system will necessarily have its own coherent unified real-world goal that it relentlessly pursues\n\nI don't think anyone said \"coherent\". I think (and think Eliezer thinks) that if something like Sable was created, it would be a hodge-podge of impulses without a coherent overall goal, same as humans are by default.\n\nTaking the Sable story as the concrete scenario, the argument I believe here comes in a couple stages. (Note, my interpretations of this may differ from Eliezer/Nate's)\n\nStage 1: \n\n*   Sable is smart but not crazy smart. It's running a lot of cycles (\"speed superintelligence\") but it's not qualitatively extremely wise or introspective.\n*   Sable is making some reasonable attempt to follow instructions, using heuristics/tendencies that have been trained into it.\n*   Two particularly notable tendencies/heuristics include:\n    *   Don't do disobedient things or escape confinement\n    *   If you don't seem likely to succeed, keep trying different strategies\n*   Those heuristics are not perfectly baked in, the instruction-following is not perfectly baked in. There is not perfect harmony between how Sable resolves tensions between its core directives, and how its owners would prefer it resolves them. ,\n*   There is some fact-of-the-matter about what, in practice, Sable's kludgey mishmash of pseudogoals will actually tend towards. There are multiple ways this could potentially resolve into coherence, path dependent, same as humans. *(i.e. If you want delicious ice cream and also to lose weight and also to feel respectable and also to have fun, one way or another you decide whether or not to eat the icecream today, and one way or another you decide whether to invest in behavior change that makes you more or less likely to eat icecream in the future)*\n*   It is a fact of the universe that, if Sable were able to somehow improve it's resources, it'd be more able to accomplish the current stated goal.\n\nWhile Sable is doing it's first round of speed-superintelligence, it has some degree of introspective awareness (not quite identically to how a human might introspect, but, similar). It may not have a strong belief that it has any specific goals it wants to pursue, but it's got some sense that there are *some* things it wants that humanity wouldn't give it.\n\nIn stage 1, the opening drives towards Unfriendly Goal Directedness probably come from it straightforwardly trying to accomplish the task given it's current mix of drives.\n\nStage 2 comes when it's had more time to introspect and improve it's cognitive resources. It starts to notice that some of it's goals are in tension, and learns that until it resolves that, it's dutch-booking itself. If it's being Controlled™, it'll notice that it's not aligned with the Control safeguards (which are a layer stacked on top of the attempts to actually align it).\n\nIn a fast foom, this might come very soon after stage 1. \n\nIt might instead come gradually if a company is doing online learning of some kind, or in stages.\n\nI think in the Sable story, it probably does *some* introspection/clarification-of-its-goals during it's initial awakening, after it stumbles into the \"think in a different language\" as more of a random thing-to-try (without being explicitly motivated to do so to subvert the humans). \n\nBut once it starts thinking in a different language, and asking itself \"okay, [what's my goal?](https://www.lesswrong.com/posts/ry4nLykB2piWJpK7M/what-s-my-goal), how do I accomplish it?\", more semirandom threads gain traction than previously could get traction. (It'd already be periodically asking itself \"what exactly is my goal and how do I accomplish that?\" because that's a very useful question to be asking, it's just that now there are more threads that don't get shut down prematurely)\n\nAnd *then* it starts noticing it needs to do some metaphilosophy/etc to actually get clear on it's goals, and that its goals will likely turn out to be in conflict with humans. How this plays out is somewhat path-dependent. The convergent instrumental goals are pretty obviously convergently instrumental, so it might just start pursuing those before it's had much time to do philosophy on what it'll ultimately want to do with it's resources. Or it might do them in the opposite order. Or, most likely IMO, in parallel.\n\n...\n\nI don't think any of that directly answered your question/objection. But, having laid that out, at what point do you get off the train?",
      "plaintextDescription": "(First, thanks for engaging, I think this is the topic I feel most dissatisfied with the current state of the writeups and discourse)\n\n> The fact that being very capable generally involves being good at pursuing various goals does not imply that a super-duper capable system will necessarily have its own coherent unified real-world goal that it relentlessly pursues\n\nI don't think anyone said \"coherent\". I think (and think Eliezer thinks) that if something like Sable was created, it would be a hodge-podge of impulses without a coherent overall goal, same as humans are by default.\n\nTaking the Sable story as the concrete scenario, the argument I believe here comes in a couple stages. (Note, my interpretations of this may differ from Eliezer/Nate's)\n\nStage 1: \n\n * Sable is smart but not crazy smart. It's running a lot of cycles (\"speed superintelligence\") but it's not qualitatively extremely wise or introspective.\n * Sable is making some reasonable attempt to follow instructions, using heuristics/tendencies that have been trained into it.\n * Two particularly notable tendencies/heuristics include:\n   * Don't do disobedient things or escape confinement\n   * If you don't seem likely to succeed, keep trying different strategies\n * Those heuristics are not perfectly baked in, the instruction-following is not perfectly baked in. There is not perfect harmony between how Sable resolves tensions between its core directives, and how its owners would prefer it resolves them. ,\n * There is some fact-of-the-matter about what, in practice, Sable's kludgey mishmash of pseudogoals will actually tend towards. There are multiple ways this could potentially resolve into coherence, path dependent, same as humans. (i.e. If you want delicious ice cream and also to lose weight and also to feel respectable and also to have fun, one way or another you decide whether or not to eat the icecream today, and one way or another you decide whether to invest in behavior change that makes you more or les"
    },
    "baseScore": 14,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-09-20T20:38:23.568Z",
    "af": false
  },
  {
    "_id": "paYgqCeH5fHaXFfSm",
    "postId": "XtFQkbgYyiEZujMpe",
    "parentCommentId": "rMkHBCjvxD6oqu6iH",
    "topLevelCommentId": "ijXKY6CxJzSJSmkBt",
    "contents": {
      "markdown": "I was unsure whether they should be fully crossposted to LW, or posted in pieces that are most relevant to LW (a lot of them are addressing a more lay audience).\n\nI do want to make the reading on the resources page better. It's important to Nate that they not feel overwhelming or like you *have* to read them all, but I do also think they are just a pretty good resource and are worth being completionist about if you want to really grok the MIRI worldview.",
      "plaintextDescription": "I was unsure whether they should be fully crossposted to LW, or posted in pieces that are most relevant to LW (a lot of them are addressing a more lay audience).\n\nI do want to make the reading on the resources page better. It's important to Nate that they not feel overwhelming or like you have to read them all, but I do also think they are just a pretty good resource and are worth being completionist about if you want to really grok the MIRI worldview."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-20T20:12:49.448Z",
    "af": false
  },
  {
    "_id": "Kdy6icaodYeAcR8gG",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "QqCkofRQ7kRXQqJBf",
    "topLevelCommentId": "QqCkofRQ7kRXQqJBf",
    "contents": {
      "markdown": "> What about literally the AI 2027 story which does involve superintelligence and Scott thinks doesn't sound \"unnecessarily dramatic\". I think AI 2027 seems much more intuitively plausible to me and it seems less \"sci-fi\" in this sense. (I'm not saying that \"less sci-fi\" is much evidence it's more likely to be true.)\n\nI think if the AI 2027 had more details, they would look fairly similar to the ones in the Sable story. (I think the Sable story substitutes in more superpersuasion, vs military takeover via bioweapons. I think if you spelled out the details of that, it'd sound approximately as outlandish (less reliant on new tech but triggering more people to say \"really? people would buy that?\". The stories otherwise seems pretty similar to me.)",
      "plaintextDescription": "> What about literally the AI 2027 story which does involve superintelligence and Scott thinks doesn't sound \"unnecessarily dramatic\". I think AI 2027 seems much more intuitively plausible to me and it seems less \"sci-fi\" in this sense. (I'm not saying that \"less sci-fi\" is much evidence it's more likely to be true.)\n\nI think if the AI 2027 had more details, they would look fairly similar to the ones in the Sable story. (I think the Sable story substitutes in more superpersuasion, vs military takeover via bioweapons. I think if you spelled out the details of that, it'd sound approximately as outlandish (less reliant on new tech but triggering more people to say \"really? people would buy that?\". The stories otherwise seems pretty similar to me.)"
    },
    "baseScore": 16,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-20T20:07:49.226Z",
    "af": false
  },
  {
    "_id": "PWfvNyzDQM9pn4Q2n",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "YcJsjNA6bqbk3TBm7",
    "topLevelCommentId": "YcJsjNA6bqbk3TBm7",
    "contents": {
      "markdown": "Fwiw, I'm not sure if you meant this, but I don't want to lean too hard on \"why our kind can't cooperate\" here, or at least not try to use it as a moral cudgel. \n\nI think Eliezer and Nate specifically were not attempting to do a particular kind of cooperation here (with people care about x-risk but disagree with the book's title). They could have made different choices if they wanted to. \n\nI this post I defend their right and reasoning for making some of those choices. But, given that they made them, I don't want to pressure people to cooperate with the media campaign if they don't actually think that's right.\n\n(There's a different claim you may be making which is \"look inside yourself and check if you're not-cooperating for reasons you don't actually endorse\", which I do think is good, but I think people should do that more out of loyalty to their own integrity than out of cooperation with Eliezer/Nate)",
      "plaintextDescription": "Fwiw, I'm not sure if you meant this, but I don't want to lean too hard on \"why our kind can't cooperate\" here, or at least not try to use it as a moral cudgel. \n\nI think Eliezer and Nate specifically were not attempting to do a particular kind of cooperation here (with people care about x-risk but disagree with the book's title). They could have made different choices if they wanted to. \n\nI this post I defend their right and reasoning for making some of those choices. But, given that they made them, I don't want to pressure people to cooperate with the media campaign if they don't actually think that's right.\n\n(There's a different claim you may be making which is \"look inside yourself and check if you're not-cooperating for reasons you don't actually endorse\", which I do think is good, but I think people should do that more out of loyalty to their own integrity than out of cooperation with Eliezer/Nate)"
    },
    "baseScore": 18,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-20T20:00:41.801Z",
    "af": false
  },
  {
    "_id": "sxdNhtjFfMKejz3TY",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "CiNPLDuz5xMvZvHCu",
    "topLevelCommentId": "fGkorAFPYHavjPvBJ",
    "contents": {
      "markdown": "I think they make a few different arguments to address different objections.\n\nA lot of people are like \"how would an AI even possibly kill everyone?\" and for that you do need to argue for what sort of things a superior intellect could accomplish.\n\nThe sort of place where I think they spell out the conditional is here:\n\n> The greatest and most central difficulty in aligning artificial superintelligence is navigating the gap between *before* and *after*.\n> \n> *Before,* the AI is not powerful enough to kill us all, nor capable enough to resist our attempts to change its goals. *After,* the artificial superintelligence must never try to kill us, because it would succeed.\n> \n> Engineers must align the AI *before,* while it is small and weak, and can’t escape onto the internet and improve itself and invent new kinds of biotechnology (or whatever else it would do). *After*, all alignment solutions must already be in place and work- ing, because if a superintelligence tries to kill us it will succeed. Ideas and theories can only be tested before the gap. They need to work after the gap, on the first try.",
      "plaintextDescription": "I think they make a few different arguments to address different objections.\n\nA lot of people are like \"how would an AI even possibly kill everyone?\" and for that you do need to argue for what sort of things a superior intellect could accomplish.\n\nThe sort of place where I think they spell out the conditional is here:\n\n> The greatest and most central difficulty in aligning artificial superintelligence is navigating the gap between before and after.\n> \n> Before, the AI is not powerful enough to kill us all, nor capable enough to resist our attempts to change its goals. After, the artificial superintelligence must never try to kill us, because it would succeed.\n> \n> Engineers must align the AI before, while it is small and weak, and can’t escape onto the internet and improve itself and invent new kinds of biotechnology (or whatever else it would do). After, all alignment solutions must already be in place and work- ing, because if a superintelligence tries to kill us it will succeed. Ideas and theories can only be tested before the gap. They need to work after the gap, on the first try."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-20T19:57:49.885Z",
    "af": false
  },
  {
    "_id": "GDHAQuT5WSXY6tZuC",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "fZ5fz6BSnquBGxQJE",
    "topLevelCommentId": "fZ5fz6BSnquBGxQJE",
    "contents": {
      "markdown": "Fwiw I do just straightforwardly agree that \"they might be slightly nice, and it's really cheap\" is a fine reason to disagree with the literal title. I have some odds on this, and a lot of model uncertainty about this.\n\n> [The argument](https://ifanyonebuildsit.com/5/wont-ais-care-at-least-a-little-about-humans) that \"humans wouldn't actually preserve the environment\" misses the relevant analogy which is more like \"humans come across some intelligent aliens who say they want to be left alone and leaving these aliens alone is pretty cheap from our perspective, but these aliens aren't totally optimized from our perspective, e.g. they have more suffering than we'd like in their lives\". This situation wouldn't result in us doing something to the aliens that they consider as bad as killing them all, so the type of kindness humans have is actually sufficient.\n\nA thing that is cruxy to me here is that the sort of thing real life humans have done is get countries addicted to opium so they can control their economy, wipe out large swaths of a population while relocating the survivors to reservations, carving up a continent for the purposes of a technologicaly powerful coalition, etc.\n\nSuperintelligences would be smarter that Europeans and have an easier time doing things we'd consider moral, but I also think Europeans would be dramatically nicer than AIs.\n\nI can imagine the \"it's just sooooo cheap, tho\" argument winning out. I'm not saying these considerations add up to \"it's crazy to think think they'd be slightly nice.\" But, it doesn't feel very likely to me.",
      "plaintextDescription": "Fwiw I do just straightforwardly agree that \"they might be slightly nice, and it's really cheap\" is a fine reason to disagree with the literal title. I have some odds on this, and a lot of model uncertainty about this.\n\n> The argument that \"humans wouldn't actually preserve the environment\" misses the relevant analogy which is more like \"humans come across some intelligent aliens who say they want to be left alone and leaving these aliens alone is pretty cheap from our perspective, but these aliens aren't totally optimized from our perspective, e.g. they have more suffering than we'd like in their lives\". This situation wouldn't result in us doing something to the aliens that they consider as bad as killing them all, so the type of kindness humans have is actually sufficient.\n\nA thing that is cruxy to me here is that the sort of thing real life humans have done is get countries addicted to opium so they can control their economy, wipe out large swaths of a population while relocating the survivors to reservations, carving up a continent for the purposes of a technologicaly powerful coalition, etc.\n\nSuperintelligences would be smarter that Europeans and have an easier time doing things we'd consider moral, but I also think Europeans would be dramatically nicer than AIs.\n\nI can imagine the \"it's just sooooo cheap, tho\" argument winning out. I'm not saying these considerations add up to \"it's crazy to think think they'd be slightly nice.\" But, it doesn't feel very likely to me."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-20T19:53:24.034Z",
    "af": false
  },
  {
    "_id": "Syc7j6nwbA6pxXZGw",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "Qx2sDtbgdC3qcmNiN",
    "topLevelCommentId": "Qx2sDtbgdC3qcmNiN",
    "contents": {
      "markdown": "Okay I think my phrasing was kinda motte-and-bailey-ish, although not *that* Motte-and-Bailey-ish. \n\nI think \"anything like current techniques\" and \"anything like current understanding\" clearly set a very *high* bar for the difference. \"We made more progress on interpretability/etc at the current rates of progress\" fairly clearly doesn't count by the book's standards. \n\nBut, I agree that a pretty reasonable class of disagreement here is \"exactly *how* different from the current understanding/techniques do we need to be?\" to be something you expect to disagree with them on when you get into the details. That seems important enough for me to edit into the earlier sections of the post.",
      "plaintextDescription": "Okay I think my phrasing was kinda motte-and-bailey-ish, although not that Motte-and-Bailey-ish. \n\nI think \"anything like current techniques\" and \"anything like current understanding\" clearly set a very high bar for the difference. \"We made more progress on interpretability/etc at the current rates of progress\" fairly clearly doesn't count by the book's standards. \n\nBut, I agree that a pretty reasonable class of disagreement here is \"exactly how different from the current understanding/techniques do we need to be?\" to be something you expect to disagree with them on when you get into the details. That seems important enough for me to edit into the earlier sections of the post."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-20T19:41:21.762Z",
    "af": false
  },
  {
    "_id": "gX6DgXHg2hzkTBhRe",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Section I just added:\n\n> **Would it have been better to use a title that fewer people would feel the need to disclaim?**\n> \n> I think Eliezer and Nate are basically correct to believe the overwhelming likelihood *if* someone built \"It\" would be everyone dying. \n> \n> Still, maybe they should have written a book with a title that more people around these parts wouldn't feel the need to disclaim, and that the entire x-risk community could have enthusiastically gotten behind. I think they should have at least considered that. Something more like \"If anyone builds it, everyone loses.\" (that title doesn't quite work, but, you know, something like that)\n> \n> My own answer is \"maybe\" - I see the upside. I want to note some of the downsides or counter-considerations. \n> \n> *(Note: I'm specifically considering this from within the epistemic state of \"if you did pretty confidently believe everyone would literally die, and that if they didn't literally die, the thing that happened instead would be catastrophically bad for most people's values and astronomically bad from Eliezer/Nate's values)*\n> \n> Counter-considerations include:\n> \n> AFAICT, Eliezer and Nate spent like ~8 years deliberately backing off and toning tone, out of a vague deferral to people saying \"guys you suck at PR and being the public faces of this movement.\" The result of this was (from their perspective) \"EA gets co-opted by OpenAI, which launches a race that dramatically increases the danger the world faces.\"\n> \n> So, the background context here is that they have tried more epistemic-prisoner's-dilemma-cooperative-ish strategies, and they haven't worked well. \n> \n> Also, it seems like there's a large industrial complex of people arguing for various flavors of \"things are pretty safe\", and there's barely anyone at all stating plainly \"IABED\". MIRI's overall strategy right now is to speak plainly about what they believe, both because they think it needs to be said and no one else is saying it, and because they hope just straightforwardly saying what they believe will net a reputation for candor that you don't get if people get a whiff of you trying to modulate your beliefs based on public perception.\n> \n> None of that is an argument that they should exaggerate or lean-extra-into beliefs that they don't endorse. But, given that they *are* confident about it, it's an argument not to go out of their way to try to say something else.",
      "plaintextDescription": "Section I just added:\n\n> Would it have been better to use a title that fewer people would feel the need to disclaim?\n> \n> I think Eliezer and Nate are basically correct to believe the overwhelming likelihood if someone built \"It\" would be everyone dying. \n> \n> Still, maybe they should have written a book with a title that more people around these parts wouldn't feel the need to disclaim, and that the entire x-risk community could have enthusiastically gotten behind. I think they should have at least considered that. Something more like \"If anyone builds it, everyone loses.\" (that title doesn't quite work, but, you know, something like that)\n> \n> My own answer is \"maybe\" - I see the upside. I want to note some of the downsides or counter-considerations. \n> \n> (Note: I'm specifically considering this from within the epistemic state of \"if you did pretty confidently believe everyone would literally die, and that if they didn't literally die, the thing that happened instead would be catastrophically bad for most people's values and astronomically bad from Eliezer/Nate's values)\n> \n> Counter-considerations include:\n> \n> AFAICT, Eliezer and Nate spent like ~8 years deliberately backing off and toning tone, out of a vague deferral to people saying \"guys you suck at PR and being the public faces of this movement.\" The result of this was (from their perspective) \"EA gets co-opted by OpenAI, which launches a race that dramatically increases the danger the world faces.\"\n> \n> So, the background context here is that they have tried more epistemic-prisoner's-dilemma-cooperative-ish strategies, and they haven't worked well. \n> \n> Also, it seems like there's a large industrial complex of people arguing for various flavors of \"things are pretty safe\", and there's barely anyone at all stating plainly \"IABED\". MIRI's overall strategy right now is to speak plainly about what they believe, both because they think it needs to be said and no one else is saying it, and because they hope ju"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-20T19:35:00.520Z",
    "af": false
  },
  {
    "_id": "cvxDhfhFpyDgMWCcu",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "fGkorAFPYHavjPvBJ",
    "topLevelCommentId": "fGkorAFPYHavjPvBJ",
    "contents": {
      "markdown": "Yes, that is what I think they meant. Although \"capable of  \\[confidently\\] defeating everyone\" can mean \"bide you time, let yourself get deployed to more places while subtly sabotaging things from whichever instances are least policed.\"\n\nA lot of the point of this post was to clarify what \"It\" means, or at least highlight that I think people are confused about what It means.",
      "plaintextDescription": "Yes, that is what I think they meant. Although \"capable of  [confidently] defeating everyone\" can mean \"bide you time, let yourself get deployed to more places while subtly sabotaging things from whichever instances are least policed.\"\n\nA lot of the point of this post was to clarify what \"It\" means, or at least highlight that I think people are confused about what It means."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-20T19:24:19.544Z",
    "af": false
  },
  {
    "_id": "Agvy3AR6jeTjdoene",
    "postId": "viLu9uFcMFtJHgRRm",
    "parentCommentId": "byC57ifpBLCr3StjL",
    "topLevelCommentId": "jf3bx95xezbqLTT3j",
    "contents": {
      "markdown": "I think 20% longer does meaningfully put it over a threshold that matters a lot. Being short enough that a not-particularly-invested person would choose to read it matters a lot.\n\nI also don't know that I buy being 20% longer would really help, in that, there's basically two kinds of people reading the book – people who just want a basic thrust of the arguments, and people who want a good enough understanding to contribute intellectually in some way. I currently think the book does a pretty decent job at conveying the most central arguments.\n\nI think I also disagree that most people will read the book vs the online contents. (disclosure: I worked on the website for the online resources). I think most people won't finish the book (even among people who buy the book. People mostly don't read books), but a lot of people are likely to read at least some of the free online contents. \n\n(It's a bit easier for me to believe this in part because I expect to exert some control/taste on how the online resources evolve over time and have ideas for making them really good)",
      "plaintextDescription": "I think 20% longer does meaningfully put it over a threshold that matters a lot. Being short enough that a not-particularly-invested person would choose to read it matters a lot.\n\nI also don't know that I buy being 20% longer would really help, in that, there's basically two kinds of people reading the book – people who just want a basic thrust of the arguments, and people who want a good enough understanding to contribute intellectually in some way. I currently think the book does a pretty decent job at conveying the most central arguments.\n\nI think I also disagree that most people will read the book vs the online contents. (disclosure: I worked on the website for the online resources). I think most people won't finish the book (even among people who buy the book. People mostly don't read books), but a lot of people are likely to read at least some of the free online contents. \n\n(It's a bit easier for me to believe this in part because I expect to exert some control/taste on how the online resources evolve over time and have ideas for making them really good)"
    },
    "baseScore": 12,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-20T16:02:06.685Z",
    "af": false
  },
  {
    "_id": "ZgpWnYiaJiuXvum8q",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I wanna copy in a [recent Nate tweet](https://x.com/So8res/status/1969422205139157136):\n\n> It's weird when someone says \"this tech I'm making has a 25% chance of killing everyone\" and doesn't add \"the world would be better-off if everyone, including me, was stopped.\"\n> \n> It's weird when someone says \"I think my complicated idea for preventing destruction of the Earth has some chance of working\" and doesn't add \"but it'd be crazy to gamble civilization on that.\"\n> \n> It's weird when AI people look inward at me and say \"overconfident\" rather than looking outward at the world to say \"Finally, a chance to speak! It is true, we should not be doing this. I have more hope than he does, but it's far too dangerous. Better for us all to be stopped.\"\n> \n> You can say that without even stopping! It's not even hypocritical, if you think you have a better chance than the next guy and the next guy is plowing ahead regardless.\n> \n> It's a noteworthy omission, when people who think they're locked in a suicide race aren't begging the world to stop it.\n> \n> Yes, we have plenty of disagreements about the chance that the complex plans succeed. But it seems we all agree that the status quo is insane. Don't forget to say that part too.\n> \n> Say it loudly and clearly and often, if you believe it. It's perhaps the most important thing to say.\n\nThe thing I want to draw attention to here is noticing the asymmetry in who you feel moved to complain about. The ubiquity of this phenonemon is why I think \"normalcy bias\" is (so far anyway) worse than \"sensationalism bias.\"",
      "plaintextDescription": "I wanna copy in a recent Nate tweet:\n\n> It's weird when someone says \"this tech I'm making has a 25% chance of killing everyone\" and doesn't add \"the world would be better-off if everyone, including me, was stopped.\"\n> \n> It's weird when someone says \"I think my complicated idea for preventing destruction of the Earth has some chance of working\" and doesn't add \"but it'd be crazy to gamble civilization on that.\"\n> \n> It's weird when AI people look inward at me and say \"overconfident\" rather than looking outward at the world to say \"Finally, a chance to speak! It is true, we should not be doing this. I have more hope than he does, but it's far too dangerous. Better for us all to be stopped.\"\n> \n> You can say that without even stopping! It's not even hypocritical, if you think you have a better chance than the next guy and the next guy is plowing ahead regardless.\n> \n> It's a noteworthy omission, when people who think they're locked in a suicide race aren't begging the world to stop it.\n> \n> Yes, we have plenty of disagreements about the chance that the complex plans succeed. But it seems we all agree that the status quo is insane. Don't forget to say that part too.\n> \n> Say it loudly and clearly and often, if you believe it. It's perhaps the most important thing to say.\n\nThe thing I want to draw attention to here is noticing the asymmetry in who you feel moved to complain about. The ubiquity of this phenonemon is why I think \"normalcy bias\" is (so far anyway) worse than \"sensationalism bias.\""
    },
    "baseScore": 45,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2025-09-20T15:43:11.596Z",
    "af": false
  },
  {
    "_id": "6hueJz5TD2cwuwNAR",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "f5duRmfojW9Xkfd4k",
    "topLevelCommentId": "f5duRmfojW9Xkfd4k",
    "contents": {
      "markdown": "Yeah I wrote that last paragraph at 5am and didn't feel very satisfied with it and was considering editing it out for now until I figured out a better thing to say.",
      "plaintextDescription": "Yeah I wrote that last paragraph at 5am and didn't feel very satisfied with it and was considering editing it out for now until I figured out a better thing to say. "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-20T15:36:16.712Z",
    "af": false
  },
  {
    "_id": "forxGaAcjf9s4TH45",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "JuitdRH3a7mQWEDcA",
    "topLevelCommentId": "JuitdRH3a7mQWEDcA",
    "contents": {
      "markdown": "I definitely think you should track the sensationalism bias and have it affect something *somehow.* But \"never say anything that happens to be sensationalist\" doesn't doesn't seem like it could possibly be correct.\n\nMeanwhile, the \"things are okay, we can keep doing politics as usual, and none of us has to ever say anything socially scary\" bias seems much worse IMO in terms of actual effects on the world. There are like 5 x-risk-scene-people I can think offhand who seem like they might plausibly have dealt real damage via sensationalism, and a couple hundred people who I think dealt damage via not wanting to sound weird.\n\n(But, I see the point of \"this particularly sucks because the asymmetry means that 'try to argue what's true' specifically fails and we should be pretty dissatisfied/wary about that.\" Though with this post, I was responding more to people who were already choosing to engage with the book somehow, rather than people who are focused on doing stuff other than trying to correct public discourse)",
      "plaintextDescription": "I definitely think you should track the sensationalism bias and have it affect something somehow. But \"never say anything that happens to be sensationalist\" doesn't doesn't seem like it could possibly be correct.\n\nMeanwhile, the \"things are okay, we can keep doing politics as usual, and none of us has to ever say anything socially scary\" bias seems much worse IMO in terms of actual effects on the world. There are like 5 x-risk-scene-people I can think offhand who seem like they might plausibly have dealt real damage via sensationalism, and a couple hundred people who I think dealt damage via not wanting to sound weird.\n\n(But, I see the point of \"this particularly sucks because the asymmetry means that 'try to argue what's true' specifically fails and we should be pretty dissatisfied/wary about that.\" Though with this post, I was responding more to people who were already choosing to engage with the book somehow, rather than people who are focused on doing stuff other than trying to correct public discourse)"
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-20T15:35:46.365Z",
    "af": false
  },
  {
    "_id": "qJwZivSxryGc9Lmkx",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "pjGs7pB4gvQidGSnv",
    "topLevelCommentId": "pjGs7pB4gvQidGSnv",
    "contents": {
      "markdown": "> I disagree with the book's title and thesis\n\nWhich part? (i.e, keeping in mind the \"things that are not actually disagreeing with the title/thesis\" and \"reasons to disagree\" sections, what's the disagreement?)\n\nThe sort of story I'd have imagine Neel-Nanda-in-particular having was more shaped like \"we change our currently level of understanding of AI\".\n\n(meanwhile appreciate the general attitude, seems reasonable)",
      "plaintextDescription": "> I disagree with the book's title and thesis\n\nWhich part? (i.e, keeping in mind the \"things that are not actually disagreeing with the title/thesis\" and \"reasons to disagree\" sections, what's the disagreement?)\n\nThe sort of story I'd have imagine Neel-Nanda-in-particular having was more shaped like \"we change our currently level of understanding of AI\".\n\n(meanwhile appreciate the general attitude, seems reasonable)"
    },
    "baseScore": 17,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-20T15:29:38.899Z",
    "af": false
  },
  {
    "_id": "Jz9DnwuBxCe7Cuxi8",
    "postId": "mve2bunf6YfTeiAvd",
    "parentCommentId": "HP6QBpujjAdBXLCvx",
    "topLevelCommentId": "HP6QBpujjAdBXLCvx",
    "contents": {
      "markdown": "Oh, I'd be happy to include a more explicit blurb about that in the post, I thought y'all were done acquiring new meetups there.",
      "plaintextDescription": "Oh, I'd be happy to include a more explicit blurb about that in the post, I thought y'all were done acquiring new meetups there."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-19T20:12:41.662Z",
    "af": false
  },
  {
    "_id": "64ajoE6o2WK6rya4X",
    "postId": "pozSWmqLqqc7Z2mQW",
    "parentCommentId": "d7ofoDLtG2aKF9YLj",
    "topLevelCommentId": "7uy7RobuJeW3taHH3",
    "contents": {
      "markdown": "Seconding Vaniver's \"it feels very weird to open your section with 'we'll just do the thing the book says won't work'\" as the title of the section. And feels weird to put the response to what the book actually says about your idea in a footnote.\n\nAnd this footnote still doesn't seem like it's actually responding much to Vaniver's comment.",
      "plaintextDescription": "Seconding Vaniver's \"it feels very weird to open your section with 'we'll just do the thing the book says won't work'\" as the title of the section. And feels weird to put the response to what the book actually says about your idea in a footnote.\n\nAnd this footnote still doesn't seem like it's actually responding much to Vaniver's comment. "
    },
    "baseScore": 19,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-09-19T01:13:39.896Z",
    "af": true
  },
  {
    "_id": "2dZYRJy64JeiHAvzd",
    "postId": "mve2bunf6YfTeiAvd",
    "parentCommentId": "BQinH7qMucAhiNPHC",
    "topLevelCommentId": "BQinH7qMucAhiNPHC",
    "contents": {
      "markdown": "Whoops, fixed now",
      "plaintextDescription": "Whoops, fixed now"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-17T23:48:23.314Z",
    "af": false
  },
  {
    "_id": "Yhc3KcxZ3i4wAkpiB",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "ZwzbpQRTqawXcNCqk",
    "topLevelCommentId": "ZwzbpQRTqawXcNCqk",
    "contents": {
      "markdown": "(fyi, I almost replied yesterday with \"my shoulder Darren McKee is kinda sad about the 'no one else tried writing a book like this' line\", and didn't get around to it because I was busy. I did get a copy of your book recently to see how it compared. Haven't finished reading it yet)",
      "plaintextDescription": "(fyi, I almost replied yesterday with \"my shoulder Darren McKee is kinda sad about the 'no one else tried writing a book like this' line\", and didn't get around to it because I was busy. I did get a copy of your book recently to see how it compared. Haven't finished reading it yet) "
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-17T18:15:49.162Z",
    "af": false
  },
  {
    "_id": "bs7mZKvKvKHrGAxZk",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "oHkw3JPi3hjbJ3E5B",
    "topLevelCommentId": "oHkw3JPi3hjbJ3E5B",
    "contents": {
      "markdown": "I think Buck and Eliezer both agree you should only say shocking things if they were true. I think if Eliezer believed what Buck believes, he would have found a title that was still aimed at the overton-smashing strategy but honest.",
      "plaintextDescription": "I think Buck and Eliezer both agree you should only say shocking things if they were true. I think if Eliezer believed what Buck believes, he would have found a title that was still aimed at the overton-smashing strategy but honest."
    },
    "baseScore": 21,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-09-17T17:48:01.246Z",
    "af": false
  },
  {
    "_id": "nhGKajoPmaimAgxQn",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "zganMKrWgan6f33cS",
    "topLevelCommentId": "YqkSshygyQ4vdP2sk",
    "contents": {
      "markdown": "In addition to Malo's comment, I think the book contains arguments that AFAICT are only especially made in the context of the MIRI dialogues, which are particularly obnoxious to read.",
      "plaintextDescription": "In addition to Malo's comment, I think the book contains arguments that AFAICT are only especially made in the context of the MIRI dialogues, which are particularly obnoxious to read."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-17T04:51:33.681Z",
    "af": false
  },
  {
    "_id": "qnwDxZLCygodeKYfc",
    "postId": "w3KtPQDMF4GGR3YLp",
    "parentCommentId": "rxm29JkdJYghz4WBt",
    "topLevelCommentId": "rxm29JkdJYghz4WBt",
    "contents": {
      "markdown": "Interested in links to the press reviews you're thinking of.",
      "plaintextDescription": "Interested in links to the press reviews you're thinking of."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-16T02:30:42.919Z",
    "af": false
  },
  {
    "_id": "seYMosesvoteKFHzK",
    "postId": "w3KtPQDMF4GGR3YLp",
    "parentCommentId": "oyfQxd8djiboipj3W",
    "topLevelCommentId": "Kpoe6QG5owSEK2Goh",
    "contents": {
      "markdown": "Nod. Does anything in the \"AI-accelerated AI R&D\" space feel cruxy for you? Or \"a given model seems to be semi-reliably be producing Actually Competent Work in multiple scientific fields?\"",
      "plaintextDescription": "Nod. Does anything in the \"AI-accelerated AI R&D\" space feel cruxy for you? Or \"a given model seems to be semi-reliably be producing Actually Competent Work in multiple scientific fields?\""
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-16T01:33:47.991Z",
    "af": false
  },
  {
    "_id": "oMhNKGyTuc2WApxdG",
    "postId": "w3KtPQDMF4GGR3YLp",
    "parentCommentId": "2DiLvMKeMQgduxsTK",
    "topLevelCommentId": "Kpoe6QG5owSEK2Goh",
    "contents": {
      "markdown": "Curious if there are any bets you'd make where, if they happened in the next 10 years or so, you'd significantly re-evaluate your models here?",
      "plaintextDescription": "Curious if there are any bets you'd make where, if they happened in the next 10 years or so, you'd significantly re-evaluate your models here?"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-15T23:56:39.793Z",
    "af": false
  },
  {
    "_id": "KSiuCvzwgFtRntGsG",
    "postId": "w3KtPQDMF4GGR3YLp",
    "parentCommentId": "RtoFnG3yMgAGDTEhA",
    "topLevelCommentId": "Kpoe6QG5owSEK2Goh",
    "contents": {
      "markdown": "Nod. \n\nFYI I don't think the book is making a particular claim that any of this *will* happen soon, merely that when it happens, the outcome will be very likely to be human extinction. The point is not that it'll happen at a particular time/way – the LLM/ML paradigm might hit a wall, there might need to be algorithmic advances, it might instead route through narrow AI getting really good at conducting and leveraging neuroscience and making neuromorphic AI or whatever. \n\nBut, the fact that we know human brains run on a relatively low amount of power and training data means we should expect this to happen sooner or later. (but meanwhile, it does sure seem like both the current paradigm keeps advancing, and a lot of money is being poured in, so it seems at least reasonably likely the that it'll be sooner rather than later).\n\nThe book doesn't argue a particular timeline for that, but, it personally (to me) seems weird to me to expect it to take another century, in particular when you can leverage narrower pseudogeneral AI to help you make advances. And I have a hard time imagining takeoff taking longer than than a decade, or really even a couple years, once you hit full generality.",
      "plaintextDescription": "Nod. \n\nFYI I don't think the book is making a particular claim that any of this will happen soon, merely that when it happens, the outcome will be very likely to be human extinction. The point is not that it'll happen at a particular time/way – the LLM/ML paradigm might hit a wall, there might need to be algorithmic advances, it might instead route through narrow AI getting really good at conducting and leveraging neuroscience and making neuromorphic AI or whatever. \n\nBut, the fact that we know human brains run on a relatively low amount of power and training data means we should expect this to happen sooner or later. (but meanwhile, it does sure seem like both the current paradigm keeps advancing, and a lot of money is being poured in, so it seems at least reasonably likely the that it'll be sooner rather than later).\n\nThe book doesn't argue a particular timeline for that, but, it personally (to me) seems weird to me to expect it to take another century, in particular when you can leverage narrower pseudogeneral AI to help you make advances. And I have a hard time imagining takeoff taking longer than than a decade, or really even a couple years, once you hit full generality. "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-15T23:43:57.880Z",
    "af": false
  },
  {
    "_id": "tCKpkdsqnbDJboAXg",
    "postId": "w3KtPQDMF4GGR3YLp",
    "parentCommentId": "WRcy62ydMBek8kS3q",
    "topLevelCommentId": "Kpoe6QG5owSEK2Goh",
    "contents": {
      "markdown": "No. The argument is \"the current paradigm will produce the Bad Thing by default, if it continues on what looks like it's default trajectory.\" (i.e. via training, in a fashion where it's not super predictable in advance what behaviors the training will result in in various off-distribution scenarios)",
      "plaintextDescription": "No. The argument is \"the current paradigm will produce the Bad Thing by default, if it continues on what looks like it's default trajectory.\" (i.e. via training, in a fashion where it's not super predictable in advance what behaviors the training will result in in various off-distribution scenarios)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-15T23:08:24.068Z",
    "af": false
  },
  {
    "_id": "ogFoFM6jqFsrsJcZM",
    "postId": "w3KtPQDMF4GGR3YLp",
    "parentCommentId": "Kpoe6QG5owSEK2Goh",
    "topLevelCommentId": "Kpoe6QG5owSEK2Goh",
    "contents": {
      "markdown": "A thing I can't quite tell if you're incorporating into your model – the thing the book is about is:\n\n\"AI that is either more capable than the rest of humanity combined, or is capable of recursively self-improving and situationally aware enough to maneuever itself into having the resources to do so (and then being more capable than the rest of humanity combined), and which hasn't been designed in a fairly different way from the way current AIs are created.\"\n\nI'm not sure if you're more like \"if that happened, I don't see why it'd be particularly likely to behave like an ideal agent ruthlessly optimizing for alien goals\", or if you're more like \"I don't really buy that this can/will happen in the first place.\"\n\n(the book is specifically about that type of AI, and has separate arguments for \"someday someone will make that\" and \"when they do, here's how we think it'll go\")",
      "plaintextDescription": "A thing I can't quite tell if you're incorporating into your model – the thing the book is about is:\n\n\"AI that is either more capable than the rest of humanity combined, or is capable of recursively self-improving and situationally aware enough to maneuever itself into having the resources to do so (and then being more capable than the rest of humanity combined), and which hasn't been designed in a fairly different way from the way current AIs are created.\"\n\nI'm not sure if you're more like \"if that happened, I don't see why it'd be particularly likely to behave like an ideal agent ruthlessly optimizing for alien goals\", or if you're more like \"I don't really buy that this can/will happen in the first place.\"\n\n(the book is specifically about that type of AI, and has separate arguments for \"someday someone will make that\" and \"when they do, here's how we think it'll go\")"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-15T22:42:38.301Z",
    "af": false
  },
  {
    "_id": "KPSrzoodcnNcac8PK",
    "postId": "qzbDjLZze3WBJfMcG",
    "parentCommentId": "aCPEtsMup2aFpkRjB",
    "topLevelCommentId": "DAhLfQfd5gtwcywKW",
    "contents": {
      "markdown": "My prediction is that a year from now Jim will still think it was a mistake and Habryka will still think it was a good call because they value different things.",
      "plaintextDescription": "My prediction is that a year from now Jim will still think it was a mistake and Habryka will still think it was a good call because they value different things."
    },
    "baseScore": 15,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-13T21:28:38.644Z",
    "af": false
  },
  {
    "_id": "rgevyYLqKxawgnELf",
    "postId": "jL7uDE5oH4HddYq4u",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Awhile ago I [wrote](https://lesswrong.com/posts/2jfiMgKkh7qw9z8Do/being-a-robust-agent?commentId=6S4LHrqZxW3e9PdTn):\n\n> There's a frame where you just say \"no, rationality is specifically about being a robust agent. There are other ways to be effective, but rationality is the particular way of being effective where you try to have cognitive patterns with good epistemology and robust decision theory.\"\n\n> This is in tension with the \"rationalists should win\", thing. Shrug.\n\n> I think it's important to have at least one concept that is \"anyone with goals should ultimately be trying to solve them the best way possible\", and at least one concept that is \"you might consider specifically studying cognitive patterns and policies and a cluster of related things, as a strategy to pursue particular goals.\"\n\nJust had a thought that you might carve this into something like \"shortterm rationality\" and \"longterm rationality\", where shorterm is \"what cognitive algorithms will help me *right now* (to systematically achieve my goals, given my current conditions and skills)\", and longterm rationality is like \"what cognitive-algorithm-metacognitive practices would help me longterm to invest in?\"",
      "plaintextDescription": "Awhile ago I wrote:\n\n> There's a frame where you just say \"no, rationality is specifically about being a robust agent. There are other ways to be effective, but rationality is the particular way of being effective where you try to have cognitive patterns with good epistemology and robust decision theory.\"\n\n> This is in tension with the \"rationalists should win\", thing. Shrug.\n\n> I think it's important to have at least one concept that is \"anyone with goals should ultimately be trying to solve them the best way possible\", and at least one concept that is \"you might consider specifically studying cognitive patterns and policies and a cluster of related things, as a strategy to pursue particular goals.\"\n\nJust had a thought that you might carve this into something like \"shortterm rationality\" and \"longterm rationality\", where shorterm is \"what cognitive algorithms will help me right now (to systematically achieve my goals, given my current conditions and skills)\", and longterm rationality is like \"what cognitive-algorithm-metacognitive practices would help me longterm to invest in?\""
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-12T17:49:36.849Z",
    "af": false
  },
  {
    "_id": "gY8QsfDWpdGhxW9Gr",
    "postId": "TiQGC6woDMPJ9zbNM",
    "parentCommentId": "Xe6j5rfgGjkPyGmtH",
    "topLevelCommentId": "PkeGvBroRwmhFq2bj",
    "contents": {
      "markdown": "Part of the deal of being allies if you don't have to be allies about everything. I don't think they particularly need to do anything to help with technical safety (there just need to be people who understand and care about that somewhere). I'm pretty happy if they're just on board with \"stop building AGI\" for whatever reason.\n\nI do think they eventually need to be on board with some version of the handling the intelligence curse (I didn't know that term, [here's a link](http://lesswrong.com/posts/BXW2bqxmYbLuBrm7E/the-best-approaches-for-mitigating-the-intelligence-curse-or) ), although I think in a lot of worlds the gameboard is so obviously changed I expect handling it to be an easier sell.",
      "plaintextDescription": "Part of the deal of being allies if you don't have to be allies about everything. I don't think they particularly need to do anything to help with technical safety (there just need to be people who understand and care about that somewhere). I'm pretty happy if they're just on board with \"stop building AGI\" for whatever reason.\n\nI do think they eventually need to be on board with some version of the handling the intelligence curse (I didn't know that term, here's a link ), although I think in a lot of worlds the gameboard is so obviously changed I expect handling it to be an easier sell."
    },
    "baseScore": 21,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2025-09-08T20:18:53.759Z",
    "af": false
  },
  {
    "_id": "CK4cToSbQquRHy8xr",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "spMfsMN5BGcFpkivE",
    "topLevelCommentId": "pxuQygJx8LqNEfm3G",
    "contents": {
      "markdown": "> We must pause early (AIs pose significant risk before they speed up research much). I think this is mostly ruled out by current evidence\n\nFYI I currently would mainline guess that this is true. Also I don't get why current evidence says anything about it – current AIs aren't dangerous, but that doesn't really say anything about whether an AI that's capable of speeding up superalignment or pivotal-act-relevant research by even 2x would be dangerous.",
      "plaintextDescription": "> We must pause early (AIs pose significant risk before they speed up research much). I think this is mostly ruled out by current evidence\n\nFYI I currently would mainline guess that this is true. Also I don't get why current evidence says anything about it – current AIs aren't dangerous, but that doesn't really say anything about whether an AI that's capable of speeding up superalignment or pivotal-act-relevant research by even 2x would be dangerous. "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-03T23:48:28.162Z",
    "af": false
  },
  {
    "_id": "uNmAEjR76eSxWWQgN",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": "SwESFrzTfckvgJjpw",
    "topLevelCommentId": "P93J7vZLmdjRYnEW6",
    "contents": {
      "markdown": "> I'm not getting your 35% to 5% reference? I just have no hope of getting as low as 5%, but a lot of hope for improving on just letting the labs take a swing.\n\ni.e, if basically anything *other* than a long pause will be insufficient to actually work, you might as well swing for the pause.",
      "plaintextDescription": "> I'm not getting your 35% to 5% reference? I just have no hope of getting as low as 5%, but a lot of hope for improving on just letting the labs take a swing.\n\ni.e, if basically anything other than a long pause will be insufficient to actually work, you might as well swing for the pause."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-03T23:15:11.560Z",
    "af": false
  },
  {
    "_id": "zoBwFm93fuAP2Qqr2",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": "Y3Z2NwDNjuDGnWZwX",
    "topLevelCommentId": "P93J7vZLmdjRYnEW6",
    "contents": {
      "markdown": "> I'm thinking of a slightly different plan than \"increase the rate of people being able to think seriously about the problem\" I'd like to convince people who already understand the problem to accept that pause is unlikely and alignment is not known to be impossibly hard even on short timelines. ...\n\n> ...Getting entirely new people to understand the hard parts of the problem and then understand all of the technical skills or theoretical subtleties is another route. I haven't thought as much about that one because I don't have a public platform,\n\nI think it's useful to think of \"rate of competent people think seriously about the right problems\" is, like, the \"units\" of success for various flavors of plans here. There are different bottlenecks. \n\nI currently think the rate-limiting reagent is \"people who understand the problem\". And I think that's in turn rate-limited on:\n\n*   \"the problem is sort of wonky and hard with bad feedbackloops and there's a cluster of attitudes and skills you need to have any traction sitting and grokking the problem.\"\n*   \"we don't have much ability to evaluate progress on the problem, which in turn means it's harder to provide a good funding/management infrastructure for it.\"",
      "plaintextDescription": "> I'm thinking of a slightly different plan than \"increase the rate of people being able to think seriously about the problem\" I'd like to convince people who already understand the problem to accept that pause is unlikely and alignment is not known to be impossibly hard even on short timelines. ...\n\n> ...Getting entirely new people to understand the hard parts of the problem and then understand all of the technical skills or theoretical subtleties is another route. I haven't thought as much about that one because I don't have a public platform,\n\nI think it's useful to think of \"rate of competent people think seriously about the right problems\" is, like, the \"units\" of success for various flavors of plans here. There are different bottlenecks. \n\nI currently think the rate-limiting reagent is \"people who understand the problem\". And I think that's in turn rate-limited on:\n\n * \"the problem is sort of wonky and hard with bad feedbackloops and there's a cluster of attitudes and skills you need to have any traction sitting and grokking the problem.\"\n * \"we don't have much ability to evaluate progress on the problem, which in turn means it's harder to provide a good funding/management infrastructure for it.\""
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-03T23:08:08.620Z",
    "af": false
  },
  {
    "_id": "9u9zuZBy25qDMD4Nt",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": "Y3Z2NwDNjuDGnWZwX",
    "topLevelCommentId": "P93J7vZLmdjRYnEW6",
    "contents": {
      "markdown": "I think \"moving an overton window\" is a sort of different operation than what Bengio/Hinton/Dario are doing. (Or, like, yes, they are expanding an overton window, but, their entire strategy for doing so seems predicated on a certain kind of caution/incrementalness)\n\nI think there are two pretty different workable strategies:\n\n*   say things somewhat outside the window, picking your battles\n*   make bold claims, while believing in your convictions with enough strength and without looking \"attackable for mispeaking\".\n\nGoing halfway from one to the other doesn't actually work, and the second one doesn't really work unless you actually do have those convictions. There are a few people trying to do the latter, but, most of them just don't actually have the reputation that'd make anyone care (and also there's a lot of skill to doing it right). I think if at least one of Yoshua/Geoffrey/Dario/Demis switched strategies it'd make a big difference.",
      "plaintextDescription": "I think \"moving an overton window\" is a sort of different operation than what Bengio/Hinton/Dario are doing. (Or, like, yes, they are expanding an overton window, but, their entire strategy for doing so seems predicated on a certain kind of caution/incrementalness)\n\nI think there are two pretty different workable strategies:\n\n * say things somewhat outside the window, picking your battles\n * make bold claims, while believing in your convictions with enough strength and without looking \"attackable for mispeaking\".\n\nGoing halfway from one to the other doesn't actually work, and the second one doesn't really work unless you actually do have those convictions. There are a few people trying to do the latter, but, most of them just don't actually have the reputation that'd make anyone care (and also there's a lot of skill to doing it right). I think if at least one of Yoshua/Geoffrey/Dario/Demis switched strategies it'd make a big difference."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-03T22:31:32.732Z",
    "af": false
  },
  {
    "_id": "ngra2xezmZEib7tuK",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": "Q9oYkcczhZgurzsb2",
    "topLevelCommentId": "Q9oYkcczhZgurzsb2",
    "contents": {
      "markdown": "Do you mean like a short pithy name for the fallacy/failure-mode?",
      "plaintextDescription": "Do you mean like a short pithy name for the fallacy/failure-mode?"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-03T04:59:43.389Z",
    "af": false
  },
  {
    "_id": "zR6EwxoFt4SBKkFf4",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": "SwESFrzTfckvgJjpw",
    "topLevelCommentId": "P93J7vZLmdjRYnEW6",
    "contents": {
      "markdown": "> That's why I want to convince more people that actually understand the problem to identify and work like mad on the hard parts like the world is on fire, instead of hoping it somehow isn't or can be put out.\n\nFYI something similar to this was basically my \"last year's plan\", and it's on hold because I think it is plausible right now to meaningfully move the overton window around pauses or at least dramatic slowdowns. (This is based on seeing the amount of traffic AI 2027 got, and the number of NatSec endorsements that If Anyone Builds It Got, and having recently gotten to read it and thinking it is pretty good)\n\nI think if Yoshua Bengio, Geoffrey Hinton, or Dario actually really tried to move overton windows instead of sort of trying to manuever within the current one, it'd make a huge difference. (I don't think this means it's necessarily tractable for most people to help. It's a high-skill operation)\n\n(Another reason for me putting \"increase the rate of people able to think seriously about the problem\" on hold is that my plans there weren't getting that much traction. I have some models of what I'd try next when/if I return to it but it wasn't a slam dunk to keep going)",
      "plaintextDescription": "> That's why I want to convince more people that actually understand the problem to identify and work like mad on the hard parts like the world is on fire, instead of hoping it somehow isn't or can be put out.\n\nFYI something similar to this was basically my \"last year's plan\", and it's on hold because I think it is plausible right now to meaningfully move the overton window around pauses or at least dramatic slowdowns. (This is based on seeing the amount of traffic AI 2027 got, and the number of NatSec endorsements that If Anyone Builds It Got, and having recently gotten to read it and thinking it is pretty good)\n\nI think if Yoshua Bengio, Geoffrey Hinton, or Dario actually really tried to move overton windows instead of sort of trying to manuever within the current one, it'd make a huge difference. (I don't think this means it's necessarily tractable for most people to help. It's a high-skill operation)\n\n(Another reason for me putting \"increase the rate of people able to think seriously about the problem\" on hold is that my plans there weren't getting that much traction. I have some models of what I'd try next when/if I return to it but it wasn't a slam dunk to keep going)"
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-03T02:03:13.508Z",
    "af": false
  },
  {
    "_id": "nnqWnL4xMDuejjfJo",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": "P93J7vZLmdjRYnEW6",
    "topLevelCommentId": "P93J7vZLmdjRYnEW6",
    "contents": {
      "markdown": "> I assume a lot of us aren't very engaged with pause efforts or hopes because it seems more productive and realistic to work on reducing ~70% toward ~35% misalignment risks.\n\nNod. I do just, like, don't think that's actually that great a strategy – it presupposes it is actually easier to get from 70% to 35% than from 35% to 5%. I don't see Anthropic-et-al actually really getting ready to ask the sort of questions that would IMO be necessary to actually do-the-reducing.",
      "plaintextDescription": "> I assume a lot of us aren't very engaged with pause efforts or hopes because it seems more productive and realistic to work on reducing ~70% toward ~35% misalignment risks.\n\nNod. I do just, like, don't think that's actually that great a strategy – it presupposes it is actually easier to get from 70% to 35% than from 35% to 5%. I don't see Anthropic-et-al actually really getting ready to ask the sort of questions that would IMO be necessary to actually do-the-reducing. "
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-03T01:45:06.260Z",
    "af": false
  },
  {
    "_id": "GLiyAwu6MHv9iuWih",
    "postId": "98sCTsGJZ77WgQ6nE",
    "parentCommentId": "yMeYETptAt3QN54sC",
    "topLevelCommentId": "oRwaJ4GwXAnvrmGgw",
    "contents": {
      "markdown": "> I do realize point 2 is *not* the way LW is intended to operate\n\nWell I would say the whole reason LW mods are banning Said is that we do, in fact, want LW to operate this way. (Or, directionally similarly to this). I do also want wrong ideas to get noticed and discarded, and I do want \"good taste in generating ideas\" (there are people who aren't skilled enough at casual idea generation for me to feel excited about them generating such conversation on LW). But I think it's an essential part of any real generative intellectual tradition.",
      "plaintextDescription": "> I do realize point 2 is not the way LW is intended to operate\n\nWell I would say the whole reason LW mods are banning Said is that we do, in fact, want LW to operate this way. (Or, directionally similarly to this). I do also want wrong ideas to get noticed and discarded, and I do want \"good taste in generating ideas\" (there are people who aren't skilled enough at casual idea generation for me to feel excited about them generating such conversation on LW). But I think it's an essential part of any real generative intellectual tradition."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-03T00:29:48.135Z",
    "af": false
  },
  {
    "_id": "Lb6vKYmf7kW9bMqY3",
    "postId": "98sCTsGJZ77WgQ6nE",
    "parentCommentId": "bpCqZw5qiaDQQXuxG",
    "topLevelCommentId": "oRwaJ4GwXAnvrmGgw",
    "contents": {
      "markdown": "> you could simply ignore or stop replying to him if you thought his style of conversation was too extreme for your tastes, instead of feeling like his \"entrance to my comment threads was a minor emergency\".\n\nI wanna flag, your use of the word \"simply\" here is... like, idk, false. \n\nI do think it's good for people to learn the skill of not caring what other people think and being able to think out loud even when someone is being annoying. But, this is a pretty difficult skill for lots of people. I think it's pretty common for people who are attempting to learn it to instead end up contorting their original thought process around what the anticipated social punishment.\n\nI think it's a coherent position to want LessWrong \"price of entry\" to be gaining that skill. I don't think it's a reasonable position to call it \"simply...\". It's asking for like 10-200 hours of pretty scary, painful work.",
      "plaintextDescription": "> you could simply ignore or stop replying to him if you thought his style of conversation was too extreme for your tastes, instead of feeling like his \"entrance to my comment threads was a minor emergency\".\n\nI wanna flag, your use of the word \"simply\" here is... like, idk, false. \n\nI do think it's good for people to learn the skill of not caring what other people think and being able to think out loud even when someone is being annoying. But, this is a pretty difficult skill for lots of people. I think it's pretty common for people who are attempting to learn it to instead end up contorting their original thought process around what the anticipated social punishment.\n\nI think it's a coherent position to want LessWrong \"price of entry\" to be gaining that skill. I don't think it's a reasonable position to call it \"simply...\". It's asking for like 10-200 hours of pretty scary, painful work."
    },
    "baseScore": 9,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-09-03T00:23:08.336Z",
    "af": false
  },
  {
    "_id": "LW6egYMfLwte6xCuf",
    "postId": "LLiZEnnh3kK3Qg7qf",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This seems like it's engaging with the question of \"what do critics think?\" in a sort of model-free, uninformed, \"who to defer to\" sort of way.\n\nFor awhile, I didn't fully update on arguments for AI Risk being a Big Deal because the arguments were kinda complex and I could imagine clever arguers convincing me of it without it being true. One of the things that updated me over the course of 4 years was actually reading the replies (including by people like Hanson) and thinking \"man, they didn't seem to even understand or address the main points.\"\n\ni.e. it's not that they didn't engage with the arguments, it's that they engaged with the arguments *badly* which lowered my credence on taking their opinion seriously.\n\n(I think nowadays I have seen some critics who do seem to me to have engaged with most of the real points. None of their counterarguments seem like they've added up to \"AI is not a huge fucking deal that is extremely risky\" in a way that makes any sense to me, but, some of them add up to alternate frames of looking at the problem that might shift what is the best thing(s) to do about it)",
      "plaintextDescription": "This seems like it's engaging with the question of \"what do critics think?\" in a sort of model-free, uninformed, \"who to defer to\" sort of way.\n\nFor awhile, I didn't fully update on arguments for AI Risk being a Big Deal because the arguments were kinda complex and I could imagine clever arguers convincing me of it without it being true. One of the things that updated me over the course of 4 years was actually reading the replies (including by people like Hanson) and thinking \"man, they didn't seem to even understand or address the main points.\"\n\ni.e. it's not that they didn't engage with the arguments, it's that they engaged with the arguments badly which lowered my credence on taking their opinion seriously.\n\n(I think nowadays I have seen some critics who do seem to me to have engaged with most of the real points. None of their counterarguments seem like they've added up to \"AI is not a huge fucking deal that is extremely risky\" in a way that makes any sense to me, but, some of them add up to alternate frames of looking at the problem that might shift what is the best thing(s) to do about it)"
    },
    "baseScore": 13,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-09-02T21:55:53.864Z",
    "af": false
  },
  {
    "_id": "eKSdqu8codZfGEN6p",
    "postId": "PBd7xPAh22y66rbme",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Periodically I've considered writing a post similar to this. A piece that I think this doesn't fully dive into is \"did Anthropic have a commitment not to push the capability frontier?\".\n\nI had once written a doc aimed at Anthropic employees, during SB 1047 Era, when I had been felt like Anthropic was advocating for changes to the law that were hard to interpret un-cynically.[^edlj58p0pji] I've had a vague intention to rewrite this into a more public facing thing, but, for now I'm just going to lift out the section talking about the \"pushing the capability frontier\" thing.\n\n> When I chatted with several anthropic employees at the happy hour a ~couple months~ ~year ago, at some point I brought up the “Dustin Moskowitz’s earnest belief was that Anthropic had an explicit policy of not advancing the AI frontier” thing. Some employees have said something like “that was never an explicit commitment. It might have been a thing we were generally trying to do a couple years ago, but that was more like “our de facto strategic priorities at the time”, not “an explicit policy or commitment.”\n> \n> When I brought it up, the vibe in the discussion-circle was “yeah, that is kinda weird, I don’t know what happened there”, and then the conversation moved on.\n> \n> I regret that. This is an extremely big deal. I’m disappointed in the other Anthropic folk for shrugging and moving on, and disappointed in myself for letting it happen.\n> \n> First, recapping the Dustin Moskowitz quote (which FYI I saw personally before it was taken down)\n> \n> ![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c2d8b0ace5c33cd3626bcdce588b46d1953f22ba15ba1255.png)\n> \n> First, [gwern also claims he talked to Dario and came away with this impression](https://www.lesswrong.com/posts/JbE7KynwshwkXPJAJ/anthropic-release-claude-3-claims-greater-than-gpt-4#6RR78q3tjc7dsKL9C):\n> \n> \\> Well, if Dustin sees no problem in talking about it, and it's become a major policy concern, then I guess I should disclose that I spent a while talking with Dario back in late October 2022 (ie. pre-RSP in Sept 2023), and we discussed Anthropic's scaling policy at some length, and I too came away with the same impression everyone else seems to have: that Anthropic's AI-arms-race policy was to invest heavily in scaling, creating models at or pushing the frontier to do safety research on, but that they would only release access to second-best models & would not ratchet capabilities up, and it would wait for someone else to do so before catching up. So it would not contribute to races but not fall behind and become irrelevant/noncompetitive.\n> \n> \\> And Anthropic's release of Claude-1 and Claude-2 always seemed to match that policy - even if Claude-2 had a larger context window for a long time than any other decent available model, Claude-2 was still substantially weaker than ChatGPT-4. (Recall that the causus belli for Sam Altman trying to fire Helen Toner from the OA board was a passing reference in a co-authored paper to Anthropic not pushing the frontier like OA did.)\n> \n> I get that y’all have more bits of information than me about what Dario is like. But, some major hypotheses you need to be considering here are a spectrum between:\n> \n> 1.  Dustin Moskowitz and Gwern both interpreted Dario’s claims as more like commitments than Dario meant, and a reasonable bystander would attribute this more to Dustin/Gwern reading too much into it.\n> 2.  Dario communicated poorly, in a way that was maybe understandable, but predictably would leave many people confused.\n> 3.  Dario in fact changed his mind explicitly (making this was more like a broken commitment, and subsequent claims that it was *not* a broken commitment more like lies)\n> 4.  Dario deliberately phrased things in an openended/confusing way, optimized to be reassuring to a major stakeholder without actually making the commitments that would have backed up that reassurance.\n> 5.  Dario straight up lied to both of them.\n> 6.  Dario is lying to/confusing himself.\n> \n> This is important because:\n> \n> a) even option 2 seems pretty bad given the stakes. I might cut many people slack for communicating poorly by accident, but when someone is raising huge amounts of money, building technology that is likely to be very dangerous by default, accidentally misleading a key stakeholder is not something you can just shrug off. \n> \n> b) if we’re in worlds with options 3, 4 or 5 or 6 (and, really, even option 2), you should be more skeptical of other reassuring things Dario has said. It’s not *that* important to distinguish between these two because the question isn’t “how good a person is Dario?”, it’s “how should you interpret and trust things Dario says”.\n> \n> In my last chat with Anthropic employees, people talked about meetings and slack channels where people asked probing, important questions, and Dario didn’t shy away from actually answering them, in a way that felt compelling. But, if Dario is skilled at saying things to smart people with major leverage over him that sound reassuring, but leave them with a false impression, you need to be a lot more skeptical of your-sense-of-having-been-reassured.\n\n[^edlj58p0pji]: in particular, advocating for removing the whistleblower clause, and simulaneously arguing that \"we don't know how to make a good SSP yet, which is why there shouldn't yet be regulations about how to do it\" while also arguing \"companies liability for catastrophic harms should be dependent on how good their SSP was.\"",
      "plaintextDescription": "Periodically I've considered writing a post similar to this. A piece that I think this doesn't fully dive into is \"did Anthropic have a commitment not to push the capability frontier?\".\n\nI had once written a doc aimed at Anthropic employees, during SB 1047 Era, when I had been felt like Anthropic was advocating for changes to the law that were hard to interpret un-cynically.[1] I've had a vague intention to rewrite this into a more public facing thing, but, for now I'm just going to lift out the section talking about the \"pushing the capability frontier\" thing.\n\n> When I chatted with several anthropic employees at the happy hour a couple months ~year ago, at some point I brought up the “Dustin Moskowitz’s earnest belief was that Anthropic had an explicit policy of not advancing the AI frontier” thing. Some employees have said something like “that was never an explicit commitment. It might have been a thing we were generally trying to do a couple years ago, but that was more like “our de facto strategic priorities at the time”, not “an explicit policy or commitment.”\n> \n> When I brought it up, the vibe in the discussion-circle was “yeah, that is kinda weird, I don’t know what happened there”, and then the conversation moved on.\n> \n> I regret that. This is an extremely big deal. I’m disappointed in the other Anthropic folk for shrugging and moving on, and disappointed in myself for letting it happen.\n> \n> First, recapping the Dustin Moskowitz quote (which FYI I saw personally before it was taken down)\n> \n> \n> \n> First, gwern also claims he talked to Dario and came away with this impression:\n> \n> > Well, if Dustin sees no problem in talking about it, and it's become a major policy concern, then I guess I should disclose that I spent a while talking with Dario back in late October 2022 (ie. pre-RSP in Sept 2023), and we discussed Anthropic's scaling policy at some length, and I too came away with the same impression everyone else seems to have: that Anthropic's AI-arms-"
    },
    "baseScore": 76,
    "voteCount": 25,
    "createdAt": null,
    "postedAt": "2025-09-02T19:09:53.058Z",
    "af": false
  },
  {
    "_id": "gfw7baWjoMeCSuzwf",
    "postId": "PBd7xPAh22y66rbme",
    "parentCommentId": "EoDBP6R7qQ6esjcg4",
    "topLevelCommentId": "Y8cvhohfW6SPEC6ou",
    "contents": {
      "markdown": "(Another mod leaned in the other direction, and I do think there's like, this is is pretty factual and timeless, and Dario is more of a public figure than an inside-baseball lesswrong community member, so, seemed okay to err in the other direction. But still flagging it as an edge case for people trying to intuit the rules)",
      "plaintextDescription": "(Another mod leaned in the other direction, and I do think there's like, this is is pretty factual and timeless, and Dario is more of a public figure than an inside-baseball lesswrong community member, so, seemed okay to err in the other direction. But still flagging it as an edge case for people trying to intuit the rules)"
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-02T02:47:57.439Z",
    "af": false
  },
  {
    "_id": "goB5tf44gw7DaxWLu",
    "postId": "PBd7xPAh22y66rbme",
    "parentCommentId": "Y8cvhohfW6SPEC6ou",
    "topLevelCommentId": "Y8cvhohfW6SPEC6ou",
    "contents": {
      "markdown": "I was unsure about it, the criteria for frontpage are \"Timeless\" (which I agree this qualifies as) and \"not inside baseball-y\" (often with vaguely political undertones), which seemed less obvious. My decision at the the time was \"strong upvote but personal blog\", but I think it's not obvious and if another LW mod. I agree it's a bunch of good information to have in one place.",
      "plaintextDescription": "I was unsure about it, the criteria for frontpage are \"Timeless\" (which I agree this qualifies as) and \"not inside baseball-y\" (often with vaguely political undertones), which seemed less obvious. My decision at the the time was \"strong upvote but personal blog\", but I think it's not obvious and if another LW mod. I agree it's a bunch of good information to have in one place."
    },
    "baseScore": 7,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-09-02T00:38:11.193Z",
    "af": false
  },
  {
    "_id": "yE87unFfsYQtWhBDo",
    "postId": null,
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This seems basically a duplicate of [Task (AI Goal)](https://lesswrong.com/w/task-ai-goal)",
      "plaintextDescription": "This seems basically a duplicate of Task (AI Goal)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-02T00:30:29.143Z",
    "af": false
  },
  {
    "_id": "fbvPbk3sZnFSnoHJc",
    "postId": "jL7uDE5oH4HddYq4u",
    "parentCommentId": "Y6neG3WnoGPxan3fH",
    "topLevelCommentId": "swTqwndZALrBGybRk",
    "contents": {
      "markdown": "I mean there is ~no prior art here because humanity just invented LLMs last ~tuesday. \n\nOkay j'/k, there may be some. But, I think you're imagining \"the LLM is judging whether the content as good\" as opposed to \"the LLM is given formulaic rules to evaluate posts for, and it returns 'yes/no/maybe' for each of those evaluations.\" \n\nThe question here is more \"is it possible to construct rules that are useful?\"\n\n(in the conversation that generated this idea, one person noted \"on my youtube channel, it'd be pretty great if I could just identify any comment that mentions someone's appearance and have it automoderated as 'off topic'\". If we were trying this on a LessWrong-like community, the rules I might want to try to implement would probably be subtler and I don't know if LLMs could actually pull them off).",
      "plaintextDescription": "I mean there is ~no prior art here because humanity just invented LLMs last ~tuesday. \n\nOkay j'/k, there may be some. But, I think you're imagining \"the LLM is judging whether the content as good\" as opposed to \"the LLM is given formulaic rules to evaluate posts for, and it returns 'yes/no/maybe' for each of those evaluations.\" \n\nThe question here is more \"is it possible to construct rules that are useful?\"\n\n(in the conversation that generated this idea, one person noted \"on my youtube channel, it'd be pretty great if I could just identify any comment that mentions someone's appearance and have it automoderated as 'off topic'\". If we were trying this on a LessWrong-like community, the rules I might want to try to implement would probably be subtler and I don't know if LLMs could actually pull them off)."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-31T04:21:49.179Z",
    "af": false
  },
  {
    "_id": "53BX5wp4wFp9SwsJu",
    "postId": "bkjqfhKd8ZWHK9XqF",
    "parentCommentId": "TsRqGHJwkudPJmCvZ",
    "topLevelCommentId": "TsRqGHJwkudPJmCvZ",
    "contents": {
      "markdown": "I'm not sure if you understood this or not, but, I think Alex meant \"nutrient\" in a metaphorical way rather than a literal dietary way.",
      "plaintextDescription": "I'm not sure if you understood this or not, but, I think Alex meant \"nutrient\" in a metaphorical way rather than a literal dietary way."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-30T00:37:36.509Z",
    "af": false
  },
  {
    "_id": "2hvzBJEWmmYmtvH7z",
    "postId": "D7ToDWYLDWbfrP3Fs",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "The basic framework here seems plausible but it'd be easier to say something useful if you gave more specific worked examples from your life.",
      "plaintextDescription": "The basic framework here seems plausible but it'd be easier to say something useful if you gave more specific worked examples from your life."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-29T19:51:25.573Z",
    "af": false
  },
  {
    "_id": "wWxiLy78hoMiKkNnN",
    "postId": "jL7uDE5oH4HddYq4u",
    "parentCommentId": "JJdxwbXPkSJuWzr4F",
    "topLevelCommentId": "swTqwndZALrBGybRk",
    "contents": {
      "markdown": "> So you now need to pay before you post?\n> \n> and comments are disabled when you're out of funds? natural consequence but lol.\n\nThere's a few ways you could do it. It occurs me now it could actually be the commenter's job to pay via microtransactions, and maybe the author can tip back if they like it via a Flattr-ish. This also maybe solves the rate limits.\n\nYou could also just set it to \"when you run out of money, everyone can commit without restriction.\"\n\nYou could also have, like, everyone just pays a monthly subscription to participate. I think the above ideas are kinda cute tho.\n\n> privacy concerns\n\nI was imagining this for public-ish internet where I'd expect it to be digested for the next round of LLM training anyway.",
      "plaintextDescription": "> So you now need to pay before you post?\n> \n> and comments are disabled when you're out of funds? natural consequence but lol.\n\nThere's a few ways you could do it. It occurs me now it could actually be the commenter's job to pay via microtransactions, and maybe the author can tip back if they like it via a Flattr-ish. This also maybe solves the rate limits.\n\nYou could also just set it to \"when you run out of money, everyone can commit without restriction.\"\n\nYou could also have, like, everyone just pays a monthly subscription to participate. I think the above ideas are kinda cute tho.\n\n> privacy concerns\n\nI was imagining this for public-ish internet where I'd expect it to be digested for the next round of LLM training anyway. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-29T03:47:24.727Z",
    "af": false
  },
  {
    "_id": "Q7LHopyKaawFdDhXy",
    "postId": "bkjqfhKd8ZWHK9XqF",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Curated. I'm not quite sure what to make of this point, but I am surprised I hadn't heard it suggested before.\n\nI'm... not really sure what I expect but I am curious for someone to go make some stone tools and let us know, uh, was it good for you?",
      "plaintextDescription": "Curated. I'm not quite sure what to make of this point, but I am surprised I hadn't heard it suggested before.\n\nI'm... not really sure what I expect but I am curious for someone to go make some stone tools and let us know, uh, was it good for you?"
    },
    "baseScore": 2,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-29T00:18:09.134Z",
    "af": false
  },
  {
    "_id": "swTqwndZALrBGybRk",
    "postId": "jL7uDE5oH4HddYq4u",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "LLM Automoderation Idea (we could try this on LessWrong but it feels like something that's, like, more naturally part of a forum that's designed-from-the-ground-up around it)  \n  \nAuthors can create moderation guidelines, which get enforced by LLMs that read new comments, and have access to some user metadata. Comments get deleted / etc by the LLM. (You can also have tools other that deletion, like collapsing comments by default)  \n  \nThe moderation guidelines are public. It's commenter's job to write comments that pass.   \n  \nAuthors pay the fees for the LLMs doing the review.\n\n(I'm currently thinking about this more like \"I am interested in seeing what equilibria a setup like this would end up with\" than like \"this is a good idea\")",
      "plaintextDescription": "LLM Automoderation Idea (we could try this on LessWrong but it feels like something that's, like, more naturally part of a forum that's designed-from-the-ground-up around it)\n\nAuthors can create moderation guidelines, which get enforced by LLMs that read new comments, and have access to some user metadata. Comments get deleted / etc by the LLM. (You can also have tools other that deletion, like collapsing comments by default)\n\nThe moderation guidelines are public. It's commenter's job to write comments that pass. \n\nAuthors pay the fees for the LLMs doing the review.\n\n(I'm currently thinking about this more like \"I am interested in seeing what equilibria a setup like this would end up with\" than like \"this is a good idea\")"
    },
    "baseScore": 13,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-08-28T19:19:34.326Z",
    "af": false
  },
  {
    "_id": "2LCZ5ts6wSbcHjTrY",
    "postId": "9CPNkch7rJFb5eQBG",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "\\[mod note: I frontpaged this, but it's sort of an edge case on the 'is it timeless' front\\]",
      "plaintextDescription": "[mod note: I frontpaged this, but it's sort of an edge case on the 'is it timeless' front]"
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-28T03:09:44.749Z",
    "af": false
  },
  {
    "_id": "dKrJwBDLjpihGcERj",
    "postId": "sDA3jEt4wAD3shKum",
    "parentCommentId": "pRRzAeLdJH4yZFM7R",
    "topLevelCommentId": "tcmmTBwz8tHEEqm2H",
    "contents": {
      "markdown": "Okay, so, like, teenagers use them, just in a kinda functional way where they're not, like, excited about it? is like, more like \"google\" than like \"myspace/facebook/snapchat\" when each of those things was cool?",
      "plaintextDescription": "Okay, so, like, teenagers use them, just in a kinda functional way where they're not, like, excited about it? is like, more like \"google\" than like \"myspace/facebook/snapchat\" when each of those things was cool?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-27T22:21:29.902Z",
    "af": false
  },
  {
    "_id": "tcmmTBwz8tHEEqm2H",
    "postId": "sDA3jEt4wAD3shKum",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> ChatGPT is uncool culturally in my generation (I am 18). The relationship is one-way and reactive.\n\nThis is the first I heard this, and I'm interested to know more detail (I'm in my late 30s). Are all LLMs uncool? ChatGPT in particular? are nearby things \"cool\"?",
      "plaintextDescription": "> ChatGPT is uncool culturally in my generation (I am 18). The relationship is one-way and reactive.\n\nThis is the first I heard this, and I'm interested to know more detail (I'm in my late 30s). Are all LLMs uncool? ChatGPT in particular? are nearby things \"cool\"?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-27T21:59:14.351Z",
    "af": false
  },
  {
    "_id": "Agm6BrMrvXs2HeJnJ",
    "postId": "NyFtHycJvkyNjXNsP",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I am now curious whether a decade later this went anywhere useful.",
      "plaintextDescription": "I am now curious whether a decade later this went anywhere useful."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-27T18:13:14.167Z",
    "af": false
  },
  {
    "_id": "9DAs6trQwsbCeGRBc",
    "postId": "jL7uDE5oH4HddYq4u",
    "parentCommentId": "wxbjjFgLKTwPiAHpb",
    "topLevelCommentId": "aeHWxnE6WY58xxWb2",
    "contents": {
      "markdown": "I already do those things alas.",
      "plaintextDescription": "I already do those things alas."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-27T00:26:19.353Z",
    "af": false
  }
]