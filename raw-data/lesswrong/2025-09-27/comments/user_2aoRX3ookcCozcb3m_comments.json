[
  {
    "_id": "5HRd3Ht275osJNDZp",
    "postId": "dqSwccGTWyBgxrR58",
    "parentCommentId": "GZXa6PDkg8afHYtn5",
    "topLevelCommentId": "9tK6DiFxk7bGqNMdT",
    "contents": {
      "markdown": "yeah, I left off this part but Nate also said\n\n> \\[people having trouble separating them\\] does maybe enhance my sense that the whole community is desperately lacking in nate!courage, if so many people have such trouble distinguishing between \"try naming your real worry\" and \"try being brazen/rude\". (tho ofc part of the phenomenon is me being bad at anticipating reader confusions; the illusion of transparency continues to be a doozy.)",
      "plaintextDescription": "yeah, I left off this part but Nate also said\n\n> [people having trouble separating them] does maybe enhance my sense that the whole community is desperately lacking in nate!courage, if so many people have such trouble distinguishing between \"try naming your real worry\" and \"try being brazen/rude\". (tho ofc part of the phenomenon is me being bad at anticipating reader confusions; the illusion of transparency continues to be a doozy.)"
    },
    "baseScore": 1,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-07-01T04:52:31.850Z",
    "af": false
  },
  {
    "_id": "DyXMEtp9pKJNX47qx",
    "postId": "dqSwccGTWyBgxrR58",
    "parentCommentId": "nZrW8Nk925XbgsmDp",
    "topLevelCommentId": "9tK6DiFxk7bGqNMdT",
    "contents": {
      "markdown": "Nate messaged me a thing in chat and I found it helpful and asked if I could copy it over:\n\n> fwiw a thing that people seem to me to be consistently missing is the distinction between what i was trying to talk about, namely the advice \"have you tried saying what you actually think is the important problem, plainly, even once? ideally without broadcasting signals of how it's a socially shameful belief to hold?\", and the alternative advice that i was *not* advocating, namely \"have you considered speaking to people in a way that might be described as 'brazen' or 'rude' depending on who's doing the describing?\".\n> \n> for instance, in personal conversation, i'm pretty happy to directly contradict others' views -- and that has nothing to do with this 'courage' thing i'm trying to describe. nate!courage is completely compatible with saying \"you don't have to agree with me, mr. senator, but my best understanding of the evidence is \\[thing i believe\\]. if ever you're interested in discussing the reasons in detail, i'd be happy to. and until then, we can work together in areas where our interests overlap.\" there are plenty of ways to name your real worry while being especially respectful and polite! nate!courage and politeness are nearly orthogonal axes, on my view.",
      "plaintextDescription": "Nate messaged me a thing in chat and I found it helpful and asked if I could copy it over:\n\n> fwiw a thing that people seem to me to be consistently missing is the distinction between what i was trying to talk about, namely the advice \"have you tried saying what you actually think is the important problem, plainly, even once? ideally without broadcasting signals of how it's a socially shameful belief to hold?\", and the alternative advice that i was not advocating, namely \"have you considered speaking to people in a way that might be described as 'brazen' or 'rude' depending on who's doing the describing?\".\n> \n> for instance, in personal conversation, i'm pretty happy to directly contradict others' views -- and that has nothing to do with this 'courage' thing i'm trying to describe. nate!courage is completely compatible with saying \"you don't have to agree with me, mr. senator, but my best understanding of the evidence is [thing i believe]. if ever you're interested in discussing the reasons in detail, i'd be happy to. and until then, we can work together in areas where our interests overlap.\" there are plenty of ways to name your real worry while being especially respectful and polite! nate!courage and politeness are nearly orthogonal axes, on my view.Â "
    },
    "baseScore": 21,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-07-01T04:15:36.550Z",
    "af": false
  },
  {
    "_id": "Gxg9AYAXxCWEeJEas",
    "postId": "dqSwccGTWyBgxrR58",
    "parentCommentId": "nZrW8Nk925XbgsmDp",
    "topLevelCommentId": "9tK6DiFxk7bGqNMdT",
    "contents": {
      "markdown": "FWIW, as someone who's been working pretty closely with Nate for the past ten years (and as someone whose preferred conversational dynamic is pretty warm-and-squishy), I actively enjoy working with the guy and feel positive about our interactions.",
      "plaintextDescription": "FWIW, as someone who's been working pretty closely with Nate for the past ten years (and as someone whose preferred conversational dynamic is pretty warm-and-squishy), I actively enjoy working with the guy and feel positive about our interactions."
    },
    "baseScore": 10,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-07-01T02:24:38.097Z",
    "af": false
  },
  {
    "_id": "piSa9YTmr5eyeC7Xi",
    "postId": "CYTwRZtrhHuYf7QYu",
    "parentCommentId": "KDAhxuZkw5ziQFJH8",
    "topLevelCommentId": "Nhpkqqcptga49vBj6",
    "contents": {
      "markdown": "> (Considering how little cherry-picking they did.)\n\nFrom my perspective, FWIW, the endorsements we got would have been surprising even if they had been maximally cherry-picked. You usually just can't find cherries like those.",
      "plaintextDescription": "> (Considering how little cherry-picking they did.)\n\nFrom my perspective, FWIW, the endorsements we got would have been surprising even if they had been maximally cherry-picked. You usually just can't find cherries like those."
    },
    "baseScore": 19,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-06-29T17:52:40.131Z",
    "af": false
  },
  {
    "_id": "RFpWyrd8RTAxRsbgh",
    "postId": "khmpWJnGJnuyPdipE",
    "parentCommentId": "pDpsnLk4jykDMB9RG",
    "topLevelCommentId": "grcDTtQQFrJGwZdHQ",
    "contents": {
      "markdown": "(That was indeed my first thought when Bernanke said he liked the book; no dice, though.)",
      "plaintextDescription": "(That was indeed my first thought when Bernanke said he liked the book; no dice, though.)"
    },
    "baseScore": 11,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-06-21T18:03:59.956Z",
    "af": false
  },
  {
    "_id": "smbaCaRsqJBwXynFT",
    "postId": "khmpWJnGJnuyPdipE",
    "parentCommentId": "df8Pfe3CJo7Bj9Dnp",
    "topLevelCommentId": "ntzjLhvwfcczS84ic",
    "contents": {
      "markdown": "Yep. And equally, the blurbs would be a lot less effective if the title were more timid and less stark.\n\nHearing that a wide range of respected figures endorse a book called *If Anyone Builds It, Everyone Dies: Why Superhuman AI Would Kill Us All* is a potential \"holy shit\" moment. If the same figures were endorsing a book with a vaguely inoffensive title like *Smarter Than Us* or *The AI Crucible*, it would spark a lot less interest (and concern).",
      "plaintextDescription": "Yep. And equally, the blurbs would be a lot less effective if the title were more timid and less stark.\n\nHearing that a wide range of respected figures endorse a book called If Anyone Builds It, Everyone Dies: Why Superhuman AI Would Kill Us All is a potential \"holy shit\" moment. If the same figures were endorsing a book with a vaguely inoffensive title like Smarter Than Us or The AI Crucible, it would spark a lot less interest (and concern)."
    },
    "baseScore": 30,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2025-06-20T20:52:26.178Z",
    "af": false
  },
  {
    "_id": "h6NHXJSihFcqn2n6A",
    "postId": "khmpWJnGJnuyPdipE",
    "parentCommentId": "7GpPr8JFas3tj32Dm",
    "topLevelCommentId": "ntzjLhvwfcczS84ic",
    "contents": {
      "markdown": "Yeah, I think people usually ignore blurbs, but sometimes blurbs are helpful. I think strong blurbs are unusually likely to be helpful when your book has a title like *If Anyone Builds It, Everyone Dies: Why Superhuman AI Would Kill Us All*.",
      "plaintextDescription": "Yeah, I think people usually ignore blurbs, but sometimes blurbs are helpful. I think strong blurbs are unusually likely to be helpful when your book has a title like If Anyone Builds It, Everyone Dies: Why Superhuman AI Would Kill Us All."
    },
    "baseScore": 15,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-06-19T22:11:27.710Z",
    "af": false
  },
  {
    "_id": "u4ruCe6YNLtXsPug2",
    "postId": "khmpWJnGJnuyPdipE",
    "parentCommentId": "boceutoqPqo9DRQzk",
    "topLevelCommentId": "ntzjLhvwfcczS84ic",
    "contents": {
      "markdown": "Aside from the usual suspects (people like Tegmark), we mostly sent the book to people following the heuristic \"would an endorsement from this person be helpful?\", much more so than \"do we know that this person would like the book?\". If you'd asked me individually about Church, Schneier, Bernanke, Shanahan, or Spaulding in advance, I'd have put most of my probability on \"this person won't be persuaded by the book (if they read it at all) and will come away strongly disagreeing and not wanting to endorse\". They seemed worth sharing the book with anyway, and then they ended up liking it (at least enough to blurb it) and some very excited MIRI slack messages ensued.\n\n(I'd have expected Eddy to agree with the book, though I wouldn't have expected him to give a blurb; and I didn't know Wolfsthal well enough to have an opinion.)\n\nNate has a blog post coming out in the next few days that will say a bit more about \"How filtered is this evidence?\" (along with other topics), but my short answer is that we haven't sent the book to *that* many people, we've mostly sent it to people whose AI opinions we didn't know much about (and who we'd guess on priors would be skeptical to some degree), and we haven't gotten many negative reactions at all. (Though we've gotten people who just didn't answer our inquiries, and some of those might have read the book and disliked it enough to not reply.)",
      "plaintextDescription": "Aside from the usual suspects (people like Tegmark), we mostly sent the book to people following the heuristic \"would an endorsement from this person be helpful?\", much more so than \"do we know that this person would like the book?\". If you'd asked me individually about Church, Schneier, Bernanke, Shanahan, or Spaulding in advance, I'd have put most of my probability on \"this person won't be persuaded by the book (if they read it at all) and will come away strongly disagreeing and not wanting to endorse\". They seemed worth sharing the book with anyway, and then they ended up liking it (at least enough to blurb it) and some very excited MIRI slack messages ensued.\n\n(I'd have expected Eddy to agree with the book, though I wouldn't have expected him to give a blurb; and I didn't know Wolfsthal well enough to have an opinion.)\n\nNate has a blog post coming out in the next few days that will say a bit more about \"How filtered is this evidence?\" (along with other topics), but my short answer is that we haven't sent the book to that many people, we've mostly sent it to people whose AI opinions we didn't know much about (and who we'd guess on priors would be skeptical to some degree), and we haven't gotten many negative reactions at all. (Though we've gotten people who just didn't answer our inquiries, and some of those might have read the book and disliked it enough to not reply.)"
    },
    "baseScore": 15,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-06-19T19:00:13.867Z",
    "af": false
  },
  {
    "_id": "QSTvoAuveC6nQTebM",
    "postId": "khmpWJnGJnuyPdipE",
    "parentCommentId": "YduwoubMGDMSmeubW",
    "topLevelCommentId": "ntzjLhvwfcczS84ic",
    "contents": {
      "markdown": "> Now, how much is that evidence about the correctness of the book? Extremely little!\n\nIt might not be much evidence for LWers, who are already steeped in arguments and evidence about AI risk. It should be a lot of evidence for people newer to this topic who start with a skeptical prior. Most books making extreme-sounding (conditional) claims about the future don't have endorsements from Nobel-winning economists, former White House officials, retired generals, computer security experts, etc. on the back cover.",
      "plaintextDescription": "> Now, how much is that evidence about the correctness of the book? Extremely little!\n\nIt might not be much evidence for LWers, who are already steeped in arguments and evidence about AI risk. It should be a lot of evidence for people newer to this topic who start with a skeptical prior. Most books making extreme-sounding (conditional) claims about the future don't have endorsements from Nobel-winning economists, former White House officials, retired generals, computer security experts, etc. on the back cover."
    },
    "baseScore": 40,
    "voteCount": 23,
    "createdAt": null,
    "postedAt": "2025-06-19T18:36:21.576Z",
    "af": false
  },
  {
    "_id": "rChcAGBkXmCgbozL4",
    "postId": "khmpWJnGJnuyPdipE",
    "parentCommentId": "GGYYbgEaoE9BFsLLq",
    "topLevelCommentId": "GGYYbgEaoE9BFsLLq",
    "contents": {
      "markdown": "We're still working out some details on the preorder events; we'll have an announcement with more info on LessWrong, the [MIRI Newsletter](https://intelligence.org/contact/), and our [Twitter](https://x.com/MIRIBerkeley) in the next few weeks.\n\nYou don't have to do anything special to get invited to preorder-only events. :) In the case of Nate's LessOnline Q&A, it was a relatively small in-person event for LessOnline attendees who had preordered the book; the main events we have planned for the future will be larger and online, so more people can participate without needing to be in the Bay Area.\n\n(Though we're considering hosting one or more in-person events at some point in the future; if so, those would be advertised more widely as well.)",
      "plaintextDescription": "We're still working out some details on the preorder events; we'll have an announcement with more info on LessWrong, the MIRI Newsletter, and our Twitter in the next few weeks.\n\nYou don't have to do anything special to get invited to preorder-only events. :) In the case of Nate's LessOnline Q&A, it was a relatively small in-person event for LessOnline attendees who had preordered the book; the main events we have planned for the future will be larger and online, so more people can participate without needing to be in the Bay Area.\n\n(Though we're considering hosting one or more in-person events at some point in the future; if so, those would be advertised more widely as well.)"
    },
    "baseScore": 10,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-06-18T22:36:18.432Z",
    "af": false
  },
  {
    "_id": "xPSwrFfXdqZzj5H9J",
    "postId": "khmpWJnGJnuyPdipE",
    "parentCommentId": "Aph2XbhYGsdvZbZMj",
    "topLevelCommentId": "Aph2XbhYGsdvZbZMj",
    "contents": {
      "markdown": "\"Inventor\" is correct!",
      "plaintextDescription": "\"Inventor\" is correct!"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-18T17:57:53.526Z",
    "af": false
  },
  {
    "_id": "TiieqCmCNGfDP9K6s",
    "postId": "iNsy7MsbodCyNTwKs",
    "parentCommentId": "TuwEvZNqevyxCm2Bm",
    "topLevelCommentId": "TuwEvZNqevyxCm2Bm",
    "contents": {
      "markdown": "> Hopefully a German pre-order from a local bookstore will make a difference.\n\nYep, this counts! :)",
      "plaintextDescription": "> Hopefully a German pre-order from a local bookstore will make a difference.\n\nYep, this counts! :)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-03T03:29:43.065Z",
    "af": false
  },
  {
    "_id": "ApPqCRkwwa2xNbQdf",
    "postId": "iNsy7MsbodCyNTwKs",
    "parentCommentId": "PkFQtbRraD8tg3Z7x",
    "topLevelCommentId": "PkFQtbRraD8tg3Z7x",
    "contents": {
      "markdown": "It's a bit complicated, but after looking into this and weighing this against other factors, MIRI and our publisher both think that the best option is for people to just buy it when they think to buy it -- the sooner, the better.\n\nWhether you're buying on Amazon or elsewhere, on net I think it's a fair bit better to buy now than to wait.",
      "plaintextDescription": "It's a bit complicated, but after looking into this and weighing this against other factors, MIRI and our publisher both think that the best option is for people to just buy it when they think to buy it -- the sooner, the better.\n\nWhether you're buying on Amazon or elsewhere, on net I think it's a fair bit better to buy now than to wait."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-29T21:15:11.061Z",
    "af": false
  },
  {
    "_id": "rtL3jyHyiKaRGpQiA",
    "postId": "iNsy7MsbodCyNTwKs",
    "parentCommentId": "RBFNNobmaRmzkAvHX",
    "topLevelCommentId": "uw5ubqPnTqkEmEase",
    "contents": {
      "markdown": "Yeah, I think the book is going to be (by a very large margin) the best resource in the world for this sort of use case. (Though I'm potentially biased as a MIRI employee.) We're not delaying; this is basically as fast as the publishing industry goes, and we expected the audience to be a lot smaller if we self-published. (A more typical timeline would have put the book another 3-20 months out.)\n\nIf Eliezer and Nate could release it sooner than September while still gaining the benefits of working with a top publishing house, doing a conventional media tour, etc., then we'd definitely be releasing it immediately. As is, our publisher has done a ton of great work already and has been extremely enthusiastic about this project, in a way that makes me feel way better about this approach. \"We have to wait till September\" is a real cost of this option, but I think it's a pretty unavoidable cost given that we need this book to reach a *lot* of people, not just the sort of people who would hear about it from a friend on LessWrong.\n\nI do think there are a lot of good resources already online, like MIRI's recently released intro resource, \"[The Problem](https://intelligence.org/the-problem/)\". It's a *very* different beast from *If Anyone Build It, Everyone Dies* (mainly written by different people, and independent of the whole book-writing process), and once the book comes out I'll consider the book strictly better for anyone willing to read something longer. But I think \"The Problem\" is a really good overview in its own right, and I expect to continue citing it regularly, because having something shorter and free-to-read does matter a lot.\n\nSome other resources I especially like include:\n\n*   Gabriel Alfour's [Preventing Extinction from Superintelligence](https://cognition.cafe/p/preventing-extinction-from-superintelligence), for a quick and to-the-point overview of the situation.\n*   Ian Hogarth's [We Must Slow Down the Race to God-Like AI](https://www.ft.com/content/03895dc4-a3b7-481e-95cc-336a524f2ac2) (requires *Financial Times* access), for an overview with a bit more discussion of recent AI progress.\n*   The AI Futures Project's [AI 2027](https://ai-2027.com/), for a discussion focused on very near-term disaster scenarios. (See also a [response from Max Harms](https://intelligence.org/2025/04/09/thoughts-on-ai-2027/), who works at MIRI.)\n*   MIRI's [AGI Ruin](https://intelligence.org/agi-ruin/), for people who want a more thorough and (semi)technical \"why does AGI alignment look hard?\" argument. This is a tweaked version of the LW [AGI Ruin post](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities), with edits aimed at making the essay more useful to share around widely. (The original post kinda assumed you were vaguely in the LW/EA ecosystem.)",
      "plaintextDescription": "Yeah, I think the book is going to be (by a very large margin) the best resource in the world for this sort of use case. (Though I'm potentially biased as a MIRI employee.) We're not delaying; this is basically as fast as the publishing industry goes, and we expected the audience to be a lot smaller if we self-published. (A more typical timeline would have put the book another 3-20 months out.)\n\nIf Eliezer and Nate could release it sooner than September while still gaining the benefits of working with a top publishing house, doing a conventional media tour, etc., then we'd definitely be releasing it immediately. As is, our publisher has done a ton of great work already and has been extremely enthusiastic about this project, in a way that makes me feel way better about this approach. \"We have to wait till September\" is a real cost of this option, but I think it's a pretty unavoidable cost given that we need this book to reach a lot of people, not just the sort of people who would hear about it from a friend on LessWrong.\n\nI do think there are a lot of good resources already online, like MIRI's recently released intro resource, \"The Problem\". It's a very different beast from If Anyone Build It, Everyone Dies (mainly written by different people, and independent of the whole book-writing process), and once the book comes out I'll consider the book strictly better for anyone willing to read something longer. But I think \"The Problem\" is a really good overview in its own right, and I expect to continue citing it regularly, because having something shorter and free-to-read does matter a lot.\n\nSome other resources I especially like include:\n\n * Gabriel Alfour's Preventing Extinction from Superintelligence, for a quick and to-the-point overview of the situation.\n * Ian Hogarth's We Must Slow Down the Race to God-Like AI (requires Financial Times access), for an overview with a bit more discussion of recent AI progress.\n * The AI Futures Project's AI 2027, for a discussion fo"
    },
    "baseScore": 39,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2025-05-21T21:49:49.933Z",
    "af": false
  },
  {
    "_id": "ovRjia6RacCSv6JLu",
    "postId": "iNsy7MsbodCyNTwKs",
    "parentCommentId": "RnbvCCf6fuZBJfcWJ",
    "topLevelCommentId": "RnbvCCf6fuZBJfcWJ",
    "contents": {
      "markdown": "In my experience, \"normal\" folks are often surprisingly open to these arguments, and I think the book is remarkably normal-person-friendly given its topic. I'd mainly recommend telling your friends what you actually think, and using practice to get better at it.\n\nContext: One of the biggest bottlenecks on the world surviving, IMO, is the amount (and quality!) of society-wide discourse about ASI. As a consequence, I already thought one of the most useful things most people can do nowadays is to just raise the alarm with more people, and raise the bar on the quality of discourse about this topic. I'm treating the book as an important lever in that regard (and an important lever for other big bottlenecks, like informing the national security community in particular). Whether you have a large audience or just a network of friends you're talking to, this is how snowballs get started.\n\nÂ If you're just looking for text you can quote to get people interested, I've been using:\n\n> **As the AI industry scrambles to build increasingly capable and general AI, two researchers speak out about a disaster on the horizon.**\n> \n> In 2023, hundreds of AI scientists and leaders in the field, including the three most cited living AI scientists, signed an [open letter](https://safe.ai/work/statement-on-ai-risk) warning that AI poses a serious risk of causing *human extinction*. Today, however, the AI race is only heating up. Tech CEOs are setting their sights on smarter-than-human AI. If they succeed, the world is radically unprepared for what comes next.\n> \n> In this book, Eliezer Yudkowsky and Nate Soares explain the nature of the threat posed by smarter-than-human AI. In a conflict between humans and AI, a superintelligence would win, as easily as modern chess AIs crush the world's best humans at chess. The conflict would not be close, or even especially interesting.\n> \n> The world is racing to build something truly new under the sun. And if anyone builds it, everyone dies.\n\nStephen Fry's blurb from Nate's post above might also be helpful here:\n\n> The most important book Iâve read for years: I want to bring it to every political and corporate leader in the world and stand over them until theyâve read it. Yudkowsky and Soares, who have studied AI and its possible trajectories for decades, sound a loud trumpet call to humanity to awaken us as we sleepwalk into disaster. Their brilliant gift for analogy, metaphor and parable clarifies for the general reader the tangled complexities of AI engineering, cognition and neuroscience better than any book on the subject Iâve ever read, and Iâve waded through scores of them. We really must rub our eyes and wake the **** up!\n\nIf your friends are looking for additional social proof that this is a serious issue, you could cite things like the [Secretary-General of the United Nations](https://x.com/gcolbourn/status/1669104216307015680):\n\n> Alarm bells over the latest form of artificial intelligence, generative AI, are deafening. And they are loudest from the developers who designed it. These scientists and experts have called on the world to act, declaring AI an existential threat to humanity on par with the risk of nuclear war. We must take those warnings seriously.\n\n(This is me spitballing ideas; if a bunch of LWers take a crack at figuring out useful things to say, I expect at least some people to have better ideas.)\n\nYou could also try sending your friends an online AI risk explainer, e.g., MIRI's [The Problem](https://intelligence.org/the-problem/) or Ian Hogarth's [We Must Slow Down the Race to God-Like AI](https://www.ft.com/content/03895dc4-a3b7-481e-95cc-336a524f2ac2) (requires *Financial Times* access) or Gabriel Alfour's [Preventing Extinction from Superintelligence](https://cognition.cafe/p/preventing-extinction-from-superintelligence).",
      "plaintextDescription": "In my experience, \"normal\" folks are often surprisingly open to these arguments, and I think the book is remarkably normal-person-friendly given its topic. I'd mainly recommend telling your friends what you actually think, and using practice to get better at it.\n\nContext: One of the biggest bottlenecks on the world surviving, IMO, is the amount (and quality!) of society-wide discourse about ASI. As a consequence, I already thought one of the most useful things most people can do nowadays is to just raise the alarm with more people, and raise the bar on the quality of discourse about this topic. I'm treating the book as an important lever in that regard (and an important lever for other big bottlenecks, like informing the national security community in particular). Whether you have a large audience or just a network of friends you're talking to, this is how snowballs get started.\n\nÂ If you're just looking for text you can quote to get people interested, I've been using:\n\n> As the AI industry scrambles to build increasingly capable and general AI, two researchers speak out about a disaster on the horizon.\n> \n> In 2023, hundreds of AI scientists and leaders in the field, including the three most cited living AI scientists, signed an open letter warning that AI poses a serious risk of causing human extinction. Today, however, the AI race is only heating up. Tech CEOs are setting their sights on smarter-than-human AI. If they succeed, the world is radically unprepared for what comes next.\n> \n> In this book, Eliezer Yudkowsky and Nate Soares explain the nature of the threat posed by smarter-than-human AI. In a conflict between humans and AI, a superintelligence would win, as easily as modern chess AIs crush the world's best humans at chess. The conflict would not be close, or even especially interesting.\n> \n> The world is racing to build something truly new under the sun. And if anyone builds it, everyone dies.\n\nStephen Fry's blurb from Nate's post above might also be help"
    },
    "baseScore": 27,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-05-21T21:07:40.899Z",
    "af": false
  },
  {
    "_id": "AEeufYed8Sudq8F5X",
    "postId": "iNsy7MsbodCyNTwKs",
    "parentCommentId": "ezjCuXbfSLsSZNWRT",
    "topLevelCommentId": "ezjCuXbfSLsSZNWRT",
    "contents": {
      "markdown": "There's a professional Russian translator lined up for the book already, though we may need volunteer help with translating the online supplements. I'll keep you (and others who have offered) in mind for that -- thanks, Tapatakt. :)",
      "plaintextDescription": "There's a professional Russian translator lined up for the book already, though we may need volunteer help with translating the online supplements. I'll keep you (and others who have offered) in mind for that -- thanks, Tapatakt. :)"
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-05-21T17:40:44.278Z",
    "af": false
  },
  {
    "_id": "Wiym3YJQYJ46AqdpY",
    "postId": "iNsy7MsbodCyNTwKs",
    "parentCommentId": "wTicNYKdNNuCtnmsD",
    "topLevelCommentId": "zWAr3k9aHNb9kKsiJ",
    "contents": {
      "markdown": "Yep! This is the first time I'm hearing the claim that hardcover matters more for bestseller lists; but I do believe hardcover preorders matter a bit more than audiobook preorders (which matters a bit more than ebook preorders). I was assuming the mechanism for this is that they provide different amounts of evidence about print demand, and thereby influence the print run a bit differently. AFAIK all the options are solidly great, though; mostly I'd pick the one(s) that you actually want the most.",
      "plaintextDescription": "Yep! This is the first time I'm hearing the claim that hardcover matters more for bestseller lists; but I do believe hardcover preorders matter a bit more than audiobook preorders (which matters a bit more than ebook preorders). I was assuming the mechanism for this is that they provide different amounts of evidence about print demand, and thereby influence the print run a bit differently. AFAIK all the options are solidly great, though; mostly I'd pick the one(s) that you actually want the most."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-15T00:03:51.498Z",
    "af": false
  },
  {
    "_id": "g8djJn6rWbicJPuqG",
    "postId": "F8sfrbPjCQj4KwJqn",
    "parentCommentId": "p7tcnwxKEFCokoREw",
    "topLevelCommentId": "aAGwttgKYtshtezRF",
    "contents": {
      "markdown": "I didn't cross-post it, but I've poked EY about the title!",
      "plaintextDescription": "I didn't cross-post it, but I've poked EY about the title!"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-09-24T16:51:10.640Z",
    "af": false
  },
  {
    "_id": "2KEKJkbtAofa9DQor",
    "postId": "j2W3zs7KTZXt2Wzah",
    "parentCommentId": "X7iBYqQzvEgsppcTb",
    "topLevelCommentId": "X7iBYqQzvEgsppcTb",
    "contents": {
      "markdown": "> I feel pretty frustrated at how rarely people actually bet or make quantitative predictions aboutÂ *existential risk from AI*. EG[Â my recent attempt](https://www.lesswrong.com/posts/AWoZBzxdm4DoGgiSj/ability-to-solve-long-horizon-tasks-correlates-with-wanting?commentId=wtuQasHREjgzxDcb4) to operationalize a bet with Nate went nowhere. Paul trying to get Eliezer to bet during the MIRI dialogues also went nowhere, or barely anywhereâI think they ended up making some random bet about how long an IMO challenge would take to be solved by AI. (feels pretty weak and unrelated to me. lame. but huge props to Paul for being so ready to bet, that made me take him a lot more seriously.)\n\nThis paragraph doesn't seem like an honest summary to me. Eliezer's position in the dialogue, as I understood it, was:\n\n*   The journey is a lot harder to predict than the destination. Cf. \"it's easier to use physics arguments to predict that humans will one day send a probe to the Moon, than it is to predict when this will happen or what the specific capabilities of rockets five years from now will be\". Eliezer isn't claiming to have secret insights about the detailed year-to-year or month-to-month changes in the field; if he thought that, he'd have been making those near-term tech predictions already back in 2010, 2015, or 2020 to show that he has this skill.\n*   From Eliezer's perspective, Paul *is* claiming to know a lot about the future trajectory of AI, and not just about the endpoints: Paul thinks progress will be relatively smooth and continuous, and thinks it will get increasingly smooth and continuous as time passes and more resources flow into the field. Eliezer, by contrast, expects the field to get *choppier* as time passes and we get closer to ASI.\n*   A way to bet on this, which Eliezer repeatedly proposed but wasn't able to get Paul to do very much, would be for Paul to list out a bunch of concrete predictions that Paul sees as \"yep, this is what smooth and continuous progress looks like\". Then, even though Eliezer doesn't necessarily have a concrete \"nope, the future will go like X instead of Y\" prediction, he'd be willing to bet against a *portfolio* of Paul-predictions: when you expect the future to be more unpredictable, you're willing to at least weakly bet against *any* sufficiently ambitious pool of concrete predictions.\n*   (Also, if Paul generated a ton of predictions like that, an occasional prediction might indeed make Eliezer go \"oh wait, I do have a strong prediction on that question in particular; I didn't realize this was one of our points of disagreement\". I don't think this is where most of the action is, but it's at least a nice side-effect of the person-who-thinks-this-tech-is-way-more-predictable spelling out predictions.)\n\nEliezer was *also* more interested in trying to reach mutual understanding of the views on offer, as opposed to *bet let's bet on things immediately never mind the world-views*. But insofar as Paul really wanted to have the bets conversation instead, Eliezer sunk an *awful* lot of time into trying to find operationalizations Paul and he could bet on, over many hours of conversation.\n\nIf your end-point take-away from that (*even after actual bets were in fact made, and tons of different high-level predictions were sketched out*) is \"wow how dare Eliezer be so unwilling to make bets on anything\", then I feel a lot less hope that world-models like Eliezer's (\"long-term outcome is more predictable than the detailed year-by-year tech pathway\") are going to be given a remotely fair hearing.\n\n(Also, in fairness to Paul, I'd say that he spent a bunch of time working with Eliezer to try to understand the basic methodologies and foundations for their perspectives on the world. I think both Eliezer and Paul did an admirable job going back and forth between the thing Paul wanted to focus on and the thing Eliezer wanted to focus on, letting us look at a bunch of different parts of the elephant. And I don't think it was *unhelpful* for Paul to try to identify operationalizations and bets, as part of the larger discussion; I just disagree with TurnTrout's summary of what happened.)",
      "plaintextDescription": "> I feel pretty frustrated at how rarely people actually bet or make quantitative predictions aboutÂ existential risk from AI. EGÂ my recent attempt to operationalize a bet with Nate went nowhere. Paul trying to get Eliezer to bet during the MIRI dialogues also went nowhere, or barely anywhereâI think they ended up making some random bet about how long an IMO challenge would take to be solved by AI. (feels pretty weak and unrelated to me. lame. but huge props to Paul for being so ready to bet, that made me take him a lot more seriously.)\n\nThis paragraph doesn't seem like an honest summary to me. Eliezer's position in the dialogue, as I understood it, was:\n\n * The journey is a lot harder to predict than the destination. Cf. \"it's easier to use physics arguments to predict that humans will one day send a probe to the Moon, than it is to predict when this will happen or what the specific capabilities of rockets five years from now will be\". Eliezer isn't claiming to have secret insights about the detailed year-to-year or month-to-month changes in the field; if he thought that, he'd have been making those near-term tech predictions already back in 2010, 2015, or 2020 to show that he has this skill.\n * From Eliezer's perspective, Paul is claiming to know a lot about the future trajectory of AI, and not just about the endpoints: Paul thinks progress will be relatively smooth and continuous, and thinks it will get increasingly smooth and continuous as time passes and more resources flow into the field. Eliezer, by contrast, expects the field to get choppier as time passes and we get closer to ASI.\n * A way to bet on this, which Eliezer repeatedly proposed but wasn't able to get Paul to do very much, would be for Paul to list out a bunch of concrete predictions that Paul sees as \"yep, this is what smooth and continuous progress looks like\". Then, even though Eliezer doesn't necessarily have a concrete \"nope, the future will go like X instead of Y\" prediction, he'd be willing "
    },
    "baseScore": 17,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2024-06-16T19:17:07.930Z",
    "af": false
  },
  {
    "_id": "wRQydSKcEany5zXmc",
    "postId": "j2W3zs7KTZXt2Wzah",
    "parentCommentId": "J3HoHz59KZBYMEP75",
    "topLevelCommentId": "X7iBYqQzvEgsppcTb",
    "contents": {
      "markdown": "> If I was misreading the blog post at the time, how come it seems like *almost no one ever explicitly predicted at the time that these particular problems were trivial for systems below or at human-level intelligence?!?*Â \n\nQuoting the abstract of MIRI's \"[The Value Learning Problem](https://intelligence.org/files/ValueLearningProblem.pdf)\" paper (emphasis added):\n\n> Autonomous AI systemsâ programmed goals can easily fall short of programmersâ intentions. **Even a machine intelligent enough to understand its designersâ intentions would not necessarily act as intended.** We discuss early ideas on how one might design smarter-than-human AI systems that can inductively learn what to value from labeled training data, and highlight questions about the construction of systems that model and act upon their operatorsâ preferences.\n\nAnd quoting from the first page of that paper:\n\n> The novelty here is not that programs can exhibit incorrect or counter-intuitive behavior, but that software agents smart enough to *understand* natural language may still *base their decisions* on misrepresentations of their programmersâ intent. The idea of superintelligent agents monomaniacally pursuing âdumbâ-seeming goals may sound odd, but it follows from the observation of Bostrom and Yudkowsky \\[2014, chap. 7\\] that AI capabilities and goals are logically independent.1 Humans can fully comprehend that their âdesignerâ (evolution) had a particular âgoalâ (reproduction) in mind for sex, without thereby feeling compelled to forsake contraception. Instilling oneâs tastes or moral values into an heir isnât impossible, but it also doesnât happen automatically.\n\nI won't weigh in on how many LessWrong posts at the time were confused about where the core of the problem lies. But \"The Value Learning Problem\" was one of the seven core papers in which MIRI laid out [our first research agenda](https://intelligence.org/technical-agenda/), so I don't think \"we're centrally worried about things that are capable enough to understand what we want, but that don't have the right goals\" was in any way hidden or treated as minor back in 2014-2015.\n\nI also wouldn't say \"MIRI predicted that NLP will largely fall years before AI can match e.g. the best human mathematicians, or the best scientists\", and if we saw a way to leverage that surprise to take a big bite out of the central problem, that would be a big positive update.\n\nI'd say:\n\n*   MIRI mostly just didn't make predictions about the exact path ML would take to get to superintelligence, and we've said we didn't expect this to be very predictable because \"the journey is harder to predict than the destination\". (Cf. \"it's easier to use physics arguments to predict that humans will one day send a probe to the Moon, than it is to predict when this will happen or what the specific capabilities of rockets five years from now will be\".)\n*   Back in 2016-2017, I think various people at MIRI updated to median timelines in the 2030-2040 range (after having had longer timelines before that), and our timelines haven't jumped around a ton since then (though they've gotten a little bit longer or shorter here and there).\n    *   So in some sense, qualitatively eyeballing the field, we don't feel surprised by \"the total amount of progress the field is exhibiting\", because it looked in 2017 like the field was just getting started, there was likely an enormous amount more you could do with 2017-style techniques (and variants on them) than had already been done, and there was likely to be a lot more money and talent flowing into the field in the coming years.\n    *   But \"the total amount of progress over the last 7 years doesn't seem that shocking\" is very different from \"we predicted what that progress would look like\". AFAIK we mostly didn't have strong guesses about that, though I think it's totally fine to say that the GPT series is *more* surprising to the circa-2017 MIRI than a lot of other paths would have been.\n    *   (Then again, we'd have expected *something* surprising to happen here, because it would be weird if our low-confidence visualizations of the mainline future just happened to line up with what happened. You can expect to be surprised a bunch without being able to guess where the surprises will come from; and in that situation, there's obviously less to be gained from putting out a bunch of predictions you don't particularly believe in.)\n*   Pre-deep-learning-revolution, we made early predictions like \"just throwing more compute at the problem without gaining deep new insights into intelligence is less likely to be the key thing that gets us there\", which was falsified. But that was a relatively high-level prediction; post-deep-learning-revolution we haven't claimed to know much about how advances are going to be sequenced.\n*   We *have* been quite interested in hearing from *others* about their advance prediction record: it's a lot easier to say \"I personally have no idea what the qualitative capabilities of GPT-2, GPT-3, etc. will be\" than to say \"... and no one else knows either\", and if someone has an amazing track record at guessing a lot of those qualitative capabilities, I'd be interested to hear about their further predictions. We're generally pessimistic that \"which of these specific systems will first unlock a specific qualitative capability?\" is particularly predictable, but this claim can be tested via people actually making those predictions.",
      "plaintextDescription": "> If I was misreading the blog post at the time, how come it seems like almost no one ever explicitly predicted at the time that these particular problems were trivial for systems below or at human-level intelligence?!?Â \n\nQuoting the abstract of MIRI's \"The Value Learning Problem\" paper (emphasis added):\n\n> Autonomous AI systemsâ programmed goals can easily fall short of programmersâ intentions. Even a machine intelligent enough to understand its designersâ intentions would not necessarily act as intended. We discuss early ideas on how one might design smarter-than-human AI systems that can inductively learn what to value from labeled training data, and highlight questions about the construction of systems that model and act upon their operatorsâ preferences.\n\nAnd quoting from the first page of that paper:\n\n> The novelty here is not that programs can exhibit incorrect or counter-intuitive behavior, but that software agents smart enough to understand natural language may still base their decisions on misrepresentations of their programmersâ intent. The idea of superintelligent agents monomaniacally pursuing âdumbâ-seeming goals may sound odd, but it follows from the observation of Bostrom and Yudkowsky [2014, chap. 7] that AI capabilities and goals are logically independent.1 Humans can fully comprehend that their âdesignerâ (evolution) had a particular âgoalâ (reproduction) in mind for sex, without thereby feeling compelled to forsake contraception. Instilling oneâs tastes or moral values into an heir isnât impossible, but it also doesnât happen automatically.\n\nI won't weigh in on how many LessWrong posts at the time were confused about where the core of the problem lies. But \"The Value Learning Problem\" was one of the seven core papers in which MIRI laid out our first research agenda, so I don't think \"we're centrally worried about things that are capable enough to understand what we want, but that don't have the right goals\" was in any way hidden or treated as min"
    },
    "baseScore": 21,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-06-16T18:21:01.452Z",
    "af": false
  },
  {
    "_id": "vNpRbBmeHWMJ6eQWP",
    "postId": "GdBwsYWGytXrkniSy",
    "parentCommentId": "RtG6gtwjjkRDfwAyB",
    "topLevelCommentId": "RtG6gtwjjkRDfwAyB",
    "contents": {
      "markdown": "> But the benefit of a Pause is that you [use the extra time to do something in particular](https://www.lesswrong.com/posts/tKk37BFkMzchtZThx/miri-2024-communications-strategy?commentId=Bohdww3hddWA3NNnZ). Why wouldn't you want to fiscally sponsor research on problems that you think need to be solved for the future of Earth-originating intelligent life to go well?Â \n\nMIRI still sponsors some alignment research, and I expect we'll sponsor more alignment research directions in the future. I'd say MIRI leadership didn't have enough aggregate hope in Agent Foundations in particular to want to keep supporting it ourselves (though I consider its existence net-positive).\n\nMy model of MIRI is that our main focus these days is \"find ways to make it likelier that a halt occurs\" and \"improve the world's general understanding of the situation in case this helps someone come up with a [better idea](https://www.lesswrong.com/posts/Zp6wG5eQFLGWwcG6j/focus-on-the-places-where-you-feel-shocked-everyone-s)\", but that we're also pretty open to taking on projects in all four of these quadrants, if we find something that's promising and that seems like a good fit at MIRI (or something promising that seems unlikely to occur if it's not housed at MIRI):\n\n| Â  | AI alignment work | Non-alignment work |\n| --- | --- | --- |\n| **High-EV absent a pause** | Â  | Â  |\n| **High-EV given a pause** | Â  | Â  |",
      "plaintextDescription": "> But the benefit of a Pause is that you use the extra time to do something in particular. Why wouldn't you want to fiscally sponsor research on problems that you think need to be solved for the future of Earth-originating intelligent life to go well?Â \n\nMIRI still sponsors some alignment research, and I expect we'll sponsor more alignment research directions in the future. I'd say MIRI leadership didn't have enough aggregate hope in Agent Foundations in particular to want to keep supporting it ourselves (though I consider its existence net-positive).\n\nMy model of MIRI is that our main focus these days is \"find ways to make it likelier that a halt occurs\" and \"improve the world's general understanding of the situation in case this helps someone come up with a better idea\", but that we're also pretty open to taking on projects in all four of these quadrants, if we find something that's promising and that seems like a good fit at MIRI (or something promising that seems unlikely to occur if it's not housed at MIRI):\n\nÂ AI alignment workNon-alignment workHigh-EV absent a pauseÂ Â High-EV given a pauseÂ Â "
    },
    "baseScore": 11,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-06-16T07:33:16.020Z",
    "af": false
  },
  {
    "_id": "8YYPf5dMe27JHHwn7",
    "postId": "Yig9oa4zGE97xM2os",
    "parentCommentId": "MmTgxHAwwrmFibx9M",
    "topLevelCommentId": "rdqdQjxbCaPT2mQYL",
    "contents": {
      "markdown": "> I don't find this convincing. I think the target \"dumb enough to be safe, honest, trustworthy, relatively non-agentic, etc., but smart enough to be super helpful for alignment\" is narrow (or just nonexistent, using the methods we're likely to have on hand).\n> \n> Even if this *exists*, verification seems extraordinarily difficult: how do we know that the system is being honest? *Separately*, how do we verify that its solutions are correct? Checking answers is *sometimes* easier than generating them, but only to a limited degree, and alignment seems like a case where checking is particularly difficult.\n\nIt's also important to keep in mind that on Leopold's model (and my own), these problems need to be solved under a ton of time pressure. To maintain a lead, the USG in Leopold's scenario will often need to figure out some of these \"under what circumstances can we trust this highly novel system and believe its alignment answers?\" issues in a matter of weeks or perhaps months, so that the overall alignment project can complete in a very short window of time. This is not a situation where we're imagining having a ton of time to develop mastery and deep understanding of these new models. (Or mastery of the alignment problem sufficient to verify when a new idea is on the right track or not.)",
      "plaintextDescription": "> I don't find this convincing. I think the target \"dumb enough to be safe, honest, trustworthy, relatively non-agentic, etc., but smart enough to be super helpful for alignment\" is narrow (or just nonexistent, using the methods we're likely to have on hand).\n> \n> Even if this exists, verification seems extraordinarily difficult: how do we know that the system is being honest? Separately, how do we verify that its solutions are correct? Checking answers is sometimes easier than generating them, but only to a limited degree, and alignment seems like a case where checking is particularly difficult.\n\nIt's also important to keep in mind that on Leopold's model (and my own), these problems need to be solved under a ton of time pressure. To maintain a lead, the USG in Leopold's scenario will often need to figure out some of these \"under what circumstances can we trust this highly novel system and believe its alignment answers?\" issues in a matter of weeks or perhaps months, so that the overall alignment project can complete in a very short window of time. This is not a situation where we're imagining having a ton of time to develop mastery and deep understanding of these new models. (Or mastery of the alignment problem sufficient to verify when a new idea is on the right track or not.)"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-06-09T14:41:05.539Z",
    "af": false
  },
  {
    "_id": "MmTgxHAwwrmFibx9M",
    "postId": "Yig9oa4zGE97xM2os",
    "parentCommentId": "5bGigTiAfSCLCBvhP",
    "topLevelCommentId": "rdqdQjxbCaPT2mQYL",
    "contents": {
      "markdown": "> one positive feature it does have, it proposes to rely on a multitude of \"limited weakly-superhuman artificial alignment researchers\" and makes a reasonable case that those can be obtained in a form factor which is alignable and controllable.\n\nI don't find this convincing. I think the target \"dumb enough to be safe, honest, trustworthy, relatively non-agentic, etc., but smart enough to be super helpful for alignment\" is narrow (or just nonexistent, using the methods we're likely to have on hand).\n\nEven if this *exists*, verification seems extraordinarily difficult: how do we know that the system is being honest? *Separately*, how do we verify that its solutions are correct? Checking answers is *sometimes* easier than generating them, but only to a limited degree, and alignment seems like a case where checking is particularly difficult.\n\n> You and Leopold seem to share the assumption that huge GPU farms or equivalently strong compute are *necessary* for superintelligence.\n\nNope! I don't assume that.\n\nI do think that it's likely the first world-endangering AI is trained using more compute than was used to train GPT-4; but I'm certainly not confident of that prediction, and I don't think it's possible to make reasonable predictions (given our current knowledge state) about *how* much more compute might be needed.\n\n(\"Needed\" for the first world-endangeringly powerful AI humans actually build, that is. I feel confident that you can in principle build world-endangeringly powerful AI with far less compute than was used to train GPT-4; but the first lethally powerful AI systems humans actually build will presumably be far from the limits of what's physically possible!)\n\n> But what would happen if one effectively closes that path? There will be huge selection pressure to look for alternative routes, to invest more heavily in those algorithmic breakthroughs which can work with modest GPU power or even with CPUs.\n\nAgreed. This is why I support humanity working on things like human enhancement and (plausibly) AI alignment, in parallel with working on an international AI development pause. I don't think that a pause on its own is a permanent solution, though if we're lucky and the laws are well-designed I imagine it could buy humanity quite a few decades.\n\n> I hope people will step back from solely focusing on *advocating* for policy-level prescriptions (as none of the existing policy-level prescriptions look particularly promising at the moment) and invest some of their time in continuing object-level discussions of AI existential safety without predefined political ends.\n\nFWIW, MIRI does already think of \"generally spreading reasonable discussion of the problem, and trying to increase the probability that someone comes up with some new promising idea for addressing x-risk\" as a top organizational priority.\n\nThe usual internal framing is some version of \"we have our own current best guess at how to save the world, but our idea is a massive longshot, and not the sort of basket humanity should put all its eggs in\". I think \"AI pause + some form of cognitive enhancement\" should be a top priority, but I also consider it a top priority for humanity to try to find other potential paths to a good future.",
      "plaintextDescription": "> one positive feature it does have, it proposes to rely on a multitude of \"limited weakly-superhuman artificial alignment researchers\" and makes a reasonable case that those can be obtained in a form factor which is alignable and controllable.\n\nI don't find this convincing. I think the target \"dumb enough to be safe, honest, trustworthy, relatively non-agentic, etc., but smart enough to be super helpful for alignment\" is narrow (or just nonexistent, using the methods we're likely to have on hand).\n\nEven if this exists, verification seems extraordinarily difficult: how do we know that the system is being honest? Separately, how do we verify that its solutions are correct? Checking answers is sometimes easier than generating them, but only to a limited degree, and alignment seems like a case where checking is particularly difficult.\n\n> You and Leopold seem to share the assumption that huge GPU farms or equivalently strong compute are necessary for superintelligence.\n\nNope! I don't assume that.\n\nI do think that it's likely the first world-endangering AI is trained using more compute than was used to train GPT-4; but I'm certainly not confident of that prediction, and I don't think it's possible to make reasonable predictions (given our current knowledge state) about how much more compute might be needed.\n\n(\"Needed\" for the first world-endangeringly powerful AI humans actually build, that is. I feel confident that you can in principle build world-endangeringly powerful AI with far less compute than was used to train GPT-4; but the first lethally powerful AI systems humans actually build will presumably be far from the limits of what's physically possible!)\n\n> But what would happen if one effectively closes that path? There will be huge selection pressure to look for alternative routes, to invest more heavily in those algorithmic breakthroughs which can work with modest GPU power or even with CPUs.\n\nAgreed. This is why I support humanity working on things like human enh"
    },
    "baseScore": 13,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-06-09T00:06:49.724Z",
    "af": false
  },
  {
    "_id": "WHwZ2aL2xnJjgtcpM",
    "postId": "Yig9oa4zGE97xM2os",
    "parentCommentId": "MkN5EpYWiMdsJucGK",
    "topLevelCommentId": "MkN5EpYWiMdsJucGK",
    "contents": {
      "markdown": "As a start, you can prohibit sufficiently large training runs. This isn't a necessary-and-sufficient condition, and doesn't necessarily solve the problem on its own, and there's room for debate about how risk changes as a function of training resources. But it's a place to start, when the field is mostly flying blind about where the risks arise; and choosing a relatively conservative threshold makes obvious sense when failing to leave enough safety buffer means human extinction. (And when algorithmic progress is likely to reduce the minimum dangerous training size over time, whatever it is today -- also a reason the cap is likely to need to lower over time to some extent, until we're out of the lethally dangerous situation we currently find ourselves in.)",
      "plaintextDescription": "As a start, you can prohibit sufficiently large training runs. This isn't a necessary-and-sufficient condition, and doesn't necessarily solve the problem on its own, and there's room for debate about how risk changes as a function of training resources. But it's a place to start, when the field is mostly flying blind about where the risks arise; and choosing a relatively conservative threshold makes obvious sense when failing to leave enough safety buffer means human extinction. (And when algorithmic progress is likely to reduce the minimum dangerous training size over time, whatever it is today -- also a reason the cap is likely to need to lower over time to some extent, until we're out of the lethally dangerous situation we currently find ourselves in.)"
    },
    "baseScore": 3,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-06-08T22:30:20.386Z",
    "af": false
  },
  {
    "_id": "JszCupLfvbPXjecnB",
    "postId": "Yig9oa4zGE97xM2os",
    "parentCommentId": "kSeT32NDJiP2LbK6M",
    "topLevelCommentId": "rdqdQjxbCaPT2mQYL",
    "contents": {
      "markdown": "> Alternatively, they either don't buy the perils or believes there's a chance the other chance may not?\n\nIf they \"don't buy the perils\", and the perils are real, then Leopold's scenario is falsified and we shouldn't be pushing for the USG to build ASI.\n\nIf there are no perils at all, then sure, Leopold's scenario and mine are both false. I didn't mean to imply that our two views are the only options.\n\nSeparately, Leopold's model of \"what are the dangers?\" is different from mine. But I don't think the dangers Leopold is worried about are dramatically easier to understand than the dangers I'm worried about (in the respective worlds where our worries are correct). Just the opposite: the level of understanding you need to literally solve alignment for superintelligences vastly exceeds the level you need to just be spooked by ASI and not want it to be built. Which is the point I was making; not \"ASI is axiomatically dangerous\", but \"this doesn't count as a strike against my plan relative to Leopold's, and in fact Leopold is making a far bigger ask of government than I am on this front\".\n\n> Nuclear war essentially has a localized p(doom) of 1\n\nI don't know what this means. If you're saying \"nuclear weapons kill the people they hit\", I don't see the relevance; guns also kill the people they hit, hut that doesn't make a gun strategically similar to a smarter-than-human AI system.",
      "plaintextDescription": "> Alternatively, they either don't buy the perils or believes there's a chance the other chance may not?\n\nIf they \"don't buy the perils\", and the perils are real, then Leopold's scenario is falsified and we shouldn't be pushing for the USG to build ASI.\n\nIf there are no perils at all, then sure, Leopold's scenario and mine are both false. I didn't mean to imply that our two views are the only options.\n\nSeparately, Leopold's model of \"what are the dangers?\" is different from mine. But I don't think the dangers Leopold is worried about are dramatically easier to understand than the dangers I'm worried about (in the respective worlds where our worries are correct). Just the opposite: the level of understanding you need to literally solve alignment for superintelligences vastly exceeds the level you need to just be spooked by ASI and not want it to be built. Which is the point I was making; not \"ASI is axiomatically dangerous\", but \"this doesn't count as a strike against my plan relative to Leopold's, and in fact Leopold is making a far bigger ask of government than I am on this front\".\n\n> Nuclear war essentially has a localized p(doom) of 1\n\nI don't know what this means. If you're saying \"nuclear weapons kill the people they hit\", I don't see the relevance; guns also kill the people they hit, hut that doesn't make a gun strategically similar to a smarter-than-human AI system."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-06-08T22:22:29.221Z",
    "af": false
  },
  {
    "_id": "238bsh9nHGzpitPkr",
    "postId": "Yig9oa4zGE97xM2os",
    "parentCommentId": "Qno22QjeL3bC2LYsh",
    "topLevelCommentId": "rdqdQjxbCaPT2mQYL",
    "contents": {
      "markdown": "Yep, I had in mind [AI Forecasting: One Year In](https://bounded-regret.ghost.io/ai-forecasting-one-year-in/).",
      "plaintextDescription": "Yep, I had in mind AI Forecasting: One Year In."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-06-08T18:52:47.279Z",
    "af": false
  },
  {
    "_id": "R7tux6wAwB22p6Ev6",
    "postId": "Yig9oa4zGE97xM2os",
    "parentCommentId": "KMt5d7RPFbGuDoWub",
    "topLevelCommentId": "QrFffZNy8j8bPBq5o",
    "contents": {
      "markdown": "Why? 95% risk of doom isn't certainty, but seems obviously more than sufficient.\n\nFor that matter, why would the USG want to build AGI if they considered it a coinflip whether this will kill everyone or not? The USG could choose the coinflip, or it could choose to try to prevent China from putting the world at risk *without* creating that risk itself. \"Sit back and watch other countries build doomsday weapons\" and \"build doomsday weapons yourself\" are not the only two options.",
      "plaintextDescription": "Why? 95% risk of doom isn't certainty, but seems obviously more than sufficient.\n\nFor that matter, why would the USG want to build AGI if they considered it a coinflip whether this will kill everyone or not? The USG could choose the coinflip, or it could choose to try to prevent China from putting the world at risk without creating that risk itself. \"Sit back and watch other countries build doomsday weapons\" and \"build doomsday weapons yourself\" are not the only two options."
    },
    "baseScore": 10,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2024-06-08T18:50:51.176Z",
    "af": false
  },
  {
    "_id": "Wgq8LqqHBMPWKKEMn",
    "postId": "Yig9oa4zGE97xM2os",
    "parentCommentId": "rdqdQjxbCaPT2mQYL",
    "topLevelCommentId": "rdqdQjxbCaPT2mQYL",
    "contents": {
      "markdown": "Leopold's scenario requires that the USG come to deeply understand all the perils and details of AGI and ASI (since they otherwise don't have a hope of building and aligning a superintelligence), but then needs to choose to gamble its hegemony, its very existence, and the lives of all its citizens on a half-baked mad science initiative, when it could simply work with its allies to block the tech's development and maintain the status quo at minimal risk.\n\nSuccess in this scenario requires a weird combination of USG prescience with self-destructiveness: enough foresight to see what's coming, but paired with a weird compulsion to race to build the very thing that puts its existence at risk, when it would potentially be vastly **easier** to spearhead an international alliance to prohibit this technology.",
      "plaintextDescription": "Leopold's scenario requires that the USG come to deeply understand all the perils and details of AGI and ASI (since they otherwise don't have a hope of building and aligning a superintelligence), but then needs to choose to gamble its hegemony, its very existence, and the lives of all its citizens on a half-baked mad science initiative, when it could simply work with its allies to block the tech's development and maintain the status quo at minimal risk.\n\nSuccess in this scenario requires a weird combination of USG prescience with self-destructiveness: enough foresight to see what's coming, but paired with a weird compulsion to race to build the very thing that puts its existence at risk, when it would potentially be vastly easier to spearhead an international alliance to prohibit this technology."
    },
    "baseScore": 5,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-06-07T22:44:18.116Z",
    "af": false
  },
  {
    "_id": "rdqdQjxbCaPT2mQYL",
    "postId": "Yig9oa4zGE97xM2os",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Responding to Matt Reardon's [point](https://forum.effectivealtruism.org/posts/RTHFCRLv34cewwMr6/response-to-aschenbrenner-s-situational-awareness?commentId=MKYazj9nwvkPr2dqp) on the EA Forum:\n\n> Leopold's implicit response as I see it:\n> \n> 1.  Convincing all stakeholders of high p(doom) such that they take decisive, coordinated action is wildly improbable (\"step 1: get everyone to agree with me\" is the foundation of many terrible plans and almost no good ones)\n> 2.  Still improbable, but less wildly, is the idea that we can steer institutions towards sensitivity to risk on the margin and that those institutions can position themselves to solve the technical and other challenges ahead\n> \n> Maybe the key insight is that both strategies walk on a knife's edge. While Moore's law, algorithmic improvement, and chip design hum along at some level, even a little breakdown in international willpower to enforce a pause/stop can rapidly convert to catastrophe. Spending a lot of effort to get that consensus also has high opportunity cost in terms of steering institutions in the world where the effort fails (and it is very likely to fail). \\[...\\]\n\nThree high-level reasons I think Leopold's plan looks a lot less workable:\n\n1.  It requires major scientific breakthroughs to occur on a very short time horizon, including unknown breakthroughs that will manifest to solve problems we don't understand or know about today.\n2.  These breakthroughs need to come in a field that has not been particularly productive or fast in the past. (Indeed, forecasters have been surprised by how slowly safety/robustness/etc. have progressed in recent years, and simultaneously surprised by the breakneck speed of capabilities.)\n3.  It requires extremely precise and correct behavior by a giant government bureaucracy that includes many staff who *won't* be the best and brightest in the field â inevitably, many technical and nontechnical people in the bureaucracy will have wrong beliefs about AGI and about alignment.\n\nThe \"extremely precise and correct behavior\" part means that we're effectively hoping to be handed an excellent bureaucracy that will rapidly and competently solve a thirty-digit combination lock requiring the invention of multiple new fields and the solving of a variety of thorny and poorly-understood technical problems â in many cases, on Leopold's view, in a space of months or weeks. This seems... not like how the real world works.\n\nIt also separately requires that various guesses about the background empirical facts all pan out. Leopold can do literally everything right and get the USG fully on board and get the *USG* doing literally everything correctly by his lights â and then the plan ends up destroying the world rather than saving it because it just happened to turn out that ASI was a lot more compute-efficient to train than he expected, resulting in the USG being unable to monopolize the tech and unable to achieve a sufficiently long lead time.\n\nMy proposal doesn't require qualitatively *that* kind of success. It requires governments to coordinate on banning things. Plausibly, it requires governments to overreact to a weird, scary, and publicly controversial new tech to some degree, since it's unlikely that governments will *exactly* hit the target we want. This is not a particularly weird ask; governments ban things (and coordinate or copy-paste each other's laws) all the time, in far less dangerous and fraught areas than AGI. This is \"trying to get the international order to lean hard in a particular direction on a yes-or-no question where there's already a lot of energy behind choosing 'no'\", not \"solving a long list of hard science and engineering problems in a matter of months and getting a bureaucracy to output the correct long string of digits to nail down all the correct technical solutions and all the correct processes to find those solutions\".\n\nThe CCP's *current* appetite for AGI seems remarkably small, and I expect them to be more worried that an AGI race would leave them in the dust (and/or put their regime at risk, and/or put their lives at risk), than excited about the opportunity such a race provides. Governments around the world *currently*, to the best of my knowledge, are nowhere near advancing any frontiers in ML.\n\nFrom my perspective, Leopold is *imagining a future problem into being* (\"all of this changes\") and then trying to find a galaxy-brained incredibly complex and assumption-laden way to wriggle out of this imagined future dilemma, when the far easier and less risky path would be to *not have the world powers race in the first place*, have them recognize that this technology is lethally dangerous (something the USG chain of command, at least, would need to fully internalize on Leopold's plan too), and have them block private labs from sending us over the precipice (again, something Leopold assumes will happen) while *not* choosing to take on the risk of destroying themselves (nor permitting other world powers to unilaterally impose that risk).",
      "plaintextDescription": "Responding to Matt Reardon's point on the EA Forum:\n\n> Leopold's implicit response as I see it:\n> \n>  1. Convincing all stakeholders of high p(doom) such that they take decisive, coordinated action is wildly improbable (\"step 1: get everyone to agree with me\" is the foundation of many terrible plans and almost no good ones)\n>  2. Still improbable, but less wildly, is the idea that we can steer institutions towards sensitivity to risk on the margin and that those institutions can position themselves to solve the technical and other challenges ahead\n> \n> Maybe the key insight is that both strategies walk on a knife's edge. While Moore's law, algorithmic improvement, and chip design hum along at some level, even a little breakdown in international willpower to enforce a pause/stop can rapidly convert to catastrophe. Spending a lot of effort to get that consensus also has high opportunity cost in terms of steering institutions in the world where the effort fails (and it is very likely to fail). [...]\n\nThree high-level reasons I think Leopold's plan looks a lot less workable:\n\n 1. It requires major scientific breakthroughs to occur on a very short time horizon, including unknown breakthroughs that will manifest to solve problems we don't understand or know about today.\n 2. These breakthroughs need to come in a field that has not been particularly productive or fast in the past. (Indeed, forecasters have been surprised by how slowly safety/robustness/etc. have progressed in recent years, and simultaneously surprised by the breakneck speed of capabilities.)\n 3. It requires extremely precise and correct behavior by a giant government bureaucracy that includes many staff who won't be the best and brightest in the field â inevitably, many technical and nontechnical people in the bureaucracy will have wrong beliefs about AGI and about alignment.\n\nThe \"extremely precise and correct behavior\" part means that we're effectively hoping to be handed an excellent bureaucracy that wil"
    },
    "baseScore": 19,
    "voteCount": 22,
    "createdAt": null,
    "postedAt": "2024-06-07T22:41:22.740Z",
    "af": false
  },
  {
    "_id": "djgzurCRFoSacRvdP",
    "postId": "2mrdHw6yM3h55bmhg",
    "parentCommentId": "v5KbvkSudw6zitwPM",
    "topLevelCommentId": "ACwkJYXBhCoCmX4JH",
    "contents": {
      "markdown": "(Though he also has an incentive to not die.)",
      "plaintextDescription": "(Though he also has an incentive to not die.)"
    },
    "baseScore": 16,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2024-06-06T23:35:12.369Z",
    "af": false
  },
  {
    "_id": "uNYe8ew8X8SDnAsaR",
    "postId": "tKk37BFkMzchtZThx",
    "parentCommentId": "fmq7HnNo8NzeMEs6u",
    "topLevelCommentId": "vsvSXjJYhYY6aAAdM",
    "contents": {
      "markdown": "As is typical for Twitter, we also signal-boosted a lot of other people's takes. Some non-MIRI people whose social media takes I've recently liked include [Wei Dai](https://twitter.com/weidai11), [Daniel Kokotajlo](https://twitter.com/DKokotajlo67142), [Jeffrey Ladish](https://twitter.com/JeffLadish), [Patrick McKenzie](https://twitter.com/patio11), [Zvi Mowshowitz](https://twitter.com/TheZvi), [Kelsey Piper](https://twitter.com/KelseyTuoc), and [Liron Shapira](https://twitter.com/liron).",
      "plaintextDescription": "As is typical for Twitter, we also signal-boosted a lot of other people's takes. Some non-MIRI people whose social media takes I've recently liked include Wei Dai, Daniel Kokotajlo, Jeffrey Ladish, Patrick McKenzie, Zvi Mowshowitz, Kelsey Piper, and Liron Shapira."
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-06-05T16:11:10.288Z",
    "af": false
  },
  {
    "_id": "fmq7HnNo8NzeMEs6u",
    "postId": "tKk37BFkMzchtZThx",
    "parentCommentId": "7cta8qKiduaaemwgd",
    "topLevelCommentId": "vsvSXjJYhYY6aAAdM",
    "contents": {
      "markdown": "The stuff I've been tweeting doesn't constitute an official MIRI statement â e.g., I don't usually run these tweets by other MIRI folks, and I'm not assuming everyone at MIRI agrees with me or would phrase things the same way. That said, some recent comments and questions from me and Eliezer:\n\n*   [May 17](1791643036268810540): Early thoughts on the news about OpenAI's crazy NDAs.\n*   [May 24](https://twitter.com/ESYudkowsky/status/1794149781419540842): Eliezer flags that GPT-4o can now pass one of Eliezer's personal ways of testing whether models are still bad at math.\n*   [May 29](https://twitter.com/robbensinger/status/1795976894099161591): My initial reaction to hearing Helen's comments on the TED AI podcast. Includes some follow-on discussion of the ChatGPT example, etc.\n*   [May 30](https://twitter.com/robbensinger/status/1796134409906626922): A conversation between me and Emmett Shear about the version of events he'd tweeted in November. (Plus a [comment from Eliezer](https://twitter.com/ESYudkowsky/status/1796200618270621951).)\n*   [May 30](https://twitter.com/ESYudkowsky/status/1796176975025029192): Eliezer signal-boosting a correction from Paul Graham.\n*   [June 4](https://twitter.com/ESYudkowsky/status/1798105252375503176): Eliezer objects to Aschenbrenner's characterization of his timelines argument as open-and-shut \"believing in straight lines on a graph\".",
      "plaintextDescription": "The stuff I've been tweeting doesn't constitute an official MIRI statement â e.g., I don't usually run these tweets by other MIRI folks, and I'm not assuming everyone at MIRI agrees with me or would phrase things the same way. That said, some recent comments and questions from me and Eliezer:\n\n * May 17: Early thoughts on the news about OpenAI's crazy NDAs.\n * May 24: Eliezer flags that GPT-4o can now pass one of Eliezer's personal ways of testing whether models are still bad at math.\n * May 29: My initial reaction to hearing Helen's comments on the TED AI podcast. Includes some follow-on discussion of the ChatGPT example, etc.\n * May 30: A conversation between me and Emmett Shear about the version of events he'd tweeted in November. (Plus a comment from Eliezer.)\n * May 30: Eliezer signal-boosting a correction from Paul Graham.\n * June 4: Eliezer objects to Aschenbrenner's characterization of his timelines argument as open-and-shut \"believing in straight lines on a graph\"."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-06-05T15:57:08.026Z",
    "af": false
  },
  {
    "_id": "BwekaxS6GDtkCzpF5",
    "postId": "tKk37BFkMzchtZThx",
    "parentCommentId": "TgZipbBvKzBuFTkDY",
    "topLevelCommentId": "TgZipbBvKzBuFTkDY",
    "contents": {
      "markdown": "> Every protest I've witnessed seemed to be designed to annoy and alienate its witnesses, making it as clear as possible that there was no way to talk to these people, that their minds were on rails. I think most people recognize that as cult shit and are alienated by that.\n\nIn the last year, I've seen a Twitter video of an AI risk protest (I think possibly in continental Europe?) that struck me as extremely good: calm, thoughtful, accessible, punchy, and sensible-sounding statements and interview answers. If I find the link again, I'll add it here as a model of what I think a robustly good protest can look like!\n\n> A leftist friend once argued that protest is not really a means, but a reward, a sort of party for those who contributed to local movementbuilding. I liked that view.\n\nI wouldn't recommend making protests *purely* this. A lot of these protests are getting news coverage and have a real chance of either intriguing/persuading or alienating potential allies; I think it's worth putting thought into how to hit the \"intriguing/persuading\" target, regardless of whether this is \"normal\" for protests.\n\nBut I like the idea of \"protest as reward\" as an *element* of protests, or as a focus for *some* protests. :)",
      "plaintextDescription": "> Every protest I've witnessed seemed to be designed to annoy and alienate its witnesses, making it as clear as possible that there was no way to talk to these people, that their minds were on rails. I think most people recognize that as cult shit and are alienated by that.\n\nIn the last year, I've seen a Twitter video of an AI risk protest (I think possibly in continental Europe?) that struck me as extremely good: calm, thoughtful, accessible, punchy, and sensible-sounding statements and interview answers. If I find the link again, I'll add it here as a model of what I think a robustly good protest can look like!\n\n> A leftist friend once argued that protest is not really a means, but a reward, a sort of party for those who contributed to local movementbuilding. I liked that view.\n\nI wouldn't recommend making protests purely this. A lot of these protests are getting news coverage and have a real chance of either intriguing/persuading or alienating potential allies; I think it's worth putting thought into how to hit the \"intriguing/persuading\" target, regardless of whether this is \"normal\" for protests.\n\nBut I like the idea of \"protest as reward\" as an element of protests, or as a focus for some protests. :)"
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-06-03T00:54:50.163Z",
    "af": false
  },
  {
    "_id": "bhJFXGLhjXRdsxNK3",
    "postId": "tKk37BFkMzchtZThx",
    "parentCommentId": "9wxu4gCo8tp6etF43",
    "topLevelCommentId": "nSdY7u2etGajmtMYu",
    "contents": {
      "markdown": "Could we talk about a specific expert you have in mind, who thinks this is a bad strategy in this particular case?\n\nAI risk is a pretty weird case, in a number of ways: it's highly counter-intuitive, not particularly politically polarized / entrenched, seems to require unprecedentedly fast and aggressive action by multiple countries, is almost maximally high-stakes, etc. \"Be careful what you say, try to look normal, and slowly accumulate political capital and connections in the hope of swaying policymakers long-term\" isn't an unconditionally good strategy, it's a strategy adapted to a particular range of situations and goals. I'd be interested in actually hearing arguments for why this strategy is the best option here, given MIRI's world-model.\n\n(Or, separately, you could argue against the world-model, if you disagree with us about how things are.)",
      "plaintextDescription": "Could we talk about a specific expert you have in mind, who thinks this is a bad strategy in this particular case?\n\nAI risk is a pretty weird case, in a number of ways: it's highly counter-intuitive, not particularly politically polarized / entrenched, seems to require unprecedentedly fast and aggressive action by multiple countries, is almost maximally high-stakes, etc. \"Be careful what you say, try to look normal, and slowly accumulate political capital and connections in the hope of swaying policymakers long-term\" isn't an unconditionally good strategy, it's a strategy adapted to a particular range of situations and goals. I'd be interested in actually hearing arguments for why this strategy is the best option here, given MIRI's world-model.\n\n(Or, separately, you could argue against the world-model, if you disagree with us about how things are.)"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-06-02T22:56:48.125Z",
    "af": false
  },
  {
    "_id": "v7w9Mcjs4Ttsk4EtP",
    "postId": "yRWv5kkDD4YhzwRLq",
    "parentCommentId": "h8yKEQ2MqBnzLGFCb",
    "topLevelCommentId": "eAFnzuF7dacHqtZyr",
    "contents": {
      "markdown": "?",
      "plaintextDescription": "?"
    },
    "baseScore": 2,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-06-02T03:20:47.475Z",
    "af": false
  },
  {
    "_id": "FLcmAhP4aorLbhN5Q",
    "postId": "tKk37BFkMzchtZThx",
    "parentCommentId": "tNcq8wwt9rF5gGKBD",
    "topLevelCommentId": "xBYimQtgASti5tgWv",
    "contents": {
      "markdown": "Two things:\n\n*   For myself, I would not feel comfortable using language as confident-sounding as \"on the default trajectory, AI is going to kill everyone\" if I assigned (e.g.) 10% probability to \"humanity \\[gets\\] a small future on a spare asteroid-turned-computer or an alien zoo or maybe even star\". I just think that scenario's way, way less likely than that.\n    *   I'd be surprised if Nate assigns 10+% probability to scenarios like that, but he can speak for himself. ð¤·ââï¸\n    *   I think some people at MIRI have significantly lower p(doom)? And I don't expect those people to use language like \"on the default trajectory, AI is going to kill everyone\".\n*   I agree with you that there's something weird about making lots of human-extinction-focused arguments when the thing we care more about is \"does the cosmic endowment get turned into paperclips\"? I do care about *both* of those things, an enormous amount; and I plan to talk about both of those things to some degree in public communications, rather than treating it as some kind of poorly-kept secret that MIRI folks care about whether flourishing interstellar civilizations get a chance to exist down the line. But I have this whole topic mentally flagged as a thing to be thoughtful and careful about, because it at least seems like an area that contains risk factors for future deceptive comms. E.g., if we update later to expecting the cosmic endowment to be wasted but all humans *not* dying, I would want us to adjust our messaging even if that means sacrificing some punchiness in our policy outreach.\n    *   Currently, however, I think the particular scenario \"AI keeps a few flourishing humans around forever\" is *incredibly* unlikely, and I don't think Eliezer, Nate, etc. would say things like \"this has a double-digit probability of happening in real life\"? And, to be honest, the idea of myself and my family and friends and every other human being all dying in the near future really fucks me up and does not seem in any sense OK, even if (with my philosopher-hat on) I think this isn't as big of a deal as \"the cosmic endowment gets wasted\".\n    *   So I don't *currently* feel bad about emphasizing a *true* prediction (\"extremely likely that literally all humans literally nonconsensually die by violent means\"), even though the philosophy-hat version of me thinks that the *separate* true prediction \"extremely likely 99+% of the potential value of the long-term future is lost\" is more morally important than that. Though I do feel obliged to semi-regularly mention the whole \"cosmic endowment\" thing in my public communication too, even if it doesn't make it into various versions of my general-audience 60-second AI risk elevator pitch.",
      "plaintextDescription": "Two things:\n\n * For myself, I would not feel comfortable using language as confident-sounding as \"on the default trajectory, AI is going to kill everyone\" if I assigned (e.g.) 10% probability to \"humanity [gets] a small future on a spare asteroid-turned-computer or an alien zoo or maybe even star\". I just think that scenario's way, way less likely than that.\n   * I'd be surprised if Nate assigns 10+% probability to scenarios like that, but he can speak for himself. ð¤·ââï¸\n   * I think some people at MIRI have significantly lower p(doom)? And I don't expect those people to use language like \"on the default trajectory, AI is going to kill everyone\".\n * I agree with you that there's something weird about making lots of human-extinction-focused arguments when the thing we care more about is \"does the cosmic endowment get turned into paperclips\"? I do care about both of those things, an enormous amount; and I plan to talk about both of those things to some degree in public communications, rather than treating it as some kind of poorly-kept secret that MIRI folks care about whether flourishing interstellar civilizations get a chance to exist down the line. But I have this whole topic mentally flagged as a thing to be thoughtful and careful about, because it at least seems like an area that contains risk factors for future deceptive comms. E.g., if we update later to expecting the cosmic endowment to be wasted but all humans not dying, I would want us to adjust our messaging even if that means sacrificing some punchiness in our policy outreach.\n   * Currently, however, I think the particular scenario \"AI keeps a few flourishing humans around forever\" is incredibly unlikely, and I don't think Eliezer, Nate, etc. would say things like \"this has a double-digit probability of happening in real life\"? And, to be honest, the idea of myself and my family and friends and every other human being all dying in the near future really fucks me up and does not seem in any sense OK, even "
    },
    "baseScore": 16,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-06-02T03:12:26.509Z",
    "af": false
  },
  {
    "_id": "qC9cqn2KxGoHbX5zH",
    "postId": "tKk37BFkMzchtZThx",
    "parentCommentId": "gmqe6J9oaKfnqCJFN",
    "topLevelCommentId": "xBYimQtgASti5tgWv",
    "contents": {
      "markdown": "Note that \"everyone will be killed (or worse)\" is a different claim from \"everyone will be killed\"! (And see Oliver's point that Ryan isn't talking about *mistreated* brain scans.)",
      "plaintextDescription": "Note that \"everyone will be killed (or worse)\" is a different claim from \"everyone will be killed\"! (And see Oliver's point that Ryan isn't talking about mistreated brain scans.)"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-06-02T02:34:18.004Z",
    "af": false
  },
  {
    "_id": "AehJprRNpPoEYyLHM",
    "postId": "tKk37BFkMzchtZThx",
    "parentCommentId": "vzrKr9Kdfj8HtqfuJ",
    "topLevelCommentId": "xBYimQtgASti5tgWv",
    "contents": {
      "markdown": "> Some of the other things you suggest, like future systems keeping humans physically alive, do not seem plausible to me.\n\nI agree with Gretta here, and I think this is a crux. If MIRI folks thought it were likely that AI will leave a few humans biologically alive (as opposed to information-theoretically revivable), I don't think we'd be comfortable saying \"AI is going to kill everyone\". (I encourage other MIRI folks to chime in if they disagree with me about the counterfactual.)\n\nI also personally have maybe half my probability mass on \"the AI just doesn't store any human brain-states long-term\", and I have less than 1% probability on \"conditional on the AI storing human brain-states for future trade, the AI does in fact encounter aliens that want to trade *and* this trade results in a flourishing human civilization\".",
      "plaintextDescription": "> Some of the other things you suggest, like future systems keeping humans physically alive, do not seem plausible to me.\n\nI agree with Gretta here, and I think this is a crux. If MIRI folks thought it were likely that AI will leave a few humans biologically alive (as opposed to information-theoretically revivable), I don't think we'd be comfortable saying \"AI is going to kill everyone\". (I encourage other MIRI folks to chime in if they disagree with me about the counterfactual.)\n\nI also personally have maybe half my probability mass on \"the AI just doesn't store any human brain-states long-term\", and I have less than 1% probability on \"conditional on the AI storing human brain-states for future trade, the AI does in fact encounter aliens that want to trade and this trade results in a flourishing human civilization\"."
    },
    "baseScore": 12,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-06-02T02:32:03.597Z",
    "af": false
  },
  {
    "_id": "eyQXRoPGSLAcbcjyk",
    "postId": "JSWF2ZLt6YahyAauE",
    "parentCommentId": "QjAfGPrrKCQwuky5h",
    "topLevelCommentId": "jyLCuRd8e2SRSkzFX",
    "contents": {
      "markdown": "FWIW I do think \"don't trust this guy\" is warranted; I don't know that he's malicious, but I think he's just exceptionally incompetent relative to the average tech reporter you're likely to see stories from.\n\nLike, in 2018 Metz wrote a *full-length article on smarter-than-human AI* that included the following frankly incredible sentence:\n\n> During a recent Tesla earnings call, Mr. Musk, who has struggled with questions about his companyâs financial losses and concerns about the quality of its vehicles, chastised the news media for not focusing on the deaths that autonomous technology could prevent â a remarkable stance from someone who has repeatedly warned the world that A.I. is a danger to humanity.",
      "plaintextDescription": "FWIW I do think \"don't trust this guy\" is warranted; I don't know that he's malicious, but I think he's just exceptionally incompetent relative to the average tech reporter you're likely to see stories from.\n\nLike, in 2018 Metz wrote a full-length article on smarter-than-human AI that included the following frankly incredible sentence:\n\n> During a recent Tesla earnings call, Mr. Musk, who has struggled with questions about his companyâs financial losses and concerns about the quality of its vehicles, chastised the news media for not focusing on the deaths that autonomous technology could prevent â a remarkable stance from someone who has repeatedly warned the world that A.I. is a danger to humanity."
    },
    "baseScore": 18,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2024-05-16T03:45:09.436Z",
    "af": false
  },
  {
    "_id": "9rxTRiQGPShKnrbmf",
    "postId": "JSWF2ZLt6YahyAauE",
    "parentCommentId": "hCFRsmgn3GCftBKhj",
    "topLevelCommentId": "jyLCuRd8e2SRSkzFX",
    "contents": {
      "markdown": "FWIW, Cade Metz was reaching out to MIRI and some other folks in the x-risk space back in January 2020, and I went to read some of his articles and came to the conclusion in January that he's one of the least competent journalists -- like, most likely to misunderstand his beat and emit obvious howlers -- that I'd ever encountered. I told folks as much at the time, and advised against talking to him just on the basis that a lot of his journalism is comically bad and you'll risk looking foolish if you tap him.\n\nThis was six months before Metz caused SSC to shut down and more than a year before his hit piece on Scott came out, so it wasn't in any way based on 'Metz has been mean to my friends' or anything like that. (At the time he wasn't even asking around about SSC or Scott, AFAIK.)\n\n(I don't think this is an idiosyncratic opinion of mine, either; I've seen other non-rationalists I take seriously flag Metz as someone unusually out of his depth and error-prone for a NYT reporter, for reporting unrelated to SSC stuff.)",
      "plaintextDescription": "FWIW, Cade Metz was reaching out to MIRI and some other folks in the x-risk space back in January 2020, and I went to read some of his articles and came to the conclusion in January that he's one of the least competent journalists -- like, most likely to misunderstand his beat and emit obvious howlers -- that I'd ever encountered. I told folks as much at the time, and advised against talking to him just on the basis that a lot of his journalism is comically bad and you'll risk looking foolish if you tap him.\n\nThis was six months before Metz caused SSC to shut down and more than a year before his hit piece on Scott came out, so it wasn't in any way based on 'Metz has been mean to my friends' or anything like that. (At the time he wasn't even asking around about SSC or Scott, AFAIK.)\n\n(I don't think this is an idiosyncratic opinion of mine, either; I've seen other non-rationalists I take seriously flag Metz as someone unusually out of his depth and error-prone for a NYT reporter, for reporting unrelated to SSC stuff.)"
    },
    "baseScore": 55,
    "voteCount": 34,
    "createdAt": null,
    "postedAt": "2024-05-16T03:41:00.506Z",
    "af": false
  },
  {
    "_id": "mwkDkxRLfK8u22W58",
    "postId": "63X9s3ENXeaDrbe5t",
    "parentCommentId": "u2qnBs8GtHxWKdjXg",
    "topLevelCommentId": "5dSwgE9jTrknL4KbP",
    "contents": {
      "markdown": "Sounds like a lot of political alliances! (And \"these two political actors are aligned\" is arguably an even weaker condition than \"these two political actors are allies\".)\n\nAt the end of the day, of course, all of these analogies are going to be flawed. AI is genuinely a different beast.",
      "plaintextDescription": "Sounds like a lot of political alliances! (And \"these two political actors are aligned\" is arguably an even weaker condition than \"these two political actors are allies\".)\n\nAt the end of the day, of course, all of these analogies are going to be flawed. AI is genuinely a different beast."
    },
    "baseScore": 8,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2024-04-19T00:00:37.122Z",
    "af": false
  },
  {
    "_id": "PfPCPXNsKoLDt4ZgY",
    "postId": "63X9s3ENXeaDrbe5t",
    "parentCommentId": "4KbJmbkj5nqNxMpz5",
    "topLevelCommentId": "5dSwgE9jTrknL4KbP",
    "contents": {
      "markdown": "> It's pretty sad to call all of these end states you describe alignment as alignment is an extremely natural word for \"actually terminally has good intentions\".\n\nAren't there a lot of clearer words for this? \"Well-intentioned\", \"nice\", \"benevolent\", etc.\n\n(And a lot of terms, like \"value loading\" and \"value learning\", that are pointing at the research project of getting good intentions into the AI.)\n\nTo my ear, \"aligned person\" sounds less like \"this person wishes the best for me\", and more like \"this person will behave in the right ways\".\n\nIf I hear that Russia and China are \"aligned\", I do assume that their intentions play a big role in that, but I also assume that their circumstances, capabilities, etc. matter too. Alignment in geopolitics can be temporary or situational, and it almost never means that Russia cares about China as much as China cares about itself, or vice versa.\n\nAnd if we step back from the human realm, an engineered system can be \"aligned\" in contexts that have nothing to do with goal-oriented behavior, but are just about ensuring components are in the right place relative to each other.\n\nCf. the [history of the term \"AI alignment\"](https://www.lesswrong.com/posts/sy4whuaczvLsn9PNc/ai-alignment-is-a-dangerously-overloaded-term?commentId=xkn5sSGwHbY4SyvRv). From my perspective, a big part of why MIRI coordinated with Stuart Russell to introduce the term \"AI alignment\" was that we wanted to switch away from \"Friendly AI\" to a term that sounded more neutral. \"Friendly AI research\" had always been intended to subsume the full technical problem of making powerful AI systems safe and aimable; but emphasizing \"Friendliness\" made it sound like the problem was purely about value loading, so a more generic and low-content word seemed desirable.\n\nBut Stuart Russell (and later Paul Christiano) had a different vision in mind for what they wanted \"alignment\" to be, and MIRI apparently failed to communicate and coordinate with Russell to avoid a namespace collision. So we ended up with a messy patchwork of different definitions.\n\nI've basically given up on trying to achieve uniformity on what \"AI alignment\" is; the best we can do, I think, is clarify whether we're talking about \"intent alignment\" vs. \"outcome alignment\" when the distinction matters.\n\nBut I do want to push back against those who think outcome alignment is just an unhelpful concept â on the contrary, if we didn't have a word for this idea I think it would be very important to invent one.Â \n\nIMO it matters more that we keep our eye on the ball (i.e., think about the actual outcomes we want and keep researchers' focus on how to achieve those outcomes) than that we define an extremely crisp, easily-packaged technical concept (that is at best a loose proxy for what we actually want). Especially now that ASI seems nearer at hand (so the need for this \"keep our eye on the ball\" skill is becoming less and less theoretical), and especially now that ASI disaster concerns have hit the mainstream (so the need to \"sell\" AI risk has diminished somewhat, and the need to direct research talent at the most important problems has increased).\n\nAnd I also want to push back against the idea that *a priori*, before we got stuck with the current terminology mess, it should have been obvious that \"alignment\" is about AI systems' goals and/or intentions, rather than about their behavior or overall designs. I think intent alignment took off because Stuart Russell and Paul Christiano advocated for that usage and encouraged others to use the term that way, not because this was the only option available to us.",
      "plaintextDescription": "> It's pretty sad to call all of these end states you describe alignment as alignment is an extremely natural word for \"actually terminally has good intentions\".\n\nAren't there a lot of clearer words for this? \"Well-intentioned\", \"nice\", \"benevolent\", etc.\n\n(And a lot of terms, like \"value loading\" and \"value learning\", that are pointing at the research project of getting good intentions into the AI.)\n\nTo my ear, \"aligned person\" sounds less like \"this person wishes the best for me\", and more like \"this person will behave in the right ways\".\n\nIf I hear that Russia and China are \"aligned\", I do assume that their intentions play a big role in that, but I also assume that their circumstances, capabilities, etc. matter too. Alignment in geopolitics can be temporary or situational, and it almost never means that Russia cares about China as much as China cares about itself, or vice versa.\n\nAnd if we step back from the human realm, an engineered system can be \"aligned\" in contexts that have nothing to do with goal-oriented behavior, but are just about ensuring components are in the right place relative to each other.\n\nCf. the history of the term \"AI alignment\". From my perspective, a big part of why MIRI coordinated with Stuart Russell to introduce the term \"AI alignment\" was that we wanted to switch away from \"Friendly AI\" to a term that sounded more neutral. \"Friendly AI research\" had always been intended to subsume the full technical problem of making powerful AI systems safe and aimable; but emphasizing \"Friendliness\" made it sound like the problem was purely about value loading, so a more generic and low-content word seemed desirable.\n\nBut Stuart Russell (and later Paul Christiano) had a different vision in mind for what they wanted \"alignment\" to be, and MIRI apparently failed to communicate and coordinate with Russell to avoid a namespace collision. So we ended up with a messy patchwork of different definitions.\n\nI've basically given up on trying to achieve uniformit"
    },
    "baseScore": 20,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2024-04-18T22:53:26.766Z",
    "af": false
  },
  {
    "_id": "7yxbgsL7d33zEGcyb",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "DRZ4H4GLCLftPpoN3",
    "topLevelCommentId": "DRZ4H4GLCLftPpoN3",
    "contents": {
      "markdown": "> \"Should\" in order to achieve a certain end? To meet some criterion? To boost a term in your utility function?\n\nIn the OP: \"Should\" in order to have more accurate beliefs/expectations. E.g., I should anticipate (with high probability) that the Sun will rise tomorrow in my part of the world, rather than it remaining night.",
      "plaintextDescription": "> \"Should\" in order to achieve a certain end? To meet some criterion? To boost a term in your utility function?\n\nIn the OP: \"Should\" in order to have more accurate beliefs/expectations. E.g., I should anticipate (with high probability) that the Sun will rise tomorrow in my part of the world, rather than it remaining night."
    },
    "baseScore": 3,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-04-18T21:51:23.962Z",
    "af": false
  },
  {
    "_id": "Ee8zfuMrYxSDvcacN",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "ZwSscp3MSdb5KLrqx",
    "topLevelCommentId": "opJbYudzcBDDAHGNp",
    "contents": {
      "markdown": "Why would the laws of physics conspire to vindicate a random human intuition that arose for unrelated reasons?\n\nWe do agree that the intuition arose for unrelated reasons, right? There's nothing in our evolutionary history, and no empirical observation, that causally connects the mechanism you're positing and the widespread human hunch \"you can't copy me\".\n\nIf the intuition is right, we agree that it's only right by coincidence. So *why* are we desperately searching for ways to try to make the intuition right?\n\n> It also doesn't force us to believe that a bunch of water pipes or gears functioning as a classical computer can ever have our own first person experience.\n\nWhy is this an *advantage* of a theory? Are you under the misapprehension that \"hypothesis H allows humans to hold on to assumption A\" is a Bayesian update in favor of H *even when we already know that humans had no reason to believe A*? This is another case where your theory seems to *require* that we only be *coincidentally* correct about A (\"sufficiently complex arrangements of water pipes can't ever be conscious\"), if we're correct about A at all.\n\nOne way to rescue this argument is by adding in an anthropic claim, like: \"If water pipes *could* be conscious, then nearly all conscious minds would be instantiated in random dust clouds and the like, not in biological brains. So given that we're not Boltzmann brains briefly coalescing from space dust, we should update that giant clouds of space dust *can't* be conscious.\"\n\nBut is this argument actually correct? There's an awful lot of complex machinery in a human brain. (And the same anthropic argument seems to suggest that some of the human-specific machinery is essential, else we'd expect to be some far-more-numerous observer, like an insect.) Is it *actually* that common for a random brew of space dust to coalesce into exactly the right shape, even briefly?",
      "plaintextDescription": "Why would the laws of physics conspire to vindicate a random human intuition that arose for unrelated reasons?\n\nWe do agree that the intuition arose for unrelated reasons, right? There's nothing in our evolutionary history, and no empirical observation, that causally connects the mechanism you're positing and the widespread human hunch \"you can't copy me\".\n\nIf the intuition is right, we agree that it's only right by coincidence. So why are we desperately searching for ways to try to make the intuition right?\n\n> It also doesn't force us to believe that a bunch of water pipes or gears functioning as a classical computer can ever have our own first person experience.\n\nWhy is this an advantage of a theory? Are you under the misapprehension that \"hypothesis H allows humans to hold on to assumption A\" is a Bayesian update in favor of H even when we already know that humans had no reason to believe A? This is another case where your theory seems to require that we only be coincidentally correct about A (\"sufficiently complex arrangements of water pipes can't ever be conscious\"), if we're correct about A at all.\n\nOne way to rescue this argument is by adding in an anthropic claim, like: \"If water pipes could be conscious, then nearly all conscious minds would be instantiated in random dust clouds and the like, not in biological brains. So given that we're not Boltzmann brains briefly coalescing from space dust, we should update that giant clouds of space dust can't be conscious.\"\n\nBut is this argument actually correct? There's an awful lot of complex machinery in a human brain. (And the same anthropic argument seems to suggest that some of the human-specific machinery is essential, else we'd expect to be some far-more-numerous observer, like an insect.) Is it actually that common for a random brew of space dust to coalesce into exactly the right shape, even briefly?"
    },
    "baseScore": 4,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2024-04-18T21:46:58.831Z",
    "af": false
  },
  {
    "_id": "fcHCaCGWGBRitZWEZ",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "J3wso45WeEiokhebR",
    "topLevelCommentId": "J3wso45WeEiokhebR",
    "contents": {
      "markdown": "Yeah, at some point we'll need a proper theory of consciousness regardless, since many humans will want to radically self-improve and it's important to know which cognitive enhancements preserve consciousness.",
      "plaintextDescription": "Yeah, at some point we'll need a proper theory of consciousness regardless, since many humans will want to radically self-improve and it's important to know which cognitive enhancements preserve consciousness."
    },
    "baseScore": 25,
    "voteCount": 20,
    "createdAt": null,
    "postedAt": "2024-04-18T16:10:59.017Z",
    "af": false
  },
  {
    "_id": "jMLGL3sWpNDL5x7KK",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "6EAjA8KCpXwXaraD5",
    "topLevelCommentId": "6EAjA8KCpXwXaraD5",
    "contents": {
      "markdown": "> You can easily clear this confusion if you rephrase it as \"You should anticipate having *any* of these experiences\". Then it's immediately clear that we are talking about two separate screens.\n\nThis introduces some other ambiguities. E.g., \"you should anticipate having any of these experiences\" may make it sound like you have a *choice* as to which experience to rationally expect.\n\n> And it's also clear that our curriocity isn't actually satisfied. That the question \"which one of these two will actually be the case\" is still very much on the table.\n\n... And the answer is \"both of these will actually be the case (but not in a split-screen sort of way)\".\n\nYour rephrase hasn't shown that there was a question left unanswered in the original post; it's just shown that there isn't a super short way to crisply express what happens in English, you do actually have to add the clarification.\n\n> Still as soon as we got Rob-y and Rob-z they are not \"metaphysically the same person\". When Rob-y says \"I\" he is reffering to Rob-y, not Rob-z and vice versa. More specifically Rob-y is refering to some causal curve through time ans Rob-z is refering to another causal curve through time. These two curves are the same to some point, but then they are not.Â \n\nYep, I think this is a perfectly fine way to think about the thing.",
      "plaintextDescription": "> You can easily clear this confusion if you rephrase it as \"You should anticipate having any of these experiences\". Then it's immediately clear that we are talking about two separate screens.\n\nThis introduces some other ambiguities. E.g., \"you should anticipate having any of these experiences\" may make it sound like you have a choice as to which experience to rationally expect.\n\n> And it's also clear that our curriocity isn't actually satisfied. That the question \"which one of these two will actually be the case\" is still very much on the table.\n\n... And the answer is \"both of these will actually be the case (but not in a split-screen sort of way)\".\n\nYour rephrase hasn't shown that there was a question left unanswered in the original post; it's just shown that there isn't a super short way to crisply express what happens in English, you do actually have to add the clarification.\n\n> Still as soon as we got Rob-y and Rob-z they are not \"metaphysically the same person\". When Rob-y says \"I\" he is reffering to Rob-y, not Rob-z and vice versa. More specifically Rob-y is refering to some causal curve through time ans Rob-z is refering to another causal curve through time. These two curves are the same to some point, but then they are not.Â \n\nYep, I think this is a perfectly fine way to think about the thing."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-04-18T16:05:11.119Z",
    "af": false
  },
  {
    "_id": "cmN9N2vKaseyLmLoj",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "opJbYudzcBDDAHGNp",
    "topLevelCommentId": "opJbYudzcBDDAHGNp",
    "contents": {
      "markdown": "> My first issue with your post is that this initial ontological assumption is neither mentioned explicitly nor motivated. Nothing in your post can be used as proof of this initial assumption.\n\nThere are always going to be many different ways someone could object to a view. If you were a Christian, you'd perhaps be objecting that the existence of incorporeal God-given Souls is the real crux of the matter, and if I were intellectually honest I'd be devoting the first half of the post to arguing against the Christian Soul.\n\nRather than trying to anticipate these objections, I'd rather just hear them stated out loud by their proponents and then hash them out in the comments. This also makes the post less boring for the sorts of people who are most likely to be on LW: physicalists and their ilk.\n\n> Now, what would be the experience of getting copied, seen from a first-person, \"internal\", perspective? I am pretty sure it would be something like: you walk into the room, you sit there, you Â hear say the scanner working for some time, it stops, you walk out. From my agnostic perspective, if I were the one to be scanned it seems like nothing special would have happened to me in this procedure. I didnt feel anything weird, I didnt feel my \"consciousness split into two\" or something.\n\nWhy do you assume that you wouldn't experience the copy's version of events?\n\nThe un-copied version of you experiences walking into the room, sitting there, hearing the scanner working, and hearing it stop; then that version of you experiences walking out. It seems like nothing special happened in this procedure; this version of you doesn't feel anything weird, and doesn't feel like their \"consciousness split into two\" or anything.\n\nThe copied version of you experiences walking into the room, sitting here, hearing the scanner working, and then an instantaneous experience of (let's say) feeling like you've been teleported into another room -- you're now inside the simulation. Assuming the simulation feels like a normal room, it could well seem like nothing special happened in this procedure -- it may feel like blinking and seeing the room suddenly change during the blink, while you yourself remain unchanged. This version of you doesn't necessarily feel anything weird either, and they don't feel like their \"consciousness split into two\" or anything.\n\nIt's a bit weird that there are two futures, here, but only one past -- that the first part of the story is the same for both versions of you. But so it goes; that just comes with the territory of copying people.\n\nIf you disagree with anything I've said above, what do you disagree with? And, again, what do you mean by saying you're \"pretty sure\" that you would experience the future of the non-copied version?\n\n> Namely, if I consider this procedure as an empirical experiment, from my first person perspective I dont get any new / unexpected observation compared to say just sitting in an ordinary room. Even if I were to go and find my copy, my experience would again be like meeting a different person which just happens to look like me and which claims to have similar memories Â up to the point when I entered the copying room. There would be no way to verify or to view things from their first person perspective.\n\nSure. But is any of this Bayesian evidence against the view I've outlined above? What *would* it feel like, if the copy were another version of yourself? Would you expect that you *could* telepathically communicate with your copy and see things from both perspectives at once, if your copies were equally \"you\"? If so, why?\n\n> On the contrary, I would be wary to, say, kill myself or to be destroyed after the copying procedure, since no change will have occured to my first person perspective, and it would thus seem less likely that my \"experience\" would somehow survive because of my copy.\n\nShall we make a million copies and then take a vote? :)\n\nI agree that \"I made a non-destructive software copy of myself and then experienced the future of my physical self rather than the future of my digital copy\" is nonzero Bayesian evidence that physical brains have a Cartesian Soul that is responsible for the brain's phenomenal consciousness; the Cartesian Soul hypothesis does predict that data. But the prior probability of Cartesian Souls is low enough that I don't think it should matter.\n\nYou need some prior reason to believe in this Soul in the first place; the same as if you flipped a coin, it came up heads, and you said \"aha, this is perfectly predicted by the existence of an invisible leprechaun who wanted that coin to come up heads!\". Losing a coinflip isn't a surprising enough outcome to overcome the prior against invisible leprechauns.\n\n> and it would also force me to accept that even a copy where the \"circuit\" is made of water pipes and pumps, or gears and levers also have an actual, first person experience as \"me\", as long as the appropriate computations are being carried out. Â \n\nWhy wouldn't it? What do you have against water pipes?",
      "plaintextDescription": "> My first issue with your post is that this initial ontological assumption is neither mentioned explicitly nor motivated. Nothing in your post can be used as proof of this initial assumption.\n\nThere are always going to be many different ways someone could object to a view. If you were a Christian, you'd perhaps be objecting that the existence of incorporeal God-given Souls is the real crux of the matter, and if I were intellectually honest I'd be devoting the first half of the post to arguing against the Christian Soul.\n\nRather than trying to anticipate these objections, I'd rather just hear them stated out loud by their proponents and then hash them out in the comments. This also makes the post less boring for the sorts of people who are most likely to be on LW: physicalists and their ilk.\n\n> Now, what would be the experience of getting copied, seen from a first-person, \"internal\", perspective? I am pretty sure it would be something like: you walk into the room, you sit there, you Â hear say the scanner working for some time, it stops, you walk out. From my agnostic perspective, if I were the one to be scanned it seems like nothing special would have happened to me in this procedure. I didnt feel anything weird, I didnt feel my \"consciousness split into two\" or something.\n\nWhy do you assume that you wouldn't experience the copy's version of events?\n\nThe un-copied version of you experiences walking into the room, sitting there, hearing the scanner working, and hearing it stop; then that version of you experiences walking out. It seems like nothing special happened in this procedure; this version of you doesn't feel anything weird, and doesn't feel like their \"consciousness split into two\" or anything.\n\nThe copied version of you experiences walking into the room, sitting here, hearing the scanner working, and then an instantaneous experience of (let's say) feeling like you've been teleported into another room -- you're now inside the simulation. Assuming the simulati"
    },
    "baseScore": 3,
    "voteCount": 21,
    "createdAt": null,
    "postedAt": "2024-04-18T02:42:54.501Z",
    "af": false
  },
  {
    "_id": "2iCWWEQJFhGEkE9Fy",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "GF6nr6p9izAhWzJqh",
    "topLevelCommentId": "GF6nr6p9izAhWzJqh",
    "contents": {
      "markdown": "> Wouldn't it follow that in the same way you anticipate the future experiences of the brain that you \"find yourself in\" (i.e. the person reading this) you should anticipate *all* experiences, i.e. that *all* brain states occur with the same kind of me-ness/vivid immediacy?\n\nWhat's the empirical or physical content of this belief?\n\nI worry that this may be another case of the Cartesian Ghost rearing its ugly head. We notice that there's no physical thingie that makes the Ghost more connected to one experience or the other; so rather than exorcising the Ghost entirely, we imagine that the Ghost is connected to *every* experience simultaneously.\n\nBut in fact there is no Ghost. There's just a bunch of experience-moments implemented in brain-moments.\n\nSome of those brain-moments resemble other brain-moments, either by coincidence or because of some (direct or indirect) causal link between the brain-moments. When we talk about Brain-1 \"anticipating\" or \"becoming\" a future brain-state Brain-2, we normally mean things like:\n\n*   There's a lawful physical connection between Brain-1 and Brain-2, such that the choices and experiences of Brain-1 influence the state of Brain-2 in a bunch of specific ways.\n*   Brain-2 retains ~all of the memories, personality traits, goals, etc. of Brain-1.\n*   If Brain-2 is a *direct* successor to Brain-1, then typically Brain-2 can remember a bunch of things about the experience Brain-1 was undergoing.\n\nThese are all fuzzy, high-level properties, which admit of edge cases. But I'm not seeing what's gained by therefore concluding \"I should anticipate *every* experience, even ones that have no causal connection to mine *and* no shared memories *and* no shared personality traits\". Tables are a fuzzy and high-level concept, but that doesn't mean that every object in existence is a table. It doesn't even mean that every object is slightly table-ish. A photon isn't \"slightly table-ish\", it's just plain not a table.\n\n> Which just means, all brain states exist in the same *vivid, for-me* way, since there is nothing further to distinguish between them that makes them *this* vivid, i.e. they all exist *HERE-NOW*.\n\nBut they don't have the anticipation-related properties I listed above; so what hypotheses are we distinguishing by updating from \"these experiences aren't mine\" to \"these experiences are mine\"?\n\nMaybe the update that's happening is something like: \"Previously it felt to me like other people's experiences weren't *fully real*. I was unduly selfish and self-centered, because my experiences seemed to me like they were the center of the universe; I abstractly and theoretically knew that other people have their own point of view, but that fact didn't really hit home for me. Then something happened, and I had a sudden realization that no, *it's all real*.\"\n\nIf so, then that seems totally fine to me. But I worry that the view in question might instead be something tacitly Cartesian, insofar as it's trying to say \"all experiences are *for me*\" \\-\\- something that doesn't make a lot of sense to say if there are two brain states on opposite sides of the universe with nothing in common and nothing connecting them, but that does make sense if there's a Ghost the experiences are all \"for\".",
      "plaintextDescription": "> Wouldn't it follow that in the same way you anticipate the future experiences of the brain that you \"find yourself in\" (i.e. the person reading this) you should anticipate all experiences, i.e. that all brain states occur with the same kind of me-ness/vivid immediacy?\n\nWhat's the empirical or physical content of this belief?\n\nI worry that this may be another case of the Cartesian Ghost rearing its ugly head. We notice that there's no physical thingie that makes the Ghost more connected to one experience or the other; so rather than exorcising the Ghost entirely, we imagine that the Ghost is connected to every experience simultaneously.\n\nBut in fact there is no Ghost. There's just a bunch of experience-moments implemented in brain-moments.\n\nSome of those brain-moments resemble other brain-moments, either by coincidence or because of some (direct or indirect) causal link between the brain-moments. When we talk about Brain-1 \"anticipating\" or \"becoming\" a future brain-state Brain-2, we normally mean things like:\n\n * There's a lawful physical connection between Brain-1 and Brain-2, such that the choices and experiences of Brain-1 influence the state of Brain-2 in a bunch of specific ways.\n * Brain-2 retains ~all of the memories, personality traits, goals, etc. of Brain-1.\n * If Brain-2 is a direct successor to Brain-1, then typically Brain-2 can remember a bunch of things about the experience Brain-1 was undergoing.\n\nThese are all fuzzy, high-level properties, which admit of edge cases. But I'm not seeing what's gained by therefore concluding \"I should anticipate every experience, even ones that have no causal connection to mine and no shared memories and no shared personality traits\". Tables are a fuzzy and high-level concept, but that doesn't mean that every object in existence is a table. It doesn't even mean that every object is slightly table-ish. A photon isn't \"slightly table-ish\", it's just plain not a table.\n\n> Which just means, all brain states exist in the "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-04-18T02:04:44.332Z",
    "af": false
  },
  {
    "_id": "koiiEYfuGjBfCnSvx",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "GL4qsH8f4dZvHmvbg",
    "topLevelCommentId": "4BLAew8qyQ8tQDH7k",
    "contents": {
      "markdown": "As a test, I asked a non-philosopher friend of mine what their view is. Here's a transcript of our short conversation: [https://docs.google.com/document/d/1s1HOhrWrcYQ5S187vmpfzZcBfolYFIbeTYgqeebNIA0/edit](https://docs.google.com/document/d/1s1HOhrWrcYQ5S187vmpfzZcBfolYFIbeTYgqeebNIA0/edit)Â \n\nI was a bit annoyingly repetitive with trying to confirm and re-confirm what their view is, but I think it's clear from the exchange that my interpretation is correct at least for *this* person.",
      "plaintextDescription": "As a test, I asked a non-philosopher friend of mine what their view is. Here's a transcript of our short conversation: https://docs.google.com/document/d/1s1HOhrWrcYQ5S187vmpfzZcBfolYFIbeTYgqeebNIA0/editÂ \n\nI was a bit annoyingly repetitive with trying to confirm and re-confirm what their view is, but I think it's clear from the exchange that my interpretation is correct at least for this person."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-04-18T01:07:27.876Z",
    "af": false
  },
  {
    "_id": "GL4qsH8f4dZvHmvbg",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "cucS2LhEPrFGyoM6J",
    "topLevelCommentId": "4BLAew8qyQ8tQDH7k",
    "contents": {
      "markdown": "> Is there even anybody claiming there is an experiential difference?\n\nYep! Ask someone with this view whether the current stream of consciousness continues from their pre-uploaded self to their post-uploaded self, like it continues when they pass through a doorway. The typical claim is some version of \"this stream of consciousness will end, what comes next is only oblivion\", not \"oh sure, the stream of consciousness is going to continue in the same way it always does, but I prefer not to use the English word 'me' to refer to the later parts of that stream of consciousness\".\n\nThis is why the disagreement here has *policy implications*: people with different views of personal identity have different beliefs about the desirability of mind uploading. They aren't just disagreeing about how to use words, and if they were, you'd be forced into the equally \"uncharitable\" perspective that someone here is very confused about how relevant word choice is to the desirability of uploading.\n\n> The alternative to this is that there is a disagreement about the appropriate semantic interpretation/analysis of the question. E.g. about what we mean when we say \"I will (not) experience such and such\". That seems more charitable than hypothesizing beliefs in \"ghosts\" or \"magic\".\n\nI didn't say that the relevant people endorse a belief in ghosts or magic. (Some may do so, but many explicitly don't!)\n\nIt's a bit darkly funny that you've reached for a clearly false and super-uncharitable interpretation of what I said, in the same sentence you're chastising me for being uncharitable! But also, [\"charity\" is a bad approach to trying to understand other people](https://www.lesswrong.com/posts/MdZyLnLHuaHrCskjy/itt-passing-and-civility-are-good-charity-is-bad), and bad epistemology can get in the way of a lot of stuff.",
      "plaintextDescription": "> Is there even anybody claiming there is an experiential difference?\n\nYep! Ask someone with this view whether the current stream of consciousness continues from their pre-uploaded self to their post-uploaded self, like it continues when they pass through a doorway. The typical claim is some version of \"this stream of consciousness will end, what comes next is only oblivion\", not \"oh sure, the stream of consciousness is going to continue in the same way it always does, but I prefer not to use the English word 'me' to refer to the later parts of that stream of consciousness\".\n\nThis is why the disagreement here has policy implications: people with different views of personal identity have different beliefs about the desirability of mind uploading. They aren't just disagreeing about how to use words, and if they were, you'd be forced into the equally \"uncharitable\" perspective that someone here is very confused about how relevant word choice is to the desirability of uploading.\n\n> The alternative to this is that there is a disagreement about the appropriate semantic interpretation/analysis of the question. E.g. about what we mean when we say \"I will (not) experience such and such\". That seems more charitable than hypothesizing beliefs in \"ghosts\" or \"magic\".\n\nI didn't say that the relevant people endorse a belief in ghosts or magic. (Some may do so, but many explicitly don't!)\n\nIt's a bit darkly funny that you've reached for a clearly false and super-uncharitable interpretation of what I said, in the same sentence you're chastising me for being uncharitable! But also, \"charity\" is a bad approach to trying to understand other people, and bad epistemology can get in the way of a lot of stuff."
    },
    "baseScore": 8,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-04-18T00:44:26.907Z",
    "af": false
  },
  {
    "_id": "etaN8si88FLvkKPrJ",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "ky4EnpLHRgycmvHee",
    "topLevelCommentId": "4BLAew8qyQ8tQDH7k",
    "contents": {
      "markdown": "> The problem was that you first seemed to belittle questions about word meanings (\"self\") as being \"just\" about \"definitions\" that are \"purely verbal\".\n\nI did no such thing!\n\n> Luckily now you concede that the question about the meaning of \"I\" isn't just about (arbitrary) \"definitions\"\n\nRead the blog post at the top of this page! It's my attempt to answer the question of when a mind is \"me\", and you'll notice it's not talking about definitions.\n\n> But we already know all the empirical facts: Someone goes into the teleporter, a bit later someone comes out at the other end and sees something. So the issue can only be about the semantic interpretation of that question, about what we mean with expressions like \"I will see x\".\n\nNope!\n\nThere are two perspectives here:\n\n1.  \"I don't want to upload myself, because I wouldn't get to experience that uploads' experiences. When I die, this stream of consciousness will end, rather than continuing in another body. Physically dying and then being being copied elsewhere is *not* phenomenologically indistinguishable from stepping through a doorway.\"\n2.  \"I do want to upload myself, because I would get to experience that uploads' experiences. Physically dying and then being copied myself *is* phenomenologically indistinguishable from stepping through a doorway.\"\n\nThe disagreement between these two perspectives isn't about word definitions at all; a fear that \"when my body dies, there will be nothing but oblivion\" is a very real fear about anticipated experiences (and anticipated absences of experience), not a verbal quibble about how we ought to define a specific word.\n\nBut it's *also* a bit confusing to call the disagreement between these two perspectives \"empirical\", **because \"empirical\" here is conflating \"third-person empirical\" with \"first-person empirical\"**.\n\nThe disagreement here is about *whether a stream of consciousness can \"continue\" across temporal and spatial gaps, in the same way that it continues when there are no obvious gaps*. It's about whether there's a *subjective, experiential difference* between stepping through a doorway and using a teleporter.\n\nThe thing I'm arguing in the OP is that there *can't* be an experiential difference here, because there's no physical difference that could be underlying the supposed experiential difference. So the disagreement about the first-person facts, I claim, stems from a cognitive error, which I characterize as \"making predictions as though you believed yourself to be a Cartesian Ghost (even if you don't on-reflection endorse the claim that Cartesian Ghosts exist)\". This is, again, a very different error from \"defining a word in a nonstandard way\".",
      "plaintextDescription": "> The problem was that you first seemed to belittle questions about word meanings (\"self\") as being \"just\" about \"definitions\" that are \"purely verbal\".\n\nI did no such thing!\n\n> Luckily now you concede that the question about the meaning of \"I\" isn't just about (arbitrary) \"definitions\"\n\nRead the blog post at the top of this page! It's my attempt to answer the question of when a mind is \"me\", and you'll notice it's not talking about definitions.\n\n> But we already know all the empirical facts: Someone goes into the teleporter, a bit later someone comes out at the other end and sees something. So the issue can only be about the semantic interpretation of that question, about what we mean with expressions like \"I will see x\".\n\nNope!\n\nThere are two perspectives here:\n\n 1. \"I don't want to upload myself, because I wouldn't get to experience that uploads' experiences. When I die, this stream of consciousness will end, rather than continuing in another body. Physically dying and then being being copied elsewhere is not phenomenologically indistinguishable from stepping through a doorway.\"\n 2. \"I do want to upload myself, because I would get to experience that uploads' experiences. Physically dying and then being copied myself is phenomenologically indistinguishable from stepping through a doorway.\"\n\nThe disagreement between these two perspectives isn't about word definitions at all; a fear that \"when my body dies, there will be nothing but oblivion\" is a very real fear about anticipated experiences (and anticipated absences of experience), not a verbal quibble about how we ought to define a specific word.\n\nBut it's also a bit confusing to call the disagreement between these two perspectives \"empirical\", because \"empirical\" here is conflating \"third-person empirical\" with \"first-person empirical\".\n\nThe disagreement here is about whether a stream of consciousness can \"continue\" across temporal and spatial gaps, in the same way that it continues when there are no obvious gaps"
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-04-17T20:28:54.036Z",
    "af": false
  },
  {
    "_id": "cjkKs7GqWctqBD9zv",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "4vSje382mQDkAxzv4",
    "topLevelCommentId": "4BLAew8qyQ8tQDH7k",
    "contents": {
      "markdown": "> You're also free to define \"I\" however you want in your values.\n\nSort of!\n\n*   It's true that no law of nature will stop you from using \"I\" in a nonstandard way; your head will not explode if you redefine \"table\" to mean \"penguin\".\n*   And it's true that there are possible minds in abstract mindspace that have all sorts of values, including strict preferences about whether they want their brain to be made of silicon vs. carbon.\n*   But it's *not* true that humans alive today have full and complete control over their own preferences.\n*   And it's *not* true that humans can never be mistaken in their beliefs about their own preferences.\n\nIn the case of teleportation, I think teleportation-phobic people are mostly making an implicit error of the form \"mistakenly modeling situations as though you are a Cartesian Ghost who is observing experiences from outside the universe\", not making a mistake about what their preferences are per se. (Though once you realize that you're not a Cartesian Ghost, that will have some implications for what experiences you expect to see next in some cases, and implications for what physical world-states you prefer relative to other world-states.)",
      "plaintextDescription": "> You're also free to define \"I\" however you want in your values.\n\nSort of!\n\n * It's true that no law of nature will stop you from using \"I\" in a nonstandard way; your head will not explode if you redefine \"table\" to mean \"penguin\".\n * And it's true that there are possible minds in abstract mindspace that have all sorts of values, including strict preferences about whether they want their brain to be made of silicon vs. carbon.\n * But it's not true that humans alive today have full and complete control over their own preferences.\n * And it's not true that humans can never be mistaken in their beliefs about their own preferences.\n\nIn the case of teleportation, I think teleportation-phobic people are mostly making an implicit error of the form \"mistakenly modeling situations as though you are a Cartesian Ghost who is observing experiences from outside the universe\", not making a mistake about what their preferences are per se. (Though once you realize that you're not a Cartesian Ghost, that will have some implications for what experiences you expect to see next in some cases, and implications for what physical world-states you prefer relative to other world-states.)"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-04-17T20:09:21.050Z",
    "af": false
  },
  {
    "_id": "aMfjhdLABJejq9MG7",
    "postId": "63X9s3ENXeaDrbe5t",
    "parentCommentId": "MiYXiaovxoP2XtMWv",
    "topLevelCommentId": "5dSwgE9jTrknL4KbP",
    "contents": {
      "markdown": "FWIW, I typically use \"alignment research\" to mean \"AI research aimed at making it possible to safely do ambitious things with sufficiently-capable AI\" (with an emphasis on \"safely\"). So I'd include things like Chris Olah's interpretability research, even if the proximate impact of this is just \"we understand what's going on better, so we may be more able to predict and finely control future systems\" and the proximate impact is not \"the AI is now less inclined to kill you\".\n\nSome examples: I wouldn't *necessarily* think of \"figure out how we want to airgap the AI\" as \"alignment research\", since it's less about designing the AI, shaping its mind, predicting and controlling it, etc., and more about designing the environment around the AI.  \n  \nBut I *would* think of things like \"figure out how to make this AI system too socially-dumb to come up with ideas like 'maybe I should deceive my operators', while keeping it superhumanly smart at nanotech research\" as central examples of \"alignment research\", even though it's about controlling capabilities ('make the AI dumb in this particular way') rather than about instilling a particular goal into the AI.\n\nAnd I'd also think of \"we know this AI is trying to kill us; let's figure out how to constrain its capabilities so that it keeps wanting that, but is too dumb to find a way to succeed in killing us, thereby forcing it to work with us rather than against us in order to achieve more of what it wants\" as a pretty central example of alignment research, albeit not the sort of alignment research I feel optimistic about. The way I think about the field, you don't have to specifically attack the \"is it trying to kill you?\" part of the system in order to be doing alignment research; there are other paths, and alignment researchers should consider all of them and focus on results rather than marrying a specific methodology.",
      "plaintextDescription": "FWIW, I typically use \"alignment research\" to mean \"AI research aimed at making it possible to safely do ambitious things with sufficiently-capable AI\" (with an emphasis on \"safely\"). So I'd include things like Chris Olah's interpretability research, even if the proximate impact of this is just \"we understand what's going on better, so we may be more able to predict and finely control future systems\" and the proximate impact is not \"the AI is now less inclined to kill you\".\n\nSome examples: I wouldn't necessarily think of \"figure out how we want to airgap the AI\" as \"alignment research\", since it's less about designing the AI, shaping its mind, predicting and controlling it, etc., and more about designing the environment around the AI.\n\nBut I would think of things like \"figure out how to make this AI system too socially-dumb to come up with ideas like 'maybe I should deceive my operators', while keeping it superhumanly smart at nanotech research\" as central examples of \"alignment research\", even though it's about controlling capabilities ('make the AI dumb in this particular way') rather than about instilling a particular goal into the AI.\n\nAnd I'd also think of \"we know this AI is trying to kill us; let's figure out how to constrain its capabilities so that it keeps wanting that, but is too dumb to find a way to succeed in killing us, thereby forcing it to work with us rather than against us in order to achieve more of what it wants\" as a pretty central example of alignment research, albeit not the sort of alignment research I feel optimistic about. The way I think about the field, you don't have to specifically attack the \"is it trying to kill you?\" part of the system in order to be doing alignment research; there are other paths, and alignment researchers should consider all of them and focus on results rather than marrying a specific methodology."
    },
    "baseScore": 19,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2024-04-17T18:54:41.932Z",
    "af": false
  },
  {
    "_id": "QXxHrThtPqKGai4Bf",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "7sdbeic47ghFfCBD9",
    "topLevelCommentId": "7sdbeic47ghFfCBD9",
    "contents": {
      "markdown": "> But that isn't *an* experience. It's two experiences. You will not have an experience of having two experiences. Two experiences will experience having been one person.\n\nSure; from my perspective, you're saying the same thing as me.\n\n> Are you going to care about 1000 different copies equally?\n\nHow am I supposed to choose between them?",
      "plaintextDescription": "> But that isn't an experience. It's two experiences. You will not have an experience of having two experiences. Two experiences will experience having been one person.\n\nSure; from my perspective, you're saying the same thing as me.\n\n> Are you going to care about 1000 different copies equally?\n\nHow am I supposed to choose between them?"
    },
    "baseScore": 3,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-04-17T16:48:43.345Z",
    "af": false
  },
  {
    "_id": "aeJhj6fygrburAzoz",
    "postId": "zPM5r3RjossttDrpw",
    "parentCommentId": "4BLAew8qyQ8tQDH7k",
    "topLevelCommentId": "4BLAew8qyQ8tQDH7k",
    "contents": {
      "markdown": "> Why? If \"I\" is arbitrary definition, then âWhen I step through this doorway, will I have another experience?\" depends on this arbitrary definition and so is also arbitrary.\n\nWhich things count as \"I\" isn't an arbitrary definition; it's just a fuzzy natural-language concept.\n\n(I guess you can call that \"arbitrary\" if you want, but then all the other words in the sentence, like \"doorway\" and \"step\", are also \"arbitrary\".)\n\nAnalogy: When you're writing in your personal diary, you're free to define \"table\" however you want. But in ordinary English-language discourse, if you call all penguins \"tables\" you'll just be wrong. And this fact isn't changed at all by the fact that \"table\" lacks a perfectly formal physics-level definition.\n\nThe same holds for \"Will Rob Bensinger's next experience be of sitting in his bedroom writing a LessWrong comment, or will it be of him grabbing some tomatoes in a supermarket in Beijing?\"\n\nTerms like 'Rob Bensinger' and 'I' aren't perfectly physically crisp â there may be cases where the answer is \"ehh, maybe?\" rather than a clear yes or no. And if we live in a Big Universe and we allow that there can be many Beijings out there in space, then we'll have to give a more nuanced quantitative answer, like \"a lot more of Rob's immediate futures are in his bedroom than in Beijing\".\n\nBut if we restrict our attention to *this* Beijing, then all that complexity goes away and we can pretty much rule out that anyone in Beijing will happen to momentarily exhibit exactly the right brain state to look like \"Rob Bensinger plus one time step\".\n\nThe nuances and wrinkles don't bleed over and make it a totally meaningless or arbitrary question; and indeed, if I thought I *were* likely to spontaneously teleport to Beijing in the next minute, I'd rightly be making very different life-choices! \"Will I experience myself spontaneously teleporting to Beijing in the next second?\" is a substantive (and easy) question, not a deep philosophical riddle.\n\n> So you always anticipate all possible experiences, because of multiverse?Â \n\nNot all possible experiences; just all experiences of brains that have the same kinds of structural similarities to your current brain as, e.g., \"me after I step through a doorway\" has to \"me before I stepped through the doorway\".",
      "plaintextDescription": "> Why? If \"I\" is arbitrary definition, then âWhen I step through this doorway, will I have another experience?\" depends on this arbitrary definition and so is also arbitrary.\n\nWhich things count as \"I\" isn't an arbitrary definition; it's just a fuzzy natural-language concept.\n\n(I guess you can call that \"arbitrary\" if you want, but then all the other words in the sentence, like \"doorway\" and \"step\", are also \"arbitrary\".)\n\nAnalogy: When you're writing in your personal diary, you're free to define \"table\" however you want. But in ordinary English-language discourse, if you call all penguins \"tables\" you'll just be wrong. And this fact isn't changed at all by the fact that \"table\" lacks a perfectly formal physics-level definition.\n\nThe same holds for \"Will Rob Bensinger's next experience be of sitting in his bedroom writing a LessWrong comment, or will it be of him grabbing some tomatoes in a supermarket in Beijing?\"\n\nTerms like 'Rob Bensinger' and 'I' aren't perfectly physically crisp â there may be cases where the answer is \"ehh, maybe?\" rather than a clear yes or no. And if we live in a Big Universe and we allow that there can be many Beijings out there in space, then we'll have to give a more nuanced quantitative answer, like \"a lot more of Rob's immediate futures are in his bedroom than in Beijing\".\n\nBut if we restrict our attention to this Beijing, then all that complexity goes away and we can pretty much rule out that anyone in Beijing will happen to momentarily exhibit exactly the right brain state to look like \"Rob Bensinger plus one time step\".\n\nThe nuances and wrinkles don't bleed over and make it a totally meaningless or arbitrary question; and indeed, if I thought I were likely to spontaneously teleport to Beijing in the next minute, I'd rightly be making very different life-choices! \"Will I experience myself spontaneously teleporting to Beijing in the next second?\" is a substantive (and easy) question, not a deep philosophical riddle.\n\n> So you always an"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-04-17T16:44:41.588Z",
    "af": false
  },
  {
    "_id": "Aa5nLQE7bFQCtQJBT",
    "postId": "sy4whuaczvLsn9PNc",
    "parentCommentId": "EnwBrNohnhDDbqPAG",
    "topLevelCommentId": "xkn5sSGwHbY4SyvRv",
    "contents": {
      "markdown": "> The problem is another way to phrase this is a superintelligent weapon system - \"ending a risk period\" by \"reliably, and efficiently doing a small number of specific concrete tasks\" means using physical force to impose your will on others.\n\nThe pivotal acts I usually think about actually don't route through physically messing with anyone else. I'm usually thinking about using aligned AGI to bootstrap to fast human whole-brain emulation, then using the ems to bootstrap to fully aligned CEV AI.\n\nIf someone pushes a \"destroy the world\" button then the ems or CEV AI would need to stop the world from being destroyed, but that won't necessarily happen if the developers have enough of a lead, if they get the job done quickly enough, and if CEV AI is able to persuade the world to step back from the precipice voluntarily (using superhumanly good persuasion that *isn't* mind-control-y, deceptive, or otherwise consent-violating). It's a big ask, but not as big as CEV itself, I expect.\n\nFrom my current perspective this is all somewhat of a moot point, however, because I don't think alignment is tractable enough that humanity should be trying to use aligned AI to prevent human extinction. I think we should instead hit the brakes on AI and shift efforts toward human enhancement, until some future generation is in a better position to handle the alignment problem.\n\n> If and only if that fails it may be appropriate to consider less consensual options.\n\nIt's not clear to me that we disagree in any action-relevant way, since I also don't think AI-enabled pivotal acts are the best path forward anymore. I think the path forward is via international agreements banning dangerous tech, and technical research to improve humanity's ability to wield such tech someday.\n\nThat said, it's not clear to me how your \"if that fails, then try X instead\" works in practice. How do you know when it's failed? Isn't it likely to be too late by the time we're sure that we've failed on that front? Indeed, it's plausibly already too late for humanity to seriously pivot to 'aligned AGI'. If I thought humanity's last best scrap of hope for survival lay in an AI-empowered pivotal act, I'd certainly want more details on when it's OK to start trying to figure out have humanity not die via this last desperate path.",
      "plaintextDescription": "> The problem is another way to phrase this is a superintelligent weapon system - \"ending a risk period\" by \"reliably, and efficiently doing a small number of specific concrete tasks\" means using physical force to impose your will on others.\n\nThe pivotal acts I usually think about actually don't route through physically messing with anyone else. I'm usually thinking about using aligned AGI to bootstrap to fast human whole-brain emulation, then using the ems to bootstrap to fully aligned CEV AI.\n\nIf someone pushes a \"destroy the world\" button then the ems or CEV AI would need to stop the world from being destroyed, but that won't necessarily happen if the developers have enough of a lead, if they get the job done quickly enough, and if CEV AI is able to persuade the world to step back from the precipice voluntarily (using superhumanly good persuasion that isn't mind-control-y, deceptive, or otherwise consent-violating). It's a big ask, but not as big as CEV itself, I expect.\n\nFrom my current perspective this is all somewhat of a moot point, however, because I don't think alignment is tractable enough that humanity should be trying to use aligned AI to prevent human extinction. I think we should instead hit the brakes on AI and shift efforts toward human enhancement, until some future generation is in a better position to handle the alignment problem.\n\n> If and only if that fails it may be appropriate to consider less consensual options.\n\nIt's not clear to me that we disagree in any action-relevant way, since I also don't think AI-enabled pivotal acts are the best path forward anymore. I think the path forward is via international agreements banning dangerous tech, and technical research to improve humanity's ability to wield such tech someday.\n\nThat said, it's not clear to me how your \"if that fails, then try X instead\" works in practice. How do you know when it's failed? Isn't it likely to be too late by the time we're sure that we've failed on that front? Indeed, i"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-03-19T20:32:24.795Z",
    "af": false
  },
  {
    "_id": "6ZJuNGC9FtSa7pKGY",
    "postId": "q3bJYTB3dGRf5fbD9",
    "parentCommentId": "QKtYcWrnXpBuegv5u",
    "topLevelCommentId": "AbGHJvFA9ysEHiEQA",
    "contents": {
      "markdown": "To pick out a couple of specific examples from your list, Wei Dai:\n\n> 14\\. Human-controlled AIs causing ethical disasters (e.g., large scale suffering that can't be \"balanced out\" later) prior to reaching moral/philosophical maturity\n\nThis is a serious long-term concern if we don't kill ourselves first, but it's not something I see as a premise for \"the priority is for governments around the world to form an international agreement to halt AI progress\". If AI were easy to use for concrete tasks like \"build nanotechnology\" but hard to use for things like CEV, I'd instead see the priority as \"use AI to prevent anyone else from destroying the world with AI\", and I wouldn't want to trade off probability of that plan working in exchange for (e.g.) more probability of the US and the EU agreeing in advance to centralize and monitor large computing clusters.\n\n*After* someone has done a pivotal act like that, you might then want to move more slowly insofar as you're worried about subtle moral errors creeping in to precursors to CEV.\n\n> 30\\. AI systems end up controlled by a group of humans representing a small range of human values (ie. an ideological or religious group that imposes values on everyone else)\n\nI currently assign very low probability to humans being able to control the first ASI systems, and redirecting governments' attention away from \"rogue AI\" and toward \"rogue humans using AI\" seems very risky to me, insofar as it causes governments to misunderstand the situation, and to specifically misunderstand it in a way that encourages racing.\n\nIf you think rogue actors can use ASI to achieve their ends, then you should probably also think that *you* could use ASI to achieve your own ends; misuse risk tends to go hand-in-hand with \"we're the Good Guys, let's try to outrace the Bad Guys so AI ends up in the right hands\". This could maybe be justified if it were *true*, but when it's not even true it strikes me as an especially bad argument to make.",
      "plaintextDescription": "To pick out a couple of specific examples from your list, Wei Dai:\n\n> 14. Human-controlled AIs causing ethical disasters (e.g., large scale suffering that can't be \"balanced out\" later) prior to reaching moral/philosophical maturity\n\nThis is a serious long-term concern if we don't kill ourselves first, but it's not something I see as a premise for \"the priority is for governments around the world to form an international agreement to halt AI progress\". If AI were easy to use for concrete tasks like \"build nanotechnology\" but hard to use for things like CEV, I'd instead see the priority as \"use AI to prevent anyone else from destroying the world with AI\", and I wouldn't want to trade off probability of that plan working in exchange for (e.g.) more probability of the US and the EU agreeing in advance to centralize and monitor large computing clusters.\n\nAfter someone has done a pivotal act like that, you might then want to move more slowly insofar as you're worried about subtle moral errors creeping in to precursors to CEV.\n\n> 30. AI systems end up controlled by a group of humans representing a small range of human values (ie. an ideological or religious group that imposes values on everyone else)\n\nI currently assign very low probability to humans being able to control the first ASI systems, and redirecting governments' attention away from \"rogue AI\" and toward \"rogue humans using AI\" seems very risky to me, insofar as it causes governments to misunderstand the situation, and to specifically misunderstand it in a way that encourages racing.\n\nIf you think rogue actors can use ASI to achieve their ends, then you should probably also think that you could use ASI to achieve your own ends; misuse risk tends to go hand-in-hand with \"we're the Good Guys, let's try to outrace the Bad Guys so AI ends up in the right hands\". This could maybe be justified if it were true, but when it's not even true it strikes me as an especially bad argument to make."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-01-06T01:39:29.697Z",
    "af": false
  },
  {
    "_id": "QKtYcWrnXpBuegv5u",
    "postId": "q3bJYTB3dGRf5fbD9",
    "parentCommentId": "Fne8ZbhetgfcbBWn8",
    "topLevelCommentId": "AbGHJvFA9ysEHiEQA",
    "contents": {
      "markdown": "Yep, before I saw orthonormal's response I had a draft-reply written that says almost literally the same thing:\n\n> we just call 'em like we see 'em\n> \n> \\[...\\]\n> \n> insofar as we make bad predictions, we should get penalized for it. and insofar as we think alignment difficulty is the crux for 'why we need to shut it all down', we'd rather directly argue against illusory alignment progress (and directly acknowledge real major alignment progress as a real reason to be less confident of shutdown as a strategy) rather than redirect to something less cruxy\n\nI'll also add: Nate (unlike Eliezer, AFAIK?) hasn't flatly said 'alignment is extremely difficult'. Quoting from Nate's \"sharp left turn\" post:\n\n> Many people wrongly believe that I'm pessimistic because I think the alignment problem is extraordinarily difficult on a purely technical level. That's flatly false, and is pretty high up there on my list of least favorite misconceptions of my views.\n> \n> I think the problem is a normal problem of mastering some scientific field, as humanity has done many times before. Maybe it's somewhat trickier, on account of (e.g.) intelligence being more complicated than, say, physics; maybe it's somewhat easier on account of how we have more introspective access to a working mind than we have to the low-level physical fields; but on the whole, I doubt it's all that qualitatively different than the sorts of summits humanity has surmounted before.\n> \n> It's made trickier by the fact that we probably have to attain mastery of general intelligence before we spend a bunch of time working with general intelligences (on account of how we seem likely to kill ourselves by accident within a few years, once we have AGIs on hand, if no pivotal act occurs), but that alone is not enough to undermine my hope.\n> \n> What undermines my hope is that nobody seems to be working on the hard bits, and I don't currently expect most people to become convinced that they need to solve those hard bits until it's too late.\n\nSo it may be that Nate's models would be less surprised by alignment breakthroughs than Eliezer's models. And some other MIRI folks are much more optimistic than Nate, FWIW.\n\nMy own view is that I don't feel nervous leaning on \"we won't crack open alignment in time\" as a premise, and absent that premise I'd indeed be much less gung-ho about government intervention.\n\n> why put all your argumentative eggs in the \"alignment is hard\" basket? (If you're right, then policymakers can't tell that you're right.)\n\nThe short answer is \"we don't put all our eggs in the basket\" (e.g., Eliezer's TED talk and TIME article emphasize that alignment is an open problem, but they emphasize other things too, and they don't go into detail on exactly how hard Eliezer thinks the problem is), plus \"we very much want at least some eggs in that basket because it's true, it's honest, it's cruxy for us, etc.\" And it's easier for policymakers to acquire strong Bayesian evidence for \"the problem is currently unsolved\" and \"there's no consensus about how to solve it\" and \"most leaders in the field seem to think there's a serious chance we won't solve it in time\" than to acquire strong Bayesian evidence for \"we're very likely generations away from solving alignment\", so the difficulty of communicating the latter isn't a strong reason to de-emphasize all the former points.\n\nThe longer answer is a lot more complicated. We're still figuring out how best to communicate our views to different audiences, and \"it's hard for policymakers to evaluate all the local arguments or know whether Yann LeCun is making more sense than Yoshua Bengio\" is a serious constraint. If there's a *specific* argument (or e.g. a specific three arguments) you think we should be emphasizing alongside \"alignment is unsolved and looks hard\", I'd be interested to hear your suggestion and your reasoning. [https://www.lesswrong.com/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk](https://www.lesswrong.com/posts/WXvt8bxYnwBYpy9oT/the-main-sources-of-ai-risk) is a very long list and isn't optimized for policymakers, so I'm not sure what specific changes you have in mind here.",
      "plaintextDescription": "Yep, before I saw orthonormal's response I had a draft-reply written that says almost literally the same thing:\n\n> we just call 'em like we see 'em\n> \n> [...]\n> \n> insofar as we make bad predictions, we should get penalized for it. and insofar as we think alignment difficulty is the crux for 'why we need to shut it all down', we'd rather directly argue against illusory alignment progress (and directly acknowledge real major alignment progress as a real reason to be less confident of shutdown as a strategy) rather than redirect to something less cruxy\n\nI'll also add: Nate (unlike Eliezer, AFAIK?) hasn't flatly said 'alignment is extremely difficult'. Quoting from Nate's \"sharp left turn\" post:\n\n> Many people wrongly believe that I'm pessimistic because I think the alignment problem is extraordinarily difficult on a purely technical level. That's flatly false, and is pretty high up there on my list of least favorite misconceptions of my views.\n> \n> I think the problem is a normal problem of mastering some scientific field, as humanity has done many times before. Maybe it's somewhat trickier, on account of (e.g.) intelligence being more complicated than, say, physics; maybe it's somewhat easier on account of how we have more introspective access to a working mind than we have to the low-level physical fields; but on the whole, I doubt it's all that qualitatively different than the sorts of summits humanity has surmounted before.\n> \n> It's made trickier by the fact that we probably have to attain mastery of general intelligence before we spend a bunch of time working with general intelligences (on account of how we seem likely to kill ourselves by accident within a few years, once we have AGIs on hand, if no pivotal act occurs), but that alone is not enough to undermine my hope.\n> \n> What undermines my hope is that nobody seems to be working on the hard bits, and I don't currently expect most people to become convinced that they need to solve those hard bits until it's "
    },
    "baseScore": 12,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2024-01-06T01:05:17.532Z",
    "af": false
  },
  {
    "_id": "6ffFZPCKpRZTDoPnY",
    "postId": "sy4whuaczvLsn9PNc",
    "parentCommentId": "ktitzaT3WmAskw6Pk",
    "topLevelCommentId": "bDggdkiigvbpyMvG4",
    "contents": {
      "markdown": "I expect it makes it easier, but I don't think it's solved.",
      "plaintextDescription": "I expect it makes it easier, but I don't think it's solved."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-12-17T00:31:56.694Z",
    "af": false
  },
  {
    "_id": "BfzBXJN4sxvn8SAgE",
    "postId": "sy4whuaczvLsn9PNc",
    "parentCommentId": "ynTEuEL8jZ43Eupim",
    "topLevelCommentId": "bDggdkiigvbpyMvG4",
    "contents": {
      "markdown": "> Suppose you want to synthesize a lot of diamonds. Instead of giving an AI some lofty goal \"maximize diamonds in an aligned way\", why not a bunch of small grounded ones.\n> \n> 1.  \"Plan the factory layout of the diamond synthesis plant with these requirements\".\n> 2.  \"Order the equipment needed, here's the payment credentials\".\n> 3.  \"Supervise construction this workday comparing to original plans\"\n> 4.  \"Given this step of the plan, do it\"\n> 5.  (Once the factory is built) \"remove the output from diamond synthesis machine A53 and clean it\".\n\nThat *is* how MIRI imagines a sane developer using just-barely-aligned AI to save the world. You don't build an open-ended maximizer and unleash it on the world to maximize some quantity that sounds good to you; that sounds insanely difficult. You carve out as many tasks as you can into concrete, verifiable chunks, and you build the weakest and most limited possible AI you can to complete each chunk, to minimize risk. (Though per faul_sname, you're likely to be pretty limited in how much you can carve up the task, given time will be a major constraint and there may be parts of the task you don't fully understand at the outset.)\n\nCf. [The Rocket Alignment Problem](https://intelligence.org/2018/10/03/rocket-alignment/). The point of solving the diamond maximizer problem isn't to go build the thing; it's that solving it is an indication that we've become less conceptually confused about real-world optimization and about aimable cognitive work. Being less conceptually confused about very basic aspects of problem-solving and goal-oriented reasoning means that you might be able to build some of your powerful AI systems out of building blocks that are relatively easy to analyze, test, design, predict, separate out into discrete modules, measure and limit the capabilities of, etc., etc.\n\n> That seems acceptable, industrial equipment causes accidents all the time, the main thing is to limit the damage. Fences to limit the robots operating area, timers that shut down control after a timeout, etc.\n\nIf everyone in the world chooses to permanently use very weak systems because they're scared of AI killing them, then yes, the impact of any given system failing will stay low. But that's not what's going to actually happen; many people will use more powerful systems, once they can, because they misunderstand the risks or have galaxy-brained their way into not caring about them (e.g. 'maybe humans don't deserve to live', 'if I don't do it someone else will anyway', 'if it's *that* easy to destroy the world then we're fucked anyway so I should just do the Modest thing of assuming nothing I do is that important'...).\n\nThe world needs some solution to the problem \"if AI keeps advancing and more-powerful AI keeps proliferating, eventually someone will destroy the world with it\". I don't know of a way to leverage AI to solve that problem without the AI being pretty dangerously powerful, so I don't think AI is going to get us out of this mess *unless* we make a shocking amount of progress on figuring out how to align more powerful systems. (Where \"aligning\" includes things like being able to predict in advance how pragmatically powerful your system is, and being able to carefully limit the ways in which it's powerful.)",
      "plaintextDescription": "> Suppose you want to synthesize a lot of diamonds. Instead of giving an AI some lofty goal \"maximize diamonds in an aligned way\", why not a bunch of small grounded ones.\n> \n>  1. \"Plan the factory layout of the diamond synthesis plant with these requirements\".\n>  2. \"Order the equipment needed, here's the payment credentials\".\n>  3. \"Supervise construction this workday comparing to original plans\"\n>  4. \"Given this step of the plan, do it\"\n>  5. (Once the factory is built) \"remove the output from diamond synthesis machine A53 and clean it\".\n\nThat is how MIRI imagines a sane developer using just-barely-aligned AI to save the world. You don't build an open-ended maximizer and unleash it on the world to maximize some quantity that sounds good to you; that sounds insanely difficult. You carve out as many tasks as you can into concrete, verifiable chunks, and you build the weakest and most limited possible AI you can to complete each chunk, to minimize risk. (Though per faul_sname, you're likely to be pretty limited in how much you can carve up the task, given time will be a major constraint and there may be parts of the task you don't fully understand at the outset.)\n\nCf. The Rocket Alignment Problem. The point of solving the diamond maximizer problem isn't to go build the thing; it's that solving it is an indication that we've become less conceptually confused about real-world optimization and about aimable cognitive work. Being less conceptually confused about very basic aspects of problem-solving and goal-oriented reasoning means that you might be able to build some of your powerful AI systems out of building blocks that are relatively easy to analyze, test, design, predict, separate out into discrete modules, measure and limit the capabilities of, etc., etc.\n\n> That seems acceptable, industrial equipment causes accidents all the time, the main thing is to limit the damage. Fences to limit the robots operating area, timers that shut down control after a timeout, etc"
    },
    "baseScore": 14,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2023-12-16T23:01:28.581Z",
    "af": false
  },
  {
    "_id": "JiJqD3Pp9Hqnv7yfS",
    "postId": "sy4whuaczvLsn9PNc",
    "parentCommentId": "prXsAsqdQMQqBBM6h",
    "topLevelCommentId": "bDggdkiigvbpyMvG4",
    "contents": {
      "markdown": "To be clear: The diamond maximizer problem is about getting specific intended content into the AI's goals (\"diamonds\" as opposed to some random physical structure it's maximizing), not just about building a stable maximizer.",
      "plaintextDescription": "To be clear: The diamond maximizer problem is about getting specific intended content into the AI's goals (\"diamonds\" as opposed to some random physical structure it's maximizing), not just about building a stable maximizer."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-12-16T22:36:59.979Z",
    "af": false
  },
  {
    "_id": "xkn5sSGwHbY4SyvRv",
    "postId": "sy4whuaczvLsn9PNc",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "From briefly talking to Eliezer about this the other day, I think the story from MIRI's perspective is more like:\n\n*   Back in 2001, we [defined](https://intelligence.org/files/CFAI.pdf) \"Friendly AI\" as \"The field of study concerned with the production of human-benefiting, non-human-harming actions in Artificial Intelligence systems that have advanced to the point of making real-world plans in pursuit of goals.\"\n\nWe could have defined the goal more narrowly or generically than that, but that just seemed like an invitation to take your eye off the ball: if we aren't going to think about the question of how to get good long-run outcomes from powerful AI systems, who will?\n\nAnd many of the technical and philosophical problems seemed particular to CEV, which seemed like an obvious sort of solution to shoot for: just find some way to leverage the AI's intelligence to solve the problem of extrapolating everyone's preferences in a reasonable way, and of aggregating those preferences fairly.\n\n*   Come 2014, Stuart Russell and MIRI were both looking for a new term to replace \"the Friendly AI problem\", now that the field was starting to become a Real Thing. Both parties disliked Bostrom's \"the control problem\". In conversation, Russell proposed \"the alignment problem\", and MIRI liked it, so Russell and MIRI both started using the term in public.\n\nUnfortunately, it gradually came to light that Russell and MIRI had understood \"Friendly AI\" to mean two moderately different things, and this disconnect now turned into a split between how MIRI used \"(AI) alignment\" and how Russell used \"(value) alignment\". (Which I think also influenced the split between Paul Christiano's \"(intent) alignment\" and MIRI's \"(outcome) alignment\".)\n\nRussell's version of \"friendliness/alignment\" was about making the AI have good, human-deferential goals. But *Creating Friendly AI 1.0* had been very explicit that \"friendliness\" was about good *behavior*, regardless of how that's achieved. MIRI's conception of \"the alignment problem\" (like Bostrom's \"control problem\") included tools like capability constraint and boxing, because the thing we wanted researchers to focus on was *the goal of leveraging AI capabilities to get actually-good outcomes, whatever technical work that requires*, not some proxy goal that might turn out to be surprisingly irrelevant.\n\nAgain, we wanted a field of people keeping their eye on the ball and looking for clever technical ways to get the job done, rather than a field that neglects some actually-useful technique because it doesn't fit their narrow definition of \"alignment\".\n\n*   Meanwhile, developments like the rise of deep learning had updated MIRI that CEV was not going to be a realistic thing to shoot for with your first AI. We were still thinking of some version of CEV as the ultimate goal, but it now seemed clear that capabilities were progressing too quickly for humanity to have time to nail down all the details of CEV, and it was also clear that the approaches to AI that were winning out would be far harder to analyze, predict, and \"aim\" than 2001-Eliezer had expected. It seemed clear that if AI was going to help make the future go well, the first order of business would be to do the minimal thing to prevent other AIs from destroying the world six months later, with other parts of alignment/friendliness deferred to later.\n\nI think considerations like this eventually trickled in to how MIRI used the term \"alignment\". Our first public writing reflecting the switch from \"Friendly AI\" to \"alignment\", our Dec. 2014 [agent foundations research agenda](https://intelligence.org/files/TechnicalAgenda.pdf), said:\n\n> We call a smarter-than-human system that reliably pursues beneficial goals âaligned with human interestsâ or simply âaligned.â\n\nWhereas by July 2016, when we released [a new research agenda that was more ML-focused](https://intelligence.org/files/AlignmentMachineLearning.pdf), \"aligned\" was shorthand for \"aligned with the interests of the operators\".\n\nIn practice, we started using \"aligned\" to mean something more like \"aimable\" (where aimability includes things like corrigibility, limiting side-effects, monitoring and limiting capabilities, etc., not just \"getting the AI to predictably tile the universe with smiley faces rather than paperclips\"). Focusing on CEV-ish systems mostly seemed like a distraction, and an invitation to get caught up in moral philosophy and pie-in-the-sky abstractions, when \"do a pivotal act\" is legitimately a hugely more philosophically shallow topic than \"implement CEV\". Instead, we went out of our way to frame the challenge of alignment in a way that seemed almost comically simple and \"un-philosophical\", but that successfully captured all of the key obstacles: 'explain how to use an AI to cause there two exist [two strawberries that are identical at the cellular level](https://www.lesswrong.com/posts/SsCQHjqNT3xQAPQ6b/yudkowsky-on-agi-ethics), without causing anything weird or disruptive to happen in the process'.\n\nSince realistic pivotal acts still seemed pretty outside the Overton window (and since we were mostly focused on our own research at the time), we wrote up our basic thoughts about the topic on Arbital but didn't try to super-popularize the topic among rationalists or EAs at the time. (Which unfortunately, I think, exacerbated a situation where the larger communities had very fuzzy models of the strategic situation, and fuzzy models of what the point even was of this \"alignment research\" thing; alignment research just become a thing-that-was-good-because-it-was-a-good, not a concrete part of a plan backchained from concrete real-world goals.)\n\nI don't think MIRI wants to stop using \"aligned\" in the context of pivotal acts, and I also don't think MIRI wants to totally divorce the term from the original long-term goal of friendliness/alignment.\n\nTurning \"alignment\" purely into a matter of \"get the AI to do what a particular stakeholder wants\" is good in some ways -- e.g., it clarifies that the level of alignment needed for pivotal acts could also be used to do bad things.\n\nBut from Eliezer's perspective, this move would also be sending a message to all the young Eliezers \"Alignment Research is what you do if you're a serious sober person who thinks it's naive to care about Doing The Right Thing and is instead just trying to make AI Useful To Powerful People; if you want to aim for the obvious desideratum of making AI friendly and beneficial to the world, go join e/acc or something\". Which does not seem ideal.\n\nSo I think my proposed solution would be to just acknowledge that 'the alignment problem' is ambiguous between three different (overlapping) efforts to figure out how to get good and/or intended outcomes from powerful AI systems:\n\n*   **intent alignment**, which is about getting AIs to try to do what the AI thinks the user wants, and in practice seems to be most interested in 'how do we get AIs to be generically trying-to-be-helpful'.\n*   **\"strawberry problem\" alignment**, which is about getting AIs to safely, reliably, and efficiently do *a small number of specific concrete tasks* that are very difficult, for the sake of ending the acute existential risk period.\n*   **CEV-style alignment**, which is about getting AIs to fully figure out how to make the future good.\n\nPlausibly it would help to have better names for the latter two things. The distinction is similar to \"narrow value learning vs. ambitious value learning\", but both problems (as MIRI thinks about them) are a lot more general than just \"value learning\", and there's a lot more content to the strawberry problem than to \"narrow alignment\", and more content to CEV than to \"ambitious value learning\" (e.g., CEV cares about aggregation across people, not just about extrapolation).\n\n(Note: Take the above summary of MIRI's history with a grain of salt; I had Nate Soares look at this comment and he said \"on a skim, it doesn't seem to quite line up with my recollections nor cut things along the joints I would currently cut them along, but maybe it's better than nothing\".)",
      "plaintextDescription": "From briefly talking to Eliezer about this the other day, I think the story from MIRI's perspective is more like:\n\n * Back in 2001, we defined \"Friendly AI\" as \"The field of study concerned with the production of human-benefiting, non-human-harming actions in Artificial Intelligence systems that have advanced to the point of making real-world plans in pursuit of goals.\"\n\nWe could have defined the goal more narrowly or generically than that, but that just seemed like an invitation to take your eye off the ball: if we aren't going to think about the question of how to get good long-run outcomes from powerful AI systems, who will?\n\nAnd many of the technical and philosophical problems seemed particular to CEV, which seemed like an obvious sort of solution to shoot for: just find some way to leverage the AI's intelligence to solve the problem of extrapolating everyone's preferences in a reasonable way, and of aggregating those preferences fairly.\n\n * Come 2014, Stuart Russell and MIRI were both looking for a new term to replace \"the Friendly AI problem\", now that the field was starting to become a Real Thing. Both parties disliked Bostrom's \"the control problem\". In conversation, Russell proposed \"the alignment problem\", and MIRI liked it, so Russell and MIRI both started using the term in public.\n\nUnfortunately, it gradually came to light that Russell and MIRI had understood \"Friendly AI\" to mean two moderately different things, and this disconnect now turned into a split between how MIRI used \"(AI) alignment\" and how Russell used \"(value) alignment\". (Which I think also influenced the split between Paul Christiano's \"(intent) alignment\" and MIRI's \"(outcome) alignment\".)\n\nRussell's version of \"friendliness/alignment\" was about making the AI have good, human-deferential goals. But Creating Friendly AI 1.0 had been very explicit that \"friendliness\" was about good behavior, regardless of how that's achieved. MIRI's conception of \"the alignment problem\" (like Bostrom's \"co"
    },
    "baseScore": 63,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2023-12-16T22:28:07.948Z",
    "af": false
  },
  {
    "_id": "vx7NJnjHrxs6wAS3w",
    "postId": "HXyGXq9YmKdjqPseW",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "In the context of a [conversation](https://twitter.com/robbensinger/status/1735027084194668668) with Balaji Srinivasan about my [AI views snapshot](https://www.lesswrong.com/posts/ZDQvYKuyH5aTrZKoY/ai-views-snapshots), I asked Nate Soares what sorts of alignment results would impress him, and he said:\n\n> example thing that would be relatively impressive to me: specific, comprehensive understanding of models (with the caveat that that knowledge may lend itself more (and sooner) to capabilities before alignment). demonstrated e.g. by the ability to precisely predict the capabilities and quirks of the next generation (before running it)\n> \n> i'd also still be impressed by simple theories of aimable cognition (i mostly don't expect that sort of thing to have time to play out any more, but if someone was able to come up with one after staring at LLMs for a while, i would at least be impressed)\n> \n> fwiw i don't myself really know how to answer the question \"technical research is more useful than policy research\"; like that question sounds to me like it's generated from a place of \"enough of either of these will save you\" whereas my model is more like \"you need both\"\n> \n> tho i'm more like \"to get the requisite technical research, aim for uploads\" at this juncture\n> \n> if this was gonna be blasted outwards, i'd maybe also caveat that, while a bunch of this is a type of interpretability work, i also expect a bunch of interpretability work to strike me as fake, shallow, or far short of the bar i consider impressive/hopeful\n> \n> (which is not itself supposed to be any kind of sideswipe; i applaud interpretability efforts even while thinking it's moving too slowly etc.)",
      "plaintextDescription": "In the context of a conversation with Balaji Srinivasan about my AI views snapshot, I asked Nate Soares what sorts of alignment results would impress him, and he said:\n\n> example thing that would be relatively impressive to me: specific, comprehensive understanding of models (with the caveat that that knowledge may lend itself more (and sooner) to capabilities before alignment). demonstrated e.g. by the ability to precisely predict the capabilities and quirks of the next generation (before running it)\n> \n> i'd also still be impressed by simple theories of aimable cognition (i mostly don't expect that sort of thing to have time to play out any more, but if someone was able to come up with one after staring at LLMs for a while, i would at least be impressed)\n> \n> fwiw i don't myself really know how to answer the question \"technical research is more useful than policy research\"; like that question sounds to me like it's generated from a place of \"enough of either of these will save you\" whereas my model is more like \"you need both\"\n> \n> tho i'm more like \"to get the requisite technical research, aim for uploads\" at this juncture\n> \n> if this was gonna be blasted outwards, i'd maybe also caveat that, while a bunch of this is a type of interpretability work, i also expect a bunch of interpretability work to strike me as fake, shallow, or far short of the bar i consider impressive/hopeful\n> \n> (which is not itself supposed to be any kind of sideswipe; i applaud interpretability efforts even while thinking it's moving too slowly etc.)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2023-12-14T20:58:40.013Z",
    "af": true
  },
  {
    "_id": "EiF6dHmbuDY9vaHrj",
    "postId": "AWoZBzxdm4DoGgiSj",
    "parentCommentId": "7nFebCepFaP9cGo5a",
    "topLevelCommentId": "mEPjkATGDExGcZWop",
    "contents": {
      "markdown": "> I can come up with plans for destroying the world without wanting to do it, and other cognitive systems probably can too.\n\nYou're changing the topic to \"can you do X without wanting Y?\", when the original question was \"can you do X without wanting anything at all?\".\n\nNate's answer to nearly all questions of the form \"can you do X without wanting Y?\" is \"yes\", hence his second claim in the OP: \"the wanting-like behavior required to pursue a particular training target X, does not need to involve the AI wanting X in particular\".\n\n> I do need to answer that question using in a goal-oriented search process. But my goal would be \"answer Paul's question\", not \"destroy the world\".\n\nYour ultimate goal would be neither of those things; you're a human, and if you're answering Paul's question it's probably because you have other goals that are served by answering.\n\nIn the same way, an AI that's sufficiently good at answering sufficiently hard and varied questions would probably also have goals, and it's unlikely by default that \"answer questions\" will be the AI's primary goal.",
      "plaintextDescription": "> I can come up with plans for destroying the world without wanting to do it, and other cognitive systems probably can too.\n\nYou're changing the topic to \"can you do X without wanting Y?\", when the original question was \"can you do X without wanting anything at all?\".\n\nNate's answer to nearly all questions of the form \"can you do X without wanting Y?\" is \"yes\", hence his second claim in the OP: \"the wanting-like behavior required to pursue a particular training target X, does not need to involve the AI wanting X in particular\".\n\n> I do need to answer that question using in a goal-oriented search process. But my goal would be \"answer Paul's question\", not \"destroy the world\".\n\nYour ultimate goal would be neither of those things; you're a human, and if you're answering Paul's question it's probably because you have other goals that are served by answering.\n\nIn the same way, an AI that's sufficiently good at answering sufficiently hard and varied questions would probably also have goals, and it's unlikely by default that \"answer questions\" will be the AI's primary goal."
    },
    "baseScore": 17,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2023-11-25T00:09:34.014Z",
    "af": false
  },
  {
    "_id": "HNxCCD7u7kwTE6TCi",
    "postId": "JcLhYQQADzTsAEaXd",
    "parentCommentId": "v9CfJmBmqhjM85kKm",
    "topLevelCommentId": "v9CfJmBmqhjM85kKm",
    "contents": {
      "markdown": "> The idea that an area of study is less scientific because the subject is *inelegant* is a blinkered view of what science is.\n\nSee my reply to Bogdan [here](https://www.lesswrong.com/posts/JcLhYQQADzTsAEaXd/ai-as-a-science-and-three-obstacles-to-alignment-strategies?commentId=ZiouXjxcrvjk65ihW#ZiouXjxcrvjk65ihW). The issue isn't \"inelegance\"; we also lack an *inelegant* ability to predict or explain how particular ML systems do what they do.\n\nModern ML is less like modern chemistry, and more like ancient culinary arts and medicine. (Or \"ancient culinary arts and medicine shortly after a cultural reboot\", such that we have a relatively small number of recently-developed shallow heuristics and facts to draw on, rather than centuries of hard-earned experience.)\n\n> The opening sounds a lot like saying \"aerodynamics used to be a science until people started building planes.\"\n\nThe reason this analogy doesn't land for me is that I don't think our epistemic position regarding LLMs is similar to, e.g., the Wright brothers' epistemic position regarding heavier-than-air flight.\n\nThe point Nate was trying to make with \"ML is no longer a science\" wasn't \"boo current ML that actually works, yay GOFAI that didn't work\". The point was exactly to draw a contrast between, e.g., our understanding of heavier-than-air flight and our understanding of how the human brain works. The invention of useful tech that interfaces with the brain doesn't entail that we understand the brain's workings in the way we've long understood flight; it depends on what the (actual or hypothetical) tech is.\n\nMaybe a clearer way of phrasing it is \"AI used to be failed science; now it's (mostly, outside of a few small oases) a not-even-attempted science\". \"Failed science\" maybe makes it clearer that the point here isn't to praise the old approaches that didn't work; there's a more nuanced point being made.",
      "plaintextDescription": "> The idea that an area of study is less scientific because the subject is inelegant is a blinkered view of what science is.\n\nSee my reply to Bogdan here. The issue isn't \"inelegance\"; we also lack an inelegant ability to predict or explain how particular ML systems do what they do.\n\nModern ML is less like modern chemistry, and more like ancient culinary arts and medicine. (Or \"ancient culinary arts and medicine shortly after a cultural reboot\", such that we have a relatively small number of recently-developed shallow heuristics and facts to draw on, rather than centuries of hard-earned experience.)\n\n> The opening sounds a lot like saying \"aerodynamics used to be a science until people started building planes.\"\n\nThe reason this analogy doesn't land for me is that I don't think our epistemic position regarding LLMs is similar to, e.g., the Wright brothers' epistemic position regarding heavier-than-air flight.\n\nThe point Nate was trying to make with \"ML is no longer a science\" wasn't \"boo current ML that actually works, yay GOFAI that didn't work\". The point was exactly to draw a contrast between, e.g., our understanding of heavier-than-air flight and our understanding of how the human brain works. The invention of useful tech that interfaces with the brain doesn't entail that we understand the brain's workings in the way we've long understood flight; it depends on what the (actual or hypothetical) tech is.\n\nMaybe a clearer way of phrasing it is \"AI used to be failed science; now it's (mostly, outside of a few small oases) a not-even-attempted science\". \"Failed science\" maybe makes it clearer that the point here isn't to praise the old approaches that didn't work; there's a more nuanced point being made."
    },
    "baseScore": 31,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2023-10-27T19:03:33.789Z",
    "af": false
  },
  {
    "_id": "ZiouXjxcrvjk65ihW",
    "postId": "JcLhYQQADzTsAEaXd",
    "parentCommentId": "7YnA5xGxvQZfddsax",
    "topLevelCommentId": "7YnA5xGxvQZfddsax",
    "contents": {
      "markdown": "Some of Nateâs quick thoughts (paraphrased), after chatting with him:\n\nNate isnât trying to say that we have literally zero understanding of deep nets. What heâs trying to do is qualitatively point to the kind of high-level situation weâre in, in part because he thinks there *is* real interpretability progress, and when youâre working in the interpretability mines and seeing real advances it can be easy to miss the forest for the trees and forget how far we are from understanding what LLMs are doing. (Compared to, e.g., how well we can predict or post-facto-mechanistically-explain a typical system humans have engineered.)\n\nNobody's been able to call the specific capabilities of systems in advance. Nobody's been able to call the specific exploits in advance. Nobody's been able to build better cognitive algorithms by hand after understanding how the AI does things we can't yet code by hand. There is clearly some other level of understanding that is possible that we lack, and that we once sought, and that only the interpretability folks continue to seek.\n\nE.g., think of that time Neel NandaÂ [figured out](https://twitter.com/robertskmiles/status/1663534255249453056) how a small transformer does modular arithmetic ([AXRP episode](https://www.alignmentforum.org/posts/r2yTwkGt3kbQG2mXi/axrp-episode-19-mechanistic-interpretability-with-neel-nanda#How_neural_nets_learn_modular_addition_)). If nobody had ever thought of that algorithm for an adder, we would have thereby learned a new algorithm for an adder. There are things that these AI systems are doing that arenât just lots of stuff we know; there are levels of organization of understanding that give you the ability to predict how things work outside of the bands where weâve observed them.\n\nIt seems trendy to declare that they never existed in the first place and that thatâs all white tower stuff, but Nate thinks this point of view is missing a pretty important and central thread.\n\nThe missing thread isnât trivial to put into words, but it includes things like:Â \n\n*   This sounds like the same sort of thing some people would say if they were staring at computer binary for the first time and didn't know about the code behind the scenes: \"We have plenty of understanding beyond just how the CPU handles instructions; we understand how memory caching works and we have recognized patterns like the stack and the heap; talking as if there's some deeper level of organization is talking like a theorist when in fact this is an engineering problem.\" Those types of understanding aren't false, but they aren't the sort of understanding of someone who has comprehended the codebase they're looking at.\n*   There are, predictably, things to learn here; the messiness and complexity of the real world doesnât mean we already know the relevant principles. You don't need to understand everything about how a bird works in order to build an airplane; there are compressible principles behind how birds fly; if you understand what's going on you can build flying devices that have significantly more carrying capacity than a bird, and this holds true even if the practical engineering of an airplane requires a bunch of trial and error and messy engineering work.\n*   A mindâs causal structure is allowed to be complicated; we can see the weights, but we donât thereby have a mastery of the high-level patterns. In the case of humans, neuroscience hasnât actually worked to give us a mastery of the high-level patterns the human brain is implementing.\n*   Mystery is in the map, not in the territory; reductionism works. Not all sciences that can exist, already exist today.\n\nPossibly the above pointers are only useful if you already grok the point weâre trying to make, and isnât so useful for communicating a new idea; but perhaps not.",
      "plaintextDescription": "Some of Nateâs quick thoughts (paraphrased), after chatting with him:\n\nNate isnât trying to say that we have literally zero understanding of deep nets. What heâs trying to do is qualitatively point to the kind of high-level situation weâre in, in part because he thinks there is real interpretability progress, and when youâre working in the interpretability mines and seeing real advances it can be easy to miss the forest for the trees and forget how far we are from understanding what LLMs are doing. (Compared to, e.g., how well we can predict or post-facto-mechanistically-explain a typical system humans have engineered.)\n\nNobody's been able to call the specific capabilities of systems in advance. Nobody's been able to call the specific exploits in advance. Nobody's been able to build better cognitive algorithms by hand after understanding how the AI does things we can't yet code by hand. There is clearly some other level of understanding that is possible that we lack, and that we once sought, and that only the interpretability folks continue to seek.\n\nE.g., think of that time Neel NandaÂ figured out how a small transformer does modular arithmetic (AXRP episode). If nobody had ever thought of that algorithm for an adder, we would have thereby learned a new algorithm for an adder. There are things that these AI systems are doing that arenât just lots of stuff we know; there are levels of organization of understanding that give you the ability to predict how things work outside of the bands where weâve observed them.\n\nIt seems trendy to declare that they never existed in the first place and that thatâs all white tower stuff, but Nate thinks this point of view is missing a pretty important and central thread.\n\nThe missing thread isnât trivial to put into words, but it includes things like:Â \n\n * This sounds like the same sort of thing some people would say if they were staring at computer binary for the first time and didn't know about the code behind the scenes: \"We have "
    },
    "baseScore": 11,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2023-10-27T18:39:02.308Z",
    "af": true
  },
  {
    "_id": "XcG3eLrKyQyx4SpHy",
    "postId": "NjtHt55nFbw3gehzY",
    "parentCommentId": "eLxMmKggFxzt82JBP",
    "topLevelCommentId": "eLxMmKggFxzt82JBP",
    "contents": {
      "markdown": "I read and responded to some pieces of that post when it came out; I don't know whether Eliezer, Nate, etc. read it, and I'm guessing it didn't shift MIRI, except as one of many data points \"person X is now loudly in favor of a pause (and other people seem receptive), so maybe this is more politically tractable than we thought\".\n\nI'd say that Kerry Vaughan was the main person who started smashing this Overton window, and this started in [April](https://twitter.com/KerryLVaughan/status/1511992548423786498)/[May](https://twitter.com/KerryLVaughan/status/1526631718047797248)/[June](https://twitter.com/KerryLVaughan/status/1533866180037038084) of 2022. By late December my recollection is that this public conversation was already fully in swing and MIRI had already added our voices to the \"stop building toward AGI\" chorus. (Though at that stage I think we were mostly doing this on general principle, for lack of any better ideas than \"share our actual long-standing views and hope that helps somehow\". Our increased optimism about policy solutions mostly came later, in 2023.)\n\nThat said, I bet Katja's post had tons of relevant positive effects even if it didn't directly shift MIRI's views.",
      "plaintextDescription": "I read and responded to some pieces of that post when it came out; I don't know whether Eliezer, Nate, etc. read it, and I'm guessing it didn't shift MIRI, except as one of many data points \"person X is now loudly in favor of a pause (and other people seem receptive), so maybe this is more politically tractable than we thought\".\n\nI'd say that Kerry Vaughan was the main person who started smashing this Overton window, and this started in April/May/June of 2022. By late December my recollection is that this public conversation was already fully in swing and MIRI had already added our voices to the \"stop building toward AGI\" chorus. (Though at that stage I think we were mostly doing this on general principle, for lack of any better ideas than \"share our actual long-standing views and hope that helps somehow\". Our increased optimism about policy solutions mostly came later, in 2023.)\n\nThat said, I bet Katja's post had tons of relevant positive effects even if it didn't directly shift MIRI's views."
    },
    "baseScore": 9,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2023-10-17T20:16:25.228Z",
    "af": false
  },
  {
    "_id": "RfBPJFeposGaaFz84",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "seyFuAFAmHDENst3K",
    "topLevelCommentId": "DTqTEwot983JAwFsc",
    "contents": {
      "markdown": "Remember that MIRI was in the business of poking at theoretical toy problems and trying to get less conceptually confused about how you could in principle cleanly design a reliable, aimable reasoner. MIRI wasn't (and isn't) in the business of issuing challenges to capabilities researchers to build a working water-bucket-filler as soon as possible, and wasn't otherwise in the business of challenging people to race to AGI faster.\n\nIt wouldn't have occurred to me that someone might think 'can a deep net fill a bucket of water, in real life, without being dangerously capable' is a crucial question in this context; I'm not sure we ever even *had the thought occur in our heads* 'when might such-and-such DL technique successfully fill a bucket?'. It would seem just as strange to me as going to check the literature to make sure no GOFAI system ever filled a bucket of water.\n\n(And while I think I understand why others see ChatGPT as a large positive update about alignment's difficulty, I hope it's also obvious why others, MIRI included, would *not* see it that way.)\n\nHacky approaches to alignment do count just as much as clean, scrutable, principled approaches -- the important thing is that the AGI transition goes well, not that it goes well *and* feels clean and tidy in the process. But in this case the messy empirical approach doesn't look to me like it actually lets you build a corrigible AI that can help with a pivotal act.\n\nIf general-ish DL methods were *already* empirically OK at filling water buckets in 2016, *just as GOFAI already was in 2016*, I suspect we still would have been happy to use the *Fantasia* example, because it's a simple well-known story that can help make the abstract talk of utility functions and off-switch buttons easier to mentally visualize and manipulate.\n\n(Though now that I've seen the confusion the example causes, I'm more inclined to think that the strawberry problem is a better frame than the *Fantasia* example.)",
      "plaintextDescription": "Remember that MIRI was in the business of poking at theoretical toy problems and trying to get less conceptually confused about how you could in principle cleanly design a reliable, aimable reasoner. MIRI wasn't (and isn't) in the business of issuing challenges to capabilities researchers to build a working water-bucket-filler as soon as possible, and wasn't otherwise in the business of challenging people to race to AGI faster.\n\nIt wouldn't have occurred to me that someone might think 'can a deep net fill a bucket of water, in real life, without being dangerously capable' is a crucial question in this context; I'm not sure we ever even had the thought occur in our heads 'when might such-and-such DL technique successfully fill a bucket?'. It would seem just as strange to me as going to check the literature to make sure no GOFAI system ever filled a bucket of water.\n\n(And while I think I understand why others see ChatGPT as a large positive update about alignment's difficulty, I hope it's also obvious why others, MIRI included, would not see it that way.)\n\nHacky approaches to alignment do count just as much as clean, scrutable, principled approaches -- the important thing is that the AGI transition goes well, not that it goes well and feels clean and tidy in the process. But in this case the messy empirical approach doesn't look to me like it actually lets you build a corrigible AI that can help with a pivotal act.\n\nIf general-ish DL methods were already empirically OK at filling water buckets in 2016, just as GOFAI already was in 2016, I suspect we still would have been happy to use the Fantasia example, because it's a simple well-known story that can help make the abstract talk of utility functions and off-switch buttons easier to mentally visualize and manipulate.\n\n(Though now that I've seen the confusion the example causes, I'm more inclined to think that the strawberry problem is a better frame than the Fantasia example.)"
    },
    "baseScore": 5,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2023-10-08T06:19:28.692Z",
    "af": false
  },
  {
    "_id": "seyFuAFAmHDENst3K",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "ujZFofbnKmjwi54EM",
    "topLevelCommentId": "DTqTEwot983JAwFsc",
    "contents": {
      "markdown": "> I think the old school MIRI cauldron-filling problem pertained to pretty mundane, everyday tasks. No one said at the time that they didnât *really* mean that it would be hard to get an AGI to do *those* things, that it was just an *allegory* for other stuff like the strawberry problem. They really seemed to believe, and said over and over again, that we didnât know how to direct a general-purpose AI to do bounded, simple, everyday tasks without it wanting to take over the world. So this should be a big update to people who held that view, even if there are still arguably risks about OOD behavior.\n\nAs someone who worked closely with Eliezer and Nate at the time, including working with Eliezer and Nate on our main write-ups that used the cauldron example, I can say that this is definitely not what we were thinking at the time. Rather:\n\n*   The point was to illustrate a weird gap in the expressiveness and coherence of our theories of rational agency: \"fill a bucket of water\" *seems* like a simple enough task, but it's bizarrely difficult to just write down a simple formal description of an optimization process that predictably does this (without any major side-effects, etc.).\n    *   (We can obviously *stipulate* \"this thing is smart enough to do the thing we want, but too dumb to do anything dangerous\", but the relevant notion of \"smart enough\" is not itself formal; we don't understand optimization well enough to formally define agents that have all the cognitive abilities we want and none of the abilities we don't want.)\n*   The point of emphasizing \"holy shit, this seems so easy and simple and yet we don't see a way to do it!\" wasn't to issue a challenge to capabilities researches to go cobble together a real-world AI that can fill a bucket of water without destroying the world. The point was to emphasize that corrigibility, low-impact problem-solving, 'real' satisficing behavior, etc. *seem conceptually simple*, and yet the *concepts* have no known formalism.\n    *   The hope was that someone would see the simple toy problems and go 'what, no way, this sounds easy', get annoyed/nerdsniped, run off to write some equations on a whiteboard, and come back a week or a year later with a formalism (maybe from some niche mathematical field) that works totally fine for this, and makes it easier to formalize lots of other alignment problems in simplified settings (e.g., with unbounded computation).\n    *   Or failing that, the hope was that someone might at least come up with a clever math hack that solves the immediate 'get the AI to fill the bucket and halt' problem and replaces this dumb-sounding theory question with a slightly deeper theory question.\n*   By using a children's cartoon to illustrate the toy problem, we hoped to make it clearer that the genre here is \"toy problem to illustrate a weird conceptual issue in trying to *define* certain alignment properties\", not \"robotics problem where we show a bunch of photos of factory robots and ask how we can build a good factory robot to refill water receptacles used in industrial applications\".\n\n[Nate's version](https://intelligence.org/2017/04/12/ensuring/) of the talk, which is mostly a more polished version of Eliezer's talk, is careful to liberally sprinkle in tons of qualifications like (emphasis added)\n\n*   \"... for systems that are *sufficiently good* at modeling their environment\",Â \n*   *'if* the system is smart enough to recognize that shutdown will lower its score',\n*   \"Relevant safety measures that *donât assume we can always outthink and outmaneuver the system*...\",\n\n... to make it clearer that the general issue is powerful, strategic optimizers that have high levels of situational awareness, etc., not necessarily 'every system capable enough to fill a bucket of water' (or 'every DL system...').\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1e2262a1ca8b77bc8e847f042862e4d0408803d9822c74c2.png)",
      "plaintextDescription": "> I think the old school MIRI cauldron-filling problem pertained to pretty mundane, everyday tasks. No one said at the time that they didnât really mean that it would be hard to get an AGI to do those things, that it was just an allegory for other stuff like the strawberry problem. They really seemed to believe, and said over and over again, that we didnât know how to direct a general-purpose AI to do bounded, simple, everyday tasks without it wanting to take over the world. So this should be a big update to people who held that view, even if there are still arguably risks about OOD behavior.\n\nAs someone who worked closely with Eliezer and Nate at the time, including working with Eliezer and Nate on our main write-ups that used the cauldron example, I can say that this is definitely not what we were thinking at the time. Rather:\n\n * The point was to illustrate a weird gap in the expressiveness and coherence of our theories of rational agency: \"fill a bucket of water\" seems like a simple enough task, but it's bizarrely difficult to just write down a simple formal description of an optimization process that predictably does this (without any major side-effects, etc.).\n   * (We can obviously stipulate \"this thing is smart enough to do the thing we want, but too dumb to do anything dangerous\", but the relevant notion of \"smart enough\" is not itself formal; we don't understand optimization well enough to formally define agents that have all the cognitive abilities we want and none of the abilities we don't want.)\n * The point of emphasizing \"holy shit, this seems so easy and simple and yet we don't see a way to do it!\" wasn't to issue a challenge to capabilities researches to go cobble together a real-world AI that can fill a bucket of water without destroying the world. The point was to emphasize that corrigibility, low-impact problem-solving, 'real' satisficing behavior, etc. seem conceptually simple, and yet the concepts have no known formalism.\n   * The hope was that"
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2023-10-08T06:15:00.052Z",
    "af": false
  },
  {
    "_id": "tEC5R5ngKksykvviS",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "6wHe7HsytWEeZyqm9",
    "topLevelCommentId": "tq7NyfQ9sivn7YSYo",
    "contents": {
      "markdown": "> I think this provides some support\n\n??? What?? It's fine to say that this is a falsified prediction, but how does \"Eliezer expected less NLP progress pre-ASI\" provide support for \"Eliezer thinks solving NLP is a major part of the alignment problem\"?\n\nI continue to be baffled at the way you're doing exegesis here, happily running with extremely tenuous evidence for P while dismissing contemporary evidence for not-P, and seeming unconcerned about the fact that Eliezer and Nate apparently managed to secretly believe P for many years without ever just saying it outright, and seeming equally unconcerned about the fact that Eliezer and Nate keep saying that your interpretation of what they said is wrong. (Which I also vouch for from having worked with them for ten years, separate from the giant list of specific arguments I've made. Good grief.)\n\n> At the very least, the two claims are consistent.\n\n?? \"Consistent\" is very different from \"supports\"! Every off-topic claim by EY is \"consistent\" with Gallabytes' assertion.",
      "plaintextDescription": "> I think this provides some support\n\n??? What?? It's fine to say that this is a falsified prediction, but how does \"Eliezer expected less NLP progress pre-ASI\" provide support for \"Eliezer thinks solving NLP is a major part of the alignment problem\"?\n\nI continue to be baffled at the way you're doing exegesis here, happily running with extremely tenuous evidence for P while dismissing contemporary evidence for not-P, and seeming unconcerned about the fact that Eliezer and Nate apparently managed to secretly believe P for many years without ever just saying it outright, and seeming equally unconcerned about the fact that Eliezer and Nate keep saying that your interpretation of what they said is wrong. (Which I also vouch for from having worked with them for ten years, separate from the giant list of specific arguments I've made. Good grief.)\n\n> At the very least, the two claims are consistent.\n\n?? \"Consistent\" is very different from \"supports\"! Every off-topic claim by EY is \"consistent\" with Gallabytes' assertion."
    },
    "baseScore": 27,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2023-10-06T07:25:58.420Z",
    "af": false
  },
  {
    "_id": "CwrEpnxZbpNQ9Xfaj",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "zEmLpWnxvdYcYr57G",
    "topLevelCommentId": "zEmLpWnxvdYcYr57G",
    "contents": {
      "markdown": "> using GOFAI methods\n\n\"Nope\" to this part. I otherwise like this comment a lot!",
      "plaintextDescription": "> using GOFAI methods\n\n\"Nope\" to this part. I otherwise like this comment a lot!"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2023-10-06T02:09:03.334Z",
    "af": true
  },
  {
    "_id": "cKeaJKr4efQ5D5hTQ",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "43YEspKRKyeuTExdJ",
    "topLevelCommentId": "nY5jAsdAFSRFn3Zrd",
    "contents": {
      "markdown": "> The main thing I'm claiming is that MIRI said it would be hard to specify (for example, write into a computer) an explicit function that reflects the human value function with high fidelity, in the sense that judgements from this function about the value of outcomes fairly accurately reflect the judgements of ordinary humans. I think this is simply a distinct concept from the idea of getting an AI to *understand* human values.Â \n> \n> The key difference is the transparency and legibility of how the values are represented: if you solve the problem of value specification/value identification, that means you have an actual function that can tell you the value of any outcome. If you get an AI that merely *understands* human values, you can't necessarily use the AI to determine the value of any outcome, because, for example, the AI might lie to you, or simply stay silent.\n\nAh, this is helpful clarification! Thanks. :)\n\nI don't think MIRI ever considered this an important part of the alignment problem, and I don't think we expect humanity to solve lots of the alignment problem as a result of having such a tool; but I think I better understand now why you think this is importantly different from \"AI ever gets good at NLP at all\".\n\n> don't know if your essay is the source of the phrase or whether you just titled it\n\nI think I came up with that particular phrase (though not the idea, of course).",
      "plaintextDescription": "> The main thing I'm claiming is that MIRI said it would be hard to specify (for example, write into a computer) an explicit function that reflects the human value function with high fidelity, in the sense that judgements from this function about the value of outcomes fairly accurately reflect the judgements of ordinary humans. I think this is simply a distinct concept from the idea of getting an AI to understand human values.Â \n> \n> The key difference is the transparency and legibility of how the values are represented: if you solve the problem of value specification/value identification, that means you have an actual function that can tell you the value of any outcome. If you get an AI that merely understands human values, you can't necessarily use the AI to determine the value of any outcome, because, for example, the AI might lie to you, or simply stay silent.\n\nAh, this is helpful clarification! Thanks. :)\n\nI don't think MIRI ever considered this an important part of the alignment problem, and I don't think we expect humanity to solve lots of the alignment problem as a result of having such a tool; but I think I better understand now why you think this is importantly different from \"AI ever gets good at NLP at all\".\n\n> don't know if your essay is the source of the phrase or whether you just titled it\n\nI think I came up with that particular phrase (though not the idea, of course)."
    },
    "baseScore": 13,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2023-10-06T02:03:48.672Z",
    "af": true
  },
  {
    "_id": "mkFzAiHbdfujdu4Xc",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "kpJpCm6ocNrCmz7Dm",
    "topLevelCommentId": "tq7NyfQ9sivn7YSYo",
    "contents": {
      "markdown": "> *   More \"outer alignment\"-like issues being given what seems/seemed to me like outsized focus compared to more \"inner alignment\"-like issues (although there has been a focus on both for as long as I can remember).\n\nIn retrospect I think we should have been more explicit about the importance of inner alignment; I think that we didn't do that in our introduction to corrigibility because it wasn't necessary for illustrating the problem and where we'd run into roadblocks.\n\nMaybe a missing piece here is some explanation of why having a formal understanding of corrigibility might be helpful for actually training corrigibility into a system? (Helpful at all, even if it's not sufficient on its own.)\n\n> *   The attempts to [think of \"tricks\"](https://youtu.be/EUjc1WuyPT8?t=1330) seeming to be focused on real-world optimization-targets to point at, rather than ways of extracting help with alignment somehow / trying to find techniques/paths/tricks for obtaining reliable oracles.\n\nAside from \"concreteness can help make the example easier to think about when you're new to the topic\", part of the explanation here might be \"if the world is solved by AI, we do actually think it will probably be via doing some concrete action in the world (e.g., build nanotech), not via helping with alignment or building a system that only outputs English-language sentence\".\n\n> *   Having utility functions so prominently/commonly be the layer of abstraction that is used[^\\[4\\]^](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument#fnlfn1okkhofq).\n\nI mean, I think utility functions are an extremely useful and basic abstraction. I think it's a lot harder to think about a lot of AI topics without invoking ideas like 'this AI thinks outcome X is better than outcome Y', or 'this AI's preference come with different weights, which can't purely be reduced to what the AI believes'.",
      "plaintextDescription": ">  * More \"outer alignment\"-like issues being given what seems/seemed to me like outsized focus compared to more \"inner alignment\"-like issues (although there has been a focus on both for as long as I can remember).\n\nIn retrospect I think we should have been more explicit about the importance of inner alignment; I think that we didn't do that in our introduction to corrigibility because it wasn't necessary for illustrating the problem and where we'd run into roadblocks.\n\nMaybe a missing piece here is some explanation of why having a formal understanding of corrigibility might be helpful for actually training corrigibility into a system? (Helpful at all, even if it's not sufficient on its own.)\n\n>  * The attempts to think of \"tricks\" seeming to be focused on real-world optimization-targets to point at, rather than ways of extracting help with alignment somehow / trying to find techniques/paths/tricks for obtaining reliable oracles.\n\nAside from \"concreteness can help make the example easier to think about when you're new to the topic\", part of the explanation here might be \"if the world is solved by AI, we do actually think it will probably be via doing some concrete action in the world (e.g., build nanotech), not via helping with alignment or building a system that only outputs English-language sentence\".\n\n>  * Having utility functions so prominently/commonly be the layer of abstraction that is used[4].\n\nI mean, I think utility functions are an extremely useful and basic abstraction. I think it's a lot harder to think about a lot of AI topics without invoking ideas like 'this AI thinks outcome X is better than outcome Y', or 'this AI's preference come with different weights, which can't purely be reduced to what the AI believes'."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2023-10-06T01:41:37.313Z",
    "af": false
  },
  {
    "_id": "9sTNzyMp4Jrx9sB7P",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "WxuXdt3jR6dR9gumw",
    "topLevelCommentId": "tq7NyfQ9sivn7YSYo",
    "contents": {
      "markdown": "Suppose that I'm trying to build a smarter-than-human AI that has a bunch of capabilities (including, e.g., 'be good at Atari games'), and that has the goal 'maximize the amount of diamond in the universe'. It's true that current techniques let you provide *greater than zero* pressure in the direction of 'maximize the amount of diamond in the universe', but there are several important senses in which reality doesn't 'bite back' here:\n\n*   If the AI acquires an *unrelated* goal (e.g., calculate as many digits of pi as possible), and acquires the *belief* 'I will better achieve my true goal if I maximize the amount of diamond' (e.g,, because it infers that its programmer wants that, or just because an SGD-ish process nudged it in the direction of having such a belief), then there's no way in which reality punishes or selects against that AGI (relative to one that actually has the intended goal).\n*   Things that make the AI better at some Atari games, will tend to make it better at other Atari games, but won't tend to make it care more about maximizing diamonds. More generally, things that make AI more capable tend to go together (especially once you get to higher levels of difficulty, generality, non-brittleness, etc.), whereas *none* of them go together with \"terminally value a universe full of diamond\".\n*   If we succeed in *partly* instilling the goal into the AI (e.g., it now likes carbon atoms a lot), then this doesn't provide additional pressure for the AI to internalize the rest of the goal. There's no attractor basin where if you have half of human values, you're under more pressure to acquire the other half. In contrast, if you give AI high levels of capability in half the capabilities, it will tend to want all the rest of the capabilities too; and whatever keeps it from succeeding *on general reasoning and problem-solving* will also tend to keep it from succeeding on the narrow task you're trying to get it to perform. (More so to the extent the task is hard.)\n\n(There are also separate issues, like 'we can't provide a training signal where we thumbs-down the AI destroying the world, because we die in those worlds'.)",
      "plaintextDescription": "Suppose that I'm trying to build a smarter-than-human AI that has a bunch of capabilities (including, e.g., 'be good at Atari games'), and that has the goal 'maximize the amount of diamond in the universe'. It's true that current techniques let you provide greater than zero pressure in the direction of 'maximize the amount of diamond in the universe', but there are several important senses in which reality doesn't 'bite back' here:\n\n * If the AI acquires an unrelated goal (e.g., calculate as many digits of pi as possible), and acquires the belief 'I will better achieve my true goal if I maximize the amount of diamond' (e.g,, because it infers that its programmer wants that, or just because an SGD-ish process nudged it in the direction of having such a belief), then there's no way in which reality punishes or selects against that AGI (relative to one that actually has the intended goal).\n * Things that make the AI better at some Atari games, will tend to make it better at other Atari games, but won't tend to make it care more about maximizing diamonds. More generally, things that make AI more capable tend to go together (especially once you get to higher levels of difficulty, generality, non-brittleness, etc.), whereas none of them go together with \"terminally value a universe full of diamond\".\n * If we succeed in partly instilling the goal into the AI (e.g., it now likes carbon atoms a lot), then this doesn't provide additional pressure for the AI to internalize the rest of the goal. There's no attractor basin where if you have half of human values, you're under more pressure to acquire the other half. In contrast, if you give AI high levels of capability in half the capabilities, it will tend to want all the rest of the capabilities too; and whatever keeps it from succeeding on general reasoning and problem-solving will also tend to keep it from succeeding on the narrow task you're trying to get it to perform. (More so to the extent the task is hard.)\n\n(There are a"
    },
    "baseScore": 18,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2023-10-06T01:20:11.275Z",
    "af": false
  },
  {
    "_id": "nY5jAsdAFSRFn3Zrd",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Nate and Eliezer have already made some of the high-level points I wanted to make, but they haven't replied to a lot of the specific examples and claims in the OP, and I see some extra value in doing that. (Like, if you think Eliezer and Nate are being revisionist in their claims about what past-MIRI thought, then them re-asserting \"no really, we used to believe X!\" is less convincing than my responding in detail to the specific quotes Matt thinks supports his interpretation, while providing [examples of us saying the opposite](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument?commentId=6nvuZekG5PDJB4T3H).)\n\n> *However, I distinctly recall MIRI people making a big deal about the* [*value identification problem*](https://arbital.com/p/value_identification/) *(AKA the value specification problem)*\n\nThe Arbital page for \"value identification problem\" is a three-sentence stub, I'm not exactly sure what the term means on that stub (e.g., whether \"pinpointing valuable outcomes to an advanced agent\" is about pinpointing them in the agent's beliefs or in its goals), and the MIRI website gives me no hits for \"value identification\".\n\nAs for \"value specification\", the main resource where MIRI talks about that is [https://intelligence.org/files/TechnicalAgenda.pdf,](https://intelligence.org/files/TechnicalAgenda.pdf,) where we introduce the problem by saying:\n\n> A highly-reliable, error-tolerant agent design does not guarantee a positive impact; the effects of the system still depend upon whether it is pursuing appropriate goals.\n> \n> A superintelligent system may find clever, unintended ways to achieve the specific goals that it is given. Imagine a superintelligent system designed to cure cancer which does so by stealing resources, proliferating robotic laboratories at the expense of the biosphere, and kidnapping test subjects: the intended goal may have been âcure cancer without doing anything bad,â but such a goal is rooted in cultural context and shared human knowledge.\n> \n> It is not sufficient to construct systems that are smart enough to figure out the intended goals. Human beings, upon learning that natural selection âintendedâ sex to be pleasurable only for purposes of reproduction, do not suddenly decide that contraceptives are abhorrent. While one should not anthropomorphize natural selection, humans are capable of understanding the process which created them while being completely unmotivated to alter their preferences. For similar reasons, when developing AI systems, it is not sufficient to develop a system intelligent enough to figure out the intended goals; the system must also somehow be deliberately constructed to pursue them (Bostrom 2014, chap. 8).\n\nSo I don't think we've ever said that an important subproblem of AI alignment is \"make AI smart enough to figure out what goals humans want\"?\n\n> *for example in* [*this 2016 talk from Yudkowsky*](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)*.*\n> \n> \\[footnote:\\] *More specifically, in* [*the talk*](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)*, at one point Yudkowsky asks \"Why expect that \\[alignment\\] is hard?\" and goes on to tell a fable about programmers misspecifying a utility function, which then gets optimized by an AI with disastrous consequences. My best interpretation of this part of the talk is that he's saying the value identification problem is one of the primary reasons why alignment is hard. However, I encourage you to read* [*the transcript*](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/) *yourself if you are skeptical of my interpretation.*\n\nI don't see him saying anywhere \"the issue is that the AI doesn't understand human goals\". In fact, the fable explicitly treats the AGI as being smart enough to understand English and have reasonable English-language conversations with the programmers:\n\n> With that said: What if programmers build an artificial general intelligence to optimize for smiles? Smiles are good, right? Smiles happen when good things happen.\n> \n> During the development phase of this artificial general intelligence, the only options available to the AI might be that it can produce smiles by making people around it happy and satisfied. The AI appears to be producing beneficial effects upon the world, and it *is* producing beneficial effects upon the world so far.\n> \n> Now the programmers upgrade the code. They add some hardware. The artificial general intelligence gets smarter. It can now evaluate a wider space of policy optionsânot necessarily because it has new motors, new actuators, but because it is now smart enough to forecast the effects of more subtle policies. It says, âI thought of a great way of producing smiles! Can I inject heroin into people?â And the programmers say, âNo! We will add a penalty term to your utility function for administering drugs to people.â And now the AGI appears to be working great again.\n> \n> They further improve the AGI. The AGI realizes that, OK, it doesnât want to add heroin anymore, but it still wants to tamper with your brain so that it expresses extremely high levels of endogenous opiates. Thatâs not heroin, right?\n> \n> It is now also smart enough to model the psychology of the programmers, at least in a very crude fashion, and realize that this is not what the programmers want. If I start taking initial actions that look like itâs heading toward genetically engineering brains to express endogenous opiates, my programmers will edit my utility function. If they edit the utility function of my future self, I will get less of my current utility. (Thatâs one of the convergent instrumental strategies, unless otherwise averted: protect your utility function.) So it keeps its outward behavior reassuring. Maybe the programmers are really excited, because the AGI seems to be getting lots of new moral problems rightâwhatever theyâre doing, itâs working great!\n\nI think the point of the smiles example here isn't \"NLP is hard, so we'd use the proxy of smiles instead, and all the issues of alignment are downstream of this\"; rather, it's that as a rule, superficially nice-seeming goals that work fine when the AI is optimizing weakly (whether or not it's good at NLP at the time) break down when those same goals are optimized very hard. The smiley example makes this obvious because the goal is simple enough that it's easy for us to see what its implications are; far more complex goals *also* tend to break down when optimized hard enough, but this is harder to see because it's harder to see the implications. (Which is why \"smiley\" is used here.)\n\n> *MIRI people frequently claimed that solving the value identification problem would be hard, or at least non-trivial.*[*^\\[6\\]^*](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument#fn9woqm9jgal)*For instance, Nate Soares wrote in his* [*2016 paper on value learning*](https://human%20preferences%20are%20complex%2C%20multi-faceted%2C%20and%20often%20contradictory.%20safely%20extracting%20preferences%20from%20a%20model%20of%20a%20human%20would%20be%20no%20easy%20task./)*, that \"Human preferences are complex, multi-faceted, and often contradictory. Safely extracting preferences from a model of a human would be no easy task.\"*\n\nThat link is broken; the paper is [https://intelligence.org/files/ValueLearningProblem.pdf.](https://intelligence.org/files/ValueLearningProblem.pdf.) The full paragraph here is:\n\n> Human preferences are complex, multi-faceted, and often contradictory. Safely extracting preferences from a model of a human would be no easy task. Problems of ontology identification recur here: the framework for extracting preferences and affecting outcome ratings needs to be robust to drastic changes in the learnerâs model of the operator. The special-case identification of the âoperator modelâ must survive as the system goes from modeling the operator as a simple reward function to modeling the operator as a fuzzy, ever-changing part of reality built out of biological cellsâwhich are made of atoms, which arise from quantum fields.\n\nRevisiting the Ontology Identification section helps clarify what Nate means by \"safely extracting preferences from a model of a human\": IIUC, he's talking about a programmer looking at an AI's brain, identifying the part of the AI's brain that is modeling the human, identifying the part of the AI's brain that is \"the human's preferences\" within that model of a human, and then manually editing the AI's brain to \"hook up\" the model-of-a-human-preference to the AI's goals/motivations, in such a way that the AI *optimizes for* what it models the humans as wanting. (Or some other, less-toy process that amounts to the same thing -- e.g., one assisted by automated interpretability tools.)\n\n> In this toy example, we can assume that the programmers look at the structure of the initial world-model and hard-code a tool for identifying the atoms within. What happens, then, if the system develops a nuclear model of physics, in which the ontology of the universe now contains primitive protons, neutrons, and electrons instead of primitive atoms? The system might fail to identify any carbon atoms in the new world-model, making the system indifferent between all outcomes in the dominant hypothesis. Its actions would then be dominated by any tiny remaining probabilities that it is in a universe where fundamental carbon atoms are hiding somewhere.\n> \n> \\[...\\]\n> \n> To design a system that classifies potential outcomes according to how much diamond is in them, some mechanism is needed for identifying the intended ontology of the training data within the potential outcomes as currently modeled by the AI. This is the *ontology identification problem* introduced by de Blanc \\[2011\\] and further discussed by Soares \\[2015\\].\n> \n> This problem is not a traditional focus of machine learning work. When our only concern is that systems form better world-models, then an argument can be made that the nuts and bolts are less important. As long as the systemâs new world-model better predicts the data than its old world-model, the question of whether diamonds or atoms are âreally representedâ in either model isnât obviously significant. When the system needs to consistently pursue certain outcomes, however, it matters that the systemâs internal dynamics preserve (or improve) its representation of which outcomes are desirable, independent of how helpful its representations are for prediction. The problem of making correct choices is not reducible to the problem of making accurate predictions.\n> \n> Inductive value learning requires the construction of an outcome-classifier from value-labeled training data, but it also requires some method for identifying, inside the states or potential states described in its world-model, the referents of the labels in the training data.\n\nAs Nate and I noted in other comments, the paper repeatedly clarifies that the core issue isn't about whether the AI is good at NLP. Quoting the paper's abstract:\n\n> Even a machine intelligent enough to understand its designersâ intentions would not necessarily *act* as intended.Â \n\nAnd the lede section:\n\n> The novelty here is not that programs can exhibit incorrect or counter-intuitive behavior, but that software agents smart enough to *understand* natural language may still *base their decisions* on misrepresentations of their programmersâ intent. The idea of superintelligent agents monomaniacally pursuing âdumbâ-seeming goals may sound odd, but it follows from the observation of Bostrom and Yudkowsky \\[2014, chap. 7\\] that AI capabilities and goals are logically independent.\\[1\\] Humans can fully comprehend that their âdesignerâ (evolution) had a particular âgoalâ (reproduction) in mind for sex, without thereby feeling compelled to forsake contraception. Instilling oneâs tastes or moral values into an heir isnât impossible, but it also doesnât happen automatically.\n\nBack to your post:\n\n> *And to be clear, I don't mean that GPT-4 merely passively \"understands\" human values. I mean that asking GPT-4 to distinguish valuable and non-valuable outcomes works pretty well at approximating the human value function* in practice\n\nI don't think I understand what difference you have in mind here, or why you think it's important. Doesn't \"this AI understands X\" more-or-less imply \"this AI can successfully distinguish X from not-X in practice\"?\n\n> *This fact is key to what I'm saying because it means that, in the near future, we can literally just query multimodal GPT-N about whether an outcome is bad or good, and use that as an adequate \"human value function\". That wouldn't solve the problem of getting an AI to care about maximizing the human value function, but it would arguably solve the problem of creating an adequate function that we can put into a machine to begin with.*\n\nBut we could already query the human value function by having the AI system query an actual human. What specific problem is meant to be solved by swapping out \"query a human\" for \"query an AI\"?\n\n> *I interpret this passage as saying that 'the problem' is extracting all the judgements that \"you would make\", and putting that into a wish. I think he's implying that these judgements are essentially fully contained in your brain. I don't think it's credible to insist he was referring to a hypothetical ideal human value function that ordinary humans only have limited access to, at least in this essay.*\n\nAbsolutely. But as Eliezer clarified in his [reply](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument?commentId=tq7NyfQ9sivn7YSYo), the issue he was worried about was getting specific complex content into the agent's goals, not getting specific complex content into the agent's beliefs. Which is maybe clearer in the [2011 paper](https://intelligence.org/files/ComplexValues.pdf) where he gave the same example and explicitly said that the issue was the agent's \"utility function\".\n\n> *For example, a straightforward reading of* [*Nate Soares' 2017 talk*](https://intelligence.org/2017/04/12/ensuring/) *supports this interpretation. In the talk, Soares provides a fictional portrayal of value misalignment, drawing from the movie Fantasia. In the story, Mickey Mouse attempts to instruct a magical broom to fill a cauldron, but the broom follows the instructions literally rather than following what Mickey Mouse intended, and floods the room. Soares comments: \"I claim that as fictional depictions of AI go, this is pretty realistic.*\n\nAs I said in another [comment](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument?commentId=fNYquXJ6xpaXZWxyh):\n\n> \"Fill the cauldron\" examples are examples where the cauldron-filler has the wrong utility function, not examples where it has the wrong beliefs. E.g., this is explicit in [https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)Â \n> \n> The idea of the \"fill the cauldron\" examples isn't \"the AI is bad at NLP and therefore doesn't understand what we mean when we say 'fill', 'cauldron', etc.\" It's \"even simple small-scale tasks are unnatural, in the sense that it's hard to define a coherent preference ordering over world-states such that maximizing it completes the task and has no serious negative impact; and there isn't an obvious patch that overcomes the unnaturalness or otherwise makes it predictably easier to aim AI systems at a bounded low-impact task like this\". (Including easier to aim via training.)\n\nIt's true that 'value is relatively complex' is part of why it's hard to get the right goal into an AGI; but it doesn't follow from this that 'AI is able to develop pretty accurate beliefs about our values' helps *get* those complex values into the AGI's goals. (It does provide nonzero evidence about *how* complex value is, but I don't see you arguing that value is very simple in any absolute sense, just that it's simple *enough* for GPT-4 to learn decently well. Which is not reassuring, because GPT-4 is able to learn a lot of very complicated things, so this doesn't do much to bound the complexity of human value.)\n\nIn any case, I take this confusion as evidence that the fill-the-cauldron example might not be very useful. Or maybe all these examples just need to explicitly specify, going forward, that the AI is part-human at understanding English.\n\n> *Perhaps more important to my point, Soares presented a clean separation between the part where we specify an AI's objectives, and the part where the AI tries to maximizes those objectives. He draws two arrows, indicating that MIRI is concerned about both parts.*\n> \n> ![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/i5kijcjFJD6bn7dwq/d4ujynglutkacpzglvcr)\n\nYour image isn't displaying for me, but I assume it's this one?\n\n![vl-argmax.png](https://intelligence.org/wp-content/uploads/2017/04/vl-argmax.png)\n\nI don't know what you mean by \"specify an AI's objectives\" here, but the specific term Nate uses here is \"value learning\" (not \"value specification\" or \"value identification\"). And Nate's [Value Learning Problem](https://intelligence.org/files/ValueLearningProblem.pdf) paper, as I noted above, explicitly disclaims that 'get the AI to be smart enough to output reasonable-sounding moral judgments' is a core part of the problem.\n\n> *He states, \"The serious question with smarter-than-human AI is how we can ensure that the objectives weâve specified are correct, and how we can minimize costly accidents and unintended consequences in cases of misspecification.\" I believe this quote refers directly to the value identification problem, rather than the problem of getting an AI to care about following the goals we've given it.*\n\nThe way you quoted this makes it sound like a gloss on the image, but it's actually a quote from the very start of the talk:\n\n> The notion of AI systems âbreaking freeâ of the shackles of their source code or spontaneously developing human-like desires is just confused. The AI system *is* its source code, and its actions will only ever follow from the execution of the instructions that we initiate. The CPU just keeps on executing the next instruction in the program register. We could write a program that manipulates its own code, including coded objectives. Even then, though, the manipulations that it makes are made as a result of executing the original code that we wrote; they do not stem from some kind of ghost in the machine.\n> \n> The serious question with smarter-than-human AI is how we can ensure that the objectives weâve specified are correct, and how we can minimize costly accidents and unintended consequences in cases of misspecification. As Stuart Russell (co-author of *Artificial Intelligence: A Modern Approach*) [puts it](https://www.edge.org/conversation/the-myth-of-ai#26015):\n> \n> > The primary concern is not spooky emergent consciousness but simply the ability to make *high-quality decisions*. Here, quality refers to the expected outcome utility of actions taken, where the utility function is, presumably, specified by the human designer. Now we have a problem:\n> > \n> > 1\\. The utility function may not be perfectly aligned with the values of the human race, which are (at best) very difficult to pin down.\n> > \n> > 2\\. Any sufficiently capable intelligent system will prefer to ensure its own continued existence and to acquire physical and computational resources â not for their own sake, but to succeed in its assigned task. \\[...\\]\n\nI wouldn't read too much into the word choice here, since I think it's just trying to introduce the Russell quote, which is (again) explicitly about getting content into the AI's goals, not about getting content into the AI's beliefs.\n\n(In general, I think the phrase \"value specification\" is sort of confusingly vague. I'm not sure what the best replacement is for it -- maybe just \"value loading\", following Bostrom? -- but I suspect MIRI's usage of it has been needlessly confusing. Back in 2014, we reluctantly settled on it as jargon for \"the part of the alignment problem that isn't subsumed in getting the AI to reliably maximize diamonds\", because this struck us as a smallish but nontrivial part of the problem; but I think it's easy to read the term as referring to something a lot more narrow.)\n\n> *The point of \"*[*the genie knows but doesn't care*](https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care)*\" wasn't that the AI would take your instructions, know what you want, and yet disobey the instructions because it doesn't care about what you asked for. If you read* [*Rob Bensinger's essay*](https://www.lesswrong.com/posts/NyFuuKQ8uCEDtd2du/the-genie-knows-but-doesn-t-care) *carefully, you'll find that he's actually warning that the AI will care too much about the utility function you gave it, and maximize it exactly, against your intentions*[*^\\[10\\]^*](https://www.lesswrong.com/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument#fnuahq0ks4gl)*.*\n\nYep -- I think I'd have endorsed claims like \"by default, a baby AGI won't share your values even if it understands them\" at the time, but IIRC the essay doesn't make that point explicitly, and some of the points it does make seem either false (wait, we're going to be able to hand AGI a hand-written utility function? that's somehow tractable?) or confusingly written. (Like, if my point was '*even if* you could hand-write a utility function, this fails at point X', I should have made that 'even if' louder.)\n\nSome MIRI staff liked that essay at the time, so I don't think it's useless, but it's not the best evidence: I wrote it not long after I first started learning about this whole 'superintelligence risk' thing, and I posted it before I'd ever worked at MIRI.",
      "plaintextDescription": "Nate and Eliezer have already made some of the high-level points I wanted to make, but they haven't replied to a lot of the specific examples and claims in the OP, and I see some extra value in doing that. (Like, if you think Eliezer and Nate are being revisionist in their claims about what past-MIRI thought, then them re-asserting \"no really, we used to believe X!\" is less convincing than my responding in detail to the specific quotes Matt thinks supports his interpretation, while providing examples of us saying the opposite.)\n\n> However, I distinctly recall MIRI people making a big deal about the value identification problem (AKA the value specification problem)\n\nThe Arbital page for \"value identification problem\" is a three-sentence stub, I'm not exactly sure what the term means on that stub (e.g., whether \"pinpointing valuable outcomes to an advanced agent\" is about pinpointing them in the agent's beliefs or in its goals), and the MIRI website gives me no hits for \"value identification\".\n\nAs for \"value specification\", the main resource where MIRI talks about that is https://intelligence.org/files/TechnicalAgenda.pdf, where we introduce the problem by saying:\n\n> A highly-reliable, error-tolerant agent design does not guarantee a positive impact; the effects of the system still depend upon whether it is pursuing appropriate goals.\n> \n> A superintelligent system may find clever, unintended ways to achieve the specific goals that it is given. Imagine a superintelligent system designed to cure cancer which does so by stealing resources, proliferating robotic laboratories at the expense of the biosphere, and kidnapping test subjects: the intended goal may have been âcure cancer without doing anything bad,â but such a goal is rooted in cultural context and shared human knowledge.\n> \n> It is not sufficient to construct systems that are smart enough to figure out the intended goals. Human beings, upon learning that natural selection âintendedâ sex to be pleasurable only "
    },
    "baseScore": 36,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2023-10-06T00:54:14.296Z",
    "af": true
  },
  {
    "_id": "fNYquXJ6xpaXZWxyh",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "NyG6t5DwdBShfojes",
    "topLevelCommentId": "tq7NyfQ9sivn7YSYo",
    "contents": {
      "markdown": "\"Fill the cauldron\" examples are examples where the cauldron-filler has the wrong utility function, not examples where it has the wrong beliefs. E.g., this is explicit in [https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/](https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/)Â \n\nThe idea of the \"fill the cauldron\" examples isn't \"the AI is bad at NLP and therefore doesn't understand what we mean when we say 'fill', 'cauldron', etc.\" It's \"even simple small-scale tasks are unnatural, in the sense that it's hard to define a coherent preference ordering over world-states such that maximizing it completes the task and has no serious negative impact; and there isn't an obvious patch that overcomes the unnaturalness or otherwise makes it predictably easier to aim AI systems at a bounded low-impact task like this\". (Including easier to aim via training.)",
      "plaintextDescription": "\"Fill the cauldron\" examples are examples where the cauldron-filler has the wrong utility function, not examples where it has the wrong beliefs. E.g., this is explicit in https://intelligence.org/2016/12/28/ai-alignment-why-its-hard-and-where-to-start/Â \n\nThe idea of the \"fill the cauldron\" examples isn't \"the AI is bad at NLP and therefore doesn't understand what we mean when we say 'fill', 'cauldron', etc.\" It's \"even simple small-scale tasks are unnatural, in the sense that it's hard to define a coherent preference ordering over world-states such that maximizing it completes the task and has no serious negative impact; and there isn't an obvious patch that overcomes the unnaturalness or otherwise makes it predictably easier to aim AI systems at a bounded low-impact task like this\". (Including easier to aim via training.)"
    },
    "baseScore": 14,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2023-10-05T23:04:07.138Z",
    "af": false
  },
  {
    "_id": "S2bDgtQeZahTohFLw",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "E82YzXxvS6nBdCAYc",
    "topLevelCommentId": "tq7NyfQ9sivn7YSYo",
    "contents": {
      "markdown": "> Straw-EY: Complexity of value means you can't just get the make-AI-care part to happen by chance; it's a small target.\n> \n> Straw-MB: Ok but now we have a very short message pointing to roughly human values: just have a piece of code that says \"and now call GPT and ask it what's good\". So now it's a very small number of bits.\n\nTo which I say: \"dial a random phone number and ask the person who answers what's good\" can also be implemented with a small number of bits. In order for GPT-4 to be a major optimistic update about alignment, we need some specific way to leverage GPT-4 to crack open part of the alignment problem, even though we presumably agree that phone-a-friend doesn't crack open part of the alignment problem. (Nor does phone-your-neighborhood-moral-philosopher, or phone-Paul-Christiano.)",
      "plaintextDescription": "> Straw-EY: Complexity of value means you can't just get the make-AI-care part to happen by chance; it's a small target.\n> \n> Straw-MB: Ok but now we have a very short message pointing to roughly human values: just have a piece of code that says \"and now call GPT and ask it what's good\". So now it's a very small number of bits.\n\nTo which I say: \"dial a random phone number and ask the person who answers what's good\" can also be implemented with a small number of bits. In order for GPT-4 to be a major optimistic update about alignment, we need some specific way to leverage GPT-4 to crack open part of the alignment problem, even though we presumably agree that phone-a-friend doesn't crack open part of the alignment problem. (Nor does phone-your-neighborhood-moral-philosopher, or phone-Paul-Christiano.)"
    },
    "baseScore": 25,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2023-10-05T22:46:11.842Z",
    "af": true
  },
  {
    "_id": "bHSDoz8zpnfwAZg6c",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "ReHNvcmhzvJYY7EdZ",
    "topLevelCommentId": "tq7NyfQ9sivn7YSYo",
    "contents": {
      "markdown": "> Why would we expect the first thing to be so hard compared to the second thing?\n\nIn large part because reality \"bites back\" when an AI has false beliefs, whereas it doesn't bite back when an AI has the wrong preferences. Deeply understanding human psychology (including our morality), astrophysics, biochemistry, economics, etc. requires reasoning well, and if you have a defect of reasoning that makes it hard for you to learn about one of those domains from the data, then it's likely that you'll have large defects of reasoning in other domains as well.\n\nThe same isn't true for terminally valuing human welfare; being less moral doesn't necessarily mean that you'll be any worse at making astrophysics predictions, or economics predictions, etc. So preferences need to be specified \"directly\", in a targeted way, rather than coming for free with sufficiently good performance on any of a wide variety of simple metrics.\n\n> If getting a model to *understand* preferences is not difficult, then the issue doesn't have to do with the *complexity* of values.\n\nThis definitely doesn't follow. This shows that complexity *alone* isn't the issue, which it's not; but *given* that reality bites back for beliefs but not for preferences, the complexity of value serves as a *multiplier* on the difficulty of instilling the right preferences.\n\nAnother way of putting the point: in order to get a maximally good model of the world's macroeconomic state into an AGI, you don't just hand the AGI a long list of macroeconomic facts and then try to get it to regurgitate those same facts. Rather, you try to give it some ability to draw good inferences, seek out new information, make predictions, etc.\n\nYou try to get something relatively *low-complexity* into the AI (something like \"good reasoning heuristics\" plus \"enough basic knowledge to get started\"), and then let it *figure out* the higher-complexity thing (\"the world's macroeconomic state\"). Similar to how human brains don't work via \"evolution built all the facts we'd need to know into our brain at birth\".\n\nIf you were instead trying to get the AI to *value* some complex macroeconomic state, then you wouldn't be able to use the shortcut \"just make it good at reasoning and teach it a few basic facts\", because that doesn't actually suffice for terminally valuing any particular thing.\n\n> It would have to be because our values are inferior to the set of values it wishes to have instead, from its own perspective.Â \n\nThis is true for preference orderings in general. If agent A and agent B have two different preference orderings, then as a rule A will think B's preference ordering is worse than A's. (And vice versa.)\n\n(\"Worse\" in the sense that, e.g., A would not take a pill to self-modify to have B's preferences, and A *would* want B to have A's preferences. This is not true for all preference orderings -- e.g., A might have self-referential preferences like \"*I* eat all the jelly beans\", or other-referential preferences like \"B gets to keep its values unchanged\", or self-undermining preferences like \"A changes its preferences to better match B's preferences\". But it's true as a rule.)\n\n> This is kind of similar to moral realism, but in which morality is understood better by superintelligent agents than we do, and that super-morality appears to dictate things that appear to be extremely wrong from our current perspective (like killing us all).Â \n\nNope, you don't need to endorse any version of moral realism in order to get the \"preference orderings tend to endorse themselves and disendorse other preference orderings\" consequence. The idea isn't that ASI would develop an \"inherently better\" or \"inherently smarter\" set of preferences, compared to human preferences. It's just that the ASI would (as a strong default, because getting a complex preference into an ASI is hard) end up with *different* preferences than a human, and different preferences than we'd likely want.\n\n> In a nutshell, if we really seem to want certain values, then those values probably have strong \"proofs\" for why those are \"good\" or more probable values for an agent to have and-or eventually acquire on their own, it just may be the case that we haven't *yet* discovered the proofs for those values.Â \n\nWhy do you think this? To my eye, the world looks as you'd expect if human values were a happenstance product of evolution operating on specific populations in a specific environment.\n\nI don't observe the fact that I like vanilla ice cream and infer that all sufficiently-advanced alien species will converge on liking vanilla ice cream too.",
      "plaintextDescription": "> Why would we expect the first thing to be so hard compared to the second thing?\n\nIn large part because reality \"bites back\" when an AI has false beliefs, whereas it doesn't bite back when an AI has the wrong preferences. Deeply understanding human psychology (including our morality), astrophysics, biochemistry, economics, etc. requires reasoning well, and if you have a defect of reasoning that makes it hard for you to learn about one of those domains from the data, then it's likely that you'll have large defects of reasoning in other domains as well.\n\nThe same isn't true for terminally valuing human welfare; being less moral doesn't necessarily mean that you'll be any worse at making astrophysics predictions, or economics predictions, etc. So preferences need to be specified \"directly\", in a targeted way, rather than coming for free with sufficiently good performance on any of a wide variety of simple metrics.\n\n> If getting a model to understand preferences is not difficult, then the issue doesn't have to do with the complexity of values.\n\nThis definitely doesn't follow. This shows that complexity alone isn't the issue, which it's not; but given that reality bites back for beliefs but not for preferences, the complexity of value serves as a multiplier on the difficulty of instilling the right preferences.\n\nAnother way of putting the point: in order to get a maximally good model of the world's macroeconomic state into an AGI, you don't just hand the AGI a long list of macroeconomic facts and then try to get it to regurgitate those same facts. Rather, you try to give it some ability to draw good inferences, seek out new information, make predictions, etc.\n\nYou try to get something relatively low-complexity into the AI (something like \"good reasoning heuristics\" plus \"enough basic knowledge to get started\"), and then let it figure out the higher-complexity thing (\"the world's macroeconomic state\"). Similar to how human brains don't work via \"evolution built all the f"
    },
    "baseScore": 50,
    "voteCount": 18,
    "createdAt": null,
    "postedAt": "2023-10-05T22:39:49.031Z",
    "af": false
  },
  {
    "_id": "ykhcqgkkdGJJYnt22",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "MQYdnHatZ3mq9LqXC",
    "topLevelCommentId": "tq7NyfQ9sivn7YSYo",
    "contents": {
      "markdown": "I appreciate the example!\n\nAre you claiming that this example solves \"a major part of the problem\" of alignment? Or that, e.g., this plus four other easy ideas solve a major part of the problem of alignment?\n\nExamples like the Visible Thoughts Project show that MIRI has been interested in research directions that leverage recent NLP progress to try to make inroads on alignment. But Matthew's claim seems to be 'systems like GPT-4 are grounds for being a lot more optimistic about alignment', and your claim is that systems like these solve \"a major part of the problem\". Which is different from thinking 'NLP opens up some new directions for research that have a nontrivial chance of being at least a tiny bit useful, but doesn't crack open the problem in any major way'.\n\nIt's not a coincidence that MIRI has historically worked on problems related to AGI analyzability / understandability / interpretability, rather than working on NLP or machine ethics. We've pretty consistently said that:\n\n*   The main problems lie in 'we can safely and reliably aim ASI at a specific goal at all'.\n*   The problem of going from 'we can aim the AI at a goal at all' to 'we can aim the AI at the right goal (e.g., corrigibly inventing nanotech)' is a smaller but nontrivial additional step.\n\n... Whereas I don't think we've ever suggested that good NLP AI would take a major bite out of either of those problems. The latter problem isn't equivalent to (or an obvious result of) 'get the AI to understand corrigibility and nanotech', or for that matter 'get the AI to understand human preferences in general'.",
      "plaintextDescription": "I appreciate the example!\n\nAre you claiming that this example solves \"a major part of the problem\" of alignment? Or that, e.g., this plus four other easy ideas solve a major part of the problem of alignment?\n\nExamples like the Visible Thoughts Project show that MIRI has been interested in research directions that leverage recent NLP progress to try to make inroads on alignment. But Matthew's claim seems to be 'systems like GPT-4 are grounds for being a lot more optimistic about alignment', and your claim is that systems like these solve \"a major part of the problem\". Which is different from thinking 'NLP opens up some new directions for research that have a nontrivial chance of being at least a tiny bit useful, but doesn't crack open the problem in any major way'.\n\nIt's not a coincidence that MIRI has historically worked on problems related to AGI analyzability / understandability / interpretability, rather than working on NLP or machine ethics. We've pretty consistently said that:\n\n * The main problems lie in 'we can safely and reliably aim ASI at a specific goal at all'.\n * The problem of going from 'we can aim the AI at a goal at all' to 'we can aim the AI at the right goal (e.g., corrigibly inventing nanotech)' is a smaller but nontrivial additional step.\n\n... Whereas I don't think we've ever suggested that good NLP AI would take a major bite out of either of those problems. The latter problem isn't equivalent to (or an obvious result of) 'get the AI to understand corrigibility and nanotech', or for that matter 'get the AI to understand human preferences in general'."
    },
    "baseScore": 17,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2023-10-05T22:03:52.428Z",
    "af": false
  },
  {
    "_id": "yXHTxBT5aatpcdvvZ",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "Q7fEoBLJTb2cF83hu",
    "topLevelCommentId": "tq7NyfQ9sivn7YSYo",
    "contents": {
      "markdown": "> Historically you very clearly thought that a major part of the problem is that AIs would not understand human concepts and preferences until after or possibly very slightly before achieving superintelligence. This is not how it seems to have gone.\n\n\"You very clearly thought that was a major part of the problem\" implies that if you could go to Eliezer-2008 and convince him \"we're going to solve a lot of NLP a bunch of years before we get to ASI\", he would respond with some version of \"oh great, that solves a major part of the problem!\". Which I'm pretty sure is false.\n\nIn order for GPT-4 (or GPT-2) to be a major optimistic update about alignment, there needs to be a way to leverage \"really good NLP\" to help with alignment. I think the crux of disagreement is that you think really-good-NLP is obviously super helpful for alignment and should be a big positive update, and Eliezer and Nate and I disagree.\n\nMaybe a good starting point would be for you to give examples of concrete ways you expect really good NLP to put humanity in a better position to wield superintelligence, e.g., if superintelligence is 8 years away?\n\n(Or say some other update we should be making on the basis of \"really good NLP today\", like \"therefore we'll probably unlock this other capability X well before ASI, and X likely makes alignment a lot easier via concrete pathway Y\".)",
      "plaintextDescription": "> Historically you very clearly thought that a major part of the problem is that AIs would not understand human concepts and preferences until after or possibly very slightly before achieving superintelligence. This is not how it seems to have gone.\n\n\"You very clearly thought that was a major part of the problem\" implies that if you could go to Eliezer-2008 and convince him \"we're going to solve a lot of NLP a bunch of years before we get to ASI\", he would respond with some version of \"oh great, that solves a major part of the problem!\". Which I'm pretty sure is false.\n\nIn order for GPT-4 (or GPT-2) to be a major optimistic update about alignment, there needs to be a way to leverage \"really good NLP\" to help with alignment. I think the crux of disagreement is that you think really-good-NLP is obviously super helpful for alignment and should be a big positive update, and Eliezer and Nate and I disagree.\n\nMaybe a good starting point would be for you to give examples of concrete ways you expect really good NLP to put humanity in a better position to wield superintelligence, e.g., if superintelligence is 8 years away?\n\n(Or say some other update we should be making on the basis of \"really good NLP today\", like \"therefore we'll probably unlock this other capability X well before ASI, and X likely makes alignment a lot easier via concrete pathway Y\".)"
    },
    "baseScore": 14,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2023-10-05T21:22:36.427Z",
    "af": true
  },
  {
    "_id": "6nvuZekG5PDJB4T3H",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "tq7NyfQ9sivn7YSYo",
    "topLevelCommentId": "tq7NyfQ9sivn7YSYo",
    "contents": {
      "markdown": "> But if you had asked us back then if a superintelligence would automatically be very good at predicting human text outputs, I guarantee we would have said yes. \\[...\\] I wish that all of these past conversations were archived to a common place, so that I could search and show you many pieces of text which would talk about this critical divide between prediction and preference (as I would now term it) and how I did in fact expect superintelligences to be able to predict things!\n\nQuoting myself [in April](https://twitter.com/robbensinger/status/1651142409571676160):\n\n> \"MIRI's argument for AI risk depended on AIs being bad at natural language\" is a weirdly common misunderstanding, given how often we said the opposite going back 15+ years.\n> \n> E.g., Nate Soares in 2016: [https://intelligence.org/files/ValueLearningProblem.pdf](https://intelligence.org/files/ValueLearningProblem.pdf)\n> \n> ![Image](https://pbs.twimg.com/media/FuoDfGBaMAAhwMs?format=png&name=900x900)\n> \n> Or Eliezer Yudkowsky in 2008, critiquing his own circa-1997 view \"sufficiently smart AI will understand morality, and therefore will be moral\": [https://www.lesswrong.com/s/SXurf2mWFw8LX2mkG/p/CcBe9aCKDgT5FSoty](https://www.lesswrong.com/s/SXurf2mWFw8LX2mkG/p/CcBe9aCKDgT5FSoty)Â \n> \n> ![Image](https://pbs.twimg.com/media/FuoI9KvaUAAJolp?format=jpg&name=large)\n> \n> (The response being, in short: \"Understanding morality doesn't mean that you're motivated to follow it.\")\n> \n> It was claimed by @perrymetzger that [https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes](https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes) makes a load-bearing \"AI is bad at NLP\" assumption.\n> \n> But the same example in [https://intelligence.org/files/ComplexValues.pdf](https://intelligence.org/files/ComplexValues.pdf) (2011) explicitly says that the challenge is to get the right content into a utility function, not into a world-model:\n> \n> ![Image](https://pbs.twimg.com/media/FuoKWfTaMAIbqps?format=jpg&name=medium)\n> \n> The example does build in the assumption \"this outcome pump is bad at NLP\", but this isn't a load-bearing assumption. If the outcome pump were instead a good conversationalist (or hooked up to one), you would still need to get the right content into its goals.\n> \n> It's true that Eliezer and I didn't predict AI would achieve GPT-3 or GPT-4 levels of NLP ability so early (e.g., before it can match humans in general science ability), so this is an update to some of our models of AI.\n> \n> But the specific update \"AI is good at NLP, therefore alignment is easy\" requires that there be an old belief like \"a big part of why alignment looks hard is that we're so bad at NLP\".\n> \n> It should be easy to find someone at MIRI like Eliezer or Nate saying that in the last 20 years if that was ever a belief here. Absent that, an obvious explanation for why we never just said that is that we didn't believe it!\n> \n> Found another example: MIRI's first technical research agenda, in 2014, went out of its way to clarify that the problem isn't \"AI is bad at NLP\".\n> \n> ![Image](https://pbs.twimg.com/media/FuoSpd8aMAEziq8?format=png&name=small)",
      "plaintextDescription": "> But if you had asked us back then if a superintelligence would automatically be very good at predicting human text outputs, I guarantee we would have said yes. [...] I wish that all of these past conversations were archived to a common place, so that I could search and show you many pieces of text which would talk about this critical divide between prediction and preference (as I would now term it) and how I did in fact expect superintelligences to be able to predict things!\n\nQuoting myself in April:\n\n> \"MIRI's argument for AI risk depended on AIs being bad at natural language\" is a weirdly common misunderstanding, given how often we said the opposite going back 15+ years.\n> \n> E.g., Nate Soares in 2016: https://intelligence.org/files/ValueLearningProblem.pdf\n> \n> Or Eliezer Yudkowsky in 2008, critiquing his own circa-1997 view \"sufficiently smart AI will understand morality, and therefore will be moral\": https://www.lesswrong.com/s/SXurf2mWFw8LX2mkG/p/CcBe9aCKDgT5FSotyÂ \n> \n> (The response being, in short: \"Understanding morality doesn't mean that you're motivated to follow it.\")\n> \n> It was claimed by @perrymetzger that https://www.lesswrong.com/posts/4ARaTpNX62uaL86j6/the-hidden-complexity-of-wishes makes a load-bearing \"AI is bad at NLP\" assumption.\n> \n> But the same example in https://intelligence.org/files/ComplexValues.pdf (2011) explicitly says that the challenge is to get the right content into a utility function, not into a world-model:\n> \n> The example does build in the assumption \"this outcome pump is bad at NLP\", but this isn't a load-bearing assumption. If the outcome pump were instead a good conversationalist (or hooked up to one), you would still need to get the right content into its goals.\n> \n> It's true that Eliezer and I didn't predict AI would achieve GPT-3 or GPT-4 levels of NLP ability so early (e.g., before it can match humans in general science ability), so this is an update to some of our models of AI.\n> \n> But the specific update \"AI is g"
    },
    "baseScore": 24,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2023-10-05T21:13:27.801Z",
    "af": true
  },
  {
    "_id": "v9MeirSzoDNynRsTc",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "8SMfyHd2SwSLFG3ge",
    "topLevelCommentId": "SfhLu5FQfbJXYvsHp",
    "contents": {
      "markdown": "> That makes sense, but I say in the post that I think we will likely have a solution to the value identification problem that's \"about as good as human judgement\" in the near future.\n\nWe already have humans who are smart enough to do par-human moral reasoning. For \"AI can do par-human moral reasoning\" to help solve the alignment problem, there needs to be some additional benefit to having AI systems that can match a human (e.g., some benefit to our being able to produce enormous numbers of novel moral judgments without relying on an existing text corpus or hiring thousands of humans to produce them). Do you have some benefit in mind?",
      "plaintextDescription": "> That makes sense, but I say in the post that I think we will likely have a solution to the value identification problem that's \"about as good as human judgement\" in the near future.\n\nWe already have humans who are smart enough to do par-human moral reasoning. For \"AI can do par-human moral reasoning\" to help solve the alignment problem, there needs to be some additional benefit to having AI systems that can match a human (e.g., some benefit to our being able to produce enormous numbers of novel moral judgments without relying on an existing text corpus or hiring thousands of humans to produce them). Do you have some benefit in mind?"
    },
    "baseScore": 10,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2023-10-05T20:53:08.929Z",
    "af": true
  },
  {
    "_id": "F7cT3hDy9rKGmD8Fr",
    "postId": "i5kijcjFJD6bn7dwq",
    "parentCommentId": "SfhLu5FQfbJXYvsHp",
    "topLevelCommentId": "SfhLu5FQfbJXYvsHp",
    "contents": {
      "markdown": "> Basically, I think your later section--\"Maybe you think\"--is pointing in the right direction, and requiring a much higher standard than human-level at moral judgment is reasonable and consistent with the explicit standard set by essays by Yudkowsky and other MIRI people. CEV was about this; talk about philosophical competence or [metaphilosophy](https://www.lesswrong.com/posts/MAhueZtNz5SnDPhsy/metaphilosophical-mysteries) was about this. \"Philosophy with a deadline\" would be a weird way to put it if you thought contemporary philosophy was good enough.\n\nI don't think this is the crux. E.g., I'd wager the number of bits you need to get into an ASI's goals in order to make it corrigible is quite a bit smaller than the number of bits required to make an ASI behave like a trustworthy human, which in turn is *way way* smaller than the number of bits required to make an ASI implement CEV.\n\nThe issue is that (a) the absolute number of bits for each of these things is still very large, (b) insofar as we're training for deep competence and efficiency we're training against corrigibility (which makes it hard to hit both targets at once), and (c) we can't safely or efficiently provide good training data for a lot of the things we care about (e.g., 'if you're a superintelligence operating in a realistic-looking environment, don't do *any* of the things that destroy the world').\n\nNone of these points require that we (or the AI) solve novel moral philosophy problems. I'd be satisfied with an AI that corrigibly built scanning tech and efficient computing hardware for whole-brain emulation, then shut itself down; the AI plausibly doesn't even need to think about any of the world outside of a particular room, much less solve tricky questions of population ethics or whatever.",
      "plaintextDescription": "> Basically, I think your later section--\"Maybe you think\"--is pointing in the right direction, and requiring a much higher standard than human-level at moral judgment is reasonable and consistent with the explicit standard set by essays by Yudkowsky and other MIRI people. CEV was about this; talk about philosophical competence or metaphilosophy was about this. \"Philosophy with a deadline\" would be a weird way to put it if you thought contemporary philosophy was good enough.\n\nI don't think this is the crux. E.g., I'd wager the number of bits you need to get into an ASI's goals in order to make it corrigible is quite a bit smaller than the number of bits required to make an ASI behave like a trustworthy human, which in turn is way way smaller than the number of bits required to make an ASI implement CEV.\n\nThe issue is that (a) the absolute number of bits for each of these things is still very large, (b) insofar as we're training for deep competence and efficiency we're training against corrigibility (which makes it hard to hit both targets at once), and (c) we can't safely or efficiently provide good training data for a lot of the things we care about (e.g., 'if you're a superintelligence operating in a realistic-looking environment, don't do any of the things that destroy the world').\n\nNone of these points require that we (or the AI) solve novel moral philosophy problems. I'd be satisfied with an AI that corrigibly built scanning tech and efficient computing hardware for whole-brain emulation, then shut itself down; the AI plausibly doesn't even need to think about any of the world outside of a particular room, much less solve tricky questions of population ethics or whatever."
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2023-10-05T20:50:14.164Z",
    "af": true
  },
  {
    "_id": "jqcpfCSumKb2NBQtb",
    "postId": "aW288uWABwTruBmgF",
    "parentCommentId": "mheNBJrHdrncXpcSa",
    "topLevelCommentId": "yeZzfs58LYPetqK4G",
    "contents": {
      "markdown": "I agreed based on how AI safety Twitter looked to me a year ago vs. today, not based on discussion here.",
      "plaintextDescription": "I agreed based on how AI safety Twitter looked to me a year ago vs. today, not based on discussion here."
    },
    "baseScore": 10,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2023-10-02T00:43:33.797Z",
    "af": false
  },
  {
    "_id": "t5hudej7W2DbBRa8P",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "dz4okmaWLQvSM4JFD",
    "topLevelCommentId": "KMf4NrDH5GXizgFqc",
    "contents": {
      "markdown": "> If [@Rob Bensinger](https://www.lesswrong.com/users/robbbb?mention=user) does in fact cross-post Linda's comment, I request he cross-posts this, too.\n\nI was going to ask if I could!\n\nI understand if people don't want to talk about it, but I do feel sad that there isn't some kind of public accounting of what happened there.\n\n(Well, I don't concretely understand why people don't want to talk about it, but I can think of possibilities!)",
      "plaintextDescription": "> If @Rob Bensinger does in fact cross-post Linda's comment, I request he cross-posts this, too.\n\nI was going to ask if I could!\n\nI understand if people don't want to talk about it, but I do feel sad that there isn't some kind of public accounting of what happened there.\n\n(Well, I don't concretely understand why people don't want to talk about it, but I can think of possibilities!)"
    },
    "baseScore": 6,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2023-09-13T02:31:08.903Z",
    "af": false
  },
  {
    "_id": "2ZqDfX3zpmpdH9qZe",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "z7JQbwkiSzEnziJn2",
    "topLevelCommentId": "iydEd9e8PrxCgE7Ah",
    "contents": {
      "markdown": "This is formally correct.\n\n(Though one of those updates might be a lot smaller than the other, if you've e.g. already thought about one of those topics a lot and reached a confident conclusion.)",
      "plaintextDescription": "This is formally correct.\n\n(Though one of those updates might be a lot smaller than the other, if you've e.g. already thought about one of those topics a lot and reached a confident conclusion.)"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2023-09-13T01:15:08.227Z",
    "af": false
  },
  {
    "_id": "GyoAHPEgCjQBtdpPk",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "FwBo3TQrhAeiL6mpb",
    "topLevelCommentId": "KMf4NrDH5GXizgFqc",
    "contents": {
      "markdown": "Can I cross-post this to the EA Forum? (Or you can do it, if you prefer; but I think this is a really useful comment.)",
      "plaintextDescription": "Can I cross-post this to the EA Forum? (Or you can do it, if you prefer; but I think this is a really useful comment.)"
    },
    "baseScore": 2,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2023-09-13T00:54:00.188Z",
    "af": false
  },
  {
    "_id": "T2GkHos529qvapFfz",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "wwLQvxg3Qtcn7BFfB",
    "topLevelCommentId": "PTw7BugBAFQBKNQRk",
    "contents": {
      "markdown": "(But insofar as you continue to be unsure about Ben, yes, you should be open to the possibility that Emerson has hidden information that justifies Emerson thinking Ben is being super dishonest. My confidence re \"no hidden information like that\" is downstream of my beliefs about Ben's character.)",
      "plaintextDescription": "(But insofar as you continue to be unsure about Ben, yes, you should be open to the possibility that Emerson has hidden information that justifies Emerson thinking Ben is being super dishonest. My confidence re \"no hidden information like that\" is downstream of my beliefs about Ben's character.)"
    },
    "baseScore": 3,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2023-09-08T17:37:42.050Z",
    "af": false
  },
  {
    "_id": "wwLQvxg3Qtcn7BFfB",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "qqvMLpCLPeSjHwq8k",
    "topLevelCommentId": "PTw7BugBAFQBKNQRk",
    "contents": {
      "markdown": "> Why do you think that's obvious?\n\nI know Ben, I've conversed with him a number of times in the past and seen lots of his LW comments, and I have a very strong and confident sense of his priorities and values. I also read the post, which \"shows its work\" to such a degree that Ben would need to be *unusually* evil and deceptive in order for this post to be an act of deception.\n\nI don't have any private knowledge about Nonlinear or about Ben's investigation, but I'm happy to vouch for Ben, such that if he turns out to have been lying, I ought to take a credibility hit too.\n\n> He's just a guy who hasn't been trained as an investigative journalist\n\nIf he were a random non-LW investigative journalist, I'd be a lot less confident in the post's honesty.\n\n> Number of hours invested in research does not necessarily correlate with objectivity of research\n\n\"Number of hours invested\" doesn't prove Ben isn't a lying sociopath (heck, if you think that you can just posit that he's lying about the hours spent), but if he isn't a lying sociopath, it's strong evidence against negligence.\n\n> So, until we know a lot more about this case, I'll withhold judgment about who might or might not be deliberately asserting falsehoods.\n\nThat's totally fine, since as you say, you'd never heard of Ben until yesterday. (FWIW, I think he's one of the best rationalists out there, and he's a well-established Berkeley-rat community member who co-runs LessWrong and who tons of other veteran LWers can vouch for.)\n\nMy claim isn't \"Geoffrey should be confident that Ben is being honest\" (that maybe depends on how much stock you put in my vouching and meta-vouching here), but rather:\n\n1.  I'm pretty sure Emerson doesn't have strong reason to think Ben *isn't* being honest here.\n2.  If Emerson lacks strong reason to think Ben is being dishonest, then he *definitely* shouldn't have threatened to sue Ben.\n\nE.g., I'm claiming here that you shouldn't sue someone for libel if you feel highly uncertain about whether they're being honest or dishonest. It's ethically necessary (though IMO not sufficient) that you feel pretty sure the other person is being super dishonest. And I'd be very surprised if Emerson has rationally reached that epistemic state (because I know Ben, and I expect he conducted himself in his interactions with Nonlinear the same way he normally conducts himself).",
      "plaintextDescription": "> Why do you think that's obvious?\n\nI know Ben, I've conversed with him a number of times in the past and seen lots of his LW comments, and I have a very strong and confident sense of his priorities and values. I also read the post, which \"shows its work\" to such a degree that Ben would need to be unusually evil and deceptive in order for this post to be an act of deception.\n\nI don't have any private knowledge about Nonlinear or about Ben's investigation, but I'm happy to vouch for Ben, such that if he turns out to have been lying, I ought to take a credibility hit too.\n\n> He's just a guy who hasn't been trained as an investigative journalist\n\nIf he were a random non-LW investigative journalist, I'd be a lot less confident in the post's honesty.\n\n> Number of hours invested in research does not necessarily correlate with objectivity of research\n\n\"Number of hours invested\" doesn't prove Ben isn't a lying sociopath (heck, if you think that you can just posit that he's lying about the hours spent), but if he isn't a lying sociopath, it's strong evidence against negligence.\n\n> So, until we know a lot more about this case, I'll withhold judgment about who might or might not be deliberately asserting falsehoods.\n\nThat's totally fine, since as you say, you'd never heard of Ben until yesterday. (FWIW, I think he's one of the best rationalists out there, and he's a well-established Berkeley-rat community member who co-runs LessWrong and who tons of other veteran LWers can vouch for.)\n\nMy claim isn't \"Geoffrey should be confident that Ben is being honest\" (that maybe depends on how much stock you put in my vouching and meta-vouching here), but rather:\n\n 1. I'm pretty sure Emerson doesn't have strong reason to think Ben isn't being honest here.\n 2. If Emerson lacks strong reason to think Ben is being dishonest, then he definitely shouldn't have threatened to sue Ben.\n\nE.g., I'm claiming here that you shouldn't sue someone for libel if you feel highly uncertain about whether the"
    },
    "baseScore": 32,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2023-09-08T17:33:31.365Z",
    "af": false
  },
  {
    "_id": "43sadiQYtg9BAqsLz",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "HvdX2wAEXDBHETRb8",
    "topLevelCommentId": "XANzqTAkw85PTiWFt",
    "contents": {
      "markdown": "Actually, I do know of an example of y'all offering money to someone for defending an org you disliked and were suspicious of. [@habryka](https://www.lesswrong.com/users/habryka4?mention=user), did that money get accepted?\n\n(The incentive effects are basically the same whether it was accepted or not, as long as it's public knowledge that the money was *offered*; so it seems good to make this public if possible.)",
      "plaintextDescription": "Actually, I do know of an example of y'all offering money to someone for defending an org you disliked and were suspicious of. @habryka, did that money get accepted?\n\n(The incentive effects are basically the same whether it was accepted or not, as long as it's public knowledge that the money was offered; so it seems good to make this public if possible.)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2023-09-08T17:20:09.073Z",
    "af": false
  },
  {
    "_id": "4JYKubjomgqEdoqpS",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "N3HRKjCEiqSBtHhFG",
    "topLevelCommentId": "KMf4NrDH5GXizgFqc",
    "contents": {
      "markdown": "> Yeah, this post makes me wonder if there are non-abusive employers in EA who are nevertheless enabling abusers by normalizing behavior that makes abuse popular. Employers who pay their employees months late without clarity on why and what the plan is to get people paid eventually. Employers who employ people without writing things down, like how much people will get paid and when. Employers who try to enforce non-disclosure of work culture and pay.\n\nDo any of those things happen much in EA? (I don't think I've ever heard of an example of one of those things outside of Nonlinear, but maybe I'm out of the loop.)",
      "plaintextDescription": "> Yeah, this post makes me wonder if there are non-abusive employers in EA who are nevertheless enabling abusers by normalizing behavior that makes abuse popular. Employers who pay their employees months late without clarity on why and what the plan is to get people paid eventually. Employers who employ people without writing things down, like how much people will get paid and when. Employers who try to enforce non-disclosure of work culture and pay.\n\nDo any of those things happen much in EA? (I don't think I've ever heard of an example of one of those things outside of Nonlinear, but maybe I'm out of the loop.)"
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2023-09-08T17:13:55.489Z",
    "af": false
  },
  {
    "_id": "G9zZ3FuYpZa5QsFd7",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "XLgXxBkvmTCeKwkRN",
    "topLevelCommentId": "D2CLyZe9LrcxB2znk",
    "contents": {
      "markdown": "As I think of it, the heart of the \"bad argument gets counterargument\" notion is \"respond to arguments using reasoning, not coercion\", rather than \"literal physical violence is a unique category of thing that is never OK\". Both strike me as good norms, but the former seems *deeper* and more novel to me, closer to the heart of things. I'm a fan of [Scott's gloss](https://slatestarcodex.com/2013/12/29/the-spirit-of-the-first-amendment/) (and am happy to cite it instead, if we want to construe Eliezer's version of the thing as something narrower):\n\n> \\[...\\] What is the âspirit of the First Amendmentâ? Eliezer Yudkowsky [writes](http://lesswrong.com/lw/lo/uncritical_supercriticality/):\n> \n> *\"There are a very few injunctions in the human art of rationality that have no ifs, ands, buts, or escape clauses. This is one of them. Bad argument gets counterargument. Does not get bullet. Never. Never ever never for ever.\"*\n> \n> Why is this a rationality injunction instead of a legal injunction? Because the point is protecting âthe marketplace of ideasâ where arguments succeed based on the evidence supporting or opposing them and not based on the relative firepower of their proponents and detractors. \\[...\\]\n> \n> What does âbulletâ mean in the quote above? Are other projectiles covered? Arrows? Boulders launched from catapults? What about melee weapons like swords or maces? Where exactly do we draw the line for âinappropriate responses to an argumentâ?\n> \n> A good response to an argument is one that addresses an idea; a bad argument is one that silences it. If you try to address an idea, your success depends on how good the idea is; if you try to silence it, your success depends on how powerful you are and how many pitchforks and torches you can provide on short notice.\n> \n> Shooting bullets is a good way to silence an idea without addressing it. So is firing stones from catapults, or slicing people open with swords, or gathering a pitchfork-wielding mob.\n> \n> But trying to get someone fired for holding an idea is *also* a way of silencing an idea without addressing it. Iâm sick of talking about Phil Robertson, so letâs talk about [the Alabama woman who was fired for having a Kerry-Edwards bumper sticker on her car](http://www.slate.com/articles/news_and_politics/chatterbox/2004/09/bumper_sticker_insubordination.html) (her boss supported Bush). Could be an easy way to quiet support for a candidate you donât like. Oh, there are more Bush voters than Kerry voters in this county? Letâs bombard her workplace with letters until they fire her! Now sheâs broke and has to sit at home trying to scrape money together to afford food and ruing the day she ever dared to challenge our prejudices! And the next person to disagree with the rest of us will think twice before opening their mouth!\n> \n> The e-version of this practice is âdoxxingâ, where you hunt down an online commenterâs personally identifiable information including address. Then you either harass people they know personally, spam their place of employment with angry comments, or post it on the Internet for everyone to see, probably with a message like âI would never threaten this person at their home address *myself*, but if one of my followers wants to, I *guess* I canât stop them.â This was the Jezebel strategy that Michael was most complaining about. Freethought Blogs is also [particularly](http://heathen-hub.com/blog.php?b=1712) [famous](http://www.patheos.com/blogs/rockbeyondbelief/2012/12/23/greg-laden-posts-a-home-address-and-employer-contacts-for-online-rival/) for this tactic and often devolves into [sagas](http://uberfeminist.blogspot.com/2013/12/the-sciam-blog-drama.html) that would make [MsScribe herself](https://slatestarcodex.com/2013/12/23/we-are-all-msscribe/) proud.\n> \n> A lot of people would argue that doxxing holds people âaccountableâ for what they say online. But like most methods of silencing speech, its ability to punish people for saying the wrong things is entirely uncorrelated with whether the thing they said is actually wrong. It distributes power based on who controls the largest mob (hint: popular people) and who has the resources, job security, and physical security necessary to outlast a personal attack (hint: rich people). If you try to hold the Koch Brothers âaccountableâ for muddying the climate change waters, they will laugh in your face. If you try to hold closeted gay people âaccountableâ for promoting gay rights, it will be very easy and you will successfully ruin their lives. Do you really want to promote a policy that works this way?\n> \n> There are even more subtle ways of silencing an idea than trying to get its proponents fired or real-life harassed. For example, you can always just harass them online. The stronger forms of this, like death threats and rape threats, are of course illegal. But that still leaves many opportunities for constant verbal abuse, crude sexual jokes, insults aimed at family members, and dozens of emails written in all capital letters about what sorts of colorful punishments you and the people close to you deserve. \\[...\\]\n> \n> My answer to the âDoctrine Of The Preferred First Speakerâ ought to be clear by now. The conflict isnât always just between first speaker and second speaker, it can also be between someone whoâs trying to debate versus someone whoâs trying to silence. Telling a bounty hunter on the phone âIâll pay you $10 million to kill Bobâ is a form of speech, but its goal is to silence rather than to counterargue. So is commenting âYOU ARE A SLUT AND I HOPE YOUR FAMILY DIESâ on a blog. And so is orchestrating a letter-writing campaign demanding a business fire someone who vocally supports John Kerry.\n> \n> Bad argument gets counterargument. Does not get bullet. Does not get doxxing. Does not get harassment. Does not get fired from job. Gets counterargument. Should not be hard.",
      "plaintextDescription": "As I think of it, the heart of the \"bad argument gets counterargument\" notion is \"respond to arguments using reasoning, not coercion\", rather than \"literal physical violence is a unique category of thing that is never OK\". Both strike me as good norms, but the former seems deeper and more novel to me, closer to the heart of things. I'm a fan of Scott's gloss (and am happy to cite it instead, if we want to construe Eliezer's version of the thing as something narrower):\n\n> [...] What is the âspirit of the First Amendmentâ? Eliezer Yudkowsky writes:\n> \n> \"There are a very few injunctions in the human art of rationality that have no ifs, ands, buts, or escape clauses. This is one of them. Bad argument gets counterargument. Does not get bullet. Never. Never ever never for ever.\"\n> \n> Why is this a rationality injunction instead of a legal injunction? Because the point is protecting âthe marketplace of ideasâ where arguments succeed based on the evidence supporting or opposing them and not based on the relative firepower of their proponents and detractors. [...]\n> \n> What does âbulletâ mean in the quote above? Are other projectiles covered? Arrows? Boulders launched from catapults? What about melee weapons like swords or maces? Where exactly do we draw the line for âinappropriate responses to an argumentâ?\n> \n> A good response to an argument is one that addresses an idea; a bad argument is one that silences it. If you try to address an idea, your success depends on how good the idea is; if you try to silence it, your success depends on how powerful you are and how many pitchforks and torches you can provide on short notice.\n> \n> Shooting bullets is a good way to silence an idea without addressing it. So is firing stones from catapults, or slicing people open with swords, or gathering a pitchfork-wielding mob.\n> \n> But trying to get someone fired for holding an idea is also a way of silencing an idea without addressing it. Iâm sick of talking about Phil Robertson, so letâs"
    },
    "baseScore": 20,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2023-09-08T07:30:17.739Z",
    "af": false
  },
  {
    "_id": "jnodfXMJYJGsdwJus",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "iydEd9e8PrxCgE7Ah",
    "topLevelCommentId": "iydEd9e8PrxCgE7Ah",
    "contents": {
      "markdown": "> But NDAs are not a red-line for me personally\n\nAn NDA to keep the organization's IP private seems fine to me; an NDA to prevent people from publicly criticizing their former workplace seems line-crossing to me.",
      "plaintextDescription": "> But NDAs are not a red-line for me personally\n\nAn NDA to keep the organization's IP private seems fine to me; an NDA to prevent people from publicly criticizing their former workplace seems line-crossing to me."
    },
    "baseScore": 37,
    "voteCount": 18,
    "createdAt": null,
    "postedAt": "2023-09-08T02:06:48.968Z",
    "af": false
  },
  {
    "_id": "NZqZyqpowviJbGaDx",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "wt3opypjqcfZ6bLkE",
    "topLevelCommentId": "XANzqTAkw85PTiWFt",
    "contents": {
      "markdown": "Notably, one way to offset the reputational issue is to sometimes give people money for saying novel *positive* things about an org. The issue is less \"people receive money for updating us\" and more \"people receive money only if they updated us in a certain direction\", or even worse \"people receive money only if they updated us in a way that fits a specific narrative (e.g., This Org Is Culty And Abusive)\".",
      "plaintextDescription": "Notably, one way to offset the reputational issue is to sometimes give people money for saying novel positive things about an org. The issue is less \"people receive money for updating us\" and more \"people receive money only if they updated us in a certain direction\", or even worse \"people receive money only if they updated us in a way that fits a specific narrative (e.g., This Org Is Culty And Abusive)\"."
    },
    "baseScore": 16,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2023-09-08T02:00:27.355Z",
    "af": false
  },
  {
    "_id": "Mwctg6qzoEWwAcaSx",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "xFadTyguehMh3WMBt",
    "topLevelCommentId": "D2CLyZe9LrcxB2znk",
    "contents": {
      "markdown": "I dunno why cata posted it, but I almost quoted this myself to explain why I dislike the proposed \"bad argument gets lawsuit\" norm.",
      "plaintextDescription": "I dunno why cata posted it, but I almost quoted this myself to explain why I dislike the proposed \"bad argument gets lawsuit\" norm."
    },
    "baseScore": 21,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2023-09-08T00:57:29.394Z",
    "af": false
  },
  {
    "_id": "MivxZZeygf3FqjWJB",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "H9CormrgxFWBQgvAg",
    "topLevelCommentId": "eLHL8FsaTwMJz2jqc",
    "contents": {
      "markdown": "This also updates me about Kat's take (as summarized by Ben Pace in the OP):\n\n> Kat doesnât trust Alice to tell the truth, and that Alice has a history of âcatastrophic misunderstandingsâ.\n\nWhen I read the post, I didn't see any particular reason for Kat to think this, and I worried it might be *just* be an attempt to dismiss a critic, given the aggressive way Nonlinear otherwise seems to have responded to criticisms.\n\nWith this new info, it now seems plausible to me that Kat was *correct* (even though I don't think this justifies threatening Alice or Ben in the way Kat and Emerson did). And if Kat's not correct, I still update that Kat was probably accurately stating her epistemic state, and that a lot of reasonable people might have reached the same epistemic state.",
      "plaintextDescription": "This also updates me about Kat's take (as summarized by Ben Pace in the OP):\n\n> Kat doesnât trust Alice to tell the truth, and that Alice has a history of âcatastrophic misunderstandingsâ.\n\nWhen I read the post, I didn't see any particular reason for Kat to think this, and I worried it might be just be an attempt to dismiss a critic, given the aggressive way Nonlinear otherwise seems to have responded to criticisms.\n\nWith this new info, it now seems plausible to me that Kat was correct (even though I don't think this justifies threatening Alice or Ben in the way Kat and Emerson did). And if Kat's not correct, I still update that Kat was probably accurately stating her epistemic state, and that a lot of reasonable people might have reached the same epistemic state."
    },
    "baseScore": 31,
    "voteCount": 23,
    "createdAt": null,
    "postedAt": "2023-09-08T00:51:14.634Z",
    "af": false
  },
  {
    "_id": "H9CormrgxFWBQgvAg",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "vRjmHrW4sWmPyivtH",
    "topLevelCommentId": "eLHL8FsaTwMJz2jqc",
    "contents": {
      "markdown": "> I think that there's a big difference between telling everyone \"I didn't get the food I wanted, but they did get/offer to cook me vegan food, and I told them it was ok!\" and \"they refused to get me vegan food and I barely ate for 2 days\".\n\nAgreed.",
      "plaintextDescription": "> I think that there's a big difference between telling everyone \"I didn't get the food I wanted, but they did get/offer to cook me vegan food, and I told them it was ok!\" and \"they refused to get me vegan food and I barely ate for 2 days\".\n\nAgreed."
    },
    "baseScore": 37,
    "voteCount": 24,
    "createdAt": null,
    "postedAt": "2023-09-08T00:51:07.336Z",
    "af": false
  },
  {
    "_id": "ooBbGAm9dTLdwPmrn",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "GLoqkeCscFJLxTkB3",
    "topLevelCommentId": "eLHL8FsaTwMJz2jqc",
    "contents": {
      "markdown": "(Crossposted)\n\n> It also seems totally reasonable that no one at Nonlinear understood there was a problem. Alice's language throughout emphasizes how she'll be fine, it's no big deal \\[...\\] I do not think that these exchanges depict the people at Nonlinear as being cruel, insane, or unusual as people.\n\n100% agreed with this. The chat log paints a wildly different picture than what was included in Ben's original post.\n\n> Given my experience with talking with people about strongly emotional events, I am inclined towards the interpretation where Alice remembers the 15th with acute distress and remembers it as 'not getting her needs met despite trying quite hard to do so', and the Nonlinear team remembers that they went out of their way that week to get Alice food - which is based on the logs from the 16th clearly true! But I don't think I'd call Alice a liar based on reading this\n\nAgreed. I did update toward \"there's likely a nontrivial amount of distortion in Alice's retelling of other things\", and toward \"normal human error and miscommunication played a larger role in some of the Bad Stuff that happened than I previously expected\". (Ben's post was still a *giant* negative update for me about Nonlinear, but Kat's comment is a smaller update in the opposite direction.)",
      "plaintextDescription": "(Crossposted)\n\n> It also seems totally reasonable that no one at Nonlinear understood there was a problem. Alice's language throughout emphasizes how she'll be fine, it's no big deal [...] I do not think that these exchanges depict the people at Nonlinear as being cruel, insane, or unusual as people.\n\n100% agreed with this. The chat log paints a wildly different picture than what was included in Ben's original post.\n\n> Given my experience with talking with people about strongly emotional events, I am inclined towards the interpretation where Alice remembers the 15th with acute distress and remembers it as 'not getting her needs met despite trying quite hard to do so', and the Nonlinear team remembers that they went out of their way that week to get Alice food - which is based on the logs from the 16th clearly true! But I don't think I'd call Alice a liar based on reading this\n\nAgreed. I did update toward \"there's likely a nontrivial amount of distortion in Alice's retelling of other things\", and toward \"normal human error and miscommunication played a larger role in some of the Bad Stuff that happened than I previously expected\". (Ben's post was still a giant negative update for me about Nonlinear, but Kat's comment is a smaller update in the opposite direction.)"
    },
    "baseScore": 27,
    "voteCount": 21,
    "createdAt": null,
    "postedAt": "2023-09-08T00:50:18.505Z",
    "af": false
  },
  {
    "_id": "oBXxHPFPqRDCDb2Nv",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "xKvZXyALkPAMt4Zix",
    "topLevelCommentId": "PTw7BugBAFQBKNQRk",
    "contents": {
      "markdown": "Jim's point here is compatible with \"US libel laws are a force for good epistemics\", since a law can be aimed at lying+bullshitting and still disincentivize bad reasoning (to some degree) as a side-effect.\n\nBut I do think Jim's point strongly suggests that we should have a norm against suing someone *merely* for reasoning poorly or getting the wrong answer. That would be moving from \"lawsuits are good for norm enforcement\" to \"*frivolous* lawsuits are good for norm enforcement\", which is way less plausible.",
      "plaintextDescription": "Jim's point here is compatible with \"US libel laws are a force for good epistemics\", since a law can be aimed at lying+bullshitting and still disincentivize bad reasoning (to some degree) as a side-effect.\n\nBut I do think Jim's point strongly suggests that we should have a norm against suing someone merely for reasoning poorly or getting the wrong answer. That would be moving from \"lawsuits are good for norm enforcement\" to \"frivolous lawsuits are good for norm enforcement\", which is way less plausible."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-09-07T22:27:18.129Z",
    "af": false
  },
  {
    "_id": "xmHPoMHKS4JfLhjw3",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "PTw7BugBAFQBKNQRk",
    "topLevelCommentId": "PTw7BugBAFQBKNQRk",
    "contents": {
      "markdown": "> Without making any comment about the accuracy or inaccuracy of this post, I would just point out that nobody in EA should be shocked that an organization (e.g. Nonlinear) that is being libeled (in its view) would threaten a libel suit to deter the false accusations (as they see them), to nudge the author(e.g. Ben Pace) towards making sure that their negative claims are factually correct and contextually fair.Â \n\n[Wikipedia](https://en.wikipedia.org/wiki/United_States_defamation_law) claims: \"The 1964 case [*New York Times Co. v. Sullivan*](https://en.wikipedia.org/wiki/New_York_Times_Co._v._Sullivan), however, radically changed the nature of libel law in the United States by establishing that public officials could win a suit for libel only when they could prove the media outlet in question knew either that the information was wholly and patently false or that it was published 'with reckless disregard of whether it was false or not'.\"\n\nSpartz isn't a \"public official\", so maybe the standard is laxer here?\n\nIf not, then it seems clear to me that Spartz wouldn't win in a fair trial, because *whether or not* Ben got tricked by Alice/Chloe and accidentally signal-boosted others' lies, it's very obvious that Ben is neither *deliberately* asserting falsehoods, nor publishing \"with reckless disregard\".\n\n(Ben says he spent \"100-200 hours\" researching this post, which is *way* beyond the level of thoroughness we should require for criticizing an organization on LessWrong or the EA Forum!)\n\nI think there should be a strong norm against threatening people with libel *merely* for saying a falsehood; the standard should *at minimum* be that you have good reason to think the person is *deliberately* lying or bullshitting.\n\n(I think the standard should be way higher than *that*, too, given the chilling effect of litigiousness; but I won't argue that here.)",
      "plaintextDescription": "> Without making any comment about the accuracy or inaccuracy of this post, I would just point out that nobody in EA should be shocked that an organization (e.g. Nonlinear) that is being libeled (in its view) would threaten a libel suit to deter the false accusations (as they see them), to nudge the author(e.g. Ben Pace) towards making sure that their negative claims are factually correct and contextually fair.Â \n\nWikipedia claims: \"The 1964 case New York Times Co. v. Sullivan, however, radically changed the nature of libel law in the United States by establishing that public officials could win a suit for libel only when they could prove the media outlet in question knew either that the information was wholly and patently false or that it was published 'with reckless disregard of whether it was false or not'.\"\n\nSpartz isn't a \"public official\", so maybe the standard is laxer here?\n\nIf not, then it seems clear to me that Spartz wouldn't win in a fair trial, because whether or not Ben got tricked by Alice/Chloe and accidentally signal-boosted others' lies, it's very obvious that Ben is neither deliberately asserting falsehoods, nor publishing \"with reckless disregard\".\n\n(Ben says he spent \"100-200 hours\" researching this post, which is way beyond the level of thoroughness we should require for criticizing an organization on LessWrong or the EA Forum!)\n\nI think there should be a strong norm against threatening people with libel merely for saying a falsehood; the standard should at minimum be that you have good reason to think the person is deliberately lying or bullshitting.\n\n(I think the standard should be way higher than that, too, given the chilling effect of litigiousness; but I won't argue that here.)"
    },
    "baseScore": 20,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2023-09-07T21:16:28.791Z",
    "af": false
  }
]