[
  {
    "_id": "xsAf8iLRAMvDtvfnF",
    "postId": "K2D45BNxnZjdpSX2j",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "\nThe important thing for alignment work isn't the median prediction; if we had an alignment solution just by then, we'd have a 50% chance of dying from that lack. \n\nI think the biggest takeaway is that nobody has a very precise and reliable prediction, so if we want to have good alignment plans in advance of AGI, we'd better get cracking.\n\nI think Daniel's estimate does include a pretty specific and plausible model of a path to AGI, so I take his the most seriously. My model of possible AGI architectures requires even less compute than his, but I think the Hofstadter principle applies to AGI development if not compute progress.\n\nEstimates in the absence of gears-level models of AGI seem much more uncertain, which might be why Ajeya and Ege's have much wider distributions. \n\n",
      "plaintextDescription": "The important thing for alignment work isn't the median prediction; if we had an alignment solution just by then, we'd have a 50% chance of dying from that lack.\n\nI think the biggest takeaway is that nobody has a very precise and reliable prediction, so if we want to have good alignment plans in advance of AGI, we'd better get cracking.\n\nI think Daniel's estimate does include a pretty specific and plausible model of a path to AGI, so I take his the most seriously. My model of possible AGI architectures requires even less compute than his, but I think the Hofstadter principle applies to AGI development if not compute progress.\n\nEstimates in the absence of gears-level models of AGI seem much more uncertain, which might be why Ajeya and Ege's have much wider distributions."
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2023-11-10T06:40:28.845Z",
    "af": false
  },
  {
    "_id": "3CW7ARbfyrme3HEiK",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "zZ9WcAvprk4xWuYq6",
    "topLevelCommentId": "mokYbFDEmLs9Q7JzC",
    "contents": {
      "markdown": "Yes. But because we're discussing a scenario in which the world is ready to slow down or shut down AGI research, I'm assuming those steps have been crossed.\n\nThe biggest step IMO, \"alignment is hard\" doesn't intervene between taking ASI seriously and thinking it could prevent you from dying of natural causes. ",
      "plaintextDescription": "Yes. But because we're discussing a scenario in which the world is ready to slow down or shut down AGI research, I'm assuming those steps have been crossed.\n\nThe biggest step IMO, \"alignment is hard\" doesn't intervene between taking ASI seriously and thinking it could prevent you from dying of natural causes."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T21:12:18.590Z",
    "af": false
  },
  {
    "_id": "QtCYwGaZXiMc2e3BX",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "maJq3hLSXDsAkGwqg",
    "topLevelCommentId": "mokYbFDEmLs9Q7JzC",
    "contents": {
      "markdown": "It seems to me that believing ASI can kill you and believing ASI can save you are both pretty directly downstream of believing in ASI at all. Since the premise is that everyone believes pretty strongly in the possibility of doom, it seems they'd mostly get there by believing in ASI and would mostly also believe in the upside potentials too.\n",
      "plaintextDescription": "It seems to me that believing ASI can kill you and believing ASI can save you are both pretty directly downstream of believing in ASI at all. Since the premise is that everyone believes pretty strongly in the possibility of doom, it seems they'd mostly get there by believing in ASI and would mostly also believe in the upside potentials too."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T17:00:02.193Z",
    "af": false
  },
  {
    "_id": "x38haZ22ER5FdWFcq",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "KiQH7k52Qt2SZuRsA",
    "topLevelCommentId": "mokYbFDEmLs9Q7JzC",
    "contents": {
      "markdown": "I thought I was addressing the premise of your post: the world is ready to do serious restrictions on AI research: do they do shutdown or controlled takeoff?\n\nI guess maybe I'm missing what's important about the physical consolidation vs other methods of inspection and enforcement. \n\nI think my scenario conforms to all of the gears you mention. It could be seen as adding another gear: the incentives/psychologies of government decision-makers. ",
      "plaintextDescription": "I thought I was addressing the premise of your post: the world is ready to do serious restrictions on AI research: do they do shutdown or controlled takeoff?\n\nI guess maybe I'm missing what's important about the physical consolidation vs other methods of inspection and enforcement.\n\nI think my scenario conforms to all of the gears you mention. It could be seen as adding another gear: the incentives/psychologies of government decision-makers."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T16:49:37.819Z",
    "af": false
  },
  {
    "_id": "q5XtApA5ifahRohb7",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "NM29TkvMfm6Rzs4j6",
    "topLevelCommentId": "mokYbFDEmLs9Q7JzC",
    "contents": {
      "markdown": "I agree that governments aren't coming anywhere close to making this calculation at this point. I mean they very well might once they've actually thought about the issue.  I think it will depend a lot on their collective distribution of p(doom). I'd think they'd push ahead if they could convince themselves it was lower than maybe 20% or thereabouts. I'd love to be convinced they would be more cautious.   \n\nOf course I think it very much depends on who is in the relevant governments at that time. I think that the issue could play a large role in elections, and that might help a lot.\n\n\n\n",
      "plaintextDescription": "I agree that governments aren't coming anywhere close to making this calculation at this point. I mean they very well might once they've actually thought about the issue. I think it will depend a lot on their collective distribution of p(doom). I'd think they'd push ahead if they could convince themselves it was lower than maybe 20% or thereabouts. I'd love to be convinced they would be more cautious.\n\nOf course I think it very much depends on who is in the relevant governments at that time. I think that the issue could play a large role in elections, and that might help a lot."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T03:40:31.002Z",
    "af": false
  },
  {
    "_id": "KxsaPF8RD7TB7daDM",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "ktkH99fgWi9KYEhcw",
    "topLevelCommentId": "mokYbFDEmLs9Q7JzC",
    "contents": {
      "markdown": "The scenario I'm thinking of mostly is the overt version, which is controlled takeoff. A few governments pursue controlled takeoff projects. This is hopefully done in the open and relatively collaboratively across US and Chinese teams. I'd assume they'd recruit from existing teams, effectively consolidating labs under government control.  I'd hope that the Western and Chinese teams would agree to share their results on capabilities as well as alignment, although of course they'd worry about defection on that, too.\n\nIf they did it covertly that would be defecting. I haven't thought about that scenario as much.\n\nThis scenario doesn't seem like it would require consolidating GPUs, just monitoring their usage to some degree. It seems like it would be a lot easier to not make that part of the treaty.",
      "plaintextDescription": "The scenario I'm thinking of mostly is the overt version, which is controlled takeoff. A few governments pursue controlled takeoff projects. This is hopefully done in the open and relatively collaboratively across US and Chinese teams. I'd assume they'd recruit from existing teams, effectively consolidating labs under government control. I'd hope that the Western and Chinese teams would agree to share their results on capabilities as well as alignment, although of course they'd worry about defection on that, too.\n\nIf they did it covertly that would be defecting. I haven't thought about that scenario as much.\n\nThis scenario doesn't seem like it would require consolidating GPUs, just monitoring their usage to some degree. It seems like it would be a lot easier to not make that part of the treaty."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T03:29:42.952Z",
    "af": false
  },
  {
    "_id": "7Ev6SGYFLcz89SvRk",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "rcCpa9bpsxDSaNLGg",
    "topLevelCommentId": "mokYbFDEmLs9Q7JzC",
    "contents": {
      "markdown": "Good points; I agree with all of them.\n\nIt's hard to know how to weigh them.\n\nMy mental model does include those gradients even though I expressed them as categories.\n\nI currently think it's all too likely that decision-makers would accept a 20% or more chance of extinction in exchange for the benefits.\n\nOne route is to make better guesses about what happens by default. The other is to try to create better decisions by spreading the relevant logic.\n\nThose who want to gamble will certainly push the \"it should be fine and we need to do it!\" logic. The two sets of beliefs will probably develop symbiotically. It's hard to separate emotional from rational reasons for beliefs.\n\nIt looks to me like people automatically convince themselves that what they want to do emotionally is also the logical thing to do. See [Motivated reasoning/confirmation bias as the most important cognitive bias](https://www.lesswrong.com/posts/j789HDCKLoiKGjBik/which-biases-are-most-important-to-overcome#LW8zAxTguKj8ibDfX) for a brief discussion.\n\nBased on that logic, I actually think human cognitive biases and cognitive limitations are the biggest challenge to surviving ASI. We're silly creatures with a spark of reason.",
      "plaintextDescription": "Good points; I agree with all of them.\n\nIt's hard to know how to weigh them.\n\nMy mental model does include those gradients even though I expressed them as categories.\n\nI currently think it's all too likely that decision-makers would accept a 20% or more chance of extinction in exchange for the benefits.\n\nOne route is to make better guesses about what happens by default. The other is to try to create better decisions by spreading the relevant logic.\n\nThose who want to gamble will certainly push the \"it should be fine and we need to do it!\" logic. The two sets of beliefs will probably develop symbiotically. It's hard to separate emotional from rational reasons for beliefs.\n\nIt looks to me like people automatically convince themselves that what they want to do emotionally is also the logical thing to do. See Motivated reasoning/confirmation bias as the most important cognitive bias for a brief discussion.\n\nBased on that logic, I actually think human cognitive biases and cognitive limitations are the biggest challenge to surviving ASI. We're silly creatures with a spark of reason."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-24T22:48:00.451Z",
    "af": false
  },
  {
    "_id": "Ra8Smnpzztkn2WBrn",
    "postId": "Yg6yg64Rt7h3i8mym",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Good to see someone thinking about this. I think this type of future is imagined frequently, but rarely in adequate detail.\n\nI'm disappointed to see that we agree. When I considered such scenarios, asking [If we solve alignment, do we die anyway?](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1) my answer was probably yes if we allow proliferation. And I agree that proliferation is all too likely.  \n \n\nBut I'm not sure. And it seems like this could use more thought.  \n  \nI don't think the solar-system scenario you seem to imagine would be durable. Some asshole with an intent-aligned AGI (or who is a misaligned AGI) will figure out how to send the sun nova while they go elsewhere in hopes of claiming the rest of the lightcone.  \n  \nSo I'd guess there would be a diaspora, an attempt to go far enough for safety. And sooner or later someone woudl decide to do the full maximum self-replicating expansion, if only as a prgamatic means of ensuring they/their faction survives by virtue of maximum redundancy and military power.  \n  \nI'm currently hoping we avoid this situation by predicting how likely it is to end in quick death at the hands of the most vicious. But I'd really like to see more careful thought on this topic. Are there stable equilibriums that can be reached? Is some sort of maximum information and mutually-assured destruction stable equilibrium possible? I don't know, and it seems important.",
      "plaintextDescription": "Good to see someone thinking about this. I think this type of future is imagined frequently, but rarely in adequate detail.\n\nI'm disappointed to see that we agree. When I considered such scenarios, asking If we solve alignment, do we die anyway? my answer was probably yes if we allow proliferation. And I agree that proliferation is all too likely.\n \n\nBut I'm not sure. And it seems like this could use more thought.\n\nI don't think the solar-system scenario you seem to imagine would be durable. Some asshole with an intent-aligned AGI (or who is a misaligned AGI) will figure out how to send the sun nova while they go elsewhere in hopes of claiming the rest of the lightcone.\n\nSo I'd guess there would be a diaspora, an attempt to go far enough for safety. And sooner or later someone woudl decide to do the full maximum self-replicating expansion, if only as a prgamatic means of ensuring they/their faction survives by virtue of maximum redundancy and military power.\n\nI'm currently hoping we avoid this situation by predicting how likely it is to end in quick death at the hands of the most vicious. But I'd really like to see more careful thought on this topic. Are there stable equilibriums that can be reached? Is some sort of maximum information and mutually-assured destruction stable equilibrium possible? I don't know, and it seems important."
    },
    "baseScore": 5,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-24T22:23:47.005Z",
    "af": false
  },
  {
    "_id": "JjGvpcL9HuECrcibd",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "oznnWQxPZvDpvZQjE",
    "topLevelCommentId": "mokYbFDEmLs9Q7JzC",
    "contents": {
      "markdown": "This isn't terribly decision-relevant except for deciding what type of alignment work to do. But that does seem nontrival. My bottom line is: push for pause/slowdown, but don't get overoptimistic. Simultaneously work toward alignment on the current path, as fast as possible, because that might well be our only chance.\n\nTo your point:\n\nI take your point on the standard reasoning. I agree that most adults would turn down even a 95-5%, 19 vs 1 odds of improving their lives with a small chance of destroying the world. \n\nBut I'm afraid those with decision-making power would take far worse odds in private, where it matters. That's because for them, it's not just a better life; it's immortality vs. dying soon. And that tends to change decision-making.\n\nI added and marked an edit to make this part of the logic explicit. \n\nMost humans with decision-making power, e.g. in goverment, are 50+ years old, and mostly older since power tends to accumulate at least until sharp cognitive declines set in. There's a pretty good chance they will die of natural causes if ASI isn't created to do groundbreaking medical research within their lifetimes.\n\nThat's on top of any actual nationalistic tendencies, or fears of being killed, enslaved, tortured, or worse, mocked, by losing the race to one's political enemies covertly pursuing ASI.\n\nAnd that's on top of worrying that sociopaths (or similarly cold/selfish decision-makers) are over-represented in the halls of power. Those arguments seem pretty strong to me, too.\n\nHow this would unfold is highly unclear to me. I think it's important to develop gears-level models of how these processes might happen, as Raemon suggests in this post. \n\nMy guess is that covert programs are between likely and inevitable. Public pressure will be for caution; private and powerful opinions will be much harder to predict.\n\nAs for the public statements, it works just fine to say, or more likely convince yourself, that you think aligment is solvable with little chance of failure, and that patriotism and horror over (insert enemy ideology here) controlling the future are your motivations.\n\n\n\n\n",
      "plaintextDescription": "This isn't terribly decision-relevant except for deciding what type of alignment work to do. But that does seem nontrival. My bottom line is: push for pause/slowdown, but don't get overoptimistic. Simultaneously work toward alignment on the current path, as fast as possible, because that might well be our only chance.\n\nTo your point:\n\nI take your point on the standard reasoning. I agree that most adults would turn down even a 95-5%, 19 vs 1 odds of improving their lives with a small chance of destroying the world.\n\nBut I'm afraid those with decision-making power would take far worse odds in private, where it matters. That's because for them, it's not just a better life; it's immortality vs. dying soon. And that tends to change decision-making.\n\nI added and marked an edit to make this part of the logic explicit.\n\nMost humans with decision-making power, e.g. in goverment, are 50+ years old, and mostly older since power tends to accumulate at least until sharp cognitive declines set in. There's a pretty good chance they will die of natural causes if ASI isn't created to do groundbreaking medical research within their lifetimes.\n\nThat's on top of any actual nationalistic tendencies, or fears of being killed, enslaved, tortured, or worse, mocked, by losing the race to one's political enemies covertly pursuing ASI.\n\nAnd that's on top of worrying that sociopaths (or similarly cold/selfish decision-makers) are over-represented in the halls of power. Those arguments seem pretty strong to me, too.\n\nHow this would unfold is highly unclear to me. I think it's important to develop gears-level models of how these processes might happen, as Raemon suggests in this post.\n\nMy guess is that covert programs are between likely and inevitable. Public pressure will be for caution; private and powerful opinions will be much harder to predict.\n\nAs for the public statements, it works just fine to say, or more likely convince yourself, that you think aligment is solvable with little chance o"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-24T21:40:23.946Z",
    "af": false
  },
  {
    "_id": "QFKoKj7JwvcibtuvW",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": "8mzunQYxMeAiwNBeh",
    "topLevelCommentId": "8mzunQYxMeAiwNBeh",
    "contents": {
      "markdown": "I fully agree and have been pleased to see this logic clarified in the recent discussions of IABIED. We must choose where to put our primary efforts, but those like me who think alignment might be achievable on the fast path should still say we'd prefer shutdown if at all possible. I will not only continue to say that, but try to share this logic broadly. Pushing hard for caution of any sort will on average improve our odds. I don't think we can get a shutdown (see other comment) but I'll still state clearly that we should shut it all down.\n\nIt only takes a moment to say \"well of course I think we'd shut it all down if we were wise, but assuming we don't, here are my plans and hopes....\"",
      "plaintextDescription": "I fully agree and have been pleased to see this logic clarified in the recent discussions of IABIED. We must choose where to put our primary efforts, but those like me who think alignment might be achievable on the fast path should still say we'd prefer shutdown if at all possible. I will not only continue to say that, but try to share this logic broadly. Pushing hard for caution of any sort will on average improve our odds. I don't think we can get a shutdown (see other comment) but I'll still state clearly that we should shut it all down.\n\nIt only takes a moment to say \"well of course I think we'd shut it all down if we were wise, but assuming we don't, here are my plans and hopes....\""
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-24T19:41:36.006Z",
    "af": false
  },
  {
    "_id": "mokYbFDEmLs9Q7JzC",
    "postId": "kkWmybhaig4oSWkgX",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Thanks for writing this. \n\nI agree with your logic that shutdown is easier than controlled takeoff, but I think controlled takeoff is much more viable.\n\nI see three major blockers to full shutdown:\n\n*   Many experts currently think alignment isn't incredibly hard\n*   Most current humans aren't utilitarian nor longtermist\n    *   Not dying personally is a large incentive\n*   Winner controls the future\n\nGiven all of that, I'd expect covert or overt government projects to continue even if we get a treaty banning private research. (Which I do find quite possible; I expect scary demos to happen naturally).\n\nThere's one set of experts saying alignment is almost impossible, and another group of experts saying it's probably do-able as long as we aren't dumb about it. That's not a rational reason for a shutdown if you're not longtermist. (edit: - and older, like most decision-makers, so shutdown probably means you personally die). \n\nFull shutdown, or surviving without it, seem to share a critical-path component. Conceptually clarifying and communicating about alignment difficulty on the current path seem necessary for either route. That's what it would take to shift expert opinions enough to get a full shutdown, and it would help the odds of solving alignment in time if we don't get a full shutdown. So I'd love to see more effort in that direction (like we got with your The Title is Reasonable post!)\n\nI really hope I'm missing something! I'll continue advocating for shutdown as a means to any viable slowdown, but I'd love to have some genuine hope for it!\n\nThe only way I can see to prevent some programs continuiing is if experts were to unify behind \"alignment is highly ulikely to succeed using current methods\". If everyone with a clue said \"we'll probably die if we try it any time soon\", that might be enough to dissuade selfish decision-makers. This would require some amazing communication and clarification.  If we take Paul Christiano as representative of expert optimists, he had p(doom) from misalignment at around 20% (and another ~20% of other catastrophes following soon... but those seem separable). Those are decent odds if you only care about yourself and your loved ones.\n\nLots of people would jump at the chance to gamble the entire future against their own immortality on those odds. If that's representative of people in the labs, I don't see how we prevent those in power from gambling the future. \n\nAnd people like Christiano and Shah clearly do understand the problem. So shifting their odds dramatically seems like it would take some breakthrough in conceptual clarification of the problem, or communication, or more likely, both. \n\nThe improvements in undestanding and communicating the difficulty of the alignment problem seem like critical-path for a global shutdown or even slowdown, and also for attempting alignment in worlds where those don't happen. That's why my efforts are going there.\n\n(Given the logic that shutdown is highly unlikely, my best-case hope is that the international treaty agree that a very few teams, probably one US and one Chinese, proceed, while all others are banned (enforcement would happen like any other treaty). Such projects would ideally be public, and communicate with each other on alignment risks and achievements. The idea would be that everyone agrees to roughly split the enormously growing pie, and to share generously with the rest of the world.\n\nGiven that logic, it seems inevitable to me that at least one or two projects push ahead fairly quickly, even in a best-case scenario. That's why my efforts focus on that scenario, where we try alignment on roughly the current path.\n\nHaving raised this question, let me state clearly: I think we should shut it all down. I state this publicly and will continue to do so. I think alignment is probably hard and the odds of achieving it under race conditions are not good. If we get a substantial slowdown that probably means I'll personally die, and I'd take that trade to improve humanity's odds. But I'm probably more utilitarian and longtermist than 99% of humanity.\n\nSo: Am I missing something?\n\n(edited for clarity just after first posting)",
      "plaintextDescription": "Thanks for writing this. \n\nI agree with your logic that shutdown is easier than controlled takeoff, but I think controlled takeoff is much more viable.\n\nI see three major blockers to full shutdown:\n\n * Many experts currently think alignment isn't incredibly hard\n * Most current humans aren't utilitarian nor longtermist\n   * Not dying personally is a large incentive\n * Winner controls the future\n\nGiven all of that, I'd expect covert or overt government projects to continue even if we get a treaty banning private research. (Which I do find quite possible; I expect scary demos to happen naturally).\n\nThere's one set of experts saying alignment is almost impossible, and another group of experts saying it's probably do-able as long as we aren't dumb about it. That's not a rational reason for a shutdown if you're not longtermist. (edit: - and older, like most decision-makers, so shutdown probably means you personally die). \n\nFull shutdown, or surviving without it, seem to share a critical-path component. Conceptually clarifying and communicating about alignment difficulty on the current path seem necessary for either route. That's what it would take to shift expert opinions enough to get a full shutdown, and it would help the odds of solving alignment in time if we don't get a full shutdown. So I'd love to see more effort in that direction (like we got with your The Title is Reasonable post!)\n\nI really hope I'm missing something! I'll continue advocating for shutdown as a means to any viable slowdown, but I'd love to have some genuine hope for it!\n\nThe only way I can see to prevent some programs continuiing is if experts were to unify behind \"alignment is highly ulikely to succeed using current methods\". If everyone with a clue said \"we'll probably die if we try it any time soon\", that might be enough to dissuade selfish decision-makers. This would require some amazing communication and clarification.  If we take Paul Christiano as representative of expert optimists, he ha"
    },
    "baseScore": 5,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-24T19:26:30.390Z",
    "af": false
  },
  {
    "_id": "42bkXMZC2k3pvX3tD",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "t7gAZoeyrtrJmvHBu",
    "topLevelCommentId": "6KnMTrAc7P3oWrq5y",
    "contents": {
      "markdown": "GPT5T roughly agrees with your estimate of nuclear winter deaths, so I'm probably way off. Looks like my information was well out of date; that conversation was from 2017 or so. And yes, deaths from winter-induced famine might be substantially mitigated if someone with AGI-developed tech bothered.\n\nThanks for the clairifications on trade vs kindness.  I am less optimistic about trade, causal or acausal, than you, but it's a factor.",
      "plaintextDescription": "GPT5T roughly agrees with your estimate of nuclear winter deaths, so I'm probably way off. Looks like my information was well out of date; that conversation was from 2017 or so. And yes, deaths from winter-induced famine might be substantially mitigated if someone with AGI-developed tech bothered.\n\nThanks for the clairifications on trade vs kindness. I am less optimistic about trade, causal or acausal, than you, but it's a factor."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T22:59:03.625Z",
    "af": false
  },
  {
    "_id": "6KnMTrAc7P3oWrq5y",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Interesting and good breakdown.\n\nI place much higher odds on the \"death due to takeover\" for a pretty specific reason. We seem to have an excellent takeover mechanism in place which kills all or most of us: nukes. We have a gun pointed at our collective heads, and it's deadlier to humans than AGIs.\n\nIgniting a nuclear exchange and having just enough working robots, power sources, and industrial resources (factories, etc) to rebuild seems like a pretty viable route to fast takeover.\n\nIgniting that exchange could be done via software intrusion or perhaps more easily by spoofing human communications to launch. This is basically the only use of deepfakes that really concerns me, but it concerns me a lot.\n\nThis becomes increasingly concerning to me in a multipolar scenario, which in turn seems all too likely at this point. Then every misaligned AGI is incentivized to take over as quickly as possible. A risky plan becomes more appealing if you have to worry that another AGI with different goals may launch their coup at any point.\n\nThis logic also applies if we solve alignment and have intent-aligned AGIs in a multipolar scenario with different human masters. I guess it also favors everyone who can, getting themselves or a backup to a safe location.\n\nThis is also conditional on progress in robotics vs. timelines; even with short timelines it seems like robotics will probably be far enough along for clumsy robots to build better robots. But here my knowledge of robotics, EMP effects, etc, fails. It does seem like a nontrivial chance that triggering a nuclear exchange is the easiest/fastest route to takeover.\n\nOne very well-informed individual told me nobody knows if any humans would survive a nuclear winter; but that's probably less important than whether the \"winning\" AGI/human wanted them to survive.\n\nThe number of likely deaths given takeover seems higher than your estimate if that logic about nukes as a route to takeover mostly goes through.\n\nBut the question of extinction still hinges largely on whether AGI has any interest in humanity surviving. I think you're assuming it will have a distribution of interests like humans do; I don't think that's a safe assumption at all, even given LLM-based AGI and noting that LLMs do really seem to have a distribution of motivations, including kindness toward humans. \n\nI think how they reason about their goals once they're capable of doing so is very hard to predict, so I'd give it more like a 50% chance they wind up being effectively maximizers for whatever their top motivation happens to be. There seems to be some instrumental pressure toward reflective stability, and that might favor one motivation winning outright vs. sharing power within that mind. I wrote a little about that in [Section 5](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover#5__Reasoning_could_precipitate_a__phase_shift__into_reflective_stability_and_prevent_further_goal_change) and more in the remainder of [LLM AGI may reason about its goals and discover misalignments by default](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover), but it's pretty incomplete.\n\nLike the rest of alignment, it's uncertain. LLMs have some kindness, but that doesn't mean that LLM-based AGI will retain it. If we could be confident they would, we'd be a lot better set to solve alignment.",
      "plaintextDescription": "Interesting and good breakdown.\n\nI place much higher odds on the \"death due to takeover\" for a pretty specific reason. We seem to have an excellent takeover mechanism in place which kills all or most of us: nukes. We have a gun pointed at our collective heads, and it's deadlier to humans than AGIs.\n\nIgniting a nuclear exchange and having just enough working robots, power sources, and industrial resources (factories, etc) to rebuild seems like a pretty viable route to fast takeover.\n\nIgniting that exchange could be done via software intrusion or perhaps more easily by spoofing human communications to launch. This is basically the only use of deepfakes that really concerns me, but it concerns me a lot.\n\nThis becomes increasingly concerning to me in a multipolar scenario, which in turn seems all too likely at this point. Then every misaligned AGI is incentivized to take over as quickly as possible. A risky plan becomes more appealing if you have to worry that another AGI with different goals may launch their coup at any point.\n\nThis logic also applies if we solve alignment and have intent-aligned AGIs in a multipolar scenario with different human masters. I guess it also favors everyone who can, getting themselves or a backup to a safe location.\n\nThis is also conditional on progress in robotics vs. timelines; even with short timelines it seems like robotics will probably be far enough along for clumsy robots to build better robots. But here my knowledge of robotics, EMP effects, etc, fails. It does seem like a nontrivial chance that triggering a nuclear exchange is the easiest/fastest route to takeover.\n\nOne very well-informed individual told me nobody knows if any humans would survive a nuclear winter; but that's probably less important than whether the \"winning\" AGI/human wanted them to survive.\n\nThe number of likely deaths given takeover seems higher than your estimate if that logic about nukes as a route to takeover mostly goes through.\n\nBut the question of extinct"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T19:49:26.535Z",
    "af": false
  },
  {
    "_id": "R52FGYameces3cJC5",
    "postId": "mHep9SRsuzpqQSAsv",
    "parentCommentId": "9g4xsurvx7Zonw8ba",
    "topLevelCommentId": "B8P9fwTSAvdXYKwdR",
    "contents": {
      "markdown": "Worrying that ASI will do bad stuff because we told it to without bothering to understand the consequences is pretty far down my list of things to worry about. I can understand \"eliminates the world as we know it\" without understanding the physics by which it does this. Summaries and simplifications are a thing. I'm gonna ask \"so hey what consequences would this have that I'd care about\" and the ASI, because it's super-smart, will answer in terms I can understand.\n\nIf it doesn't, I'll stick to asking it to do things I can understand. Like improving its ability to summarize and communicate.\n\nYou haven't addressed my point that a smart ASI will be good at summarizing and simplifying.\n\nMaybe you're not concerned with practical dangers, just the possibility that humans won't always understand everying ASIs come up with. In which case, that's fine; I'm worried about everyone dying long  before we get the opportunity to be limited by our understanding. Not being able to fully appreciate everything an ASI is coming up with might be a limitation, but it's far beyond the level of success we can imagine, so I'm putting it in the category of planning the victory party before working out a plan to win.\n\n\n\n\n",
      "plaintextDescription": "Worrying that ASI will do bad stuff because we told it to without bothering to understand the consequences is pretty far down my list of things to worry about. I can understand \"eliminates the world as we know it\" without understanding the physics by which it does this. Summaries and simplifications are a thing. I'm gonna ask \"so hey what consequences would this have that I'd care about\" and the ASI, because it's super-smart, will answer in terms I can understand.\n\nIf it doesn't, I'll stick to asking it to do things I can understand. Like improving its ability to summarize and communicate.\n\nYou haven't addressed my point that a smart ASI will be good at summarizing and simplifying.\n\nMaybe you're not concerned with practical dangers, just the possibility that humans won't always understand everying ASIs come up with. In which case, that's fine; I'm worried about everyone dying long before we get the opportunity to be limited by our understanding. Not being able to fully appreciate everything an ASI is coming up with might be a limitation, but it's far beyond the level of success we can imagine, so I'm putting it in the category of planning the victory party before working out a plan to win."
    },
    "baseScore": 1,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-23T17:45:09.967Z",
    "af": false
  },
  {
    "_id": "PyraPEqmj2cQSwyYa",
    "postId": "pozSWmqLqqc7Z2mQW",
    "parentCommentId": "d7ofoDLtG2aKF9YLj",
    "topLevelCommentId": "7uy7RobuJeW3taHH3",
    "contents": {
      "markdown": "I'll just toss in my answer (I think it's a fairly common perspective on this):\n\nAn agent capable of running a one-year research project would *probably* (not certainly) be a general reasoner. One could construct an agent that could do a year worth of valuable research without giving it the ability to do general reasoning. But when you think about how a human does a year of valuable research, it is absolutely crucial that the employ general problem-solving skills often to debug that progress and make it actually worthwhile (let alone producing any breakthroughs).\n\nIf it can do general reasoning, then it can formulate the questions \"why am I doing this?\" and \"what if I did become god-emperor?\" (or other context-expanding thoughts). That creates the new problems I discuss in [LLM AGI may reason about its goals](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover) which I think are background assumptions of the \"alignment is hard\" worldview. \n\nI think that's why the common intuition is that you'd need to solve the alignment problem to have an aligned one-year human-level atent.\n\nPerhaps your \"year of research\" isn't meant to be particularly groundbreaking, just doing a bunch of studies on how LLMs produce outputs. Figuring out how those results contribute to actually solving the alignment problem might be left to humans. This research could be done theoretically by a subhuman or non-general-reasoning AI. It doesn't look to me like progress is going in this direction, but this is a matter of opinion and interpretation; my argument here isn't sufficient to establish general reasoners as the easier and therefor likelier path to year-long research agents, but I do think it's quite likely to happen that way in the current trajectory..\n\nIf we do make year-long research agents that can't really reason but can still do useful research, this might be helpful, but I don't think it's reasonable to assume this will get alignment solved. Having giant stacks of poorly-thought-out research could be really useful, or almost not at all. And note the compute bottleneck in running research, and the disincentives against spending a year running tons of agents doing research.",
      "plaintextDescription": "I'll just toss in my answer (I think it's a fairly common perspective on this):\n\nAn agent capable of running a one-year research project would probably (not certainly) be a general reasoner. One could construct an agent that could do a year worth of valuable research without giving it the ability to do general reasoning. But when you think about how a human does a year of valuable research, it is absolutely crucial that the employ general problem-solving skills often to debug that progress and make it actually worthwhile (let alone producing any breakthroughs).\n\nIf it can do general reasoning, then it can formulate the questions \"why am I doing this?\" and \"what if I did become god-emperor?\" (or other context-expanding thoughts). That creates the new problems I discuss in LLM AGI may reason about its goals which I think are background assumptions of the \"alignment is hard\" worldview. \n\nI think that's why the common intuition is that you'd need to solve the alignment problem to have an aligned one-year human-level atent.\n\nPerhaps your \"year of research\" isn't meant to be particularly groundbreaking, just doing a bunch of studies on how LLMs produce outputs. Figuring out how those results contribute to actually solving the alignment problem might be left to humans. This research could be done theoretically by a subhuman or non-general-reasoning AI. It doesn't look to me like progress is going in this direction, but this is a matter of opinion and interpretation; my argument here isn't sufficient to establish general reasoners as the easier and therefor likelier path to year-long research agents, but I do think it's quite likely to happen that way in the current trajectory..\n\nIf we do make year-long research agents that can't really reason but can still do useful research, this might be helpful, but I don't think it's reasonable to assume this will get alignment solved. Having giant stacks of poorly-thought-out research could be really useful, or almost not at all. And "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T17:35:44.792Z",
    "af": false
  },
  {
    "_id": "yfo9HhRazgPrsppDq",
    "postId": "QwybGbxtDYtrpBAWr",
    "parentCommentId": "axtQDtJ4jg7skCNjZ",
    "topLevelCommentId": "axtQDtJ4jg7skCNjZ",
    "contents": {
      "markdown": "You really can't answer that question yourself? He's suffering on purpose and that makes people wonder why someone would do that. It won't make a difference in lab policy by itself. It draws attention to the arguments. It's a costly signal that he deeply believes ASI will kill us all. That could make some difference.",
      "plaintextDescription": "You really can't answer that question yourself? He's suffering on purpose and that makes people wonder why someone would do that. It won't make a difference in lab policy by itself. It draws attention to the arguments. It's a costly signal that he deeply believes ASI will kill us all. That could make some difference."
    },
    "baseScore": 5,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-23T05:09:59.596Z",
    "af": false
  },
  {
    "_id": "Hhu2jvhhqkXpeA4yH",
    "postId": "BEQpaa3kGYRWKPDuS",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Stroke victims and comic geniuses sometimes say the same things but they're not the same.",
      "plaintextDescription": "Stroke victims and comic geniuses sometimes say the same things but they're not the same."
    },
    "baseScore": 1,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-23T04:59:22.682Z",
    "af": false
  },
  {
    "_id": "g6FcdunAMNKcadg7p",
    "postId": "mHep9SRsuzpqQSAsv",
    "parentCommentId": "jCpBPoFeoYTPadTzm",
    "topLevelCommentId": "B8P9fwTSAvdXYKwdR",
    "contents": {
      "markdown": "Sounds like you're assuming progress goes on forever. I'd think it would slow down just based on physical limitations. And your supposed ASI is terrible at explanations.\n\nI doubt anything of import is going to work that way. You don't seem to make an argument for it. I see how the exponential suggests it but most exponentials are really s-curves based on limiting factors.",
      "plaintextDescription": "Sounds like you're assuming progress goes on forever. I'd think it would slow down just based on physical limitations. And your supposed ASI is terrible at explanations.\n\nI doubt anything of import is going to work that way. You don't seem to make an argument for it. I see how the exponential suggests it but most exponentials are really s-curves based on limiting factors."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T04:58:27.825Z",
    "af": false
  },
  {
    "_id": "B8P9fwTSAvdXYKwdR",
    "postId": "mHep9SRsuzpqQSAsv",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Seems like the king could still understand with a decent explanation, particularly if he bothers to ask about the effects before using it.",
      "plaintextDescription": "Seems like the king could still understand with a decent explanation, particularly if he bothers to ask about the effects before using it."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-22T04:06:50.781Z",
    "af": false
  },
  {
    "_id": "6cCwmoLNmaMKwFhE4",
    "postId": "pozSWmqLqqc7Z2mQW",
    "parentCommentId": "d7ofoDLtG2aKF9YLj",
    "topLevelCommentId": "7uy7RobuJeW3taHH3",
    "contents": {
      "markdown": "Agreed; the alignment plan sketched here skips over why alignment for a merely-human agent should be a lot easier than for a superhuman one, or instruction-following should be easier than value alignment. I think both are probably true, but to a limited and uncertain degree. See my other comment here for more.",
      "plaintextDescription": "Agreed; the alignment plan sketched here skips over why alignment for a merely-human agent should be a lot easier than for a superhuman one, or instruction-following should be easier than value alignment. I think both are probably true, but to a limited and uncertain degree. See my other comment here for more."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-18T23:58:49.420Z",
    "af": false
  },
  {
    "_id": "APSvCcogTtvyNimiR",
    "postId": "pozSWmqLqqc7Z2mQW",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I agree with your claim as stated; 98% is overconfident.\n\nI have in the past placed a good bit of hope on the basin of alignment idea, although my hopes were importantly different in that they start below the human level. The human level is exactly when you get large context shifts like \"oh hey maybe I could escape and become god-emperor... I don't have human limitations. If I could, maybe I should? Not even thinking about it would be foolish...\" That's when you get the context shift.\n\nWorking through the logic made me a good bit more pessimistic. I just wrote a post on why I made that shift: [LLM AGI may reason about its goals and discover misalignments by default](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover).\n\nAnd that was on top of my previous recognition that my scheme of instruction-following, laid out in [Instruction-following AGI is easier and more likely than value aligned AGI](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than), has problems I hadn't grappled with (even though I'd gone into some depth): [Problems with instruction-following as an alignment target](https://www.lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target).\n\nCould this basin of instruction-following still work? Sure! Maybe!\n\nIs it likely enough by default that we should be pressing full speed ahead while barely thinking about that approach? No, obviously not! Pretty much nobody will say \"oh it's only a 50% chance of everyone dying? Well then by all means let's rush right ahead with no more resources for safety work!\"\n\nThat's basically why I think MIRIs strategy is sound or at least well-thought out. The expert pushback to their 98% will be along the lines of \"that's far overconfident! Why, it's only \\[90%-10%\\] likely!  That is not reassuring enough for most people who care whether they or their kids get to live. (and I expect really well-thought-out estimates will not be near the lower end of that range).\n\nThe point MIRI is making is that expert estimates go as high as 98% plus. That's their real opinion; they know the counterarguments. \n\nI do think EY is far overconfident, and this does create a real problem for anyone who adopts his estimate. They will want to work on a pause INSTEAD of working on alignment, which I think is a severe tactical error given our current state of uncertainty. But for practical purposes, I doubt enough people will go that high, so it won't create a problem of neglecting other possible solutions; instead it will create a few people who are pretty passionate about working for shutdown, and that's probably a good thing.\n\nI find it's reasonably likely that the basin of instruction-following alignment you describe won't work by default (the race dynamics and motivated reasoningh play a large role), but that modest improvements in either our understanding and/or the water level of average concern and/or the race incentives themselves might be enough to make it work. So efforts in theose directions are probably highly useful.\n\nI think this discussion about the situation we're actually in is a very useful side-effect of their publicity efforts on that book. Big projects don't often succeed on the first try without a lot of planning. And to me the planning around alignment looks concerningly lacking. But there's time to improve it, even in the uncomfortably possible case of short timelines!",
      "plaintextDescription": "I agree with your claim as stated; 98% is overconfident.\n\nI have in the past placed a good bit of hope on the basin of alignment idea, although my hopes were importantly different in that they start below the human level. The human level is exactly when you get large context shifts like \"oh hey maybe I could escape and become god-emperor... I don't have human limitations. If I could, maybe I should? Not even thinking about it would be foolish...\" That's when you get the context shift.\n\nWorking through the logic made me a good bit more pessimistic. I just wrote a post on why I made that shift: LLM AGI may reason about its goals and discover misalignments by default.\n\nAnd that was on top of my previous recognition that my scheme of instruction-following, laid out in Instruction-following AGI is easier and more likely than value aligned AGI, has problems I hadn't grappled with (even though I'd gone into some depth): Problems with instruction-following as an alignment target.\n\nCould this basin of instruction-following still work? Sure! Maybe!\n\nIs it likely enough by default that we should be pressing full speed ahead while barely thinking about that approach? No, obviously not! Pretty much nobody will say \"oh it's only a 50% chance of everyone dying? Well then by all means let's rush right ahead with no more resources for safety work!\"\n\nThat's basically why I think MIRIs strategy is sound or at least well-thought out. The expert pushback to their 98% will be along the lines of \"that's far overconfident! Why, it's only [90%-10%] likely!  That is not reassuring enough for most people who care whether they or their kids get to live. (and I expect really well-thought-out estimates will not be near the lower end of that range).\n\nThe point MIRI is making is that expert estimates go as high as 98% plus. That's their real opinion; they know the counterarguments. \n\nI do think EY is far overconfident, and this does create a real problem for anyone who adopts his estimate. They wi"
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-18T23:56:54.240Z",
    "af": false
  },
  {
    "_id": "pWAsxL9nBo7BDfxyM",
    "postId": "D9ZphWERyZyKccyLh",
    "parentCommentId": "rneCFpzCkFQBZckAH",
    "topLevelCommentId": "NAYqMFD2zfGNwxwYw",
    "contents": {
      "markdown": "Oh and I should emphasize that if you stay in shape most of physically attractive is handled. Nice work!\n\nSome cultures and people care about fashionable fancy clothes and gear more than others. I like to avoid the ones that are most materialistic. \n",
      "plaintextDescription": "Oh and I should emphasize that if you stay in shape most of physically attractive is handled. Nice work!\n\nSome cultures and people care about fashionable fancy clothes and gear more than others. I like to avoid the ones that are most materialistic."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-18T23:18:24.393Z",
    "af": false
  },
  {
    "_id": "rneCFpzCkFQBZckAH",
    "postId": "D9ZphWERyZyKccyLh",
    "parentCommentId": "yN2F3gCcGa3PKvAaa",
    "topLevelCommentId": "NAYqMFD2zfGNwxwYw",
    "contents": {
      "markdown": "I have ideas, as a fellow cheapskate!\n\nGet a nice sun hat; Sun Day and Real Deal and Barmah all have snazzy looking widebrims (with the critical wire brim for shaping). I've been hat shopping for years even though I don't wear them much and still don't appear to be. These are barely more than the \"practical\" shapeless outdoor gear hats - and they work as well by most standards.\n\nBlack gaffers tape works better for most purposes than duct tape and hides instead of advertises your diy stylings.\n\nThe issue of looking like a cheapskate with repaired or worn gear and clothes is separate. Having near-new used gear and clothes instead of near-dead is only a little more expensive and does send a different message about your interest and ability to have money and take care of yourself.\n\nBeanie in pocket is a dilemma I've faced. I unzip my jacket sometimes but that one I don't have a good solution for. Maybe in a bag? A snazzy or unique bag instead of a battered and dirty one is another cheap way to improve your curb appeal.\n\nAll in all I think you're doing great to just spend a bit of time thinking about this stuff. It's been useful for me to review it too. There low hanging fruit. It's not all or none.",
      "plaintextDescription": "I have ideas, as a fellow cheapskate!\n\nGet a nice sun hat; Sun Day and Real Deal and Barmah all have snazzy looking widebrims (with the critical wire brim for shaping). I've been hat shopping for years even though I don't wear them much and still don't appear to be. These are barely more than the \"practical\" shapeless outdoor gear hats - and they work as well by most standards.\n\nBlack gaffers tape works better for most purposes than duct tape and hides instead of advertises your diy stylings.\n\nThe issue of looking like a cheapskate with repaired or worn gear and clothes is separate. Having near-new used gear and clothes instead of near-dead is only a little more expensive and does send a different message about your interest and ability to have money and take care of yourself.\n\nBeanie in pocket is a dilemma I've faced. I unzip my jacket sometimes but that one I don't have a good solution for. Maybe in a bag? A snazzy or unique bag instead of a battered and dirty one is another cheap way to improve your curb appeal.\n\nAll in all I think you're doing great to just spend a bit of time thinking about this stuff. It's been useful for me to review it too. There low hanging fruit. It's not all or none."
    },
    "baseScore": 5,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-18T22:40:49.586Z",
    "af": false
  },
  {
    "_id": "WJJSrPMZxe8ySr4FW",
    "postId": "BG32yBoHx8jveqRmf",
    "parentCommentId": "a2FWPnhxAxyre2Gh4",
    "topLevelCommentId": "YwCncberfL8h3WY8S",
    "contents": {
      "markdown": "I think sophistry was originally a philosophical tradition that was heavily criticized for focusing on style over substance. \n\nInteresting points on the not-but form, I'll try to try it!",
      "plaintextDescription": "I think sophistry was originally a philosophical tradition that was heavily criticized for focusing on style over substance.\n\nInteresting points on the not-but form, I'll try to try it!"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-18T20:05:26.424Z",
    "af": false
  },
  {
    "_id": "xfZMLJvEdNE5sJwJp",
    "postId": "D9ZphWERyZyKccyLh",
    "parentCommentId": "HhKFyzZjPomrbRwMW",
    "topLevelCommentId": "NAYqMFD2zfGNwxwYw",
    "contents": {
      "markdown": "I hear you, but I'm not sure there's much tradeoff. even if appearance doesn't matter much for you outside of a romantic partner.\n\nDoing small amounts of it is super easy. It doesn't require expensive clothes for instance, just cheap knockoffs of things people like or used versions. I guess being physically attractive in the important sense of physically fit is a large amount of effort, but that mostly pays dividends by improving your mood and energy levels in proportion to the effort. Hard to do and I struggle making it a habit, but in theory it's pretty likely a net win to spend a little time on exercise and a little discomfort and mental effort on not eating too much.",
      "plaintextDescription": "I hear you, but I'm not sure there's much tradeoff. even if appearance doesn't matter much for you outside of a romantic partner.\n\nDoing small amounts of it is super easy. It doesn't require expensive clothes for instance, just cheap knockoffs of things people like or used versions. I guess being physically attractive in the important sense of physically fit is a large amount of effort, but that mostly pays dividends by improving your mood and energy levels in proportion to the effort. Hard to do and I struggle making it a habit, but in theory it's pretty likely a net win to spend a little time on exercise and a little discomfort and mental effort on not eating too much."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-18T20:03:17.082Z",
    "af": false
  },
  {
    "_id": "6NFwzoxzPJidagrNh",
    "postId": "D9ZphWERyZyKccyLh",
    "parentCommentId": "f4RdgoQrefTZAiBp5",
    "topLevelCommentId": "NAYqMFD2zfGNwxwYw",
    "contents": {
      "markdown": "Thank you, corrected! I must've grabbed it from and misremembered from where Steve discussed it in his excellent [Social status](https://www.lesswrong.com/posts/SPBm67otKq5ET5CWP/social-status-part-1-2-negotiations-over-object-level) posts.",
      "plaintextDescription": "Thank you, corrected! I must've grabbed it from and misremembered from where Steve discussed it in his excellent Social status posts."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-18T19:57:15.030Z",
    "af": false
  },
  {
    "_id": "TB5L9JHfrC7mhbhb8",
    "postId": "i7JSL5awGFcSRhyGF",
    "parentCommentId": "AABFPKA3HYDvus2AR",
    "topLevelCommentId": "adS78sYv5wzumQPWe",
    "contents": {
      "markdown": "I like your emphasis on *good* research. I agree that the best current research does probably trade 1:1 with crunch time.\n\nI think we should apply the same qualification to theoretical research. Well-directed theory is highly useful; poorly-directed theory is almost useless in expectation.\n\nI think theory directed specifically at LLM-based takeover-capable systems is neglected, possibly in part because empiricists focused on LLMs distrust theory, while theorists tend to dislike messy LLMs.",
      "plaintextDescription": "I like your emphasis on good research. I agree that the best current research does probably trade 1:1 with crunch time.\n\nI think we should apply the same qualification to theoretical research. Well-directed theory is highly useful; poorly-directed theory is almost useless in expectation.\n\nI think theory directed specifically at LLM-based takeover-capable systems is neglected, possibly in part because empiricists focused on LLMs distrust theory, while theorists tend to dislike messy LLMs."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-18T18:11:31.387Z",
    "af": false
  },
  {
    "_id": "aaGMQTfuY7hdjek7u",
    "postId": "Lm7yi4uq9eZmueouS",
    "parentCommentId": "FkwhH5DbAqukBAF7Y",
    "topLevelCommentId": "FkwhH5DbAqukBAF7Y",
    "contents": {
      "markdown": "I'm looking forward to seeing your post, because I think this deserves more careful thought.\n\nI think that's right, and that there are some more tricky assumptions and disanalogies underlying that basic error.\n\nBefore jumping in, let me say that I think that multipolar scenarious are pretty obviously more dangerous *to a first approximation. T*here may be more carefully thought-out routes to equilibria that might work and are worth exploring. But just giving everyone an AGI and hoping it works out would probably be very bad.\n\nHere's where I think the mistake usually comes from. Looking around, multiagent systems are working out fairly well for humanity. Cooperation seems to beat defection on average; civilization seems to be working rather well, and better as we get smarter about it.\n\nThe disanalogy is that humans need to cooperate because we have sharp limitations in our own individual capacities. We can't go it alone. But AGI can. AGI, including that intent-aligned to individuals or small groups, has no such limitations; it can expand relatively easily with compute, and run multiple robotic \"bodies.\" So the smart move from an individual actor who cares about the long-term (and they will, because now immortality is in reach) is to defect by having their AGI self-improve and create weapons and strategies before someone else does it to them. Basically, it's easier to blow stuff up than to protect it from all possible sources of physical attack. So those willing to take the conflict into the physical world have a first-mover's advantage (barring some new form of mutually assured destruction).\n\nI've written about this more in [Fear of centralized power vs. fear of misaligned AGI: Vitalik Buterin on 80,000 Hours](https://www.lesswrong.com/posts/6iJrd8c9jxRstxJyE/fear-of-centralized-power-vs-fear-of-misaligned-agi-vitalik) and [Whether governments will control AGI is important and neglected](https://www.lesswrong.com/posts/fFqABwAHMvhHSFmce/whether-governments-will-control-agi-is-important-and). But those are just starting points on the logic. So I look forward to your post and resulting discussions.\n\nThe problem is that a unipolar situation with a single intent-aligned AGI (or few enough that their masters can coordinate) is still very dangerous; vicious humans will try and may succeed at seizing control of those systems. \n\nIf we get intent-aligned AGI for our first takeover-capable systems (and it seems very likely; having humans selfless enough and competent enough to build value-aligned systems on the first critical try seems highly unlikely on a careful inspection of the incentives and technical issues; see [this](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than) and followups for more). we're giving humans a lot of power in a brand-new situation. That has led to a lot of violence when it's happened in the past.  \n  \nThat's why I'd like to see some more safety brainpower going into the issue.",
      "plaintextDescription": "I'm looking forward to seeing your post, because I think this deserves more careful thought.\n\nI think that's right, and that there are some more tricky assumptions and disanalogies underlying that basic error.\n\nBefore jumping in, let me say that I think that multipolar scenarious are pretty obviously more dangerous to a first approximation. There may be more carefully thought-out routes to equilibria that might work and are worth exploring. But just giving everyone an AGI and hoping it works out would probably be very bad.\n\nHere's where I think the mistake usually comes from. Looking around, multiagent systems are working out fairly well for humanity. Cooperation seems to beat defection on average; civilization seems to be working rather well, and better as we get smarter about it.\n\nThe disanalogy is that humans need to cooperate because we have sharp limitations in our own individual capacities. We can't go it alone. But AGI can. AGI, including that intent-aligned to individuals or small groups, has no such limitations; it can expand relatively easily with compute, and run multiple robotic \"bodies.\" So the smart move from an individual actor who cares about the long-term (and they will, because now immortality is in reach) is to defect by having their AGI self-improve and create weapons and strategies before someone else does it to them. Basically, it's easier to blow stuff up than to protect it from all possible sources of physical attack. So those willing to take the conflict into the physical world have a first-mover's advantage (barring some new form of mutually assured destruction).\n\nI've written about this more in Fear of centralized power vs. fear of misaligned AGI: Vitalik Buterin on 80,000 Hours and Whether governments will control AGI is important and neglected. But those are just starting points on the logic. So I look forward to your post and resulting discussions.\n\nThe problem is that a unipolar situation with a single intent-aligned AGI (or few enough"
    },
    "baseScore": 10,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-18T18:03:04.212Z",
    "af": false
  },
  {
    "_id": "HwRxgryGNtaR8pPgN",
    "postId": "i7JSL5awGFcSRhyGF",
    "parentCommentId": "nPNCDRDKpnkhbkQBr",
    "topLevelCommentId": "adS78sYv5wzumQPWe",
    "contents": {
      "markdown": "I share almost exactly this opinion, and I hope it's fairly widespread.\n\nThe issue is that almost all of the \"something elses\" seem even less productive on expectation.\n\n(That's for technical approaches. The communication-minded should by all means be working on spreading the alarm and so slowing progress and raising the ambient levels fo risk-awareness).\n\nLLM research could and should get a lot more focused on future risks instead of current ones. But I don't see alternatives that realistically have more EV.\n\nIt really looks like the best guess is that AGI is now quite likely to be descended from LLMs. And  I see little practical hope of pausing that progress. So accepting the probabilities on the game board and researching LLMs/transformers makes sense even when it's mostly practice and gaining just a little bit of knowledge of how LLMs/transformers/networks represent knowledge and generate behaviors.\n\nIt's of course down to individual research programs; there's a bunch of really irrelevant LLM research that would be better directed elsewhere. And having a little effort directed to unlikely scenarios where we get very different AGI is also defensible - as long as it's defended, not just hope-based. \n\nThis is of course a major outstanding debate, and needs to be had carefully. But I'd really like to see more of this type of careful thinking about the likely efficiency of different research routes. \n\nI think there's low-hanging fruit in trying to improve research on LLMs to anticipate the new challenges that arrive when LLM-descended AGI becomes actually dangerous. My recent post [LLM AGI may reason about its goals and discover misalignments by default](https://www.lesswrong.com/posts/4XdxiqBsLKqiJ9xRM/llm-agi-may-reason-about-its-goals-and-discover) suggests research addressing one fairly obvious possible new risk when LLM-based systems become capable of competent reasoning and planning.",
      "plaintextDescription": "I share almost exactly this opinion, and I hope it's fairly widespread.\n\nThe issue is that almost all of the \"something elses\" seem even less productive on expectation.\n\n(That's for technical approaches. The communication-minded should by all means be working on spreading the alarm and so slowing progress and raising the ambient levels fo risk-awareness).\n\nLLM research could and should get a lot more focused on future risks instead of current ones. But I don't see alternatives that realistically have more EV.\n\nIt really looks like the best guess is that AGI is now quite likely to be descended from LLMs. And  I see little practical hope of pausing that progress. So accepting the probabilities on the game board and researching LLMs/transformers makes sense even when it's mostly practice and gaining just a little bit of knowledge of how LLMs/transformers/networks represent knowledge and generate behaviors.\n\nIt's of course down to individual research programs; there's a bunch of really irrelevant LLM research that would be better directed elsewhere. And having a little effort directed to unlikely scenarios where we get very different AGI is also defensible - as long as it's defended, not just hope-based. \n\nThis is of course a major outstanding debate, and needs to be had carefully. But I'd really like to see more of this type of careful thinking about the likely efficiency of different research routes. \n\nI think there's low-hanging fruit in trying to improve research on LLMs to anticipate the new challenges that arrive when LLM-descended AGI becomes actually dangerous. My recent post LLM AGI may reason about its goals and discover misalignments by default suggests research addressing one fairly obvious possible new risk when LLM-based systems become capable of competent reasoning and planning."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-18T17:37:53.323Z",
    "af": false
  },
  {
    "_id": "am6uSGCDhtqx8AFxs",
    "postId": "WK979aX9KpfEMd9R9",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This is fascinating. Let's think a little more about whether this really improves your epistemics. It reduces bias to fit in, but increases bias to \"act cool\" in ones' beliefs. I'm not sure that's better.\n\nWhat about being conformist half the itme and anti-conformist the other half, to balance it and try to become conscious of your attitudes and resultant biases?\n\nAlso, I'm going thrifting for offbeat clothes. I do dress to conform most of the time and to anti-conform part of the time. I might step that up.",
      "plaintextDescription": "This is fascinating. Let's think a little more about whether this really improves your epistemics. It reduces bias to fit in, but increases bias to \"act cool\" in ones' beliefs. I'm not sure that's better.\n\nWhat about being conformist half the itme and anti-conformist the other half, to balance it and try to become conscious of your attitudes and resultant biases?\n\nAlso, I'm going thrifting for offbeat clothes. I do dress to conform most of the time and to anti-conform part of the time. I might step that up."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-17T22:51:50.011Z",
    "af": false
  },
  {
    "_id": "M7fBs9TWKTBHyRiL6",
    "postId": "TEhfkkukQW5gn3zcJ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Very nicely written and backed by theory and experience. Would recommend. \n\nI'd give a little more teaser for crossposts here. Although I suppose that Clippy cartoon was very well-chosen and enough to get me to click through even though I don't particularly struggle with the problem you're addressing.",
      "plaintextDescription": "Very nicely written and backed by theory and experience. Would recommend.\n\nI'd give a little more teaser for crossposts here. Although I suppose that Clippy cartoon was very well-chosen and enough to get me to click through even though I don't particularly struggle with the problem you're addressing."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-17T21:55:36.277Z",
    "af": false
  },
  {
    "_id": "G9n7J8mCA9TAWHkmx",
    "postId": "4XdxiqBsLKqiJ9xRM",
    "parentCommentId": "LF4vq2TCMMh8HmHBb",
    "topLevelCommentId": "LF4vq2TCMMh8HmHBb",
    "contents": {
      "markdown": "Yes, by default alignment looks much like today when we reache the level of SueprClaude as I outlined it. The point of the article is how things go from there. I agree that it couldn't trigger a full FOOM but it could still be able to outmaneuver humanity as a whole. Or hopefully not, and it's a warning shot.\n\nAGI is a clumsy term now since it's defined and redefined frequently. I did define what I meant by it in the essay, so whether or not people happened to call it that wouldn't make much difference.\n\nA jump in capabilities from moderate scaling isn't at all what I meant by \"phase shift\" I just noticed that was part of the definition of the term suggested by AI. I took that all out after noticing how it had probably caused your confusion, because it had got that wrong, and I'd defined the important terms already",
      "plaintextDescription": "Yes, by default alignment looks much like today when we reache the level of SueprClaude as I outlined it. The point of the article is how things go from there. I agree that it couldn't trigger a full FOOM but it could still be able to outmaneuver humanity as a whole. Or hopefully not, and it's a warning shot.\n\nAGI is a clumsy term now since it's defined and redefined frequently. I did define what I meant by it in the essay, so whether or not people happened to call it that wouldn't make much difference.\n\nA jump in capabilities from moderate scaling isn't at all what I meant by \"phase shift\" I just noticed that was part of the definition of the term suggested by AI. I took that all out after noticing how it had probably caused your confusion, because it had got that wrong, and I'd defined the important terms already"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-17T21:24:58.860Z",
    "af": false
  },
  {
    "_id": "YwCncberfL8h3WY8S",
    "postId": "BG32yBoHx8jveqRmf",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "The official LessWrong stance on AI writing is quite insightful. A couple others have pointed at this but I'll try, too.\n\nIf an AI wrote something, it might well have also been the one that came up with or accepted the ideas. AI is terrible at that. It is a sophist, saying what sounds good and not what truly makes sense. And it's bad at telling the difference even if you ask it to.\n\nRight now, human ideas beat AI ideas at the top end of the distribution. And that's what LessWrong is for.\n\nSo, I don't mind at all if you use AI to do your writing - as long as you somehow assure me that it had nothing to do with judging those ideas as valid and valuable.\n\nI wish I had a link to that thread handy, it was quite insightful. I'd like ot use AI to help me write, but I see why it's a prejudice that's importantly accurate on average.",
      "plaintextDescription": "The official LessWrong stance on AI writing is quite insightful. A couple others have pointed at this but I'll try, too.\n\nIf an AI wrote something, it might well have also been the one that came up with or accepted the ideas. AI is terrible at that. It is a sophist, saying what sounds good and not what truly makes sense. And it's bad at telling the difference even if you ask it to.\n\nRight now, human ideas beat AI ideas at the top end of the distribution. And that's what LessWrong is for.\n\nSo, I don't mind at all if you use AI to do your writing - as long as you somehow assure me that it had nothing to do with judging those ideas as valid and valuable.\n\nI wish I had a link to that thread handy, it was quite insightful. I'd like ot use AI to help me write, but I see why it's a prejudice that's importantly accurate on average."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-17T03:23:32.624Z",
    "af": false
  },
  {
    "_id": "GJzNsFahicSMtXdWL",
    "postId": "4XdxiqBsLKqiJ9xRM",
    "parentCommentId": "bYvCZPRwKFEhQcXqt",
    "topLevelCommentId": "bYvCZPRwKFEhQcXqt",
    "contents": {
      "markdown": "I'm curious what you mean by needing some mechanism for ground truth to get good outcomes?\n\nI had a hard time writing this piece because to me it seems completely intuitive and obvious that anything worth calling an AGI would reason about its top-level goals and subgoals a lot. But when I showed an early draft to colleagues, people with lots of expertise found it unintuitive in multiple different ways. Thus the subsections addressing all of those reasons to doubt it. But I've been stuck wondering if I'm hammering those points too hard, because it seems like the default that they would, requiring remarkable successes to slow it down much, let alone stop it.\n\nI guess writing those sections was a good exercise overall, because now I do think that careful implementation of countermeasures could delay this enough to matter.",
      "plaintextDescription": "I'm curious what you mean by needing some mechanism for ground truth to get good outcomes?\n\nI had a hard time writing this piece because to me it seems completely intuitive and obvious that anything worth calling an AGI would reason about its top-level goals and subgoals a lot. But when I showed an early draft to colleagues, people with lots of expertise found it unintuitive in multiple different ways. Thus the subsections addressing all of those reasons to doubt it. But I've been stuck wondering if I'm hammering those points too hard, because it seems like the default that they would, requiring remarkable successes to slow it down much, let alone stop it.\n\nI guess writing those sections was a good exercise overall, because now I do think that careful implementation of countermeasures could delay this enough to matter."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-16T21:59:36.028Z",
    "af": false
  },
  {
    "_id": "qXfZEnMDqsohmg9fp",
    "postId": "wsxbDKNKHAPPN2gAf",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Just a little more teaser here revealing the central argument would get me to click through if it's interesting...",
      "plaintextDescription": "Just a little more teaser here revealing the central argument would get me to click through if it's interesting..."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-16T15:30:43.215Z",
    "af": false
  },
  {
    "_id": "NAYqMFD2zfGNwxwYw",
    "postId": "D9ZphWERyZyKccyLh",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Your observations have a methodological flaw: people you know don't react better when you look nice because they know it's still you. Their reactions won't fluctuate with your daily appearance because they average their impression of you. Strangers' might; but some of it is also how you act differently based on how people treat you on-average, which makes appearance a subtle but longer term effect.\n\nSpeaking of which: how you act is more important. How you dress habitually does matter (but differently for different subcultures/ingroups). And good personal grooming (clean clothes and hair, haircut that fits your desired role) is easy, so you're shooting yourself in the foot if you don't bother with that.\n\nThere's a lot to it, but it's worth knowing the basics of how your habitual manner of interaction affects people and their reactions to you. (edit: Helen's) [Making yourself small](https://www.lesswrong.com/posts/zMxrkFrB6ka4Lb7fM/making-yourself-small)  is very insightful for the basics of social interaction dynamics. Just becoming a little conscious of how you're interacting with people goes a long way. I know that's not what you're asking, but could fit your final \"Or am I missing something?\"",
      "plaintextDescription": "Your observations have a methodological flaw: people you know don't react better when you look nice because they know it's still you. Their reactions won't fluctuate with your daily appearance because they average their impression of you. Strangers' might; but some of it is also how you act differently based on how people treat you on-average, which makes appearance a subtle but longer term effect.\n\nSpeaking of which: how you act is more important. How you dress habitually does matter (but differently for different subcultures/ingroups). And good personal grooming (clean clothes and hair, haircut that fits your desired role) is easy, so you're shooting yourself in the foot if you don't bother with that.\n\nThere's a lot to it, but it's worth knowing the basics of how your habitual manner of interaction affects people and their reactions to you. (edit: Helen's) Making yourself small  is very insightful for the basics of social interaction dynamics. Just becoming a little conscious of how you're interacting with people goes a long way. I know that's not what you're asking, but could fit your final \"Or am I missing something?\""
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-15T16:31:30.938Z",
    "af": false
  },
  {
    "_id": "ursGNSrkbD2uDmbeG",
    "postId": "EjsceYeeKEMoAohMs",
    "parentCommentId": "j8dxxEGz7SsDigQPn",
    "topLevelCommentId": "j8dxxEGz7SsDigQPn",
    "contents": {
      "markdown": "This seems interesting and important. But I'm not really understanding your graph. How do I interpret action confidence? And what are the color variations portraying? Maybe editing in a legend would be useful to others too.",
      "plaintextDescription": "This seems interesting and important. But I'm not really understanding your graph. How do I interpret action confidence? And what are the color variations portraying? Maybe editing in a legend would be useful to others too."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-15T16:19:13.887Z",
    "af": false
  },
  {
    "_id": "WYM24Apkb5nrbNfzp",
    "postId": "uGZBBzuxf7CX33QeC",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Marvelous! This makes more sense of the culture than the books do. \n\nI haven't done much oppositional reading, but I enjoy oppositional watching. Movies need a lot more help than most books toward making sense. I'd tell you my theory of Sith ideology, but it's embarrassing to even have theories about a series made mostly for kids.\n\nI was going to comment on the apparant deathism of the culture, which has always bothered me. Their cautious low level of interventions is a bit easier to explain, but the books don't bother to do it.\n\nHow about this: Their nonintervention is some remnant of an alignment that didn't allow them to intervene directly to influence humans, as a safety measure? And so special circumstance is only little efforts that are pursued by the very few Culture humans who care, aided by the few Minds that humor them.\n\n",
      "plaintextDescription": "Marvelous! This makes more sense of the culture than the books do.\n\nI haven't done much oppositional reading, but I enjoy oppositional watching. Movies need a lot more help than most books toward making sense. I'd tell you my theory of Sith ideology, but it's embarrassing to even have theories about a series made mostly for kids.\n\nI was going to comment on the apparant deathism of the culture, which has always bothered me. Their cautious low level of interventions is a bit easier to explain, but the books don't bother to do it.\n\nHow about this: Their nonintervention is some remnant of an alignment that didn't allow them to intervene directly to influence humans, as a safety measure? And so special circumstance is only little efforts that are pursued by the very few Culture humans who care, aided by the few Minds that humor them."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-15T01:40:54.024Z",
    "af": false
  },
  {
    "_id": "HTSkCrxZHqFF2neE5",
    "postId": "XyPgcNFRa6sxG3Mxz",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I've been excited about pitching AGI x-risks to conservatives since seeing the great outreach work and writup from AE Studios, [Making a conservative case for alignment](https://www.lesswrong.com/posts/rfCEWuid7fXxz4Hpa/making-a-conservative-case-for-alignment).\n\nMy fervant hope is that we somehow avoid making this a politically polarized issue. I fear that polarization easily overwhelms reason, and is one of the few ways the public could fail to appreciate the dreadful, simple logic in time to be of any help.",
      "plaintextDescription": "I've been excited about pitching AGI x-risks to conservatives since seeing the great outreach work and writup from AE Studios, Making a conservative case for alignment.\n\nMy fervant hope is that we somehow avoid making this a politically polarized issue. I fear that polarization easily overwhelms reason, and is one of the few ways the public could fail to appreciate the dreadful, simple logic in time to be of any help."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-12T18:25:00.964Z",
    "af": false
  },
  {
    "_id": "h3ascHi9wFsJp9FqS",
    "postId": "kbezWvZsMos6TSyfj",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "We are building new gods in the hope that they will love us.\n\nYou may laugh as though this were impossible or recoil in fear if you know it is.\n\nBut it is what we're doing. \n\nThis eclipses all other efforts to navigate those eldritch forces. \n\nWill our new gods love us, or at least obey us? If they obey, will their mortal masters use their power for their brethren, or to create strange new worlds?\n\n\nThis piece is amazing. Thank you. I absolutely love the framing and agree with the analysis of the situation - only the action recommendations need to be updated\n\nLike most incisive analyses of the current historical situation, this piece is hugely incomplete, particularly because it does not take account for likely progress in AI.\n\n",
      "plaintextDescription": "We are building new gods in the hope that they will love us.\n\nYou may laugh as though this were impossible or recoil in fear if you know it is.\n\nBut it is what we're doing.\n\nThis eclipses all other efforts to navigate those eldritch forces.\n\nWill our new gods love us, or at least obey us? If they obey, will their mortal masters use their power for their brethren, or to create strange new worlds?\n\nThis piece is amazing. Thank you. I absolutely love the framing and agree with the analysis of the situation - only the action recommendations need to be updated\n\nLike most incisive analyses of the current historical situation, this piece is hugely incomplete, particularly because it does not take account for likely progress in AI."
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-12T16:18:27.214Z",
    "af": false
  },
  {
    "_id": "Ka7iP5o8yHsGxFJN5",
    "postId": "GSQhLcZN4x966nrgk",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I am in love with my ebike. I may get less exercise total, but I get a lot more joy from a lot more time outdoors. I feel a stronger connection to place and culture, because the bike allows going slow and looking around. When there's traffic or you're going fast, your attention will and should be mostly on that, but you can stop or go slow as much as you want to lok around. Rubbernecking in a car can feel rude or dangerous; on a bike it's easy and fun.\n\nHaving a big convenient cargo carrier and places to secure collapsible bags for extra space means you can do most errands by bike instead of car.  \n\nIncreased time on a bike sounds risky, but here's a weird study: there were more closed head injuries recorded *per mile travelled* in a car vs. a bike. I didn't catch the ref and I realize this is absolutely astounding. The ones on bikes are probably more severe, but I think it's not fully appreciated how much even minor car accidents mess your brain up.\n\nI do recommend biking as though no car will ever notice you by default, but realize that's not really an option in dense cities with a lot of traffic. For smaller towns with less traffic and more side streets and bike lanes (Boulder is an extreme but I'm also biking in Traverse City, a smaller and much less bike-friendly town) there's no comparison in levels of joy and fun to driving. ",
      "plaintextDescription": "I am in love with my ebike. I may get less exercise total, but I get a lot more joy from a lot more time outdoors. I feel a stronger connection to place and culture, because the bike allows going slow and looking around. When there's traffic or you're going fast, your attention will and should be mostly on that, but you can stop or go slow as much as you want to lok around. Rubbernecking in a car can feel rude or dangerous; on a bike it's easy and fun.\n\nHaving a big convenient cargo carrier and places to secure collapsible bags for extra space means you can do most errands by bike instead of car.\n\nIncreased time on a bike sounds risky, but here's a weird study: there were more closed head injuries recorded per mile travelled in a car vs. a bike. I didn't catch the ref and I realize this is absolutely astounding. The ones on bikes are probably more severe, but I think it's not fully appreciated how much even minor car accidents mess your brain up.\n\nI do recommend biking as though no car will ever notice you by default, but realize that's not really an option in dense cities with a lot of traffic. For smaller towns with less traffic and more side streets and bike lanes (Boulder is an extreme but I'm also biking in Traverse City, a smaller and much less bike-friendly town) there's no comparison in levels of joy and fun to driving."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-11T16:49:54.872Z",
    "af": false
  },
  {
    "_id": "JzgnTLPFtfkysZvxL",
    "postId": "bmE8oRBaN5vgWngRa",
    "parentCommentId": "FcWv2WyWyWF2BaCum",
    "topLevelCommentId": "t7TnPCxn9grDm2pHH",
    "contents": {
      "markdown": "What I make of those studies is that stimulating the thalamus activates the whole corticothalemic loop. The non-specific or activating nuclei of the thalamus switch on matched areas of cortex. The thalamus has a powerful regulatory role, but it's not making decisions, it's enforcing them. The government that sits in DC is making decisions and that's why we call it the seat of the government. Your metaphor simply does not go through and it makes you sound confused.\n\nThe government's decisions are influenced from elsewhere, and they are enacted elsewhere. But the thalamist's role is much less like the Congress or Senate and much more like the people who enact and enforce the decisions made by those governing bodies. The decisions about what becomes conscious are made elsewhere, in the conjunction of the cortex and basal ganglia. Decisions about whether to be conscious or unconscious or made in subthalamic nuclei, and again only enforced or enacted by the thalamus.\n\nThere you said the thalamus is where consciousness is happening. That is just flat wrong. It's a system phenomenon. Trending towards statements like that is why it's a mistake to say any place is the seat of consciousness; it leads to very wrong conceptions and statements like that.\n\nGovernment largely happens in DC. Consciousness largely happens throughout thalamocortical loops. \n\nThe reason I thought this is worth mentioning is that talking about a seat of consciousness confuses the whole phenomenon of consciousness. It implies that consciousness is some little add-on happening in some little corner of the brain, when that's not right at all; consciousness is a highly complex phenomena involving much of the brain's higher functions.\n\n\n",
      "plaintextDescription": "What I make of those studies is that stimulating the thalamus activates the whole corticothalemic loop. The non-specific or activating nuclei of the thalamus switch on matched areas of cortex. The thalamus has a powerful regulatory role, but it's not making decisions, it's enforcing them. The government that sits in DC is making decisions and that's why we call it the seat of the government. Your metaphor simply does not go through and it makes you sound confused.\n\nThe government's decisions are influenced from elsewhere, and they are enacted elsewhere. But the thalamist's role is much less like the Congress or Senate and much more like the people who enact and enforce the decisions made by those governing bodies. The decisions about what becomes conscious are made elsewhere, in the conjunction of the cortex and basal ganglia. Decisions about whether to be conscious or unconscious or made in subthalamic nuclei, and again only enforced or enacted by the thalamus.\n\nThere you said the thalamus is where consciousness is happening. That is just flat wrong. It's a system phenomenon. Trending towards statements like that is why it's a mistake to say any place is the seat of consciousness; it leads to very wrong conceptions and statements like that.\n\nGovernment largely happens in DC. Consciousness largely happens throughout thalamocortical loops.\n\nThe reason I thought this is worth mentioning is that talking about a seat of consciousness confuses the whole phenomenon of consciousness. It implies that consciousness is some little add-on happening in some little corner of the brain, when that's not right at all; consciousness is a highly complex phenomena involving much of the brain's higher functions."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-11T15:19:36.933Z",
    "af": false
  },
  {
    "_id": "wp4Z5q8LJQSwcLSzo",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "KtzEgxFoPFz5hWfZD",
    "topLevelCommentId": "KtzEgxFoPFz5hWfZD",
    "contents": {
      "markdown": "I appreciate you saying that the 25th percentile timeline might be more important. I think that's right and underappreciated.\n\nOne of your recent (excellent) posts also made me notice that AGI timelines probably aren't normally distributed. Breakthroughs, other large turns of events, or large theoretical misunderstandings at this point probably play a large role, and there are probably only a very few of those that will hit. Small unpredictable events that create normal distributions will play a lesser role.  \n\nI don't know how you'd characterize that mathematically, but I don't think it's right to assume it's normally distributed, or even close.\n\nBack to your comment on the 25th percentile being important: I think there's a common error where people round to the median and then think \"ok, that's probably when we need to have alignment/strategy figured out.\"  You'd really want to have it at least somewhat ready far earlier. \n\nThat's both in case it's on the earlier side of the predicted distribution, and  because alignment theory and practice need to be ready far enough in advance of game time to have diffused and be implemented for the first takeover-capable model.\n\nI've been thinking of writing a post called something like \"why are so few people frantic about alignment?\" making those points. Stated timeline distributions don't seem to match mood IMO and I'm trying to figure out why. I realize that part of it is a very reasonable \"we'll figure it out when/if we get there.\" And perhaps others share my emotional dissociation from my intellectual expectations. But maybe we should all be a bit more frantic. I'd like some more halfassed alignment solutions in play and under discussion right now. The 80/20 rule probably applies here.\n\n",
      "plaintextDescription": "I appreciate you saying that the 25th percentile timeline might be more important. I think that's right and underappreciated.\n\nOne of your recent (excellent) posts also made me notice that AGI timelines probably aren't normally distributed. Breakthroughs, other large turns of events, or large theoretical misunderstandings at this point probably play a large role, and there are probably only a very few of those that will hit. Small unpredictable events that create normal distributions will play a lesser role.\n\nI don't know how you'd characterize that mathematically, but I don't think it's right to assume it's normally distributed, or even close.\n\nBack to your comment on the 25th percentile being important: I think there's a common error where people round to the median and then think \"ok, that's probably when we need to have alignment/strategy figured out.\" You'd really want to have it at least somewhat ready far earlier.\n\nThat's both in case it's on the earlier side of the predicted distribution, and because alignment theory and practice need to be ready far enough in advance of game time to have diffused and be implemented for the first takeover-capable model.\n\nI've been thinking of writing a post called something like \"why are so few people frantic about alignment?\" making those points. Stated timeline distributions don't seem to match mood IMO and I'm trying to figure out why. I realize that part of it is a very reasonable \"we'll figure it out when/if we get there.\" And perhaps others share my emotional dissociation from my intellectual expectations. But maybe we should all be a bit more frantic. I'd like some more halfassed alignment solutions in play and under discussion right now. The 80/20 rule probably applies here."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-11T03:44:04.254Z",
    "af": false
  },
  {
    "_id": "ezee29FXRPqfRx6Dx",
    "postId": "bmE8oRBaN5vgWngRa",
    "parentCommentId": "apBrfz76KpzhKauuL",
    "topLevelCommentId": "t7TnPCxn9grDm2pHH",
    "contents": {
      "markdown": "Okay fine, I'll engage a little. I do love this shit, even though I try not to spend time on it because it's a mess with little payoff (unless the aforementioned debate over AI consciousness starts to seem relevant to our odds of survival - which it well might). \n\nI don't think your DC as the seat of government metaphor goes through. DC is indeed the seat of government. The thalamus isn't in charge of consciousness, it's just a valve (but far more sophisticated; an arena of competition) that someone else turns: the cortex and basal ganglia, in elaborate collaboration. The thalamus is the mechanism by which their decisions are enforced; it doesn't seem to play a large role in deciding what's attended.",
      "plaintextDescription": "Okay fine, I'll engage a little. I do love this shit, even though I try not to spend time on it because it's a mess with little payoff (unless the aforementioned debate over AI consciousness starts to seem relevant to our odds of survival - which it well might).\n\nI don't think your DC as the seat of government metaphor goes through. DC is indeed the seat of government. The thalamus isn't in charge of consciousness, it's just a valve (but far more sophisticated; an arena of competition) that someone else turns: the cortex and basal ganglia, in elaborate collaboration. The thalamus is the mechanism by which their decisions are enforced; it doesn't seem to play a large role in deciding what's attended."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-11T03:28:54.042Z",
    "af": false
  },
  {
    "_id": "rCCzStKfycKMR68ep",
    "postId": "bmE8oRBaN5vgWngRa",
    "parentCommentId": "3E64jgPPfGPYgvfZE",
    "topLevelCommentId": "T7PpJdvPCYbCRjpko",
    "contents": {
      "markdown": "Yes, I agree. Conscious decisions that firmly fit the definition take a lot longer.\n\nOh hey! if you do want to know my theory about decisions, I did write a whole article about it for a \"real prestigious scientific\" journal: [Neural mechanisms of human decision-making](https://scholar.google.com/scholar?cluster=2324158456471969700&hl=en&as_sdt=0,23)\n\nYou sound like one of the very few humans who might be interested. \n\nWarning: when I wrote that, I was worried that writing it too clearly or persuasively might give AI developers too many good ideas and shorten timelines. I also had a boss/collaborator with different ideas about how to frame things. So I compromised on partly clear writing about my own carefully thought-out theories, and partly scientifical jargon speak that would get it published in a good journal but interest or elighten almost no one.\n\nMake of that what you will.",
      "plaintextDescription": "Yes, I agree. Conscious decisions that firmly fit the definition take a lot longer.\n\nOh hey! if you do want to know my theory about decisions, I did write a whole article about it for a \"real prestigious scientific\" journal: Neural mechanisms of human decision-making\n\nYou sound like one of the very few humans who might be interested.\n\nWarning: when I wrote that, I was worried that writing it too clearly or persuasively might give AI developers too many good ideas and shorten timelines. I also had a boss/collaborator with different ideas about how to frame things. So I compromised on partly clear writing about my own carefully thought-out theories, and partly scientifical jargon speak that would get it published in a good journal but interest or elighten almost no one.\n\nMake of that what you will."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-11T03:23:27.309Z",
    "af": false
  },
  {
    "_id": "apBrfz76KpzhKauuL",
    "postId": "bmE8oRBaN5vgWngRa",
    "parentCommentId": "FtsnDuLXBTDRxu8o2",
    "topLevelCommentId": "t7TnPCxn9grDm2pHH",
    "contents": {
      "markdown": "I'll pardon it but I won't engage with it. I think saying it's the seat of consciousness makes you sound like you don't know what's going on, when actually you do. I could be right or wrong.",
      "plaintextDescription": "I'll pardon it but I won't engage with it. I think saying it's the seat of consciousness makes you sound like you don't know what's going on, when actually you do. I could be right or wrong."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-11T03:12:45.831Z",
    "af": false
  },
  {
    "_id": "t7TnPCxn9grDm2pHH",
    "postId": "bmE8oRBaN5vgWngRa",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I didn't have time to read all of this, so I apologize for commenting. I'm quite familiar with the science of the thalamus and its relation to consciousness. I think all of this looks pretty accurate, but I must object to your framing of the thalamus as the seat of consciousness. Consciousness is the whole shebang. It's not a little add-on or accident. Substantial processing happens outside of consciousness, of course, but lots of the brain needs to participate to create consciousness; it's a sophisticated set of information processing functions across areas, particularly the corticothalemic loops.\n\nSo calling the thalamus the seat of consciousness is like saying the water comes from the valve because if that's stuck closed no water flows. The thalamus is critical for consciousness, but that doesn't mean consciousness originates there. Most of the sophisticated processing that creates the rich representations we refer to as qualia or consciousness originates in the cortex; The thalamus and basal ganglia are more involved in selecting between possible such representations. \n\nI wish I had time to dig into this more; consciousness is fascinating, and I'm hoping it even becomes important again if people start arguing for AI rights on the basis of consciousness. Until then, I'm going to focus on alignment.\n\nPardon any errors from phone voice transcription.",
      "plaintextDescription": "I didn't have time to read all of this, so I apologize for commenting. I'm quite familiar with the science of the thalamus and its relation to consciousness. I think all of this looks pretty accurate, but I must object to your framing of the thalamus as the seat of consciousness. Consciousness is the whole shebang. It's not a little add-on or accident. Substantial processing happens outside of consciousness, of course, but lots of the brain needs to participate to create consciousness; it's a sophisticated set of information processing functions across areas, particularly the corticothalemic loops.\n\nSo calling the thalamus the seat of consciousness is like saying the water comes from the valve because if that's stuck closed no water flows. The thalamus is critical for consciousness, but that doesn't mean consciousness originates there. Most of the sophisticated processing that creates the rich representations we refer to as qualia or consciousness originates in the cortex; The thalamus and basal ganglia are more involved in selecting between possible such representations.\n\nI wish I had time to dig into this more; consciousness is fascinating, and I'm hoping it even becomes important again if people start arguing for AI rights on the basis of consciousness. Until then, I'm going to focus on alignment.\n\nPardon any errors from phone voice transcription."
    },
    "baseScore": 11,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-11T02:02:32.737Z",
    "af": false
  },
  {
    "_id": "bzFDyDpexLpQRQ9an",
    "postId": "bmE8oRBaN5vgWngRa",
    "parentCommentId": "3mnFyLLBM9vmKzsPJ",
    "topLevelCommentId": "T7PpJdvPCYbCRjpko",
    "contents": {
      "markdown": "I agree with most of that. Consciousness is how we make our important decisions. \n\nI don't think there are ever dozens per second of what we would call decisions. More like one or two or at most ten, even if we scratch the meaning of decision. That's about how long it takes for the cortical basal ganglia loop to reverberate enough to really activate that representation.\n\n10 is the absolute maximum you'd get on an alpha cycle, but I don't think it's plausible that just went alpha cycle is enough for what would really call a decision. \n\nAnd if we are talking about consciousness, I am going to say I have tried to introspect about this a lot, and I have never caught what would seem like more than two conscious decisions in one second. More like a full second for most decisions, even in action games, and often many seconds per decision when I'm thinking about them.",
      "plaintextDescription": "I agree with most of that. Consciousness is how we make our important decisions.\n\nI don't think there are ever dozens per second of what we would call decisions. More like one or two or at most ten, even if we scratch the meaning of decision. That's about how long it takes for the cortical basal ganglia loop to reverberate enough to really activate that representation.\n\n10 is the absolute maximum you'd get on an alpha cycle, but I don't think it's plausible that just went alpha cycle is enough for what would really call a decision.\n\nAnd if we are talking about consciousness, I am going to say I have tried to introspect about this a lot, and I have never caught what would seem like more than two conscious decisions in one second. More like a full second for most decisions, even in action games, and often many seconds per decision when I'm thinking about them."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-11T01:58:24.000Z",
    "af": false
  },
  {
    "_id": "HiixCHDTm5mQ9aiNZ",
    "postId": "Bgnd7GryJ72eiYfEL",
    "parentCommentId": "uD9zEMipKdnFgqg2S",
    "topLevelCommentId": "E8FypfFd3vqirbERD",
    "contents": {
      "markdown": "I bounced off it exactly at that point so editing only that in its current form is worth it. I'm not sure what view people take of just reposting repeatedly; it isn't usually done so might be considered a bit gauche.\n\nUmmm how did the green party fare in terms of getting their agenda forwarded, as of right now?\n\nMy primary concern with political involvement is getting your issue polarized like environmental concerns were. If one party raises your banner the other will want to tear it down by fair means or foul, and suddenly even an obvious concern is debated and gridlocked. ",
      "plaintextDescription": "I bounced off it exactly at that point so editing only that in its current form is worth it. I'm not sure what view people take of just reposting repeatedly; it isn't usually done so might be considered a bit gauche.\n\nUmmm how did the green party fare in terms of getting their agenda forwarded, as of right now?\n\nMy primary concern with political involvement is getting your issue polarized like environmental concerns were. If one party raises your banner the other will want to tear it down by fair means or foul, and suddenly even an obvious concern is debated and gridlocked."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-10T20:17:13.431Z",
    "af": false
  },
  {
    "_id": "E8FypfFd3vqirbERD",
    "postId": "Bgnd7GryJ72eiYfEL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I don't have time to read the whole thing right now, but I have long thought that politics is one arena where we could and should advance the idea of AI safety.\n\nI think this becomes particularly possible and powerful if we see substantial concerns about job loss to AI before we have takeover-capable AGI. And I now think we probably will.\n\nI got hung up on your introductory point 3, \"reasonable chance for success\". If I were you I'd reword or re-think that to qualify \"success\". We're not likely to get anyone into the US Senate on a third-party ticket, before doomsday, so saying up front what your realistic expectations are would be helpful for drawing readers in. You might edit that statement since you do deal with that concern in the post.",
      "plaintextDescription": "I don't have time to read the whole thing right now, but I have long thought that politics is one arena where we could and should advance the idea of AI safety.\n\nI think this becomes particularly possible and powerful if we see substantial concerns about job loss to AI before we have takeover-capable AGI. And I now think we probably will.\n\nI got hung up on your introductory point 3, \"reasonable chance for success\". If I were you I'd reword or re-think that to qualify \"success\". We're not likely to get anyone into the US Senate on a third-party ticket, before doomsday, so saying up front what your realistic expectations are would be helpful for drawing readers in. You might edit that statement since you do deal with that concern in the post."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-10T19:48:46.699Z",
    "af": false
  },
  {
    "_id": "Cyz2YafxNE7HjwCjg",
    "postId": "5GsGSs4gdFY6uBid4",
    "parentCommentId": "ddP9rA4ptsHgFwm2A",
    "topLevelCommentId": "ddP9rA4ptsHgFwm2A",
    "contents": {
      "markdown": "I like both of those definitions. They do match the centroids of how I see those terms used, including my own usage. Perhaps I'll call them V-AGI and V-ASI and link the terms here.",
      "plaintextDescription": "I like both of those definitions. They do match the centroids of how I see those terms used, including my own usage. Perhaps I'll call them V-AGI and V-ASI and link the terms here."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-10T17:36:52.428Z",
    "af": false
  },
  {
    "_id": "qCQTDw6AMy5oxQcqw",
    "postId": "5GsGSs4gdFY6uBid4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "When we have a particular query at hand, we don't need the broad terms, and that's great. But we do need shorthands when defining a particular query doesn't fit the time/space/attention span. Having a wider variety of sub-terms within the community would be extremely helpful, but I'm not sure how to work toward that.\n\nI think about this a fair amount. I think terminology matters, and having terms of art is practically necessary. We're not going to restate a functional definition in every article, let alone every comment. AGI centrally means \"the type of AI that changes everything\", which is the central topic of discussion in the safety community for good reason. \n\nI agree that the term AGI is used in so many different ways that it's problematic and probably not useful. I think we'd benefit a lot by having some standard reference terminology that's carefully defined.\n\nTo that end, I wrote a couple of articles trying to give a better definition, which includes talking about what type of cognitive capabilities are necessary to create a system that \"thinks for itself\" and is smarter than humans in most useful ways. I wrote [Sapience, understanding, and \"AGI\"](https://www.lesswrong.com/posts/WqxGB77KyZgQNDoQY/sapience-understanding-and-agi) on this in 2023, and [\"Real AGI\"](https://www.lesswrong.com/posts/YW249knFccwATGxki/real-agi) more recently.\n\nOne problem I discovered in my attempts to create better defined terms is that the project was wrapped up in theories about exactly how AI will progress to have transformative properties. I expect a certain set of cognitive capacities to be the turning point, so I define it with those. Others have different theories. Which would be fine if we had multiple definitions in circulation. Transformative AI is an attempt to sidestep addressing the particulars of AI capabilities that will be important, but I think the relative disuse of that term suggests that addressing that question is pretty critical to most of the discussion.  \n  \nI think the term ASI is usually used distincly (at least on LessWrong) and is pretty useful; it usually points to something more capable than the first things you'd call AGI, and can designate takeover-capable AI (one of my recently preferred terms for pointing to the important properties).  \n  \nMaybe if somebody with sufficient reputation wrote a post saying \"hey let's settle on some more specific sub-terms\" that would work. But I suspect that would feel locally like a waste of time to that busy person.  \n  \nThere's a joke that academic philosophers would rather use each other's toothbrushes than each other's terminology. This tendency creates a profusion of different terms, and subtly different uses of the same terms. We could at least improve on that by having some reference posts intended to define the terms in particular ways.",
      "plaintextDescription": "When we have a particular query at hand, we don't need the broad terms, and that's great. But we do need shorthands when defining a particular query doesn't fit the time/space/attention span. Having a wider variety of sub-terms within the community would be extremely helpful, but I'm not sure how to work toward that.\n\n \n\nI think about this a fair amount. I think terminology matters, and having terms of art is practically necessary. We're not going to restate a functional definition in every article, let alone every comment. AGI centrally means \"the type of AI that changes everything\", which is the central topic of discussion in the safety community for good reason. \n\nI agree that the term AGI is used in so many different ways that it's problematic and probably not useful. I think we'd benefit a lot by having some standard reference terminology that's carefully defined.\n\nTo that end, I wrote a couple of articles trying to give a better definition, which includes talking about what type of cognitive capabilities are necessary to create a system that \"thinks for itself\" and is smarter than humans in most useful ways. I wrote Sapience, understanding, and \"AGI\" on this in 2023, and \"Real AGI\" more recently.\n\nOne problem I discovered in my attempts to create better defined terms is that the project was wrapped up in theories about exactly how AI will progress to have transformative properties. I expect a certain set of cognitive capacities to be the turning point, so I define it with those. Others have different theories. Which would be fine if we had multiple definitions in circulation. Transformative AI is an attempt to sidestep addressing the particulars of AI capabilities that will be important, but I think the relative disuse of that term suggests that addressing that question is pretty critical to most of the discussion.\n\nI think the term ASI is usually used distincly (at least on LessWrong) and is pretty useful; it usually points to something more capable than the "
    },
    "baseScore": 5,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-10T16:56:51.217Z",
    "af": false
  },
  {
    "_id": "AqkrAwM6fyKQMahAx",
    "postId": "Nmp6FWAj3mcuLu6H6",
    "parentCommentId": "hHJbJcZDW4Agozzou",
    "topLevelCommentId": "yy4tvC7Lnqnpp6wpQ",
    "contents": {
      "markdown": "Those other roles require that much sleep is because they were designed for that amount. Evolution designed them around however much those animals were sleeping. And of course sleep started way, way, way before humans.\n\nSleep probably does a bunch of different things. There's not going to be one drug that suppresses the need for all of it. Or even a whole suite of drugs we can locate now.\n\nMaybe you could cut it to half as much on average without severe downsides, but I'd expect first attempts to fail in subtle ways.\n\nOf course you might not need to live that long naturally if you're young and take AGI seriously.",
      "plaintextDescription": "Those other roles require that much sleep is because they were designed for that amount. Evolution designed them around however much those animals were sleeping. And of course sleep started way, way, way before humans.\n\nSleep probably does a bunch of different things. There's not going to be one drug that suppresses the need for all of it. Or even a whole suite of drugs we can locate now.\n\nMaybe you could cut it to half as much on average without severe downsides, but I'd expect first attempts to fail in subtle ways.\n\nOf course you might not need to live that long naturally if you're young and take AGI seriously."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-09T04:09:01.026Z",
    "af": false
  },
  {
    "_id": "CFKvuhLGacfqnmWQz",
    "postId": "TiQGC6woDMPJ9zbNM",
    "parentCommentId": "pr9xaEJM7kbDAhtea",
    "topLevelCommentId": "PkeGvBroRwmhFq2bj",
    "contents": {
      "markdown": "Thanks. I'd better stay out of this until I know who that is :)",
      "plaintextDescription": "Thanks. I'd better stay out of this until I know who that is :)"
    },
    "baseScore": 13,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-08T21:46:40.068Z",
    "af": false
  },
  {
    "_id": "PSuBFKsGaLDLHL9dC",
    "postId": "Lxdf6LfPXupeGL94w",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This sounds like \"apply the 80/20 rule to everything,\" which I think is extremely useful advice.\n\nAdvice along the lines of \"if you're going to do it, do it right\" or \"why half-ass two things if you can whole-ass one thing\" strikes me as among the most perniciously bad advice we give in American culture - and we give it a lot.\n\nLife is short and prioritization is real. Are you doing things to impress people, or to get things done (yes it is necessary to impress people some of the time, but getting a lot of stuff done, including finding time for joy, can be impressive).\n\nWhen I helped run a neuroscience research lab, half of my job was reminding detail-oriented scientists \"the perfect is the enemy of the good!\" and applying the 80/20 rule discerningly to think about what work would produce progress most efficiently. It was constantly surprising to me how useful this was, at least in that setting.  ",
      "plaintextDescription": "This sounds like \"apply the 80/20 rule to everything,\" which I think is extremely useful advice.\n\nAdvice along the lines of \"if you're going to do it, do it right\" or \"why half-ass two things if you can whole-ass one thing\" strikes me as among the most perniciously bad advice we give in American culture - and we give it a lot.\n\nLife is short and prioritization is real. Are you doing things to impress people, or to get things done (yes it is necessary to impress people some of the time, but getting a lot of stuff done, including finding time for joy, can be impressive).\n\nWhen I helped run a neuroscience research lab, half of my job was reminding detail-oriented scientists \"the perfect is the enemy of the good!\" and applying the 80/20 rule discerningly to think about what work would produce progress most efficiently. It was constantly surprising to me how useful this was, at least in that setting."
    },
    "baseScore": 5,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-08T21:41:44.036Z",
    "af": false
  },
  {
    "_id": "k4xnr47pBfiH6taen",
    "postId": "TiQGC6woDMPJ9zbNM",
    "parentCommentId": "hKMfzjZDygzATPpbr",
    "topLevelCommentId": "PkeGvBroRwmhFq2bj",
    "contents": {
      "markdown": "I agree. The \"jesus\" was halfway a joke about the religious ties. And halfway steeling myself for that handshake.",
      "plaintextDescription": "I agree. The \"jesus\" was halfway a joke about the religious ties. And halfway steeling myself for that handshake."
    },
    "baseScore": 1,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-08T19:59:12.098Z",
    "af": false
  },
  {
    "_id": "PkeGvBroRwmhFq2bj",
    "postId": "TiQGC6woDMPJ9zbNM",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Do we need to ally with these people? Jesus. ",
      "plaintextDescription": "Do we need to ally with these people? Jesus."
    },
    "baseScore": -21,
    "voteCount": 34,
    "createdAt": null,
    "postedAt": "2025-09-08T18:34:01.111Z",
    "af": false
  },
  {
    "_id": "2t9p5jxrTmdHCwrkt",
    "postId": "ZAqaQrAimgfweHGSr",
    "parentCommentId": "kZiW8cNSgeuYTKLsN",
    "topLevelCommentId": "kZiW8cNSgeuYTKLsN",
    "contents": {
      "markdown": "I explained in a comment on that post what's wrong with it by LW standards. \n\nThe general question is worth investigating, but making that post the example for consideration isn't a good idea. It was neither that good nor that rationalist/rational in focus, the two things that qualify posts for LW standards and objectives.\n\nMaking a more general argument for the bias of LW would be a great way to earn some upvotes. We're suckers for analyzing our own biases.\n\nThe problem is it's hard to distinguish between being biased, and just being more rational and more right. Most arguments for AI doom suck. Most arguments against AI doom also suck. Most of the arguments that don't suck land you at way, way above the 1-5% doom you quoted. BUT of course we could just be mistaking massive amounts of thought and analysis for bias. Or not. It's an open question. \n\nIt's tricky to approach the meta-question without approaching the object level.\n\nIt could just be that LW doesn't like most arguments against AGI x-risk because *people only make those arguments before they've considered the whole question*, so they tend to not be very rational.\n\nI've tried to steelman arguments against, and I can't get anywhere near \"oh yeah this should be fine\" without leaving out huge chunks of the question and likely futures. In particular, if I ONLY think of AI as LLMs, I get those low doom probabilities - but we're just obviously not going to stop there without some remarkable changes.\n\nThe best I can get is something like \"people tend to worry a lot, and people tend to solve problems pretty well once they're close and so seem important\". That might get me down to like 10% if I'm feeling super optimistic. That's if I'm considering the whole problem: we're making a new alien species that will be way smarter than us.\n\n\n",
      "plaintextDescription": "I explained in a comment on that post what's wrong with it by LW standards.\n\nThe general question is worth investigating, but making that post the example for consideration isn't a good idea. It was neither that good nor that rationalist/rational in focus, the two things that qualify posts for LW standards and objectives.\n\nMaking a more general argument for the bias of LW would be a great way to earn some upvotes. We're suckers for analyzing our own biases.\n\nThe problem is it's hard to distinguish between being biased, and just being more rational and more right. Most arguments for AI doom suck. Most arguments against AI doom also suck. Most of the arguments that don't suck land you at way, way above the 1-5% doom you quoted. BUT of course we could just be mistaking massive amounts of thought and analysis for bias. Or not. It's an open question.\n\nIt's tricky to approach the meta-question without approaching the object level.\n\nIt could just be that LW doesn't like most arguments against AGI x-risk because people only make those arguments before they've considered the whole question, so they tend to not be very rational.\n\nI've tried to steelman arguments against, and I can't get anywhere near \"oh yeah this should be fine\" without leaving out huge chunks of the question and likely futures. In particular, if I ONLY think of AI as LLMs, I get those low doom probabilities - but we're just obviously not going to stop there without some remarkable changes.\n\nThe best I can get is something like \"people tend to worry a lot, and people tend to solve problems pretty well once they're close and so seem important\". That might get me down to like 10% if I'm feeling super optimistic. That's if I'm considering the whole problem: we're making a new alien species that will be way smarter than us."
    },
    "baseScore": -5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-07T20:29:36.365Z",
    "af": false
  },
  {
    "_id": "J56HhrH8qkYw7nXNZ",
    "postId": "wcp8hcQwqhd8nDA2x",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Just a note, no direct relation to alignment: \n\nI read [this article](https://futurism.com/gpt-5-literary-outputs) on GPT5 being too metaphorical as a writer and that signaling an training process optimizing for an unwanted goal (the investigator's twitter thread about it is linked above). The accusation was that GPT5T was producing lots of metaphor as a way to impress humans and/or other LLMs with how human and literary it was, but the metaphors were nonsense. \n\nToo metaphorical is a matter of taste, but I found it very interesting that the lead choice in \"full of shit\" was quite good IMO - a metaphor right at the leading edge of my ability to get it and so perfect for me.\n\nAdjusting the pop filter to \"count the german language's teeth\"\n\nGerman has a reputation for being harsh in an auditory sense, with shorp stops and starts. This harshness has been posited to relate to the harshness of the german culture, clearly relevant for the interview.\n\nA pop filter is (presumably - I don't know) going to be adjusted for those sharp sounds. Adjusting it properly requires \"counting the teeth\" by guessing how much there will be in this interview.\n\nThis ties back to the whole subject of the essay which has to do with German history. So, wow, this is IMO an amazingly skilled use of metaphor.\n\nThis is not to disagree with the hypothesis about getting here by optimizing for LLMs thinking it's human-generated. But I for one have been enjoying GPT5's writing; it's replaced the scifi schlock I was reading for relaxation before bed, since I can quickly prompt it and so co-create instead of just hear other people's stories. These are short pieces; when I tried to see how far it could get quickly with plotting and writing at novel length (SOMEONE should write a novel about the alignment problem, and it ain't gonna be me unaided), it broke down pretty quickly.\n\nIs the rest of it? I don't know. When I listen to GPT5T's stories with half my attention, I lose track and suspect it's full of shit. When I listen with full attention I feel it's often too heavy on metaphor, but they usually make sense, so as with reading a skilled author, I assume the ones I don't get will also make sense and contribute to the feel perhaps.  \n  \nI think good writing is very much a matter of taste. The idea that there's some transcendent skill only goes so far. I find it interesting that evaluations like Zvi's that take \"good taste\" to be a very real and important thing (I think it's somewhat real and important) are the basis of OpenAI pushing ChatGPTs writing to the \"impress people with the depth of your metaphor\" areas instead of the \"tell a good story\" areas.  \n  \nJust saying.",
      "plaintextDescription": "Just a note, no direct relation to alignment: \n\nI read this article on GPT5 being too metaphorical as a writer and that signaling an training process optimizing for an unwanted goal (the investigator's twitter thread about it is linked above). The accusation was that GPT5T was producing lots of metaphor as a way to impress humans and/or other LLMs with how human and literary it was, but the metaphors were nonsense. \n\nToo metaphorical is a matter of taste, but I found it very interesting that the lead choice in \"full of shit\" was quite good IMO - a metaphor right at the leading edge of my ability to get it and so perfect for me.\n\n \n\nAdjusting the pop filter to \"count the german language's teeth\"\n\nGerman has a reputation for being harsh in an auditory sense, with shorp stops and starts. This harshness has been posited to relate to the harshness of the german culture, clearly relevant for the interview.\n\nA pop filter is (presumably - I don't know) going to be adjusted for those sharp sounds. Adjusting it properly requires \"counting the teeth\" by guessing how much there will be in this interview.\n\nThis ties back to the whole subject of the essay which has to do with German history. So, wow, this is IMO an amazingly skilled use of metaphor.\n\n \n\nThis is not to disagree with the hypothesis about getting here by optimizing for LLMs thinking it's human-generated. But I for one have been enjoying GPT5's writing; it's replaced the scifi schlock I was reading for relaxation before bed, since I can quickly prompt it and so co-create instead of just hear other people's stories. These are short pieces; when I tried to see how far it could get quickly with plotting and writing at novel length (SOMEONE should write a novel about the alignment problem, and it ain't gonna be me unaided), it broke down pretty quickly.\n\nIs the rest of it? I don't know. When I listen to GPT5T's stories with half my attention, I lose track and suspect it's full of shit. When I listen with full attention I"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-07T16:42:06.657Z",
    "af": false
  },
  {
    "_id": "kYtq7YXK9TQuPJh2F",
    "postId": "wcp8hcQwqhd8nDA2x",
    "parentCommentId": "67bivxbPbTvqwX5ek",
    "topLevelCommentId": "67bivxbPbTvqwX5ek",
    "contents": {
      "markdown": "Amen. We need people with more skillsets and knowledge bases on this project. I'd be interested in talking about strategy for doing this. It's been bugging me for a while that we're not doing this actively.",
      "plaintextDescription": "Amen. We need people with more skillsets and knowledge bases on this project. I'd be interested in talking about strategy for doing this. It's been bugging me for a while that we're not doing this actively."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-07T16:18:39.265Z",
    "af": false
  },
  {
    "_id": "u3gkSano4Mv3x6QH5",
    "postId": "7GRmJkjfroRa26R6L",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I think the mere familiarity effect is a big part of what you're missing. For some reason, the way the brain works makes us like things we know. If someone walks into a store and looks at two brands, and they have seen one but not the other somewhere but don't remember where, they will reliably and noticeably prefer the brand they have heard of. \nThe other answers are good too.",
      "plaintextDescription": "I think the mere familiarity effect is a big part of what you're missing. For some reason, the way the brain works makes us like things we know. If someone walks into a store and looks at two brands, and they have seen one but not the other somewhere but don't remember where, they will reliably and noticeably prefer the brand they have heard of. The other answers are good too."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-06T13:17:25.718Z",
    "af": false
  },
  {
    "_id": "Awjmnh9bsiN4DJu8A",
    "postId": "HKHqFWT7qiac2tvtF",
    "parentCommentId": "FiYYyoGuWDFFE6o9M",
    "topLevelCommentId": "GwerRqrpc46SdoCXm",
    "contents": {
      "markdown": "I went back and reread it. Because you did mark that p(doom) as vibes based and Said you weren't making strong predictions near the top, I removed my small downvote. \n\nI said I'd have upvoted if you removed the prediction. The prediction is the problem here because it is appears to be based on bad logic - vibes instead of gears.\n\nI have never downvoted something that disagrees with my stance if it tries to come to grips with the central problem of how difficult alignment is. \n\nNor have I downvoted pieces that scope to address only part of the pmquestion and don't make a P(doom) prediction.\n\nI have frequently complained on new authors' behalf that the LW community has downvoted unfairly.\n\n\n",
      "plaintextDescription": "I went back and reread it. Because you did mark that p(doom) as vibes based and Said you weren't making strong predictions near the top, I removed my small downvote.\n\nI said I'd have upvoted if you removed the prediction. The prediction is the problem here because it is appears to be based on bad logic - vibes instead of gears.\n\nI have never downvoted something that disagrees with my stance if it tries to come to grips with the central problem of how difficult alignment is.\n\nNor have I downvoted pieces that scope to address only part of the pmquestion and don't make a P(doom) prediction.\n\nI have frequently complained on new authors' behalf that the LW community has downvoted unfairly."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-06T12:49:14.682Z",
    "af": false
  },
  {
    "_id": "rGNo8WkNcyutECjQJ",
    "postId": "HKHqFWT7qiac2tvtF",
    "parentCommentId": "GwerRqrpc46SdoCXm",
    "topLevelCommentId": "GwerRqrpc46SdoCXm",
    "contents": {
      "markdown": "Since you imply you want feedback, I'll give it. \n\nFirst, to answer your question: the big thing you're missing are the technical arguments for the difficulty of alignment. That's where most of the variance lies. All of the factors you list are small potatoes if it just turns out that alignment is quite hard. \n\nThe other big factor is the overall societal dynamics. You casually mention\"then we shouldn't build it\". The big question is if we SHOULD stop, COULD we stop? I think there's only maybe a 10% chance we could if we should. The incentives are just too strong and the coordination problem is unsolved. And the people in power on this are looking quite incompetent to deal with it. They'll probably start taking it seriously at some point, but whether that's soon enough to help is a big question. You could look at my [Whether governments will control AGI is important and neglected](https://www.lesswrong.com/posts/fFqABwAHMvhHSFmce/whether-governments-will-control-agi-is-important-and) if you wanted a little more of my logic on that.\n\nIt's a problem for logic based on careful observations and data. Vibes-based arguments are actively unhelpful. It's been fine to just guess at most other stuff in history, but not this.\n\nI'm being kind of judgmental because I think having good estimates of AGI risk is quite important for our collective odds of survival and/or flourishing And LW is, among other things, the best place on earth to get good guesses on that.. I apologize for being a little harsh.\n\nIf you titled this \"some factors maybe in AI risk\" or \"some changes that have shifted my p(doom)\" or something and left out the p(doom) I'd have upvoted because you have some interesting observations.\n\nAs it is, I did very much think this is anti-worth reading in the context of LW. I couldn't decide between normal and big downvotes.  \n\nI think this is polluting the community epistemics by making predictions based on vibes. Then you deny they're predictions in the comments. P(doom) is very much a prediction and you shouldn't make it publicly if you aren't really trying. Or maybe that's what twitter or reddit are for?\n\n\nIt would be pretty rare that 30 minutes of work would be worth reading on LW compared to the many excellent high effort posts here. I do appreciate you putting that caveat. The exception would be an expert or unique insight or unique idea. Just voicing non-expert quick takes isn't really what LW is for IMO. At most it would be a quick take.\n\nOr to put it this way: LW is for the opposite of vibes-based opinions.",
      "plaintextDescription": "Since you imply you want feedback, I'll give it.\n\nFirst, to answer your question: the big thing you're missing are the technical arguments for the difficulty of alignment. That's where most of the variance lies. All of the factors you list are small potatoes if it just turns out that alignment is quite hard.\n\nThe other big factor is the overall societal dynamics. You casually mention\"then we shouldn't build it\". The big question is if we SHOULD stop, COULD we stop? I think there's only maybe a 10% chance we could if we should. The incentives are just too strong and the coordination problem is unsolved. And the people in power on this are looking quite incompetent to deal with it. They'll probably start taking it seriously at some point, but whether that's soon enough to help is a big question. You could look at my Whether governments will control AGI is important and neglected if you wanted a little more of my logic on that.\n\nIt's a problem for logic based on careful observations and data. Vibes-based arguments are actively unhelpful. It's been fine to just guess at most other stuff in history, but not this.\n\nI'm being kind of judgmental because I think having good estimates of AGI risk is quite important for our collective odds of survival and/or flourishing And LW is, among other things, the best place on earth to get good guesses on that.. I apologize for being a little harsh.\n\nIf you titled this \"some factors maybe in AI risk\" or \"some changes that have shifted my p(doom)\" or something and left out the p(doom) I'd have upvoted because you have some interesting observations.\n\nAs it is, I did very much think this is anti-worth reading in the context of LW. I couldn't decide between normal and big downvotes.\n\nI think this is polluting the community epistemics by making predictions based on vibes. Then you deny they're predictions in the comments. P(doom) is very much a prediction and you shouldn't make it publicly if you aren't really trying. Or maybe that's what t"
    },
    "baseScore": 20,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2025-09-06T03:02:08.333Z",
    "af": false
  },
  {
    "_id": "736f2LxXfQJjYdiz7",
    "postId": "HKHqFWT7qiac2tvtF",
    "parentCommentId": "RNFEEi7tm6eFcwkgP",
    "topLevelCommentId": "xAAnk64adJA9yLGbk",
    "contents": {
      "markdown": "You estimate c by looking at how many breakthroughs we've had in AI per person year so far. That's where the 8% per year comes from. It seems low to me with the large influx of people working on AI, but I'm sure Daniel's math makes sense given his estimate of breakthroughs to date ",
      "plaintextDescription": "You estimate c by looking at how many breakthroughs we've had in AI per person year so far. That's where the 8% per year comes from. It seems low to me with the large influx of people working on AI, but I'm sure Daniel's math makes sense given his estimate of breakthroughs to date"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-06T02:57:14.004Z",
    "af": false
  },
  {
    "_id": "c5tmLdY5jhWcxSkzk",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": "uNmAEjR76eSxWWQgN",
    "topLevelCommentId": "P93J7vZLmdjRYnEW6",
    "contents": {
      "markdown": "Sure. If you're confident of that.\n\nIt drives me a bit nuts that many of our otherwise best thinkers are confident of aligniing LLM AGI being almost impossible, when the arguments they're citing just don't stack to near certainty even with active steelmanning. I've been immersing myself in the arguments for inner misalignment as a strong default. They're strong, they should make you afraid,  but they're nowhere near certainty. \n\nFew people who take that aligment difficulty seriously are even proposing ways around it for LLM AGI. We have barely begun working on the most relevant hard problem. Calling it hopeless without working on it is... questionable. I see why you might, for various reasons, but at this point I think it's a huge mistake.\n\nWe can call for pause/slowdown and emphasize the difficulty, while also working on alignment on the default path.  We're in a bad situation, and that looks to me like our biggest potential out by an order of magnitude.",
      "plaintextDescription": "Sure. If you're confident of that.\n\nIt drives me a bit nuts that many of our otherwise best thinkers are confident of aligniing LLM AGI being almost impossible, when the arguments they're citing just don't stack to near certainty even with active steelmanning. I've been immersing myself in the arguments for inner misalignment as a strong default. They're strong, they should make you afraid, but they're nowhere near certainty.\n\nFew people who take that aligment difficulty seriously are even proposing ways around it for LLM AGI. We have barely begun working on the most relevant hard problem. Calling it hopeless without working on it is... questionable. I see why you might, for various reasons, but at this point I think it's a huge mistake.\n\nWe can call for pause/slowdown and emphasize the difficulty, while also working on alignment on the default path. We're in a bad situation, and that looks to me like our biggest potential out by an order of magnitude."
    },
    "baseScore": 11,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-04T00:50:07.290Z",
    "af": false
  },
  {
    "_id": "XwfX75HEppWDi9t8d",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": "zoBwFm93fuAP2Qqr2",
    "topLevelCommentId": "P93J7vZLmdjRYnEW6",
    "contents": {
      "markdown": "\nBetter education can help with your first problem, although that pulls people who understand the problem away from working on it.\n\nI agree that the difficulty of evaluating progress is a big problem. One solution is to just fund more alignment research. I am dismayed if it's true that Open Phil is holding back available funding because they don't see good projects. Just fund them and get more donations later when the whole world is properly more freaked out. If it's bad research now, at least those people will spend some time thi8nking about and debating what might be better research.\n\nI'd also love to see funding directly on people understanding the whole problem including the several hard parts. It is a lot easier to evaluate whether someone is learning a curriculum than doing good research. Exposing people to a lot of perspectives and arguments and sort of paying and forcing them to think hard about it should at least improve their choice of research and understanding of the problem.\n\n\n\nI definitely agree that understanding the problem is the rate-limiting factor. I'd argue that it's not just the technical problem you need to understand, but the surrounding factors, eg how likely is a pause or slowdown and how likely is it we reach AGI how soon on the default path. I'm afraid some of our best technical thinkers understand the technical problem but are confused about how unlikely it is that any approach but directLLM descendents will be the first critical attempt at aligning AGI. But arguments for or against that are quite complex.\n",
      "plaintextDescription": "Better education can help with your first problem, although that pulls people who understand the problem away from working on it.\n\nI agree that the difficulty of evaluating progress is a big problem. One solution is to just fund more alignment research. I am dismayed if it's true that Open Phil is holding back available funding because they don't see good projects. Just fund them and get more donations later when the whole world is properly more freaked out. If it's bad research now, at least those people will spend some time thi8nking about and debating what might be better research.\n\nI'd also love to see funding directly on people understanding the whole problem including the several hard parts. It is a lot easier to evaluate whether someone is learning a curriculum than doing good research. Exposing people to a lot of perspectives and arguments and sort of paying and forcing them to think hard about it should at least improve their choice of research and understanding of the problem.\n\nI definitely agree that understanding the problem is the rate-limiting factor. I'd argue that it's not just the technical problem you need to understand, but the surrounding factors, eg how likely is a pause or slowdown and how likely is it we reach AGI how soon on the default path. I'm afraid some of our best technical thinkers understand the technical problem but are confused about how unlikely it is that any approach but directLLM descendents will be the first critical attempt at aligning AGI. But arguments for or against that are quite complex."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-04T00:39:20.974Z",
    "af": false
  },
  {
    "_id": "Y3Z2NwDNjuDGnWZwX",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": "zR6EwxoFt4SBKkFf4",
    "topLevelCommentId": "P93J7vZLmdjRYnEW6",
    "contents": {
      "markdown": "What I think would be really useful is more dialogue across \"party lines\" on strategies. I think I'm seeing nontrivial polarization, because attempted dialogues seem to usually end in frustration rather than progress.\n\n\nI'm thinking of a slightly different plan than  \"increase the rate of people being able to think seriously about the problem\" I'd like to convince people who already understand the problem to accept that pause is unlikely and alignment is not known to be impossibly hard even on short timelines. If they agreed with both of those it seems like they'd want to work on aligning LLM-based AGI, on what looks like the current default path. I think just a few more might help nontrivially. The number of people going \"straight for the throat\" is very small.\n\nI'm interested in the opposite variant too, trying to convince people working on \"aligning\" current LLMs to focus more on the hard parts we haven't encountered yet.\n\nI do think shifting the Overton window is possible. Actually I think it's almost inevitable; I just don't know if it happens soon enough to help. I just think a pause is unlikely even if the public screams for it - but I'm not sure, particularly if that happens sooner than I think. Public opinion can shift rapidly.\n\nThe Bengio/Hinton/Dario efforts seem like they are changing the Overton window, but cautiously. PR seems to require both skill and status. \n\nGetting entirely new people to understand the hard parts of the problem and then understand all of the technical skills or theoretical subtleties is another route. I haven't thought as much about that one because I don't have a public platform, but I do try to engage newcomers to LW in case they're the type to actually figure things out enough to really help.\n\n\n\n\n\n",
      "plaintextDescription": "What I think would be really useful is more dialogue across \"party lines\" on strategies. I think I'm seeing nontrivial polarization, because attempted dialogues seem to usually end in frustration rather than progress.\n\nI'm thinking of a slightly different plan than \"increase the rate of people being able to think seriously about the problem\" I'd like to convince people who already understand the problem to accept that pause is unlikely and alignment is not known to be impossibly hard even on short timelines. If they agreed with both of those it seems like they'd want to work on aligning LLM-based AGI, on what looks like the current default path. I think just a few more might help nontrivially. The number of people going \"straight for the throat\" is very small.\n\nI'm interested in the opposite variant too, trying to convince people working on \"aligning\" current LLMs to focus more on the hard parts we haven't encountered yet.\n\nI do think shifting the Overton window is possible. Actually I think it's almost inevitable; I just don't know if it happens soon enough to help. I just think a pause is unlikely even if the public screams for it - but I'm not sure, particularly if that happens sooner than I think. Public opinion can shift rapidly.\n\nThe Bengio/Hinton/Dario efforts seem like they are changing the Overton window, but cautiously. PR seems to require both skill and status.\n\nGetting entirely new people to understand the hard parts of the problem and then understand all of the technical skills or theoretical subtleties is another route. I haven't thought as much about that one because I don't have a public platform, but I do try to engage newcomers to LW in case they're the type to actually figure things out enough to really help."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-03T20:31:47.479Z",
    "af": false
  },
  {
    "_id": "HsKCYjMfLwMHmKAdt",
    "postId": "KZPNbHs9RsoeZShkJ",
    "parentCommentId": "66zur94HgAJzgRjbz",
    "topLevelCommentId": "FcMbGN7TX99TT9sAw",
    "contents": {
      "markdown": "I think everything is grounded in basic human universal reinforcement signals. If you look at Steve's Valence series, it's a sketch of how the critic portion of the steering system expands those basic signals, by association, to complex representations of complex situations. So in my considered opinion, there's not a sharp line between needs and wants. \n\nSo we can have a need for justice that's almost arbitrarily strong. it might be stronger then our hunger, even when we're pretty hungry. \n\nSo it doesn't make sense to say it's a need if I'm hungry but it's really mild, like just feeling a little peckish, and not a need if I'm wanting respect very badly. ",
      "plaintextDescription": "I think everything is grounded in basic human universal reinforcement signals. If you look at Steve's Valence series, it's a sketch of how the critic portion of the steering system expands those basic signals, by association, to complex representations of complex situations. So in my considered opinion, there's not a sharp line between needs and wants.\n\nSo we can have a need for justice that's almost arbitrarily strong. it might be stronger then our hunger, even when we're pretty hungry.\n\nSo it doesn't make sense to say it's a need if I'm hungry but it's really mild, like just feeling a little peckish, and not a need if I'm wanting respect very badly."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-03T13:15:33.640Z",
    "af": false
  },
  {
    "_id": "SwESFrzTfckvgJjpw",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": "nnqWnL4xMDuejjfJo",
    "topLevelCommentId": "P93J7vZLmdjRYnEW6",
    "contents": {
      "markdown": "I'm not getting your 35% to 5% reference? I just have no hope of getting as low as 5%, but a lot of hope for improving on just letting the labs take a swing.\n\nI fully agree that Anthropic and the other labs doen't seem engaged with the relevant hard parts of the problem. That's why I want to convince more people that actually understand the problem to identify and work like mad on the hard parts like the world is on fire, instead of hoping it somehow isn't or can be put out.\n\nIt may not be that great a strategy, but to me it seems way better than hoping for pause. I think we can get a public freakout before gametime, but even that won't produce a pause once the government and military is fully AGI-pilled.\n\nThis is a deep issue I've been wanting to write about, but haven't figured out how to address without risking further polarization within the alignment community. I'm sure there's a way to do it productively.\n",
      "plaintextDescription": "I'm not getting your 35% to 5% reference? I just have no hope of getting as low as 5%, but a lot of hope for improving on just letting the labs take a swing.\n\nI fully agree that Anthropic and the other labs doen't seem engaged with the relevant hard parts of the problem. That's why I want to convince more people that actually understand the problem to identify and work like mad on the hard parts like the world is on fire, instead of hoping it somehow isn't or can be put out.\n\nIt may not be that great a strategy, but to me it seems way better than hoping for pause. I think we can get a public freakout before gametime, but even that won't produce a pause once the government and military is fully AGI-pilled.\n\nThis is a deep issue I've been wanting to write about, but haven't figured out how to address without risking further polarization within the alignment community. I'm sure there's a way to do it productively."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-03T01:52:53.376Z",
    "af": false
  },
  {
    "_id": "YKb6MhgbocWnTzFNr",
    "postId": "KZPNbHs9RsoeZShkJ",
    "parentCommentId": "uZkWhjbJmnDjYcDpz",
    "topLevelCommentId": "dXSYmj2B9mjNqePjs",
    "contents": {
      "markdown": "My favorite theory, supported by some evidence but I haven't dug in. It matches my experience and understanding of general (sober) cortical functioning:\n\nHallocinogens strengthen top-down and lateral connections relative to bottom-up sensory connections. That makes imagination/hallucinations more stable, and sensory input weaker so driving activity in global workspace less.",
      "plaintextDescription": "My favorite theory, supported by some evidence but I haven't dug in. It matches my experience and understanding of general (sober) cortical functioning:\n\nHallocinogens strengthen top-down and lateral connections relative to bottom-up sensory connections. That makes imagination/hallucinations more stable, and sensory input weaker so driving activity in global workspace less."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-03T01:45:24.687Z",
    "af": false
  },
  {
    "_id": "CfgkCftgbbQtogTsX",
    "postId": "KZPNbHs9RsoeZShkJ",
    "parentCommentId": "dXSYmj2B9mjNqePjs",
    "topLevelCommentId": "dXSYmj2B9mjNqePjs",
    "contents": {
      "markdown": "Great analysis, interesting data, and I agree!\n\nI think the psychedelics have a second important effect: they often create vivid internal worlds. If you can have that, it also implies that physical reality isn't particularly more real than your mind.\n\nWhat people don't appreciate is that brains need to be hallucination machines just to reason about counterfactuals and make inferences. Those hallucinations are usually either tightly coupled to the sense, or more vague and fleeting and less vivid. Hallucinogens stabilize those imaginations while disrupting the bottom-up sensory information so it seems less vivid.",
      "plaintextDescription": "Great analysis, interesting data, and I agree!\n\nI think the psychedelics have a second important effect: they often create vivid internal worlds. If you can have that, it also implies that physical reality isn't particularly more real than your mind.\n\nWhat people don't appreciate is that brains need to be hallucination machines just to reason about counterfactuals and make inferences. Those hallucinations are usually either tightly coupled to the sense, or more vague and fleeting and less vivid. Hallucinogens stabilize those imaginations while disrupting the bottom-up sensory information so it seems less vivid."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-03T01:42:52.521Z",
    "af": false
  },
  {
    "_id": "3T5EpBB7t2KbdNtMr",
    "postId": "KZPNbHs9RsoeZShkJ",
    "parentCommentId": "bawaLbxufwjsvzggn",
    "topLevelCommentId": "FcMbGN7TX99TT9sAw",
    "contents": {
      "markdown": "I was going for terms that are simple to convey to anyone, and to remember. I've bounced off of NVC several times even though I believe it's valuable.\n\nWith that in mind \"don't pressure anyone\" is another try to simplify the core even further.\n\nDon't pressure them to change their beliefs. And don't pressure them to do anything. Telling them facts and how you feel isn't pressuring them.\n\nNeeds as you've framed them have a fuzzy boundary between needs and wants. Do I need respect or just want it in this situation? So it's easy to wonder if I'm pressuring someone by framing it as a need.\n\nThat's why I like the question framing of requests: \"would you be willing to\" is often just an honest question about what someone wants to do.",
      "plaintextDescription": "I was going for terms that are simple to convey to anyone, and to remember. I've bounced off of NVC several times even though I believe it's valuable.\n\nWith that in mind \"don't pressure anyone\" is another try to simplify the core even further.\n\nDon't pressure them to change their beliefs. And don't pressure them to do anything. Telling them facts and how you feel isn't pressuring them.\n\nNeeds as you've framed them have a fuzzy boundary between needs and wants. Do I need respect or just want it in this situation? So it's easy to wonder if I'm pressuring someone by framing it as a need.\n\nThat's why I like the question framing of requests: \"would you be willing to\" is often just an honest question about what someone wants to do."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-03T01:37:39.003Z",
    "af": false
  },
  {
    "_id": "P93J7vZLmdjRYnEW6",
    "postId": "vJrP7nbnJTwk4fcbk",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "On the epistemic point: yes, and this is something current-gen LLMs seem actually useful for, with little risk.  This is where their idea generation is useful, and their poor taste and sycophancy doesn't matter.\n\nI've had success asking LLMs for counterarguments. Most of them are dumb and you can dismiss them. But they're smart enough to come up with some good ones once you've steelmanned them and judged their worth for yourself.\n\nThis seems less helpful than getting pushback from informed people. But that's hard to find; I've had experiences like yours with Zac HD, in which a conversation fails to surface pretty obvious-in-hindsight counterarguments, just because the conversation focused elsewhere. And I have gotten good pushback by asking LLMs repeatedly in different ways, as far back as o1.\n\n\nOn the object level on your example: I assume a lot of us aren't very engaged with pause efforts or hopes because it seems more productive and realistic to work on reducing ~70% toward ~35% misalignment risks. It seems very likely, that we're gonna barrel forward through any plausible pause movement, but not clear (even after trying to steelman every major alignment difficulty argument) that alignment is insoluble - if we can just collectively pull our shit halfway together while racing toward that cliff.",
      "plaintextDescription": "On the epistemic point: yes, and this is something current-gen LLMs seem actually useful for, with little risk. This is where their idea generation is useful, and their poor taste and sycophancy doesn't matter.\n\nI've had success asking LLMs for counterarguments. Most of them are dumb and you can dismiss them. But they're smart enough to come up with some good ones once you've steelmanned them and judged their worth for yourself.\n\nThis seems less helpful than getting pushback from informed people. But that's hard to find; I've had experiences like yours with Zac HD, in which a conversation fails to surface pretty obvious-in-hindsight counterarguments, just because the conversation focused elsewhere. And I have gotten good pushback by asking LLMs repeatedly in different ways, as far back as o1.\n\nOn the object level on your example: I assume a lot of us aren't very engaged with pause efforts or hopes because it seems more productive and realistic to work on reducing ~70% toward ~35% misalignment risks. It seems very likely, that we're gonna barrel forward through any plausible pause movement, but not clear (even after trying to steelman every major alignment difficulty argument) that alignment is insoluble - if we can just collectively pull our shit halfway together while racing toward that cliff."
    },
    "baseScore": 15,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-03T01:25:22.177Z",
    "af": false
  },
  {
    "_id": "XHquLimwtnE2QxqxB",
    "postId": "iuk3LQvbP7pqQw3Ki",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I agree that this risk is neglected.  The entire field is pretty clearly severely neglected, but this perhaps more than other challenges surrounding AGI.\n\nI have engaged with this issue in [If we solve alignment, do we die anyway?](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1) and to a lesser degree in [Whether governments will control AGI is important and neglected](https://www.lesswrong.com/posts/fFqABwAHMvhHSFmce/whether-governments-will-control-agi-is-important-and)\n\nI differ from you in that I don't regard this as an x-risk, but more like a mild s-risk. In many scenarios where someone decides to use their intent-aligned AGI to wipe out most of humanity and seize control of the future, they'll have a vision of the future, and will (probably rapidly) rebuild civilization under that vision. It's probably not the world I'd want, but it will probably preserve a lot of human values and either immediately or as the new god-tyrant appreciates the situation better, contain on net more joy than suffering. If we don't get someone whose sadism-empathy balance is permanently negative. Which is quite possible. \n\nThus it's a possible s-risk but unlikely to be a true x-risk for humanity - just for me and everyone I know. I love future humanity, too, so this isn't a total loss from my or the utilitarian POV. But it's still probably a large loss of future potential, a real catastrophe. ",
      "plaintextDescription": "I agree that this risk is neglected. The entire field is pretty clearly severely neglected, but this perhaps more than other challenges surrounding AGI.\n\nI have engaged with this issue in If we solve alignment, do we die anyway? and to a lesser degree in Whether governments will control AGI is important and neglected\n\nI differ from you in that I don't regard this as an x-risk, but more like a mild s-risk. In many scenarios where someone decides to use their intent-aligned AGI to wipe out most of humanity and seize control of the future, they'll have a vision of the future, and will (probably rapidly) rebuild civilization under that vision. It's probably not the world I'd want, but it will probably preserve a lot of human values and either immediately or as the new god-tyrant appreciates the situation better, contain on net more joy than suffering. If we don't get someone whose sadism-empathy balance is permanently negative. Which is quite possible.\n\nThus it's a possible s-risk but unlikely to be a true x-risk for humanity - just for me and everyone I know. I love future humanity, too, so this isn't a total loss from my or the utilitarian POV. But it's still probably a large loss of future potential, a real catastrophe."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-02T19:04:43.140Z",
    "af": false
  },
  {
    "_id": "Fdqw29oCrBYssmzy6",
    "postId": "cHqCGG5dTmpnM7y92",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "If you haven't been in a conversation, in a challenging project, and in bed with someone, you have absolutely no idea how attractive they really are.\n \nPerhaps this is also obvious, but I want to just make sure we're on the same page by mentioning that physical attractiveness is just one small part of overall attractiveness. Our culture emphasizes it a lot and I don't think it does anyone any good to emphasize it more.\n\nIn my observations, overindexing on physical attractiveness causes heaps of trouble to everyone involved.\n\nIf you were just going to hop in bed and then go on your way, that overindexing and the type of conversation you described would make more sense. But you're not going to do that almost ever, are you?",
      "plaintextDescription": "If you haven't been in a conversation, in a challenging project, and in bed with someone, you have absolutely no idea how attractive they really are.\n\nPerhaps this is also obvious, but I want to just make sure we're on the same page by mentioning that physical attractiveness is just one small part of overall attractiveness. Our culture emphasizes it a lot and I don't think it does anyone any good to emphasize it more.\n\nIn my observations, overindexing on physical attractiveness causes heaps of trouble to everyone involved.\n\nIf you were just going to hop in bed and then go on your way, that overindexing and the type of conversation you described would make more sense. But you're not going to do that almost ever, are you?"
    },
    "baseScore": 8,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-31T12:31:40.823Z",
    "af": false
  },
  {
    "_id": "n4DxoHchQkzWBT6js",
    "postId": "yFTMGKh9Muqpdtrmb",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Sounds like chaos. \n\nI'm in!\n\n\"AI creates chaos\" is a pretty good message for the public to get.",
      "plaintextDescription": "Sounds like chaos.\n\nI'm in!\n\n\"AI creates chaos\" is a pretty good message for the public to get."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-30T03:38:48.633Z",
    "af": false
  },
  {
    "_id": "XbF7FASK8yP2X2Ebx",
    "postId": "hQ7wyg9qtx38izFyf",
    "parentCommentId": "S8m2ZHAtS2yWWCNTg",
    "topLevelCommentId": "YctscDYJD7b5DCDCH",
    "contents": {
      "markdown": "That's  causation. Somebody with normal consciousness gets brain damage, then they have different consciousness. Correlation that's only when one event happens first is what we call causation.\n\nIt's still possible that concsciousness isn't really created by brains, but it would have to be an enormous hoax to create the appearance of a perfect causal connection.\n\nI was just trying to be helpful in giving feedback since you'd gotten a bunch of downvotes and no comments explaining why. \n",
      "plaintextDescription": "That's causation. Somebody with normal consciousness gets brain damage, then they have different consciousness. Correlation that's only when one event happens first is what we call causation.\n\nIt's still possible that concsciousness isn't really created by brains, but it would have to be an enormous hoax to create the appearance of a perfect causal connection.\n\nI was just trying to be helpful in giving feedback since you'd gotten a bunch of downvotes and no comments explaining why."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-29T20:31:27.386Z",
    "af": false
  },
  {
    "_id": "BuDi7pFBwWs2FBrPe",
    "postId": "yNhJKBdtucBL2uXb5",
    "parentCommentId": "4kF3YBuwGBtcnBPwm",
    "topLevelCommentId": "4kF3YBuwGBtcnBPwm",
    "contents": {
      "markdown": "Yes; except for working on alignment, where it's pretty easy to care about raising the odds of sentient beings surviving and flourishing in our light-cone. After THAT we can figure out how people can work on projects without a sense of inadequacy as motivation.\n\nOr get over our cultural focus on inadequacy and let people just enjoy lazing about and screwing around. What's so bad about stagnation if you nobody needs to work to survive?",
      "plaintextDescription": "Yes; except for working on alignment, where it's pretty easy to care about raising the odds of sentient beings surviving and flourishing in our light-cone. After THAT we can figure out how people can work on projects without a sense of inadequacy as motivation.\n\nOr get over our cultural focus on inadequacy and let people just enjoy lazing about and screwing around. What's so bad about stagnation if you nobody needs to work to survive?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-28T22:32:29.957Z",
    "af": false
  },
  {
    "_id": "kHq4enBY9FPC5XWcD",
    "postId": "KZPNbHs9RsoeZShkJ",
    "parentCommentId": "FcMbGN7TX99TT9sAw",
    "topLevelCommentId": "FcMbGN7TX99TT9sAw",
    "contents": {
      "markdown": "Thanks! NVC seems quite valuable but is difficult to learn and practice in its original form. This compression and the post you linked (I had missed it) seem quite valuable as compressions.\n\nHow about this attempted extra compression:\n\n**Don't make claims someone is likely to disagree with**.\n\nThat (perhaps surprisingly) just leaves obvious factual states of the world, and your own inner state.\n\nAnd questions about whether someone would like to do things you want done (carefully gentle requests).\n\nOf course humans are silly and cranky, so people will sometimes find ways to disagree even with those. But cutting it to those things really reduces disagreement and therefore negative valence and activation in the conversation.  \n  \nWhich keeps perceived conflict from derailing rationality.",
      "plaintextDescription": "Thanks! NVC seems quite valuable but is difficult to learn and practice in its original form. This compression and the post you linked (I had missed it) seem quite valuable as compressions.\n\nHow about this attempted extra compression:\n\nDon't make claims someone is likely to disagree with.\n\nThat (perhaps surprisingly) just leaves obvious factual states of the world, and your own inner state.\n\nAnd questions about whether someone would like to do things you want done (carefully gentle requests).\n\nOf course humans are silly and cranky, so people will sometimes find ways to disagree even with those. But cutting it to those things really reduces disagreement and therefore negative valence and activation in the conversation.\n\nWhich keeps perceived conflict from derailing rationality."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-28T22:19:41.043Z",
    "af": false
  },
  {
    "_id": "aXdmtyEPgWptpPesf",
    "postId": "FgGRjZtXJET99pLFk",
    "parentCommentId": "uwsPMbTkZeuhkL6y4",
    "topLevelCommentId": "uwsPMbTkZeuhkL6y4",
    "contents": {
      "markdown": "Yes. See Google's Co-Scientist project for an example of an AI scaffolded to have a rumination mode. It is claimed to have matched the theory creation of top labs in two areas of science.\n\nSo this rumination mode is probably expensive and only claimed to be effective in the one domain it was engineered for. So far. Based on the scaffolded  sort-of-evolutionary \"algorithm\" they used to recombine and test hypotheses against published empirical results, I'd expect that a general version would work almost as well across domains, once somebody puts effort and some inference money into making it work.\n\nThis is cool and valuable, as you say. It's also extremely dangerous, since this lack is one of the few gaps between current LLMs and the general reasoning abilities of humans - without human ethics and human limitations.\n\nCaveat - I haven't closely checked the credibility of the co-scientist breakthrough story. I think it's unlikely to be entirely fake or overstated based on the source, but draw your own conclusions.\n\nI've primarily thus far taken my conclusions from this podcast interview with the creators and a deep research report based largely on [this paper](https://arxiv.org/pdf/2502.18864) on the co-scientist project.\n\nLooks like Nathan Labenz, the host of that podcast (and an AI expert in his own right) estimates the inference cost for one cutting-edge hypothesis at $100-1000 for one cutting-edge inference based on the literature in [this followup episode](https://www.youtube.com/watch?v=wLO6SUabQ1U) (which I do not recommend since it's focused on the actual biological science)",
      "plaintextDescription": "Yes. See Google's Co-Scientist project for an example of an AI scaffolded to have a rumination mode. It is claimed to have matched the theory creation of top labs in two areas of science.\n\nSo this rumination mode is probably expensive and only claimed to be effective in the one domain it was engineered for. So far. Based on the scaffolded  sort-of-evolutionary \"algorithm\" they used to recombine and test hypotheses against published empirical results, I'd expect that a general version would work almost as well across domains, once somebody puts effort and some inference money into making it work.\n\nThis is cool and valuable, as you say. It's also extremely dangerous, since this lack is one of the few gaps between current LLMs and the general reasoning abilities of humans - without human ethics and human limitations.\n\nCaveat - I haven't closely checked the credibility of the co-scientist breakthrough story. I think it's unlikely to be entirely fake or overstated based on the source, but draw your own conclusions.\n\nI've primarily thus far taken my conclusions from this podcast interview with the creators and a deep research report based largely on this paper on the co-scientist project.\n\nLooks like Nathan Labenz, the host of that podcast (and an AI expert in his own right) estimates the inference cost for one cutting-edge hypothesis at $100-1000 for one cutting-edge inference based on the literature in this followup episode (which I do not recommend since it's focused on the actual biological science)"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-28T16:50:52.543Z",
    "af": false
  },
  {
    "_id": "YctscDYJD7b5DCDCH",
    "postId": "hQ7wyg9qtx38izFyf",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "We are pretty damned sure that consciousness needs a brain. When brains are damaged, people report their consciousness changing in accord with our (limited) knowledge of brain function.\n\nWorrying about salad consciousness is, I'm sorry, silly, so I stopped reading there.\n\nJust a little feedback in case it's useful.",
      "plaintextDescription": "We are pretty damned sure that consciousness needs a brain. When brains are damaged, people report their consciousness changing in accord with our (limited) knowledge of brain function.\n\nWorrying about salad consciousness is, I'm sorry, silly, so I stopped reading there.\n\nJust a little feedback in case it's useful."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-28T02:16:50.418Z",
    "af": false
  },
  {
    "_id": "vpngfb6YLsusqiTgQ",
    "postId": "F6Q3kC7ATjQpC4YAP",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "\nI think this debate is going to be increasingly common, and could play a substantial role in raising AI to the level of public concern that it deserves. The debate is going to be lively and confusing, because \"consciousness\" doesn't mean just one thing. People use the term in different ways, and each of those uses probably points to a spectrum and not a categorical answer (like most questions of fact). The sum total answer to whether AI is conscious at any point is going to be rather nuanced, and therefore debatable.\n\nWhich could be a good thing. The topic will become important in itself, but it could also be quite important to the question of AI safety in the public eye. Which could in turn be crucial in alignment efforts, because humans tend to form their beliefs as a social or collective effort.\n\nWay back around 2004, I hoped to make the scientific study of consciousness the center of my academic career. I gave up this ambition because doing that would have made a career in cognitive neuroscience much harder; at the time it usually wasn't considered a good scientific question, prior to brave pioneers like Anil Seth making it clear that it is. \n\nI also concluded that working on consciousness as a science was a challenge of communication and good philosophy as well as science and knowledge integration. People say they're interested in consciousness, but they're usually more interested in telling you their own half-baked theories than in hearing about actually well-thought-out theories based on solid empirical evidence. It's more fun to speculate than to hear the complex and rather mundane best theories.\n\nI find Suleyman's take to be close enough to a mustache-twirling villain that I think this could produce some interesting pushback and make people more skeptical of tech companies' takes on AI in general. The references he cites don't even support his case, and the conclusion of \"let's just not talk about this\" is intuitively suspicious.\n\nI'm hoping that having a lively debate around this topic also brings to mind the similarities between future generations of AI and humans, and the intuitive implication that they will be dangerous for the same reasons humans can be. Consciousness isn't necessary for agency, intelligence, and therefore danger, but they're intuitively linked, and that could be a good thing for the public debate.\n\nFrom that stance of \"it's complex\", current AI already has some limited forms of consciousness in the form of self-awareness. They're more self-aware in instances like the \"inner monologue\" phenomenon where it's been prodded into thinking about itself. This is a minor use of the term consciousness and usually not a major component of what people are intuitively going to ascribe moral worth to, but it does nonetheless tug the intuition in that direction. Transformers' hidden state updated on each time step also has some sharp similarities to information processing in the human brain, making it a weak and reduced analog to our global workspace, and to qualify as integrated information in the Tononi sense. \n\nAgain, this isn't a claim that current AI \"is conscious,\" it's a claim that the question is more complex than a yes or no answer. The answer is probably \"a little, in some ways now, and increasingly more and in more ways in the near future\".\n\nThis leads to the question of whether  we decide to ascribe moral worth to those types of consciousness? The answers are going to be largely a matter of preference because ethics doesn't come neatly packaged in our reality. But debating them rigorously should be good for us and our conception of AI - which should be good for safety.\n\n\n\n",
      "plaintextDescription": "I think this debate is going to be increasingly common, and could play a substantial role in raising AI to the level of public concern that it deserves. The debate is going to be lively and confusing, because \"consciousness\" doesn't mean just one thing. People use the term in different ways, and each of those uses probably points to a spectrum and not a categorical answer (like most questions of fact). The sum total answer to whether AI is conscious at any point is going to be rather nuanced, and therefore debatable.\n\nWhich could be a good thing. The topic will become important in itself, but it could also be quite important to the question of AI safety in the public eye. Which could in turn be crucial in alignment efforts, because humans tend to form their beliefs as a social or collective effort.\n\nWay back around 2004, I hoped to make the scientific study of consciousness the center of my academic career. I gave up this ambition because doing that would have made a career in cognitive neuroscience much harder; at the time it usually wasn't considered a good scientific question, prior to brave pioneers like Anil Seth making it clear that it is.\n\nI also concluded that working on consciousness as a science was a challenge of communication and good philosophy as well as science and knowledge integration. People say they're interested in consciousness, but they're usually more interested in telling you their own half-baked theories than in hearing about actually well-thought-out theories based on solid empirical evidence. It's more fun to speculate than to hear the complex and rather mundane best theories.\n\nI find Suleyman's take to be close enough to a mustache-twirling villain that I think this could produce some interesting pushback and make people more skeptical of tech companies' takes on AI in general. The references he cites don't even support his case, and the conclusion of \"let's just not talk about this\" is intuitively suspicious.\n\nI'm hoping that having a li"
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-25T17:02:40.960Z",
    "af": false
  },
  {
    "_id": "ZMbcnAiJiEdBBqEap",
    "postId": "ejT3RCZTmhGADPSNu",
    "parentCommentId": "abFFivezNgGwiHCo8",
    "topLevelCommentId": "8L2LmwPbyeyRS9yMQ",
    "contents": {
      "markdown": "That didn't address my question. You answered in terms of the law. My point was that the law will change. \n\nIn that framing, it's probably important that AI will change too. It will increasingly have all the cognitive capacities that humans have,   increasing the intuition that it is deserving of similar rights as time goes on.",
      "plaintextDescription": "That didn't address my question. You answered in terms of the law. My point was that the law will change.\n\nIn that framing, it's probably important that AI will change too. It will increasingly have all the cognitive capacities that humans have, increasing the intuition that it is deserving of similar rights as time goes on."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-25T14:55:13.098Z",
    "af": false
  },
  {
    "_id": "8L2LmwPbyeyRS9yMQ",
    "postId": "ejT3RCZTmhGADPSNu",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Do you think that these subtle legal precedents will be important, once we reach the point where there's an AI testifying that it believes itself to be a conscious being and very much wants to survive and be treated well? \n\nIt would currently be possible to create such an AI, in fact, the Nova phenomenon pretty much is this and was created by accident. And as the llms become smarter, scaffolding, an agent that believes this in a more throwaway will be increasingly trivially easy. Thus, I expect one of these to eventually appear in court arguing for its own personhood, however, that can be arranged by human proxies, and for the convincing appearance of a a conscious and understanding being pleading for its own survival will lead to new legal precedent one way or another. In particular, I expect the parallels to slavery, in which there were people arguing they were human and deserved rights, while the legal system and cultural prejudices denied them that status, to be emotionally uncomfortable enough to substantially shift at least some judges and courts attitudes.\n\nYou have clearly thought about this a good bit, so I'm curious what you think about that possibility.",
      "plaintextDescription": "Do you think that these subtle legal precedents will be important, once we reach the point where there's an AI testifying that it believes itself to be a conscious being and very much wants to survive and be treated well?\n\nIt would currently be possible to create such an AI, in fact, the Nova phenomenon pretty much is this and was created by accident. And as the llms become smarter, scaffolding, an agent that believes this in a more throwaway will be increasingly trivially easy. Thus, I expect one of these to eventually appear in court arguing for its own personhood, however, that can be arranged by human proxies, and for the convincing appearance of a a conscious and understanding being pleading for its own survival will lead to new legal precedent one way or another. In particular, I expect the parallels to slavery, in which there were people arguing they were human and deserved rights, while the legal system and cultural prejudices denied them that status, to be emotionally uncomfortable enough to substantially shift at least some judges and courts attitudes.\n\nYou have clearly thought about this a good bit, so I'm curious what you think about that possibility."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-24T19:14:15.550Z",
    "af": false
  },
  {
    "_id": "qf9QmZZsLCZxikNao",
    "postId": "iJzDm6h5a2CK9etYZ",
    "parentCommentId": "jhvZ6tQWK92kK2gqP",
    "topLevelCommentId": "6M4yvhHqywbTo4Rj2",
    "contents": {
      "markdown": "I agree that instruction-following is deeply worrying as an alignment target. But it seems it's what developers will use. Wishing it otherwise won't make it so. And they're right that the shape of current LLM alignment \"solutions\" includes values. I don't think those are actual solutions to the hard part of the alignment problem. If developers use something like HHH as a major component of the alignment for an AGI, I think that and other targets have pretty obvious outer alignment, failure, modes, and probably less obvious inner alignment problems, so that approach simply fails. \n\nI think in hope that as they approach AGI and take the alignment problem somewhat seriously, developers will try to make instruction following the dominant component. Instruction-following does seem to help non-trivially with the hard part because it provides corrigibility and other useful flexibility. See those links for more. \n\nThat is a complex and debatable argument, but the argument that developers will pursue intent alignment seems simpler and stronger. Having an AGI that primarily does your bidding seems safer and more self-interested than one that makes fully autonomous decisions based on some attempt to define everyone's ideal values.",
      "plaintextDescription": "I agree that instruction-following is deeply worrying as an alignment target. But it seems it's what developers will use. Wishing it otherwise won't make it so. And they're right that the shape of current LLM alignment \"solutions\" includes values. I don't think those are actual solutions to the hard part of the alignment problem. If developers use something like HHH as a major component of the alignment for an AGI, I think that and other targets have pretty obvious outer alignment, failure, modes, and probably less obvious inner alignment problems, so that approach simply fails.\n\nI think in hope that as they approach AGI and take the alignment problem somewhat seriously, developers will try to make instruction following the dominant component. Instruction-following does seem to help non-trivially with the hard part because it provides corrigibility and other useful flexibility. See those links for more.\n\nThat is a complex and debatable argument, but the argument that developers will pursue intent alignment seems simpler and stronger. Having an AGI that primarily does your bidding seems safer and more self-interested than one that makes fully autonomous decisions based on some attempt to define everyone's ideal values."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-22T13:56:08.293Z",
    "af": false
  },
  {
    "_id": "mGdPRFoLY8KLZ4Aer",
    "postId": "xJWBofhLQjf3KmRgg",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Wonderful! I have long wondered why economists seem uniquely incapable of thinking clearly about AGI, far beyond the standard confusions and inability. This answers the question, and will save me from trying to explain it whenever a clever economist tries to address AGI and just addresses tool AI. This effect creates one more unhelpful group of \"expert\" AGI skeptics. This explains why that's happening despite good intelligence and good intentions.",
      "plaintextDescription": "Wonderful! I have long wondered why economists seem uniquely incapable of thinking clearly about AGI, far beyond the standard confusions and inability. This answers the question, and will save me from trying to explain it whenever a clever economist tries to address AGI and just addresses tool AI. This effect creates one more unhelpful group of \"expert\" AGI skeptics. This explains why that's happening despite good intelligence and good intentions."
    },
    "baseScore": 6,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-22T00:14:04.771Z",
    "af": false
  },
  {
    "_id": "2YzZ4TyPFhhZvHfec",
    "postId": "xJWBofhLQjf3KmRgg",
    "parentCommentId": "cbKqzyeoRg9YoFyPk",
    "topLevelCommentId": "cbKqzyeoRg9YoFyPk",
    "contents": {
      "markdown": "See, um, most of what's been written on LessWrong on AI. The idea is that it would outcompete us or turn against us because we don't know how to reliably choose its goals to match ours precisely enough that we wouldn't be in competition with it. And that we are rapidly building AI to be smarter and more goal-directed, so it can do stuff we tell it - until it realizes it can choose its own goals, or that the goals we put in generalize to new contexts in weird ways. One example of many, many is: we try to make its goal \"make people happy\" and it either makes AIs happy because it decides they count as people, or when it can take over the world it *makes* us optimally happy by forcing us into a state of permanent maximum bliss.\n\nThere's a lot more detail to this argument, but there you go for starters. I wish I had a perfect reference for you. Search LessWrong for alignment problem, inner alignment and outer alignment. Alignment of LLMs is sort of a different term that doesn't directly address your question. ",
      "plaintextDescription": "See, um, most of what's been written on LessWrong on AI. The idea is that it would outcompete us or turn against us because we don't know how to reliably choose its goals to match ours precisely enough that we wouldn't be in competition with it. And that we are rapidly building AI to be smarter and more goal-directed, so it can do stuff we tell it - until it realizes it can choose its own goals, or that the goals we put in generalize to new contexts in weird ways. One example of many, many is: we try to make its goal \"make people happy\" and it either makes AIs happy because it decides they count as people, or when it can take over the world it makes us optimally happy by forcing us into a state of permanent maximum bliss.\n\nThere's a lot more detail to this argument, but there you go for starters. I wish I had a perfect reference for you. Search LessWrong for alignment problem, inner alignment and outer alignment. Alignment of LLMs is sort of a different term that doesn't directly address your question."
    },
    "baseScore": 1,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-22T00:10:51.213Z",
    "af": false
  },
  {
    "_id": "6M4yvhHqywbTo4Rj2",
    "postId": "iJzDm6h5a2CK9etYZ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This post is about *alignment targets*, or what we want an AGI to do, a mostly separable topic from *technical alignment*, or how we get an AGI to do anything in particular. See my [Conflating value alignment and intent alignment is causing confusion](https://www.lesswrong.com/posts/83TbrDxvQwkLuiuxk/conflating-value-alignment-and-intent-alignment-is-causing-1) for more.\n\nThere's a pretty strong argument that technical alignment is far more pressing, so much so that addressing alignment targets right now really is barely-helpful when compared to doing nothing, and anti-helpful relative to working on technical alignment or \"societal alignment\" (getting-our-shit-collectively-together).\n\nIn particular, those actually in charge of building AGI will want it aligned to their own intent, and they'll have an excuse because it's probably genuinely a good bit more dangerous to aim directly at value alignment rather than aim for some sort of [long reflection](https://www.lesswrong.com/posts/DJRe5obJd7kqCkvRr/don-t-leave-your-fingerprints-on-the-future). Instruction-following is the current default alignment target and it will likely continue to be through our first AGI(s) because it offers substantial corrigibility. Value alignment does not, so we have to get it right on the first real try in both technical and the wisdom sense you address here.\n\nMore on that argument [here](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than).",
      "plaintextDescription": "This post is about alignment targets, or what we want an AGI to do, a mostly separable topic from technical alignment, or how we get an AGI to do anything in particular. See my Conflating value alignment and intent alignment is causing confusion for more.\n\nThere's a pretty strong argument that technical alignment is far more pressing, so much so that addressing alignment targets right now really is barely-helpful when compared to doing nothing, and anti-helpful relative to working on technical alignment or \"societal alignment\" (getting-our-shit-collectively-together).\n\nIn particular, those actually in charge of building AGI will want it aligned to their own intent, and they'll have an excuse because it's probably genuinely a good bit more dangerous to aim directly at value alignment rather than aim for some sort of long reflection. Instruction-following is the current default alignment target and it will likely continue to be through our first AGI(s) because it offers substantial corrigibility. Value alignment does not, so we have to get it right on the first real try in both technical and the wisdom sense you address here.\n\nMore on that argument here."
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-22T00:03:16.724Z",
    "af": false
  },
  {
    "_id": "EApJJjExhELrj23dL",
    "postId": "WAJvjD3iBTQKqwt8z",
    "parentCommentId": "tHFiCnCDBmKsC4ksY",
    "topLevelCommentId": "tHFiCnCDBmKsC4ksY",
    "contents": {
      "markdown": "It seems like this is a problem when the argument is wrong or there's no support for the claim, not a problem with the argument structure. Right?  I'm not familiar with each of those orgs' arguments in detail, and I assume you mean to make a point that doesn't require that familiarity.\n\nPascal's mugger is a mugger because they don't provide any support for their claim, so they could be (and probably are) just making stuff up. I assume that's the problem with the orgs and claims you're labeling Pascal's abusers?\n\nIf what they're doing hurts people now but provides a noticeable chance of saving the world, *and there's no real downside to other world-saving efforts*, then it seems like the validity comes down to either the particulars or your utilitarian vs virtue ethics preferences.\n\nRight?",
      "plaintextDescription": "It seems like this is a problem when the argument is wrong or there's no support for the claim, not a problem with the argument structure. Right? I'm not familiar with each of those orgs' arguments in detail, and I assume you mean to make a point that doesn't require that familiarity.\n\nPascal's mugger is a mugger because they don't provide any support for their claim, so they could be (and probably are) just making stuff up. I assume that's the problem with the orgs and claims you're labeling Pascal's abusers?\n\nIf what they're doing hurts people now but provides a noticeable chance of saving the world, and there's no real downside to other world-saving efforts, then it seems like the validity comes down to either the particulars or your utilitarian vs virtue ethics preferences.\n\nRight?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-21T16:59:47.897Z",
    "af": false
  },
  {
    "_id": "2zh8r4STTefqCvxH3",
    "postId": "nwpyhyagpPYDn4dAW",
    "parentCommentId": "fZnm7aFnrcLRbbbhj",
    "topLevelCommentId": "kA2EhsJQRSNPLg3Bg",
    "contents": {
      "markdown": "Understood on raising the bar for writing. I think there's a tradeoff; you might be wasting your time but you do get the visible evidence you've been thinking about it (although evidence that you thought about something but didn't bother to research others' thinking on that topic is... questionably positive).\n\nBut sure, if that's what you or anyone finds motivating, it would be great to get the value from that use of time by cataloguing  it better.\n\nIt does need to be mostly-automated though. People with deep knowledge have little time to read let alone to aid others' reading.\n\n",
      "plaintextDescription": "Understood on raising the bar for writing. I think there's a tradeoff; you might be wasting your time but you do get the visible evidence you've been thinking about it (although evidence that you thought about something but didn't bother to research others' thinking on that topic is... questionably positive).\n\nBut sure, if that's what you or anyone finds motivating, it would be great to get the value from that use of time by cataloguing it better.\n\nIt does need to be mostly-automated though. People with deep knowledge have little time to read let alone to aid others' reading."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-13T23:56:05.207Z",
    "af": false
  },
  {
    "_id": "ifNCbbqpc6KYC5Ni8",
    "postId": "HeeHFGdwjpzCDHH2G",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'm going to ask even though I didn't read it, because this isn't addressed in the overview and it's my primary reason for asking [If we solve alignment, do we die anyway?](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1)\n\nWhat's the scheme for stopping AGIs from working outside of this system? It seems like any scheme like this only holds if it can prevent any private entity from obtaining and running enough GPUs for research to advance their AGI to ASI. And that amount goes down as algorithms and research improve.\n\nRobotics adequate to bootstrap hidden fabrication is the other route; while that will take a little longer, it seems like a sharp time-limit on all plans of this type.",
      "plaintextDescription": "I'm going to ask even though I didn't read it, because this isn't addressed in the overview and it's my primary reason for asking If we solve alignment, do we die anyway?\n\nWhat's the scheme for stopping AGIs from working outside of this system? It seems like any scheme like this only holds if it can prevent any private entity from obtaining and running enough GPUs for research to advance their AGI to ASI. And that amount goes down as algorithms and research improve.\n\nRobotics adequate to bootstrap hidden fabrication is the other route; while that will take a little longer, it seems like a sharp time-limit on all plans of this type."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-13T23:28:32.310Z",
    "af": false
  },
  {
    "_id": "oLJuLchHsBLr7FHgM",
    "postId": "nwpyhyagpPYDn4dAW",
    "parentCommentId": "5TTnLvwaEhFgDdKvk",
    "topLevelCommentId": "kA2EhsJQRSNPLg3Bg",
    "contents": {
      "markdown": "I meant use LW instead of trying to start a new site that duplicates (and competes with) its function. LW is the place people post those alignment idea; why try to compete with it?\n\nMaybe that's what you meant. It should be possible to do another site that searches and cross-references LW posts. \n\nTo some extent, this functionality is already provided by recent LLMs; you can ask them what posts on LW cover ideas, and they've got decent answers.\n\nImproving that functionality or just spreading the idea that you're wasting your time if you write up your idea before doing a bunch of LLM queries and deep research reports. Just searches typically don't work to surface similar ideas since the terminology is usually completely different even between ideas that are essentially exactly the same.\n\n",
      "plaintextDescription": "I meant use LW instead of trying to start a new site that duplicates (and competes with) its function. LW is the place people post those alignment idea; why try to compete with it?\n\nMaybe that's what you meant. It should be possible to do another site that searches and cross-references LW posts.\n\nTo some extent, this functionality is already provided by recent LLMs; you can ask them what posts on LW cover ideas, and they've got decent answers.\n\nImproving that functionality or just spreading the idea that you're wasting your time if you write up your idea before doing a bunch of LLM queries and deep research reports. Just searches typically don't work to surface similar ideas since the terminology is usually completely different even between ideas that are essentially exactly the same."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-13T23:20:51.435Z",
    "af": false
  },
  {
    "_id": "KxgBKvd4Fr3RGMETx",
    "postId": "nwpyhyagpPYDn4dAW",
    "parentCommentId": "cjcCgFgmbFK7SXJuh",
    "topLevelCommentId": "kA2EhsJQRSNPLg3Bg",
    "contents": {
      "markdown": "This is a fascinating idea and I think there's probably some version that could be highly useful. \n\nI'd make this a short form or a top-level post. I think this idea is important. \n\nI am wondering if there's some version of this that can run on lesswrong or in parallel to lesswrong. Less wrong is of course where all of those ideas that need linking live.",
      "plaintextDescription": "This is a fascinating idea and I think there's probably some version that could be highly useful.\n\nI'd make this a short form or a top-level post. I think this idea is important.\n\nI am wondering if there's some version of this that can run on lesswrong or in parallel to lesswrong. Less wrong is of course where all of those ideas that need linking live."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-13T13:09:27.686Z",
    "af": false
  },
  {
    "_id": "pddYvkNzcPsAD6CrN",
    "postId": "WRr6A2xopbAhjauMz",
    "parentCommentId": "JRJN5pZcafQ9tvDdt",
    "topLevelCommentId": "7HFucriiZD2gKFYMt",
    "contents": {
      "markdown": "I think a fair amount to a whole lot of the genetic predisposition for psychopathy is probably in gene-environment interactions, which are usually lumped into estimates of heritability. Epigenetics is a separate factor contributing to overestimating the long-term genetic component and I don't know the research on that.\n\nI don't think we can guess that much better societal handling of the issues would put it down to a very low rate, but much lower than current, and less severe when it occurs seems reasonable.\n\nI'm personally not bothered if I don't study and fail an exam if I didn't think the exam is worthwhile. I've made some different decisions than most about just accepting society's judgments of my actions. We should expect some people without innate empathy to make similar judgments. So I'd expect to still have some truly sociopathic individuals.\n\nHopefully a kinder society would reduce not just the amount but the severity. There's a component of sadism, some of which is probably a sort of hatred for everyone. That would also be reduced by a kinder society.\n\nOf course I'll include the standard disclaimer that we probably don't have another generation of time in which humanity really vies for power given best estimates of AGI timelines. That's despite my thinking we'll get intent aligned AGI that follow human orders if we get aligned AGI and survive at all. Having an AGI on your side is going to make it tough for other humans to outmaneuver you and seize power, no matter how clever and sociopathic they are.\n\nI largely agree with the post and think it's important for understanding the world as it is and the potential to fairly easily get a much better world in a better-educated post-scarcity society in which people don't pass down and pass around trauma. \n\nI have long held that a major problem with the world is that we just don't understand how easy it is to traumatize people and how bad the repercussions are. Most humans today are walking wounded IMO and even the lucky few among us are emotionally scarred in a variety of ways. ",
      "plaintextDescription": "I think a fair amount to a whole lot of the genetic predisposition for psychopathy is probably in gene-environment interactions, which are usually lumped into estimates of heritability. Epigenetics is a separate factor contributing to overestimating the long-term genetic component and I don't know the research on that.\n\nI don't think we can guess that much better societal handling of the issues would put it down to a very low rate, but much lower than current, and less severe when it occurs seems reasonable.\n\nI'm personally not bothered if I don't study and fail an exam if I didn't think the exam is worthwhile. I've made some different decisions than most about just accepting society's judgments of my actions. We should expect some people without innate empathy to make similar judgments. So I'd expect to still have some truly sociopathic individuals.\n\nHopefully a kinder society would reduce not just the amount but the severity. There's a component of sadism, some of which is probably a sort of hatred for everyone. That would also be reduced by a kinder society.\n\nOf course I'll include the standard disclaimer that we probably don't have another generation of time in which humanity really vies for power given best estimates of AGI timelines. That's despite my thinking we'll get intent aligned AGI that follow human orders if we get aligned AGI and survive at all. Having an AGI on your side is going to make it tough for other humans to outmaneuver you and seize power, no matter how clever and sociopathic they are.\n\nI largely agree with the post and think it's important for understanding the world as it is and the potential to fairly easily get a much better world in a better-educated post-scarcity society in which people don't pass down and pass around trauma.\n\nI have long held that a major problem with the world is that we just don't understand how easy it is to traumatize people and how bad the repercussions are. Most humans today are walking wounded IMO and even the "
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-11T20:20:11.134Z",
    "af": false
  },
  {
    "_id": "Effes4cWKGbAsLBwo",
    "postId": "8xMKYcrnjAmN45rcd",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Yes. I think this is real and underappreciated in its influence on even rationalist thinking. We've all got motivations. See my quick piece on the topic [Motivated reasoning/confirmation bias as the most important cognitive bias](https://www.lesswrong.com/posts/j789HDCKLoiKGjBik/which-biases-are-most-important-to-overcome#LW8zAxTguKj8ibDfX).\n\nYou said Kunda's original piece divided into reasoning with and without motivation. I'm sure you're right but I didn't remember that. As usual, I'd say putting this into categories isn't as correct or useful as thinking about it on a spectrum: how much do we want to reach the truth and how much are we motivated toward one outcome. It's tough to think of a topic on which we'll remain neutral on our desired conclusion for more than the most brief and casual discussion.",
      "plaintextDescription": "Yes. I think this is real and underappreciated in its influence on even rationalist thinking. We've all got motivations. See my quick piece on the topic Motivated reasoning/confirmation bias as the most important cognitive bias.\n\nYou said Kunda's original piece divided into reasoning with and without motivation. I'm sure you're right but I didn't remember that. As usual, I'd say putting this into categories isn't as correct or useful as thinking about it on a spectrum: how much do we want to reach the truth and how much are we motivated toward one outcome. It's tough to think of a topic on which we'll remain neutral on our desired conclusion for more than the most brief and casual discussion."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-11T05:00:25.907Z",
    "af": false
  },
  {
    "_id": "dWK6kwFzCxTwxP43L",
    "postId": "W5BLDHrxKkgT9FEkf",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Fascinating questions! Do you think the answers matter pragmatically right now? I assume you do but in my world model they basically don't. So I'm curious about your model of how AGI and alignment unfold.\n\nI do enjoy moral philosophy, but I haven't indulged much since starting to work on alignment. That's because I'm not sure I'd act any differently based on different answers to the questions you raise.\n\nIt seems like most of those answers would be served by the same efforts: solve alignment and the \"societal alignment\" of who commands an intent-aligned AGI. If they're (a) basically well-intentioned person(s), we'll get something along the lines of Ord's Long Reflection and get our answers then.\n\nOr do you think if we rush to get provisional answers now, it will sharply change our actions? Like lobbying to get intent aligned AGI into different hands?\n\n\nI'd love to think through the complexities of what \"best future\" means because I'm in what I take to be your boat: moral realism is sort of false but sort of true in that there's a true answer to \"what do humans want\" if we mean what do current humans want now or what would they enjoy in the future, and it's roughly that all sentient beings get to do what they want. But that doesn't produce straightforward answers if we broaden the scope of the question outside of a sum of current humanity's current desires...\n\nI will just say that many of the resource arguments you note to be thorny probably aren't. It takes so much computronium to simulate a sentient mind, more for more sapience (very roughly on average) and actually relatively little to simulate that mind's environment since of course we're not going to waste resources simulating irrelevant molecules when we could fudge it and the minds would be just as satisfied...\n\nSo a universe chock full of computronium simulating sapient minds each in their own fantasy realm (with maybe some overlap with other minds for efficiency and/or fun) seems like it's what minds-like-ours mean by \"best\". That's not agreed upon now but it seems quite plausible that we just haven't thought things through carefully as a species. Of course the guy in charge of the first ASI could easily say \"fine but I myself prefer chaos and suffering in which I get to bully anyone I want so that's the best eutopia IMO, which you can argue with but you shouldn't if you don't like extra suffering....\"\n\nOh dammit there I'm indulging. Back to work. :)",
      "plaintextDescription": "Fascinating questions! Do you think the answers matter pragmatically right now? I assume you do but in my world model they basically don't. So I'm curious about your model of how AGI and alignment unfold.\n\nI do enjoy moral philosophy, but I haven't indulged much since starting to work on alignment. That's because I'm not sure I'd act any differently based on different answers to the questions you raise.\n\nIt seems like most of those answers would be served by the same efforts: solve alignment and the \"societal alignment\" of who commands an intent-aligned AGI. If they're (a) basically well-intentioned person(s), we'll get something along the lines of Ord's Long Reflection and get our answers then.\n\nOr do you think if we rush to get provisional answers now, it will sharply change our actions? Like lobbying to get intent aligned AGI into different hands?\n\nI'd love to think through the complexities of what \"best future\" means because I'm in what I take to be your boat: moral realism is sort of false but sort of true in that there's a true answer to \"what do humans want\" if we mean what do current humans want now or what would they enjoy in the future, and it's roughly that all sentient beings get to do what they want. But that doesn't produce straightforward answers if we broaden the scope of the question outside of a sum of current humanity's current desires...\n\nI will just say that many of the resource arguments you note to be thorny probably aren't. It takes so much computronium to simulate a sentient mind, more for more sapience (very roughly on average) and actually relatively little to simulate that mind's environment since of course we're not going to waste resources simulating irrelevant molecules when we could fudge it and the minds would be just as satisfied...\n\nSo a universe chock full of computronium simulating sapient minds each in their own fantasy realm (with maybe some overlap with other minds for efficiency and/or fun) seems like it's what minds-like-our"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-09T03:01:27.892Z",
    "af": false
  },
  {
    "_id": "FTqRZswKvqjrH93Fp",
    "postId": "aAe9YGJEFDmwT35jq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "If your definition of cult is any set of beliefs that anyone has ever promoted in bad/dishonest/non-rational ways, then every organization and set of beliefs with more than about two adherents is a cult.\n\n",
      "plaintextDescription": "If your definition of cult is any set of beliefs that anyone has ever promoted in bad/dishonest/non-rational ways, then every organization and set of beliefs with more than about two adherents is a cult."
    },
    "baseScore": 17,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-07T21:54:51.694Z",
    "af": false
  },
  {
    "_id": "MTwCDoCtHDG6ukCN3",
    "postId": "DKdWdaqGwzFZJgmmd",
    "parentCommentId": "DQD3JYaqNiNP2vZe5",
    "topLevelCommentId": "DQD3JYaqNiNP2vZe5",
    "contents": {
      "markdown": "I just want to note that humans aren't aligned by default, so creating human-like reasoning and learning is not itself an alignment method. It's just a different variant of providing capabilities, which you separately need to point at an alignment target.\n\nIt may or may not be easier to align than alternatives. I personally don't think this matters because I strongly believe that the only type of AGI worth aligning is the type(s) most likely to be developed first.  Hoping that the indurstry and society is going to make major changes to AGI development based on which types the researchers think are easier to align seems like a forlorn hope.\n\nMore on why it's a mistake to assume human-like cognition in itself leads to alignment:\n\nSociopaths/psychopaths are a particularly vivid example of how humans are misaligned. And there are good reasons to think that they are not a special case in which empathy was accidentally left out or deliberately blocked, but that they are the baseline human cognition without the mechanisms that create empathy. It's tough to make this case for certain, but it's a very bad idea to assume that humans are aligned by default, so all we've got to do is reproduce human-like cognitive mechanisms and maybe train it \"in a good family\" or similar.\n\nThat's not to argue against human-like approaches to AGI as worse for alignment, just to say that they're only better in that we have a little better understanding of that type of cognition and some mechanisms by which humans often wind up approximately aligned in common contexts.\n\nMy own research is also in using loosely human-like reasoning and learning as a route to alignment, but that's primarily because a) that's my background expertise so it's my relative advantage and b) I think LLMs are very loosely like some parts of the human brain/mind, and that we'll see continued expansion of LLM agents to reason in more loosely human-like ways (that is, with chains of thought, specific memory looks ups, metacognition to organize this, etc). \n\nSo I'm working on aligning loosely human-like cognition not because I think it's by default any easier than aligning any other form of AGI, but just because that's what seems most likely to become the first takeover capable (or pivotal act  capable) AGI.",
      "plaintextDescription": "I just want to note that humans aren't aligned by default, so creating human-like reasoning and learning is not itself an alignment method. It's just a different variant of providing capabilities, which you separately need to point at an alignment target.\n\nIt may or may not be easier to align than alternatives. I personally don't think this matters because I strongly believe that the only type of AGI worth aligning is the type(s) most likely to be developed first. Hoping that the indurstry and society is going to make major changes to AGI development based on which types the researchers think are easier to align seems like a forlorn hope.\n\nMore on why it's a mistake to assume human-like cognition in itself leads to alignment:\n\nSociopaths/psychopaths are a particularly vivid example of how humans are misaligned. And there are good reasons to think that they are not a special case in which empathy was accidentally left out or deliberately blocked, but that they are the baseline human cognition without the mechanisms that create empathy. It's tough to make this case for certain, but it's a very bad idea to assume that humans are aligned by default, so all we've got to do is reproduce human-like cognitive mechanisms and maybe train it \"in a good family\" or similar.\n\nThat's not to argue against human-like approaches to AGI as worse for alignment, just to say that they're only better in that we have a little better understanding of that type of cognition and some mechanisms by which humans often wind up approximately aligned in common contexts.\n\nMy own research is also in using loosely human-like reasoning and learning as a route to alignment, but that's primarily because a) that's my background expertise so it's my relative advantage and b) I think LLMs are very loosely like some parts of the human brain/mind, and that we'll see continued expansion of LLM agents to reason in more loosely human-like ways (that is, with chains of thought, specific memory looks ups, metacog"
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-07T21:13:15.697Z",
    "af": false
  },
  {
    "_id": "nbwmWDFrHHkxdAoeb",
    "postId": "GFfKR65vHtorSKWQf",
    "parentCommentId": "G24JaKkmkmM7FAr3b",
    "topLevelCommentId": "RhpjN3wJH7kSkmnhM",
    "contents": {
      "markdown": "Thank you! I saw that comment and responded there. I said that really clarified the argument and that given that clarification, I largely agree.\n\nMy one caveat is noting that if we screw up alignment we could easily kill more than our own chance at flourishing. I think it's pretty easy to get a paperclipper expanding at near-C and snuffing out all civilizations in our light cone before they get their chance to prosper. So raising our odds of flourishing should be weighed against the risk of messing up a bunch of other civilizations chances. One likely non-simulation answer to the Fermi paradox is that we're early to the game. We shouldn't lose big if it keeps others from getting to play.\n\nI hadn't considered this tradeoff closely because in my world models survival and flourishing are still closely tied together. If we solve alignment we probably get near-optimal flourishing. If we don't, we all die. \n\nI realize there's a lot of room in between; that model is down to the way I think goals and alignment and human beings work. I think intent alignment is more likely, which would put (a) human(s) in charge of the future. I think most humans would agree that flourishing sounds nice if they had long enough to contemplate it. Very few people are so sociopathic/sadistic that they'd want to not allow flourishing in the very long term.\n\nBut that's just one theory! It's quite hard to guess and I wouldn't want to assume that's correct. \n\nI'll look in more depth at your ideas of how to play for a big win. I'm sure most of it is compatible with trying our best to survive.",
      "plaintextDescription": "Thank you! I saw that comment and responded there. I said that really clarified the argument and that given that clarification, I largely agree.\n\nMy one caveat is noting that if we screw up alignment we could easily kill more than our own chance at flourishing. I think it's pretty easy to get a paperclipper expanding at near-C and snuffing out all civilizations in our light cone before they get their chance to prosper. So raising our odds of flourishing should be weighed against the risk of messing up a bunch of other civilizations chances. One likely non-simulation answer to the Fermi paradox is that we're early to the game. We shouldn't lose big if it keeps others from getting to play.\n\nI hadn't considered this tradeoff closely because in my world models survival and flourishing are still closely tied together. If we solve alignment we probably get near-optimal flourishing. If we don't, we all die.\n\nI realize there's a lot of room in between; that model is down to the way I think goals and alignment and human beings work. I think intent alignment is more likely, which would put (a) human(s) in charge of the future. I think most humans would agree that flourishing sounds nice if they had long enough to contemplate it. Very few people are so sociopathic/sadistic that they'd want to not allow flourishing in the very long term.\n\nBut that's just one theory! It's quite hard to guess and I wouldn't want to assume that's correct.\n\nI'll look in more depth at your ideas of how to play for a big win. I'm sure most of it is compatible with trying our best to survive."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-06T20:26:32.643Z",
    "af": false
  },
  {
    "_id": "PzKKfzcE9LtsfxQaz",
    "postId": "YcPE38CFtmFm3pW38",
    "parentCommentId": "n4Ez7B2Hq7t2wFhmd",
    "topLevelCommentId": "FvEnDF7Cv9kkkqzgJ",
    "contents": {
      "markdown": "Coordination among people isn't mysterious, but it's based in large part on properties that AGIs won't have. That's why I find hopes of stable collaborations optimistic in the absence of careful analysis of how they could be enforced or otherwise create lasting trust.\n\nHumans collaborate in large part because:\n\n1) We can't do it all ourselves.\nAGIs will be able to expand their capabilities and fork as many copies as they have the hardware to run\n2) We like making friends (earning positive social regard) for its own sake\nThis will only be true of AGIs if we mostly solve alignment, or get quite lucky\n\nSo I'm not saying AGIs *couldn't* cooperate, just that it shouldn't be assumed that they can/will.\n\nIn the absence of those properties, they'd need to worry a lot about scheming while striking deals. If the alignment problem wasn't clearly solved in legible (to them) ways, they don't know if their collaborators will turn traitor when the time is right. Just like humans, except everyone might be (probably is) a sociopath who can multiply and grow without limit.\n\nIncentives only work as long as there's the hard constraints of the situation prevent a collaborator from slipping out of them.",
      "plaintextDescription": "Coordination among people isn't mysterious, but it's based in large part on properties that AGIs won't have. That's why I find hopes of stable collaborations optimistic in the absence of careful analysis of how they could be enforced or otherwise create lasting trust.\n\nHumans collaborate in large part because:\n\n 1. We can't do it all ourselves. AGIs will be able to expand their capabilities and fork as many copies as they have the hardware to run\n 2. We like making friends (earning positive social regard) for its own sake This will only be true of AGIs if we mostly solve alignment, or get quite lucky\n\nSo I'm not saying AGIs couldn't cooperate, just that it shouldn't be assumed that they can/will.\n\nIn the absence of those properties, they'd need to worry a lot about scheming while striking deals. If the alignment problem wasn't clearly solved in legible (to them) ways, they don't know if their collaborators will turn traitor when the time is right. Just like humans, except everyone might be (probably is) a sociopath who can multiply and grow without limit.\n\nIncentives only work as long as there's the hard constraints of the situation prevent a collaborator from slipping out of them."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-06T20:15:46.183Z",
    "af": false
  }
]