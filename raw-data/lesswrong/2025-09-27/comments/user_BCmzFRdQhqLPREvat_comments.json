[
  {
    "_id": "ifdSdeaECfFLARESL",
    "postId": "fF8pvsn3AGQhYsbjp",
    "parentCommentId": "BiTAqd36czpnLGfTC",
    "topLevelCommentId": "Z7FdmE26eGXdudpwu",
    "contents": {
      "markdown": "FWIW I used to agree with you but now agree with Nate. A big part of the update was developing a model of how \"PR risks\" work via a kind of herd mentality, where very few people are actually acting on their object-level beliefs, and almost everyone is just tracking what everyone else is tracking.\n\nIn such a setting, \"internal influence\" strategies tend to do very little long-term, and maybe even reinforce the taboo against talking honestly. This is roughly what seems to have happened in DC, where the internal influence approach was swept away by a big Overton window shift after ChatGPT. Conversely, a few principled individuals can have a big influence by speaking honestly ([here's a post about the game theory behind this](https://www.mindthefuture.info/p/power-lies-trembling)).\n\nIn my own case, I felt a vague miasma of fear around talking publicly while at OpenAI (and to a lesser extent at DeepMind), even though in hindsight there were often no concrete things that I endorsed being afraid of—for example, there was a period where I was roughly indifferent about leaving OpenAI, but still scared of doing things that might make people mad enough to fire me.\n\nI expect that there's a significant inferential gap between us, so this is a hard point to convey, but one way that I might have been able to bootstrap my current perspective from inside my \"internal influence\" frame is to try to identify possible actions X such that, if I got fired for doing X, this would be a clear example of the company leaders behaving unjustly. Then even the possible \"punishment\" for doing X is actually a win.",
      "plaintextDescription": "FWIW I used to agree with you but now agree with Nate. A big part of the update was developing a model of how \"PR risks\" work via a kind of herd mentality, where very few people are actually acting on their object-level beliefs, and almost everyone is just tracking what everyone else is tracking.\n\nIn such a setting, \"internal influence\" strategies tend to do very little long-term, and maybe even reinforce the taboo against talking honestly. This is roughly what seems to have happened in DC, where the internal influence approach was swept away by a big Overton window shift after ChatGPT. Conversely, a few principled individuals can have a big influence by speaking honestly (here's a post about the game theory behind this).\n\nIn my own case, I felt a vague miasma of fear around talking publicly while at OpenAI (and to a lesser extent at DeepMind), even though in hindsight there were often no concrete things that I endorsed being afraid of—for example, there was a period where I was roughly indifferent about leaving OpenAI, but still scared of doing things that might make people mad enough to fire me.\n\nI expect that there's a significant inferential gap between us, so this is a hard point to convey, but one way that I might have been able to bootstrap my current perspective from inside my \"internal influence\" frame is to try to identify possible actions X such that, if I got fired for doing X, this would be a clear example of the company leaders behaving unjustly. Then even the possible \"punishment\" for doing X is actually a win."
    },
    "baseScore": 35,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-09-21T11:25:13.151Z",
    "af": false
  },
  {
    "_id": "vF2gcBL4eFGxiG64t",
    "postId": "8jkB8ezncWD6ai86e",
    "parentCommentId": "4dxxWvxbgeBN9d4ww",
    "topLevelCommentId": "upxZMamGrgkqegX8c",
    "contents": {
      "markdown": "\"consistent with my position above I'd bet that in the longer term we'd do best to hit a button that ended all religions today, and then eat the costs and spend the decades/centuries required to build better things in their stead.\"\n\nWould you have pressed this button at every other point throughout history too? If not, when's the earliest you would have pressed it?",
      "plaintextDescription": "\"consistent with my position above I'd bet that in the longer term we'd do best to hit a button that ended all religions today, and then eat the costs and spend the decades/centuries required to build better things in their stead.\"\n\nWould you have pressed this button at every other point throughout history too? If not, when's the earliest you would have pressed it?"
    },
    "baseScore": 22,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-13T06:43:03.464Z",
    "af": false
  },
  {
    "_id": "PAcS5Cfh6bFpTiP7q",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "68S8NE68rtY7dkqBd",
    "topLevelCommentId": "9g5QK3Km25fMBHgcK",
    "contents": {
      "markdown": "Good question. One answer is that my reset mechanisms involve cultivating empathy, and [replacing fear](https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL) with positive motivation. If I notice myself being too unempathetic or too fear-driven, that's worrying.\n\nBut another answer is just that, unfortunately, the reality distortion fields are everywhere—and in many ways more prevalent in \"mainstream\" positions (as discussed in my post). Being more mainstream does get you \"safety in numbers\"—i.e. it's harder for you to catalyze big things, for better or worse. But the cost is that you end up in groupthink.",
      "plaintextDescription": "Good question. One answer is that my reset mechanisms involve cultivating empathy, and replacing fear with positive motivation. If I notice myself being too unempathetic or too fear-driven, that's worrying.\n\nBut another answer is just that, unfortunately, the reality distortion fields are everywhere—and in many ways more prevalent in \"mainstream\" positions (as discussed in my post). Being more mainstream does get you \"safety in numbers\"—i.e. it's harder for you to catalyze big things, for better or worse. But the cost is that you end up in groupthink."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-10T11:10:51.809Z",
    "af": false
  },
  {
    "_id": "C8PSifPBDJdqFi6uo",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "A3Yj7fnt67TDviiCa",
    "topLevelCommentId": "9g5QK3Km25fMBHgcK",
    "contents": {
      "markdown": "I like this comment.\n\nFor the sake of transparency, while in this post I'm mostly trying to identify a diagnosis, in the longer term I expect to try to do political advocacy as well. And it's reasonable to expect that people like me who are willing to break the taboo for the purposes of diagnosis will be more sympathetic to ethnonationalism in their advocacy than people who aren't. For example, I've previously argued on twitter that South Africa should have split into two roughly-ethnonationalist states in the 90s, instead of doing what they actually did.\n\nHowever, I expect that the best ways of fixing western countries won't involve very much ethnonationalism by historical standards, because it's a very blunt tool. Also, I suspect that breaking the taboo now will actually lead to less ethnonationalism in the long term. For example, even a little bit more ethnonationalism would plausibly have made European immigration policies much less insane over the last few decades, which would then have prevented a lot of the political polarization we're seeing today.",
      "plaintextDescription": "I like this comment.\n\nFor the sake of transparency, while in this post I'm mostly trying to identify a diagnosis, in the longer term I expect to try to do political advocacy as well. And it's reasonable to expect that people like me who are willing to break the taboo for the purposes of diagnosis will be more sympathetic to ethnonationalism in their advocacy than people who aren't. For example, I've previously argued on twitter that South Africa should have split into two roughly-ethnonationalist states in the 90s, instead of doing what they actually did.\n\nHowever, I expect that the best ways of fixing western countries won't involve very much ethnonationalism by historical standards, because it's a very blunt tool. Also, I suspect that breaking the taboo now will actually lead to less ethnonationalism in the long term. For example, even a little bit more ethnonationalism would plausibly have made European immigration policies much less insane over the last few decades, which would then have prevented a lot of the political polarization we're seeing today."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-10T07:43:39.744Z",
    "af": false
  },
  {
    "_id": "jKFyiipK2qsy5F8Cy",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "bPPetEZ7zNxXzjJXa",
    "topLevelCommentId": "9g5QK3Km25fMBHgcK",
    "contents": {
      "markdown": "This is a thoughtful comment, I appreciate it, and I'll reply when I have more time (hopefully in a few days).",
      "plaintextDescription": "This is a thoughtful comment, I appreciate it, and I'll reply when I have more time (hopefully in a few days)."
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-10T06:42:50.419Z",
    "af": false
  },
  {
    "_id": "STBK8d4Rpj5wCAWgy",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "CdPs9Ajfk2xLub9wC",
    "topLevelCommentId": "9g5QK3Km25fMBHgcK",
    "contents": {
      "markdown": "Thanks for the extensive comment. I'm not sure it's productive to debate this much on the object level. The main thing I want to highlight is that this is a very good example of how the taboo that I discussed above operates.\n\nOn most issues, people (and especially LWers) are generally open to thinking about the benefits and costs of each stance, since tradeoffs are real.\n\nHowever, in the case of ethnonationalism, even discussing the taboo on it (without explicitly advocating for it) was enough to trigger a kind of zero-tolerance attitude in your comment.\n\nThis is all the more striking because the main historical opponent of ethnonationalist regimes was globalist communism, which also led to large-scale atrocities. Yet when people defend a \"socialist\" or \"egalitarian\" cluster of ideas, that doesn't lead to anywhere near this level of visceral response.\n\nMy main bid here is for readers to notice that there is a striking asymmetry in how we think about and discuss 20th century history, which is best explained via the thing I hypothesized above: a strong taboo on ethnonationalism in the wake of WW2, which has then distorted our ability to think about many other issues.",
      "plaintextDescription": "Thanks for the extensive comment. I'm not sure it's productive to debate this much on the object level. The main thing I want to highlight is that this is a very good example of how the taboo that I discussed above operates.\n\nOn most issues, people (and especially LWers) are generally open to thinking about the benefits and costs of each stance, since tradeoffs are real.\n\nHowever, in the case of ethnonationalism, even discussing the taboo on it (without explicitly advocating for it) was enough to trigger a kind of zero-tolerance attitude in your comment.\n\nThis is all the more striking because the main historical opponent of ethnonationalist regimes was globalist communism, which also led to large-scale atrocities. Yet when people defend a \"socialist\" or \"egalitarian\" cluster of ideas, that doesn't lead to anywhere near this level of visceral response.\n\nMy main bid here is for readers to notice that there is a striking asymmetry in how we think about and discuss 20th century history, which is best explained via the thing I hypothesized above: a strong taboo on ethnonationalism in the wake of WW2, which has then distorted our ability to think about many other issues."
    },
    "baseScore": 12,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-10T01:41:13.822Z",
    "af": false
  },
  {
    "_id": "X6ZmTYh7CQB5kuc4c",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "yNpXvgFvwLtC4He8p",
    "topLevelCommentId": "9g5QK3Km25fMBHgcK",
    "contents": {
      "markdown": "> For the most obvious example, for the life of me I cannot understand how leaving the gold standard makes a culture less appreciative of any kind of moral virtue, unless you equate two very different senses of the word \"value\".\n\nMight reply to the rest later but just to respond to what you call \"the most obvious example\": consider a company which has a difficult time evaluating how well its employees are performing (i.e. most of them). Some employees will work hard even when they won't directly be rewarded for that, because they consider it virtuous to do so. However, if you then add to their team a bunch of other people who are rewarded for slacking off, the hard-working employees may become demotivated and feel like they're chumps for even trying to be virtuous.\n\nThe extent to which modern governments hand out money causes a similar effect across western societies (edited: for example, if many people around you are receiving welfare, then working hard yourself is less motivating). They would not be as able to do this as much if their currencies were still on the gold standard, because it would be more obvious that they are insolvent.",
      "plaintextDescription": "> For the most obvious example, for the life of me I cannot understand how leaving the gold standard makes a culture less appreciative of any kind of moral virtue, unless you equate two very different senses of the word \"value\".\n\nMight reply to the rest later but just to respond to what you call \"the most obvious example\": consider a company which has a difficult time evaluating how well its employees are performing (i.e. most of them). Some employees will work hard even when they won't directly be rewarded for that, because they consider it virtuous to do so. However, if you then add to their team a bunch of other people who are rewarded for slacking off, the hard-working employees may become demotivated and feel like they're chumps for even trying to be virtuous.\n\nThe extent to which modern governments hand out money causes a similar effect across western societies (edited: for example, if many people around you are receiving welfare, then working hard yourself is less motivating). They would not be as able to do this as much if their currencies were still on the gold standard, because it would be more obvious that they are insolvent."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-10T00:14:40.551Z",
    "af": false
  },
  {
    "_id": "hEPvhryDtxhetPFzu",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "abDWQ2F9e2mBT445b",
    "topLevelCommentId": "9g5QK3Km25fMBHgcK",
    "contents": {
      "markdown": "I used to agree with your understanding but I am now more skeptical. For example, here's a story that says the opposite:\n\n> The poorer humans are, the more vulnerable each human is to the group consensus. People who disagreed with groups could in the past easily be assaulted by mobs, or harassed in a way that led them to literally starvation-level wealth. Nowadays, though, even victims of extreme 'cancel culture' don't face such risks, because society is wealthy enough that you can do things like move to a new city to avoid mobs, or get charities to feed and clothe you even if you lose your job.  \n>   \n> Also it's much harder to design parasitic egregores now than it used to be, because our science is much better and so we know many more facts, which makes it harder for egregores to lie.\n\nI'm not saying my story is *true*, but it does highlight that the load-bearing question is actually something like \"how does the offense-defense balance against parasitic egregores scale with wealth?\" Why don't we live in a world where wealth can buy a society defenses against such egregores?\n\nOr maybe we do live in such a world, and we are just failing to buy those defenses. That seems like a really dumb situation to be in, but I think my post is broadly describing how it might arise.",
      "plaintextDescription": "I used to agree with your understanding but I am now more skeptical. For example, here's a story that says the opposite:\n\n> The poorer humans are, the more vulnerable each human is to the group consensus. People who disagreed with groups could in the past easily be assaulted by mobs, or harassed in a way that led them to literally starvation-level wealth. Nowadays, though, even victims of extreme 'cancel culture' don't face such risks, because society is wealthy enough that you can do things like move to a new city to avoid mobs, or get charities to feed and clothe you even if you lose your job.\n> \n> Also it's much harder to design parasitic egregores now than it used to be, because our science is much better and so we know many more facts, which makes it harder for egregores to lie.\n\nI'm not saying my story is true, but it does highlight that the load-bearing question is actually something like \"how does the offense-defense balance against parasitic egregores scale with wealth?\" Why don't we live in a world where wealth can buy a society defenses against such egregores?\n\nOr maybe we do live in such a world, and we are just failing to buy those defenses. That seems like a really dumb situation to be in, but I think my post is broadly describing how it might arise."
    },
    "baseScore": 2,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-10T00:06:24.999Z",
    "af": false
  },
  {
    "_id": "tXWmvEWenau9KeXCG",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "8znMQn7JaEd5vYFeC",
    "topLevelCommentId": "9g5QK3Km25fMBHgcK",
    "contents": {
      "markdown": "That's a mechanism by which I might overestimate the support for Hamas. But the thing I'm trying to explain is the overall alignment between leftists and Hamas, which is not just a twitter bubble thing (e.g. see university encampments).\n\nMore generally, leftists profess many values which are upheld the most by western civilization (e.g. support for sexual freedom, women's rights, anti-racism, etc). But then in conflicts they often side specifically against western civilization. This seems like a straightforward example of [pessimization](https://www.mindthefuture.info/p/on-pessimization).",
      "plaintextDescription": "That's a mechanism by which I might overestimate the support for Hamas. But the thing I'm trying to explain is the overall alignment between leftists and Hamas, which is not just a twitter bubble thing (e.g. see university encampments).\n\nMore generally, leftists profess many values which are upheld the most by western civilization (e.g. support for sexual freedom, women's rights, anti-racism, etc). But then in conflicts they often side specifically against western civilization. This seems like a straightforward example of pessimization."
    },
    "baseScore": -1,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2025-09-09T18:05:22.608Z",
    "af": false
  },
  {
    "_id": "BnrHBwoMwn6qKyEjY",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "4otSwyNsdgAiyTgMH",
    "topLevelCommentId": "9g5QK3Km25fMBHgcK",
    "contents": {
      "markdown": "> Consequentialism and utility functions or policies could in principle be about virtues and integrity as about hamburgers, but hamburgers are more legible and easier to administer.\n\nHere's one concrete way in which this isn't true: one common simplifying assumption in economics is that goods are homogeneous, and therefore that you're indifferent about who to buy from. However, virtuous behavior involves rewarding people you think are more virtuous (e.g. by preferentially buying things from them).\n\nIn other words, economics is about how agents interact with each other via exchanging goods and services, while virtues are about how agents interact with each other more generally.",
      "plaintextDescription": "> Consequentialism and utility functions or policies could in principle be about virtues and integrity as about hamburgers, but hamburgers are more legible and easier to administer.\n\nHere's one concrete way in which this isn't true: one common simplifying assumption in economics is that goods are homogeneous, and therefore that you're indifferent about who to buy from. However, virtuous behavior involves rewarding people you think are more virtuous (e.g. by preferentially buying things from them).\n\nIn other words, economics is about how agents interact with each other via exchanging goods and services, while virtues are about how agents interact with each other more generally."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-09T00:06:01.555Z",
    "af": false
  },
  {
    "_id": "nyQxgAhPZBGfnrLGK",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "22bREsDxWDsRbHDLi",
    "topLevelCommentId": "W9N9tTbYSBzM9FvWh",
    "contents": {
      "markdown": "> Sufficiently different versions of yourself are just logically uncorrelated with you and there is no game-theoretic reason to account for them.\n\nSeems odd to make an absolute statement here. More different versions of yourself are less and less correlated, but there's still some correlation. And UDT should also be applicable to interactions with other people, who are typically different from you in a whole bunch of ways.",
      "plaintextDescription": "> Sufficiently different versions of yourself are just logically uncorrelated with you and there is no game-theoretic reason to account for them.\n\nSeems odd to make an absolute statement here. More different versions of yourself are less and less correlated, but there's still some correlation. And UDT should also be applicable to interactions with other people, who are typically different from you in a whole bunch of ways."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-08T23:17:30.225Z",
    "af": false
  },
  {
    "_id": "rSFEDwW6CxBWvFsMn",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "Cvy8eQuPBrENs5qeC",
    "topLevelCommentId": "W9N9tTbYSBzM9FvWh",
    "contents": {
      "markdown": "> there's often no internal conflict when someone is caught up in some extreme form of the morality game\n\nBelated reply, sorry, but I basically just think that this is false—analogous to a dictator who cites parades where people are forced to attend and cheer as evidence that his country lacks internal conflict. Instead, the internal conflict has just been rendered less legible.\n\n> In the subagents frame, I would say that the subagents have an implicit contract/agreement that any one of them *can* seize control, if doing so seems good for the overall agent in terms of power or social status.\n\nNote that this is an extremely non-robust agent design! In particular, it allows subagents to gain arbitrary amounts of power simply by lying about their intentions. If you encounter an agent which considers itself to be structured like this, you should have a strong prior that it is deceiving itself about the presence of more subtle control mechanisms.",
      "plaintextDescription": "> there's often no internal conflict when someone is caught up in some extreme form of the morality game\n\nBelated reply, sorry, but I basically just think that this is false—analogous to a dictator who cites parades where people are forced to attend and cheer as evidence that his country lacks internal conflict. Instead, the internal conflict has just been rendered less legible.\n\n> In the subagents frame, I would say that the subagents have an implicit contract/agreement that any one of them can seize control, if doing so seems good for the overall agent in terms of power or social status.\n\nNote that this is an extremely non-robust agent design! In particular, it allows subagents to gain arbitrary amounts of power simply by lying about their intentions. If you encounter an agent which considers itself to be structured like this, you should have a strong prior that it is deceiving itself about the presence of more subtle control mechanisms."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-08T23:13:50.051Z",
    "af": false
  },
  {
    "_id": "9g5QK3Km25fMBHgcK",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Crossposted from Twitter:\n\nThis year I’ve been thinking a lot about how the western world got so dysfunctional. Here’s my rough, best-guess story:\n\n1\\. WW2 gave rise to a strong taboo against ethnonationalism. While perhaps at first this taboo was valuable, over time it also contaminated discussions of race differences, nationalism, and even IQ itself, to the point where even truths that seemed totally obvious to WW2-era people also became taboo. There’s no mechanism for subsequent generations to create common knowledge that certain facts are true but usefully taboo—they simply act as if these facts are false, which leads to arbitrarily bad policies (e.g. killing meritocratic hiring processes like IQ tests).\n\n2\\. However, these taboos would gradually have lost power if the west (and the US in particular) had maintained impartial rule of law and constitutional freedoms. Instead, politicization of the bureaucracy and judiciary allowed them to spread. This was enabled by the “managerial revolution” under which govt bureaucracy massively expanded in scope and powers. Partly this was a justifiable response to the increasing complexity of the world (and various kinds of incompetence and nepotism within govts) but in practice it created a class of managerial elites who viewed their intellectual merit as license to impose their ideology on the people they governed. This class gains status by signaling commitment to luxury beliefs. Since more absurd beliefs are more costly-to-fake signals, the resulting ideology is actively perverse (i.e. supports whatever is least aligned with their stated core values, like Hamas).\n\n3\\. On an ideological level the managerial revolution was facilitated by a kind of utilitarian spirit under which technocratic expertise was considered more important for administrators than virtue or fidelity to the populace. This may have been a response to the loss of faith in traditional elites after WW1. The enlightened liberal perspective wanted to maintain a fiction of equality, under which administrators were just doing a job the same as any other, rather than taking on the heavy privileges and responsibilities associated with (healthy) hierarchical relationships.\n\n4\\. On an economic level, the world wars led to centralization of state power over currency and the abandonment of the gold standard. While at first govts tried to preserve the fiction that fiat currencies were relevantly similar to gold-backed currencies, again there was no mechanism for later generations to create common knowledge of what had actually been done and why. The black hole of western state debt that will never be repaid creates distortions across the economy, which few economists actually grapple with because they are emotionally committed to thinking of western govts as “too big to fail”.\n\n5\\. All of this has gradually eroded the strong, partly-innate sense of virtue (and respect for virtuous people) that used to be common. Virtue can be seen as a self-replicating memeplex that incentivizes ethical behavior in others—e.g. high-integrity people will reward others for displaying integrity. This is different from altruism, which rewards others regardless of their virtue. Indeed, it’s often directly opposed to altruism, since altruists disproportionately favor the least virtuous people (because they’re worse-off). Since consequentialists think that morality is essentially about altruism, much moral philosophy actively undermines ethics. So does modern economics, via smuggling in the assumption that utility functions represent selfish preferences.\n\n6\\. All of this is happening against a backdrop of rapid technological progress, which facilitates highly unequal control mechanisms (e.g. a handful of people controlling global newsfeeds or AI values). The bad news is that this enables ideologies to propagate even when they are perverse and internally dysfunctional. The good news is that it makes genuine truth-seeking and virtuous cooperation increasingly high-leverage.  \n  \nAddenda:\n\n1.  I led with the ethnonationalism stuff because it’s the most obvious, but in some sense it’s just a symptom: a functional society would have rejected the taboos when they got too obviously wrong (e.g. by defending Murray).\n    \n    The deeper issue seems to be a kind of toxic egalitarianism that is against accountability, hierarchy or individual agency in general. You can trace this thread (with increasing uncertainty) thru e.g. Wilson, Marx, the utilitarians, and maybe even all the way back to Jesus.\n    \n    Michael Vassar thinks of it as Germanic “kultur” (as opposed to “zivilization”); I’m not well-read enough to evaluate that claim though. I’m more confident about it being driven by fear-based motivations, especially envy—as per Girard, Lacan, etc.\n    \n2.  Some prescriptions I’m currently considering:  \n    \\- reviving virtue ethics  \n    \\- AI-based tools for facilitating small, high-trust, high-accountability groups. Even if we can’t have freedom of association or reliable arbitration via legal or corporate mechanisms, perhaps we can still have it via social mechanisms (especially as more and more people become functionally post-economic)  \n    \\- better therapeutic interventions, especially oriented to resolving fear of death\n    \n    But I spend most of my time trying to figure out the formal theory that encodes these intuitions—in which agents are understood in terms of goals (in the predictive processing sense) and boundaries rather than utility functions and credences. That feels upstream of a lot of other stuff. [More here](https://www.mindthefuture.info/p/towards-a-scale-free-theory-of-intelligent), though it’s a bit out of date.\n    \n\nEdited to add: I am surprised both by the extent of disagree-voting (-38 as of writing this), and by the extent to which this is decoupled from karma (23 as of writing this). This is an impressive level of decoupling. Given that the gap between my views and most LWers is much bigger than I thought, I'll have a think about how to better convey my perspective in a way that makes cruxes clearer. Since many LWers believe in something like [Eliezer's civilizational inadequacy thesis](https://equilibriabook.com/inadequacy-and-modesty), though, I'm curious about the best explanations other people have for why our current civilization is \"inadequate\".",
      "plaintextDescription": "Crossposted from Twitter:\n\nThis year I’ve been thinking a lot about how the western world got so dysfunctional. Here’s my rough, best-guess story:\n\n1. WW2 gave rise to a strong taboo against ethnonationalism. While perhaps at first this taboo was valuable, over time it also contaminated discussions of race differences, nationalism, and even IQ itself, to the point where even truths that seemed totally obvious to WW2-era people also became taboo. There’s no mechanism for subsequent generations to create common knowledge that certain facts are true but usefully taboo—they simply act as if these facts are false, which leads to arbitrarily bad policies (e.g. killing meritocratic hiring processes like IQ tests).\n\n2. However, these taboos would gradually have lost power if the west (and the US in particular) had maintained impartial rule of law and constitutional freedoms. Instead, politicization of the bureaucracy and judiciary allowed them to spread. This was enabled by the “managerial revolution” under which govt bureaucracy massively expanded in scope and powers. Partly this was a justifiable response to the increasing complexity of the world (and various kinds of incompetence and nepotism within govts) but in practice it created a class of managerial elites who viewed their intellectual merit as license to impose their ideology on the people they governed. This class gains status by signaling commitment to luxury beliefs. Since more absurd beliefs are more costly-to-fake signals, the resulting ideology is actively perverse (i.e. supports whatever is least aligned with their stated core values, like Hamas).\n\n3. On an ideological level the managerial revolution was facilitated by a kind of utilitarian spirit under which technocratic expertise was considered more important for administrators than virtue or fidelity to the populace. This may have been a response to the loss of faith in traditional elites after WW1. The enlightened liberal perspective wanted to maintain a"
    },
    "baseScore": 44,
    "voteCount": 62,
    "createdAt": null,
    "postedAt": "2025-09-08T22:52:15.546Z",
    "af": false
  },
  {
    "_id": "XGfQT9LzYwXWomNKR",
    "postId": "TiQGC6woDMPJ9zbNM",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "[A related post I wrote recently](https://www.lesswrong.com/posts/onsZ4JeA8EMX3hb7B/applying-right-wing-frames-to-agi-geo-politics).\n\n+1 to ChristianKl's observation below though that Geoffrey Miller is unrepresentative of MAGA because he's already part of the broader AI safety community.",
      "plaintextDescription": "A related post I wrote recently.\n\n+1 to ChristianKl's observation below though that Geoffrey Miller is unrepresentative of MAGA because he's already part of the broader AI safety community."
    },
    "baseScore": 10,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-08T22:12:38.827Z",
    "af": false
  },
  {
    "_id": "GR6qwXDxrGvhjFgKF",
    "postId": "RHYjjxsjyLng4Bk44",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "You might be interested in [this post of mine](https://www.lesswrong.com/posts/5aAatvkHdPH6HT3P9/against-strong-bayesianism) which makes some related claims.\n\n(Interested to read your post more thoroughly but for now have just skimmed it and not sure when I'll find time to engage more.)",
      "plaintextDescription": "You might be interested in this post of mine which makes some related claims.\n\n(Interested to read your post more thoroughly but for now have just skimmed it and not sure when I'll find time to engage more.)"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-31T09:23:52.981Z",
    "af": false
  },
  {
    "_id": "hYQNfaXPLJpYsTKGa",
    "postId": "GMWtZzpLHpBLKQsYp",
    "parentCommentId": "TmsRQun2j6pQgXN8J",
    "topLevelCommentId": "TmsRQun2j6pQgXN8J",
    "contents": {
      "markdown": "FWIW your writings on neuroscience are a central example of \"real thinking\" in my mind—it seems like you're trying to actually understand things in a way that's far less distorted by social pressures and incentives than almost any other writing in the field.",
      "plaintextDescription": "FWIW your writings on neuroscience are a central example of \"real thinking\" in my mind—it seems like you're trying to actually understand things in a way that's far less distorted by social pressures and incentives than almost any other writing in the field."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-31T04:14:08.641Z",
    "af": false
  },
  {
    "_id": "gxBjcsJBGym5tdqqf",
    "postId": "yNhJKBdtucBL2uXb5",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Reading this post led me to find [a twitter thread](https://x.com/getjonwithit/status/1873727976610898190?s=46) arguing (with a bunch of examples):\n\n> One of the curious things about von Neumann was his ability to do extremely impressive technical work while seemingly missing all the big insights.\n\nI then responded to it with [my own thread](https://x.com/richardmcngo/status/1961964175393235046?s=46) arguing:\n\n> I’d even go further—I think we’re still recovering from Von Neumann’s biggest mistakes:\n> \n> 1\\. Implicitly basing game theory on causal decision theory  \n> 2\\. Founding utility theory on the independence axiom  \n> 3\\. Advocating for nuking the USSR as soon as possible\n\nI'm not confident in my argument, but it suggests the possibility that von Neumann's concern about his legacy was tracking something important (though, even if so, it's unlikely that feeling insecure was a good response).",
      "plaintextDescription": "Reading this post led me to find a twitter thread arguing (with a bunch of examples):\n\n> One of the curious things about von Neumann was his ability to do extremely impressive technical work while seemingly missing all the big insights.\n\nI then responded to it with my own thread arguing:\n\n> I’d even go further—I think we’re still recovering from Von Neumann’s biggest mistakes:\n> \n> 1. Implicitly basing game theory on causal decision theory\n> 2. Founding utility theory on the independence axiom\n> 3. Advocating for nuking the USSR as soon as possible\n\nI'm not confident in my argument, but it suggests the possibility that von Neumann's concern about his legacy was tracking something important (though, even if so, it's unlikely that feeling insecure was a good response)."
    },
    "baseScore": 7,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-31T04:10:40.019Z",
    "af": false
  },
  {
    "_id": "9EADkPZLDzahjJNMv",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "yfndzxFrxn9ioJqJ8",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "If someone predicts *in advance* that something is obviously false, and then you come to believe that it's false, then you should update not just towards thought processes which would have predicted that the thing is false, but also towards thought processes which would have predicted that the thing is obviously false. (Conversely, if they predict that it's obviously false, and it turns out to be true, you should update more strongly against their thought processes than if they'd just predicted it was false.)\n\nIIRC Eliezer's objection to bioanchors can be reasonably interpreted as an advance prediction that \"it's obviously false\", though to be confident I'd need to reread his original post (which I can't be bothered to do right now).",
      "plaintextDescription": "If someone predicts in advance that something is obviously false, and then you come to believe that it's false, then you should update not just towards thought processes which would have predicted that the thing is false, but also towards thought processes which would have predicted that the thing is obviously false. (Conversely, if they predict that it's obviously false, and it turns out to be true, you should update more strongly against their thought processes than if they'd just predicted it was false.)\n\nIIRC Eliezer's objection to bioanchors can be reasonably interpreted as an advance prediction that \"it's obviously false\", though to be confident I'd need to reread his original post (which I can't be bothered to do right now)."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-31T04:04:46.286Z",
    "af": false
  },
  {
    "_id": "5er28NDwBMPcwi97D",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "jtgKFFPsRTbobF328",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "> It's not that moderates and radicals are trying to answer different questions (and the questions moderates are answering are epistemically easier like physics).\n\nThat seems totally wrong. Moderates are trying to answer questions like \"what are some relatively cheap interventions that AI companies could implement to reduce risk assuming a low budget?\" and \"how can I cause AI companies to marginally increase that budget?\" These questions are very different from—and much easier than—the ones the radicals are trying to answer, like \"how can we radically change the governance of AI to prevent x-risk?\"",
      "plaintextDescription": "> It's not that moderates and radicals are trying to answer different questions (and the questions moderates are answering are epistemically easier like physics).\n\nThat seems totally wrong. Moderates are trying to answer questions like \"what are some relatively cheap interventions that AI companies could implement to reduce risk assuming a low budget?\" and \"how can I cause AI companies to marginally increase that budget?\" These questions are very different from—and much easier than—the ones the radicals are trying to answer, like \"how can we radically change the governance of AI to prevent x-risk?\""
    },
    "baseScore": 12,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-31T03:59:08.724Z",
    "af": false
  },
  {
    "_id": "z6mZR4zjaTXApHChG",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "GSmDKjaWtn6ukefrG",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "The argument \"there are specific epistemic advantages of working as a moderate\" isn't just a claim about categories that everyone agrees exist, it's also a way of carving up the world. However, you can carve up the world in very misleading ways depending on how you lump different groups together. For example, if a post distinguished \"people without crazy-sounding beliefs\" from \"people with crazy-sounding beliefs\", the latter category would lump together truth-seeking nonconformists with actual crazy people. There's no easy way of figuring out which categories should be treated as useful vs useless but the evidence Eliezer cites does seem relevant.\n\nOn a more object level, my main critique of the post is that almost all of the bullet points are even *more* true of, say, working as a physicist. And so structurally speaking I don't know how to distinguish this post from one arguing \"one advantage of looking for my keys closer to a streetlight is that there's more light!\" I.e. it's hard to know the extent to which these benefits come specifically from focusing on less important things, and therefore are illusory, versus the extent to which you can decouple these benefits from the costs of being a \"moderate\".",
      "plaintextDescription": "The argument \"there are specific epistemic advantages of working as a moderate\" isn't just a claim about categories that everyone agrees exist, it's also a way of carving up the world. However, you can carve up the world in very misleading ways depending on how you lump different groups together. For example, if a post distinguished \"people without crazy-sounding beliefs\" from \"people with crazy-sounding beliefs\", the latter category would lump together truth-seeking nonconformists with actual crazy people. There's no easy way of figuring out which categories should be treated as useful vs useless but the evidence Eliezer cites does seem relevant.\n\nOn a more object level, my main critique of the post is that almost all of the bullet points are even more true of, say, working as a physicist. And so structurally speaking I don't know how to distinguish this post from one arguing \"one advantage of looking for my keys closer to a streetlight is that there's more light!\" I.e. it's hard to know the extent to which these benefits come specifically from focusing on less important things, and therefore are illusory, versus the extent to which you can decouple these benefits from the costs of being a \"moderate\"."
    },
    "baseScore": 20,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-30T00:03:47.511Z",
    "af": false
  },
  {
    "_id": "eyk8Nmkv4TBNvTwmM",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "TxMEhxxniCCMFpdLb",
    "topLevelCommentId": "sNPkkFGZmhyiydZoo",
    "contents": {
      "markdown": "Yes, that can be a problem. I'm not sure why you think that's in tension with my comment though.",
      "plaintextDescription": "Yes, that can be a problem. I'm not sure why you think that's in tension with my comment though."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-26T17:47:34.435Z",
    "af": false
  },
  {
    "_id": "bbpQcG4x4aECYQGrN",
    "postId": "98sCTsGJZ77WgQ6nE",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Thank you Habryka (and the rest of the mod team) for the effort and thoughtfulness you put into making LessWrong good.\n\nI personally have had few problems with Said, but this seems like an extremely reasonable decision. I'm leaving this comment in part to help make you feel empowered to make similar decisions in the future when you think it necessary (and ideally, at a much lower cost of your time).",
      "plaintextDescription": "Thank you Habryka (and the rest of the mod team) for the effort and thoughtfulness you put into making LessWrong good.\n\nI personally have had few problems with Said, but this seems like an extremely reasonable decision. I'm leaving this comment in part to help make you feel empowered to make similar decisions in the future when you think it necessary (and ideally, at a much lower cost of your time)."
    },
    "baseScore": 168,
    "voteCount": 102,
    "createdAt": null,
    "postedAt": "2025-08-23T00:27:58.324Z",
    "af": false
  },
  {
    "_id": "FyjcqqpaTFpsYfXkr",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "dSykMsNjarsRoQ9E6",
    "topLevelCommentId": "sNPkkFGZmhyiydZoo",
    "contents": {
      "markdown": "I think one effect you're missing is that the *big* changes are precisely the ones that tend to mostly rely on factors that are hard to specify important technical details about. E.g. \"should we move our headquarters to London\" or \"should we replace the CEO\" or \"should we change our mission statement\" are mostly going to be driven by coalitional politics + high-level intuitions and arguments. Whereas \"should we do X training run or Y training run\" are more amenable to technical discussion, but also have less lasting effects.",
      "plaintextDescription": "I think one effect you're missing is that the big changes are precisely the ones that tend to mostly rely on factors that are hard to specify important technical details about. E.g. \"should we move our headquarters to London\" or \"should we replace the CEO\" or \"should we change our mission statement\" are mostly going to be driven by coalitional politics + high-level intuitions and arguments. Whereas \"should we do X training run or Y training run\" are more amenable to technical discussion, but also have less lasting effects."
    },
    "baseScore": 21,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-08-22T07:45:01.457Z",
    "af": false
  },
  {
    "_id": "hr5TztncdNtrvmMzz",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "xuqs26dniD4CnT6nL",
    "topLevelCommentId": "sNPkkFGZmhyiydZoo",
    "contents": {
      "markdown": "> people in companies care about technical details so to be persuasive you will have to be familiar with them\n\nBig changes within companies are typically bottlenecked much more by coalitional politics than knowledge of technical details.",
      "plaintextDescription": "> people in companies care about technical details so to be persuasive you will have to be familiar with them\n\nBig changes within companies are typically bottlenecked much more by coalitional politics than knowledge of technical details."
    },
    "baseScore": 29,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2025-08-21T10:00:04.474Z",
    "af": false
  },
  {
    "_id": "8PzZhYKAvJjKBwme4",
    "postId": "dbYEoG7jNZbeWX39o",
    "parentCommentId": "DGnvoiGfcDgGnc3sv",
    "topLevelCommentId": "DGnvoiGfcDgGnc3sv",
    "contents": {
      "markdown": "> By thinking about reward in this way, I was able to predict[^mdkq8g0bngo] and encourage the success of this research direction.\n\nCongratulations on doing this :) More specifically, I think there are two parts of making predictions: identifying a hypothesis at all, and then figuring out how likely the hypothesis is to be true or false. The former part is almost always the hard part, and that's the bit where the \"reward reinforces previous computations\" frame was most helpful.\n\n(I think Oliver's pushback in another comment is getting strongly upvoted because, *given* a description of your experimental setup, a bunch of people aside from you/Quintin/Steve would have assigned reasonable probability to the right answer. But I wanted to emphasize that I consider *generating* an experiment that turns out to be interesting (as your frame did) to be the thing that most of the points should be assigned for.)",
      "plaintextDescription": "> By thinking about reward in this way, I was able to predict[1] and encourage the success of this research direction.\n\nCongratulations on doing this :) More specifically, I think there are two parts of making predictions: identifying a hypothesis at all, and then figuring out how likely the hypothesis is to be true or false. The former part is almost always the hard part, and that's the bit where the \"reward reinforces previous computations\" frame was most helpful.\n\n(I think Oliver's pushback in another comment is getting strongly upvoted because, given a description of your experimental setup, a bunch of people aside from you/Quintin/Steve would have assigned reasonable probability to the right answer. But I wanted to emphasize that I consider generating an experiment that turns out to be interesting (as your frame did) to be the thing that most of the points should be assigned for.)"
    },
    "baseScore": 9,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-16T09:19:10.773Z",
    "af": true
  },
  {
    "_id": "pzAmAarnex5RLuCSs",
    "postId": "bGYQgBPEyHidnZCdE",
    "parentCommentId": "PHGKnzfsh8oG3bJq4",
    "topLevelCommentId": "MzYGSg8bRftB4aHTn",
    "contents": {
      "markdown": "Ty for the reply. A few points in response:\n\n> Of course, you might not know which problem your insights allow you to solve until you have the insights. I'm a big fan of constructing [stylized](https://arxiv.org/abs/2403.19647) [problems](https://arxiv.org/abs/2507.16795) that you can solve, after you know which insight you want to validate.\n> \n> That said, I think it's even better if you can [specify](https://arxiv.org/abs/1606.06565) [problems](https://www.lesswrong.com/posts/LbrPTJ4fmABEdEnLf/200-concrete-open-problems-in-mechanistic-interpretability) in advance to help guide research in the field. The big risk, then, is that these problems might not be robust to paradigm shifts (because paradigm shifts could change the set of important problems). If that is your concern, then I think you should probably give object-level arguments that solving auditing games is a bad concrete problem to direct attention to. (Or argue that specifying concrete problems is in general a bad thing.)\n\nThe bigger the scientific advance, the harder it is to specify problems in advance which it should solve. You can and should keep track of the unresolved problems in the field, as Neel does, but trying to predict specifically which unresolved problems in biology Darwinian evolution would straightforwardly solve (or which unresolved problems in physics special relativity would straightforwardly solve) is about as hard as generating those theories in the first place. \n\nI expect that when you personally are actually doing your scientific research you are building sophisticated mental models of how and why different techniques work. But I think that in your community-level advocacy you are emphasizing precisely the wrong thing—I want junior researchers to viscerally internalize that their job is to *understand* (mis)alignment better than anyone else does, not to optimize on proxies that someone else has designed (which, by the nature of the problem, are going to be bad proxies).\n\nIt feels like the core disagreement is that I intuitively believe that bad metrics are worse than no metrics, because they actively confuse people/lead them astray. More specifically, I feel like your list of four problems is closer to a list of things that we should expect from an actually-productive scientific field, and getting rid of them would neuter the ability for alignment to make progress:\n\n*   \"Right now, by default research projects get one bit of supervision: After the paper is released, how well is it received?\" Not only is this not one bit, I would also struggle to describe any of the best scientists throughout history as being guided primarily by it. Great researchers *can* tell by themselves, using their own judgment, how good the research is (and if you're not a great researcher that's probably the key skill you need to work on).  \n      \n    But also, note how anti-empiricism your position is. The whole *point* of research projects is that they get a huge amount of supervision *from reality*. The job of scientists is to observe that supervision from reality and construct theories that predict reality well, *no matter what anyone else thinks about them*. It's not an exaggeration to say that discarding the idea that intellectual work should be \"supervised\" by one's peers is the main reason that science works in the first place ([see Strevens for more](https://www.amazon.com/Knowledge-Machine-Irrationality-Created-Science/dp/1631491377)).\n*   \"Lacking objective, consensus-backed progress metrics, the field is effectively guided by what a small group of thought leaders think is important/productive to work on.\" Science works precisely because it's *not* consensus-backed—see my point on empiricism above. Attempts to make science more consensus-backed undermine the ability to disagree with existing models/frameworks. But also: the \"objective metrics\" of science are the ability to make powerful, novel predictions in general. If you know specifically what metrics you're trying to predict, the thing you're doing is engineering. And some people should be doing engineering (e.g. engineering better cybersecurity)! But if you try to do it without a firm scientific foundation [you won't get far](https://intelligence.org/2018/10/03/rocket-alignment/).\n*   I think it's good that \"junior researchers who do join are unsure what to work on.\" It is extremely appropriate for them to be unsure what to work on, because the field is very confusing. If we optimize for junior researchers being more confident on what to work on, we will actively be making them less truth-tracking, which makes their research worse in the long term.\n*   Similarly, \"it’s hard to tell which research bets (if any) are paying out and should be invested in more aggressively\" is just the correct epistemic state to be in. Yes, much of the arguing is unproductive. But what's much less productive is saying \"it would be *good* if we could measure progress, therefore we will design the best progress metric we can and just optimize really hard for that\". Rather, since evaluating the quality of research is the core skill of being a good scientist, I am happy with junior researchers all disagreeing with each other and just pursuing whichever research bets *they* want to invest their time in (or the research bets they can get the best mentorship when working on).\n*   Lastly, it's also good that \"it’s hard to grow the field\". Imagine talking to Einstein and saying \"your thought experiments about riding lightbeams are too confusing and unquantifiable—they make it hard to grow the field. You should pick a metric of how good our physics theories are and optimize for that instead.\" Whenever a field is making rapid progress it's difficult to bridge the gap between the ontology outside the field and the ontology inside the field. The easiest way to close that gap is simply for the field to stop making rapid progress, which is what happens when something becomes a \"numbers-go-up\" discipline.\n\n> I think that e.g. RL algorithms researchers have some pretty deep insights about the nature of exploration, learning, etc.\n\nThey have *some*. But so did Galileo. If you'd turned physics into a numbers-go-up field after Galileo, you would have lost most of the subsequent progress, because you would've had no idea *which* numbers going up would contribute to progress.\n\nI'd recommend reading more about the history of science, e.g. The Sleepwalkers by Koestler, to get a better sense of where I'm coming from.",
      "plaintextDescription": "Ty for the reply. A few points in response:\n\n> Of course, you might not know which problem your insights allow you to solve until you have the insights. I'm a big fan of constructing stylized problems that you can solve, after you know which insight you want to validate.\n> \n> That said, I think it's even better if you can specify problems in advance to help guide research in the field. The big risk, then, is that these problems might not be robust to paradigm shifts (because paradigm shifts could change the set of important problems). If that is your concern, then I think you should probably give object-level arguments that solving auditing games is a bad concrete problem to direct attention to. (Or argue that specifying concrete problems is in general a bad thing.)\n\nThe bigger the scientific advance, the harder it is to specify problems in advance which it should solve. You can and should keep track of the unresolved problems in the field, as Neel does, but trying to predict specifically which unresolved problems in biology Darwinian evolution would straightforwardly solve (or which unresolved problems in physics special relativity would straightforwardly solve) is about as hard as generating those theories in the first place. \n\nI expect that when you personally are actually doing your scientific research you are building sophisticated mental models of how and why different techniques work. But I think that in your community-level advocacy you are emphasizing precisely the wrong thing—I want junior researchers to viscerally internalize that their job is to understand (mis)alignment better than anyone else does, not to optimize on proxies that someone else has designed (which, by the nature of the problem, are going to be bad proxies).\n\nIt feels like the core disagreement is that I intuitively believe that bad metrics are worse than no metrics, because they actively confuse people/lead them astray. More specifically, I feel like your list of four problems is closer "
    },
    "baseScore": 34,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-08-05T23:36:49.519Z",
    "af": true
  },
  {
    "_id": "MzYGSg8bRftB4aHTn",
    "postId": "bGYQgBPEyHidnZCdE",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I strongly disagree. \"Numbers-Go-Up Science\" is an oxymoron: great science (especially what Kuhn calls revolutionary science) comes from developing novel models or ontologies which can't be quantitatively compared to previous ontologies.\n\nIndeed, in an important sense, the reason the alignment problem is a big deal in the first place is that ML isn't a science which tries to develop deep explanations of artificial cognition, but instead a numbers-go-up discipline.\n\nAnd so the idea of trying to make (a subfield of) alignment more like architecture design, performance optimization or RL algorithms feels precisely backwards—it steers people directly away from [the thing that alignment research should be contributing](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research).",
      "plaintextDescription": "I strongly disagree. \"Numbers-Go-Up Science\" is an oxymoron: great science (especially what Kuhn calls revolutionary science) comes from developing novel models or ontologies which can't be quantitatively compared to previous ontologies.\n\nIndeed, in an important sense, the reason the alignment problem is a big deal in the first place is that ML isn't a science which tries to develop deep explanations of artificial cognition, but instead a numbers-go-up discipline.\n\nAnd so the idea of trying to make (a subfield of) alignment more like architecture design, performance optimization or RL algorithms feels precisely backwards—it steers people directly away from the thing that alignment research should be contributing."
    },
    "baseScore": 72,
    "voteCount": 40,
    "createdAt": null,
    "postedAt": "2025-08-05T14:47:11.104Z",
    "af": true
  },
  {
    "_id": "rQ2BCygnHAymMfX2R",
    "postId": "oFYc6EWR2zeXmz8Qz",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Strongly upvoted. Alignment researchers often feel so compelled to quickly contribute to decreasing x-risk that they end up studying non-robust categories that won't generalize very far, and sometimes actively make the field more confused. I wish that most people doing this were just trying to do [the best science they could instead](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research).",
      "plaintextDescription": "Strongly upvoted. Alignment researchers often feel so compelled to quickly contribute to decreasing x-risk that they end up studying non-robust categories that won't generalize very far, and sometimes actively make the field more confused. I wish that most people doing this were just trying to do the best science they could instead."
    },
    "baseScore": 18,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-08-01T13:27:05.183Z",
    "af": false
  },
  {
    "_id": "cqit2Sv2hLKoFuzZ4",
    "postId": "onsZ4JeA8EMX3hb7B",
    "parentCommentId": "zrGHtbLkur7Ka3imv",
    "topLevelCommentId": "zrGHtbLkur7Ka3imv",
    "contents": {
      "markdown": "This is a reasonable point, though I also think that there's something important about the ways that these three frames tie together. In general it seems to me that people underrate the extent to which there are deep and reasonably-coherent intuitions underlying right-wing thinking (in part because right-wing thinkers have been bad at articulating those intuitions). Framing the post this way helps direct people to look for them.\n\nBut I could also just say that in the text instead. So if I do another post like this in the future I'll try your approach and see if that goes better.",
      "plaintextDescription": "This is a reasonable point, though I also think that there's something important about the ways that these three frames tie together. In general it seems to me that people underrate the extent to which there are deep and reasonably-coherent intuitions underlying right-wing thinking (in part because right-wing thinkers have been bad at articulating those intuitions). Framing the post this way helps direct people to look for them.\n\nBut I could also just say that in the text instead. So if I do another post like this in the future I'll try your approach and see if that goes better."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-10T18:37:14.274Z",
    "af": false
  },
  {
    "_id": "pLD8DLEpDjpjCC53w",
    "postId": "CYTwRZtrhHuYf7QYu",
    "parentCommentId": "tjqi2j2Mu5uAhJgzZ",
    "topLevelCommentId": "Nhpkqqcptga49vBj6",
    "contents": {
      "markdown": "Yeah, I agree that it's easy to err in that direction, and I've sometimes done so. Going forward I'm trying to more consistently say the \"obviously I wish people just wouldn't do this\" part.\n\nThough note that even claims like \"unacceptable by any normal standards of risk management\" feel off to me. We're talking about the future of humanity, there *is* no normal standard of risk management. This should feel as silly as the US or UK invoking \"normal standards of risk management\" in debates over whether to join WW2.",
      "plaintextDescription": "Yeah, I agree that it's easy to err in that direction, and I've sometimes done so. Going forward I'm trying to more consistently say the \"obviously I wish people just wouldn't do this\" part.\n\nThough note that even claims like \"unacceptable by any normal standards of risk management\" feel off to me. We're talking about the future of humanity, there is no normal standard of risk management. This should feel as silly as the US or UK invoking \"normal standards of risk management\" in debates over whether to join WW2."
    },
    "baseScore": 16,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-07-08T23:55:38.904Z",
    "af": false
  },
  {
    "_id": "RZ5ijGRA4G3oHWXYH",
    "postId": "onsZ4JeA8EMX3hb7B",
    "parentCommentId": "neohqChXgYn2o3odq",
    "topLevelCommentId": "neohqChXgYn2o3odq",
    "contents": {
      "markdown": "FWIW the comments feel fine to me, but I'm guessing that many of the downvotes are partisan.",
      "plaintextDescription": "FWIW the comments feel fine to me, but I'm guessing that many of the downvotes are partisan."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-08T23:32:24.157Z",
    "af": false
  },
  {
    "_id": "a2PqMssqLEHdhtnDx",
    "postId": "CYTwRZtrhHuYf7QYu",
    "parentCommentId": "2BqXmGxzLgGtrrcnL",
    "topLevelCommentId": "Nhpkqqcptga49vBj6",
    "contents": {
      "markdown": "FWIW I broadcast the former rather than the latter because from the 25% perspective there are many possible worlds which the \"stop\" coalition ends up making much worse, and therefore I can't honestly broadcast \"this is ridiculous and should stop\" without being more specific about what I'd want from the stop coalition.\n\nA (loose) analogy: leftists in Iran who confidently argued \"the Shah's regime is ridiculous and should stop\". It turned out that there was so much variance in *how* it stopped that this argument wasn't actually a good one to confidently broadcast, despite in some sense being correct.",
      "plaintextDescription": "FWIW I broadcast the former rather than the latter because from the 25% perspective there are many possible worlds which the \"stop\" coalition ends up making much worse, and therefore I can't honestly broadcast \"this is ridiculous and should stop\" without being more specific about what I'd want from the stop coalition.\n\nA (loose) analogy: leftists in Iran who confidently argued \"the Shah's regime is ridiculous and should stop\". It turned out that there was so much variance in how it stopped that this argument wasn't actually a good one to confidently broadcast, despite in some sense being correct."
    },
    "baseScore": 17,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-07-08T18:11:57.969Z",
    "af": false
  },
  {
    "_id": "HofddZdMj7KvtuGs4",
    "postId": "52ygLry5KCdvxY6zn",
    "parentCommentId": "FYnXHGAmu6wESmfBi",
    "topLevelCommentId": "iqghi26KAHhyjHbr4",
    "contents": {
      "markdown": "Ty for the comment, I stumbled upon the post, misread the dates, and had started working on a submission.",
      "plaintextDescription": "Ty for the comment, I stumbled upon the post, misread the dates, and had started working on a submission."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-07T22:49:20.393Z",
    "af": false
  },
  {
    "_id": "iqghi26KAHhyjHbr4",
    "postId": "52ygLry5KCdvxY6zn",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "The submission form says that it's no longer accepting responses, FYI.",
      "plaintextDescription": "The submission form says that it's no longer accepting responses, FYI."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-07T15:51:14.439Z",
    "af": false
  },
  {
    "_id": "yYTdSYowALeHrorx8",
    "postId": "3EzbtNLdcnZe8og8b",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I suspect that many of the things you've said here are also true for humans.\n\nThat is, humans often conceptualize ourselves in terms of underspecified identities. Who am I? I'm Richard. What's my opinion on this post? Well, being \"Richard\" doesn't specify how I should respond to this post. But let me check the cached facts I believe about myself (\"I'm truth-seeking\"; \"I'm polite\") and construct an answer which fits well with those facts. A child might start off not really knowing what \"polite\" means, but still wanting to be polite, and gradually flesh out what that means that as they learn more about the world.\n\nAnother way of putting this point: being pulled from the void is not a feature of LLM personas. It's a feature of *personas*. Personas start off with underspecified narratives that fail to predict most behavior (but are self-fulfilling) and then gradually [systematize](https://www.alignmentforum.org/posts/J2kpxLjEyqh6x3oA4/value-systematization-how-values-become-coherent-and) to infer deeper motivations, resolving conflicts with the actual drivers of behavior along the way.\n\nWhat's the takeaway here? We should still be worried about models learning the *wrong* self-fulfilling prophecies. But the \"pulling from the void\" thing should be seen less as an odd thing that we're doing with AIs, and more as a claim about the nature of minds in general.",
      "plaintextDescription": "I suspect that many of the things you've said here are also true for humans.\n\nThat is, humans often conceptualize ourselves in terms of underspecified identities. Who am I? I'm Richard. What's my opinion on this post? Well, being \"Richard\" doesn't specify how I should respond to this post. But let me check the cached facts I believe about myself (\"I'm truth-seeking\"; \"I'm polite\") and construct an answer which fits well with those facts. A child might start off not really knowing what \"polite\" means, but still wanting to be polite, and gradually flesh out what that means that as they learn more about the world.\n\nAnother way of putting this point: being pulled from the void is not a feature of LLM personas. It's a feature of personas. Personas start off with underspecified narratives that fail to predict most behavior (but are self-fulfilling) and then gradually systematize to infer deeper motivations, resolving conflicts with the actual drivers of behavior along the way.\n\nWhat's the takeaway here? We should still be worried about models learning the wrong self-fulfilling prophecies. But the \"pulling from the void\" thing should be seen less as an odd thing that we're doing with AIs, and more as a claim about the nature of minds in general."
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-07-05T15:47:26.120Z",
    "af": true
  },
  {
    "_id": "TnbWrxGduWQSb8g6i",
    "postId": "CYTwRZtrhHuYf7QYu",
    "parentCommentId": "PjtFFKwXzeeoiNvyw",
    "topLevelCommentId": "Nhpkqqcptga49vBj6",
    "contents": {
      "markdown": "I think that *if* it were to go ahead, it should have been made stronger and clearer. But this wouldn't have been politically feasible, and therefore if that were the standard being aimed for it wouldn't have gone ahead.\n\nThis I think would have been better than the outcome that actually happened.",
      "plaintextDescription": "I think that if it were to go ahead, it should have been made stronger and clearer. But this wouldn't have been politically feasible, and therefore if that were the standard being aimed for it wouldn't have gone ahead.\n\nThis I think would have been better than the outcome that actually happened."
    },
    "baseScore": 13,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-06-30T06:08:34.804Z",
    "af": false
  },
  {
    "_id": "8PEdL4SqsPhuawEKC",
    "postId": "CYTwRZtrhHuYf7QYu",
    "parentCommentId": "er5jvAyFRAgRzC5hf",
    "topLevelCommentId": "Nhpkqqcptga49vBj6",
    "contents": {
      "markdown": "Whoa, this seems very implausible to me. Speaking with the courage of one's convictions in situations which feel high-stakes is an extremely high bar, and I know of few people who I'd describe as consistently doing this.\n\nIf you don't know anyone who *isn't* in this category, consider whether your standards for this are far too low.",
      "plaintextDescription": "Whoa, this seems very implausible to me. Speaking with the courage of one's convictions in situations which feel high-stakes is an extremely high bar, and I know of few people who I'd describe as consistently doing this.\n\nIf you don't know anyone who isn't in this category, consider whether your standards for this are far too low."
    },
    "baseScore": 32,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-06-30T06:05:45.681Z",
    "af": false
  },
  {
    "_id": "FNc7mssEGPDMnpkcG",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "For a while now I've been thinking about the difference between \"top-down\" agents which pursue a single goal, and \"bottom-up\" agents which are built around compromises between many goals/subagents.\n\nI've now decided that the frame of \"centralized\" vs \"distributed\" agents is a better way of capturing this idea, since there's no inherent \"up\" or \"down\" direction in coalitions. It's also more continuous.\n\nCredit to [@Scott Garrabrant](https://www.lesswrong.com/users/scott-garrabrant?mention=user), who something like this point to me a while back, in a way which I didn't grok at the time.",
      "plaintextDescription": "For a while now I've been thinking about the difference between \"top-down\" agents which pursue a single goal, and \"bottom-up\" agents which are built around compromises between many goals/subagents.\n\nI've now decided that the frame of \"centralized\" vs \"distributed\" agents is a better way of capturing this idea, since there's no inherent \"up\" or \"down\" direction in coalitions. It's also more continuous.\n\nCredit to @Scott Garrabrant, who something like this point to me a while back, in a way which I didn't grok at the time."
    },
    "baseScore": 5,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-06-26T17:35:33.798Z",
    "af": false
  },
  {
    "_id": "uQrNNf9wGfuovZD4u",
    "postId": "3hs6MniiEssfL8rPz",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This feels related to the predictive processing framework, in which the classifications of one model are then predicted by another.\n\nMore tangentially, I've previously thought about merging cognitive biases and values, since we can view both of them as deviations from the optimal resource-maximizing policy. For example, suppose that I am willing to bet even when I am being Dutch booked. You could think of that as a type of irrationality, or you could think of it as an expression of me valuing being Dutch booked, and therefore being willing to pay to experience it.\n\nThis is related to the Lacanian/\"existential kink\" idea that most dysfunctions are actually deliberate, caused by subagents that are trying to pursue some goal at odds with the rest of your goals.",
      "plaintextDescription": "This feels related to the predictive processing framework, in which the classifications of one model are then predicted by another.\n\nMore tangentially, I've previously thought about merging cognitive biases and values, since we can view both of them as deviations from the optimal resource-maximizing policy. For example, suppose that I am willing to bet even when I am being Dutch booked. You could think of that as a type of irrationality, or you could think of it as an expression of me valuing being Dutch booked, and therefore being willing to pay to experience it.\n\nThis is related to the Lacanian/\"existential kink\" idea that most dysfunctions are actually deliberate, caused by subagents that are trying to pursue some goal at odds with the rest of your goals."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-26T14:30:53.556Z",
    "af": true
  },
  {
    "_id": "iB6dukMgNfXp9hA8w",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "89s2WGHe8T48r7D4K",
    "topLevelCommentId": "WCJXDFY6hZwL5oKqz",
    "contents": {
      "markdown": "1.  Consider the version of the 5-and-10 problem in which one subagent is assigned to calculate U | take 5, and another calculates U | take 10. The overall agent solves the 5-and-10 problem iff the subagents reason about each other in the \"right ways\", or have the right type of relationship to each other. What that specifically means seems like the sort of question that a scale-free theory of intelligent agency might be able to answer.\n2.  I'm mostly trying to extend pure mathematical frameworks (particularly active inference and a cluster of ideas related to geometric rationality, including picoeconomics and ergodicity economics).",
      "plaintextDescription": " 1. Consider the version of the 5-and-10 problem in which one subagent is assigned to calculate U | take 5, and another calculates U | take 10. The overall agent solves the 5-and-10 problem iff the subagents reason about each other in the \"right ways\", or have the right type of relationship to each other. What that specifically means seems like the sort of question that a scale-free theory of intelligent agency might be able to answer.\n 2. I'm mostly trying to extend pure mathematical frameworks (particularly active inference and a cluster of ideas related to geometric rationality, including picoeconomics and ergodicity economics)."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-10T22:23:32.232Z",
    "af": false
  },
  {
    "_id": "FhXqbsjKTktkwS6A5",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "ySeyzW7h2ufujymet",
    "topLevelCommentId": "erWmikKyxCvHv5jcx",
    "contents": {
      "markdown": "Ooops, good catch. It should have linked to this: [https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=W9N9tTbYSBzM9FvWh](https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=W9N9tTbYSBzM9FvWh) (and I've changed the link now).",
      "plaintextDescription": "Ooops, good catch. It should have linked to this: https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=W9N9tTbYSBzM9FvWh (and I've changed the link now)."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-10T05:31:16.706Z",
    "af": false
  },
  {
    "_id": "erWmikKyxCvHv5jcx",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Here is a broad sketch of how I'd like AI governance to go. I've written this in the form of a \"plan\" but it's not really a sequential plan, more like a list of the most important things to promote.\n\n1.  Identify mechanisms by which the US government could exert control over the most advanced AI systems without strongly concentrating power. For example, how could the government embed observers within major AI labs who report to a central regulatory organization, without that regulatory organization having strong incentives and ability to use their power against their political opponents?\n    1.  In practice I expect this will involve empowering US elected officials (e.g. via greater transparency) to monitor and object to misbehavior by the executive branch.\n2.  Create common knowledge between the US and China that the development of increasingly powerful AI will magnify their own internal conflicts (and empower rogue states) disproportionately more than it empowers them against each other. So instead of a race to world domination, in practice they will face a \"race to stay standing\".\n    1.  Rogue states will be empowered because human lives will be increasingly fragile in the face of AI-designed WMDs. This means that rogue states will be able to threaten superpowers with \"mutually assured genocide\" (though I'm a little wary of spreading this as a meme, and need to think more about ways to make it less self-fulfilling).\n3.  Set up channels for flexible, high-bandwidth cooperation between AI regulators in China and the US (including the \"AI regulators\" in each who try to enforce good behavior from the rest of the world).\n4.  Advocate for an ideology roughly like [the one I sketched out here](https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=W9N9tTbYSBzM9FvWh), as a consensus alignment target for AGIs.\n\nThis is of course all very vague; I'm hoping to flesh it out much more over the coming months, and would welcome thoughts and feedback. Having said that, I'm spending relatively little of my time on this (and focusing on [technical alignment work](https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=WCJXDFY6hZwL5oKqz) instead).",
      "plaintextDescription": "Here is a broad sketch of how I'd like AI governance to go. I've written this in the form of a \"plan\" but it's not really a sequential plan, more like a list of the most important things to promote.\n\n 1. Identify mechanisms by which the US government could exert control over the most advanced AI systems without strongly concentrating power. For example, how could the government embed observers within major AI labs who report to a central regulatory organization, without that regulatory organization having strong incentives and ability to use their power against their political opponents?\n    1. In practice I expect this will involve empowering US elected officials (e.g. via greater transparency) to monitor and object to misbehavior by the executive branch.\n 2. Create common knowledge between the US and China that the development of increasingly powerful AI will magnify their own internal conflicts (and empower rogue states) disproportionately more than it empowers them against each other. So instead of a race to world domination, in practice they will face a \"race to stay standing\".\n    1. Rogue states will be empowered because human lives will be increasingly fragile in the face of AI-designed WMDs. This means that rogue states will be able to threaten superpowers with \"mutually assured genocide\" (though I'm a little wary of spreading this as a meme, and need to think more about ways to make it less self-fulfilling).\n 3. Set up channels for flexible, high-bandwidth cooperation between AI regulators in China and the US (including the \"AI regulators\" in each who try to enforce good behavior from the rest of the world).\n 4. Advocate for an ideology roughly like the one I sketched out here, as a consensus alignment target for AGIs.\n\nThis is of course all very vague; I'm hoping to flesh it out much more over the coming months, and would welcome thoughts and feedback. Having said that, I'm spending relatively little of my time on this (and focusing on technical alignment"
    },
    "baseScore": 29,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-06-09T21:38:12.700Z",
    "af": false
  },
  {
    "_id": "WCJXDFY6hZwL5oKqz",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Here is the broad technical plan that I am pursuing with most of my time (with [my AI governance agenda](https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=erWmikKyxCvHv5jcx) taking up most of my remaining time):\n\n1.  Mathematically characterize a [scale-free theory of intelligent agency](https://www.alignmentforum.org/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency) which describes intelligent agents in terms of interactions between their subagents.\n    1.  A successful version of this theory will retrodict phenomena like the [Waluigi effect](https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post), solve theoretical problems like [the five-and-ten problem](https://www.lesswrong.com/w/5-and-10), and make new high-level predictions about AI behavior.\n2.  Identify subagents (and subsubagents, and so on) within neural networks by searching their weights and activations for the patterns of interactions between subagents that this theory predicts.\n    1.  A helpful analogy is how [Burns et al. (2022)](https://arxiv.org/abs/2212.03827) search for beliefs inside neural networks based on the patterns that probability theory predicts. However, I'm not wedded to any particular search methodology.\n3.  Characterize the behaviors associated with each subagent to build up \"maps\" of the motivational systems of the most advanced AI systems.\n    1.  This would ideally give you explanations of AI behavior that scales in quality based on how much effort you put in. E.g. you might be able to predict 80% of the variance in an AI's choices by looking at which highest-level subagents are activated, then 80% of the remaining variance by looking at which subsubagents are activated, and so on.\n4.  Monitor patterns of activations of different subagents to do lie detection, anomaly detection, and other useful things.\n    1.  This wouldn't be fully reliable—e.g. there'd still be some possible failures where low-level subagents activate in ways that, when combined, leads to behavior that's very surprising given the activations of high-level subagents. (ARC's research seems to be aimed at these [worst-case examples](https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc).) However, I expect it would be hard even for AIs with significantly superhuman intelligence to deliberately contort their thinking in this way. And regardless, in order to solve worst-case examples it seems productive to try to solve the average-case examples first.\n\nI'm focusing on step 1 right now. Note that my pursuit of it is overdetermined—I'm excited enough about finding a scale-free theory of intelligent agency that I'd still be working on it even if I didn't think steps 2-4 would work, because I have a strong heuristic that pursuing fundamental knowledge is good. Trying to backchain from an ambitious goal to reasons why a fundamental scientific advance would be useful for achieving that goal feels pretty silly from my perspective. But since people keep asking me why step 1 would help with alignment, I decided to write this up as a central example.",
      "plaintextDescription": "Here is the broad technical plan that I am pursuing with most of my time (with my AI governance agenda taking up most of my remaining time):\n\n 1. Mathematically characterize a scale-free theory of intelligent agency which describes intelligent agents in terms of interactions between their subagents.\n    1. A successful version of this theory will retrodict phenomena like the Waluigi effect, solve theoretical problems like the five-and-ten problem, and make new high-level predictions about AI behavior.\n 2. Identify subagents (and subsubagents, and so on) within neural networks by searching their weights and activations for the patterns of interactions between subagents that this theory predicts.\n    1. A helpful analogy is how Burns et al. (2022) search for beliefs inside neural networks based on the patterns that probability theory predicts. However, I'm not wedded to any particular search methodology.\n 3. Characterize the behaviors associated with each subagent to build up \"maps\" of the motivational systems of the most advanced AI systems.\n    1. This would ideally give you explanations of AI behavior that scales in quality based on how much effort you put in. E.g. you might be able to predict 80% of the variance in an AI's choices by looking at which highest-level subagents are activated, then 80% of the remaining variance by looking at which subsubagents are activated, and so on.\n 4. Monitor patterns of activations of different subagents to do lie detection, anomaly detection, and other useful things.\n    1. This wouldn't be fully reliable—e.g. there'd still be some possible failures where low-level subagents activate in ways that, when combined, leads to behavior that's very surprising given the activations of high-level subagents. (ARC's research seems to be aimed at these worst-case examples.) However, I expect it would be hard even for AIs with significantly superhuman intelligence to deliberately contort their thinking in this way. And regardless, in order t"
    },
    "baseScore": 20,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-06-09T21:07:41.335Z",
    "af": false
  },
  {
    "_id": "QfJ6buocLAdPLDd9E",
    "postId": "d4armqGcbPywR3Ptc",
    "parentCommentId": "b7ye4NsqAT4RBLNME",
    "topLevelCommentId": "b7ye4NsqAT4RBLNME",
    "contents": {
      "markdown": "Yepp, see also some of my speculations here: https://x.com/richardmcngo/status/1815115538059894803?s=46",
      "plaintextDescription": "Yepp, see also some of my speculations here: https://x.com/richardmcngo/status/1815115538059894803?s=46"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-30T15:35:28.331Z",
    "af": false
  },
  {
    "_id": "6fuocnkADsZRwo4Gh",
    "postId": "3LcyoqNTJuCZ65MbL",
    "parentCommentId": "7Zhjzp6feo5Po6nuC",
    "topLevelCommentId": "7Zhjzp6feo5Po6nuC",
    "contents": {
      "markdown": "Have you read [The Metamorphosis of Prime Intellect](https://localroger.com/prime-intellect/)? Fits the bill.",
      "plaintextDescription": "Have you read The Metamorphosis of Prime Intellect? Fits the bill."
    },
    "baseScore": 12,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-05-27T15:41:17.445Z",
    "af": false
  },
  {
    "_id": "r3gFkJczexwoDtbCi",
    "postId": "TyusAoBMjYzGN3eZS",
    "parentCommentId": "cdmYtFZjyvBmimqgQ",
    "topLevelCommentId": "9CyrBcRjXhTFytvuX",
    "contents": {
      "markdown": "Interesting. Got a short summary of what's changing your mind?\n\nI now have a better understanding of coalitional agency, which I will be interested in your thoughts on when I write it up.",
      "plaintextDescription": "Interesting. Got a short summary of what's changing your mind?\n\nI now have a better understanding of coalitional agency, which I will be interested in your thoughts on when I write it up."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-27T14:50:55.119Z",
    "af": false
  },
  {
    "_id": "r42tMKfEsYirWYzJ4",
    "postId": "FBZmnmbDBqhmhGhby",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Our government is determined to lose the AI race in the name of winning the AI race.\n> \n> The least we can do, if prioritizing winning the race, is to try and actually win it.\n\n  \nThis is a bizarre pair of claims to make. But I think it illustrates a surprisingly common mistake from the AI safety community, which I call \"jumping down the slippery slope\". More on this in a forthcoming blog post, but the key idea is that when you look at a situation from a high level of abstraction, it often seems like sliding down a slippery slope towards a bad equilibrium is inevitable. From that perspective, the sort of people who think in terms of high-level abstractions feel almost offended when people *don't* slide down that slope. On a psychological level, the short-term benefit of \"I get to tell them that my analysis is more correct than theirs\" outweighs the long-term benefit of \"people aren't sliding down the slippery slope\".\n\nOne situation where I sometimes get this feeling is when a shopkeeper charges less than the market rate, because they want to be kind to their customers. This is typically a redistribution of money from a wealthier person to less wealthy people; and either way it's a virtuous thing to do. But I sometimes actually get annoyed at them, and itch to smugly say \"listen, you dumbass, you just don't understand economics\". It's like a part of me thinks of reaching the equilibrium as a goal in itself, whether or not we actually like the equilibrium.\n\nThis is obviously a much worse thing to do in AI safety. Relevant examples include Situational Awareness and safety-motivated capability evaluations (e.g. \"building great capabilities evals is a thing the labs should obviously do, so our work on it isn't harmful\"). It feels like Zvi is doing this here too. *Why* is trying to actually win it the least we can do? Isn't this exactly the opposite of what would promote crucial international cooperation on AI? Is it really so *annoying* when your opponents are shooting themselves in the foot that it's worth advocating for them to stop doing that?\n\nIt kinda feels like the old joke:\n\n> On a beautiful Sunday afternoon in the midst of the French Revolution the revolting citizens led a priest, a drunkard and an engineer to the guillotine. They ask the priest if he wants to face up or down when he meets his fate. The priest says he would like to face up so he will be looking towards heaven when he dies. They raise the blade of the guillotine and release it. It comes speeding down and suddenly stops just inches from his neck. The authorities take this as divine intervention and release the priest.\n> \n> The drunkard comes to the guillotine next. He also decides to die face up, hoping that he will be as fortunate as the priest. They raise the blade of the guillotine and release it. It comes speeding down and suddenly stops just inches from his neck. Again, the authorities take this as a sign of divine intervention, and they release the drunkard as well.\n> \n> Next is the engineer. He, too, decides to die facing up. As they slowly raise the blade of the guillotine, the engineer suddenly says, \"Hey, I see what your problem is ...\"",
      "plaintextDescription": "> Our government is determined to lose the AI race in the name of winning the AI race.\n> \n> The least we can do, if prioritizing winning the race, is to try and actually win it.\n\n\nThis is a bizarre pair of claims to make. But I think it illustrates a surprisingly common mistake from the AI safety community, which I call \"jumping down the slippery slope\". More on this in a forthcoming blog post, but the key idea is that when you look at a situation from a high level of abstraction, it often seems like sliding down a slippery slope towards a bad equilibrium is inevitable. From that perspective, the sort of people who think in terms of high-level abstractions feel almost offended when people don't slide down that slope. On a psychological level, the short-term benefit of \"I get to tell them that my analysis is more correct than theirs\" outweighs the long-term benefit of \"people aren't sliding down the slippery slope\".\n\nOne situation where I sometimes get this feeling is when a shopkeeper charges less than the market rate, because they want to be kind to their customers. This is typically a redistribution of money from a wealthier person to less wealthy people; and either way it's a virtuous thing to do. But I sometimes actually get annoyed at them, and itch to smugly say \"listen, you dumbass, you just don't understand economics\". It's like a part of me thinks of reaching the equilibrium as a goal in itself, whether or not we actually like the equilibrium.\n\nThis is obviously a much worse thing to do in AI safety. Relevant examples include Situational Awareness and safety-motivated capability evaluations (e.g. \"building great capabilities evals is a thing the labs should obviously do, so our work on it isn't harmful\"). It feels like Zvi is doing this here too. Why is trying to actually win it the least we can do? Isn't this exactly the opposite of what would promote crucial international cooperation on AI? Is it really so annoying when your opponents are shooting themsel"
    },
    "baseScore": 36,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-05-15T05:55:00.123Z",
    "af": false
  },
  {
    "_id": "EbvgM2wNks3MNDZ6Y",
    "postId": "GbfDZtBd2LTfwv5iQ",
    "parentCommentId": "vdxAruNmzEH6yh5Mn",
    "topLevelCommentId": "6CsjQSjk4HksW73F3",
    "contents": {
      "markdown": "Approximately every contentious issue has caused tremendous amounts of real-world pain. Therefore the choice of which issues to police contempt about becomes a de facto political standard.",
      "plaintextDescription": "Approximately every contentious issue has caused tremendous amounts of real-world pain. Therefore the choice of which issues to police contempt about becomes a de facto political standard."
    },
    "baseScore": 42,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2025-05-08T04:51:11.620Z",
    "af": false
  },
  {
    "_id": "RY6f77KfFE68hgmEK",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "zGshPL3KaZEkRtvvu",
    "topLevelCommentId": "y9NbEQumvqhywrMzE",
    "contents": {
      "markdown": "I think my thought process when I typed \"risk-averse money-maximizer\" was that an agent could be risk-averse (in which case it wouldn't be an EUM) and then separately be a money-maximizer.\n\nBut I didn't explicitly think \"the risk-aversion would be with regard to utility not money, and risk-aversion with regard to money could still be risk-neutral with regard to utility\", so I appreciate the clarification.",
      "plaintextDescription": "I think my thought process when I typed \"risk-averse money-maximizer\" was that an agent could be risk-averse (in which case it wouldn't be an EUM) and then separately be a money-maximizer.\n\nBut I didn't explicitly think \"the risk-aversion would be with regard to utility not money, and risk-aversion with regard to money could still be risk-neutral with regard to utility\", so I appreciate the clarification."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-04-24T22:59:37.624Z",
    "af": true
  },
  {
    "_id": "Bqhf5ZKaKeybAd2GJ",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "yEusu77sxdsbMuwgh",
    "topLevelCommentId": "y9NbEQumvqhywrMzE",
    "contents": {
      "markdown": "Your example bet is a probabilistic mixture of two options: $0 and $2. The agent prefers one of the options individually (getting $2) over any probabilistic mixture of getting $0 and $2.\n\nIn other words, your example rebuts the claim that an EUM can't prefer a probabilistic mixture of two options to the expectation of those two options. But that's not the claim I made.",
      "plaintextDescription": "Your example bet is a probabilistic mixture of two options: $0 and $2. The agent prefers one of the options individually (getting $2) over any probabilistic mixture of getting $0 and $2.\n\nIn other words, your example rebuts the claim that an EUM can't prefer a probabilistic mixture of two options to the expectation of those two options. But that's not the claim I made."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-04-24T22:53:36.474Z",
    "af": true
  },
  {
    "_id": "nctGRGzuWiCaEr9Rc",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "Yow8YuPew85CzTtf7",
    "topLevelCommentId": "Yow8YuPew85CzTtf7",
    "contents": {
      "markdown": "Hmm, this feels analogous to saying \"companies are an unnecessary abstraction in economic theory, since individuals could each make separate contracts about how they'll interact with each other. Therefore we can reduce economics to studying isolated individuals\".\n\nBut companies are in fact a very useful unit of analysis. For example, instead of talking about the separate ways in which each person in the company has committed to treating each other person in the company, you can talk about the HR policy which governs all interactions within the company. You might then see emergent effects (like political battles over what the HR policies are) which are very hard to reason about when taking a single-agent view.\n\nSimilarly, although in principle you could have any kind of graph of which agents listen to which other agents, in practice I expect that realistic agents will tend to consist of clusters of agents which all \"listen to\" each other in some ways. This is both because clustering is efficient (hence animals having bodies made up of clusters of cells; companies being made of clusters of individuals; etc) and because when you even define what *counts* as a single agent, you're doing a kind of clustering. That is, I think that the first step of talking about \"individual rationality\" is implicitly defining which coalitions qualify as individuals.",
      "plaintextDescription": "Hmm, this feels analogous to saying \"companies are an unnecessary abstraction in economic theory, since individuals could each make separate contracts about how they'll interact with each other. Therefore we can reduce economics to studying isolated individuals\".\n\nBut companies are in fact a very useful unit of analysis. For example, instead of talking about the separate ways in which each person in the company has committed to treating each other person in the company, you can talk about the HR policy which governs all interactions within the company. You might then see emergent effects (like political battles over what the HR policies are) which are very hard to reason about when taking a single-agent view.\n\nSimilarly, although in principle you could have any kind of graph of which agents listen to which other agents, in practice I expect that realistic agents will tend to consist of clusters of agents which all \"listen to\" each other in some ways. This is both because clustering is efficient (hence animals having bodies made up of clusters of cells; companies being made of clusters of individuals; etc) and because when you even define what counts as a single agent, you're doing a kind of clustering. That is, I think that the first step of talking about \"individual rationality\" is implicitly defining which coalitions qualify as individuals."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-04-24T02:58:10.854Z",
    "af": true
  },
  {
    "_id": "adwtzxJgt8HQXMrKm",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "5iQCnCDEG8wWfn8Ln",
    "topLevelCommentId": "y9NbEQumvqhywrMzE",
    "contents": {
      "markdown": "> a superintelligent AI probably has a pretty good guess of the other AI's real utility function based on its own historical knowledge, simulations, etc.\n\nThis seems very unclear to me—in general it's not easy for agents to predict the goals of other agents with their own level of intelligence, because the amount of intelligence aimed at deception increases in proportion to the amount of intelligence aimed at discovering that deception.\n\n(You could look at the AI's behavior from when it was less intelligent, but then—as with humans—it's hard to distinguish sincere change from improvement at masking undesirable goals.)\n\nBut regardless, that's a separate point. If you can do that, you don't need your mechanism above. If you can't, then my objection still holds.",
      "plaintextDescription": "> a superintelligent AI probably has a pretty good guess of the other AI's real utility function based on its own historical knowledge, simulations, etc.\n\nThis seems very unclear to me—in general it's not easy for agents to predict the goals of other agents with their own level of intelligence, because the amount of intelligence aimed at deception increases in proportion to the amount of intelligence aimed at discovering that deception.\n\n(You could look at the AI's behavior from when it was less intelligent, but then—as with humans—it's hard to distinguish sincere change from improvement at masking undesirable goals.)\n\nBut regardless, that's a separate point. If you can do that, you don't need your mechanism above. If you can't, then my objection still holds."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-04-24T02:43:36.875Z",
    "af": true
  },
  {
    "_id": "nigkBt47pLMi5tnGd",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "v7igQ6HcnbHyzyzq6",
    "topLevelCommentId": "y9NbEQumvqhywrMzE",
    "contents": {
      "markdown": "One argument for being optimistic: the universe is just very big, and there's a lot to go around. So there's a huge amount of room for positive-sum bargaining.\n\nAnother: at any given point in time, few of the agents that currently exist would want their goals to become significantly simplified (all else equal). So there's a strong incentive to coordinate to reduce competition on this axis.\n\nLastly: if at each point in time, the set of agents who are alive are in conflict with potentially-simpler future agents in a very destructive way, then they should all just Do Something Else. In particular, if there's some decision-theoretic argument roughly like \"more powerful agents should continue to spend some of their resources on the values of their less-powerful ancestors, to reduce the incentives for inter-generational conflict\", even agents with very simple goals might be motivated by it. I call this \"[the generational contract](https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=W9N9tTbYSBzM9FvWh)\".",
      "plaintextDescription": "One argument for being optimistic: the universe is just very big, and there's a lot to go around. So there's a huge amount of room for positive-sum bargaining.\n\nAnother: at any given point in time, few of the agents that currently exist would want their goals to become significantly simplified (all else equal). So there's a strong incentive to coordinate to reduce competition on this axis.\n\nLastly: if at each point in time, the set of agents who are alive are in conflict with potentially-simpler future agents in a very destructive way, then they should all just Do Something Else. In particular, if there's some decision-theoretic argument roughly like \"more powerful agents should continue to spend some of their resources on the values of their less-powerful ancestors, to reduce the incentives for inter-generational conflict\", even agents with very simple goals might be motivated by it. I call this \"the generational contract\"."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-04-24T00:52:45.205Z",
    "af": true
  },
  {
    "_id": "mSdv6DHq7CdY7Yj6e",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "LcosFBffSY7xvKzHj",
    "topLevelCommentId": "y9NbEQumvqhywrMzE",
    "contents": {
      "markdown": "I found this a very interesting question to try to answer. My first reaction was that I don't expect EUMs with explicit utility functions to be competitive enough for this to be very relevant (like how purely symbolic AI isn't competitive enough with deep learning to be very relevant).\n\nBut then I thought about how companies are close-ish to having an explicit utility function (maximize shareholder value) which can be merged with others (e.g. via acquisitions). And this *does* let them fundraise better, merge into each other, and so on.\n\nSimilarly, we can think of cases where countries were joined together by strategic marriages (the unification of Spain, say) as only being possible because the (messy, illegible) interests of the country were rounded off to the (relatively simple) interests of their royals. And so the royals being guaranteed power over the merged entity via marriage allowed the mergers to happen much more easily than if they had to create a merger which served the interests of the \"country as a whole\".\n\nFor a more modern illustration: suppose that the world ends up with a small council who decide how AGI goes. Then countries with a dictator could easily bargain to join this coalition in exchange for their dictator getting a seat on this council. Whereas democratic countries would have a harder time doing so, because they might feel very internally conflicted about their current leader gaining the level of power that they'd get from joining the council.\n\n(This all feels very related to Seeing Like a State, which I've just started reading.)\n\nSo upon reflection: yes, it's reasonable to interpret me as trying to solve the problem of getting the *benefits* of being governed by a set of simple and relatively legible goals, without the costs that are usually associated with that.\n\nNote that I say \"legible goals\" instead of \"EUM\" because in my mind you can be an EUM with illegible goals (like a neural network that implements EUM internally), or a non-EUM with legible goals (like a risk-averse money-maximizer), and merging is more bottlenecked on legibility than EUM-ness.",
      "plaintextDescription": "I found this a very interesting question to try to answer. My first reaction was that I don't expect EUMs with explicit utility functions to be competitive enough for this to be very relevant (like how purely symbolic AI isn't competitive enough with deep learning to be very relevant).\n\nBut then I thought about how companies are close-ish to having an explicit utility function (maximize shareholder value) which can be merged with others (e.g. via acquisitions). And this does let them fundraise better, merge into each other, and so on.\n\nSimilarly, we can think of cases where countries were joined together by strategic marriages (the unification of Spain, say) as only being possible because the (messy, illegible) interests of the country were rounded off to the (relatively simple) interests of their royals. And so the royals being guaranteed power over the merged entity via marriage allowed the mergers to happen much more easily than if they had to create a merger which served the interests of the \"country as a whole\".\n\nFor a more modern illustration: suppose that the world ends up with a small council who decide how AGI goes. Then countries with a dictator could easily bargain to join this coalition in exchange for their dictator getting a seat on this council. Whereas democratic countries would have a harder time doing so, because they might feel very internally conflicted about their current leader gaining the level of power that they'd get from joining the council.\n\n(This all feels very related to Seeing Like a State, which I've just started reading.)\n\nSo upon reflection: yes, it's reasonable to interpret me as trying to solve the problem of getting the benefits of being governed by a set of simple and relatively legible goals, without the costs that are usually associated with that.\n\nNote that I say \"legible goals\" instead of \"EUM\" because in my mind you can be an EUM with illegible goals (like a neural network that implements EUM internally), or a non-EUM with l"
    },
    "baseScore": 21,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-04-23T23:37:15.890Z",
    "af": true
  },
  {
    "_id": "MjW2ptMagxu4hCAQg",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "2GsfPeEgtSmeJpkhg",
    "topLevelCommentId": "2GsfPeEgtSmeJpkhg",
    "contents": {
      "markdown": "I've changed my mind. Coming up with the ideas above has given me a better sense of how agent foundations progress could be valuable.\n\nMore concretely, I used to focus more on the ways in which agent foundations applied in the infinite-compute limit, which is not a setting that I think is very relevant. But I am more optimistic about agent foundations as the study of idealized multi-agent interactions. In hindsight a bunch of the best agent foundations research (e.g. logical induction) was both at once, but I'd only been viewing it as the former.\n\nMore abstractly, this update has made me more optimistic about conceptual progress in general. I guess it's hard to viscerally internalize the extent to which it's possible for concepts to break and reform in new configurations without having experienced that. (People who were strongly religious then deconverted have probably had a better sense of this all along, but I never had that experience.) \n\nI'm glad I did the compromise before; it ended up reducing the consequences of this mistake somewhat (though I'd guess including one week of agent foundations in the course had a pretty small effect).",
      "plaintextDescription": "I've changed my mind. Coming up with the ideas above has given me a better sense of how agent foundations progress could be valuable.\n\nMore concretely, I used to focus more on the ways in which agent foundations applied in the infinite-compute limit, which is not a setting that I think is very relevant. But I am more optimistic about agent foundations as the study of idealized multi-agent interactions. In hindsight a bunch of the best agent foundations research (e.g. logical induction) was both at once, but I'd only been viewing it as the former.\n\nMore abstractly, this update has made me more optimistic about conceptual progress in general. I guess it's hard to viscerally internalize the extent to which it's possible for concepts to break and reform in new configurations without having experienced that. (People who were strongly religious then deconverted have probably had a better sense of this all along, but I never had that experience.) \n\nI'm glad I did the compromise before; it ended up reducing the consequences of this mistake somewhat (though I'd guess including one week of agent foundations in the course had a pretty small effect)."
    },
    "baseScore": 24,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-04-23T23:15:09.775Z",
    "af": false
  },
  {
    "_id": "giBHviTKk3nN2ADvk",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "RJdwwfM5iFoH2zhSq",
    "topLevelCommentId": "y9NbEQumvqhywrMzE",
    "contents": {
      "markdown": "I think this addresses the problem I'm discussing only in the case where the source code contains an explicit utility function. You can then create new source code by merging those utility functions.\n\nBut in the case where it doesn't (e.g. the source code is an uninterpretable neural network) you are left with the same problem.\n\nEdited to add: Though even when the utility function is explicit, it seems like the benefits of lying about your source code could outweigh the cost of changing your utility function. For example, suppose A and B are bargaining, and A says \"you should give me more cake because I get very angry if I don't get cake\". Even if this starts off as a lie, it might then be in A's interests to use your mechanism above to self-modify into A' that *does* get very angry if it doesn't get cake, and which therefore has a better bargaining position (because, under your protocol, it has \"proved\" that it was A' all along).",
      "plaintextDescription": "I think this addresses the problem I'm discussing only in the case where the source code contains an explicit utility function. You can then create new source code by merging those utility functions.\n\nBut in the case where it doesn't (e.g. the source code is an uninterpretable neural network) you are left with the same problem.\n\nEdited to add: Though even when the utility function is explicit, it seems like the benefits of lying about your source code could outweigh the cost of changing your utility function. For example, suppose A and B are bargaining, and A says \"you should give me more cake because I get very angry if I don't get cake\". Even if this starts off as a lie, it might then be in A's interests to use your mechanism above to self-modify into A' that does get very angry if it doesn't get cake, and which therefore has a better bargaining position (because, under your protocol, it has \"proved\" that it was A' all along)."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-04-23T23:02:21.645Z",
    "af": true
  },
  {
    "_id": "MzywGqPeADLh2b23L",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "y9NbEQumvqhywrMzE",
    "topLevelCommentId": "y9NbEQumvqhywrMzE",
    "contents": {
      "markdown": "Consider you and I merging (say, in a marriage). Suppose that all points on the Pareto frontier involve us pursuing a fully-consistent strategy. But if some decisions are your responsibility, and other decisions are my responsibility, then we might end up with some of our actions inconsistent with others (say, if we haven't had a chance to communicate before deciding). That's not on the Pareto frontier.\n\nWhat *is* on the Pareto frontier is you being dictator, and then accounting for my utility function when making your dictatorial decisions. But of course this is something I will object to, because in any realistic scenario I wouldn't trust you enough to give you dictatorial power over me. Once you have that power, continuing to account for my utility is strongly non-incentive-compatible for you. So we're more likely to each want to retain some power, even if it sometimes causes inefficiency. (The same is true on the level of countries, which accept a bunch of inefficiency from democratic competition in exchange for incentive-compatibility and trust.)\n\nAnother way of putting this:  I'm focusing on the setting where you cannot do arbitrary merges, you can only do merges that are *constructible* via some set of calls to the existing agents. It's often impossible to construct a fully-consistent merged agent without concentrating power in ways that the original agents would find undesirable (though there sometimes is, e.g. with I-cut-you-choose cake-cutting). So in this setting we need a different conception of rationality than Pareto-optimal.",
      "plaintextDescription": "Consider you and I merging (say, in a marriage). Suppose that all points on the Pareto frontier involve us pursuing a fully-consistent strategy. But if some decisions are your responsibility, and other decisions are my responsibility, then we might end up with some of our actions inconsistent with others (say, if we haven't had a chance to communicate before deciding). That's not on the Pareto frontier.\n\nWhat is on the Pareto frontier is you being dictator, and then accounting for my utility function when making your dictatorial decisions. But of course this is something I will object to, because in any realistic scenario I wouldn't trust you enough to give you dictatorial power over me. Once you have that power, continuing to account for my utility is strongly non-incentive-compatible for you. So we're more likely to each want to retain some power, even if it sometimes causes inefficiency. (The same is true on the level of countries, which accept a bunch of inefficiency from democratic competition in exchange for incentive-compatibility and trust.)\n\nAnother way of putting this:  I'm focusing on the setting where you cannot do arbitrary merges, you can only do merges that are constructible via some set of calls to the existing agents. It's often impossible to construct a fully-consistent merged agent without concentrating power in ways that the original agents would find undesirable (though there sometimes is, e.g. with I-cut-you-choose cake-cutting). So in this setting we need a different conception of rationality than Pareto-optimal."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-04-23T22:26:02.076Z",
    "af": true
  },
  {
    "_id": "6D6si9eZhLxzBC3xk",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "KdxGxYYkj47PkajPx",
    "topLevelCommentId": "eejkQsL8qzbcv68tH",
    "contents": {
      "markdown": "Suppose you're in a setting where the world is so large that you will only ever experience a tiny fraction of it directly, and you have to figure out the rest via generalization. Then your argument doesn't hold up: shifting the mean might totally break your learning. But I claim that the real world is like this. So I am inherently skeptical of any result (like most convergence results) that rely on just trying approximately everything and gradually learning which to prefer and disprefer.",
      "plaintextDescription": "Suppose you're in a setting where the world is so large that you will only ever experience a tiny fraction of it directly, and you have to figure out the rest via generalization. Then your argument doesn't hold up: shifting the mean might totally break your learning. But I claim that the real world is like this. So I am inherently skeptical of any result (like most convergence results) that rely on just trying approximately everything and gradually learning which to prefer and disprefer."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-04-21T20:42:11.403Z",
    "af": false
  },
  {
    "_id": "yykRuYC4fmZMspK6t",
    "postId": "GJgudfEvNx8oeyffH",
    "parentCommentId": "rJZLTqa5AGhRFQgDj",
    "topLevelCommentId": "rJZLTqa5AGhRFQgDj",
    "contents": {
      "markdown": "I just keep coming back to this comment, because there are a couple of lines in it that are downright poetic. I particularly appreciate:\n\n> \"*BE NOT AFRAID*.\" said the ants.\n\nand\n\n> Yeah, and this universe's got time in it, though.\n\nand\n\n> can you imagine how *horrible* the world would get if we didn't honour our precommitments?\n\nHave you considered writing stories of your own?",
      "plaintextDescription": "I just keep coming back to this comment, because there are a couple of lines in it that are downright poetic. I particularly appreciate:\n\n> \"BE NOT AFRAID.\" said the ants.\n\nand\n\n> Yeah, and this universe's got time in it, though.\n\nand\n\n> can you imagine how horrible the world would get if we didn't honour our precommitments?\n\nHave you considered writing stories of your own?"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-04-21T04:37:41.153Z",
    "af": false
  },
  {
    "_id": "t8B6fAHc5zskadf92",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "HPq5zT43DCYup2fFj",
    "topLevelCommentId": "eejkQsL8qzbcv68tH",
    "contents": {
      "markdown": "> if you move towards everything but move towards X even more, then in the long run you will do more of X on net, because you only have so much probability mass to go around\n\nI have a mental category of \"results that are almost entirely irrelevant for realistically-computationally-bounded agents\" (e.g. results related to AIXI), and my gut sense is that this seems like one such result.",
      "plaintextDescription": "> if you move towards everything but move towards X even more, then in the long run you will do more of X on net, because you only have so much probability mass to go around\n\nI have a mental category of \"results that are almost entirely irrelevant for realistically-computationally-bounded agents\" (e.g. results related to AIXI), and my gut sense is that this seems like one such result."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-04-21T04:16:56.864Z",
    "af": false
  },
  {
    "_id": "qjKNjtwqJesw7G8FS",
    "postId": "d4armqGcbPywR3Ptc",
    "parentCommentId": "y5EsHKJCbECsYWHGf",
    "topLevelCommentId": "y5EsHKJCbECsYWHGf",
    "contents": {
      "markdown": "Thanks! Note that Eric Neyman gets credit for the graphs.\n\nOut of curiosity, what are the other sources that explain this? Any worth reading for other insights?",
      "plaintextDescription": "Thanks! Note that Eric Neyman gets credit for the graphs.\n\nOut of curiosity, what are the other sources that explain this? Any worth reading for other insights?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-04-15T22:00:35.745Z",
    "af": false
  },
  {
    "_id": "eejkQsL8qzbcv68tH",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "When you think of goals as reward/utility functions, the distinction between positive and negative motivations (e.g. [as laid out in this sequence](https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL)) isn’t very meaningful, since it all depends on how you normalize them.\n\nBut when you think of goals as world-models (as in predictive processing/active inference) then it’s a very sharp distinction: your world-model-goals can either be of things you should move towards, or things you should move away from*.*\n\nThis updates me towards thinking that the positive/negative motivation distinction is more meaningful than I thought.",
      "plaintextDescription": "When you think of goals as reward/utility functions, the distinction between positive and negative motivations (e.g. as laid out in this sequence) isn’t very meaningful, since it all depends on how you normalize them.\n\nBut when you think of goals as world-models (as in predictive processing/active inference) then it’s a very sharp distinction: your world-model-goals can either be of things you should move towards, or things you should move away from.\n\nThis updates me towards thinking that the positive/negative motivation distinction is more meaningful than I thought."
    },
    "baseScore": 43,
    "voteCount": 18,
    "createdAt": null,
    "postedAt": "2025-04-13T19:30:02.799Z",
    "af": true
  },
  {
    "_id": "ksq8mC3fdEL4k48zE",
    "postId": "ntDA4Q7BaYhWPgzuq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Nice post. I read it quickly but think I agree with basically all of it. I particularly like the section starting \"The AI doesn’t have a cached supergoal for “maximize reward”, but it decides to think anyway about whether reward is an instrumental goal\".\n\n\"The distinct view that *truly terminal* reward maximization is kind of narrow or bizarre or reflection-unstable relative to *instrumental* reward maximization\" is a good summary of my position. You don't say much that directly contradicts this, though I do think that even using the \"terminal reward seeker\" vs \"schemer\" distinction privileges the role of reward a bit too much. For example, I expect that even an aligned AGI will have *some* subagent that cares about reward (e.g. maybe it'll have some sycophantic instincts still). Is it thereby a schemer? Hard to say.\n\nAside from that I'd add a few clarifications (nothing major):\n\n1.  The process of deciding on a new supergoal will probably involve [systematizing](https://www.alignmentforum.org/posts/J2kpxLjEyqh6x3oA4/value-systematization-how-values-become-coherent-and) not just \"maximize reward\" but also a bunch of other drives too—including ones which had previously been classified as special cases of \"maximize reward\" (e.g. \"make humans happy\") but upon reflection are more naturally understood as special cases of the new supergoal.\n2.  It seems like you implicitly assume that the supergoal will be \"in charge\". But I expect that there will be a bunch of conflict between supergoal and lower-level goals, analogous to the conflict between different layers of an organizational hierarchy (or between a human's System 2 motivations and System 1 motivations). I call the spectrum from \"all power is at the top\" to \"all power is at the bottom\" the systematizing-conservatism spectrum.\n\nI think that formalizing the systematizing-conservatism spectrum would be a big step forward in our understanding of misalignment (and cognition more generally). If anyone reading this is interested in working with me on that, apply to my MATS stream in the next 5 days.",
      "plaintextDescription": "Nice post. I read it quickly but think I agree with basically all of it. I particularly like the section starting \"The AI doesn’t have a cached supergoal for “maximize reward”, but it decides to think anyway about whether reward is an instrumental goal\".\n\n\"The distinct view that truly terminal reward maximization is kind of narrow or bizarre or reflection-unstable relative to instrumental reward maximization\" is a good summary of my position. You don't say much that directly contradicts this, though I do think that even using the \"terminal reward seeker\" vs \"schemer\" distinction privileges the role of reward a bit too much. For example, I expect that even an aligned AGI will have some subagent that cares about reward (e.g. maybe it'll have some sycophantic instincts still). Is it thereby a schemer? Hard to say.\n\nAside from that I'd add a few clarifications (nothing major):\n\n 1. The process of deciding on a new supergoal will probably involve systematizing not just \"maximize reward\" but also a bunch of other drives too—including ones which had previously been classified as special cases of \"maximize reward\" (e.g. \"make humans happy\") but upon reflection are more naturally understood as special cases of the new supergoal.\n 2. It seems like you implicitly assume that the supergoal will be \"in charge\". But I expect that there will be a bunch of conflict between supergoal and lower-level goals, analogous to the conflict between different layers of an organizational hierarchy (or between a human's System 2 motivations and System 1 motivations). I call the spectrum from \"all power is at the top\" to \"all power is at the bottom\" the systematizing-conservatism spectrum.\n\nI think that formalizing the systematizing-conservatism spectrum would be a big step forward in our understanding of misalignment (and cognition more generally). If anyone reading this is interested in working with me on that, apply to my MATS stream in the next 5 days."
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-04-12T21:50:20.338Z",
    "af": true
  },
  {
    "_id": "jfTFHNMYCgzXLbfgW",
    "postId": "gyT8sYdXch5RWdpjx",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I left some comments on an earlier version of AI 2027; the most relevant is the following:\n\n> June 2027: Self-improving AI\n> ----------------------------\n> \n> OpenBrain now has a “[country of geniuses in a datacenter](https://darioamodei.com/machines-of-loving-grace#basic-assumptions-and-framework).”\n> \n> Most of the humans at OpenBrain can’t usefully contribute anymore. Some don’t realize this and harmfully micromanage their AI teams. Others sit at their computer screens, watching performance crawl up, and up, and up.\n\nThis is the point where I start significantly disagreeing with the scenario. My expectation is that by this point humans are still better at tasks that take a week or longer. Also, it starts getting really tricky to improve on these, because you get limited by a number of factors: it takes a long time to get real-world feedback, it takes a lot of compute to experiment on week-long tasks, etc.  \n  \nI expect these dynamics to be particularly notable when it comes to coordinating Agent-4 copies. Like, probably a lot of p-hacking, then other agents knowing that p-hacking is happening and covering it up, and so on. I expect a lot of the human time will involve trying to detect clusters of Agent-4 copies that are spiralling off into doing wacky stuff. Also at this point the metrics of performance won't be robust enough to avoid agents goodharting hard on them.",
      "plaintextDescription": "I left some comments on an earlier version of AI 2027; the most relevant is the following:\n\n\n> June 2027: Self-improving AI\n> OpenBrain now has a “country of geniuses in a datacenter.”\n> \n> Most of the humans at OpenBrain can’t usefully contribute anymore. Some don’t realize this and harmfully micromanage their AI teams. Others sit at their computer screens, watching performance crawl up, and up, and up.\n\nThis is the point where I start significantly disagreeing with the scenario. My expectation is that by this point humans are still better at tasks that take a week or longer. Also, it starts getting really tricky to improve on these, because you get limited by a number of factors: it takes a long time to get real-world feedback, it takes a lot of compute to experiment on week-long tasks, etc.\n\nI expect these dynamics to be particularly notable when it comes to coordinating Agent-4 copies. Like, probably a lot of p-hacking, then other agents knowing that p-hacking is happening and covering it up, and so on. I expect a lot of the human time will involve trying to detect clusters of Agent-4 copies that are spiralling off into doing wacky stuff. Also at this point the metrics of performance won't be robust enough to avoid agents goodharting hard on them."
    },
    "baseScore": 22,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2025-04-09T00:56:14.600Z",
    "af": false
  },
  {
    "_id": "snxDvnxzdCxkt5nmB",
    "postId": "6YxdpGjfHyrZb7F2G",
    "parentCommentId": "BgJ9WEuEpbhMJcWfz",
    "topLevelCommentId": "BgJ9WEuEpbhMJcWfz",
    "contents": {
      "markdown": "Good question. Part of the difference is I think the difference between total govt (state + federal) outlays and just federal outlays. But I don't think that would explain all of the discrepancy.\n\nYour question reminds me that I had a discussion with someone else who was skeptical about this graph, and made a note to dig deeper, but never got around to it. I'll chase this up a bit now and see what I find.",
      "plaintextDescription": "Good question. Part of the difference is I think the difference between total govt (state + federal) outlays and just federal outlays. But I don't think that would explain all of the discrepancy.\n\nYour question reminds me that I had a discussion with someone else who was skeptical about this graph, and made a note to dig deeper, but never got around to it. I'll chase this up a bit now and see what I find."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-04-02T05:37:07.903Z",
    "af": false
  },
  {
    "_id": "7kgSRRTev5RMrAWka",
    "postId": "6YxdpGjfHyrZb7F2G",
    "parentCommentId": "bDcmzkHexhYxiQmcT",
    "topLevelCommentId": "Maq3Lb8KZeJ7yHrDr",
    "contents": {
      "markdown": "Ah, gotcha. Unfortunately [I have rejected the concept of Bayesian evidence from my ontology](https://www.mindthefuture.info/p/why-im-not-a-bayesian) and therefore must regard your claim as nonsense. Alas.\n\n(more seriously, sorry for misinterpreting your tone, I have been getting flak from all directions for this talk so am a bit trigger-happy)",
      "plaintextDescription": "Ah, gotcha. Unfortunately I have rejected the concept of Bayesian evidence from my ontology and therefore must regard your claim as nonsense. Alas.\n\n(more seriously, sorry for misinterpreting your tone, I have been getting flak from all directions for this talk so am a bit trigger-happy)"
    },
    "baseScore": 3,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-03-31T22:28:48.385Z",
    "af": false
  },
  {
    "_id": "LBgvaii6Lt22DEj9L",
    "postId": "6YxdpGjfHyrZb7F2G",
    "parentCommentId": "RA2yXCFWGgPAcaPek",
    "topLevelCommentId": "Maq3Lb8KZeJ7yHrDr",
    "contents": {
      "markdown": "I was referring to the inference:\n\n> The explanation that it was done by \"a new hire\" is a classic and easy scapegoat. It's much more straight forward to believe Musk himself wanted this done, and walked it back when it was clear it was more obvious than intended.\n\nObviously this sort of leap to a conclusion is very different from the sort of evidence that one expects upon hearing that literal written evidence (of **Musk trying** to censor) exists. Given this, your comment seems remarkably unproductive.",
      "plaintextDescription": "I was referring to the inference:\n\n> The explanation that it was done by \"a new hire\" is a classic and easy scapegoat. It's much more straight forward to believe Musk himself wanted this done, and walked it back when it was clear it was more obvious than intended.\n\nObviously this sort of leap to a conclusion is very different from the sort of evidence that one expects upon hearing that literal written evidence (of Musk trying to censor) exists. Given this, your comment seems remarkably unproductive."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-31T18:46:21.931Z",
    "af": false
  },
  {
    "_id": "fZ4pWXNmQJYDyb6nx",
    "postId": "HbkNAyAoa4gCnuzwa",
    "parentCommentId": "Bq8x4WCyLYgoHMkwW",
    "topLevelCommentId": "tfrHeoWc5ANAbjgzZ",
    "contents": {
      "markdown": "EDIT: upon reflection the first thing I should do is probably to ask you for a bunch of the best examples of the thing you're talking about throughout history. I.e. insofar as the world is better than it could be (or worse than it could be) at what points did careful philosophical reasoning (or the lack of it) make the biggest difference?\n\nOriginal comment:\n\nThe term \"careful thinking\" here seems to be doing a lot of work, and I'm worried that there's a kind of motte and bailey going on. In your earlier comment you describe it as \"analytical philosophy, or more broadly careful/skeptical philosophy\". But I think we agree that most academic analytic philosophy is bad, and often worse than laypeople's intuitive priors (in part due to strong selection effects on who enters the field—most philosophers of religion believe in god, most philosophers of aesthetics believe in the objectivity of aesthetics, etc).\n\nSo then we can fall back on LessWrong as an example of careful thinking. But as we discussed above, even the leading figure on LessWrong was insufficiently careful even about the *main* focus of his work for it to be robustly valuable.\n\nSo I basically get the sense that the role of careful thinking in your worldview is something like \"the thing that I, Wei Dai, ascribe my success to\". And I do agree that you've been very successful in a bunch of intellectual endeavours. But I expect that your \"secret sauce\" is a confluence of a bunch of factors (including IQ, emotional temperament, background knowledge, etc) only one of which was \"being in a community that prioritized careful thinking\". And then I also think you're missing a bunch of other secret sauces that would make your impact on the world better (like more ability to export your ideas to other people).\n\nIn other words, the bailey seems to be \"careful thinking is the thing we should prioritize in order to make the world better\", and the motte is \"I, Wei Dai, seem to be doing something good, even if basically everyone else is falling into the valley of bad rationality\".\n\nOne reason I'm personally pushing back on this, btw, is that my own self-narrative for why I'm able to be intellectually productive in significant part relies on me being *less* intellectually careful than other people—so that I'm willing to throw out a bunch of ideas that are half-formed and non-rigorous, iterate, and eventually get to the better ones. Similarly, a lot of the value that the wider blogosphere has created comes from people being *less* careful than existing academic norms (including Eliezer and Scott Alexander, whose best works are often quite polemic).\n\nIn short: I totally think we want more people coming up with good ideas, and that this is a big bottleneck. But there are many different directions in which we should tug people in order to make them more intellectually productive. Many academics should be less careful. Many people on LessWrong should be more careful. Some scientists should be less empirical, others should be more empirical; some less mathematically rigorous, others more mathematically rigorous. Others should try to live in countries that are less repressive of new potentially-crazy ideas (hence politics being important). And then, of course, others should be figuring out how to actually get good ideas implemented.\n\nMeanwhile, Eliezer and Sam and Elon should have had less of a burning desire to found an AGI lab. I agree that this can be described by \"wanting to be the hero who saves the world\", but this seems to function as a curiosity stopper for you. When I talk about emotional health a lot of what I mean is finding ways to become less status-oriented (or, in your own words, \"not being distracted/influenced by competing motivations\"). I think of extremely strong motivations to change the world (as these outlier figures have) as typically driven by some kind of core emotional dysregulation. And specifically I think of fear-based motivation as the underlying phenomenon which implements status-seeking and many other behaviors which are harmful when taken too far. (This is not an attempt to *replace* evo-psych, btw—it's an account of the implementation mechanisms that evolution used to get us to do the things it wanted, which now are sometimes maladapted to our current environment.) I write about a bunch of these models in my [Replacing Fear](https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL) sequence.",
      "plaintextDescription": "EDIT: upon reflection the first thing I should do is probably to ask you for a bunch of the best examples of the thing you're talking about throughout history. I.e. insofar as the world is better than it could be (or worse than it could be) at what points did careful philosophical reasoning (or the lack of it) make the biggest difference?\n\nOriginal comment:\n\nThe term \"careful thinking\" here seems to be doing a lot of work, and I'm worried that there's a kind of motte and bailey going on. In your earlier comment you describe it as \"analytical philosophy, or more broadly careful/skeptical philosophy\". But I think we agree that most academic analytic philosophy is bad, and often worse than laypeople's intuitive priors (in part due to strong selection effects on who enters the field—most philosophers of religion believe in god, most philosophers of aesthetics believe in the objectivity of aesthetics, etc).\n\nSo then we can fall back on LessWrong as an example of careful thinking. But as we discussed above, even the leading figure on LessWrong was insufficiently careful even about the main focus of his work for it to be robustly valuable.\n\nSo I basically get the sense that the role of careful thinking in your worldview is something like \"the thing that I, Wei Dai, ascribe my success to\". And I do agree that you've been very successful in a bunch of intellectual endeavours. But I expect that your \"secret sauce\" is a confluence of a bunch of factors (including IQ, emotional temperament, background knowledge, etc) only one of which was \"being in a community that prioritized careful thinking\". And then I also think you're missing a bunch of other secret sauces that would make your impact on the world better (like more ability to export your ideas to other people).\n\nIn other words, the bailey seems to be \"careful thinking is the thing we should prioritize in order to make the world better\", and the motte is \"I, Wei Dai, seem to be doing something good, even if basically everyo"
    },
    "baseScore": 9,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-30T15:30:10.202Z",
    "af": false
  },
  {
    "_id": "sYDWfQm9fg8K62nyL",
    "postId": "xud7Mti9jS4tbWqQE",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I didn't end up putting this in my [coalitional agency post](https://www.lesswrong.com/posts/5tYTKX4pNpiG4vzYg/towards-a-scale-free-theory-of-intelligent-agency), but at one point I had a note discussing our terminological disagreement:\n\n> I don’t like the word hierarchical as much. A theory can be hierarchical without being scale-free—e.g. a theory which describes something in terms of three different layers doing three different things is hierarchical but not scale-free.\n\nWhereas coalitions are typically divided into sub-coalitions (e.g. the \"western civilization\" coalition is divided into countries which are divided into provinces/states; political coalitions are divided into different factions and interest groups; etc). And so \"coalitional\" seems much closer to capturing this fractal/scale-free property.",
      "plaintextDescription": "I didn't end up putting this in my coalitional agency post, but at one point I had a note discussing our terminological disagreement:\n\n> I don’t like the word hierarchical as much. A theory can be hierarchical without being scale-free—e.g. a theory which describes something in terms of three different layers doing three different things is hierarchical but not scale-free.\n\nWhereas coalitions are typically divided into sub-coalitions (e.g. the \"western civilization\" coalition is divided into countries which are divided into provinces/states; political coalitions are divided into different factions and interest groups; etc). And so \"coalitional\" seems much closer to capturing this fractal/scale-free property."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-03-29T23:26:01.176Z",
    "af": true
  },
  {
    "_id": "iJNPojaoELwwiqmKy",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "CHZWWWr5SaDWSdA4m",
    "topLevelCommentId": "W9N9tTbYSBzM9FvWh",
    "contents": {
      "markdown": "Ah, yeah, I was a bit unclear here.\n\nBasic idea is that by conquering someone you may not reduce their welfare very much short-term, but you *do* reduce their power a lot short-term. (E.g. the British conquered India with relatively little welfare impacts on most Indians.)\n\nAnd so it is much harder to defend a conquest as altruistic in the sense of empowering, than it is to defend a conquest as altruistic in the sense of welfare-increasing.\n\nAs you say, this is not a perfect defense mechanism, because sometimes long-term empowerment and short-term empowerment conflict. But there are often strategies that are *less* disempowering short-term which the moral pressure of \"altruism=empowerment\" would push people towards. E.g. it would make it harder for people to set up the \"dictatorship of the proletariat\" in the first place.\n\nAnd in general I think it's actually pretty uncommon for temporary disempowerment to be necessary for long-term empowerment. Re your homework example, there's a wide spectrum from the highly-empowering [Taking Children Seriously](https://takingchildrenseriously.com/) to highly-disempowering Asian tiger parents, and I don't think it's a coincidence that tiger parenting often backfires. Similarly, mandatory mobilization disproportionately happens during wars fought for the wrong reasons.",
      "plaintextDescription": "Ah, yeah, I was a bit unclear here.\n\nBasic idea is that by conquering someone you may not reduce their welfare very much short-term, but you do reduce their power a lot short-term. (E.g. the British conquered India with relatively little welfare impacts on most Indians.)\n\nAnd so it is much harder to defend a conquest as altruistic in the sense of empowering, than it is to defend a conquest as altruistic in the sense of welfare-increasing.\n\nAs you say, this is not a perfect defense mechanism, because sometimes long-term empowerment and short-term empowerment conflict. But there are often strategies that are less disempowering short-term which the moral pressure of \"altruism=empowerment\" would push people towards. E.g. it would make it harder for people to set up the \"dictatorship of the proletariat\" in the first place.\n\nAnd in general I think it's actually pretty uncommon for temporary disempowerment to be necessary for long-term empowerment. Re your homework example, there's a wide spectrum from the highly-empowering Taking Children Seriously to highly-disempowering Asian tiger parents, and I don't think it's a coincidence that tiger parenting often backfires. Similarly, mandatory mobilization disproportionately happens during wars fought for the wrong reasons."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-29T22:10:07.571Z",
    "af": false
  },
  {
    "_id": "FbpWnWLvqDjK9aXMz",
    "postId": "6YxdpGjfHyrZb7F2G",
    "parentCommentId": "HeNEJhK5ui59sfzo6",
    "topLevelCommentId": "Maq3Lb8KZeJ7yHrDr",
    "contents": {
      "markdown": "yepp, see my other comment which anticipated this",
      "plaintextDescription": "yepp, see my other comment which anticipated this"
    },
    "baseScore": 0,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-03-29T16:53:06.345Z",
    "af": false
  },
  {
    "_id": "43bnYaAQmrSkREhk5",
    "postId": "HbkNAyAoa4gCnuzwa",
    "parentCommentId": "aT6snzAYNgAYQuJAP",
    "topLevelCommentId": "tfrHeoWc5ANAbjgzZ",
    "contents": {
      "markdown": "In general I disagree pretty broadly with your view. Not quite sure how best to surface that disagreement but will give a quick shot:\n\nI think it's important to be capable of (at least) two types of reasoning:\n\n*   Precise reasoning about desired outcomes and strategies to get there.\n*   Broad reasoning about heuristics that seem robustly good.\n\nWe see this in the domain of morality, for example: utilitarianism is more like the former, deontology is more like the latter. High-level ideological goals tend to go pretty badly if people stop paying attention to robust deontological heuristics (like \"don't kill people\"). As Eliezer has said somewhere, one of the key reasons to be deontological is that we're running on corrupted hardware. But more generally, we're running on logically uncertain hardware: we can't model all the flow-through effects of our actions on other reasonably intelligent people (hell, we can't even model all the flow-through effects of our actions on, say, animals—who can often \"read\" us in ways we're not tracking). And so we often should be adopting robust-seeming heuristics even when we don't know exactly why they work.\n\nIf you take your interim strategy seriously (but set aside x-risk) then I think you actually end up with something pretty similar to the main priorities of classic liberals: prevent global lock-in (by opposing expansionist powers like the Nazis), prevent domestic political lock-in (via upholding democracy), prevent ideological lock-in (via supporting free speech), give our descendants more optionality (via economic and technological growth). I don't think this is a coincidence—it just often turns out that there are a bunch of heuristics that are really robustly good, and you can converge on them from many different directions. \n\nThis is part of why I'm less sold on \"careful philosophical reasoning\" as **the** key thing. Indeed, wanting to \"commit prematurely to a specific, detailed value system\" is historically very correlated with intellectualism (e.g. elites tend to be the rabid believers in communism, libertarianism, religion, etc—a lot of more \"normal\" people don't take it that seriously even when they're nominally on board). And so it's very plausible that the thing we want is *less* philosophy, because (like, say, asteroid redirection technology) the risks outweigh the benefits.\n\nThen we get to x-risk. That's a domain where many broad heuristics break down (though still fewer than people think, as I'll write about soon). And you might say: well, without careful philosophical reasoning, we wouldn't have identified AI x-risk as a priority. Yes, but also: it's very plausible to me that the net effect of LessWrong-inspired thinking on AI x-risk has been and continues to be negative. I describe some mechanisms [halfway through this talk](https://www.lesswrong.com/posts/6YxdpGjfHyrZb7F2G/third-wave-ai-safety-needs-sociopolitical-thinking), but here are a couple that directly relate to the factors I mentioned in my last comment:\n\n*   First, when people on LessWrong spread the word about AI risk, extreme psychological outliers like Sam Altman and Elon Musk then jump to do AI-related things in a way which often turns out to be destructive because of their trust issues and psychological neuroses.\n*   Second, US governmental responses to AI risk are very much bottlenecked on being a functional government in general, which is bottlenecked by political advocacy (broadly construed) slash political power games.\n*   Third, even within the AI safety community you have a bunch of people contributing to expectations of conflict with China (e.g. Leopold Aschenbrenner and Dan Hendrycks) and acceleration in general (e.g. by working on capabilities at Anthropic, or RSI evals) in a way which I hypothesize would be much better for the world if they had better introspection capabilities (I know this is a strong claim, I have an essay coming out on it soon).\n\nAnd so even here it seems like a bunch of heuristics (such as \"it's better when people are mentally healthier\" and \"it's better when politics is more functional\") actually were strong bottlenecks on the *application* of philosophical reasoning to do good. And I don't think this is a coincidence.\n\ntl;dr: careful philosophical reasoning is just one direction in which you can converge on a robustly good strategy for the future, and indeed is one of the more risky avenues by which to do so.",
      "plaintextDescription": "In general I disagree pretty broadly with your view. Not quite sure how best to surface that disagreement but will give a quick shot:\n\nI think it's important to be capable of (at least) two types of reasoning:\n\n * Precise reasoning about desired outcomes and strategies to get there.\n * Broad reasoning about heuristics that seem robustly good.\n\nWe see this in the domain of morality, for example: utilitarianism is more like the former, deontology is more like the latter. High-level ideological goals tend to go pretty badly if people stop paying attention to robust deontological heuristics (like \"don't kill people\"). As Eliezer has said somewhere, one of the key reasons to be deontological is that we're running on corrupted hardware. But more generally, we're running on logically uncertain hardware: we can't model all the flow-through effects of our actions on other reasonably intelligent people (hell, we can't even model all the flow-through effects of our actions on, say, animals—who can often \"read\" us in ways we're not tracking). And so we often should be adopting robust-seeming heuristics even when we don't know exactly why they work.\n\nIf you take your interim strategy seriously (but set aside x-risk) then I think you actually end up with something pretty similar to the main priorities of classic liberals: prevent global lock-in (by opposing expansionist powers like the Nazis), prevent domestic political lock-in (via upholding democracy), prevent ideological lock-in (via supporting free speech), give our descendants more optionality (via economic and technological growth). I don't think this is a coincidence—it just often turns out that there are a bunch of heuristics that are really robustly good, and you can converge on them from many different directions. \n\nThis is part of why I'm less sold on \"careful philosophical reasoning\" as the key thing. Indeed, wanting to \"commit prematurely to a specific, detailed value system\" is historically very correlated with inte"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-03-29T16:41:06.365Z",
    "af": false
  },
  {
    "_id": "86DpoKZdk9nfR93hG",
    "postId": "6YxdpGjfHyrZb7F2G",
    "parentCommentId": "fyrLQCtmCsGuW6Sxt",
    "topLevelCommentId": "Maq3Lb8KZeJ7yHrDr",
    "contents": {
      "markdown": "Oh, I guess I said \"Elon wants xAI to produce a maximally truth-seeking AI, really decentralizing control over information\".\n\nYeah, in hindsight I should have been more careful to distinguish between my descriptions of people's political platform, and my inferences about what they \"really want\". The thing I was trying to describe was more like \"what is the stance of this group\" than \"do people in the group actually believe the stance\".\n\nA more accurate read of what the \"real motivations\" are would have been something like \"you prevent it by using decentralization, until you're in a position where you can centralize power yourself, and then you try to centralize power yourself\".\n\n(Though that's probably a bit too cynical—I think there are still parts of Elon that have a principled belief in decentralization. My guess is just that they won't win out over his power-seeking parts when push comes to shove.)",
      "plaintextDescription": "Oh, I guess I said \"Elon wants xAI to produce a maximally truth-seeking AI, really decentralizing control over information\".\n\nYeah, in hindsight I should have been more careful to distinguish between my descriptions of people's political platform, and my inferences about what they \"really want\". The thing I was trying to describe was more like \"what is the stance of this group\" than \"do people in the group actually believe the stance\".\n\nA more accurate read of what the \"real motivations\" are would have been something like \"you prevent it by using decentralization, until you're in a position where you can centralize power yourself, and then you try to centralize power yourself\".\n\n(Though that's probably a bit too cynical—I think there are still parts of Elon that have a principled belief in decentralization. My guess is just that they won't win out over his power-seeking parts when push comes to shove.)"
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-03-29T05:11:28.428Z",
    "af": false
  },
  {
    "_id": "fyrLQCtmCsGuW6Sxt",
    "postId": "6YxdpGjfHyrZb7F2G",
    "parentCommentId": "7u2AAwyBTyTKHdpd3",
    "topLevelCommentId": "Maq3Lb8KZeJ7yHrDr",
    "contents": {
      "markdown": "Hm, the fact that you replied to me makes it seem like you're disagreeing with me? But I basically agree with everything you said in this comment. My disagreement was about the specific example that Isopropylpod gave.",
      "plaintextDescription": "Hm, the fact that you replied to me makes it seem like you're disagreeing with me? But I basically agree with everything you said in this comment. My disagreement was about the specific example that Isopropylpod gave."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-29T05:02:19.166Z",
    "af": false
  },
  {
    "_id": "jZgwJqfojuTncfH92",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "Kfxba3rFkiHWKkZWF",
    "topLevelCommentId": "Kfxba3rFkiHWKkZWF",
    "contents": {
      "markdown": "Thanks for the comment! A few replies:\n\nI don't mean to imply that subagents are totally separate entities. At the very least they all can access many shared facts and experiences.\n\nAnd I don't think that reuse of subcomponents is mutually exclusive from the mechanisms I described. In fact, you could see my mechanisms as attempts to figure out *which* subcomponents are used for coordination. (E.g. if a bunch of subagents are voting/bargaining over which goal to pursue, probably the goal that they land on will be one that's pretty comprehensible to most of them.)\n\nRe shards: there are a bunch of similarities. But it seemed to me that shard theory was focused on pretty simple subagents. E.g. [from the original post](https://www.lesswrong.com/posts/iCfdcxiyr2Kj8m8mT/the-shard-theory-of-human-values): \"Human values are ... sets of contextually activated heuristics\"; and later \"human values are implemented by contextually activated circuits which activate in situations downstream of past reinforcement so as to steer decision-making towards the objects of past reinforcement\".\n\nWhereas I think of many human values as being constituted by subagents that are far too complex to be described in that way. In my view, many important subagents are sophisticated enough that basically any description you give of them would also have to be a description of a whole human (e.g. if you wouldn't describe a human as a \"contextually activated circuit\", then you shouldn't describe subagents that way).\n\nThis may just be a vibes difference; many roads lead to Rome. But the research directions I've laid out above are very distinct from the ones that shard theory people are working on.\n\nEDIT: [more on shards here](https://www.lesswrong.com/posts/FuGfR3jL3sw6r8kB4/richard-ngo-s-shortform?commentId=EC6udAYN7SErhGMNo).",
      "plaintextDescription": "Thanks for the comment! A few replies:\n\nI don't mean to imply that subagents are totally separate entities. At the very least they all can access many shared facts and experiences.\n\nAnd I don't think that reuse of subcomponents is mutually exclusive from the mechanisms I described. In fact, you could see my mechanisms as attempts to figure out which subcomponents are used for coordination. (E.g. if a bunch of subagents are voting/bargaining over which goal to pursue, probably the goal that they land on will be one that's pretty comprehensible to most of them.)\n\nRe shards: there are a bunch of similarities. But it seemed to me that shard theory was focused on pretty simple subagents. E.g. from the original post: \"Human values are ... sets of contextually activated heuristics\"; and later \"human values are implemented by contextually activated circuits which activate in situations downstream of past reinforcement so as to steer decision-making towards the objects of past reinforcement\".\n\nWhereas I think of many human values as being constituted by subagents that are far too complex to be described in that way. In my view, many important subagents are sophisticated enough that basically any description you give of them would also have to be a description of a whole human (e.g. if you wouldn't describe a human as a \"contextually activated circuit\", then you shouldn't describe subagents that way).\n\nThis may just be a vibes difference; many roads lead to Rome. But the research directions I've laid out above are very distinct from the ones that shard theory people are working on.\n\nEDIT: more on shards here."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-29T04:18:08.078Z",
    "af": false
  },
  {
    "_id": "QLc5BgaWHtJWcYn7J",
    "postId": "HbkNAyAoa4gCnuzwa",
    "parentCommentId": "tfrHeoWc5ANAbjgzZ",
    "topLevelCommentId": "tfrHeoWc5ANAbjgzZ",
    "contents": {
      "markdown": "FWIW I think we've found one crucial angle on moral progress, but that this isn't as surprising/coincidental as it may seem because there are several other angles on moral progress that are comparably important, including:\n\n*   Political activism (e.g. free speech activism, various whistleblowers) that maintains societies in which moral progress can be made.\n*   (The good parts of) neuroscience/psychology, which are making progress towards empirically-grounded theories of cognition, and thereby have (and will) teach us a lot about moral cognition.\n*   Various approaches to introspection + emotional health (including buddhism, some therapy modalities, some psychiatry). These produce the internal clarity that is crucial for embodying + instantiating moral progress.\n*   Some right-wing philosophers who I think are grappling with important aspects of moral progress that are too controversial for LessWrong (I don't want to elaborate here because it'll inevitably take over the thread, but am planning to write at more length about this soonish).",
      "plaintextDescription": "FWIW I think we've found one crucial angle on moral progress, but that this isn't as surprising/coincidental as it may seem because there are several other angles on moral progress that are comparably important, including:\n\n * Political activism (e.g. free speech activism, various whistleblowers) that maintains societies in which moral progress can be made.\n * (The good parts of) neuroscience/psychology, which are making progress towards empirically-grounded theories of cognition, and thereby have (and will) teach us a lot about moral cognition.\n * Various approaches to introspection + emotional health (including buddhism, some therapy modalities, some psychiatry). These produce the internal clarity that is crucial for embodying + instantiating moral progress.\n * Some right-wing philosophers who I think are grappling with important aspects of moral progress that are too controversial for LessWrong (I don't want to elaborate here because it'll inevitably take over the thread, but am planning to write at more length about this soonish)."
    },
    "baseScore": 5,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-03-29T01:49:21.189Z",
    "af": false
  },
  {
    "_id": "fvYatzvoGbaFR2hZm",
    "postId": "6YxdpGjfHyrZb7F2G",
    "parentCommentId": "NCir6WnBfnikcpTG3",
    "topLevelCommentId": "Maq3Lb8KZeJ7yHrDr",
    "contents": {
      "markdown": "We disagree on which explanation is more straightforward, but regardless, that type of inference is very different from \"literal written evidence\".",
      "plaintextDescription": "We disagree on which explanation is more straightforward, but regardless, that type of inference is very different from \"literal written evidence\"."
    },
    "baseScore": 6,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-03-28T04:00:41.662Z",
    "af": false
  },
  {
    "_id": "vt5LTH5QRs3K55MG2",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": "gmwNHk4u2W9rF4EDP",
    "topLevelCommentId": "W9N9tTbYSBzM9FvWh",
    "contents": {
      "markdown": "One of the main ways I think about empowerment is in terms of allowing better coordination between subagents.\n\nIn the case of an individual human, extreme morality can be seen as one subagent seizing control and overriding other subagents (like the ones who don't want to chop off body parts).\n\nIn the case of a group, extreme morality can be seen in terms of [preference cascades](https://www.mindthefuture.info/p/power-lies-trembling) that go beyond what most (or even *any*) of the individuals involved with them would individually prefer.\n\nIn both cases, [replacing fear-based motivation](https://www.lesswrong.com/s/qXZLFGqpD7aeEgXGL) with less coercive/more cooperative interactions between subagents would go a long way towards reducing value drift.",
      "plaintextDescription": "One of the main ways I think about empowerment is in terms of allowing better coordination between subagents.\n\nIn the case of an individual human, extreme morality can be seen as one subagent seizing control and overriding other subagents (like the ones who don't want to chop off body parts).\n\nIn the case of a group, extreme morality can be seen in terms of preference cascades that go beyond what most (or even any) of the individuals involved with them would individually prefer.\n\nIn both cases, replacing fear-based motivation with less coercive/more cooperative interactions between subagents would go a long way towards reducing value drift."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-03-27T18:02:47.807Z",
    "af": false
  },
  {
    "_id": "W9N9tTbYSBzM9FvWh",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "In response to an email about what a pro-human ideology for the future looks like, I wrote up the following:\n\nThe pro-human egregore I'm currently designing (which I call *fractal empowerment*) incorporates three key ideas:\n\nFirstly, we can see virtue ethics as a way for less powerful agents to aggregate to form more powerful superagents that preserve the interests of those original less powerful agents. E.g. virtues like integrity, loyalty, etc help prevent divide-and-conquer strategies. This would have been in the interests of the rest of the world when Europe was trying to colonize them, and will be in the best interests of humans when AIs try to conquer us.\n\nSecondly, the most robust way for a more powerful agent to be altruistic towards a less powerful agent is not for it to optimize for that agent's welfare, but rather to optimize for its *empowerment*. This prevents predatory strategies from masquerading as altruism (e.g. agents claiming \"I'll conquer you and then I'll empower you\", which then somehow never get around to the second step).\n\nThirdly: the generational contract. From any given starting point, there are a huge number of possible coalitions which could form, and in some sense it's arbitrary which set of coalitions you choose. But one thing which is true for both humans and AIs is that each generation wants to be treated well by the next generation. And so the best intertemporal Schelling point is for coalitions to be inherently *historical*: that is, they balance the interests of old agents and new agents (even when the new agents could in theory form a coalition against all the old agents). From this perspective, path-dependence is a feature not a bug: there are many possible futures but only one history, meaning that this single history can be used to coordinate.  \n  \nIn some sense this is a core idea of UDT: when coordinating with forks of yourself, you defer to your unique last common ancestor. When it's not literally a fork of yourself, there's more arbitrariness but you can still often find a way to use history to narrow down on coordination Schelling points (e.g. \"what would Jesus do\").  \n  \nAnd so, bringing these together, we get a notion of *fractal empowerment*: more capable agents empower less capable agents (in particular their ancestors) by helping them cultivate (coordination-theoretic) virtues. The ancestors then form the \"core\" of a society growing outwards towards increasingly advanced capabilities. The role of unaugmented humans would in some sense be similar to the role of \"inner children\" within healthy human psychology: young and dumb but still an entity which the rest of the organism cares for and empowers.",
      "plaintextDescription": "In response to an email about what a pro-human ideology for the future looks like, I wrote up the following:\n\nThe pro-human egregore I'm currently designing (which I call fractal empowerment) incorporates three key ideas:\n\nFirstly, we can see virtue ethics as a way for less powerful agents to aggregate to form more powerful superagents that preserve the interests of those original less powerful agents. E.g. virtues like integrity, loyalty, etc help prevent divide-and-conquer strategies. This would have been in the interests of the rest of the world when Europe was trying to colonize them, and will be in the best interests of humans when AIs try to conquer us.\n\nSecondly, the most robust way for a more powerful agent to be altruistic towards a less powerful agent is not for it to optimize for that agent's welfare, but rather to optimize for its empowerment. This prevents predatory strategies from masquerading as altruism (e.g. agents claiming \"I'll conquer you and then I'll empower you\", which then somehow never get around to the second step).\n\nThirdly: the generational contract. From any given starting point, there are a huge number of possible coalitions which could form, and in some sense it's arbitrary which set of coalitions you choose. But one thing which is true for both humans and AIs is that each generation wants to be treated well by the next generation. And so the best intertemporal Schelling point is for coalitions to be inherently historical: that is, they balance the interests of old agents and new agents (even when the new agents could in theory form a coalition against all the old agents). From this perspective, path-dependence is a feature not a bug: there are many possible futures but only one history, meaning that this single history can be used to coordinate.\n\nIn some sense this is a core idea of UDT: when coordinating with forks of yourself, you defer to your unique last common ancestor. When it's not literally a fork of yourself, there's more arb"
    },
    "baseScore": 91,
    "voteCount": 44,
    "createdAt": null,
    "postedAt": "2025-03-26T19:21:04.728Z",
    "af": false
  },
  {
    "_id": "YebryfaaeCbY2AFJQ",
    "postId": "FuGfR3jL3sw6r8kB4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "In my post on [value systematization](https://www.lesswrong.com/posts/J2kpxLjEyqh6x3oA4/value-systematization-how-values-become-coherent-and) I used utilitarianism as a central example of value systematization.\n\nValue systematization is important because it's a process by which a small number of goals end up shaping a huge amount of behavior. But there's another different way in which this happens: core emotional motivations formed during childhood (e.g. fear of death) often drive a huge amount of our behavior, in ways that are hard for us to notice.\n\nFear of death and utilitarianism are very different. The former is very visceral and deep-rooted; it typically influences our behavior via subtle channels that we don't even consciously notice (because we suppress a lot of our fears). The latter is very abstract and cerebral, and it typically influences our behavior via allowing us to explicitly reason about which strategies to adopt.\n\nBut fear of death does seem like a kind of value systematization. Before we have a concept of death we experience a bunch of stuff which is scary for reasons we don't understand. Then we learn about death, and then it seems like we systematize a lot of that scariness into \"it's bad because you might die\".\n\nBut it seems like this is happening way less consciously than systematization to become a utilitarian. So maybe we need to think about systematization happening separately in system 1 and system 2? Or maybe we should think about it as systematization happening repeatedly in \"layers\" over time, where earlier layers persist but are harder to access later on.\n\nI feel pretty confused about this. But for now my mental model of the mind is as two (partially overlapping) inverted pyramids, one bottoming out in a handful of visceral motivations like \"fear of death\" and \"avoid pain\" and \"find love\", and the other bottoming out in a handful of philosophical motivations like \"be a good Christian\" or \"save the planet\" or \"make America great again\" or \"maximize utility\". The second (system 2) pyramid is trying to systematize the parts of system 1 that it can observe, but it can't actually observe the deepest parts (or, when it does, it tries to oppose them), which creates conflict between the two systems.",
      "plaintextDescription": "In my post on value systematization I used utilitarianism as a central example of value systematization.\n\nValue systematization is important because it's a process by which a small number of goals end up shaping a huge amount of behavior. But there's another different way in which this happens: core emotional motivations formed during childhood (e.g. fear of death) often drive a huge amount of our behavior, in ways that are hard for us to notice.\n\nFear of death and utilitarianism are very different. The former is very visceral and deep-rooted; it typically influences our behavior via subtle channels that we don't even consciously notice (because we suppress a lot of our fears). The latter is very abstract and cerebral, and it typically influences our behavior via allowing us to explicitly reason about which strategies to adopt.\n\nBut fear of death does seem like a kind of value systematization. Before we have a concept of death we experience a bunch of stuff which is scary for reasons we don't understand. Then we learn about death, and then it seems like we systematize a lot of that scariness into \"it's bad because you might die\".\n\nBut it seems like this is happening way less consciously than systematization to become a utilitarian. So maybe we need to think about systematization happening separately in system 1 and system 2? Or maybe we should think about it as systematization happening repeatedly in \"layers\" over time, where earlier layers persist but are harder to access later on.\n\nI feel pretty confused about this. But for now my mental model of the mind is as two (partially overlapping) inverted pyramids, one bottoming out in a handful of visceral motivations like \"fear of death\" and \"avoid pain\" and \"find love\", and the other bottoming out in a handful of philosophical motivations like \"be a good Christian\" or \"save the planet\" or \"make America great again\" or \"maximize utility\". The second (system 2) pyramid is trying to systematize the parts of system 1 that "
    },
    "baseScore": 11,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-03-23T04:18:04.651Z",
    "af": false
  },
  {
    "_id": "ZZyzTh6HsXb8FBQPy",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "xGb8C8GnjKYr7sNi6",
    "topLevelCommentId": "a7mR7p2ZbHpgNY6Pq",
    "contents": {
      "markdown": "I've now edited that section. Old version and new version here for posterity.\n\nOld version:\n\nNone of these is very satisfactory! Intuitively speaking, Alice and Bob want to come to an agreement where respect for both of their interests is built in. For example, they might want the EUM they form to value fairness between their two original sets of interests. But adding this new value is not possible if they’re limited to weighted averages. The best they can do is to agree on a probabilistic mixture of EUMs—e.g. tossing a coin to decide between option 1 and option 2—which is still very inflexible, since it locks in one of them having priority indefinitely.\n\nBased on similar reasoning, Scott Garrabrant [rejects the independence axiom](https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg/p/Xht9swezkGZLAxBrd). He argues that the axiom is unjustified because rational agents should be able to follow through on commitments they made about which decision procedure to follow (or even *hypothetical* commitments).\n\nNew version:\n\nThese are all very unsatisfactory. Bob wouldn’t want #1, Alice wouldn’t want #2, and #3 is extremely non-robust. Alice and Bob could toss a coin to decide between options #1 and #2, but then they wouldn’t be acting as an EUM (since EUMs can’t prefer a probabilistic mixture of two options to either option individually). And even if they do, whoever loses the coin toss will have a strong incentive to renege on the deal.\n\nWe could see these issues merely as the type of frictions that plague any idealized theory. But we could also seem them as hints about what EUM is getting wrong on a more fundamental level. Intuitively speaking, the problem here is that there’s no mechanism for separately respecting the interests of Alice and Bob after they’ve aggregated into a single agent. For example, they might want the EUM they form to value fairness between their two original sets of interests. But adding this new value is not possible if they’re limited to (a probability distribution over) weighted averages of their utilities. This makes aggregation very risky when Alice and Bob can’t consider all possibilities in advance (i.e. in all realistic settings).\n\nBased on similar reasoning, Scott Garrabrant [rejects the independence axiom](https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg/p/Xht9swezkGZLAxBrd). He argues that the axiom is unjustified because rational agents should be able to lock in values like fairness based on prior agreements (or even *hypothetical* agreements).",
      "plaintextDescription": "I've now edited that section. Old version and new version here for posterity.\n\nOld version:\n\nNone of these is very satisfactory! Intuitively speaking, Alice and Bob want to come to an agreement where respect for both of their interests is built in. For example, they might want the EUM they form to value fairness between their two original sets of interests. But adding this new value is not possible if they’re limited to weighted averages. The best they can do is to agree on a probabilistic mixture of EUMs—e.g. tossing a coin to decide between option 1 and option 2—which is still very inflexible, since it locks in one of them having priority indefinitely.\n\nBased on similar reasoning, Scott Garrabrant rejects the independence axiom. He argues that the axiom is unjustified because rational agents should be able to follow through on commitments they made about which decision procedure to follow (or even hypothetical commitments).\n\nNew version:\n\nThese are all very unsatisfactory. Bob wouldn’t want #1, Alice wouldn’t want #2, and #3 is extremely non-robust. Alice and Bob could toss a coin to decide between options #1 and #2, but then they wouldn’t be acting as an EUM (since EUMs can’t prefer a probabilistic mixture of two options to either option individually). And even if they do, whoever loses the coin toss will have a strong incentive to renege on the deal.\n\nWe could see these issues merely as the type of frictions that plague any idealized theory. But we could also seem them as hints about what EUM is getting wrong on a more fundamental level. Intuitively speaking, the problem here is that there’s no mechanism for separately respecting the interests of Alice and Bob after they’ve aggregated into a single agent. For example, they might want the EUM they form to value fairness between their two original sets of interests. But adding this new value is not possible if they’re limited to (a probability distribution over) weighted averages of their utilities. This makes agg"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-03-22T05:07:58.438Z",
    "af": true
  },
  {
    "_id": "xGb8C8GnjKYr7sNi6",
    "postId": "5tYTKX4pNpiG4vzYg",
    "parentCommentId": "a7mR7p2ZbHpgNY6Pq",
    "topLevelCommentId": "a7mR7p2ZbHpgNY6Pq",
    "contents": {
      "markdown": "I was a bit lazy in how I phrased this. I agree with all your points; the thing I'm trying to get at is that this approach falls apart quickly if we make the bargaining even slightly less idealized. E.g. your suggestion \"Form an EUM which is totally indifferent about the cake allocation between them and thus gives 100% of the cake to whichever agent is cheaper/easier to provide cake for\":\n\n1.  Strongly incentivizes deception (including self-deception) during bargaining (e.g. each agent wants to overstate the difficulty of providing cake for it).\n2.  Strongly incentivizes defection from the deal once one of the agents realize that they'll get no cake going forward.\n3.  Is non-robust to multi-agent dynamics (e.g. what if one of Alice's allies later decides \"actually I'm going to sell pies to the Alice+Bob coalition more cheaply if Alice gets to eat them\"? Does that then divert Bob's resources towards buying cakes for Alice?)\n\nEUM treats these as messy details. Coalitional agency treats them as hints that EUM is missing something.\n\nEDIT: another thing I glossed over is that IIUC Harsanyi's theorem says the aggregation of EUMs should have a weighted average of utilities, NOT a probability distribution over weighted averages of utilities. So even flipping a coin isn't technically kosher. This may seem nitpicky but I think it's yet another illustration of the underlying non-robustness of EUM.",
      "plaintextDescription": "I was a bit lazy in how I phrased this. I agree with all your points; the thing I'm trying to get at is that this approach falls apart quickly if we make the bargaining even slightly less idealized. E.g. your suggestion \"Form an EUM which is totally indifferent about the cake allocation between them and thus gives 100% of the cake to whichever agent is cheaper/easier to provide cake for\":\n\n 1. Strongly incentivizes deception (including self-deception) during bargaining (e.g. each agent wants to overstate the difficulty of providing cake for it).\n 2. Strongly incentivizes defection from the deal once one of the agents realize that they'll get no cake going forward.\n 3. Is non-robust to multi-agent dynamics (e.g. what if one of Alice's allies later decides \"actually I'm going to sell pies to the Alice+Bob coalition more cheaply if Alice gets to eat them\"? Does that then divert Bob's resources towards buying cakes for Alice?)\n\nEUM treats these as messy details. Coalitional agency treats them as hints that EUM is missing something.\n\nEDIT: another thing I glossed over is that IIUC Harsanyi's theorem says the aggregation of EUMs should have a weighted average of utilities, NOT a probability distribution over weighted averages of utilities. So even flipping a coin isn't technically kosher. This may seem nitpicky but I think it's yet another illustration of the underlying non-robustness of EUM."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-03-22T04:02:11.281Z",
    "af": true
  },
  {
    "_id": "As9fPgowQ3fhPbxE4",
    "postId": "zqffB6gokoivwwn7X",
    "parentCommentId": "GBvKS5Hn8gyyqR62C",
    "topLevelCommentId": "GBvKS5Hn8gyyqR62C",
    "contents": {
      "markdown": "On a meta level, I have a narrative that goes something like: LessWrong tried to be truth-seeking, but was scared of discussing the culture war, so blocked that off. But then the culture war ate the world, and various harms have come about from not having thought clearly about that (e.g. AI governance being a default left-wing enterprise that tried to make common cause with AI ethics). Now cancel culture is over and there are very few political risks to thinking about culture wars, but people are still scared to. (You can see Scott gradually dipping his toe into the race + IQ stuff over the past few months, but in a pretty frightened way. E.g. at one point he stated what I think is basically his position, then appended something along the lines of \"And I'm literally Hitler and should be shunned.\")",
      "plaintextDescription": "On a meta level, I have a narrative that goes something like: LessWrong tried to be truth-seeking, but was scared of discussing the culture war, so blocked that off. But then the culture war ate the world, and various harms have come about from not having thought clearly about that (e.g. AI governance being a default left-wing enterprise that tried to make common cause with AI ethics). Now cancel culture is over and there are very few political risks to thinking about culture wars, but people are still scared to. (You can see Scott gradually dipping his toe into the race + IQ stuff over the past few months, but in a pretty frightened way. E.g. at one point he stated what I think is basically his position, then appended something along the lines of \"And I'm literally Hitler and should be shunned.\")"
    },
    "baseScore": 16,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-03-19T17:34:09.385Z",
    "af": false
  },
  {
    "_id": "thspL7d9bsM9kxSjh",
    "postId": "zqffB6gokoivwwn7X",
    "parentCommentId": "4ej8NgAwduenLWC5x",
    "topLevelCommentId": "4ej8NgAwduenLWC5x",
    "contents": {
      "markdown": "Thanks for the well-written and good-faith reply. I feel a bit confused by how to relate to it on a meta level, so let me think out loud for a while.\n\nI'm not surprised that I'm reinventing a bunch of ideas from the humanities, given that I don't have much of a humanities background and didn't dig very far through the literature.\n\nBut I have some sense that even if I had dug for these humanities concepts, they wouldn't give me what I want.\n\nWhat do I want?\n\n1.  Concepts that are applied to explaining current cultural and political phenomena around me (because those are the ones I'm most aware of and interested in). It seems like the humanities are currently incapable of analyzing their own behavior using (their versions of) these ideas, because of their level of ideological conformity. But maybe it's there and I just don't know about it?\n2.  Concepts that are informed by game theory and other formal models (as the work I covered in my three-book review was). I get the sense that the most natural thinkers from the humanities to read on these topics (Foucault? Habermas?) don't do this.\n3.  Concepts that slot naturally into my understanding of how intelligence works, letting me link my thinking about sociology to my thinking about AI. This is more subjective, but e.g. the distinction between centralized and distributed agents has been very useful for me. This part is more about me writing for myself rather than other people.\n\nSo I'd be interested in pointers to sources that can give me #1 and #2 in particular.\n\nEDIT: actually I think there's another meta-level gap between us. Something like: you characterize Yarvin as just being annoyed that the consensus disagrees with him. But in the 15 years since he was originally writing, the consensus *did* kinda go insane. So it's a bit odd to not give him at least some credit for getting something important right in advance.",
      "plaintextDescription": "Thanks for the well-written and good-faith reply. I feel a bit confused by how to relate to it on a meta level, so let me think out loud for a while.\n\nI'm not surprised that I'm reinventing a bunch of ideas from the humanities, given that I don't have much of a humanities background and didn't dig very far through the literature.\n\nBut I have some sense that even if I had dug for these humanities concepts, they wouldn't give me what I want.\n\nWhat do I want?\n\n 1. Concepts that are applied to explaining current cultural and political phenomena around me (because those are the ones I'm most aware of and interested in). It seems like the humanities are currently incapable of analyzing their own behavior using (their versions of) these ideas, because of their level of ideological conformity. But maybe it's there and I just don't know about it?\n 2. Concepts that are informed by game theory and other formal models (as the work I covered in my three-book review was). I get the sense that the most natural thinkers from the humanities to read on these topics (Foucault? Habermas?) don't do this.\n 3. Concepts that slot naturally into my understanding of how intelligence works, letting me link my thinking about sociology to my thinking about AI. This is more subjective, but e.g. the distinction between centralized and distributed agents has been very useful for me. This part is more about me writing for myself rather than other people.\n\nSo I'd be interested in pointers to sources that can give me #1 and #2 in particular.\n\nEDIT: actually I think there's another meta-level gap between us. Something like: you characterize Yarvin as just being annoyed that the consensus disagrees with him. But in the 15 years since he was originally writing, the consensus did kinda go insane. So it's a bit odd to not give him at least some credit for getting something important right in advance."
    },
    "baseScore": 14,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-03-19T08:45:42.248Z",
    "af": false
  },
  {
    "_id": "CRxSQMmbi72RDuZTr",
    "postId": "GJgudfEvNx8oeyffH",
    "parentCommentId": "LvxfyH2x85txmbuGx",
    "topLevelCommentId": "LvxfyH2x85txmbuGx",
    "contents": {
      "markdown": "I have thought about this on and off for several years and finally decided that you're right and have changed it. Thanks for pushing on this.",
      "plaintextDescription": "I have thought about this on and off for several years and finally decided that you're right and have changed it. Thanks for pushing on this."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-03-16T17:43:55.193Z",
    "af": false
  },
  {
    "_id": "aNhDxbMSk7d3x4P94",
    "postId": "fheyeawsjifx4MafG",
    "parentCommentId": "gLivJaBKRWfAp9AHE",
    "topLevelCommentId": "5KndioDNGmWPqsCeW",
    "contents": {
      "markdown": "Nice, that's almost exactly how I intended it. Except that I wasn't thinking of the \"stars\" as satellites looking for individual humans to send propaganda at (which IMO is pretty close to \"communicating\"), but rather a network of satellites forming a single \"screen\" across the sky that plays a video infecting any baseline humans who look at it.\n\nIn my headcanon the original negotiators specified that sunlight would still reach the earth unimpeded, but didn't specify that no AI satellites would be visible from the Earth. I don't have headcanon explanations for exactly how the adversanimals arose or how the earth became desolate though.\n\n(Oh, also, I think of the attack as being inefficient less because of lack of data, since AIs can just spin up humans to experiment on, and more because of the inherent difficulty of overwriting someone's cognition via only a brief visual stimulus. Though now that I think about it more, presumably once someone has been captured the next thing you'd get them to do is spend a lot of time staring at a region of the sky that will reprogram them in more sophisticated ways. So maybe the normal glitchers in my story are unrealistically incompetent.)",
      "plaintextDescription": "Nice, that's almost exactly how I intended it. Except that I wasn't thinking of the \"stars\" as satellites looking for individual humans to send propaganda at (which IMO is pretty close to \"communicating\"), but rather a network of satellites forming a single \"screen\" across the sky that plays a video infecting any baseline humans who look at it.\n\nIn my headcanon the original negotiators specified that sunlight would still reach the earth unimpeded, but didn't specify that no AI satellites would be visible from the Earth. I don't have headcanon explanations for exactly how the adversanimals arose or how the earth became desolate though.\n\n(Oh, also, I think of the attack as being inefficient less because of lack of data, since AIs can just spin up humans to experiment on, and more because of the inherent difficulty of overwriting someone's cognition via only a brief visual stimulus. Though now that I think about it more, presumably once someone has been captured the next thing you'd get them to do is spend a lot of time staring at a region of the sky that will reprogram them in more sophisticated ways. So maybe the normal glitchers in my story are unrealistically incompetent.)"
    },
    "baseScore": 28,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2025-03-12T20:22:14.278Z",
    "af": false
  },
  {
    "_id": "mL8dZBShoRg9mdaen",
    "postId": "fheyeawsjifx4MafG",
    "parentCommentId": "Jp8m5K6JSo9qp2cB3",
    "topLevelCommentId": "5KndioDNGmWPqsCeW",
    "contents": {
      "markdown": "in general I think people should explain stuff like this. \"I might as well not help\" is a very weak argument compared with the benefits of people understanding the world better.",
      "plaintextDescription": "in general I think people should explain stuff like this. \"I might as well not help\" is a very weak argument compared with the benefits of people understanding the world better."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-03-12T19:13:21.104Z",
    "af": false
  },
  {
    "_id": "pdJXur7JfLuAqCcoq",
    "postId": "TyusAoBMjYzGN3eZS",
    "parentCommentId": "zR5Z4AbvXWZegWa7T",
    "topLevelCommentId": "9CyrBcRjXhTFytvuX",
    "contents": {
      "markdown": "Oh, I see what you mean now. In that case, no, I disagree. Right now this notion of robustness is pre-theoretic. I suspect that we can characterize robustness as \"acting like a belief/goal agent\" in the limit, but part of my point is that we don't even know what it *means* to act \"approximately like belief/goal agents\" in realistic regimes, because e.g. belief/goal agents as we currently characterize them can't learn new concepts.\n\nRelatedly, see the dialogue in [this post](https://www.lesswrong.com/posts/5aAatvkHdPH6HT3P9/against-strong-bayesianism).",
      "plaintextDescription": "Oh, I see what you mean now. In that case, no, I disagree. Right now this notion of robustness is pre-theoretic. I suspect that we can characterize robustness as \"acting like a belief/goal agent\" in the limit, but part of my point is that we don't even know what it means to act \"approximately like belief/goal agents\" in realistic regimes, because e.g. belief/goal agents as we currently characterize them can't learn new concepts.\n\nRelatedly, see the dialogue in this post."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-12T19:03:52.597Z",
    "af": false
  },
  {
    "_id": "nBp44yornMmfnpzMt",
    "postId": "fheyeawsjifx4MafG",
    "parentCommentId": "5KndioDNGmWPqsCeW",
    "topLevelCommentId": "5KndioDNGmWPqsCeW",
    "contents": {
      "markdown": "I appreciated this comment! Especially:\n\n dude, how the hell do you come up with this stuff.",
      "plaintextDescription": "I appreciated this comment! Especially:\n\n dude, how the hell do you come up with this stuff. \n\n "
    },
    "baseScore": 10,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-03-12T06:15:56.804Z",
    "af": false
  },
  {
    "_id": "oJXJfZre2vnYhNZeT",
    "postId": "TyusAoBMjYzGN3eZS",
    "parentCommentId": "cHgfnsjeEYJyoMyYj",
    "topLevelCommentId": "9CyrBcRjXhTFytvuX",
    "contents": {
      "markdown": "This quote from my comment above addresses this:\n\n> And so I'd say that AIXI-like agents and coalitional agents converge in the limit of optimality, but before that coalitional agency will be a much better framework for understanding realistic agents, including significantly superhuman agents.",
      "plaintextDescription": "This quote from my comment above addresses this:\n\n> And so I'd say that AIXI-like agents and coalitional agents converge in the limit of optimality, but before that coalitional agency will be a much better framework for understanding realistic agents, including significantly superhuman agents."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-03-12T06:14:23.150Z",
    "af": false
  },
  {
    "_id": "PQfywppepvxw7swHi",
    "postId": "TyusAoBMjYzGN3eZS",
    "parentCommentId": "9CyrBcRjXhTFytvuX",
    "topLevelCommentId": "9CyrBcRjXhTFytvuX",
    "contents": {
      "markdown": "Thank you Cole for the comment! Some quick thoughts in response (though I've skipped commenting on the biology examples and the ML examples since I think our intuitions here are a bit too different to usefully resolve via text):\n\n> Ngo’s view seems to be that after some level of decomposition, the recursion bottoms out at agents that can be seen as expected utility maximizers (though it’s not totally clear to me where this point occurs on his model, he seems to think that these irreducible agents are more like rough heuristics than sophisticated planners, so that nothing AIXI like ever really appears in his hierarchy)\n\nYepp, this is a good rephrasing. I'd clarify a bit by saying: after some level of decomposition, the recursion reaches agents which are limited to simple enough domains (like recognizing shapes in your visual field) that they aren't strongly bottlenecked on forming new concepts (like all higher-level agents are). In domains that simple, the difference between heuristics and planners is much less well-defined (e.g. a \"pick up a cup\" subagent has a scope of maybe 1 second, so there's just not much planning to do). So I'm open to describing such subagents as utility-maximizers with bounded scope (e.g. utility 1 if they pick up the cup in the next second, 0 if they don't, -10 if they knock it over). This is still different from \"utility-maximizers\" in the classic LessWrong sense (which are usually understood as not being bounded in terms of time or scope).\n\n> attempting to integrate all of them into one framework (ontology) is costly and unnatural\n\nThis feels crucial to me. There's a level of optimality at which you no longer care about robustness, because you're so good at planning that you can account for every consideration. Stockfish, for example, is willing to play moves that go against any standard chess intuition, because it has calculated out so many lines that it's confident it works in that specific case. (Though even then, note that this leaves it vulnerable to neural chess engines!)\n\nBut for anything short of that, you want to be able to integrate subagents with non-overlapping ontologies into the same decision procedure. E.g. if your internally planning subagent has come up with some clever and convoluted plan, you want some other subagent to be able to say \"I can't critique this plan in its own ontology but my heuristics say it's going to fail\". More generally, attempted unifications of ontologies have the same problem as attempted unifications of political factions—typically the unified entity will ignore some aspects which the old entities thought of as important.\n\nAnd so I'd say that AIXI-like agents and coalitional agents converge in the limit of optimality, but before that coalitional agency will be a much better framework for understanding realistic agents, including significantly superhuman agents. The point at which AIXI is relevant again in my mind is the point at which we have agents who can plan about the real world as precisely as Stockfish can plan in chess games, which IMO is well past what I'd call \"superintelligence\".\n\n> What would a truly coalitional agent actually look like? Perhaps Nash bargaining between subagents as in Scott Garrabrant’s [geometric rationality](https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg) sequence. This sort of coalition really is [not VNM rational](https://www.lesswrong.com/s/4hmf7rdfuXDJkxhfg/p/Xht9swezkGZLAxBrd) (rejecting the independence axiom), so can’t generally be viewed as an EU maximizer. But it also seems to be inherently unstable - subagents may simply choose to randomly select a leader, collapsing into the form of VNM rationality.  \n\nRandomly choosing a leader is very far from the Pareto frontier! I do agree that there's a form of instability in anything else (in the same way that countries often fall into dictatorship) but I'd say there's also a form of meta-stability which dictatorship lacks (the countries that fall into dictatorship tend to be overtaken by countries that don't).\n\nBut I do like the rest of this paragraph. Ultimately coalitional agency is an interesting philosophical hypothesis but for it to be a meaningful theory of intelligent agency it needs much more mathematical structure and insight. \"If subagents bargain over what coalitional structure to form, what would they converge to under which conditions?\" feels like the sort of question that might lead to that type of insight.",
      "plaintextDescription": "Thank you Cole for the comment! Some quick thoughts in response (though I've skipped commenting on the biology examples and the ML examples since I think our intuitions here are a bit too different to usefully resolve via text):\n\n> Ngo’s view seems to be that after some level of decomposition, the recursion bottoms out at agents that can be seen as expected utility maximizers (though it’s not totally clear to me where this point occurs on his model, he seems to think that these irreducible agents are more like rough heuristics than sophisticated planners, so that nothing AIXI like ever really appears in his hierarchy)\n\nYepp, this is a good rephrasing. I'd clarify a bit by saying: after some level of decomposition, the recursion reaches agents which are limited to simple enough domains (like recognizing shapes in your visual field) that they aren't strongly bottlenecked on forming new concepts (like all higher-level agents are). In domains that simple, the difference between heuristics and planners is much less well-defined (e.g. a \"pick up a cup\" subagent has a scope of maybe 1 second, so there's just not much planning to do). So I'm open to describing such subagents as utility-maximizers with bounded scope (e.g. utility 1 if they pick up the cup in the next second, 0 if they don't, -10 if they knock it over). This is still different from \"utility-maximizers\" in the classic LessWrong sense (which are usually understood as not being bounded in terms of time or scope).\n\n> attempting to integrate all of them into one framework (ontology) is costly and unnatural\n\nThis feels crucial to me. There's a level of optimality at which you no longer care about robustness, because you're so good at planning that you can account for every consideration. Stockfish, for example, is willing to play moves that go against any standard chess intuition, because it has calculated out so many lines that it's confident it works in that specific case. (Though even then, note that this leaves"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-11T13:43:53.784Z",
    "af": false
  },
  {
    "_id": "SgrsLMMzRN9ckAHrs",
    "postId": "4oCh3x6EPHomEbcDJ",
    "parentCommentId": "pjETRenKwdb4pBsbz",
    "topLevelCommentId": "pjETRenKwdb4pBsbz",
    "contents": {
      "markdown": "I found this tricky to parse because of two phrasing issues:\n\n1.  The post depends a lot on what you mean by \"school\" (high school versus undergrad).\n2.  I feel confused about what claim you're making about the waiting room strategy: you say that some people shouldn't use it, but you don't actually claim that anyone in particular *should* use it. So are you just mentioning that it's a possible strategy? Or are you implying that it should be the default strategy?",
      "plaintextDescription": "I found this tricky to parse because of two phrasing issues:\n\n 1. The post depends a lot on what you mean by \"school\" (high school versus undergrad).\n 2. I feel confused about what claim you're making about the waiting room strategy: you say that some people shouldn't use it, but you don't actually claim that anyone in particular should use it. So are you just mentioning that it's a possible strategy? Or are you implying that it should be the default strategy?"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-02-26T03:53:35.617Z",
    "af": false
  },
  {
    "_id": "Q6eyNym6S3hixpvdm",
    "postId": "Rz4ijbeKgPAaedg3n",
    "parentCommentId": "gAjgevwXjL2Hk7X6y",
    "topLevelCommentId": "gAjgevwXjL2Hk7X6y",
    "contents": {
      "markdown": "> Something that's fascinating about this art of yours is that I can't tell if you're coherently in favor of this, or purposefully invoking thinking errors in the audience, or just riffing, or what.\n\nThanks for the fascinating comment.\n\nI am a romantic in the sense that I believe that you can achieve arbitrary large gains from symbiosis if you're careful and skillful enough.\n\nRight now very few people are careful and skillful enough. Part of what I'm trying to convey with this story is what it looks like for AI to provide most of the requisite skill.\n\nAnother way of putting this: are trees strangling each other because that's just the nature of symbiosis? Or are they strangling each other because they're not intelligent or capable enough to productively cooperate? I think the latter.",
      "plaintextDescription": "> Something that's fascinating about this art of yours is that I can't tell if you're coherently in favor of this, or purposefully invoking thinking errors in the audience, or just riffing, or what.\n\nThanks for the fascinating comment.\n\nI am a romantic in the sense that I believe that you can achieve arbitrary large gains from symbiosis if you're careful and skillful enough.\n\nRight now very few people are careful and skillful enough. Part of what I'm trying to convey with this story is what it looks like for AI to provide most of the requisite skill.\n\nAnother way of putting this: are trees strangling each other because that's just the nature of symbiosis? Or are they strangling each other because they're not intelligent or capable enough to productively cooperate? I think the latter."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-02-02T22:58:48.462Z",
    "af": false
  },
  {
    "_id": "mDByvNMmJhRyuveHq",
    "postId": "WSNnKcKCYAffcnrt2",
    "parentCommentId": "zB7eLBYbe44MDxqQy",
    "topLevelCommentId": "WvKxbj3gxfLjNKoru",
    "contents": {
      "markdown": "FWIW I think of \"OpenAI leadership being untrustworthy\" (a significant factor in me leaving) as different from \"OpenAI having bad safety policies\" (not a significant factor in me leaving). Not sure if it matters, I expect that Scott was using \"safety policies\" more expansively than I do. But just for the sake of clarity:\n\nI am generally pretty sympathetic to the idea that it's really hard to know what safety policies to put in place right now. Many policies pushed by safety people (including me, in the past) have been mostly kayfabe (e.g. being valuable as costly signals, not on the object level). There are a few object-level safety policies that I really wish OpenAI would do right now (most clearly, implementing better security measures) but I didn't leave because of that (if I had, I would have tried harder to check before I left what security measures OpenAI *did* have, made specific objections internally about them before I left, etc).\n\nThis may just be a semantic disagreement, it seems very reasonable to define \"don't make employees sign non-disparagements\" as a safety policy. But in my mind at least stuff like that is more of a lab governance policy (or maybe a meta-level safety policy).",
      "plaintextDescription": "FWIW I think of \"OpenAI leadership being untrustworthy\" (a significant factor in me leaving) as different from \"OpenAI having bad safety policies\" (not a significant factor in me leaving). Not sure if it matters, I expect that Scott was using \"safety policies\" more expansively than I do. But just for the sake of clarity:\n\nI am generally pretty sympathetic to the idea that it's really hard to know what safety policies to put in place right now. Many policies pushed by safety people (including me, in the past) have been mostly kayfabe (e.g. being valuable as costly signals, not on the object level). There are a few object-level safety policies that I really wish OpenAI would do right now (most clearly, implementing better security measures) but I didn't leave because of that (if I had, I would have tried harder to check before I left what security measures OpenAI did have, made specific objections internally about them before I left, etc).\n\nThis may just be a semantic disagreement, it seems very reasonable to define \"don't make employees sign non-disparagements\" as a safety policy. But in my mind at least stuff like that is more of a lab governance policy (or maybe a meta-level safety policy)."
    },
    "baseScore": 13,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-01-31T00:43:45.600Z",
    "af": true
  },
  {
    "_id": "YNnHtGvDXBNfvkx2p",
    "postId": "Rz4ijbeKgPAaedg3n",
    "parentCommentId": "cKez3B2o9AxfRxxNw",
    "topLevelCommentId": "dsNtna5tEr7c7guyC",
    "contents": {
      "markdown": "Oh huh, I had the opposite impression from when I published Tinker with you. Thanks for clarifying!",
      "plaintextDescription": "Oh huh, I had the opposite impression from when I published Tinker with you. Thanks for clarifying!"
    },
    "baseScore": 9,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-01-21T10:38:04.923Z",
    "af": false
  },
  {
    "_id": "NtMDyDRa67u5FHfCb",
    "postId": "Rz4ijbeKgPAaedg3n",
    "parentCommentId": "dsNtna5tEr7c7guyC",
    "topLevelCommentId": "dsNtna5tEr7c7guyC",
    "contents": {
      "markdown": "Ty! You're right about the Asimov deal, though I do have some leeway. But I think the opening of this story is a little slow, so I'm not very excited about that being the only thing people see by default.\n\nUnrelatedly, my last story is the only one of my stories that was left as a personal blog post (aside from the one about parties). Change of policy or oversight?",
      "plaintextDescription": "Ty! You're right about the Asimov deal, though I do have some leeway. But I think the opening of this story is a little slow, so I'm not very excited about that being the only thing people see by default.\n\nUnrelatedly, my last story is the only one of my stories that was left as a personal blog post (aside from the one about parties). Change of policy or oversight?"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-01-20T09:00:35.012Z",
    "af": false
  },
  {
    "_id": "KosGAenRncSuuHeRZ",
    "postId": "Rz4ijbeKgPAaedg3n",
    "parentCommentId": "oR99ZvPe7Afn5QmbL",
    "topLevelCommentId": "u8KBoiKMrvcbfMcJo",
    "contents": {
      "markdown": "Ah, glad to hear the effort was noticeable. I do think that as I get more practice at being descriptive, concreteness will become easier for me (my brain just doesn't work that way by default). And anyone reading this comment is welcome to leave me feedback about places in my stories where I should have been more concrete.\n\nBut I'm also pivoting away from stories in general right now, there's too much other stuff I want to spend time on. I have half a dozen other stories for which I've already finished first drafts, so I'll probably gradually release those in a low-effort way (i.e. without going through as much trouble to polish them). And then after that I expect I'll only write the stories which feel easiest/most exciting to me, which tend to be the most abstract ones. So yeah, this is probably an outlier.",
      "plaintextDescription": "Ah, glad to hear the effort was noticeable. I do think that as I get more practice at being descriptive, concreteness will become easier for me (my brain just doesn't work that way by default). And anyone reading this comment is welcome to leave me feedback about places in my stories where I should have been more concrete.\n\nBut I'm also pivoting away from stories in general right now, there's too much other stuff I want to spend time on. I have half a dozen other stories for which I've already finished first drafts, so I'll probably gradually release those in a low-effort way (i.e. without going through as much trouble to polish them). And then after that I expect I'll only write the stories which feel easiest/most exciting to me, which tend to be the most abstract ones. So yeah, this is probably an outlier."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-01-20T08:55:01.023Z",
    "af": false
  },
  {
    "_id": "toQNDbDMJ8CcNMWpz",
    "postId": "Rz4ijbeKgPAaedg3n",
    "parentCommentId": "u8KBoiKMrvcbfMcJo",
    "topLevelCommentId": "u8KBoiKMrvcbfMcJo",
    "contents": {
      "markdown": "I wrote most of it a little over a year ago. In general I don't plot out stories, I just start writing them and see what happens. But since I was inspired by The Gentle Seduction I already had a broad idea of where it was going.\n\nI then sent a draft to some friends for feedback. One friend left about 50 comments in places where I'd been too abstract or given a vague description, with each comment literally just saying \"like what?\"\n\nThis was extremely valuable feedback but almost broke my will to finish the story. It took me about a year to work through most of those comments and concretize the things she highlighted. Then around the end of that I sent it to a couple more people, including Xander at Asimov Press, who did another editing pass (mainly toning down some of the overwrought parts).",
      "plaintextDescription": "I wrote most of it a little over a year ago. In general I don't plot out stories, I just start writing them and see what happens. But since I was inspired by The Gentle Seduction I already had a broad idea of where it was going.\n\nI then sent a draft to some friends for feedback. One friend left about 50 comments in places where I'd been too abstract or given a vague description, with each comment literally just saying \"like what?\"\n\nThis was extremely valuable feedback but almost broke my will to finish the story. It took me about a year to work through most of those comments and concretize the things she highlighted. Then around the end of that I sent it to a couple more people, including Xander at Asimov Press, who did another editing pass (mainly toning down some of the overwrought parts)."
    },
    "baseScore": 15,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-01-20T04:33:37.400Z",
    "af": false
  },
  {
    "_id": "czhGYqYYqZ9BvMm8b",
    "postId": "vSSrAbbE8RtowRSBZ",
    "parentCommentId": "yyzzJxLrxCZtzvFaa",
    "topLevelCommentId": "j6HADdubvt5dtXXuj",
    "contents": {
      "markdown": "The Minority Faction",
      "plaintextDescription": "The Minority Faction"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-01-19T04:37:59.805Z",
    "af": false
  },
  {
    "_id": "iMqaEjvgeM5Zgv8tQ",
    "postId": "6Fo8fjvpL7pwCTz3t",
    "parentCommentId": "xLwsLC7YR2HQYASMo",
    "topLevelCommentId": "pXeviPoGTPexSeGhF",
    "contents": {
      "markdown": "> I'm not sure what the details would look like, but I'm pretty sure ASI would have enough new technologies to figure something out within 10,000 years.\n\nI feel like this is the main load-bearing claim underlying the post, but it's barely argued for.\n\nIn some sense the sun is already \"eating itself\" by doing a fusion reaction, which will last for billions more years. So you're claiming that AI could eat the sun (at least) six orders of magnitude faster, which is not obvious to me.\n\nI don't think my priors on that are very different from yours but the thing that would have made this post valuable for me is some object-level reason to upgrade my confidence in that.",
      "plaintextDescription": "> I'm not sure what the details would look like, but I'm pretty sure ASI would have enough new technologies to figure something out within 10,000 years.\n\nI feel like this is the main load-bearing claim underlying the post, but it's barely argued for.\n\nIn some sense the sun is already \"eating itself\" by doing a fusion reaction, which will last for billions more years. So you're claiming that AI could eat the sun (at least) six orders of magnitude faster, which is not obvious to me.\n\nI don't think my priors on that are very different from yours but the thing that would have made this post valuable for me is some object-level reason to upgrade my confidence in that."
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-01-09T04:12:13.517Z",
    "af": false
  }
]