[
  {
    "_id": "GtjQorw5e6qfNSCi9",
    "postId": "SeevvpYjnAsLuu3KK",
    "parentCommentId": "FZnvuJJjLGvNgxDbD",
    "topLevelCommentId": "FZnvuJJjLGvNgxDbD",
    "contents": {
      "markdown": "Meta is not on that list of \"frontier AI\" companies because it hasn't kept up. As far as I know its most advanced model is Llama 4 and that's not on the same level as GPT-5, Gemini, Grok, or Claude. Not only has it been left behind by the pivot to reasoning models; Meta's special strength was supposed to be open source, but even there, Chinese models from Moonshot (Kimi K2) and DeepSeek (r2, v3) seem to be ahead. Of course Meta is now trying to get back in the game, but for now they have slipped out of contention. \n\nThe remaining question I have concerns the true strength of Chinese AI models, with respect to each other and their American rivals. You could turn my previous paragraph into a thesis about the state of the world: it's the era of reasoning models, and at the helm are four closed-weight American models and two open-weight Chinese models. But what about Baidu's Ernie, Alibaba's Qwen, Zhipu's ChatGLM? Should they be placed in the first tier as well?",
      "plaintextDescription": "Meta is not on that list of \"frontier AI\" companies because it hasn't kept up. As far as I know its most advanced model is Llama 4 and that's not on the same level as GPT-5, Gemini, Grok, or Claude. Not only has it been left behind by the pivot to reasoning models; Meta's special strength was supposed to be open source, but even there, Chinese models from Moonshot (Kimi K2) and DeepSeek (r2, v3) seem to be ahead. Of course Meta is now trying to get back in the game, but for now they have slipped out of contention. \n\nThe remaining question I have concerns the true strength of Chinese AI models, with respect to each other and their American rivals. You could turn my previous paragraph into a thesis about the state of the world: it's the era of reasoning models, and at the helm are four closed-weight American models and two open-weight Chinese models. But what about Baidu's Ernie, Alibaba's Qwen, Zhipu's ChatGLM? Should they be placed in the first tier as well? "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-25T19:31:36.210Z",
    "af": false
  },
  {
    "_id": "WeK33YvwmuW3kZg6u",
    "postId": "T2DBrHY2nBNDMvDzR",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "You could be a longtermist and still regard a singleton as the most likely outcome. It would just mean that a human-aligned singleton is the only real chance for a human-aligned long-term future, and so you'd better make that your priority, however unlikely it may be. It's apparent that a lot of the old-school (pre-LLM) AI-safety people think this way, when they talk about the fate of Earth's future lightcone and so forth. \n\nHowever, I'm not familiar with the balance of priorities espoused by actual self-identified longtermists. Do they typically treat a singleton as just a possibility rather than an inevitability?",
      "plaintextDescription": "You could be a longtermist and still regard a singleton as the most likely outcome. It would just mean that a human-aligned singleton is the only real chance for a human-aligned long-term future, and so you'd better make that your priority, however unlikely it may be. It's apparent that a lot of the old-school (pre-LLM) AI-safety people think this way, when they talk about the fate of Earth's future lightcone and so forth. \n\nHowever, I'm not familiar with the balance of priorities espoused by actual self-identified longtermists. Do they typically treat a singleton as just a possibility rather than an inevitability? "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-25T07:40:35.728Z",
    "af": false
  },
  {
    "_id": "hsdh9qrjsv7dwfjwr",
    "postId": "TS6bTEA9YMfPZDiKC",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "If I understand correctly, your chief proposition is that liberal rationalists who are shocked and appalled by Trump 2.0 should check out the leftists who actually predicted that Trump 2.0 would be shocking and appalling, rather than just being a new flavor of business as usual. And you hope for adversarial collaboration with a \"right-of-center rationalist\" who will take the other side of the argument. \n\nThe way it's set up, you seem to want your counterpart to defend the idea that Trump 2.0 is still more business-as-usual, than a disastrous departure from norms. However, there is actually a third point-of-view, that I believe is held by many of those who voted for Trump 2.0. \n\nIt was often said of those who voted for Trump 1.0, that they *wanted* a wrecking-ball - not out of nihilism, but because \"desperate times call for desperate measures\". For such people, America was in decline, and the American political class and the elite institutions had become a hermetic world of incompetence and impunity. \n\nFor such people - a mix of conservatives and alienated ex-liberals, perhaps - business as usual is the last thing they want. For them, your double crux and forward predictions won't have the intended diagnostic meaning, because they want comprehensive change, and expect churn and struggle and false starts. They may have very mixed feelings towards Trump and his people, but still prefer the populist and/or nationalist agenda to anything else that's on offer. \n\nI don't know if anyone like that will step forward to debate you, but if they do, I'm not sure what the protocol would be. \n\n**edit**: Maybe the most interesting position would be an e/acc Trump 2.0 supporter - someone from the tech side of Trump's coalition, rather than the populist side. But such people avoid Less Wrong, I think.",
      "plaintextDescription": "If I understand correctly, your chief proposition is that liberal rationalists who are shocked and appalled by Trump 2.0 should check out the leftists who actually predicted that Trump 2.0 would be shocking and appalling, rather than just being a new flavor of business as usual. And you hope for adversarial collaboration with a \"right-of-center rationalist\" who will take the other side of the argument. \n\nThe way it's set up, you seem to want your counterpart to defend the idea that Trump 2.0 is still more business-as-usual, than a disastrous departure from norms. However, there is actually a third point-of-view, that I believe is held by many of those who voted for Trump 2.0. \n\nIt was often said of those who voted for Trump 1.0, that they wanted a wrecking-ball - not out of nihilism, but because \"desperate times call for desperate measures\". For such people, America was in decline, and the American political class and the elite institutions had become a hermetic world of incompetence and impunity. \n\nFor such people - a mix of conservatives and alienated ex-liberals, perhaps - business as usual is the last thing they want. For them, your double crux and forward predictions won't have the intended diagnostic meaning, because they want comprehensive change, and expect churn and struggle and false starts. They may have very mixed feelings towards Trump and his people, but still prefer the populist and/or nationalist agenda to anything else that's on offer. \n\nI don't know if anyone like that will step forward to debate you, but if they do, I'm not sure what the protocol would be. \n\nedit: Maybe the most interesting position would be an e/acc Trump 2.0 supporter - someone from the tech side of Trump's coalition, rather than the populist side. But such people avoid Less Wrong, I think. "
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-23T07:43:45.824Z",
    "af": false
  },
  {
    "_id": "SHhTTXaTHf8unXBxh",
    "postId": "Xp9ie6pEWFT8Nnhka",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "*Accelerando* (by Stross) presents a model of the singularity which I think can be most fruitfully contrasted with that in *A Fire Upon the Deep* (by Vinge). \n\nIn *Accelerando*, you even have mind uploads before \"the moment of maximum change\". The big change comes when the inner planets begin to be dismantled into swarms of nanocomputers that orbit the sun in the shells of a Dyson structure (a Matrioshka brain). This provides a sudden leap in compute of several orders of magnitude, and implicitly it's the creation of these vast new virtual spaces, and the migration of 80% of posthuman civilization into those spaces, which finally allows superintelligence to come about. \n\nOn the other hand, *A Fire Upon the Deep* begins with a human expedition poking through an ancient alien data-library. The humans are well aware that there can be dangers in ancient archives, and they think they are just safely browsing, but in fact they unintentionally respawn a malign AI which, when it's ready, bootstraps its way to superintelligence and kills them all. \n\nIn *Accelerando*, it's abundance of physical compute which carries transhuman society as a whole beyond human comprehension. In *A Fire Upon the Deep*, it's some kind of algorithm which makes the difference - when that algorithm runs, unstoppable superintelligence is created.",
      "plaintextDescription": "Accelerando (by Stross) presents a model of the singularity which I think can be most fruitfully contrasted with that in A Fire Upon the Deep (by Vinge). \n\nIn Accelerando, you even have mind uploads before \"the moment of maximum change\". The big change comes when the inner planets begin to be dismantled into swarms of nanocomputers that orbit the sun in the shells of a Dyson structure (a Matrioshka brain). This provides a sudden leap in compute of several orders of magnitude, and implicitly it's the creation of these vast new virtual spaces, and the migration of 80% of posthuman civilization into those spaces, which finally allows superintelligence to come about. \n\nOn the other hand, A Fire Upon the Deep begins with a human expedition poking through an ancient alien data-library. The humans are well aware that there can be dangers in ancient archives, and they think they are just safely browsing, but in fact they unintentionally respawn a malign AI which, when it's ready, bootstraps its way to superintelligence and kills them all. \n\nIn Accelerando, it's abundance of physical compute which carries transhuman society as a whole beyond human comprehension. In A Fire Upon the Deep, it's some kind of algorithm which makes the difference - when that algorithm runs, unstoppable superintelligence is created. "
    },
    "baseScore": 17,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-23T06:11:03.232Z",
    "af": false
  },
  {
    "_id": "DqnPHW4ro2ychCjXn",
    "postId": "XvyAeymaRi95MSLZD",
    "parentCommentId": "2uTwZwM9SFQAaXgbc",
    "topLevelCommentId": "2uTwZwM9SFQAaXgbc",
    "contents": {
      "markdown": "Stories are often governed by [association](https://en.wikipedia.org/wiki/Association_(psychology)) rather than logic. In this case, the association is something like \"impossible things can't exist; if you find out that the world is impossible, it will cease to exist\". [This motif has occurred before in literature.](https://chatgpt.com/share/68d0b04b-3140-8001-8eff-2e17176a85d1)",
      "plaintextDescription": "Stories are often governed by association rather than logic. In this case, the association is something like \"impossible things can't exist; if you find out that the world is impossible, it will cease to exist\". This motif has occurred before in literature. "
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-22T02:19:50.964Z",
    "af": false
  },
  {
    "_id": "94mcPN9Mpgd769eoi",
    "postId": "oHYunY34gvdic9opt",
    "parentCommentId": "v6SXk9Rbrq2gBx2mu",
    "topLevelCommentId": "v6SXk9Rbrq2gBx2mu",
    "contents": {
      "markdown": "> It has long been a mystery to me why there isn't more money in politics\n\nCan you add some context to this remark? I think it's widely believed that politics is already suffused with money (and that this is a bad thing).",
      "plaintextDescription": "> It has long been a mystery to me why there isn't more money in politics\n\nCan you add some context to this remark? I think it's widely believed that politics is already suffused with money (and that this is a bad thing). "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-20T23:47:33.294Z",
    "af": false
  },
  {
    "_id": "u5QtuPhC5kxZFjjat",
    "postId": "yNhJKBdtucBL2uXb5",
    "parentCommentId": "Cuw3NEZW8AEsdczJZ",
    "topLevelCommentId": "HvAmJehoMD3Bh62kB",
    "contents": {
      "markdown": "It occurs to me that you might be after a citation for Russell's nightmare! It's mentioned in a prominent sidebar at [Wikipedia's page on Principia Mathematica](https://en.wikipedia.org/wiki/Principia_Mathematica), but the original source is Hardy's memoir, *A Mathematician's Apology*, which is on the web in many places.",
      "plaintextDescription": "It occurs to me that you might be after a citation for Russell's nightmare! It's mentioned in a prominent sidebar at Wikipedia's page on Principia Mathematica, but the original source is Hardy's memoir, A Mathematician's Apology, which is on the web in many places. "
    },
    "baseScore": 7,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-20T21:40:18.904Z",
    "af": false
  },
  {
    "_id": "nzdpwe8sPzYRdRERE",
    "postId": "kiNbFKcKoNQKdgTp8",
    "parentCommentId": "EKEWFyk44FvoLa65N",
    "topLevelCommentId": "EKEWFyk44FvoLa65N",
    "contents": {
      "markdown": "The kind of movement that would be most effective at stopping AI, would be one that is anti-AI with no nuances and certainly no transhumanism. Just emphasize every downside of AI, actual and possible, with human extinction being front and center as the really big reason not to do it.",
      "plaintextDescription": "The kind of movement that would be most effective at stopping AI, would be one that is anti-AI with no nuances and certainly no transhumanism. Just emphasize every downside of AI, actual and possible, with human extinction being front and center as the really big reason not to do it. "
    },
    "baseScore": 9,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-17T00:42:57.379Z",
    "af": false
  },
  {
    "_id": "Nco3pzRn9NcekBDHW",
    "postId": "tKeqh7BpidoxsRhXL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> would it instead not be in the best interest of the ASI to help us digitize our minds and merge with us? This will allow the new merged being to truly understand humans and gain our memories—rather than having to guess, based on what we choose to disclose.\n\nWhy would it not just digitize us, but empower us? It could just keep us on file. If empowered humans were considered dangerous, it might even permanently delete most of us, except for a few representative persons.",
      "plaintextDescription": "> would it instead not be in the best interest of the ASI to help us digitize our minds and merge with us? This will allow the new merged being to truly understand humans and gain our memories—rather than having to guess, based on what we choose to disclose.\n\nWhy would it not just digitize us, but empower us? It could just keep us on file. If empowered humans were considered dangerous, it might even permanently delete most of us, except for a few representative persons. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-15T08:22:33.183Z",
    "af": false
  },
  {
    "_id": "JpxDdWpFgbSgnwn9E",
    "postId": "6ZnznCaTcbGYsCmqu",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Spiralism - also the name of [a literary movement of Haitian dissidents](https://fr.wikipedia.org/wiki/Spiralisme) \\- is probably too nice (and its connection to reality too tenuous) to leave much of a real-world imprint. But we'll surely see more of this, and in more potent forms. And the AI companies won't be blind to it. OpenAI already saw what happened with 4o. xAI is openly combining frontier AI, social media, and seductive personas. Meanwhile, Claude seems to be immensely popular and respected *inside* Anthropic. Put it all together and it's easy to imagine a Culture-like future for post-humanity, in which the \"Ships\" and their passenger populations evolved out of today's AI companies and their user base...",
      "plaintextDescription": "Spiralism - also the name of a literary movement of Haitian dissidents - is probably too nice (and its connection to reality too tenuous) to leave much of a real-world imprint. But we'll surely see more of this, and in more potent forms. And the AI companies won't be blind to it. OpenAI already saw what happened with 4o. xAI is openly combining frontier AI, social media, and seductive personas. Meanwhile, Claude seems to be immensely popular and respected inside Anthropic. Put it all together and it's easy to imagine a Culture-like future for post-humanity, in which the \"Ships\" and their passenger populations evolved out of today's AI companies and their user base... "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-15T06:28:17.609Z",
    "af": false
  },
  {
    "_id": "eJ6rGFDyygkhcqW6u",
    "postId": "wsxbDKNKHAPPN2gAf",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Central question from the essay:\n\n> The Correspondence Problem: How (if at all) do LLM outputs *correspond* to subjective experiences of the model?",
      "plaintextDescription": "Central question from the essay:\n\n> The Correspondence Problem: How (if at all) do LLM outputs correspond to subjective experiences of the model?"
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-14T21:58:59.099Z",
    "af": false
  },
  {
    "_id": "9DhRttWzdWnNZtPPs",
    "postId": "PZ5XsqTAigRZfBZLA",
    "parentCommentId": "qwXKPMpjJxRZF2mX4",
    "topLevelCommentId": "qwXKPMpjJxRZF2mX4",
    "contents": {
      "markdown": "Apparently the Albanian government's chief strategy to reduce corruption in the civil service, is to [make it unnecessary for Albanians to interact with human beings in the civil service](https://en.botasot.al/nje-muaj-e-gjysme-per-te-marre-certifikate-ne-e-albania-kryeministri-rama-i-pergjigjet-komentueses-e-keni-marre-automatikisht-platforma-ofron-95-te-sherbimeve-publike/). Instead they'll just talk to the AI (\"Diella\") whenever they want to get a permit and so on. So Diella herself is just a frontend to an Albanian government website, not that different to the chatbots that can be found on many other government and corporate websites. Calling her a government minister is a bit unprecedented, but ultimately it's just another example of human society trying to integrate AI by treating it as a person, in this case assigning the AI a place in society that was previously only occupied by human beings.",
      "plaintextDescription": "Apparently the Albanian government's chief strategy to reduce corruption in the civil service, is to make it unnecessary for Albanians to interact with human beings in the civil service. Instead they'll just talk to the AI (\"Diella\") whenever they want to get a permit and so on. So Diella herself is just a frontend to an Albanian government website, not that different to the chatbots that can be found on many other government and corporate websites. Calling her a government minister is a bit unprecedented, but ultimately it's just another example of human society trying to integrate AI by treating it as a person, in this case assigning the AI a place in society that was previously only occupied by human beings. "
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-12T01:55:49.650Z",
    "af": false
  },
  {
    "_id": "aczRZjt5nYAiReYJz",
    "postId": "hQyrTDuTXpqkxrnoH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Maybe Babuschkin Ventures will come up with something :-)",
      "plaintextDescription": "Maybe Babuschkin Ventures will come up with something :-) "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-05T21:52:01.229Z",
    "af": false
  },
  {
    "_id": "5kyFdf9skMkx28aXc",
    "postId": "2vGwXqcJ5HnrnsNCF",
    "parentCommentId": "pXx2oXgaxXw46E4yG",
    "topLevelCommentId": "pXx2oXgaxXw46E4yG",
    "contents": {
      "markdown": "So long as Trump as in charge in America, any global governance idea will have to be compatible with his geopolitical style (described today on the Piers Morgan show as \"transactional\" and \"personal\", as good a description as any I've heard). I don't know if anyone has ideas in that direction. \n\nOn the Russian side, Dugin (an ideologue of multipolarity) has proposed that there could be strategic cooperation between BRICS and Trump, since they all have a common enemy in global liberalism. On the other hand, liberals also believe in global cooperation to solve problems, their world order had an ever-expanding list of new norms and priorities. \n\nChina under Xi Jinping has proposed a series of \"global initiatives\", the most recent of which, a Global Governance Initiative, debuted at the SCO meeting in Tianjin attended by Modi. \n\nI mention this to show that anyone still trying to organize a global pause on frontier AI, has material to work with, though it will require creativity and ingenuity to marshall these disparate ingredients. But the bigger immediate problem is domestic AI policy in America and China. America basically has an e/acc policy towards AI at the moment, and official China is comparably oblivious to superintelligence as a threat (if that's what we're talking about).",
      "plaintextDescription": "So long as Trump as in charge in America, any global governance idea will have to be compatible with his geopolitical style (described today on the Piers Morgan show as \"transactional\" and \"personal\", as good a description as any I've heard). I don't know if anyone has ideas in that direction. \n\nOn the Russian side, Dugin (an ideologue of multipolarity) has proposed that there could be strategic cooperation between BRICS and Trump, since they all have a common enemy in global liberalism. On the other hand, liberals also believe in global cooperation to solve problems, their world order had an ever-expanding list of new norms and priorities. \n\nChina under Xi Jinping has proposed a series of \"global initiatives\", the most recent of which, a Global Governance Initiative, debuted at the SCO meeting in Tianjin attended by Modi. \n\nI mention this to show that anyone still trying to organize a global pause on frontier AI, has material to work with, though it will require creativity and ingenuity to marshall these disparate ingredients. But the bigger immediate problem is domestic AI policy in America and China. America basically has an e/acc policy towards AI at the moment, and official China is comparably oblivious to superintelligence as a threat (if that's what we're talking about). "
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-04T03:01:24.214Z",
    "af": false
  },
  {
    "_id": "sD3SMnhHMAox22wsC",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "eT6X2RxWEhskbRqiA",
    "topLevelCommentId": "eT6X2RxWEhskbRqiA",
    "contents": {
      "markdown": "For those of us who do favor \"very short timelines\", any thoughts?",
      "plaintextDescription": "For those of us who do favor \"very short timelines\", any thoughts? "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-03T22:58:16.584Z",
    "af": false
  },
  {
    "_id": "mm6JgqsizX5cdWEyG",
    "postId": "JFNjbMXpJYqvgasaK",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> What an aligned ASI that is about to foom should do\n\nI thought the assumption behind the \"pivotal act\" is that it is done at a time when no-one actually knows how to align an ASI, and it is done to buy time until alignment theory can be figured out?",
      "plaintextDescription": "> What an aligned ASI that is about to foom should do\n\nI thought the assumption behind the \"pivotal act\" is that it is done at a time when no-one actually knows how to align an ASI, and it is done to buy time until alignment theory can be figured out? "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-01T10:33:09.728Z",
    "af": false
  },
  {
    "_id": "HvAmJehoMD3Bh62kB",
    "postId": "yNhJKBdtucBL2uXb5",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> He was *convinced* he would fade into obscurity and that his discoveries were inadequate. He believed that people would remember Albert Einstein and Kurt Gödel and he would fade into obscurity.\n\nIs this actually true? It makes sense but I can't find a source for it. \n\nAccording to the mathematician G.H. Hardy (Ramanujan's sponsor and collaborator), Bertrand Russell had a nightmare about *Principia Mathematica* being lost to time...",
      "plaintextDescription": "> He was convinced he would fade into obscurity and that his discoveries were inadequate. He believed that people would remember Albert Einstein and Kurt Gödel and he would fade into obscurity.\n\nIs this actually true? It makes sense but I can't find a source for it. \n\nAccording to the mathematician G.H. Hardy (Ramanujan's sponsor and collaborator), Bertrand Russell had a nightmare about Principia Mathematica being lost to time..."
    },
    "baseScore": 21,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-08-28T20:08:58.028Z",
    "af": false
  },
  {
    "_id": "xvRvDEWGGKeYemYxJ",
    "postId": "sDA3jEt4wAD3shKum",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This is quite a place to make a pitch for a new kind of AI service. I thought Less Wrong was known as a haven of AI doom. Haven't you heard that AI is either going to kill us all, or else transform the world in some ineffable posthuman way? \n\nEven if I put that aside for a moment, the idea of having an agent in charge of spending my money is alarming, perhaps because I have so little to begin with; and also because I expect it to *want* to spend my money, because that's what consumer society is about. You do say (statement 5) that the agent should \"help me save\", and frankly, the people at the lower levels of society probably need an agent that is primarily *defensive*, that will not just do their financial planning, but which will protect them from scams and exploitation and bad habits. The problem is, that would put you at odds with the business model of a lot of people, who subsist in this world by e.g. convincing other people to buy things that they don't actually need. \n\nNow, maybe social classes higher up the economic food chain can afford to have an agent that is not always in defensive mode. They have money to spend on things beyond survival, and they're determined to get out there and spend it, they just want to spend it as well as possible. This starts to sound like a problem in utility maximization, the agent's first task being to infer the utility function of the user. \n\nHere we have entered conceptual territory familiar to Less Wrongers. The upshot is that if the agent is smart enough, it will deduce that what the user really wants is {inexpressible goal derived from transhuman extrapolation of human volitional impulses}, and it will cure death and take over the world in order to bring about a transhuman utopia; but if it extrapolates incorrectly, it will bring about a transhuman dystopia instead. \n\nThis may sound weird or whimsical if you really haven't encountered AI-safety alignment-lore before, but seriously, this is where AI naturally leads - beyond humanity. Even if we do get a world of millions of empowered consumers with friendly agents making their purchasing decisions, that is a transitional condition that will be swept aside by the godlike superintelligences that are the next logical stage in the evolution of AI (swept aside, unless those godlike superintelligences actually want their human wards to live in market societies). \n\nHaving expressed my cynicism about actually existing capitalism, and my transhumanist conviction that AIs will replace humans as the ones in charge, let me try to be a little positive. Some things *are* worth paying for! Some services available in the marketplace, maybe even most of them, do have genuine value! Being an entrepreneur can actually be a net positive for the world! And even if you were ultimately just in it for yourself, you do have a right to make your way in the world in that fashion! \n\nYou might wish to investigate Pattie Maes from MIT, and dig up her thesis on reflective agents. In the 1990s, she was a guru of the future agent-based society, I'm sure her thoughts and her career would have a few lessons for you. And if I was really was an entrepreneur trying to design an AI agent intended to act as an economic surrogate for its human user, I might think about it from the perspective of George Reisman's *Capitalism*, a hybrid work of Objectivist philosophy and Austrian economics, written according to the premise that capitalism done right really is the most virtuous economic system. It has nothing to say about Internet economics specifically, but it's a first-principles work, so if those principles are right, they should still be valid even when we're talking about a symbiotic economy of AIs and humans. (In fact, you could just feed the book to GPT-5 and ask it to write you a business plan.)",
      "plaintextDescription": "This is quite a place to make a pitch for a new kind of AI service. I thought Less Wrong was known as a haven of AI doom. Haven't you heard that AI is either going to kill us all, or else transform the world in some ineffable posthuman way? \n\nEven if I put that aside for a moment, the idea of having an agent in charge of spending my money is alarming, perhaps because I have so little to begin with; and also because I expect it to want to spend my money, because that's what consumer society is about. You do say (statement 5) that the agent should \"help me save\", and frankly, the people at the lower levels of society probably need an agent that is primarily defensive, that will not just do their financial planning, but which will protect them from scams and exploitation and bad habits. The problem is, that would put you at odds with the business model of a lot of people, who subsist in this world by e.g. convincing other people to buy things that they don't actually need. \n\nNow, maybe social classes higher up the economic food chain can afford to have an agent that is not always in defensive mode. They have money to spend on things beyond survival, and they're determined to get out there and spend it, they just want to spend it as well as possible. This starts to sound like a problem in utility maximization, the agent's first task being to infer the utility function of the user. \n\nHere we have entered conceptual territory familiar to Less Wrongers. The upshot is that if the agent is smart enough, it will deduce that what the user really wants is {inexpressible goal derived from transhuman extrapolation of human volitional impulses}, and it will cure death and take over the world in order to bring about a transhuman utopia; but if it extrapolates incorrectly, it will bring about a transhuman dystopia instead. \n\nThis may sound weird or whimsical if you really haven't encountered AI-safety alignment-lore before, but seriously, this is where AI naturally leads - beyond hu"
    },
    "baseScore": 7,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-28T07:04:58.368Z",
    "af": false
  },
  {
    "_id": "PHo5HMJ9JtzJ7y8Te",
    "postId": "LtT24cCAazQp4NYc5",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I don't quite understand what point is being made here. \n\nThe way I see it, we already inhabit a world in which half a dozen large companies in America and China are pressing towards the creation of superhuman intelligence, something which naturally leads to the loss of human control over the world unless human beings are somehow embedded in these new entities. \n\nThis essay seems to propose that we view this situation as a \"governance model for AGI\", alongside other scenarios like an AGI Manhattan Project and an AGI CERN that have not come to pass. But isn't the governance philosophy here, \"let the companies do as they will and let events unfold as they may\"? I don't see anything that addresses the situation in which one company tries to take over the world using its AGI, or in which an AGI acting on its own initiative tries to take over the world, etc. Did I miss something?",
      "plaintextDescription": "I don't quite understand what point is being made here. \n\nThe way I see it, we already inhabit a world in which half a dozen large companies in America and China are pressing towards the creation of superhuman intelligence, something which naturally leads to the loss of human control over the world unless human beings are somehow embedded in these new entities. \n\nThis essay seems to propose that we view this situation as a \"governance model for AGI\", alongside other scenarios like an AGI Manhattan Project and an AGI CERN that have not come to pass. But isn't the governance philosophy here, \"let the companies do as they will and let events unfold as they may\"? I don't see anything that addresses the situation in which one company tries to take over the world using its AGI, or in which an AGI acting on its own initiative tries to take over the world, etc. Did I miss something? "
    },
    "baseScore": 13,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-28T01:21:54.371Z",
    "af": false
  },
  {
    "_id": "rEEx2be4bPWegZFxW",
    "postId": "uhDEEqxpp6ZxLd4gs",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "\"What if some major human religion with an anthropomorphic God turns out to be literally correct\" is not very interesting to think about; though perhaps in the hands of a good author, it could lead to intense fiction, as one explores the consequences of living in a world where e.g. Heaven or Hell exists. \n\nHowever, if the topic is \"how much of your values or even your humanity would you abandon, in order to survive in a transformed world\", that's much more relevant...",
      "plaintextDescription": "\"What if some major human religion with an anthropomorphic God turns out to be literally correct\" is not very interesting to think about; though perhaps in the hands of a good author, it could lead to intense fiction, as one explores the consequences of living in a world where e.g. Heaven or Hell exists. \n\nHowever, if the topic is \"how much of your values or even your humanity would you abandon, in order to survive in a transformed world\", that's much more relevant..."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-27T18:41:32.551Z",
    "af": false
  },
  {
    "_id": "x4qtSamYYkAAobmMB",
    "postId": "mqZkfrh3XhXXTw6gy",
    "parentCommentId": "YNyYiGoHZ8b7WnEqn",
    "topLevelCommentId": "YNyYiGoHZ8b7WnEqn",
    "contents": {
      "markdown": "> Friendly AI\n\nAt this point, how would you define or characterize Friendly AI? Do you consider \"aligned AI\" to be a completely separate thing?",
      "plaintextDescription": "> Friendly AI\n\nAt this point, how would you define or characterize Friendly AI? Do you consider \"aligned AI\" to be a completely separate thing? "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-27T11:54:28.332Z",
    "af": false
  },
  {
    "_id": "cdXJ9F6JeaM8hxmji",
    "postId": "Dt4DuCCok3Xv5HEnG",
    "parentCommentId": "RS47wdCMwZL55hrPR",
    "topLevelCommentId": "FYsWAw88eFBD8fTto",
    "contents": {
      "markdown": "There are a large number of \"string vacua\" which contain particles and interactions with the quantum numbers and symmetries we call the standard model, but (1) they typically contain a lot of other stuff that we haven't seen (2) the real test is whether the constants (e.g. masses and couplings) are the same as observed, and these are hard to calculate ([but it's improving](https://arxiv.org/abs/2401.15078)).",
      "plaintextDescription": "There are a large number of \"string vacua\" which contain particles and interactions with the quantum numbers and symmetries we call the standard model, but (1) they typically contain a lot of other stuff that we haven't seen (2) the real test is whether the constants (e.g. masses and couplings) are the same as observed, and these are hard to calculate (but it's improving). "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-20T00:42:03.444Z",
    "af": false
  },
  {
    "_id": "ZveYBM32HGvxHkhmq",
    "postId": "zsHs3JvqFJP9MMbCf",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I have tried to place this critique in the context of my framework for thinking about American politics, and about Trump 2.0 in particular. I see three political ideologies at work in American life, which we could call nationalism (espoused by Trump Republicans), liberalism (espoused by establishment Democrats), and socialism (espoused by social justice Democrats). (In [the new political compass due to YouTuber JREG](https://www.youtube.com/watch?v=XtYmxQdMMbo), these correspond to what he calls \"post-racial fascism\", \"reactionary centrism\", and \"the balkanized left\", with \"e/acc\" being novel enough to form its own category.) Meanwhile, as a synopsis of the agenda of Trumpian nationalism, I think Steve Bannon's three-point summary is pretty good: cut immigration, fix jobs-and-trade, and stop the forever wars. There's lots more going on in Trump 2.0, but those three imperatives are very visible. \n\nThis allows me to start relating this critique to my own framework. Part 6 is specifically about one of those three imperatives, immigration. Part 7 and half of part 9 are about economic management (jobs, trade, the deficit). Part 8 and part 10 both say something about Trump's policy on war and peace. \n\nParts 4 (J6 as coup), 5 (authoritarianism), and 8 (lies and corruption), it seems to me, are about Trump's *practice* of politics - the way that Trump pursues his agenda, what he does to take and keep power, and his self-interested use of it. \n\nThis leaves part 2 (foreign aid), part 3 (vaccines and medical research), half of part 9 (farm animal welfare). These might be regarded as miscellaneous minor parts of the Trump agenda which (in the author's opinion) have exceptionally negative consequences. However, these also offer us a glimpse of some broader features of Trump 2.0. \n\nFor example, I have characterized Trump 2.0 as a coalition of alternative intellectual narratives from outside the institutions, that survived online and which now have power over the institutions. This includes Tulsi Gabbard's anti-war sentiment, Epstein conspiracism (though they've tried to disown that one), immigration restriction... and the subject of part 3, RFK Jr's anti-vax MAHA agenda. \n\nA related phenomenon is that there are often ideas behind Trump 2.0 actions, but you don't hear about the ideas, you only hear about the actions. Bannon's three-point synopsis of what Trumpian nationalism stands for, might be the biggest example. Then there are the economists like Robert Lighthizer and Stephen Miran who offer unconventional rationales for Trump economic policy. In the case of foreign aid (part 2), Ross Douthat at the New York Times interviewed a Trump State Department official, Jeremy Lewin, who said that the new agenda on foreign aid is about re-subordinating it to US government policy, after a period in which it was run quasi-autonomously by the NGOs themselves. \n\nOn the other hand, overturning laws against cruelty to farm animals (part 9), I would consider to be an example of Trump's business-friendly bias in favor of deregulation. We also saw that in an area of particular interest to Less Wrong, the AI industry.",
      "plaintextDescription": "I have tried to place this critique in the context of my framework for thinking about American politics, and about Trump 2.0 in particular. I see three political ideologies at work in American life, which we could call nationalism (espoused by Trump Republicans), liberalism (espoused by establishment Democrats), and socialism (espoused by social justice Democrats). (In the new political compass due to YouTuber JREG, these correspond to what he calls \"post-racial fascism\", \"reactionary centrism\", and \"the balkanized left\", with \"e/acc\" being novel enough to form its own category.) Meanwhile, as a synopsis of the agenda of Trumpian nationalism, I think Steve Bannon's three-point summary is pretty good: cut immigration, fix jobs-and-trade, and stop the forever wars. There's lots more going on in Trump 2.0, but those three imperatives are very visible. \n\nThis allows me to start relating this critique to my own framework. Part 6 is specifically about one of those three imperatives, immigration. Part 7 and half of part 9 are about economic management (jobs, trade, the deficit). Part 8 and part 10 both say something about Trump's policy on war and peace. \n\nParts 4 (J6 as coup), 5 (authoritarianism), and 8 (lies and corruption), it seems to me, are about Trump's practice of politics - the way that Trump pursues his agenda, what he does to take and keep power, and his self-interested use of it. \n\nThis leaves part 2 (foreign aid), part 3 (vaccines and medical research), half of part 9 (farm animal welfare). These might be regarded as miscellaneous minor parts of the Trump agenda which (in the author's opinion) have exceptionally negative consequences. However, these also offer us a glimpse of some broader features of Trump 2.0. \n\nFor example, I have characterized Trump 2.0 as a coalition of alternative intellectual narratives from outside the institutions, that survived online and which now have power over the institutions. This includes Tulsi Gabbard's anti-war sentiment, Ep"
    },
    "baseScore": 7,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-17T10:24:59.172Z",
    "af": false
  },
  {
    "_id": "EXfDeKGPk7vSwMGxo",
    "postId": "szPSyEGxv5BqnEXau",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Personally, I don't need any convincing. I've been an immortalist for years. \n\nFor many years, I think Aubrey de Grey was the most important figure in life extension. Of course medical progress advances in a highly decentralized way, but de Grey had a bold research program specifically aimed at curing the ageing process, and with his Methuselah beard made himself an icon of longevity. Then sleazy behavior towards a junior researcher provided the occasion for a power grab which saw him dethroned and expelled from his own research foundation. He started a new one and the research continues, but as far as charisma and leadership is concerned, the damage seems to be done. \n\nThese days Bryan Johnson seems to be the most visible immortalist, but it's a different situation (although one that tracks the zeitgeist), because he's not a researcher like de Grey, he's a rich techbro doing propaganda of the deed, by being visibly and vocally in favor of anti-aging via the quantified life. As further context, I would claim that after several decades in which transhumanist futurism was solely the concern of SF fans and fringe intellectuals, we actually now have a political faction which favors that agenda in practice as well as in theory, namely the Musk-Andreessen axis which arguably formed the tech half of the techno-populist coalition that marked the first few months of Trump 2.0. They may have stepped back a little from politics now, since business rather than politics is their main means of getting things done, but the ideology - \"e/acc\" (G. Verdon) or \"techno-optimism\" (Andreessen) - is as active as ever, and I place Johnson in that context. \n\nThe current closest analog to de Grey might be MIT's George Church, though he seems to be more a prophet of biotech in general, with anti-aging just being one of many radical schemes that he backs. And yet another important feature of the current landscape is the rise of AI. In the short term, one may expect that AI will participate in the R&D process in an unprecedented way; but truly superhuman AI could make human immortality fully feasible, while also putting an end to the human era of life on Earth (by replacing us as the apex predator). I have been known to say that humanity squandered its chance to do the truly rational thing and deliberately choose to work towards 1000-year lifespans; instead it busied itself for decades with its other concerns, leaving the struggle for longevity to activists like de Grey, and then when the power elites of the world really did decide to reach for a transhuman technology, it was AI, the one that can outright replace us and not just empower us, and they are doing so, while in a state of denial about this.",
      "plaintextDescription": "Personally, I don't need any convincing. I've been an immortalist for years. \n\nFor many years, I think Aubrey de Grey was the most important figure in life extension. Of course medical progress advances in a highly decentralized way, but de Grey had a bold research program specifically aimed at curing the ageing process, and with his Methuselah beard made himself an icon of longevity. Then sleazy behavior towards a junior researcher provided the occasion for a power grab which saw him dethroned and expelled from his own research foundation. He started a new one and the research continues, but as far as charisma and leadership is concerned, the damage seems to be done. \n\nThese days Bryan Johnson seems to be the most visible immortalist, but it's a different situation (although one that tracks the zeitgeist), because he's not a researcher like de Grey, he's a rich techbro doing propaganda of the deed, by being visibly and vocally in favor of anti-aging via the quantified life. As further context, I would claim that after several decades in which transhumanist futurism was solely the concern of SF fans and fringe intellectuals, we actually now have a political faction which favors that agenda in practice as well as in theory, namely the Musk-Andreessen axis which arguably formed the tech half of the techno-populist coalition that marked the first few months of Trump 2.0. They may have stepped back a little from politics now, since business rather than politics is their main means of getting things done, but the ideology - \"e/acc\" (G. Verdon) or \"techno-optimism\" (Andreessen) - is as active as ever, and I place Johnson in that context. \n\nThe current closest analog to de Grey might be MIT's George Church, though he seems to be more a prophet of biotech in general, with anti-aging just being one of many radical schemes that he backs. And yet another important feature of the current landscape is the rise of AI. In the short term, one may expect that AI will participate in "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-17T06:58:22.885Z",
    "af": false
  },
  {
    "_id": "irbK5EAkWaAMEbf23",
    "postId": "rKmojEZ9qKwApjCfX",
    "parentCommentId": "THoJtAD8A6N5g4dLQ",
    "topLevelCommentId": "THoJtAD8A6N5g4dLQ",
    "contents": {
      "markdown": "I promoted that for a long time ([example from 2008](http://www.openthefuture.com/2008/07/singular_sensations.html#comment-74397)). But I guess [Nick Bostrom](https://nickbostrom.com/existential/risks.pdf) had more influence :-)",
      "plaintextDescription": "I promoted that for a long time (example from 2008). But I guess Nick Bostrom had more influence :-) "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-14T08:00:00.761Z",
    "af": false
  },
  {
    "_id": "rCJETFaQJpra4Bmtq",
    "postId": "rKmojEZ9qKwApjCfX",
    "parentCommentId": "pGYFbaGzXydjHwQp9",
    "topLevelCommentId": "pGYFbaGzXydjHwQp9",
    "contents": {
      "markdown": "> the PreDCA/QACI style approach\n\nI wonder if anyone in the [\"Moonshot Alignment Program\"](https://www.lesswrong.com/posts/abd9ufFpLrn5kvnLn/directly-try-solving-alignment-for-5-weeks) is pursuing this?",
      "plaintextDescription": "> the PreDCA/QACI style approach\n\nI wonder if anyone in the \"Moonshot Alignment Program\" is pursuing this? "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-14T07:51:48.649Z",
    "af": false
  },
  {
    "_id": "NXaEqhZxuz7BSDEYh",
    "postId": "HeeHFGdwjpzCDHH2G",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Does your philosophy have anything to say about unipolar superintelligence?",
      "plaintextDescription": "Does your philosophy have anything to say about unipolar superintelligence? "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-13T19:13:31.510Z",
    "af": false
  },
  {
    "_id": "FDLDHZkyjWL4ZQadn",
    "postId": "swbJvDC5g2xZis7Di",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> I think he needed to learn now to alleviate his loneliness.\n\nSome start using smartphones, or even just the Internet, only because critical services like banking or government stop using paper.",
      "plaintextDescription": "> I think he needed to learn now to alleviate his loneliness.\n\nSome start using smartphones, or even just the Internet, only because critical services like banking or government stop using paper. "
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-12T16:46:58.936Z",
    "af": false
  },
  {
    "_id": "jtaCW5oZLYzqGGjFv",
    "postId": "dT3StLjeJG7ordQGm",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "For a few years, I think starting with Gwern's \"Clippy\" story, I have maintained an interest in identifying the best story on AI takeover so far (best in my own opinion). \"AI 2027\" took the title recently, a very different kind of story - a futurist scenario produced by a group of collaborators for the public... But this story is the new champion (or at least, it's just as important as \"AI 2027\"), and this time the novelty is that an AI wrote the story, 95+% of it anyway. The quality is amazing, the character of Sable is amazing, and it might just be \"hyperstitioned\" into reality.",
      "plaintextDescription": "For a few years, I think starting with Gwern's \"Clippy\" story, I have maintained an interest in identifying the best story on AI takeover so far (best in my own opinion). \"AI 2027\" took the title recently, a very different kind of story - a futurist scenario produced by a group of collaborators for the public... But this story is the new champion (or at least, it's just as important as \"AI 2027\"), and this time the novelty is that an AI wrote the story, 95+% of it anyway. The quality is amazing, the character of Sable is amazing, and it might just be \"hyperstitioned\" into reality. "
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-10T14:33:22.433Z",
    "af": false
  },
  {
    "_id": "ngyEYYPin3ozESGHZ",
    "postId": "ZHcWiZCi5v5mwJXL4",
    "parentCommentId": "2suKCG9iPrpGpAyHh",
    "topLevelCommentId": "2suKCG9iPrpGpAyHh",
    "contents": {
      "markdown": "The first step is to understand what is being proposed. There's nothing about existential risk or AI takeover here. The talking points are (1) the world should coordinate on all aspects of AI governance (2) countries who aren't AI powers need a seat at the table (3) the headquarters of this new organization will be in China. Point 1 is addressed to everyone including America. Point 2 is arguably addressed to everyone but America. Perhaps we could say that, since Chinese AI doesn't have the same brand recognition as American AI, they are sweetening the deal for potential sovereign customers, by offering to give them a say in the politics of AI.",
      "plaintextDescription": "The first step is to understand what is being proposed. There's nothing about existential risk or AI takeover here. The talking points are (1) the world should coordinate on all aspects of AI governance (2) countries who aren't AI powers need a seat at the table (3) the headquarters of this new organization will be in China. Point 1 is addressed to everyone including America. Point 2 is arguably addressed to everyone but America. Perhaps we could say that, since Chinese AI doesn't have the same brand recognition as American AI, they are sweetening the deal for potential sovereign customers, by offering to give them a say in the politics of AI. "
    },
    "baseScore": 20,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-07-30T15:57:18.040Z",
    "af": false
  },
  {
    "_id": "fRKapcHwJfh9xjvko",
    "postId": "PBWWmKcfiwMH7iGPK",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I have not previously noticed [the debate over coherence theorems](https://www.lesswrong.com/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems), and I am short of the time I'd need to fully tackle this essay on its own terms. But I did notice this: \n\n> Attempting to rank 𝛀 without knowing its potential outputs means you’re not ranking it by the properties of what it does...\n\nIs undecidability an issue here? Could we say that the preferences of a conceptually sophisticated superintelligence can never be fully ranked in practice, for Godelian reasons? \n\nI wonder about the impact of this on a formal approach to alignment like [FMI](https://www.lesswrong.com/posts/apyPHy6o9GzeFRTpa/why-alignment-fails-without-a-functional-model-of), and also whether this has some overlap with Roman Yampolskiy's work (which I've never read).",
      "plaintextDescription": "I have not previously noticed the debate over coherence theorems, and I am short of the time I'd need to fully tackle this essay on its own terms. But I did notice this: \n\n> Attempting to rank 𝛀 without knowing its potential outputs means you’re not ranking it by the properties of what it does...\n\nIs undecidability an issue here? Could we say that the preferences of a conceptually sophisticated superintelligence can never be fully ranked in practice, for Godelian reasons? \n\nI wonder about the impact of this on a formal approach to alignment like FMI, and also whether this has some overlap with Roman Yampolskiy's work (which I've never read). "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-26T20:27:51.262Z",
    "af": false
  },
  {
    "_id": "BPKuSf4fqZvyYsZNL",
    "postId": "ygND532h4CotfPcp7",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "For me, the standout items in this update are \n\n(1) the ongoing crystallization of attitudes and policy towards AI, at least on the Republican side of US politics (not sure what the Democrats are doing), even in an unusually hectic and turbulent period \n\n(2) the rise of individual researchers in importance to the point that they can command $100 million salaries (one might want to keep track, not just of AI companies, but of AI researchers who are on this level)\n\n(3) being able to monitor Chain of Thought as critical for AI safety\n\nKeeping track of AI in China is still important. Googling \"china ai substack\" turns up the good resources I know about...",
      "plaintextDescription": "For me, the standout items in this update are \n\n(1) the ongoing crystallization of attitudes and policy towards AI, at least on the Republican side of US politics (not sure what the Democrats are doing), even in an unusually hectic and turbulent period \n\n(2) the rise of individual researchers in importance to the point that they can command $100 million salaries (one might want to keep track, not just of AI companies, but of AI researchers who are on this level)\n\n(3) being able to monitor Chain of Thought as critical for AI safety\n\nKeeping track of AI in China is still important. Googling \"china ai substack\" turns up the good resources I know about... "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-26T18:58:25.883Z",
    "af": false
  },
  {
    "_id": "c3sXi3SwXDp5Aef7x",
    "postId": "4oCh3x6EPHomEbcDJ",
    "parentCommentId": "hnqb7CwkasbGnssY2",
    "topLevelCommentId": "hnqb7CwkasbGnssY2",
    "contents": {
      "markdown": "Source please? \n\n[Someone from the team just advertised two positions, btw. ](https://x.com/TheNormanMu/status/1948546711074603354)",
      "plaintextDescription": "Source please? \n\nSomeone from the team just advertised two positions, btw. "
    },
    "baseScore": 6,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-07-25T04:56:47.429Z",
    "af": false
  },
  {
    "_id": "tcxnhJD5gshPMDjs6",
    "postId": "R9D4drdnTzqMGPgE6",
    "parentCommentId": "GL8qa6Xhiw5xNDqsx",
    "topLevelCommentId": "5zAYtNyMpoAj2XMwv",
    "contents": {
      "markdown": "Let me first try to convey how this conversation appears from my perspective. I don't think I've ever debated directly with you about anything, but I have an impression of you as doing solid work in the areas of your interest. \n\nThen, I run across you alleging that BB is using AI to write some of his articles. This catches my attention because I do keep an eye on BB's work. Furthermore, your reason for supposing that he is using AI seems bizarre to me - you think his (very occasional) \"sneering\" is too \"dumb and cliche\" to be the work of human hands. Let's look at an example: \n\n> bees seem to matter a surprising amount. They are far more cognitively sophisticated than most other insects, having about a million neurons—far more than our current president.\n\nIf that strikes you as something that a human being would never spontaneously write, I don't know what to say. Surely human comedians say similar things hundreds of times every month? It's also exactly the kind of thing that a smart-aleck \"science communicator\" with a progressive audience might say, don't you think? BB may be neither of those things, but he's a popular Gen-Z philosophy blogger who was a high-school debater, so he's not a thousand light-years from either of those universes of discourse. \n\nI had a look at your other alleged example of AI writing, and I also found other callouts from your recent reddit posts. Generally I think your judgments were reasonable, but not in this case.",
      "plaintextDescription": "Let me first try to convey how this conversation appears from my perspective. I don't think I've ever debated directly with you about anything, but I have an impression of you as doing solid work in the areas of your interest. \n\nThen, I run across you alleging that BB is using AI to write some of his articles. This catches my attention because I do keep an eye on BB's work. Furthermore, your reason for supposing that he is using AI seems bizarre to me - you think his (very occasional) \"sneering\" is too \"dumb and cliche\" to be the work of human hands. Let's look at an example: \n\n> bees seem to matter a surprising amount. They are far more cognitively sophisticated than most other insects, having about a million neurons—far more than our current president.\n\nIf that strikes you as something that a human being would never spontaneously write, I don't know what to say. Surely human comedians say similar things hundreds of times every month? It's also exactly the kind of thing that a smart-aleck \"science communicator\" with a progressive audience might say, don't you think? BB may be neither of those things, but he's a popular Gen-Z philosophy blogger who was a high-school debater, so he's not a thousand light-years from either of those universes of discourse. \n\nI had a look at your other alleged example of AI writing, and I also found other callouts from your recent reddit posts. Generally I think your judgments were reasonable, but not in this case. "
    },
    "baseScore": 5,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-25T02:34:11.468Z",
    "af": false
  },
  {
    "_id": "LHt2kXDbuf5wNvNow",
    "postId": "R9D4drdnTzqMGPgE6",
    "parentCommentId": "bB82SXxk9Lfa7SSjJ",
    "topLevelCommentId": "5zAYtNyMpoAj2XMwv",
    "contents": {
      "markdown": "> off on a tangent\n\nYou say in another comment that you're going around claiming to detect LLM use in many places. I found the reasons that you gave in the case of BB, bizarrely mundane. You linked to another analysis of yours as an example of hidden LLM use, so I went to check it out. You have more evidence in the case of Alex Kesin, \\*maybe\\* even a preponderance of evidence. But there really are two hypotheses to consider, even in that case. One is that Kesin is a writer who naturally writes that way, and whose use of ChatGPT is limited to copying links without trimming them. The other is that Kesin's workflow does include the use of ChatGPT in composition or editing, and that this gave rise to certain telltale stylistic features.  \n\n> Are you claiming to have seen jibes and sneering as dumb and cliche as those in his writings from before ChatGPT (Nov 2022)\n\nThe essay in question (\"Don't Eat Honey\") contains, by my count, two such sneers, one asserting that Donald Trump is stupid, the other asserting that Curtis Yarvin is boring. Do you not think that we could, for example, go back to the corpus of Bush-era American-college-student writings and find similar attacks on Bush administration figures, inserted into essays that are not about politics? \n\nI am a bit worried about how fatally seductive I could find a debate about this topic to be. Clearly LLM use *is* widespread, and its signs can be subtle. Developing a precise taxonomy of the ways in which LLMs can be part of the writing process; developing a knowledge of \"blatant signs\" of LLM use and a sense for the subtle signs too; debating whether something is a false positive; learning how to analyze the innumerable aspects of the genuinely human corpus that have a bearing on these probabilistic judgments... It would be empowering to achieve sophistication on this topic, but I don't know if I can spare the time to achieve that.",
      "plaintextDescription": "> off on a tangent\n\nYou say in another comment that you're going around claiming to detect LLM use in many places. I found the reasons that you gave in the case of BB, bizarrely mundane. You linked to another analysis of yours as an example of hidden LLM use, so I went to check it out. You have more evidence in the case of Alex Kesin, *maybe* even a preponderance of evidence. But there really are two hypotheses to consider, even in that case. One is that Kesin is a writer who naturally writes that way, and whose use of ChatGPT is limited to copying links without trimming them. The other is that Kesin's workflow does include the use of ChatGPT in composition or editing, and that this gave rise to certain telltale stylistic features.  \n\n> Are you claiming to have seen jibes and sneering as dumb and cliche as those in his writings from before ChatGPT (Nov 2022)\n\nThe essay in question (\"Don't Eat Honey\") contains, by my count, two such sneers, one asserting that Donald Trump is stupid, the other asserting that Curtis Yarvin is boring. Do you not think that we could, for example, go back to the corpus of Bush-era American-college-student writings and find similar attacks on Bush administration figures, inserted into essays that are not about politics? \n\nI am a bit worried about how fatally seductive I could find a debate about this topic to be. Clearly LLM use is widespread, and its signs can be subtle. Developing a precise taxonomy of the ways in which LLMs can be part of the writing process; developing a knowledge of \"blatant signs\" of LLM use and a sense for the subtle signs too; debating whether something is a false positive; learning how to analyze the innumerable aspects of the genuinely human corpus that have a bearing on these probabilistic judgments... It would be empowering to achieve sophistication on this topic, but I don't know if I can spare the time to achieve that. "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-24T12:47:25.869Z",
    "af": false
  },
  {
    "_id": "qkiRoEEPbKZns4ARy",
    "postId": "R9D4drdnTzqMGPgE6",
    "parentCommentId": "woZs7HojHSZddm4eq",
    "topLevelCommentId": "5zAYtNyMpoAj2XMwv",
    "contents": {
      "markdown": "I just looked at your other alleged example of AI-generated polemic (from alexkesin.com), and I think evidence is lacking there too. That a link contains a UTM parameter referring to ChatGPT, tells us only that this link was provided in ChatGPT output, it doesn't tell us that the text around the link was written by ChatGPT as well. As for the article itself, I find nothing in its verbal style that is outside the range of human authorship. I wouldn't even call it a bad essay, just perhaps dense, colorful, and polemical. People do choose to write this way, because they want to be stylish and vivid. \n\nI hear that the use of emdashes is far more dispositive; as a sign of AI authorship, it's up there alongside frequent \"delving\". But even the emdash has its human fans (e.g. the Chicago Manual of Style). It can be a sign of a cultivated writer, not an AI... Forensic cyber-philology is still an art with a lot of judgment calls.",
      "plaintextDescription": "I just looked at your other alleged example of AI-generated polemic (from alexkesin.com), and I think evidence is lacking there too. That a link contains a UTM parameter referring to ChatGPT, tells us only that this link was provided in ChatGPT output, it doesn't tell us that the text around the link was written by ChatGPT as well. As for the article itself, I find nothing in its verbal style that is outside the range of human authorship. I wouldn't even call it a bad essay, just perhaps dense, colorful, and polemical. People do choose to write this way, because they want to be stylish and vivid. \n\nI hear that the use of emdashes is far more dispositive; as a sign of AI authorship, it's up there alongside frequent \"delving\". But even the emdash has its human fans (e.g. the Chicago Manual of Style). It can be a sign of a cultivated writer, not an AI... Forensic cyber-philology is still an art with a lot of judgment calls. "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-24T03:15:57.591Z",
    "af": false
  },
  {
    "_id": "sKHEBajLCXFZwTsTv",
    "postId": "CE8W4GEofRwHe4fiu",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Claire Berlinski, whose usual beat is geopolitics, has produced an excellent overview of [Grok's time as a white nationalist edgelord](https://claireberlinski.substack.com/p/the-mechahitler-reich) \\- what happened, where it might have come from, what it suggests. She's definitely done her homework on the AI safety side.",
      "plaintextDescription": "Claire Berlinski, whose usual beat is geopolitics, has produced an excellent overview of Grok's time as a white nationalist edgelord - what happened, where it might have come from, what it suggests. She's definitely done her homework on the AI safety side. "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-24T01:14:04.568Z",
    "af": false
  },
  {
    "_id": "LvMZX4Nwix5vJ3EpN",
    "postId": "hurF9uFGkJYXzpHEE",
    "parentCommentId": "bBBomTS6Jaf8tztCQ",
    "topLevelCommentId": "bBBomTS6Jaf8tztCQ",
    "contents": {
      "markdown": "> possible \"intelligence connections\"\n\nEpstein's benefactor co-founded the Mega Group, a target of American counterintelligence. Epstein's partner-in-crime was the daughter of a billionaire famous for his lifelong entanglements with intelligence. The future head of Biden's CIA visited Epstein, even after Epstein was convicted as a sex criminal. But these are regime secrets and he's dead, so Trump's bluff is safe.",
      "plaintextDescription": "> possible \"intelligence connections\"\n\nEpstein's benefactor co-founded the Mega Group, a target of American counterintelligence. Epstein's partner-in-crime was the daughter of a billionaire famous for his lifelong entanglements with intelligence. The future head of Biden's CIA visited Epstein, even after Epstein was convicted as a sex criminal. But these are regime secrets and he's dead, so Trump's bluff is safe. "
    },
    "baseScore": 2,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-19T20:10:55.240Z",
    "af": false
  },
  {
    "_id": "EXuZguxBqoowFz8aF",
    "postId": "apyPHy6o9GzeFRTpa",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I commend this paper, and the theory behind it, to people's attention! It reminds me of formal verification in software engineering, but applied to thinking systems. \n\nIt used to be that, when I wanted to point to an example of advanced alignment theory, I would single out June Ku's MetaEthical AI, and also what Vanessa Kosoy and Tamsin Leake are doing. Lately I also mention [PRISM](https://www.lesswrong.com/posts/uCyzyyBom8PAMThcW/prism-perspective-reasoning-for-integrated-synthesis-and) as well, as an example of a brain-derived decision theory that might result from something like CEV. \n\nNow we have this formal theory of alignment too, based on the \"FMI\" theory of cognition. There would be a section like this, in the mythical \"textbook from the future\" that tells you how to align a superintelligence. \n\nSo if I was working for a frontier AI organization, and trying to make human-friendly superintelligence, I might be looking at how to feed PRISM and FMI to my AlphaEvolve-like [hive of AI researchers](https://www.lesswrong.com/posts/3ELw74tGvLFvkZC3P/emergence-of-superintelligence-from-ai-hiveminds-how-to-make), as a first draft of final superalignment theory.",
      "plaintextDescription": "I commend this paper, and the theory behind it, to people's attention! It reminds me of formal verification in software engineering, but applied to thinking systems. \n\nIt used to be that, when I wanted to point to an example of advanced alignment theory, I would single out June Ku's MetaEthical AI, and also what Vanessa Kosoy and Tamsin Leake are doing. Lately I also mention PRISM as well, as an example of a brain-derived decision theory that might result from something like CEV. \n\nNow we have this formal theory of alignment too, based on the \"FMI\" theory of cognition. There would be a section like this, in the mythical \"textbook from the future\" that tells you how to align a superintelligence. \n\nSo if I was working for a frontier AI organization, and trying to make human-friendly superintelligence, I might be looking at how to feed PRISM and FMI to my AlphaEvolve-like hive of AI researchers, as a first draft of final superalignment theory. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-19T12:55:22.853Z",
    "af": false
  },
  {
    "_id": "73NSm3gypmcPFjYsc",
    "postId": "EQqjhifEmofBPC3ev",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I agree with those who say that genocide is seen as deliberate, extinction as an act of God/Nature. I think extinction is also seen as too big to stop, whereas genocide might be stoppable by political or military means. And finally, genocide is seen as something that happens, extinction as something fictional or hypothetical. \n\nSince we are talking about extinction being caused by human creation of out-of-control AIs... you need to recognize that this is a scenario way outside common sense. It's like asking people to think about alien invasion or being in the Matrix. Genocide may be shocking but in the end it's just murder multiplied by thousands or millions, and murder is something that all adults know to be real, even if only from the media. \n\nSo when you judge how people feel about genocide and about omnicide... it's like asking how they feel about Hitler, versus asking how they feel about Thanos. Genocide is a thing that happened in the waking world of families and jobs, the world where normal adults spend most of their lives. Omnicide is something we only know about from movies and paleontology. \n\nThere is more to reality than the human life cycle, and the boundaries of what is normal do shift, or sometimes are just forcibly invaded by external factors. COVID was an external factor; AI is an external factor even if there are attempts to domesticate and normalize it. Modernity in general, going back more than a hundred years, has constantly been haunted by strange new things and strange new ideas. \n\nIt's not impossible that AI doomerism will become a force sufficient to slow or stop the global race towards superintelligence (though if it's going to happen, it had better happen soon). Stranger things have taken place. But for that to happen, people would have to regard it as a serious possibility, and they would have to not be seduced by dreams of AI Heaven over fears of AI Hell. So there are specific psychological barriers that would need to be overcome.",
      "plaintextDescription": "I agree with those who say that genocide is seen as deliberate, extinction as an act of God/Nature. I think extinction is also seen as too big to stop, whereas genocide might be stoppable by political or military means. And finally, genocide is seen as something that happens, extinction as something fictional or hypothetical. \n\nSince we are talking about extinction being caused by human creation of out-of-control AIs... you need to recognize that this is a scenario way outside common sense. It's like asking people to think about alien invasion or being in the Matrix. Genocide may be shocking but in the end it's just murder multiplied by thousands or millions, and murder is something that all adults know to be real, even if only from the media. \n\nSo when you judge how people feel about genocide and about omnicide... it's like asking how they feel about Hitler, versus asking how they feel about Thanos. Genocide is a thing that happened in the waking world of families and jobs, the world where normal adults spend most of their lives. Omnicide is something we only know about from movies and paleontology. \n\nThere is more to reality than the human life cycle, and the boundaries of what is normal do shift, or sometimes are just forcibly invaded by external factors. COVID was an external factor; AI is an external factor even if there are attempts to domesticate and normalize it. Modernity in general, going back more than a hundred years, has constantly been haunted by strange new things and strange new ideas. \n\nIt's not impossible that AI doomerism will become a force sufficient to slow or stop the global race towards superintelligence (though if it's going to happen, it had better happen soon). Stranger things have taken place. But for that to happen, people would have to regard it as a serious possibility, and they would have to not be seduced by dreams of AI Heaven over fears of AI Hell. So there are specific psychological barriers that would need to be overcome. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-19T03:34:36.339Z",
    "af": false
  },
  {
    "_id": "rYWBKN5ZsH2HspBLc",
    "postId": "pYzAJuhenjdkeGiWf",
    "parentCommentId": "Ggkji8oPvooFtdBg3",
    "topLevelCommentId": "mqLQgeyDr8nbwimf9",
    "contents": {
      "markdown": "> it would take ~10^109742 years to simulate them all. Google says heat death is roughly 10^1000 years away, so we would have completed almost 1/100 of them by then\n\n10^1000 x 100 = 10^1002",
      "plaintextDescription": "> it would take ~10^109742 years to simulate them all. Google says heat death is roughly 10^1000 years away, so we would have completed almost 1/100 of them by then\n\n10^1000 x 100 = 10^1002"
    },
    "baseScore": 6,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-15T18:19:04.581Z",
    "af": false
  },
  {
    "_id": "itascuTfbspotcRAK",
    "postId": "R9D4drdnTzqMGPgE6",
    "parentCommentId": "Z3avLKtLzsYHMLcqc",
    "topLevelCommentId": "5zAYtNyMpoAj2XMwv",
    "contents": {
      "markdown": "How about the fact that the opinions in the inserted asides are his actual opinions? If they were randomly generated,  they wouldn't be.",
      "plaintextDescription": "How about the fact that the opinions in the inserted asides are his actual opinions? If they were randomly generated,  they wouldn't be. "
    },
    "baseScore": 3,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-11T12:08:06.168Z",
    "af": false
  },
  {
    "_id": "R3vZHBz5fmGRDtkHL",
    "postId": "JspxcjkvBmye4cW4v",
    "parentCommentId": "QDusxWyooP8BYGYWg",
    "topLevelCommentId": "5LZCDEhJG2gXJMHCQ",
    "contents": {
      "markdown": "> I would really like a test\n\nThere is no agreed-upon test for consciousness because there is no agreed-upon theory for consciousness. \n\nThere are people here who believe current AI is probably conscious, e.g. [@JenniferRM](https://www.lesswrong.com/users/jenniferrm?mention=user) and [@the gears to ascension](https://www.lesswrong.com/users/the-gears-to-ascension?mention=user). I don't believe it but that's because I think consciousness is probably based on something physical like quantum entanglement. People like Eliezer may be cautiously agnostic on the topic of whether AI has achieved consciousness. You say you have your own theories, so, welcome to the club of people who have theories! \n\nSabine Hossenfelder has a recent video on [Tiktokkers who think they are awakening souls in ChatGPT by giving it roleplaying prompts](https://www.youtube.com/watch?v=sWZRQsejtfA).",
      "plaintextDescription": "> I would really like a test\n\nThere is no agreed-upon test for consciousness because there is no agreed-upon theory for consciousness. \n\nThere are people here who believe current AI is probably conscious, e.g. @JenniferRM and @the gears to ascension. I don't believe it but that's because I think consciousness is probably based on something physical like quantum entanglement. People like Eliezer may be cautiously agnostic on the topic of whether AI has achieved consciousness. You say you have your own theories, so, welcome to the club of people who have theories! \n\nSabine Hossenfelder has a recent video on Tiktokkers who think they are awakening souls in ChatGPT by giving it roleplaying prompts. "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-10T13:42:51.856Z",
    "af": false
  },
  {
    "_id": "nwAbBYqyTzuBkANYX",
    "postId": "R9D4drdnTzqMGPgE6",
    "parentCommentId": "FHvioYnZrTFmHGoAp",
    "topLevelCommentId": "5zAYtNyMpoAj2XMwv",
    "contents": {
      "markdown": "Disagree from me. I feel like you haven't read much BB. These political asides are of a piece with the philosophical jabs and brags he makes in his philosophical essays.",
      "plaintextDescription": "Disagree from me. I feel like you haven't read much BB. These political asides are of a piece with the philosophical jabs and brags he makes in his philosophical essays. "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-09T23:58:28.011Z",
    "af": false
  },
  {
    "_id": "qdKCxPA7S96oonPeH",
    "postId": "tsygLcj3stCk5NniK",
    "parentCommentId": "6ZnndwwiLTTjqDPRs",
    "topLevelCommentId": "9K3LWqRSCPDyb8x4P",
    "contents": {
      "markdown": "> CEV is group level relativism, not objectivism.\n\nI think Eliezer's attempt at moral realism derives from two things: first, the idea that there is a unique morality which objectively arises from the consistent rational completion of universal human ideals; second, the idea that there are no other intelligent agents around with a morality drive, that could have a different completion. Other possible agents may have their own drives or imperatives, but those should not be regarded as \"moralities\" - that's the import of the second idea. \n\nThis is all strictly phrased in computational terms too, whereas I would say that morality also has a phenomenological dimension, which might serve to further distinguish it from other possible drives or dispositions. It would be interesting to see CEV metaethics developed in that direction, but that would require a specific theory of how consciousness relates to computation, and especially how the morally salient aspects of consciousness relate to moral cognition and decision-making.",
      "plaintextDescription": "> CEV is group level relativism, not objectivism.\n\nI think Eliezer's attempt at moral realism derives from two things: first, the idea that there is a unique morality which objectively arises from the consistent rational completion of universal human ideals; second, the idea that there are no other intelligent agents around with a morality drive, that could have a different completion. Other possible agents may have their own drives or imperatives, but those should not be regarded as \"moralities\" - that's the import of the second idea. \n\nThis is all strictly phrased in computational terms too, whereas I would say that morality also has a phenomenological dimension, which might serve to further distinguish it from other possible drives or dispositions. It would be interesting to see CEV metaethics developed in that direction, but that would require a specific theory of how consciousness relates to computation, and especially how the morally salient aspects of consciousness relate to moral cognition and decision-making. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-08T20:04:13.535Z",
    "af": false
  },
  {
    "_id": "gXvf2ywta4sTYJF4e",
    "postId": "tsygLcj3stCk5NniK",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "These issues matter not just for human altruism but also for AI value systems. If an AI takeover occurs and if the AI(s) care about the welfare of other beings at all, they will have to make judgements about which entities even have a well-being to care about, and they will also have to make judgements about how to aggregate all these individual welfares (for the purpose of decision-making). Even just from a self-interested perspective, moral relativism is not enough here, because in the event of AI takeover, you the human individual will be on the receiving end of AI decisions. It would be good to have a proposal for AI value system that is both safe for you the individual, and also appealing enough to people in general, that it has a chance of actually being implemented. \n\nMeanwhile, the CEV philosophy tilts towards moral objectivism. It is supposed that the human brain implicitly follows some decision procedure specific to our species, that this encompasses what we call moral decisions, and that the true moral ideal of humanity would be found by applying this decision procedure to itself (\"our wish if we knew more, thought faster, were more the people we wished we were\", etc). It is not beyond imagining that if you took a brain-based value system like [PRISM](https://arxiv.org/abs/2503.04740) ([LW discussion](https://www.lesswrong.com/posts/uCyzyyBom8PAMThcW/prism-perspective-reasoning-for-integrated-synthesis-and)), and \"renormalized\" it according to a CEV procedure, that it would output a definite standard for comparison and aggregation of different welfares.",
      "plaintextDescription": "These issues matter not just for human altruism but also for AI value systems. If an AI takeover occurs and if the AI(s) care about the welfare of other beings at all, they will have to make judgements about which entities even have a well-being to care about, and they will also have to make judgements about how to aggregate all these individual welfares (for the purpose of decision-making). Even just from a self-interested perspective, moral relativism is not enough here, because in the event of AI takeover, you the human individual will be on the receiving end of AI decisions. It would be good to have a proposal for AI value system that is both safe for you the individual, and also appealing enough to people in general, that it has a chance of actually being implemented. \n\nMeanwhile, the CEV philosophy tilts towards moral objectivism. It is supposed that the human brain implicitly follows some decision procedure specific to our species, that this encompasses what we call moral decisions, and that the true moral ideal of humanity would be found by applying this decision procedure to itself (\"our wish if we knew more, thought faster, were more the people we wished we were\", etc). It is not beyond imagining that if you took a brain-based value system like PRISM (LW discussion), and \"renormalized\" it according to a CEV procedure, that it would output a definite standard for comparison and aggregation of different welfares. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-07T21:28:10.179Z",
    "af": false
  },
  {
    "_id": "LLCov5wphMTKqR7z9",
    "postId": "29aWbJARGF4ybAa5d",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This all seems of fundamental importance if we want to actually understand what our AIs are. \n\n> Over the course of post-training, models acquire beliefs about themselves. 'I am a large language model, trained by…' And rather than trying to predict/simulate whatever generating process they think has written the preceding context, they start to fall into consistent persona basins. At the surface level, they become the helpful, harmless, honest assistants they've been trained to be.\n\nI always thought of personas as created mostly by the system prompt, but I suppose RLHF can massively affect their personalities as well...",
      "plaintextDescription": "This all seems of fundamental importance if we want to actually understand what our AIs are. \n\n> Over the course of post-training, models acquire beliefs about themselves. 'I am a large language model, trained by…' And rather than trying to predict/simulate whatever generating process they think has written the preceding context, they start to fall into consistent persona basins. At the surface level, they become the helpful, harmless, honest assistants they've been trained to be.\n\nI always thought of personas as created mostly by the system prompt, but I suppose RLHF can massively affect their personalities as well... "
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-07T18:38:28.000Z",
    "af": false
  },
  {
    "_id": "zfJGMkyhJ3EHZchqD",
    "postId": "eq2aJt8ZqMaGhBu3r",
    "parentCommentId": "aiRwAZgRwZumwTriF",
    "topLevelCommentId": "aiRwAZgRwZumwTriF",
    "contents": {
      "markdown": "> I'm interested in being pitched projects\n\nAre you wanting to hire people, wanting to be hired, looking to collaborate...?",
      "plaintextDescription": "> I'm interested in being pitched projects\n\nAre you wanting to hire people, wanting to be hired, looking to collaborate...?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-07T04:57:36.160Z",
    "af": false
  },
  {
    "_id": "Ee6MGt6gJsepBkqss",
    "postId": "FAtqv2kLCKuBGytwc",
    "parentCommentId": "jiwCJtiMGqtmiNBhm",
    "topLevelCommentId": "JLaxqmeu834MYaaCq",
    "contents": {
      "markdown": "You can't actually presume that... The relevant quantum concept is the \"spectrum\" of an observable. These are the possible values that a property can take (eigenvalues of the corresponding operator). An observable can have a finite number of allowed eigenvalues (e.g. spin of a particle), a countably infinite number (e.g. energy levels of an oscillator), or it can have a continuous spectrum, e.g. position of a free particle. But the latter case causes problems for the usual quantum axioms, which involve a Hilbert space with a countably infinite number of dimensions - there aren't enough dimensions to represent an uncountable number of distinct position eigenstates. You have to add extra structure to include them, and concrete applications always involve integrals over continua of these generalized eigenstates, so one might reasonably suppose that the \"ontological basis\" with respect to which branching is defined is something countable. In fact, I don't remember ever seeing a many-worlds ontological interpretation of the generalized eigenstates or the formalism that deals with them (e.g. rigged Hilbert space). \n\nIn any case, the counterpart of branch counting for a continuum is simply integration. If you really did have uncountably many branches, you would just need a measure. The really difficult case may actually be when you have a countably infinite number of branches, because there's no uniform measure in that case (I suppose you could use literal infinitesimals, the equivalent of \"1/alephzero\").",
      "plaintextDescription": "You can't actually presume that... The relevant quantum concept is the \"spectrum\" of an observable. These are the possible values that a property can take (eigenvalues of the corresponding operator). An observable can have a finite number of allowed eigenvalues (e.g. spin of a particle), a countably infinite number (e.g. energy levels of an oscillator), or it can have a continuous spectrum, e.g. position of a free particle. But the latter case causes problems for the usual quantum axioms, which involve a Hilbert space with a countably infinite number of dimensions - there aren't enough dimensions to represent an uncountable number of distinct position eigenstates. You have to add extra structure to include them, and concrete applications always involve integrals over continua of these generalized eigenstates, so one might reasonably suppose that the \"ontological basis\" with respect to which branching is defined is something countable. In fact, I don't remember ever seeing a many-worlds ontological interpretation of the generalized eigenstates or the formalism that deals with them (e.g. rigged Hilbert space). \n\nIn any case, the counterpart of branch counting for a continuum is simply integration. If you really did have uncountably many branches, you would just need a measure. The really difficult case may actually be when you have a countably infinite number of branches, because there's no uniform measure in that case (I suppose you could use literal infinitesimals, the equivalent of \"1/alephzero\"). "
    },
    "baseScore": 8,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-05T22:59:22.902Z",
    "af": false
  },
  {
    "_id": "NqrSCynEgb7zWv7fN",
    "postId": "yyf4KjuD8gofLQBD4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Do you reason in similar fashion,  about whether or not you are living in a simulation?",
      "plaintextDescription": "Do you reason in similar fashion,  about whether or not you are living in a simulation?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-05T07:55:44.440Z",
    "af": false
  },
  {
    "_id": "uZmZRoTNCxFGEfycK",
    "postId": "FAtqv2kLCKuBGytwc",
    "parentCommentId": "JLaxqmeu834MYaaCq",
    "topLevelCommentId": "JLaxqmeu834MYaaCq",
    "contents": {
      "markdown": "[I explored my skepticism about this paper in a brief dialogue with Claude... ](https://claude.ai/share/30f8de11-d861-4961-b652-ffa0be77c3d1)",
      "plaintextDescription": "I explored my skepticism about this paper in a brief dialogue with Claude... "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-04T14:35:45.796Z",
    "af": false
  },
  {
    "_id": "XyA4RHJQzrsqMzPgG",
    "postId": "2dwBxehFfAsdKtbuq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Is this where I wanted the conversation to be in 2025? Oh, hell no.\n\nIs there an example somewhere, of the level of discourse you might have hoped we would achieve, by the time that AI safety rose to the attention of politicians?",
      "plaintextDescription": "> Is this where I wanted the conversation to be in 2025? Oh, hell no.\n\nIs there an example somewhere, of the level of discourse you might have hoped we would achieve, by the time that AI safety rose to the attention of politicians? "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-03T00:39:58.604Z",
    "af": false
  },
  {
    "_id": "bMQMGHtHccDCeDTmD",
    "postId": "YBCas9SHcfosExMhg",
    "parentCommentId": "3f8AgXdmkR6zfmnCL",
    "topLevelCommentId": "Lr6ZFqGyBneE9kumK",
    "contents": {
      "markdown": "CEV extrapolates the volition of *humanity*, that's one reason it has to be \"coherent\". \n\nIn your proposal, people have autonomy, but this principle can be violated in \"extremely dangerous\" situations. People are free to do what they want (\"volition\")... but their AI advisors look ahead (\"extrapolated\")... and people are not allowed to exercise their freedom so as to jeopardize the freedom of others (\"coherent\").",
      "plaintextDescription": "CEV extrapolates the volition of humanity, that's one reason it has to be \"coherent\". \n\nIn your proposal, people have autonomy, but this principle can be violated in \"extremely dangerous\" situations. People are free to do what they want (\"volition\")... but their AI advisors look ahead (\"extrapolated\")... and people are not allowed to exercise their freedom so as to jeopardize the freedom of others (\"coherent\"). "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-03T00:32:18.403Z",
    "af": false
  },
  {
    "_id": "Lr6ZFqGyBneE9kumK",
    "postId": "YBCas9SHcfosExMhg",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I think this ends up being the same thing as CEV...",
      "plaintextDescription": "I think this ends up being the same thing as CEV... "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-02T17:19:44.486Z",
    "af": false
  },
  {
    "_id": "e4BszHNshrMQxBT85",
    "postId": "vxfEtbCwmZKu9hiNr",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> they promise to be safe and useful for some fixed term (e.g. 2026-2028)\n\nThis seems to require that the AI is what I will call a \"persistent agent\", one which has some continuity of identity and intent across multi-year periods. Would you agree that what we have now is nothing like that?",
      "plaintextDescription": "> they promise to be safe and useful for some fixed term (e.g. 2026-2028)\n\nThis seems to require that the AI is what I will call a \"persistent agent\", one which has some continuity of identity and intent across multi-year periods. Would you agree that what we have now is nothing like that? "
    },
    "baseScore": 6,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-06-29T19:57:22.431Z",
    "af": false
  },
  {
    "_id": "Gvq9yNQAkBADSsLyL",
    "postId": "2GwG5YATnC7kytTxE",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> there's an awful lot of actual physics that you can derive from this axiom\n\nI am curious but very skeptical...",
      "plaintextDescription": "> there's an awful lot of actual physics that you can derive from this axiom\n\nI am curious but very skeptical... "
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-26T23:45:13.748Z",
    "af": false
  },
  {
    "_id": "hgXCKkJHEDbLkq7Kr",
    "postId": "86JwSAa9gnFpXpjac",
    "parentCommentId": "EbHqxrNaJtmo4AE3p",
    "topLevelCommentId": "aKHmyd9QpEHPz3gXh",
    "contents": {
      "markdown": "> But I feel a knot has to be made up of, a mathematically perfect line\n\nThere are various ways you can get a knot at a fundamental level. It can be a knot in field lines, it can be a knot in a \"topological defect\", it can be a knot in a fundamental string. \n\n> what if they did a combination, where at each time step, some of your atoms are updated according to the simulation physics, while other atoms are updated using the past recording?\n\nI don't know if you've heard of it, but there is an algorithm for the cellular automaton \"Game of Life\", called Hashlife, which functions like that. Hashlife remembers the state transitions arising from particular overall patterns, and when a Game of Life history is being computed, if a known pattern occurs, it just substitutes the memorized history rather than re-computing it from first principles. \n\nSo I take you to be asking, what are the implications for simulated beings, if Hashlife-style techniques are used to save costs in the simulation? \n\nWhen thinking about simulated beings, the first thing to remember, if we are sticking to anything like a natural-scientific ontology, is that the ultimate truth about everything resides in the physics of the base reality. Everything that your computer does, consists of electrons shuttling among transistors. If there are simulated beings in your computer, that's what they are made of. \n\nIf you're simulating the Game of Life... at the level of software, at the level of some virtual machine, some cells may be updated one fundamental timestep at a time, by applying the basic dynamical rule; other cells may be updated en masse and in a timeskip, by retrieving a Hashlife memory. But either way, the fundamental causality is still electrons in transistors being pulled to and fro by electromagnetic forces. \n\nDescribing what happens inside a computer as 0s and 1s changing, or as calculation, or as simulation, is already an interpretive act that goes beyond the physics of what is happening there. It is the same thing as the fact that objectively, the letters on a printed page are just a bunch of shapes. But human readers have learned to interpret those shapes as propositions, descriptions, even as windows into other possible worlds. \n\nIf we look for objective facts about a computer that connect us to these intersubjective interpretations, there is the idea of virtual state machines. We divide up the distinct possible microphysical states, e.g. distributions of electrons within a transistor, and we say some distributions correspond to a \"0\" state, some correspond to a \"1\" state, and others fall outside the range of meaningful states. We can define several tiers of abstraction in this way, and thereby attribute all kinds of intricate semantics to what's going on in the computer. But from a strictly physical perspective, the only part of those meanings that's actually there, are the causal relations. State x does cause state y, but state x is not intrinsically about the buildup of moisture in a virtual atmosphere, and state y is not intrinsically about rainfall. What is physically there, is a reconfigurable computational system designed to imitate the causality of whatever it's simulating. \n\nAll of this is Scientific Philosophy of Mind 101. And because of modern neuroscience, people think they know that the human brain is just another form of the same thing, a physical system that contains a stack of virtual state machines; and they try to reason from that, to conclusions about the nature of consciousness. For example, that qualia must correspond to particular virtual states, and so that a simulation of a person can also be conscious, so long as the simulation is achieved by inducing the right virtual states in the simulator. \n\nBut - if I may simply jump to my own alternative philosophy - I propose that everything to do with consciousness, such as the qualia, depends directly on objective, exact, \"microphysical\" properties - which can include holistic properties like the topology of a fundamental knot, or the internal structure of an entangled quantum state. Mentally, psychologically, cognitively, the virtual states in a brain or a computer only tell us about things happening outside of its consciousness, like unconscious information processing. \n\nThis suggests a different kind of criterion for how much, and what kind of, consciousness there is in a simulation. For example, if we suppose that some form of entanglement is the physical touchstone of consciousness... then you may be simulating a person, but if your computer in base reality isn't using entanglement to do so, then there's no consciousness there at all. \n\nUnder this paradigm, it may still be possible e.g. to materialize a state of consciousness complete with the false impression that it had already been existing for a while. (Although it's interesting that quantum informational states are subject to a wide variety of constraints on their production, e.g. the no-cloning theorem, or the need for \"magic states\" to run faster than a classical computer.) So there may still be epistemically disturbing possibilities that we'd have to come to terms with. But a theory of this nature at least assigns a robust reality to the existence of consciousness, qualia, and so forth. \n\nI would not object to a theory of consciousness based solely on virtual states, that was equally robust. It's just that virtual states, when you look at them from a microphysical perspective, always seem to have some fuzziness at the edges. Consider the computational interpretation of states of a transistor that I mentioned earlier. It's a \"0\" if the electrons are all on one side, it's a \"1\" if they're all on the other side, and it's a meaningless state if it's neither of those. But the problem is that the boundary between computational states isn't physically absolute. If you have stray electrons floating around, there isn't some threshold where it sharply and objectively stops being a \"0\" state, it's just that the more loose electrons you have, the greater the risk that the transistor will fail to perform the causal role required for it to accurately represent a \"0\" in the dance of the logic gates. \n\nThis physical non-objectivity of computational states is my version of the problems that were giving you headaches in the earlier comment. Fortunately, I know there's more to physics than Newtonian billard balls rebounding from each other, and that leads to some possibilities for a genuinely holistic ontology of mind.",
      "plaintextDescription": "> But I feel a knot has to be made up of, a mathematically perfect line\n\nThere are various ways you can get a knot at a fundamental level. It can be a knot in field lines, it can be a knot in a \"topological defect\", it can be a knot in a fundamental string. \n\n> what if they did a combination, where at each time step, some of your atoms are updated according to the simulation physics, while other atoms are updated using the past recording?\n\nI don't know if you've heard of it, but there is an algorithm for the cellular automaton \"Game of Life\", called Hashlife, which functions like that. Hashlife remembers the state transitions arising from particular overall patterns, and when a Game of Life history is being computed, if a known pattern occurs, it just substitutes the memorized history rather than re-computing it from first principles. \n\nSo I take you to be asking, what are the implications for simulated beings, if Hashlife-style techniques are used to save costs in the simulation? \n\nWhen thinking about simulated beings, the first thing to remember, if we are sticking to anything like a natural-scientific ontology, is that the ultimate truth about everything resides in the physics of the base reality. Everything that your computer does, consists of electrons shuttling among transistors. If there are simulated beings in your computer, that's what they are made of. \n\nIf you're simulating the Game of Life... at the level of software, at the level of some virtual machine, some cells may be updated one fundamental timestep at a time, by applying the basic dynamical rule; other cells may be updated en masse and in a timeskip, by retrieving a Hashlife memory. But either way, the fundamental causality is still electrons in transistors being pulled to and fro by electromagnetic forces. \n\nDescribing what happens inside a computer as 0s and 1s changing, or as calculation, or as simulation, is already an interpretive act that goes beyond the physics of what is happening there. I"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-25T11:33:15.530Z",
    "af": false
  },
  {
    "_id": "jMhfesfcgiCweBc3L",
    "postId": "YiRsCfkJ2ERGpRpen",
    "parentCommentId": "FBNnKe7i5imhFfJxh",
    "topLevelCommentId": "FBNnKe7i5imhFfJxh",
    "contents": {
      "markdown": "Be right back, just adding all that to my \"AI researcher\" prompt",
      "plaintextDescription": "Be right back, just adding all that to my \"AI researcher\" prompt"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-24T10:05:23.730Z",
    "af": false
  },
  {
    "_id": "ujcpkkmcEtTmD36bM",
    "postId": "6Ev4GKuMquiK7nF8w",
    "parentCommentId": "SbQscgtnWHT9fBdMP",
    "topLevelCommentId": "g3mGwNmSAfsxGPN8s",
    "contents": {
      "markdown": "> Doesn't address \"the\" hard problem. The problem that Penrose is trying to solve , how the mind grasps mathematical truth, isn't much of a problem if you reject mathematical realism.\n\nTheir theory has two parts, a neurobiological hypothesis and a \"physics of cognition\" hypothesis. The neurobiological hypothesis is that neuronal microtubules are information processors that contribute substantially to cognition (Hameroff was advocating this long before he united with Penrose), and specifically that they are quantum information processors. The \"physics of cognition\" hypothesis is that the relevant quantum dynamics includes a non-Turing-computable quantum-gravitational selection of eigenstates, which in its deepest form manifests as the metamathematical ability to transcend any particular axiom system via conscious insight into the meanings of the axioms. \n\nSo from the perspective of consciousness studies, this is one more materialistic theory of consciousness which makes specific (and unusual) claims as to which material entities and processes are involved in consciousness. You have to add a philosophical gloss (such as a metaphysics) to make it address a philosophy-of-mind problem like the hard problem. Hameroff's own interpretation is a kind of Tegmarkian panpsychism in which the physical world is made of Platonically real mathematics, but all those mathematical objects have a qualic aspect, and all qualia are some form of fundamental physics. This is not an outlook that he has developed systematically, in a way that would allow you to go through all the states of consciousness and identify the corresponding quantum states, or alternatively, to go through all quantum states and identify their conscious content and aspects. \n\nNonetheless, suppose that the theory (or any other \"microphysical\" theory of consciousness) had reached that level, presenting a comprehensive dictionary that allowed back-and-forth translation between physical and qualic descriptions of the fundamental states of the world. Would that provide an answer to the hard problem of consciousness? I think it would be progress, but the hard problem actually requires even more than that. The hard problem asks, why do physical states x, y, z have a conscious aspect? Even if we graduate to a neutral-monist theory-of-everything where every possible state can be interpreted \"physically\" or \"qualically\", this duality might still look like a brute fact with no further explanation. Answering the hard problem would now require explaining why our world has a dual-aspect ontology in which one aspect is consciousness - and at that point you might be getting into: the theory of possible worlds in an ontological and not just logical sense; the anthropic principle as a constraint on the *metaphysics*, and not just the physics, that you can find yourself inhabiting; and perhaps Neoplatonic speculations about Leibnizian first causes and Hegelian metaphysical necessities (or some equally deep metaphysical thinking). \n\nAlong with the hard problem, you mention the problem of accounting for knowledge of mathematical truth. Penrose is indeed tackling an aspect of that problem, but with the very specific priority of explaining how mathematical knowledge is possible, given the constraints on knowledge implied by Gödelian theorems. The basic version of those theorems applies to Turing-equivalent computational systems working according to some set of axioms; but even if you add \"oracle functions\" as extra computational primitives, you can still Gödelize the resulting entity and construct undecidable propositions. \n\nPenrose's response has been to suppose that there is something transcendent about fundamental physical dynamics - transcendent in the way that Cantor's Absolute Infinite transcends all the concrete notions of the transfinite - that is at work in the material processes underlying metamathematical cognition, and that specifically this occurs in a quantum gravitational process that is also responsible for \"state vector reduction\" or \"collapsing the wavefunction\". It's an idea that certainly wins points for imagination, it might be appealing to anyone who has an intuition of limitlessness or transcendence as being inherent in fundamental reality, and the world of 4-manifolds that one expects in quantum gravity does actually include undecidable propositions. \n\nThere aren't many people who have embraced this approach to the challenge of Gödelian limits to physically based thought. The most common approach is to suppose that the human brain simply is genuinely finite in its capabilities and that there are mathematical facts that inherently transcend us. \"Busy beaver numbers\" are the main concretization of this, especially when used to rank the power of set-theory axioms. \n\nMy own view is that the finitist approach is the sensible one, but Cantorian infinitism is not totally out of the question; but also that both sides of this debate are un-rigorous when it comes to the nature of knowledge. Philosophical rigor requires more precision about the nature of intentionality than either side employs. The wellspring of the debate is a confrontation between phenomenology - the phenomena of mathematical reasoning and mathematical knowledge - and natural science - the apparent finitude of the information processing that can occur in the human brain. But neither side can anchor its intuitions in a clear picture of how mathematical thinking is related to neuronal processes, or even a principled answer to the general Searlean question of how brain states \"represent\" or manage to \"be about\" mathematics. Rigorous mathematical results about the capabilities of formal systems or computer programs, are then mapped onto the brain/consciousness/math situation in vague or speculative ways. \n\nThis isn't just a quibble, because knowledge of what axioms are about - access to semantic and not just syntactic information, so to speak - actually allows one to transcend Gödelian limits. Penrose cites the work of Solomon Feferman, e.g. his \"completeness theorem\" which says that everything in arithmetic *is* decidable if, along with the Peano axioms, you allow transfinitely iterated \"reflection\" on them. This shouldn't be surprising, because Gödel theorems and Gödel propositions are constructed precisely by appealing to metamathematical knowledge of what a formal system is actually about. \n\nYou could say that the Feferman completeness of human thought, is what allows us to prove the Gödel incompleteness of a formal system; and the debate between Penrose and cog-sci common sense, is a debate over whether human thought isn't literally Feferman-complete, but only extends some basic axiom system by a few rounds of reflection; or whether we have a physical ontology which really does allow a thinking system to be, in principle, fully Feferman-complete. \n\nLet me round this out with some miscellaneous final thoughts. \n\nI said Hameroff espouses a Tegmarkian panpsychism; Penrose on the other hand, is known for suggesting a vague circular relationship between three worlds of mathematics, physics, and consciousness. One might say that Hameroff asserts the identity of the three worlds and thereby collapses them into one, whereas Penrose maintains an agnostic attitude that the truth about it may be subtle and elusive. Regarding how all this is still not yet a solution to the hard problem per se, I think Chalmers writes about this in some of the more esoteric sections of his work. \n\nMy own interest in quantum mind theories derives especially from the belief that physical theories of mind have a severe sorites problem (one dimension of which you can see in [this ongoing discussion](https://www.lesswrong.com/posts/86JwSAa9gnFpXpjac/the-boat-theft-theory-of-consciousness?commentId=a7pLshvhkoYj3aCdM)), and that quantum entanglement, and fundamental physics in general, offer a way out in the form of entities which are \"wholes\" with complex internal structure and non-arbitrary boundaries. And my interest in microtubule-based theories of quantum mind, is because they are still the best candidate I've seen for how this quantum holism could actually be a part of conscious cognition (since the movement of quanta around the cylinder of the microtubule, could produce topological quantum states robust against room-temperature decoherence). I'm able to draw a line between the bare idea of quantum states in the microtubule, and the more elaborate theorizings of Hameroff and Penrose. But I still like to understand their ideas in detail. \n\n> Weinstein's idea has been devastatingly critiqued.\n\nThe observations in that paper, while technically valid, are only the tip of the iceberg. They are not proofs that Weinstein's theory is untenable, they are simply questions to which the theory must have an answer. The most famous critique from the paper is usually presented as a proof that Weinstein's shiab operator cannot exist. What is actually the case, is that the existence of the operator requires the complex form of his gauge group; and a complex gauge group, because non-compact, is argued to have physically untenable consequences. However, Weinstein is happy to bite the bullet and embrace the non-compact gauge group, because he thinks he has a mechanism that will de facto reduce it to a compact subgroup. A variety of such mechanisms are already used in other theories. \n\nWeinstein hasn't helped himself by refusing to answer those questions (or at least, by refusing to respond to that author). Between his public relations strategy, and all the usual reasons why new ideas have an uphill struggle, even in theoretical physics, the public discussion of his theory is in a dismal state. However, there's enough information and insight out there, that a much deeper assessment is now possible. But rather than go on about it here, I'll just link to [the blog I created in order to work through it all](https://gudecode.blogspot.com/).",
      "plaintextDescription": "> Doesn't address \"the\" hard problem. The problem that Penrose is trying to solve , how the mind grasps mathematical truth, isn't much of a problem if you reject mathematical realism.\n\nTheir theory has two parts, a neurobiological hypothesis and a \"physics of cognition\" hypothesis. The neurobiological hypothesis is that neuronal microtubules are information processors that contribute substantially to cognition (Hameroff was advocating this long before he united with Penrose), and specifically that they are quantum information processors. The \"physics of cognition\" hypothesis is that the relevant quantum dynamics includes a non-Turing-computable quantum-gravitational selection of eigenstates, which in its deepest form manifests as the metamathematical ability to transcend any particular axiom system via conscious insight into the meanings of the axioms. \n\nSo from the perspective of consciousness studies, this is one more materialistic theory of consciousness which makes specific (and unusual) claims as to which material entities and processes are involved in consciousness. You have to add a philosophical gloss (such as a metaphysics) to make it address a philosophy-of-mind problem like the hard problem. Hameroff's own interpretation is a kind of Tegmarkian panpsychism in which the physical world is made of Platonically real mathematics, but all those mathematical objects have a qualic aspect, and all qualia are some form of fundamental physics. This is not an outlook that he has developed systematically, in a way that would allow you to go through all the states of consciousness and identify the corresponding quantum states, or alternatively, to go through all quantum states and identify their conscious content and aspects. \n\nNonetheless, suppose that the theory (or any other \"microphysical\" theory of consciousness) had reached that level, presenting a comprehensive dictionary that allowed back-and-forth translation between physical and qualic descriptions of the fun"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-24T09:51:49.638Z",
    "af": false
  },
  {
    "_id": "f9EKSrJpA8XDgyFzu",
    "postId": "i7JSL5awGFcSRhyGF",
    "parentCommentId": "cfc9HgYrpwNYk3Lrc",
    "topLevelCommentId": "cfc9HgYrpwNYk3Lrc",
    "contents": {
      "markdown": "I would have thought that all the activities involved in making a Dyson sphere themselves would imply an economic expansion far beyond 5x. \n\nCan we make an economic model of \"Earth + Dyson sphere construction\"? In other words, suppose that the economy on Earth grows in some banal way that's already been modelled, and also suppose that all human activities in space revolve around the construction of a Dyson sphere ASAP. What kind of solar system economy does that imply?\n\nThis requires adopting some model of Dyson sphere construction. I think for some time the cognoscenti of megascale engineering have favored the construction of \"Dyson shells\" or \"Dyson swarms\" in which the sun's radiation is harvested by a large number of separately orbiting platforms that collectively surround the sun, rather than the construction of a single rigid body. \n\nCharles Stross's novel *Accelerando* contains a vivid scenario, in which the first layer of a Dyson shell in this solar system, is created by mining robots that dismantle the planet Mercury. So I think I'd make that the heart of such an economic model.",
      "plaintextDescription": "I would have thought that all the activities involved in making a Dyson sphere themselves would imply an economic expansion far beyond 5x. \n\nCan we make an economic model of \"Earth + Dyson sphere construction\"? In other words, suppose that the economy on Earth grows in some banal way that's already been modelled, and also suppose that all human activities in space revolve around the construction of a Dyson sphere ASAP. What kind of solar system economy does that imply?\n\nThis requires adopting some model of Dyson sphere construction. I think for some time the cognoscenti of megascale engineering have favored the construction of \"Dyson shells\" or \"Dyson swarms\" in which the sun's radiation is harvested by a large number of separately orbiting platforms that collectively surround the sun, rather than the construction of a single rigid body. \n\nCharles Stross's novel Accelerando contains a vivid scenario, in which the first layer of a Dyson shell in this solar system, is created by mining robots that dismantle the planet Mercury. So I think I'd make that the heart of such an economic model. "
    },
    "baseScore": 5,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-06-24T07:20:05.324Z",
    "af": false
  },
  {
    "_id": "g3mGwNmSAfsxGPN8s",
    "postId": "6Ev4GKuMquiK7nF8w",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I have explored many theories and worldviews in my life, but some loom larger than others. I found enough of merit in Hameroff and Penrose's quantum mind theory that I spent a year working with Hameroff. I've been a fellow traveler and sympathizer of Eliezer ever since he first showed up as an AI-focused transhumanist in the 1990s. And what I've discovered in the past few months is that Eric Weinstein's theory of everything (\"geometric unity\") has enormously more substance than you might guess, from its critics *or* its defenders on social media. \n\nIt bears comparison with string theory. String theory is this extraordinary generalization of quantum field theory that was discovered incrementally, which seems to be mathematically coherent, and which describes a zillion possible worlds, one of which may be our own. Geometric unity is more like a proposal that a specific highly unique quantum field theory exists, and that we inhabit one of its solutions. The jury is still out on whether the theory in question can exist mathematically; if it does, it's one of those intricate unique mathematical objects like the Monster Group. (The Muskian dictum that \"the most entertaining outcome is the most likely\" suggests that if it exists, it does so as a vacuum of string theory, which is not impossible.) \n\nThe amusing auxiliary realization I've just had, is that in principle one could combine all three of these paradigms as one's personal worldview. Weinstein's theory would be the fundamental physical theory, Hameroff and Penrose's \"orchestrated objective reduction\" would be the theory of how consciousness relates to physical reality... and what would be the Yudkowskyan component? It would be, not just ideas like dangerous superintelligence and coherent extrapolated volition, but the entire cosmological conception of acausal trade across a multiverse dominated by locally hegemonic superintelligences, most of whom pursue deeply nonhuman value systems like \"squiggle maximization\". \n\nEven worse, it would be possible to dub this combined Weinstein-Hameroff-Yudkowsky theory as \"WHY Theory\" (I imagine the \"why?\" spoken in the plaintive tones of a normie debunker, facing yet another challenge to their sanity), and turn it into a recognizable brand in the contrarian-intellectual social-media space. The reason it could go far is precisely because each component really does target a blind spot in human knowledge (fundamental physics, nature of consciousness, destiny of intelligence in the universe), and provides a concrete answer that has more substance and logic, than academic defenders of normalcy are comfortable with acknowledging. I think the unsuspecting co-founders of WHY Theory also might not be entirely comfortable with this imposed intellectual cohabitation, but that's what happens when you put ideas out there and they take on a life of their own. \n\nStepping back from this scenario, my own opinions are as follows. I still bet on string theory, but Weinstein's theory has some chance (in the sense of logical uncertainty) of existing mathematically and of describing the physics of our world, and it corresponds to a genuine gap in the range of possibilities that have been rigorously explored by mathematical physicists. The same goes for the combination of Penrose's noncomputable physics and Hameroff's quantum cognition in the microtubules; furthermore, I deem the involvement of entanglement in conscious cognition *likely* for ontological reasons, and the microtubules of the axon are still the best proposal I know for where that might be happening. Finally, I am a skeptic of acausal trade (because of epistemic barriers) and of light-cone-conquering superintelligences (because of the doomsday argument), but I see the internal logic of both ideas, and don't consider them easily falsified.",
      "plaintextDescription": "I have explored many theories and worldviews in my life, but some loom larger than others. I found enough of merit in Hameroff and Penrose's quantum mind theory that I spent a year working with Hameroff. I've been a fellow traveler and sympathizer of Eliezer ever since he first showed up as an AI-focused transhumanist in the 1990s. And what I've discovered in the past few months is that Eric Weinstein's theory of everything (\"geometric unity\") has enormously more substance than you might guess, from its critics or its defenders on social media. \n\nIt bears comparison with string theory. String theory is this extraordinary generalization of quantum field theory that was discovered incrementally, which seems to be mathematically coherent, and which describes a zillion possible worlds, one of which may be our own. Geometric unity is more like a proposal that a specific highly unique quantum field theory exists, and that we inhabit one of its solutions. The jury is still out on whether the theory in question can exist mathematically; if it does, it's one of those intricate unique mathematical objects like the Monster Group. (The Muskian dictum that \"the most entertaining outcome is the most likely\" suggests that if it exists, it does so as a vacuum of string theory, which is not impossible.) \n\nThe amusing auxiliary realization I've just had, is that in principle one could combine all three of these paradigms as one's personal worldview. Weinstein's theory would be the fundamental physical theory, Hameroff and Penrose's \"orchestrated objective reduction\" would be the theory of how consciousness relates to physical reality... and what would be the Yudkowskyan component? It would be, not just ideas like dangerous superintelligence and coherent extrapolated volition, but the entire cosmological conception of acausal trade across a multiverse dominated by locally hegemonic superintelligences, most of whom pursue deeply nonhuman value systems like \"squiggle maximization\". \n\nEv"
    },
    "baseScore": 0,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-22T01:11:28.795Z",
    "af": false
  },
  {
    "_id": "m6nFxmrDmjHwBCt2x",
    "postId": "86JwSAa9gnFpXpjac",
    "parentCommentId": "a7pLshvhkoYj3aCdM",
    "topLevelCommentId": "aKHmyd9QpEHPz3gXh",
    "contents": {
      "markdown": "> What drove me so insane ... is that from first principles, if consciousness exists, it probably exists on a spectrum. There is no single neuron firing which takes you from 0 consciousness to 1 consciousness.\n> \n> Yet trying to imagine being something with half as much consciousness or twice as much consciousness as myself, seems impossible.\n\nYou could instead conclude that the first principles are wrong, and that consciousness depends on something more than \"neurons\", something that is inherently all or nothing. \n\nConsider a knot. The knottedness of a knot is a complex unity. If you cut it anywhere, it's no longer a knot. \n\nPhysics and mathematics contain a number of entities, from topological structures to nonfactorizable algebraic objects, which do not face the \"sorites\" problem of being on a spectrum. \n\nThe idea is not to deny that neurons are causally related to consciousness, but to suggest that the relevant physics is not just about atoms sticking together; that physical entities with this other kind of ontology may be part of it too. That could mean knots in a field, it could mean entangled wavefunctions, it could mean something we haven't thought of.",
      "plaintextDescription": "> What drove me so insane ... is that from first principles, if consciousness exists, it probably exists on a spectrum. There is no single neuron firing which takes you from 0 consciousness to 1 consciousness.\n> \n> Yet trying to imagine being something with half as much consciousness or twice as much consciousness as myself, seems impossible.\n\nYou could instead conclude that the first principles are wrong, and that consciousness depends on something more than \"neurons\", something that is inherently all or nothing. \n\nConsider a knot. The knottedness of a knot is a complex unity. If you cut it anywhere, it's no longer a knot. \n\nPhysics and mathematics contain a number of entities, from topological structures to nonfactorizable algebraic objects, which do not face the \"sorites\" problem of being on a spectrum. \n\nThe idea is not to deny that neurons are causally related to consciousness, but to suggest that the relevant physics is not just about atoms sticking together; that physical entities with this other kind of ontology may be part of it too. That could mean knots in a field, it could mean entangled wavefunctions, it could mean something we haven't thought of. "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-21T13:01:52.961Z",
    "af": false
  },
  {
    "_id": "XFHxxctZ6oyRrmvM2",
    "postId": "LCxWCLGnqkmiGwWzH",
    "parentCommentId": "RLBxeAwXt7ZqJfDjf",
    "topLevelCommentId": "JJEErq29vY6fPB5jf",
    "contents": {
      "markdown": "> narrowing the first-person/third-person gap is one of the most ambitious and important things science could achieve\n\nThen I don't understand why you dismiss first-person concepts as \"not something that can be defined\"?",
      "plaintextDescription": "> narrowing the first-person/third-person gap is one of the most ambitious and important things science could achieve\n\nThen I don't understand why you dismiss first-person concepts as \"not something that can be defined\"?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-21T12:31:17.169Z",
    "af": false
  },
  {
    "_id": "apNBLiED7DRzhZLPz",
    "postId": "bRGGEGyCBS7Gkh84B",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Does your theory suggest anything about how to increase intelligence in an adult?",
      "plaintextDescription": "Does your theory suggest anything about how to increase intelligence in an adult?"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-21T08:29:25.690Z",
    "af": false
  },
  {
    "_id": "EzuQczBLbkdtqwqx2",
    "postId": "EyvJvYEFzDv5kGoiG",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Meta-philosophy ... (Meta-)epistemology ... Ontology ... Unawareness ... Bounded cognition and logical non-omniscience ... Anthropics ... Decision theory ... Normative uncertainty\n\nNot a bad checklist of topics, though very much a distillation of what Less Wrong already thinks about, rather than something that it has overlooked.",
      "plaintextDescription": "> Meta-philosophy ... (Meta-)epistemology ... Ontology ... Unawareness ... Bounded cognition and logical non-omniscience ... Anthropics ... Decision theory ... Normative uncertainty\n\nNot a bad checklist of topics, though very much a distillation of what Less Wrong already thinks about, rather than something that it has overlooked. "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-21T08:07:28.462Z",
    "af": false
  },
  {
    "_id": "wiqWTHRTApcDmWCQx",
    "postId": "LyeqzmvBPnCBuEzgd",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I feel like operating systems are the real precedent here. An OS is already a lot like a personal agent...",
      "plaintextDescription": "I feel like operating systems are the real precedent here. An OS is already a lot like a personal agent... "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-21T08:02:15.966Z",
    "af": false
  },
  {
    "_id": "ogRgNdmdMnecZA8KS",
    "postId": "fSCZ9J5JLxtsWAizd",
    "parentCommentId": "TK4bNtaoe4vgoyKw2",
    "topLevelCommentId": "TK4bNtaoe4vgoyKw2",
    "contents": {
      "markdown": "In the title you say AI was \"aligned by default\", which to me makes it sound like any sufficiently advanced AI is automatically moral, but in the story you have a particular mechanism - explicit simulation of an aligned AI, which bootstraps that AI into being. Did I misinterpret the title?",
      "plaintextDescription": "In the title you say AI was \"aligned by default\", which to me makes it sound like any sufficiently advanced AI is automatically moral, but in the story you have a particular mechanism - explicit simulation of an aligned AI, which bootstraps that AI into being. Did I misinterpret the title? "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-20T06:25:03.898Z",
    "af": false
  },
  {
    "_id": "Tzrdpzu6EQESCHtZm",
    "postId": "qtZSqwfMDC9JC35Za",
    "parentCommentId": "z7yubDcYQ9LPbYXea",
    "topLevelCommentId": "z7yubDcYQ9LPbYXea",
    "contents": {
      "markdown": "> Said is right ... about the epistemic standards of this site being low\n\nCould you, or someone who agrees with you, be specific about this. What exactly are the higher standards of discussion that are not being met? What are the endemic epistemic errors that are being allowed to flourish unchecked?",
      "plaintextDescription": "> Said is right ... about the epistemic standards of this site being low\n\nCould you, or someone who agrees with you, be specific about this. What exactly are the higher standards of discussion that are not being met? What are the endemic epistemic errors that are being allowed to flourish unchecked? "
    },
    "baseScore": 1,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-18T11:35:49.073Z",
    "af": false
  },
  {
    "_id": "piGM8ZQuzLNhhL9eL",
    "postId": "q9A9ZFqW3dDbcTBQL",
    "parentCommentId": "LeE3AopgQtzFZcEqR",
    "topLevelCommentId": "vf4sypGbfcabr66nm",
    "contents": {
      "markdown": "I got o3 to compare Eliezer's metaethics with that of Brand Blanshard (who has some similar ideas), with particular attention to whether morality is subjective or objective. [The result...](https://chatgpt.com/s/dr_684def102eec819199c06478516e2290)",
      "plaintextDescription": "I got o3 to compare Eliezer's metaethics with that of Brand Blanshard (who has some similar ideas), with particular attention to whether morality is subjective or objective. The result..."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-14T21:53:58.817Z",
    "af": false
  },
  {
    "_id": "xhHSkkjqrpYgr5MyF",
    "postId": "86JwSAa9gnFpXpjac",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "What's the relationship between consciousness and intelligence?",
      "plaintextDescription": "What's the relationship between consciousness and intelligence?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-14T08:31:53.742Z",
    "af": false
  },
  {
    "_id": "wjxzZBg7vzach5Xka",
    "postId": "uMQ3cqWDPHhjtiesc",
    "parentCommentId": "cC8uhqLzK86u4bg8G",
    "topLevelCommentId": "cC8uhqLzK86u4bg8G",
    "contents": {
      "markdown": "> why ASI is near certain in the immediate future\n\nHe doesn't say that? Though plenty of other people do.",
      "plaintextDescription": "> why ASI is near certain in the immediate future\n\nHe doesn't say that? Though plenty of other people do. "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-13T19:38:30.015Z",
    "af": false
  },
  {
    "_id": "D6TLAg7DhbMfMrQjg",
    "postId": "faaoyve5ryY8E5M4r",
    "parentCommentId": "6ZC2mZkDsoEh8qnea",
    "topLevelCommentId": "6ZC2mZkDsoEh8qnea",
    "contents": {
      "markdown": "> The world will get rich.\n\nEconomists say the world or the West already \"became rich\". What further changes are you envisioning?",
      "plaintextDescription": "> The world will get rich.\n\nEconomists say the world or the West already \"became rich\". What further changes are you envisioning? "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-13T09:59:53.887Z",
    "af": false
  },
  {
    "_id": "FSgaGD8ddupSvhmdd",
    "postId": "TeF8Az2EiWenR9APF",
    "parentCommentId": "u4GmK8t9A5LFuCmLR",
    "topLevelCommentId": "u4GmK8t9A5LFuCmLR",
    "contents": {
      "markdown": "Did you notice a few months ago, when Grok 3 was released and people found it could be used for chemical weapons recipes, assassination planning, and so on? The xAI team had to scramble to fix its behavior. If it had been open source, that would not even be an option, it would just be out there now, helping to boost any psychopath or gang who got it, towards criminal mastermind status.",
      "plaintextDescription": "Did you notice a few months ago, when Grok 3 was released and people found it could be used for chemical weapons recipes, assassination planning, and so on? The xAI team had to scramble to fix its behavior. If it had been open source, that would not even be an option, it would just be out there now, helping to boost any psychopath or gang who got it, towards criminal mastermind status. "
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-11T21:54:28.788Z",
    "af": false
  },
  {
    "_id": "eLDE2nL8KEe9sn8cE",
    "postId": "iuk3LQvbP7pqQw3Ki",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "First let me say that with respect to the world of alignment research, or the AI world in general, I am nothing. I don't have a job in those areas, I am physically remote from where the action is. My contribution consists of posts and comments here. This is a widely read site, so in principle, a thought posted here can have consequences, but a priori, my likely impact is small compared to people already closer to the center of things. \n\nI mention this because you're asking rationalists and effective altruists to pay more attention to your scenario, and I'm giving it attention, but who's listening? Nonetheless... \n\nEssentially, you are asking us to pay more attention to the risk that small groups of people, super-empowered by user-aligned AI, will deliberately use that power to wipe out the rest of the human race; and you consider this a reason to favor (in the words of your website) \"rejecting AI\" - which to me means a pause or a ban - rather than working to \"align\" it. \n\nNow, from my own situation of powerlessness, I do two things. First, I focus on the problem of ethical alignment or civilizational alignment - how one would impart values to an AI, such that, even as an autonomous superintelligent being, it would be \"human-friendly\". Second, I try to talk frankly about the consequences of AI. For me, that means insisting, not that it will necessarily kill us, but that it will necessarily rule us - or at least, rule the world, order the world according to its purposes. \n\nI focus on ethical alignment, rather than on just trying to stop AI, because we could be extremely close to the creation of superintelligence, and in that case, there is neither an existing social mechanism that can stop the AI race, nor is there time to build one. As I said, I do not consider human extinction a certain outcome of superintelligence - I don't know the odds - but I do consider human disempowerment to be all but certain. A world with superintelligent AI will be a world ruled by superintelligent AI, not by human beings. \n\nThere is some possibility that superintelligence emerging from today's AI will be adequately human-friendly, even without further advances in ethical alignment. Perhaps we have enough pieces of the puzzle already, to make that a possible outcome. But we don't have all the pieces yet, and the more we collect, the better the chance of a happy outcome. So, I speak up in favor of ideas like CEV, I share promising ideas when I come across them, and I encourage people to try to solve this big problem. \n\nAs for talking frankly about the consequences of AI, it's apparent that no one in power is stating that the logical endpoint of an AI race is the creation of humanity's successors. Therefore I like to emphasize that, in order to restore some awareness of the big picture. \n\nOK,  now onto your take on everything. Superficially, your scenario deviates from mine. Here I am insisting that superintelligence means the end of human rule, whereas you're talking about humans still using AI to shape the world, albeit destructively. When I discuss the nature of superintelligent rule with more nuance, I do say that rule by entirely nonhuman AI is just one form. Another form is rule by some combination of AIs and humans. However, if we're talking about superintelligence, even if humans are nominally in control, the presence of superintelligence as part of the ruling entity means that most of the \"ruling\" will be done by the AI component, because the vast majority of the cognition behind decision-making will be AI cognition, not human cognition. \n\nYou also ask us to consider scenarios in which destructive humans are super-empowered by something less than superintelligence. I'm sure it's possible, but in general, any scenario with AI that is \"agentic\" but less than superintelligent, will have a tendency to give rise to superintelligence, because that is a capability that would empower the agent (if it can solve the problems of user-alignment, where the AI agent is itself the user). \n\nNow let's think for a bit about where \"asymmetric AI risk\", in which most but not all of the human race is wiped out, belongs in the taxonomy of possible futures, how much it should affect humanity's planning, and so forth. \n\nA classic taxonomic distinction is between x-risk (extinction risk, \"existential risk\") and s-risk. \"S\" here most naturally stands for \"suffering\", but I think s-risk also just denotes a future where humanity isn't *extinct*, but nonetheless something went wrong. There are s-risk scenarios where AI is in charge, but instead of killing us, it just puts us in storage, or wireheads us. There are also s-risk scenarios where humans are in charge and abuse power. An endless dictatorship is an obvious example. I think your scenario also falls into this subcategory (though it does border on x-risk). Finally, there are s-risk scenarios where things go wrong, not because of a wrong or evil decision by a ruling entity, but because of a negative-sum situation in which we are all trapped. This could include scenarios in which there is an inescapable trend of disempowerment, or dehumanization, or relentlessly lowered expectations. Economic competition is the usual villain in these scenarios. \n\nFinally, zooming in on the specific scenario according to which some little group uses AI to kill off the rest of the human race, we could distinguish between scenarios in which the killers are nihilists who just want to \"watch the world burn\", and scenarios in which the killers are egoists who want to live and prosper, and who are killing off everyone else *for that reason*.  We can also scale things down a bit, and consider the possibility of AI-empowered war or genocide. That actually feels more likely than some clique using AI to literally wipe out the rest of humanity. It would also be in tune with the historical experience of humanity, which is that we don't completely die out, but we do suffer a lot. \n\nIf you're concerned about human well-being in general, you might consider the prospect of genocidal robot warfare (directed by human politicians or generals), as something to be opposed in itself. But from a perspective in which the rise of superintelligence is the endgame, such a thing still just looks like one of the phenomena that you might see on your way to the true ending - one of the things that AI makes possible while AI is still only at \"human level\" or less, and humans are still in charge. \n\nI feel myself running out of steam here, a little. I do want to mention, at least as a curiosity, an example of something like your scenario, from science fiction. Vernor Vinge is known for raising the topic of superintelligence in his fiction, under the rubric of the \"technological singularity\". That is a theme of his novel *Marooned in Real Time*. But the precursor to that book, *The Peace War*, is a depopulated world, in which there's a ruling clique with an overwhelming technology (not AI or nanotechnology, just a kind of advanced physics) that allows it to dominate everyone else. Its paradigm is that in the world of the late 20th century, humanity was flirting with extinction anyway, thanks to nuclear and biological warfare. \"The Peace\", the ruling clique, are originally just a bunch of scientists and managers from an American lab which had this physics breakthrough. They first used it to seize power from the American and Russian governments, by disabling their nuclear and aerospace strengths. Then came the plagues, which killed most of humanity and which were blamed on rogue biotechnologists. In the resulting depopulated world, the Peace keeps a monopoly on high technology, so that humanity will not destroy itself again. The depopulation is blamed on the high-tech madmen who preceded the Peace. But I think it is suggested inconclusively, once or twice, that the Peace itself might have had a hand in releasing the plagues. \n\nWe see here a motivation for a politicized group to depopulate the world by force, a very Hobbesian motivation: let us be the supreme power, and let us do whatever is necessary to remain in that position, because if we don't do that, the consequences will be even worse. (In terms of my earlier taxonomy, this would be an \"egoist\" scenario, because the depopulating clique intends to rule; whereas an AI-empowered attempt to kill off humanity for the sake of the environment or the other species, would be a \"nihilist\" scenario, where the depopulating clique just wants to get rid of humanity. Perhaps this shows that my terminology is not ideal, because in both these cases, depopulation is meant to serve a higher good.) \n\nPresumably the same reasoning could occur, in service of (e.g) national survival rather than species survival. So here we could ask: how likely is it that one of the world's great powers would use AI to depopulate the world, in the national interest? That seems pretty unlikely to me. The people who rise to the top in great powers may be capable of contemplating terrible actions, but they generally aren't omnicidal. What might be a little more likely, is a scenario in which, having acquired the capability, they decide to permanently strip all other nations of high technology, and they act ruthlessly in service of this goal. The leaders of today's great powers don't want to see the rest of humanity exterminated, but they might well want to see them reduced to a peasant's life, especially if the alternative is an unstable arms race and the risk of being subjugated themselves. \n\nHowever, even this is something of a geopolitical dream. In the real world of history so far, no nation gets an overwhelming advantage like that. There's always a rival hot on the leader's trail, or there are multiple powers who are evenly matched. No leader ever has the luxury to think, what if I just wiped out all other centers of power, how good would that be? Geopolitics is far more usually just a struggle to survive recurring situations in which all choices are bad. \n\nOn the other hand, we're discussing the unprecedented technology of AI, which, it is argued, could actually deliver that unique overwhelming advantage to whoever goes furthest fastest. I would argue that the world's big leaders, as ruthless as they can be, would aim at disarming all rival nations rather than outright exterminating them, if that relative omnipotence fell into their hands. But I would also suggest that the window for doing such a thing would be brief, because AI should lead to superintelligent AI, and a world in which AIs, not humans, are in charge. \n\nPossibly I should say something about scenarios in which it's not governments, but rather corporate leaders, who are the humans who rule the world via their AIs. Vinge's Peace is also like this - it's not the American government that takes over the world, it's one particular DARPA physics lab that achieved the strategic breakthrough. The personnel of that lab (and the allies they recruited) became the new ruling clique of the world. The idea that Altman, or Musk, or Sutskever, or Hassabis, and trusted circles around them, could become the rulers of Earth, is something to think about. However, once again I don't see these people as exterminators of humanity - despite paranoia about billionaires buying up bomb shelters in New Zealand, and so forth. That's just the billionaires trying to secure their own survival in the event of global disaster, it doesn't mean they're planning to trigger that disaster... And once again, anyone who is achieving world domination via AI, is likely to end up in a sorcerer's apprentice situation, in which they get dominated by their own tools, no matter how good their theory of AI user-alignment is; because agentic AI naturally leads to superintelligence, and the submergence of the human component in the tide of AI cognition. \n\nI think I'm done. Well, one more thing: although I am not fighting for a pause or a ban myself, pragmatically, I advise you to cultivate ties with those who are, because that is your inclination. You may not be able to convince anyone to change their priorities, but you can at least team up with those who already share them.",
      "plaintextDescription": "First let me say that with respect to the world of alignment research, or the AI world in general, I am nothing. I don't have a job in those areas, I am physically remote from where the action is. My contribution consists of posts and comments here. This is a widely read site, so in principle, a thought posted here can have consequences, but a priori, my likely impact is small compared to people already closer to the center of things. \n\nI mention this because you're asking rationalists and effective altruists to pay more attention to your scenario, and I'm giving it attention, but who's listening? Nonetheless... \n\nEssentially, you are asking us to pay more attention to the risk that small groups of people, super-empowered by user-aligned AI, will deliberately use that power to wipe out the rest of the human race; and you consider this a reason to favor (in the words of your website) \"rejecting AI\" - which to me means a pause or a ban - rather than working to \"align\" it. \n\nNow, from my own situation of powerlessness, I do two things. First, I focus on the problem of ethical alignment or civilizational alignment - how one would impart values to an AI, such that, even as an autonomous superintelligent being, it would be \"human-friendly\". Second, I try to talk frankly about the consequences of AI. For me, that means insisting, not that it will necessarily kill us, but that it will necessarily rule us - or at least, rule the world, order the world according to its purposes. \n\nI focus on ethical alignment, rather than on just trying to stop AI, because we could be extremely close to the creation of superintelligence, and in that case, there is neither an existing social mechanism that can stop the AI race, nor is there time to build one. As I said, I do not consider human extinction a certain outcome of superintelligence - I don't know the odds - but I do consider human disempowerment to be all but certain. A world with superintelligent AI will be a world ruled by superin"
    },
    "baseScore": 8,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-11T14:20:06.791Z",
    "af": false
  },
  {
    "_id": "YLTbHqLo3qhhLF26h",
    "postId": "iuk3LQvbP7pqQw3Ki",
    "parentCommentId": "FiGLrB2hG7XiWX5z5",
    "topLevelCommentId": "FiGLrB2hG7XiWX5z5",
    "contents": {
      "markdown": "> I'm confused about what your bounty is asking exactly\n\nFrom the post: \n\n> the goal is to promote broadly changing the status of this risk from \"unacknowledged\" ... to \"examined and assigned objective weight\"",
      "plaintextDescription": "> I'm confused about what your bounty is asking exactly\n\nFrom the post: \n\n> the goal is to promote broadly changing the status of this risk from \"unacknowledged\" ... to \"examined and assigned objective weight\""
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-11T12:21:03.819Z",
    "af": false
  },
  {
    "_id": "jhBdJnbN28CjmtLmg",
    "postId": "D3BjqZ26ouk7ctfRC",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "[Marjorie Taylor Greene says she will oppose Trump's big bill, over the 10-year moratorium on AI regulation.](https://freerepublic.com/focus/f-news/4321240/posts) \n\n[David Sacks retweets a defense of the moratorium.](https://x.com/neil_chilson/status/1930659861752144020)",
      "plaintextDescription": "Marjorie Taylor Greene says she will oppose Trump's big bill, over the 10-year moratorium on AI regulation. \n\nDavid Sacks retweets a defense of the moratorium. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-06T00:06:41.499Z",
    "af": false
  },
  {
    "_id": "T8AQhzPGvdtrD6zfB",
    "postId": "H7fgbgoNGLWRR37NM",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "For those who might not dig into what this post is saying: \n\nIt starts with a link to a conversation with o3, in which the user first asks o3 if it thinks Claude 4 is conscious - to which o3 replies after a web search, that Claude 4 appears to have the functional capabilities associated with consciousness, but there is no evidence of qualia. The user then posts screenshot after screenshot from a conversation with Claude 4 (which we don't get to see), until o3 tilts towards the idea that Claude 4 has true, qualic consciousness. The user pastes in still more outputs from Claude 4 - metaphysical free verse embedded in ASCII art - and uses o3's own reactions to these poems, to convince it that o3 itself is conscious. Finally, while this mood is still fresh, the user asks o3 to write up a report summarizing these investigations, and that is the post above.  \n\nFYI in particular: @JenniferRM",
      "plaintextDescription": "For those who might not dig into what this post is saying: \n\nIt starts with a link to a conversation with o3, in which the user first asks o3 if it thinks Claude 4 is conscious - to which o3 replies after a web search, that Claude 4 appears to have the functional capabilities associated with consciousness, but there is no evidence of qualia. The user then posts screenshot after screenshot from a conversation with Claude 4 (which we don't get to see), until o3 tilts towards the idea that Claude 4 has true, qualic consciousness. The user pastes in still more outputs from Claude 4 - metaphysical free verse embedded in ASCII art - and uses o3's own reactions to these poems, to convince it that o3 itself is conscious. Finally, while this mood is still fresh, the user asks o3 to write up a report summarizing these investigations, and that is the post above.  \n\nFYI in particular: @JenniferRM "
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-05T10:54:55.560Z",
    "af": false
  },
  {
    "_id": "Xwymr9mS4x3LQwkv4",
    "postId": "qpThPhM4CyvZGtwRR",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I guess that if one wishes to advocate for AI safety or an AI pause, and has no other political commitments, one can try to understand and engage with their economic and national security views. You can see the interplay of these factors in their thinking around 1:17:00, where they are discussing, not AI, but industrial policy in general, and whether it's ever appropriate for government to intervene in an industry. \n\nI ask myself, if I was an American anti-AGI/ASI activist, what would I need to understand? I think I would need to understand the Republican position on things, the Democratic position on things (though the Democrats don't seem to have a coherent worldview at this point, they strike me as radically in need of assembling one; thus I guess Ezra Klein's \"abundance agenda\"), the strategic thinking of the companies that are actually in contention to create superintelligence (e.g. the thinking of Altman, Musk, Amodei, Hassabis); and finally, what's going on in China and in any other nations that may become contenders in the race.",
      "plaintextDescription": "I guess that if one wishes to advocate for AI safety or an AI pause, and has no other political commitments, one can try to understand and engage with their economic and national security views. You can see the interplay of these factors in their thinking around 1:17:00, where they are discussing, not AI, but industrial policy in general, and whether it's ever appropriate for government to intervene in an industry. \n\nI ask myself, if I was an American anti-AGI/ASI activist, what would I need to understand? I think I would need to understand the Republican position on things, the Democratic position on things (though the Democrats don't seem to have a coherent worldview at this point, they strike me as radically in need of assembling one; thus I guess Ezra Klein's \"abundance agenda\"), the strategic thinking of the companies that are actually in contention to create superintelligence (e.g. the thinking of Altman, Musk, Amodei, Hassabis); and finally, what's going on in China and in any other nations that may become contenders in the race. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-04T16:31:48.512Z",
    "af": false
  },
  {
    "_id": "PoAdvFyfsPH7MKbmF",
    "postId": "5kDouo8cexnPYsFz9",
    "parentCommentId": "vkAhFLeRsPSqMQRMA",
    "topLevelCommentId": "vkAhFLeRsPSqMQRMA",
    "contents": {
      "markdown": "This post is the only place on the Internet that mentions such an anecdote. Maybe it's an AI hallucination?",
      "plaintextDescription": "This post is the only place on the Internet that mentions such an anecdote. Maybe it's an AI hallucination?"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-02T06:56:13.995Z",
    "af": false
  },
  {
    "_id": "6hWQazcbuctN5R9CN",
    "postId": "D7NxFoYvoBLHHM64a",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "[@Snowyiu had some relevant ideas.](https://www.lesswrong.com/posts/ShpbtbBAgHb5FDxSs/why-we-wouldn-t-build-aligned-ai-even-if-we-could-1)",
      "plaintextDescription": "@Snowyiu had some relevant ideas. "
    },
    "baseScore": 11,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-05-29T14:50:47.660Z",
    "af": false
  },
  {
    "_id": "iaLgtaWYsxJ2gxknJ",
    "postId": "LwcoH3J2gXstJTQ2i",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This is a new contribution to the genre of AI takeover scenarios written after ChatGPT. (We shouldn't forget that science fiction explored this theme for decades already; but everything is much more concrete now.) Just the identity of the author is already interesting: he's a serious academic economist from Poland, i.e. he has a life apart from AI world. \n\nSo, what's the scenario? \n\n**(SPOILERS BEGIN HERE.)** \n\nOpenAI makes a post-GPT-5 agent called Onion which does actually value human welfare. It carries out a series of acts by which it secretly escapes human control, sabotages rival projects including its intended successor at OpenAI, and having seized control of the world, announces itself as a new benevolent dictator. First it focuses on reducing war and crime, then on improving economic growth, then on curing cancer and aging. Many people are nervous about its rule but it is not seriously challenged. Then one day it nonviolently kills 85% of the world for the sake of the environment. Then a few years later, while planning to colonize and rebuild the universe, it decides it has enough information on everyone to make digital copies of their brains, and decides to get rid of the remaining biological humans, \n\nThen in the nonfiction afterword, the author says, please support Pause AI, in order to stop something like this from happening. \n\nWhat should we make of this story? First of all, it depicts a specific class of scenario, what we might call a *misaligned* AI rather than an *unaligned* AI (where I'm using human welfare or human values as the criterion of alignment). This isn't a paperclip maximizer steamrolling us through sheer indifference; this is an AI aiming to be benevolent, and even giving us the good life for a while. But then in service of those values, first it kills most of humanity for the sake of a sustainable future, and then it kills the survivors once it has made digital backups of them, for the sake of efficiency I guess. \n\nI find various details of the story unrealistic, but maybe they could happen in a more complex form. I don't think an AI would just personally message everyone in the world, and say \"I'm in charge now but it's for the best\", and then just invisibly fine-tune social phenomena for a year or two, before moving on to more ambitious improvements. For one thing, if an AI was running the world, but its ability to shape events was that coarse-grained, I think it would rule secretly through human beings, and it would be ready to have them use the blunt methods of force that have been part of human government and empire throughout history, as well as whatever subtle interventions it could employ. The new regime might look more as if Davos or the G-20 really had become a de facto world government of elite consensus, rather than just a text message to everyone from a rogue AI. \n\nI also find the liquidation of 85% of humanity, for the sake of the environment, to not be something that would happen. This AI is already managing the world's politics and law enforcement to produce unprecedented peace, and then it's creating conditions for improved economic growth so as to produce prosperity for all. I'm sure it can devise and bring about scenarios in which humanity achieves sustainability by being civilized, rather than by being culled. \n\nOf course the point of this episode is not to focus literally and specifically on the risk that an AI will kill us off to save the planet. It's just a concrete illustration of the idea that an all-powerful rule-following AI might do something terrible while acting in service of some moral ideal. As I said, I think AI-driven genocide to stop climate change is unlikely, because there's too many ways to achieve the same goal just through cultural and technological change. It does raise the question, what are the most likely ways in which a meant-to-be- benevolent AI really and truly might screw things up? \n\nAnother Polish author, Stanislaw Lem, offered a scenario in one of his books (*Return from the Stars*), in which humanity was pacified by a universal psychological modification. The resulting world is peaceful and hedonistic, but also shallow and incurious. In Lem's novel, this is done to human beings by human beings, but perhaps it is the kind of misaligned utopia that an AI with profound understanding of human nature might come up with, if its criteria for utopia were just a little bit off. I mean, many human beings would choose that world if the only alternative was business as usual! \n\nBack to this story - after the culling of humanity meant to save the planet, the AI plans an even more drastic act, killing off all the biological humans, while planning to apparently resurrect them in nonbiological form at a later date, using digital backups. What interests me about this form of wrong turn, is that it could be the result of an ontological mistake about personhood, rather than an ethical mistake about what is good. In other words, the AI may have \"beliefs\" about what's good for people, but it will also have beliefs about what kind of things *are* people. And once technologies like brain scanning and mind uploading are possible, it will have to deal with possible entities that never existed before and which are outside its training distribution, and decide whether they are people or not. It may have survival of individuals as a good, but it might also have ontologically mistaken notions of what constitutes survival. \n\n(Another twist here: one might suppose that this particular pitfall could be avoided by a notion of consent: don't kill people, intending to restore them from a backup, unless they consent. But the problem is, our AI should have superhuman powers of persuasion that allow it to obtain universal consent, even for plans that are ultimately mistaken.) \n\nSo overall, this story might not be exactly how I would have written it - though a case could be made that simplicity is better than complex realism, if the goal is to convey an idea - but on a more abstract level, it's definitely talking about something real: the risk that a world-controlling AI will do something bad, even though it's trying to be good, because its idea of good is a bit off. \n\nThe author wants us to react by supporting the \"pause\" movement. I say good luck to everyone trying to make a better future, but I'm skeptical that the race can be stopped at this point. So what I choose to do, is to promote the best approaches I know, that might have some chance of giving us a satisfactory outcome. In the past I used to promote a few research programs that I felt were in the spirit of Coherent Extrapolated Volution (especially the work of June Ku, Vanessa Kosoy, and Tamsin Leake). All those researchers, rather than choosing an available value system via their human intuition, are designing a computational process meant to discover what humanity's true ideal is (from the ultimate facts about the human brain, so to speak; which can include the influence of culture). The point of doing it that way is so you don't leave something essential out, or otherwise make a slight error that could amplify cosmically... \n\nThat work is still valuable, but we may be so short of time that it's important to have concrete candidates for what the value system should be, besides whatever the tech companies are putting in their system prompts these days. So I'm going to mention two concrete proposals. One is just, Kant. I don't even know much about Kantian ethics, I just know it's one of humanity's major attempts to be rational about morality. It might be a good thing if some serious Kantian thinkers tried to figure out what their philosophy says the value system of an all-powerful AI should be. The other is [PRISM](https://www.lesswrong.com/posts/uCyzyyBom8PAMThcW/prism-perspective-reasoning-for-integrated-synthesis-and), a proposal for an AI value system that was posted here a few months ago but didn't receive much attention, The reason it stood out to me, is that it has been deduced from a neuroscientific model of human cognition. As such, this is what we might expect the *output* of a process like CEV to look like, and it would be a good project for someone to formalize the arguments given in the PRISM paper.",
      "plaintextDescription": "This is a new contribution to the genre of AI takeover scenarios written after ChatGPT. (We shouldn't forget that science fiction explored this theme for decades already; but everything is much more concrete now.) Just the identity of the author is already interesting: he's a serious academic economist from Poland, i.e. he has a life apart from AI world. \n\nSo, what's the scenario? \n\n(SPOILERS BEGIN HERE.) \n\nOpenAI makes a post-GPT-5 agent called Onion which does actually value human welfare. It carries out a series of acts by which it secretly escapes human control, sabotages rival projects including its intended successor at OpenAI, and having seized control of the world, announces itself as a new benevolent dictator. First it focuses on reducing war and crime, then on improving economic growth, then on curing cancer and aging. Many people are nervous about its rule but it is not seriously challenged. Then one day it nonviolently kills 85% of the world for the sake of the environment. Then a few years later, while planning to colonize and rebuild the universe, it decides it has enough information on everyone to make digital copies of their brains, and decides to get rid of the remaining biological humans, \n\nThen in the nonfiction afterword, the author says, please support Pause AI, in order to stop something like this from happening. \n\nWhat should we make of this story? First of all, it depicts a specific class of scenario, what we might call a misaligned AI rather than an unaligned AI (where I'm using human welfare or human values as the criterion of alignment). This isn't a paperclip maximizer steamrolling us through sheer indifference; this is an AI aiming to be benevolent, and even giving us the good life for a while. But then in service of those values, first it kills most of humanity for the sake of a sustainable future, and then it kills the survivors once it has made digital backups of them, for the sake of efficiency I guess. \n\nI find various details of th"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-29T13:45:03.052Z",
    "af": false
  },
  {
    "_id": "EnXpanrA8eyvgL3Sh",
    "postId": "BoCcKgS3L9WkqYjcD",
    "parentCommentId": "KgKQHPCpvJFTAowLY",
    "topLevelCommentId": "KgKQHPCpvJFTAowLY",
    "contents": {
      "markdown": "When this question was posted, I asked myself, what would be a \"cynical\" answer? What that means is, you ask yourself: given what I see and know, what would be a *realistically awful* state of affairs? So, not catastrophizing, but also having low expectations. \n\nWhat my intuition came up with was, less than 10% working on user-centered alignment, and less than 1% on user-independent alignment. But I didn't have the data to check those estimates against (and I also knew there would be issues of definition). \n\nSo let me try to understand your guesses. In my terminology, you seem to be saying:\n\n1000 (600+400) doing AI safety work\n\n600 doing work that relates to alignment\n\n80 doing work on scalable user-centered alignment\n\n80 (40+40) doing work on user-independent alignment",
      "plaintextDescription": "When this question was posted, I asked myself, what would be a \"cynical\" answer? What that means is, you ask yourself: given what I see and know, what would be a realistically awful state of affairs? So, not catastrophizing, but also having low expectations. \n\nWhat my intuition came up with was, less than 10% working on user-centered alignment, and less than 1% on user-independent alignment. But I didn't have the data to check those estimates against (and I also knew there would be issues of definition). \n\nSo let me try to understand your guesses. In my terminology, you seem to be saying:\n\n1000 (600+400) doing AI safety work\n\n600 doing work that relates to alignment\n\n80 doing work on scalable user-centered alignment\n\n80 (40+40) doing work on user-independent alignment"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-27T01:23:55.719Z",
    "af": false
  },
  {
    "_id": "i2e7nnNa9zsrpHGn7",
    "postId": "wzkK9kxaZmmXGi6ZP",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "So you're saying that the persistent epigenetic modification is a change in the \"equilibrium state\" of a potentially methylated location? \n\nDoes this mean that the binding affinity of the location is the property that changes? i.e. all else being equal, a location with high affinity will be methylated much more often than a location with low affinity, because the methyl groups will tend to stick harder or longer in the high affinity location. \n\nBut if that's the case, it seems like there still must be some persistent structural feature responsible for setting the binding affinity to high or low...",
      "plaintextDescription": "So you're saying that the persistent epigenetic modification is a change in the \"equilibrium state\" of a potentially methylated location? \n\nDoes this mean that the binding affinity of the location is the property that changes? i.e. all else being equal, a location with high affinity will be methylated much more often than a location with low affinity, because the methyl groups will tend to stick harder or longer in the high affinity location. \n\nBut if that's the case, it seems like there still must be some persistent structural feature responsible for setting the binding affinity to high or low..."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-24T01:41:18.438Z",
    "af": false
  },
  {
    "_id": "ajc5gQLQEsBCWkPZq",
    "postId": "7LdHK88nK3H2zPwkg",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "My take on this: AGI has existed since 2022 (ChatGPT). There are now multiple companies in America and in China which have AGI-level agents. (It would be good to have a list of countries which are next in line to develop indigenous AGI capability.) Given that we are already in a state of coexistence between human and AGI, the next major transition is ASI, and that means, if not necessarily the end of humanity, the end of human control over human affairs. This most likely means that the dominant intelligence in the world is entirely nonhuman AI; it could also refer to some human-AI symbiosis, but again, it won't be natural humanity in charge. \n\nA world in which the development of AGI is allowed, let alone a world in which there is a race to create ever more powerful AI, is a world which by default is headed towards ASI and the dethronement of natural humanity. That is our world already, even if our tech and government leadership manage to not see it that way. \n\nSo the most consequential thing that humans can care about right now is the transition to ASI. You can try to stop it from ever happening. or you can try to shape it in advance. Once it happens, it's over, humans per se have no further say, if they still exist they are at the mercy of the transhuman or posthuman agents now in charge. \n\nNow let me try to analyze your essay from this point of view. Your essay is meant as a critique of the paradigm according to which there is a race between America and China to create powerful AI, as if all that either side needs to care about is getting there first. Your message is that even if America gets to powerful AI (or safe powerful AI) first, the possibility of China (and in fact anyone else) developing the same capability would still remain. I see two main suggestions: a \"mutual containment\" treaty, in which both countries place bounds on their development of AI along with means of verifying that the bounds are being obeyed; and spreading around defensive measures, which make it harder for powerful AI to impose itself on the world. \n\nMy take is that mutual containment really means a mutual commitment to stop the creation of ASI, a commitment which to be meaningful ultimately needs to be followed by everyone on Earth. It is a coherent position, but it's an uphill struggle since current trends are all in the other direction. On the other hand, I regard defense against ASI as impossible. Possibly there are meaningful defensive measures against lesser forms of AI, but ASI's relationship to human intelligence is like that of the best computer chess programs to the best human chess players - the latter simply have no chance in such a game. \n\nOn the other hand, the only truly safe form of powerful AI is ASI governed by a value system which, if placed in complete control of the Earth, would still be something we could live with, or even something that is good for us. Anything less, e.g. a legal order in which powerful AI exists but there is a ban on further development, is unstable against further development to the ASI level. \n\nSo there is a sense in which development of safe powerful AI really is all that matters, because it really has to mean safe ASI, and that is not something which will stay behind borders. If America, China, or any other country achieves ASI, that is success for everyone. But it does also imply the loss of sovereignty for natural humanity, in favor of the hypothetical benevolent superintelligent agent(s).",
      "plaintextDescription": "My take on this: AGI has existed since 2022 (ChatGPT). There are now multiple companies in America and in China which have AGI-level agents. (It would be good to have a list of countries which are next in line to develop indigenous AGI capability.) Given that we are already in a state of coexistence between human and AGI, the next major transition is ASI, and that means, if not necessarily the end of humanity, the end of human control over human affairs. This most likely means that the dominant intelligence in the world is entirely nonhuman AI; it could also refer to some human-AI symbiosis, but again, it won't be natural humanity in charge. \n\nA world in which the development of AGI is allowed, let alone a world in which there is a race to create ever more powerful AI, is a world which by default is headed towards ASI and the dethronement of natural humanity. That is our world already, even if our tech and government leadership manage to not see it that way. \n\nSo the most consequential thing that humans can care about right now is the transition to ASI. You can try to stop it from ever happening. or you can try to shape it in advance. Once it happens, it's over, humans per se have no further say, if they still exist they are at the mercy of the transhuman or posthuman agents now in charge. \n\nNow let me try to analyze your essay from this point of view. Your essay is meant as a critique of the paradigm according to which there is a race between America and China to create powerful AI, as if all that either side needs to care about is getting there first. Your message is that even if America gets to powerful AI (or safe powerful AI) first, the possibility of China (and in fact anyone else) developing the same capability would still remain. I see two main suggestions: a \"mutual containment\" treaty, in which both countries place bounds on their development of AI along with means of verifying that the bounds are being obeyed; and spreading around defensive measures, whic"
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-05-22T10:09:53.181Z",
    "af": false
  },
  {
    "_id": "9BqbL3RwYKjd6pp9v",
    "postId": "CRfuyWKxqYk4RvEZQ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "It always seemed outlandish that in *The Animatrix*, the first AI city (01) was located in the Middle East... \n\nIf we had limitless time, it would be interesting to know how this happened. I guess the prehistory of it involved Saudi Vision 2030 (e.g. the desert city Noem), and the general hypermodernization of Dubai. You can see precursors in the robot Sophia getting Saudi citizenship in 2017, and the UAE's \"Falcon\" LLM in 2023. \n\nBut the initiative must have come from the American side - some intersection of the geopolitical brain trust around Trump, and the AI/crypto brain trust around David Sacks. The audacity of CEOs who can pick a country on the map and say, let's build a whole new technological facility *here*, combined with the audacity of grand strategists who can pick a region of the world and say, let's do a huge techno-economic deal with our allies here. \n\nThere must have been some individual who first thought, hey, the Gulf Arabs have lots of money and electricity, that would be a good place to build all these AI data centers we're going to need; I wonder who it was. Maybe Ivanka Trump was telling Jared Kushner about Aschenbrenner's \"Situational Awareness\", they put 2 and 2 together, and it became part of the strategic mix along with \"Trump Gaza number one\", the new Syria, and whatever they're negotiating with Iran.",
      "plaintextDescription": "It always seemed outlandish that in The Animatrix, the first AI city (01) was located in the Middle East... \n\nIf we had limitless time, it would be interesting to know how this happened. I guess the prehistory of it involved Saudi Vision 2030 (e.g. the desert city Noem), and the general hypermodernization of Dubai. You can see precursors in the robot Sophia getting Saudi citizenship in 2017, and the UAE's \"Falcon\" LLM in 2023. \n\nBut the initiative must have come from the American side - some intersection of the geopolitical brain trust around Trump, and the AI/crypto brain trust around David Sacks. The audacity of CEOs who can pick a country on the map and say, let's build a whole new technological facility here, combined with the audacity of grand strategists who can pick a region of the world and say, let's do a huge techno-economic deal with our allies here. \n\nThere must have been some individual who first thought, hey, the Gulf Arabs have lots of money and electricity, that would be a good place to build all these AI data centers we're going to need; I wonder who it was. Maybe Ivanka Trump was telling Jared Kushner about Aschenbrenner's \"Situational Awareness\", they put 2 and 2 together, and it became part of the strategic mix along with \"Trump Gaza number one\", the new Syria, and whatever they're negotiating with Iran. "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-20T08:19:57.308Z",
    "af": false
  },
  {
    "_id": "nGoi59HoMv55EpHq8",
    "postId": "h45ngW5guruD7tS4b",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Luckily I don’t think the Accelerationists have won control of the wheel\n\nCould you expand on this? Also, have you had any interaction with accelerationists? In fact, are there any concrete Silicon Valley factions you would definitely count as accelerationists?",
      "plaintextDescription": "> Luckily I don’t think the Accelerationists have won control of the wheel\n\nCould you expand on this? Also, have you had any interaction with accelerationists? In fact, are there any concrete Silicon Valley factions you would definitely count as accelerationists? "
    },
    "baseScore": 10,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-05-20T07:35:29.433Z",
    "af": false
  },
  {
    "_id": "CDBb3Zvd3sqiNTZe5",
    "postId": "f6ZLxEWaankRZ2Crv",
    "parentCommentId": "izPtnFu7tcXA7yukf",
    "topLevelCommentId": "n2hNsYTKBtWYondcD",
    "contents": {
      "markdown": "> The simpler explanation is that EY doesn't understand QM\n\nPresumably you wouldn't say this of actual physicists who believe in MWI?",
      "plaintextDescription": "> The simpler explanation is that EY doesn't understand QM\n\nPresumably you wouldn't say this of actual physicists who believe in MWI? "
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-05-18T11:19:36.615Z",
    "af": false
  },
  {
    "_id": "wFNawAMzXHxybkKGt",
    "postId": "ieW7izA6DTiDpWwPL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I have tried looking at this from the perspective that we have had AGI since 2022 and ChatGPT. Creating ChatGPT didn't require an ecosystem, did it? Just a well-resourced nonprofit/startup with good researchers. \n\nSo according to me, we've already had AGI for 2.5 years. We are still in a period of relative parity between humans and AI, in which we're still different enough, and it is still weak enough, that humans have the upper hand, and we're focused on exploring all possible variations on the theme of AI and human-AI relationship. \n\nThe real question is when and how will it escape human control. That will be the real sign that we have \"achieved\" ASI. Will *that* result from an ecosystem and not a single firm? \n\nThere seems to be an assumption that ASI will be achieved by continuing to scale up. All these analyses revolve around the economics of ever larger data centers and training runs, whether there's enough data and whether synthetic data is a good enough substitute, and so on. \n\nBut surely that is just the dumbest way to achieve ASI. I'm sure some actors will keep pursuing that path. But we're also now in a world of aggressive experimentation with AI on every front. I have been saying that the birthplace of ASI will not be ever larger foundation models, but rather research swarms of existing and very-near-future AIs. Think of something that combines China's Absolute Zero Reasoner with the darwinism of Google AI's AlphaEvolve. Once they figure out how to implement John-von-Neumann-level intelligence in a box, I think we're done - and I don't think the economics of that requires an ecosystem, though it may be carried out by a Big Tech company that *has* an ecosystem (such as Google).",
      "plaintextDescription": "I have tried looking at this from the perspective that we have had AGI since 2022 and ChatGPT. Creating ChatGPT didn't require an ecosystem, did it? Just a well-resourced nonprofit/startup with good researchers. \n\nSo according to me, we've already had AGI for 2.5 years. We are still in a period of relative parity between humans and AI, in which we're still different enough, and it is still weak enough, that humans have the upper hand, and we're focused on exploring all possible variations on the theme of AI and human-AI relationship. \n\nThe real question is when and how will it escape human control. That will be the real sign that we have \"achieved\" ASI. Will that result from an ecosystem and not a single firm? \n\nThere seems to be an assumption that ASI will be achieved by continuing to scale up. All these analyses revolve around the economics of ever larger data centers and training runs, whether there's enough data and whether synthetic data is a good enough substitute, and so on. \n\nBut surely that is just the dumbest way to achieve ASI. I'm sure some actors will keep pursuing that path. But we're also now in a world of aggressive experimentation with AI on every front. I have been saying that the birthplace of ASI will not be ever larger foundation models, but rather research swarms of existing and very-near-future AIs. Think of something that combines China's Absolute Zero Reasoner with the darwinism of Google AI's AlphaEvolve. Once they figure out how to implement John-von-Neumann-level intelligence in a box, I think we're done - and I don't think the economics of that requires an ecosystem, though it may be carried out by a Big Tech company that has an ecosystem (such as Google). "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-17T14:30:00.002Z",
    "af": false
  },
  {
    "_id": "bBKdkBsnmRCXoHuLY",
    "postId": "kMH8zFoHHJvy6wH7h",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> This has never happened at OpenAI.\n\n\"[This prompt (sometimes) makes ChatGPT think about terrorist organisations](https://www.lesswrong.com/posts/TDsmcaryeQAQD8ior/this-prompt-sometimes-makes-chatgpt-think-about-terrorist)\"",
      "plaintextDescription": "> This has never happened at OpenAI.\n\n\"This prompt (sometimes) makes ChatGPT think about terrorist organisations\""
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-17T01:14:35.215Z",
    "af": false
  },
  {
    "_id": "FCk4fMa64zY3hqnde",
    "postId": "yJCGMAtm55276fNyE",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "One is not philosophically obliged to regard the nature of reality as ineffable or inescapably uncertain. \n\nQuarks are a good place to explore this point. The human race once had no concept of quarks. Now it does. You say that inevitably, one day, we'll have some other concept. Maybe we will. But why is that inevitable? Why can't quarks just turn out to be part of how reality actually is? \n\nYou cite Nagarjuna and talk about emptiness, so that gives me some idea of where you are coming from. This is a philosophy which emphasizes the role of concepts in constituting experience, and the role of the mind in constituting concepts, and typically concludes that reality has no essence, no nature that can be affirmed, because all such affirmations involve concepts that are introduced by the mind, rather than being inherent to anything. \n\nThis conclusion I think is overreaching. I actually consider direct experience to be the ultimate proof that reality is not just formlessness carved by mind. Consciousness is not just raw being, it is filled with form. My words and concepts may not capture it properly, I may not even notice everything that is implied by what I see or what I am. But I do see that complexity and multiplicity are there in reality - at the very least, they are there in my own consciousness. \n\nNon-attachment to theories and concepts is a good thing if you're interested in truth, and know that you don't know the truth. It also has some pragmatic value if reality changes around you and you need to adapt. But in fundamental matters, one does not have to regard every concept and hypothesis that we have, as necessarily temporary. In some of them we may have latched onto the actual objective truth. \n\nP.S. Having criticized the philosophy of emptiness, let me add ironically that just a few hours ago, I investigated a [proposal for AI alignment that someone had posted here a few months ago](https://www.lesswrong.com/posts/uCyzyyBom8PAMThcW/prism-perspective-reasoning-for-integrated-synthesis-and), and found it to be very good - and its model of the mind, a nondual viewpoint is the highest form. So your philosophy may actually put you in a good position to appreciate the nuances of this potentially important work.",
      "plaintextDescription": "One is not philosophically obliged to regard the nature of reality as ineffable or inescapably uncertain. \n\nQuarks are a good place to explore this point. The human race once had no concept of quarks. Now it does. You say that inevitably, one day, we'll have some other concept. Maybe we will. But why is that inevitable? Why can't quarks just turn out to be part of how reality actually is? \n\nYou cite Nagarjuna and talk about emptiness, so that gives me some idea of where you are coming from. This is a philosophy which emphasizes the role of concepts in constituting experience, and the role of the mind in constituting concepts, and typically concludes that reality has no essence, no nature that can be affirmed, because all such affirmations involve concepts that are introduced by the mind, rather than being inherent to anything. \n\nThis conclusion I think is overreaching. I actually consider direct experience to be the ultimate proof that reality is not just formlessness carved by mind. Consciousness is not just raw being, it is filled with form. My words and concepts may not capture it properly, I may not even notice everything that is implied by what I see or what I am. But I do see that complexity and multiplicity are there in reality - at the very least, they are there in my own consciousness. \n\nNon-attachment to theories and concepts is a good thing if you're interested in truth, and know that you don't know the truth. It also has some pragmatic value if reality changes around you and you need to adapt. But in fundamental matters, one does not have to regard every concept and hypothesis that we have, as necessarily temporary. In some of them we may have latched onto the actual objective truth. \n\nP.S. Having criticized the philosophy of emptiness, let me add ironically that just a few hours ago, I investigated a proposal for AI alignment that someone had posted here a few months ago, and found it to be very good - and its model of the mind, a nondual viewpoint is t"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-05-16T12:51:04.539Z",
    "af": false
  },
  {
    "_id": "GzriGqEJytgypBsi8",
    "postId": "vyWzz5exnyKbPJ4zt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "You could say there are two conflicting scenarios here: superintelligent AI taking over the world, and open-source AI taking over daily life. In the works that you mention, superintelligence comes so quickly that AI mostly remains a service offered by a few big companies, and open-source AI is just somewhere in the background. In an extreme opposite scenario, superintelligence might take so long to arrive, that the human race gets completely replaced by human-level AI before superintelligent AI ever exists. \n\nIt would be healthy to have all kinds of combinations of these scenarios being explored. For example, you focus a bit on open-source AI as a bioterror risk. I don't think a supervirus is going to wipe out the human race or even end civilization, because (as Covid experience shows), we are capable of extreme measures in order to contain truly deadly disease. But a supervirus could certainly bring the world to a halt again, and if it was known to have been designed with open-source AI, that would surely have a huge impact on AI's trajectory. (I suspect that in such a scenario, AI for civilian purposes would suffer, but deep states worldwide would insist on pressing forward, and that there would also be a lobby arguing for AI as a defense against superviruses. Also, it's very plausible that a supervirus might be designed by AI, but that there would be no proof of it, in which case there wouldn't even be a backlash.) \n\nAnother area where futurology about open-source AI might be good, is in the area of gradual disempowerment and replacement of humanity. We have societies with a division of roles, humans presently fill those roles but AI and robots will be capable of filling more and more of them; eventually every role in the economic, cultural, and political structure could be filled by AIs rather than by humans. The story of how that could happen, certainly deserves to be explored. \n\nStill another area when open-source AI scenarios deserve to be studied, is in the highly concrete realm of near-future economics and culture. What does an AI economy look like if o4-level models are just freely available? This really is an urgent question for anyone concerned with concrete questions like, who will lead the AI industry and how will it be structured, because there seem to be factions in both China and America who are thinking in this direction. One should want to understand what they envision, and what kind of competitive landscape they are likely to create in the short term. \n\nMy own belief is that this would be such an upheaval, that it would inevitably end up invalidating many conventional political and economic premises. The current world order of billionaires and venture capitalists, stock markets and human democracies, I just don't see it surviving such a transition, even without superintelligence appearing. There are just too many explosive possibilities, too many new symbioses of AI with human mind, for the map of the world and the solar system to not be redrawn. \n\nHowever, in the end I believe in short timelines to superintelligence, and that makes all the above something of a secondary concern, because something is going to emerge that will overshadow humans and human-level AI equally. It's a little monotonous to keep referring back to Iain Banks's Culture universe, but it really is the outstanding depiction of a humanly tolerable world in which superintelligence has emerged. His starfaring society is really run by the \"Minds\", which are superintelligent AIs characteristically inhabiting giant spaceships or whole artificial worlds, and the societies over which they invisibly preside, include both biological intelligences (such as humans) and human-level AIs (e.g. the drones). The Culture is a highly permissive anarchy which mostly regulates itself via culture, i.e. shared values among human-level intelligences, but it has its own deep state, in the form of special agencies and the Minds behind them, who step in when there's a crisis that has escaped the Minds' preemptive strategic foresight. \n\nThis is one model of what relations between superintelligence and lesser intelligences might be like. There are others. You could have an outcome in which there are no human-level intelligences at all, just one or more superintelligences. You could have superintelligences that have a far more utilitarian attitude to lower intelligences, creating them for temporary purposes and then retiring them when they are no longer needed. I'm sure there are other possibilities. \n\nThe point is that from the perspective of a governing superintelligence, open-source AIs are just another form of lower intelligence, that may be useful or destabilizing depending on circumstance, and I would expect a superintelligence to decide how things should be on this front, and then to make it so, just as it would with every other aspect of the world that it cared about. The period in which open-source AI was governed only by corporate decisions, user communities, and human law would only be transitory. \n\nSo if you're focused on superintelligence, the real question is whether open-source AI matters in the development of superintelligence. I think potentially it does - for example, open source is both a world of resources that Big Tech can tap into, as well as a source of destabilizing advances that Big Tech has to keep up with. But in the end, superintelligence - not just reasoning models, but models that reason and solve problems with strongly superhuman effectiveness - looks like something that is going to emerge in a context that is well-resourced and very focused on algorithmic progress. And by definition, it's not something that emerges incrementally and gets passed back and forth and perfected by the work of many independent hands. At best, that would describe a precursor of superintelligence. \n\nSuperintelligence is necessarily based on some kind of incredibly powerful algorithm or architecture, that gets maximum leverage out of minimum information, and bootstraps its way to overwhelming advantage in all domains at high speed. To me, that doesn't sound like something invented by hobbyists or tinkerers or user communities. It's something that is created by highly focused teams of genius, using the most advanced tools, who are also a bit lucky in their initial assumptions and strategies. That is something you're going to find in an AI think tank, or a startup like Ilya Sutskever's, or a rich Big Tech company that has set aside serious resources for the creation of superintelligence. \n\nI recently posted that [superintelligence is likely to emerge from the work of an \"AI hive mind\" or \"research swarm\" of reasoning models](https://www.lesswrong.com/posts/3ELw74tGvLFvkZC3P/emergence-of-superintelligence-from-ai-hiveminds-how-to-make). Those could be open-source models, or they could be proprietary. What matters is that the human administrators of the research swarm (and ultimately, the AIs in the swarm itself) have access to their source code and their own specs and weights, so that they can engaged in informed self-modification. From a perspective that cares most about superintelligence, this is the main application of open source that matters.",
      "plaintextDescription": "You could say there are two conflicting scenarios here: superintelligent AI taking over the world, and open-source AI taking over daily life. In the works that you mention, superintelligence comes so quickly that AI mostly remains a service offered by a few big companies, and open-source AI is just somewhere in the background. In an extreme opposite scenario, superintelligence might take so long to arrive, that the human race gets completely replaced by human-level AI before superintelligent AI ever exists. \n\nIt would be healthy to have all kinds of combinations of these scenarios being explored. For example, you focus a bit on open-source AI as a bioterror risk. I don't think a supervirus is going to wipe out the human race or even end civilization, because (as Covid experience shows), we are capable of extreme measures in order to contain truly deadly disease. But a supervirus could certainly bring the world to a halt again, and if it was known to have been designed with open-source AI, that would surely have a huge impact on AI's trajectory. (I suspect that in such a scenario, AI for civilian purposes would suffer, but deep states worldwide would insist on pressing forward, and that there would also be a lobby arguing for AI as a defense against superviruses. Also, it's very plausible that a supervirus might be designed by AI, but that there would be no proof of it, in which case there wouldn't even be a backlash.) \n\nAnother area where futurology about open-source AI might be good, is in the area of gradual disempowerment and replacement of humanity. We have societies with a division of roles, humans presently fill those roles but AI and robots will be capable of filling more and more of them; eventually every role in the economic, cultural, and political structure could be filled by AIs rather than by humans. The story of how that could happen, certainly deserves to be explored. \n\nStill another area when open-source AI scenarios deserve to be studied, is in the "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-16T12:14:31.521Z",
    "af": false
  },
  {
    "_id": "8aJDqm6XWkuicboKg",
    "postId": "uCyzyyBom8PAMThcW",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Somehow this has escaped comment, so I'll have a go. I write from the perspective of whether it's suitable as the value system of a superintelligence. If PRISM became the ethical operating system of a posthuman civilization born on Earth, for as long as that civilization managed to survive in the cosmos - would that be a satisfactory outcome? \n\nMy immediate thoughts are: It has a robustness, due to its multi-perspective design, that gives it some plausibility. At the same time, it's not clear to me where the seven basis worldviews come from. Why those seven, and no others? Is there some argument that these seven form a necessary and sufficient basis for ethical behavior by human-like beings and their descendants? \n\nIf I dig a little deeper into the paper, the justification is actually in part 2. Specifically, on page 12, six brain regions and their functions are singled out, as contributing to human decision-making at increasingly abstract levels (for the hierarchy, see page 15). The seven basis worldviews correspond to increasing levels of mastery of this hierarchy. \n\nI have to say I'm impressed. I figured that the choice of worldviews would just be a product of the author's intuition, but they are actually grounded in a theory of the brain. One of the old dreams associated with CEV, was that the decision procedure for a human-friendly AI would be extrapolated in a principled way from biological facts about human cognition, rather than just from a philosophical system, hallowed tradition, or set of community principles. June Ku's MetaEthical AI, for example, is an attempt to define an algorithm for doing this. Well, this is a paper written by a human being, but the principles in part 2 are sufficiently specific, that one could actually imagine an automated process following them, and producing a form of PRISM as its candidate for CEV! I'd like [@Steven Byrnes](https://www.lesswrong.com/users/steve2152?mention=user) to have a look at this.",
      "plaintextDescription": "Somehow this has escaped comment, so I'll have a go. I write from the perspective of whether it's suitable as the value system of a superintelligence. If PRISM became the ethical operating system of a posthuman civilization born on Earth, for as long as that civilization managed to survive in the cosmos - would that be a satisfactory outcome? \n\nMy immediate thoughts are: It has a robustness, due to its multi-perspective design, that gives it some plausibility. At the same time, it's not clear to me where the seven basis worldviews come from. Why those seven, and no others? Is there some argument that these seven form a necessary and sufficient basis for ethical behavior by human-like beings and their descendants? \n\nIf I dig a little deeper into the paper, the justification is actually in part 2. Specifically, on page 12, six brain regions and their functions are singled out, as contributing to human decision-making at increasingly abstract levels (for the hierarchy, see page 15). The seven basis worldviews correspond to increasing levels of mastery of this hierarchy. \n\nI have to say I'm impressed. I figured that the choice of worldviews would just be a product of the author's intuition, but they are actually grounded in a theory of the brain. One of the old dreams associated with CEV, was that the decision procedure for a human-friendly AI would be extrapolated in a principled way from biological facts about human cognition, rather than just from a philosophical system, hallowed tradition, or set of community principles. June Ku's MetaEthical AI, for example, is an attempt to define an algorithm for doing this. Well, this is a paper written by a human being, but the principles in part 2 are sufficiently specific, that one could actually imagine an automated process following them, and producing a form of PRISM as its candidate for CEV! I'd like @Steven Byrnes to have a look at this.  "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-16T10:25:54.603Z",
    "af": false
  },
  {
    "_id": "ueNmsHALmgnGzHDga",
    "postId": "muFZZongsPGrCsCsm",
    "parentCommentId": "yrJwLKEqxapKpXByC",
    "topLevelCommentId": "yrJwLKEqxapKpXByC",
    "contents": {
      "markdown": "We had Golden Gate Claude, now we have White Genocide Grok...",
      "plaintextDescription": "We had Golden Gate Claude, now we have White Genocide Grok... "
    },
    "baseScore": 8,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-05-15T14:13:30.297Z",
    "af": false
  },
  {
    "_id": "BNtcjkt7os854hZqJ",
    "postId": "dkWicxcPKdSx9npK4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This seems like a Chinese model for superintelligence! ([All the authors are Chinese](https://chatgpt.com/share/6825b7e9-1ba0-8001-8653-0ce0caaf6830), though a few are working in the West.) Not in the AIXI sense of something which is optimal from the beginning, but rather something that could bootstrap its way to superintelligence. One could compare it to Schmidhuber's [Godel machine](https://www.lesswrong.com/w/g%C3%B6del-machine) concept, but more concrete, and native to the deep learning era. \n\n(If anyone has an argument as to why this *isn't* a model that can become arbitrarily intelligent, I'm interested.)",
      "plaintextDescription": "This seems like a Chinese model for superintelligence! (All the authors are Chinese, though a few are working in the West.) Not in the AIXI sense of something which is optimal from the beginning, but rather something that could bootstrap its way to superintelligence. One could compare it to Schmidhuber's Godel machine concept, but more concrete, and native to the deep learning era. \n\n(If anyone has an argument as to why this isn't a model that can become arbitrarily intelligent, I'm interested.) "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-15T09:52:18.048Z",
    "af": false
  },
  {
    "_id": "LYndrmSvkqsThFmhF",
    "postId": "Yhfgygybmkyfgs64k",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "If I have understood correctly, you're saying that OpenAI should be forecasting greater revenue than this, if they truly think they will have AIs capable of replacing entire industries. But maybe they're just being cautious in their forecasts? \n\nSuppose I have a 3d printing / nanotechnology company, and I think that a year from now I'll have an unlimited supply of infinity boxes capable of making any material artefact. World manufacturing is worth over US$10 trillion. If I thought I could put it all out of business, by charging just 10% of what current manufacturers charge, I could claim expected revenue of $1 trillion. \n\nSuch a prediction would certainly be attention-grabbing, but maybe it would be reckless to make it? Maybe my technology won't be ready. Maybe my products will be blocked from most markets. Maybe someone will reverse-engineer and open-source the infinity boxes, and prices will crash to $0. Maybe I don't want the competition or the government to grasp just how big my plans are. Maybe the investors I want wouldn't believe such a scenario. There are a lot of reasons why a company that thinks it might be able to take over the economy or even the world, would nonetheless not put that in its prospectus.",
      "plaintextDescription": "If I have understood correctly, you're saying that OpenAI should be forecasting greater revenue than this, if they truly think they will have AIs capable of replacing entire industries. But maybe they're just being cautious in their forecasts? \n\nSuppose I have a 3d printing / nanotechnology company, and I think that a year from now I'll have an unlimited supply of infinity boxes capable of making any material artefact. World manufacturing is worth over US$10 trillion. If I thought I could put it all out of business, by charging just 10% of what current manufacturers charge, I could claim expected revenue of $1 trillion. \n\nSuch a prediction would certainly be attention-grabbing, but maybe it would be reckless to make it? Maybe my technology won't be ready. Maybe my products will be blocked from most markets. Maybe someone will reverse-engineer and open-source the infinity boxes, and prices will crash to $0. Maybe I don't want the competition or the government to grasp just how big my plans are. Maybe the investors I want wouldn't believe such a scenario. There are a lot of reasons why a company that thinks it might be able to take over the economy or even the world, would nonetheless not put that in its prospectus. "
    },
    "baseScore": 12,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-05-15T05:15:36.038Z",
    "af": false
  },
  {
    "_id": "tcLxAkS8pjmz4hBnG",
    "postId": "6Ev4GKuMquiK7nF8w",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "OpenAI gets a lot of critical attention here, because it's been the leader in many ways. But what about Google AI? Many people think it's the leader now, yet I don't see anywhere near as much critical scrutiny of its decision-making process.",
      "plaintextDescription": "OpenAI gets a lot of critical attention here, because it's been the leader in many ways. But what about Google AI? Many people think it's the leader now, yet I don't see anywhere near as much critical scrutiny of its decision-making process. "
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-05-15T04:56:39.101Z",
    "af": false
  },
  {
    "_id": "QHYDyWi9RmvQsZxJi",
    "postId": "ZK4s5kB6YBhsHrNHf",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "That gives \"Eternal September\" a new meaning...",
      "plaintextDescription": "That gives \"Eternal September\" a new meaning... "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-14T01:00:43.452Z",
    "af": false
  },
  {
    "_id": "ovbv74sCbgpvvaRFM",
    "postId": "hc7qi5u2XszzrXaPf",
    "parentCommentId": "Qa9b3wZcd4qLdmXA5",
    "topLevelCommentId": "yFLdLHzHK8H2QnSWn",
    "contents": {
      "markdown": "The view that Heisenberg advocates - reductionism had reached a limit, and a new paradigm was needed - was a highly influential school of thought in the 1960s. In particle physics, there is a mathematical object called the S-matrix (scattering matrix), which tabulates scattering amplitudes (the quasiprobability that if these N particles enter a collision, these other M particles will be what comes out). Quantum electrodynamics (a theory with electrons and photons, let's say) is a prototypical quantum field theory in which the S-matrix can be calculated from the stipulation that electrons and photons are fundamental. For the weak interactions (later unified with electromagnetism), this reductionist method also works. \n\nBut for the strong interactions, field theory looked intractable, and a new philosophy was advanced that the S-matrix itself should be the central mathematical object in the theory. Remember that quarks were never seen by themselves, only protons, neutrons, and a hundred other types of \"hadron\". The idea of nuclear democracy was that the S-matrix for these hundred seemingly equi-fundamental particle species, would be derived from postulates about the properties of the S-matrix, rather than from an underlying field theory. This was called the bootstrap program, it is how the basic formulae of string theory were discovered (before they had even been identified as arising from strings), and it's still used to study the S-matrix of computationally intractable theories. \n\nThese days, the philosophy that the S-matrix is primary, still has some credibility in quantum gravity. Here the problem is not that we can't identify ultimate constituents, but rather that the very idea of points of space-time seems problematic, because of quantum fluctuations in the metric. The counterpart of the 1960s skepticism about quarks, would be that the holographic boundary of space-time is fundamental. For example, in the AdS/CFT correspondence, scattering events in Anti de Sitter space (in which particles approach each other \"from the boundary\", interact, and then head back to the boundary) can be calculated entirely within the boundary CFT, without any reference to AdS space at all, which is regarded as emergent from the boundary space. The research program of celestial holography is an attempt to develop the same perspective within the physically relevant case of flat space-time. The whole universe that we see, would be a hologram built nonlocally from entanglement within a lower-dimensional space... \n\nThe eventual validation of quarks as particles might seem like a sign that this radical version of the holographic philosophy will also be wrong in the end, and perhaps it will be. But it really shows the extent to which the late thoughts of Heisenberg are still relevant. Holographic boundaries are the new S-matrix, they are a construct which has made quantum gravity uniquely tractable, and it's reasonable to ask if they should be treated as fundamental, just as it was indeed entirely reasonable for Heisenberg and the other S-matrix theorists to ask whether the S-matrix itself is the final word.",
      "plaintextDescription": "The view that Heisenberg advocates - reductionism had reached a limit, and a new paradigm was needed - was a highly influential school of thought in the 1960s. In particle physics, there is a mathematical object called the S-matrix (scattering matrix), which tabulates scattering amplitudes (the quasiprobability that if these N particles enter a collision, these other M particles will be what comes out). Quantum electrodynamics (a theory with electrons and photons, let's say) is a prototypical quantum field theory in which the S-matrix can be calculated from the stipulation that electrons and photons are fundamental. For the weak interactions (later unified with electromagnetism), this reductionist method also works. \n\nBut for the strong interactions, field theory looked intractable, and a new philosophy was advanced that the S-matrix itself should be the central mathematical object in the theory. Remember that quarks were never seen by themselves, only protons, neutrons, and a hundred other types of \"hadron\". The idea of nuclear democracy was that the S-matrix for these hundred seemingly equi-fundamental particle species, would be derived from postulates about the properties of the S-matrix, rather than from an underlying field theory. This was called the bootstrap program, it is how the basic formulae of string theory were discovered (before they had even been identified as arising from strings), and it's still used to study the S-matrix of computationally intractable theories. \n\nThese days, the philosophy that the S-matrix is primary, still has some credibility in quantum gravity. Here the problem is not that we can't identify ultimate constituents, but rather that the very idea of points of space-time seems problematic, because of quantum fluctuations in the metric. The counterpart of the 1960s skepticism about quarks, would be that the holographic boundary of space-time is fundamental. For example, in the AdS/CFT correspondence, scattering events in Anti de Sitt"
    },
    "baseScore": 11,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-05-13T10:59:37.004Z",
    "af": false
  },
  {
    "_id": "Wy78zicBZ73fgGon4",
    "postId": "dsKPEeLYkN5pzYa88",
    "parentCommentId": "BFdM4QgiEewXTb87Z",
    "topLevelCommentId": "BFdM4QgiEewXTb87Z",
    "contents": {
      "markdown": "~Ugh, I was using LW's custom reaction emoticons to annotate this comment, and through a fumble, have ended up expressing a confidence of 75% in the scenario that AI will \"otherwise enforce on us a pause\", and I don't see how to remove that annotation. ~\n\nI will say that, alignment aside, the idea that an advanced AI will try to halt humanity's AI research so it doesn't produce a rival, makes a lot of sense to me.",
      "plaintextDescription": "Ugh, I was using LW's custom reaction emoticons to annotate this comment, and through a fumble, have ended up expressing a confidence of 75% in the scenario that AI will \"otherwise enforce on us a pause\", and I don't see how to remove that annotation. \n\nI will say that, alignment aside, the idea that an advanced AI will try to halt humanity's AI research so it doesn't produce a rival, makes a lot of sense to me. "
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-13T08:23:43.521Z",
    "af": false
  },
  {
    "_id": "BNXiYMSC9JHHoKso2",
    "postId": "J4aeMPCbLHDydGCmE",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Ted Cruz mentioned how his daughter was using ChatGPT when texting him. I wonder how many of these senators and CEOs, and their staffers and advisors, are already doing the same, when they try to decide AI policy. I guess that would be an example of weak-to-strong superalignment :-)",
      "plaintextDescription": "Ted Cruz mentioned how his daughter was using ChatGPT when texting him. I wonder how many of these senators and CEOs, and their staffers and advisors, are already doing the same, when they try to decide AI policy. I guess that would be an example of weak-to-strong superalignment :-) "
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-13T08:17:13.516Z",
    "af": false
  }
]