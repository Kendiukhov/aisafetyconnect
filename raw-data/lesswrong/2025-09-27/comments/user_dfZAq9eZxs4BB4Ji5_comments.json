[
  {
    "_id": "PxpPeWdfrZ5aFnnQR",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "px9JSsLon33PeJEwJ",
    "topLevelCommentId": "px9JSsLon33PeJEwJ",
    "contents": {
      "markdown": "I agree that paused uploads which never get unpaused count as death. However, it's extremely, extremely relatively cheap on a cosmic scale to run some uploads, so if you make the uploads and there is literally any motive to run them, I expect they get run.",
      "plaintextDescription": "I agree that paused uploads which never get unpaused count as death. However, it's extremely, extremely relatively cheap on a cosmic scale to run some uploads, so if you make the uploads and there is literally any motive to run them, I expect they get run."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-25T15:16:54.785Z",
    "af": false
  },
  {
    "_id": "WogrTfcoHed9P92rC",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "C8BuQ7FPDdScznzPv",
    "topLevelCommentId": "CB7eYziCCfTRmSgoN",
    "contents": {
      "markdown": "Another way to put this: some types of caring are the types of caring where you're willing to allocate 1 / billionth of your resources on it even if you learn that you are extremely rich (so these resource could buy a lot of what you want in absolute terms). But you can also have preferences for what to spend your money on which depend strongly on what options are available; you'd be willing to trade some absolute amount of value for the thing and if it ends up being that your wealthier and can buy more in absolute terms, you'd be willing to spend a smaller fraction of money on the thing.\n\nI'm predicting a type of caring/kindness which is the first rather than a type of preference which cares about questions like \"exactly how big is the cosmic endowment\".",
      "plaintextDescription": "Another way to put this: some types of caring are the types of caring where you're willing to allocate 1 / billionth of your resources on it even if you learn that you are extremely rich (so these resource could buy a lot of what you want in absolute terms). But you can also have preferences for what to spend your money on which depend strongly on what options are available; you'd be willing to trade some absolute amount of value for the thing and if it ends up being that your wealthier and can buy more in absolute terms, you'd be willing to spend a smaller fraction of money on the thing.\n\nI'm predicting a type of caring/kindness which is the first rather than a type of preference which cares about questions like \"exactly how big is the cosmic endowment\"."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-24T04:46:04.713Z",
    "af": false
  },
  {
    "_id": "aa4xeysYqukb2Ej7M",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "6f5GvxQcLFkvRHZFk",
    "topLevelCommentId": "TqqN34ec7vgLPuhHf",
    "contents": {
      "markdown": "My claim is that you're reasonably likely to get a small preference for something like this by default even in the absence of serious effort to ensure alignment beyond what is commercially expedient. For instance, note that humans put some weight on \"respect the actual preferences of existing intelligent agents\" despite this being a \"narrow target in mind-space\".",
      "plaintextDescription": "My claim is that you're reasonably likely to get a small preference for something like this by default even in the absence of serious effort to ensure alignment beyond what is commercially expedient. For instance, note that humans put some weight on \"respect the actual preferences of existing intelligent agents\" despite this being a \"narrow target in mind-space\"."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-24T04:36:37.442Z",
    "af": false
  },
  {
    "_id": "yzCncGTjqoodWXqvz",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "BzBuJ25RBykrkqEsc",
    "topLevelCommentId": "BzBuJ25RBykrkqEsc",
    "contents": {
      "markdown": "> human coup instigators generally still need humans to rule over, whereas AIs wouldn't\n\nSometimes? In countries where most wealth is mineral wealth, they actually need very few citizens. The more basic point is that human coups aren't particularly helped by indiscriminate killing while AI takeover might be. But I still think coups are surprisingly bloodless and often keeping coups bloodless is the optimal strategy for the person doing the coup. I think this transfers to AI.\n\n> I don't see how the number of AIs makes a big difference here, rather than the absolute power level of the leading AI?\n\nMy argument here is basically just that even if a given rogue AI wouldn't benefit from mass fatalities (which maybe you think is implausible, fair enough), if there are many rogue AIs in different situations, then mass fatalities become more likely.\n\nIf you think most humans dying as part of the takeover is over determined, then fair enough.\n\n> An extinction or near-extinction event seems beneficial to just about any unaligned AI that is not all-powerful enough to not have to worry about humanity at all.\n\nI think this is pretty unclear because of the possiblity of responses from human actors and the potential for slowly building up control over time. E.g., in the AI 2027 scenario, killing (large fractions of) humans at the end didn't matter almost at all for the AI's takeover prospects and at each earlier point, killing tons of humans wouldn't be helpful as this would just cause variance and the potential for stronger responses while the AI can just build up power over time.\n\nEdit: there is also a live question of \"marginal returns to death\"; e.g. does it increase your odds of takeover to kill 90% instead of 30%?",
      "plaintextDescription": "> human coup instigators generally still need humans to rule over, whereas AIs wouldn't\n\nSometimes? In countries where most wealth is mineral wealth, they actually need very few citizens. The more basic point is that human coups aren't particularly helped by indiscriminate killing while AI takeover might be. But I still think coups are surprisingly bloodless and often keeping coups bloodless is the optimal strategy for the person doing the coup. I think this transfers to AI.\n\n> I don't see how the number of AIs makes a big difference here, rather than the absolute power level of the leading AI?\n\nMy argument here is basically just that even if a given rogue AI wouldn't benefit from mass fatalities (which maybe you think is implausible, fair enough), if there are many rogue AIs in different situations, then mass fatalities become more likely.\n\nIf you think most humans dying as part of the takeover is over determined, then fair enough.\n\n> An extinction or near-extinction event seems beneficial to just about any unaligned AI that is not all-powerful enough to not have to worry about humanity at all.\n\nI think this is pretty unclear because of the possiblity of responses from human actors and the potential for slowly building up control over time. E.g., in the AI 2027 scenario, killing (large fractions of) humans at the end didn't matter almost at all for the AI's takeover prospects and at each earlier point, killing tons of humans wouldn't be helpful as this would just cause variance and the potential for stronger responses while the AI can just build up power over time.\n\nEdit: there is also a live question of \"marginal returns to death\"; e.g. does it increase your odds of takeover to kill 90% instead of 30%?"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-24T04:31:45.298Z",
    "af": false
  },
  {
    "_id": "C8BuQ7FPDdScznzPv",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "CB7eYziCCfTRmSgoN",
    "topLevelCommentId": "CB7eYziCCfTRmSgoN",
    "contents": {
      "markdown": "> I think there's a fallacy in going from \"slight caring\" to \"slight percentage of resources allocated.\"\n>\n> Suppose that preserving earth now costs one galaxy that could be colonized later. Even if that one galaxy is merely one billionth of the total reachable number, it's still an entire galaxy (\"our galaxy itself contains a hundred billion stars...\"), and its usefulness in absolute terms is very large.\n>\n> So there's a hidden step where you assume that the AIs that take over have diminishing returns\n\nNo, I'm not assuming diminishing returns or a preference for variety, when I talk about sufficient caring to result in a \"extremely small amount of motivation\" I just mean \"caring enough to allocate ~1 / billionth to 1 / trillionth of our resources on it\". This is a pretty natural notion of slight caring IMO. I agree 1 / billionth of resources will be absolutely very large to an AI and there is a type of caring which gets divided out by vast cosmic scale.\n\nIMO, it's pretty natural for slight caring to manifest in this way and the thing you describe with peaches and apples is more like \"slight preferences\". I'm not saying \"the AI will slightly prefer humans to survive over humans not surviving (but not more than what it could do with a marginal galaxy)\", I mean that it will actually pay small costs at the margin of the actual decisions it has to make. I agree that AIs might prefer to preserve earth, but have this small preference swamped by the vastness of the galatic resources they would have to give up in exchange. I just wouldn't call this an amount of caring which suffices for paying 1 / billionth of resources.\n\nI agree that the way this manifests will depend on the motivational structure of the AI, and motivational structures which purely care about the the ultimate arrangement of matter without regard to anything else will end up not caring at all about keeping humans alive (after all, why privilege humans, they just happened to be using the matter/energy to start with). (In the same way that these motivations would kill aliens to tile that matter with something that seems more optimal even though keeping these aliens alive might be insanely cheap as a fraction of resources.) Humans demonstrate motivations which don't just care about the ultimate arrangement of matter all the time, e.g. even relatively utilitarian people probably wouldn't kill everyone on earth to get an additional galaxy or kill aliens to get their stuff when it would be this cheap to leave them alone.\n\nI think the relevant type of \"kindness\" is pretty natural, though obviously not guaranteed.\n\n> If small motivations do matter, I think you can't discount \"weird\" preferences to do other things with Earth than preserve it.\n\nI don't discount this, see discussion in the section \"How likely is it that AIs will actively have motivations to kill (most/many) humans\". Note that it's pretty cheap to keep humans alive while also doing something that destroys earth.",
      "plaintextDescription": "> I think there's a fallacy in going from \"slight caring\" to \"slight percentage of resources allocated.\"\n> \n> Suppose that preserving earth now costs one galaxy that could be colonized later. Even if that one galaxy is merely one billionth of the total reachable number, it's still an entire galaxy (\"our galaxy itself contains a hundred billion stars...\"), and its usefulness in absolute terms is very large.\n> \n> So there's a hidden step where you assume that the AIs that take over have diminishing returns\n\nNo, I'm not assuming diminishing returns or a preference for variety, when I talk about sufficient caring to result in a \"extremely small amount of motivation\" I just mean \"caring enough to allocate ~1 / billionth to 1 / trillionth of our resources on it\". This is a pretty natural notion of slight caring IMO. I agree 1 / billionth of resources will be absolutely very large to an AI and there is a type of caring which gets divided out by vast cosmic scale.\n\nIMO, it's pretty natural for slight caring to manifest in this way and the thing you describe with peaches and apples is more like \"slight preferences\". I'm not saying \"the AI will slightly prefer humans to survive over humans not surviving (but not more than what it could do with a marginal galaxy)\", I mean that it will actually pay small costs at the margin of the actual decisions it has to make. I agree that AIs might prefer to preserve earth, but have this small preference swamped by the vastness of the galatic resources they would have to give up in exchange. I just wouldn't call this an amount of caring which suffices for paying 1 / billionth of resources.\n\nI agree that the way this manifests will depend on the motivational structure of the AI, and motivational structures which purely care about the the ultimate arrangement of matter without regard to anything else will end up not caring at all about keeping humans alive (after all, why privilege humans, they just happened to be using the matter/energy to s"
    },
    "baseScore": 8,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-09-24T04:18:51.264Z",
    "af": false
  },
  {
    "_id": "pYffG6zQWLpgLdPub",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "[@So8res](https://www.lesswrong.com/users/so8res?mention=user)[^uqb6b085w38] responded on twitter, copying his response here for completeness:\n\n> I appreciate your emphasis the bottom line that (IIUC) we agree upon: rogue superintelligence would permanently ruin the future and (at best) relegate humanity to something like zoo animals.\n> \n> As I understand your arguments, they boil down to (a) maybe the AI will care a little and (b) maybe the AI will make backups of humans and sell them to aliens.\n> \n> My basic response to (a) is: weak prefs about humans (if any) would probably get distorted, and existing ppl probably wouldn't be the optimum (and if they were, the result probably wouldn't be pretty). cf\n> \n> My basic response to (b) is: I'm skeptical of your apparent high probabilities (>50%?) on this outcome, which looks somewhat specific and narrow to me. I also expect most implementations to route through a step most people would call \"death\".\n> \n> In case (b), the traditional concept of \"death\" gets dodgy. (What if copies of you are sold multiple times? What if you only start running in an alien zoo billions of years later? What if the aliens distort your mind before restoring you?) I consider this topic a tangent.\n> \n> Mostly I consider these cases to be exotic enough and death-flavored enough that I don't think \"maybe AIs will sell backups of us to aliens\" merits a caveat when communicating the basic danger. But I'm happy to acknowledge the caveat, and that some people think it's likely.\n\n[^uqb6b085w38]: If Nate cross posts himself, I'll delete this comment.",
      "plaintextDescription": "@So8res[1] responded on twitter, copying his response here for completeness:\n\n> I appreciate your emphasis the bottom line that (IIUC) we agree upon: rogue superintelligence would permanently ruin the future and (at best) relegate humanity to something like zoo animals.\n> \n> As I understand your arguments, they boil down to (a) maybe the AI will care a little and (b) maybe the AI will make backups of humans and sell them to aliens.\n> \n> My basic response to (a) is: weak prefs about humans (if any) would probably get distorted, and existing ppl probably wouldn't be the optimum (and if they were, the result probably wouldn't be pretty). cf\n> \n> My basic response to (b) is: I'm skeptical of your apparent high probabilities (>50%?) on this outcome, which looks somewhat specific and narrow to me. I also expect most implementations to route through a step most people would call \"death\".\n> \n> In case (b), the traditional concept of \"death\" gets dodgy. (What if copies of you are sold multiple times? What if you only start running in an alien zoo billions of years later? What if the aliens distort your mind before restoring you?) I consider this topic a tangent.\n> \n> Mostly I consider these cases to be exotic enough and death-flavored enough that I don't think \"maybe AIs will sell backups of us to aliens\" merits a caveat when communicating the basic danger. But I'm happy to acknowledge the caveat, and that some people think it's likely.\n\n 1. ^\n    If Nate cross posts himself, I'll delete this comment."
    },
    "baseScore": 29,
    "voteCount": 14,
    "createdAt": null,
    "postedAt": "2025-09-23T21:43:30.468Z",
    "af": false
  },
  {
    "_id": "t7gAZoeyrtrJmvHBu",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "6KnMTrAc7P3oWrq5y",
    "topLevelCommentId": "6KnMTrAc7P3oWrq5y",
    "contents": {
      "markdown": "> I place much higher odds on the \"death due to takeover\" for a pretty specific reason. We seem to have an excellent takeover mechanism in place which kills all or most of us: nukes. We have a gun pointed at our collective heads, and it's deadlier to humans than AGIs.\n\nI ended up feeling moderately persuaded by an argument that human coups typically kill a much smaller fraction of people in that country. I think some of the relevant reference classes don't look that deadly and then there are some specific arguments for thinking AIs will kill lots of people as part of takeover, and I overall come to a not-that-high bottom line.\n\n> The number of likely deaths given takeover seems higher than your estimate if that logic about nukes as a route to takeover mostly goes through.\n\nMy understanding is that a large scale nuclear exchange would probably kill well less than half of people (I'd guess <25%). The initial exchange would probably directly kill <1 billion and then I don't see a strong argument for nuclear winter killing far more people. This winter would be occuring during takeoff which has an unclear effect on the number of deaths.\n\n> But the question of extinction still hinges largely on whether AGI has any interest in humanity surviving. I think you're assuming it will have a distribution of interests like humans do\n\nI don't think I'm particularly assuming this. I guess I think there is a roughly 50% chance that AI will be slightly kind in the sense needed to want to keep humans alive. Then, the rest is driven by trade. (Edit: I think trade is more important than slightly kind, but both are >50% likely.)",
      "plaintextDescription": "> I place much higher odds on the \"death due to takeover\" for a pretty specific reason. We seem to have an excellent takeover mechanism in place which kills all or most of us: nukes. We have a gun pointed at our collective heads, and it's deadlier to humans than AGIs.\n\nI ended up feeling moderately persuaded by an argument that human coups typically kill a much smaller fraction of people in that country. I think some of the relevant reference classes don't look that deadly and then there are some specific arguments for thinking AIs will kill lots of people as part of takeover, and I overall come to a not-that-high bottom line.\n\n> The number of likely deaths given takeover seems higher than your estimate if that logic about nukes as a route to takeover mostly goes through.\n\nMy understanding is that a large scale nuclear exchange would probably kill well less than half of people (I'd guess <25%). The initial exchange would probably directly kill <1 billion and then I don't see a strong argument for nuclear winter killing far more people. This winter would be occuring during takeoff which has an unclear effect on the number of deaths.\n\n> But the question of extinction still hinges largely on whether AGI has any interest in humanity surviving. I think you're assuming it will have a distribution of interests like humans do\n\nI don't think I'm particularly assuming this. I guess I think there is a roughly 50% chance that AI will be slightly kind in the sense needed to want to keep humans alive. Then, the rest is driven by trade. (Edit: I think trade is more important than slightly kind, but both are >50% likely.)"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-23T21:05:55.065Z",
    "af": false
  },
  {
    "_id": "afQDGdzEa54TCRCaF",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "TqqN34ec7vgLPuhHf",
    "topLevelCommentId": "TqqN34ec7vgLPuhHf",
    "contents": {
      "markdown": "Suppose humans retained control and came across some aliens who didn't want to be replaced with computronium and wanted to live on their planet. Do you think that humans would do something to these aliens as bad (from their perspective) as killing them?\n\nI think this type of caring (where you respect the actual preferences of existing intelligent agents) seems reasonably plausible to me.",
      "plaintextDescription": "Suppose humans retained control and came across some aliens who didn't want to be replaced with computronium and wanted to live on their planet. Do you think that humans would do something to these aliens as bad (from their perspective) as killing them?\n\nI think this type of caring (where you respect the actual preferences of existing intelligent agents) seems reasonably plausible to me."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-23T20:57:35.105Z",
    "af": false
  },
  {
    "_id": "jpMFws7CqcT2sBBst",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "vKxw9DaoDbWxXeAXF",
    "topLevelCommentId": "Sfxpy8DMDJXNEc6TM",
    "contents": {
      "markdown": "> You note \"seems like it'd only slow down the AI a bit, to first round up humans\". I haven't done a calculation about it, but, seemed potentially like a very big deal to decide between \"full steam ahead on beginning the dyson sphere using Earth Parts\" vs \"start the process from the moon and then mercury\", since there are harder-to-circumvent delays there.\n\nCan't the AI proceed full steam ahead until it has enough industrial capacity to build shelters, then build shelters and put humans in the shelters (while pausing as needed at this point to avoid fatalities from environmental damage while humans are being rounded up), and then finish the industrial expansion (possibly upgrading shelters along the way as needed as the AI gets more resources)? Seems like naively this only delays you for as long as is needed to round up humans and up them in shelter which seems probably <1 year and probably <1 month. (At least if takeoff is pretty fast.)\n\nSeparately, not destroying the earth (and instead doing more of the growth in space) seems like it should cost <3 years of delay and probably <1 year of delay which is still pretty small as an absolute fraction of resources (for patient AIs). Like we're talking 1 / billion or something.",
      "plaintextDescription": "> You note \"seems like it'd only slow down the AI a bit, to first round up humans\". I haven't done a calculation about it, but, seemed potentially like a very big deal to decide between \"full steam ahead on beginning the dyson sphere using Earth Parts\" vs \"start the process from the moon and then mercury\", since there are harder-to-circumvent delays there.\n\nCan't the AI proceed full steam ahead until it has enough industrial capacity to build shelters, then build shelters and put humans in the shelters (while pausing as needed at this point to avoid fatalities from environmental damage while humans are being rounded up), and then finish the industrial expansion (possibly upgrading shelters along the way as needed as the AI gets more resources)? Seems like naively this only delays you for as long as is needed to round up humans and up them in shelter which seems probably <1 year and probably <1 month. (At least if takeoff is pretty fast.)\n\nSeparately, not destroying the earth (and instead doing more of the growth in space) seems like it should cost <3 years of delay and probably <1 year of delay which is still pretty small as an absolute fraction of resources (for patient AIs). Like we're talking 1 / billion or something."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T19:35:49.927Z",
    "af": false
  },
  {
    "_id": "dACCFswAaZiJLdMFB",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "EDTtJiWzvz6K5C9r3",
    "topLevelCommentId": "Sfxpy8DMDJXNEc6TM",
    "contents": {
      "markdown": "It's kinda messy how to think about non-consensual uploads that the person is totally fine with after some reflection (especially if the reason the AI did the upload is because it know the person would be fine with this on reflection). I also don't think considering uploads without informed consent to be death makes a big difference to my numbers.\n\n> I think this sort of thing is also why I don't think the \"AI may be very slightly nice\" is likely to result in something that's clearly \"not death\". It seems really unlikely to me that very slightly nice things would go the \"preserve parochial humans exactly as is\", it's just such a narrow target to hit even within niceness.\n\nI don't really agree? It seems like \"don't upload people without their consent when they consider this to be death unless there are no better options\" is pretty natural and the increase in resources for keeping people physically alive is pretty small.",
      "plaintextDescription": "It's kinda messy how to think about non-consensual uploads that the person is totally fine with after some reflection (especially if the reason the AI did the upload is because it know the person would be fine with this on reflection). I also don't think considering uploads without informed consent to be death makes a big difference to my numbers.\n\n> I think this sort of thing is also why I don't think the \"AI may be very slightly nice\" is likely to result in something that's clearly \"not death\". It seems really unlikely to me that very slightly nice things would go the \"preserve parochial humans exactly as is\", it's just such a narrow target to hit even within niceness.\n\nI don't really agree? It seems like \"don't upload people without their consent when they consider this to be death unless there are no better options\" is pretty natural and the increase in resources for keeping people physically alive is pretty small."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-23T19:31:09.244Z",
    "af": false
  },
  {
    "_id": "aYzDFrWvDbesv6ibN",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "Sfxpy8DMDJXNEc6TM",
    "topLevelCommentId": "Sfxpy8DMDJXNEc6TM",
    "contents": {
      "markdown": "> How much of your plausibility-of-caring-slightly routes through this, vs the \"actually just slightly intrinisically nice?\" thing.\n\nMaybe 60% trade, 40% intrinsic? Idk though.\n\n> I'm surprised at the \"only 3 OOMs more expensive.\" I haven't done a calculation but that seems really implausible. Maybe I am not imagining how big an OOM is properly.\n>\n> In particular, comparing \"keep humans alive indefinitely, multigenerationally\" vs \"store them on ice without evening running them, turn them back on when/if you trade them.\" \n\nThe dominant cost of \"keeping humans alive physically\" from a linear-returns+patient industrial expansion perspective is a small delay from rounding humans up into shelters (that you have to build and supply etc) or avoiding boiling the oceans (and other catastrophic environmental damage). The dominant cost of uploads is the small delay from rounding up and scanning people before they die. They both seem small, but the delays seem comparable.\n\nGiving humans an entire star long term (e.g. after a few decades) is negligible in cost (like, $<<10^{-20}$ of all galactic resources) relative to this delay (edit: for patient AIs which don't prefer resources closer to earth),  so I think keeping humans physically alive longer term is totally fine and just has higher upfront costs.\n\nThere's also the cost of not killing humans as part of a takeover attempt and not using them for some other purpose (reasons (1) and (3)), but these are equal between uploads and physically keeping humans alive. I wasn't intending to include this as I said \"after AIs have fully solidified their power\", but if I did, then this makes the gap much smaller depending on how important reason (1) is. ",
      "plaintextDescription": "> How much of your plausibility-of-caring-slightly routes through this, vs the \"actually just slightly intrinisically nice?\" thing.\n\nMaybe 60% trade, 40% intrinsic? Idk though.\n\n> I'm surprised at the \"only 3 OOMs more expensive.\" I haven't done a calculation but that seems really implausible. Maybe I am not imagining how big an OOM is properly.\n> \n> In particular, comparing \"keep humans alive indefinitely, multigenerationally\" vs \"store them on ice without evening running them, turn them back on when/if you trade them.\"\n\nThe dominant cost of \"keeping humans alive physically\" from a linear-returns+patient industrial expansion perspective is a small delay from rounding humans up into shelters (that you have to build and supply etc) or avoiding boiling the oceans (and other catastrophic environmental damage). The dominant cost of uploads is the small delay from rounding up and scanning people before they die. They both seem small, but the delays seem comparable.\n\nGiving humans an entire star long term (e.g. after a few decades) is negligible in cost (like, <<10−20 of all galactic resources) relative to this delay (edit: for patient AIs which don't prefer resources closer to earth), so I think keeping humans physically alive longer term is totally fine and just has higher upfront costs.\n\nThere's also the cost of not killing humans as part of a takeover attempt and not using them for some other purpose (reasons (1) and (3)), but these are equal between uploads and physically keeping humans alive. I wasn't intending to include this as I said \"after AIs have fully solidified their power\", but if I did, then this makes the gap much smaller depending on how important reason (1) is."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T19:23:46.403Z",
    "af": false
  },
  {
    "_id": "fzdTo9cNq4rjw6QHG",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "qvpJ3LWj9zB74tixK",
    "topLevelCommentId": "qvpJ3LWj9zB74tixK",
    "contents": {
      "markdown": "sure",
      "plaintextDescription": "sure"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-23T18:32:27.987Z",
    "af": false
  },
  {
    "_id": "RBKLK9H4KGqRtTNCe",
    "postId": "4YvSSKTPhPC43K3vn",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "My best guess is that the elicitation overhang has reduced or remained stable after the release of GPT-4. I think we've now elicited much more of the underlying \"easy to access\" capabilities with RL, and there is a pretty good a priori case that scaled up RL should do an OK job eliciting capabilities (at least when considering elicitation that uses the same inference budget used in RL) on at least the distribution of tasks which models are RL'd on.\n\nIt seems totally possible that performance on messier real world tasks with hard-to-check objectives is under elicited, but we can roughly bound this elicitation gap by looking at performance on the types of tasks which are RL'd on and then conservatively assuming that models could perform this well on messy tasks with better elicitation (figuring out the exact transfer isn't trivial, but often we can get a conservative bound which isn't too scary).\n\nAdditionally, it seems possible that you could use vast amounts of inference compute (e.g. comparable to human wages or much more than human wages) more effectively and elicit more performance using this. But, GPT-5 reasoning on high is already decently expensive and this is probably not a terrible use of inference compute, so we don't have a ton of headroom here. We can get a rough sense of the returns to additional inference compute by looking at scaling with existing methods and the returns don't look that amazing overall. I could imagine this effectively moving you forward in time by like 6-12 months though (as in, you'd see the capabilities we'd see in 6-12 months, but at much higher cost, so this would be somewhat impractical).\n\nAnother potential source of elicitation overhang is niche superhuman capabilities being better elicited and leveraged. I don't have a strong view on this, but we haven't really seen this.\n\nNone of this is to say there isn't an elicitation overhang, I just think it probably hasn't really increased recently. (I can't really tell if this post is trying to argue the overhang is increasing or just that there is some moderately sized overhang ongoingly.) For instance, it seems pretty plausible to me that a decent amount of elicitation effort could make models provide much more bioweapon uplift and this dynamic means that open weight models are more dangerous than they seem ([this doesn't necessarily mean the costs outweigh the benefits](https://www.lesswrong.com/posts/TeF8Az2EiWenR9APF/when-is-it-important-that-open-weight-models-aren-t-released)).",
      "plaintextDescription": "My best guess is that the elicitation overhang has reduced or remained stable after the release of GPT-4. I think we've now elicited much more of the underlying \"easy to access\" capabilities with RL, and there is a pretty good a priori case that scaled up RL should do an OK job eliciting capabilities (at least when considering elicitation that uses the same inference budget used in RL) on at least the distribution of tasks which models are RL'd on.\n\nIt seems totally possible that performance on messier real world tasks with hard-to-check objectives is under elicited, but we can roughly bound this elicitation gap by looking at performance on the types of tasks which are RL'd on and then conservatively assuming that models could perform this well on messy tasks with better elicitation (figuring out the exact transfer isn't trivial, but often we can get a conservative bound which isn't too scary).\n\nAdditionally, it seems possible that you could use vast amounts of inference compute (e.g. comparable to human wages or much more than human wages) more effectively and elicit more performance using this. But, GPT-5 reasoning on high is already decently expensive and this is probably not a terrible use of inference compute, so we don't have a ton of headroom here. We can get a rough sense of the returns to additional inference compute by looking at scaling with existing methods and the returns don't look that amazing overall. I could imagine this effectively moving you forward in time by like 6-12 months though (as in, you'd see the capabilities we'd see in 6-12 months, but at much higher cost, so this would be somewhat impractical).\n\nAnother potential source of elicitation overhang is niche superhuman capabilities being better elicited and leveraged. I don't have a strong view on this, but we haven't really seen this.\n\nNone of this is to say there isn't an elicitation overhang, I just think it probably hasn't really increased recently. (I can't really tell if this post is t"
    },
    "baseScore": 34,
    "voteCount": 20,
    "createdAt": null,
    "postedAt": "2025-09-23T16:17:58.683Z",
    "af": false
  },
  {
    "_id": "R2uSFn3cFgKvL3ovR",
    "postId": "Xp9ie6pEWFT8Nnhka",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> But, I do think it'd be good for someone to write a really fleshed out takeoff story and/or forecast that runs with those assumptions.\n\nYou might be interested in [this episode of Epoch After Hours](https://epoch.ai/epoch-after-hours/ai-in-2040). I think this is pretty close to what you want.",
      "plaintextDescription": "> But, I do think it'd be good for someone to write a really fleshed out takeoff story and/or forecast that runs with those assumptions.\n\nYou might be interested in this episode of Epoch After Hours. I think this is pretty close to what you want."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-23T03:07:39.978Z",
    "af": false
  },
  {
    "_id": "pLqBXfP9WdwLhWaTg",
    "postId": "JWH63Aed3TA2cTFMt",
    "parentCommentId": "keZcn2ynewoXSqukp",
    "topLevelCommentId": "ndvkPbyRMhEJD9NMx",
    "contents": {
      "markdown": "> I can't actually think of that many cases of humans failing at aligning existing systems because the problem is too technically hard.\n\nYou're probably already tracking this, but the biggest cases of \"alignment was actually pretty tricky\" I'm aware are:\n\n- Recent systems doing egregious reward hacking in some cases (including o3, 3.7 sonnet, and 4 Opus). This problem has gotten better recently (and I currently expect it to mostly get better over time, prior to superhuman capabilities), but AI companies knew about the problem before release and couldn't solve the problem quickly enough to avoid deploying a model with this property. And note this is pretty costly to consumers!\n- There are a bunch of aspects of current AI propensities which are undesired and AI companies don't know how to reliably solve these in a way that will actually generalize to similar such problems. For instance, see the model card for opus 4 which includes the model doing a bunch of undesired stuff that Anthropic doesn't want but also can't easily avoid except via patching it non-robustly (because they don't necessarily know exactly what causes the issue).\n\nNone of these are cases where alignment was extremely hard TBC, though I think it might be extremely hard to consistently avoid *all* alignment problems of this rough character before release. It's unclear whether this sort of thing is a good analogy for misalignment in future models which would be catastrophic.",
      "plaintextDescription": "> I can't actually think of that many cases of humans failing at aligning existing systems because the problem is too technically hard.\n\nYou're probably already tracking this, but the biggest cases of \"alignment was actually pretty tricky\" I'm aware are:\n\n * Recent systems doing egregious reward hacking in some cases (including o3, 3.7 sonnet, and 4 Opus). This problem has gotten better recently (and I currently expect it to mostly get better over time, prior to superhuman capabilities), but AI companies knew about the problem before release and couldn't solve the problem quickly enough to avoid deploying a model with this property. And note this is pretty costly to consumers!\n * There are a bunch of aspects of current AI propensities which are undesired and AI companies don't know how to reliably solve these in a way that will actually generalize to similar such problems. For instance, see the model card for opus 4 which includes the model doing a bunch of undesired stuff that Anthropic doesn't want but also can't easily avoid except via patching it non-robustly (because they don't necessarily know exactly what causes the issue).\n\nNone of these are cases where alignment was extremely hard TBC, though I think it might be extremely hard to consistently avoid all alignment problems of this rough character before release. It's unclear whether this sort of thing is a good analogy for misalignment in future models which would be catastrophic."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-23T00:06:52.188Z",
    "af": false
  },
  {
    "_id": "3q4jsSe9YfdCX6KJe",
    "postId": "pzChyAuu7ggmLd75B",
    "parentCommentId": "GioM88xDDky4GictG",
    "topLevelCommentId": "GioM88xDDky4GictG",
    "contents": {
      "markdown": "It's a cost and convenience thing. See discussion [here](https://docs.google.com/document/d/1fSaw3NArDj2ndbem96V4E7x4hAlEaqrrBawjf0wNQ-A/edit?tab=t.0#heading=h.z8q3vakmkive) for some ideas for better methods that are also cheap.",
      "plaintextDescription": "It's a cost and convenience thing. See discussion here for some ideas for better methods that are also cheap."
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-22T22:50:30.385Z",
    "af": false
  },
  {
    "_id": "EP2DuyyXG4MmHRkBY",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "XfrfwJrDKrvnfHEB8",
    "topLevelCommentId": "Qx2sDtbgdC3qcmNiN",
    "contents": {
      "markdown": "> Yes, primarily due to the asymmetry where capabilities can work with existing systems while alignment is mostly stuck waiting for future systems, but that should be much less true by the time of DAI.\n\nI think studying scheming in current/future systems has ongoingly worse feedback loops? Like suppose our DAI level system wants to study scheming in a +3 SD DAI level system. This is structurally kinda tricky because schemers try to avoid detection. I agree having access to capable AIs makes this much easier to get good feedback loops, but there is an asymmetry.",
      "plaintextDescription": "> Yes, primarily due to the asymmetry where capabilities can work with existing systems while alignment is mostly stuck waiting for future systems, but that should be much less true by the time of DAI.\n\nI think studying scheming in current/future systems has ongoingly worse feedback loops? Like suppose our DAI level system wants to study scheming in a +3 SD DAI level system. This is structurally kinda tricky because schemers try to avoid detection. I agree having access to capable AIs makes this much easier to get good feedback loops, but there is an asymmetry."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-22T18:57:13.046Z",
    "af": false
  },
  {
    "_id": "KG9ikcvqabhJDM8YG",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "faAkLKRAt8dr2dNdE",
    "topLevelCommentId": "Qx2sDtbgdC3qcmNiN",
    "contents": {
      "markdown": "Do you agree the feedback loops for capabilities are better right now?\n\n> Sure, AI societies could go off the rails that hurts alignment R&D but not AI R&D; they could also go off the rails in a way that hurts AI R&D and not alignment R&D. Not sure why I should expect one rather than the other.\n\nFor this argument it's not a crux that it is asymmetric (though due to better feedback for AI R&D I think it actually is). E.g., suppose that in 10% of worlds safety R&D goes totally off the rails while capabilities proceed and in 10% of worlds capabilities R&D goes off totally the rails while safety proceeds. This still results in an additional 10% takeover risk from the subset of worlds where safety R&D goes off the rails. (Edit: though risk could be lower in the worlds where capabilities R&D goes off the rails due to having more time for safety, depending on whether this also applies to the next actor etc.)",
      "plaintextDescription": "Do you agree the feedback loops for capabilities are better right now?\n\n> Sure, AI societies could go off the rails that hurts alignment R&D but not AI R&D; they could also go off the rails in a way that hurts AI R&D and not alignment R&D. Not sure why I should expect one rather than the other.\n\nFor this argument it's not a crux that it is asymmetric (though due to better feedback for AI R&D I think it actually is). E.g., suppose that in 10% of worlds safety R&D goes totally off the rails while capabilities proceed and in 10% of worlds capabilities R&D goes off totally the rails while safety proceeds. This still results in an additional 10% takeover risk from the subset of worlds where safety R&D goes off the rails. (Edit: though risk could be lower in the worlds where capabilities R&D goes off the rails due to having more time for safety, depending on whether this also applies to the next actor etc.)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-22T16:55:52.787Z",
    "af": false
  },
  {
    "_id": "jZDfWEJAbEy8ggaSE",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "pDLNx376XPTafxmen",
    "topLevelCommentId": "Qx2sDtbgdC3qcmNiN",
    "contents": {
      "markdown": "I don't think I understand this comment.\n\nIt sounds like you're saying:\n\n\"Slower takeoff should be correlated with 'harder' alignment (in terms of cognitive labor requirements) because slower takeoff implies returns to cognitive labor in capabilities R&D are relatively lower and we should expect this means that alignment returns to cognitive labor are relatively lower (due to common causes like 'small experiments and theory don't generalize well and it is hard to work around this'). For the same reasons, faster takeoff should be correlated with 'easier' alignment.\"\n\nI think I agree with this mostly, though there are some reasons for anti-correlation, e.g., worlds where there is a small simple core to intelligence which can be found substantially from first principles make alignment harder, in practice there is an epistemic correlation among humans between absolute alignment difficulty (in terms of cognitive labor requirements) and slower takeoff.\n\nI don't really understand why this should extremize my probabilities, but I agree this correlation isn't accounted for at all in my analysis.",
      "plaintextDescription": "I don't think I understand this comment.\n\nIt sounds like you're saying:\n\n\"Slower takeoff should be correlated with 'harder' alignment (in terms of cognitive labor requirements) because slower takeoff implies returns to cognitive labor in capabilities R&D are relatively lower and we should expect this means that alignment returns to cognitive labor are relatively lower (due to common causes like 'small experiments and theory don't generalize well and it is hard to work around this'). For the same reasons, faster takeoff should be correlated with 'easier' alignment.\"\n\nI think I agree with this mostly, though there are some reasons for anti-correlation, e.g., worlds where there is a small simple core to intelligence which can be found substantially from first principles make alignment harder, in practice there is an epistemic correlation among humans between absolute alignment difficulty (in terms of cognitive labor requirements) and slower takeoff.\n\nI don't really understand why this should extremize my probabilities, but I agree this correlation isn't accounted for at all in my analysis."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-22T15:09:42.456Z",
    "af": false
  },
  {
    "_id": "T9ehPWmbPxxSDoKvk",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "ppfeGpRBJk3t7u2qF",
    "topLevelCommentId": "Qx2sDtbgdC3qcmNiN",
    "contents": {
      "markdown": ">  The key disanalogy between capabilities and alignment work at the point of DAI is that the DAI might be scheming, but you're in a subjunctive case where we've assumed the DAI is not scheming. Whence the pessimism?\n\nI don't think this is the only disanalogy. It seems to me like getting AIs to work efficiently on automating AI R&D might not result in solving all the problems you need to solve for it to be safe to hand off ~all x-safety labor to AIs. This is a mix of capabilities, elicitation, and alignment. This is similar to how a higher level of mission alignment might be required for AI company employees working on conceptually tricky alignment research relative to advancing AI R&D.\n\nAnother issue is that AI societies might go off the rails over some longer period in some way which doesn't eliminate AI R&D productivity, but would be catastrophic from an alignment perspective.\n\nThis isn't to say there isn't anything which is hard to check or conceptually tricky about AI R&D, just that the feedback loops seem much better.",
      "plaintextDescription": "> The key disanalogy between capabilities and alignment work at the point of DAI is that the DAI might be scheming, but you're in a subjunctive case where we've assumed the DAI is not scheming. Whence the pessimism?\n\nI don't think this is the only disanalogy. It seems to me like getting AIs to work efficiently on automating AI R&D might not result in solving all the problems you need to solve for it to be safe to hand off ~all x-safety labor to AIs. This is a mix of capabilities, elicitation, and alignment. This is similar to how a higher level of mission alignment might be required for AI company employees working on conceptually tricky alignment research relative to advancing AI R&D.\n\nAnother issue is that AI societies might go off the rails over some longer period in some way which doesn't eliminate AI R&D productivity, but would be catastrophic from an alignment perspective.\n\nThis isn't to say there isn't anything which is hard to check or conceptually tricky about AI R&D, just that the feedback loops seem much better."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-22T15:01:21.667Z",
    "af": false
  },
  {
    "_id": "tHPtEhyYa7JbD6Rr8",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "pn5ixLBKB2zeLEfae",
    "topLevelCommentId": "Qx2sDtbgdC3qcmNiN",
    "contents": {
      "markdown": "> I was glad to have read it, but am a bit confused about your numbers seeming different from the ones I objected to.\n\nI gave \"1 million AIs somewhat smarter than humans with the equivalent of 100 years each\" as an example of a situation I thought wouldn't count as \"anything like current techniques/understanding\". In this comment, I picked a lower number which is maybe my best guess for an amount of labor which eliminates most of the risk by a given level of capability.\n\nI do think that \"a factor of 5 and a factor of 10 different\" is within my margin of error for amount of labor you need. (Note that there might be aggressively diminishing returns on parallel labor, though possibly not due to very superhuman coordination abilities by AI.)\n\nMy modeling/guesses are pretty shitty in this comment (I was just picking some numbers to see how things work out), so if that's a crux, I should probably try to be thoughtful (I was trying to write this quickly to get something written up).",
      "plaintextDescription": "> I was glad to have read it, but am a bit confused about your numbers seeming different from the ones I objected to.\n\nI gave \"1 million AIs somewhat smarter than humans with the equivalent of 100 years each\" as an example of a situation I thought wouldn't count as \"anything like current techniques/understanding\". In this comment, I picked a lower number which is maybe my best guess for an amount of labor which eliminates most of the risk by a given level of capability.\n\nI do think that \"a factor of 5 and a factor of 10 different\" is within my margin of error for amount of labor you need. (Note that there might be aggressively diminishing returns on parallel labor, though possibly not due to very superhuman coordination abilities by AI.)\n\nMy modeling/guesses are pretty shitty in this comment (I was just picking some numbers to see how things work out), so if that's a crux, I should probably try to be thoughtful (I was trying to write this quickly to get something written up)."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-22T14:56:51.561Z",
    "af": false
  },
  {
    "_id": "MQndsjXejBB3K9HYF",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "AfFnAn67npdfBzL7W",
    "topLevelCommentId": "Qx2sDtbgdC3qcmNiN",
    "contents": {
      "markdown": "Thanks for the nudge! I currently disagree with \"very unlikely\", but more importantly, I noticed that I haven't really properly analyzed the question of \"given how much cognitive labor is available between different capability levels, should we expect that alignment can keep up with capabilities if a small fraction (e.g. 5%) is ongoingly spent on alignment (in addition to whatever alignment-ish work is directly commercially expedient)\". I should spend more time thinking about this question and it seems plausible I'll end up updating towards thinking risk is substantially higher/lower on the basis of this. I think I was underestimating the case that even if AIs are reasonably aligned, it might just be seriously hard for them to improve alignment tech fast enough to keep up with capabilities (I wasn't ignoring this in my prior thinking, but I when I thought about some examples, the situation seemed worse than I was previously thinking), so I currently expect to update towards thinking risk is higher.\n\n---\n\n(At least somewhat rambly from here on.)\n\nThe short reason why I currently disagree: it seems pretty likely that we'll have an absolutely very large amount of cognitive labor (in parallel copies, serial speed, and quality) before we're cooked because improving algorithms will require more and more cognitive resources before it's too late. This depends on the takeoff dynamics. If you think that going from \"can full automate AI R&D\" to \"level of capability where we are totally cooked\" given a fixed amount of compute will take a small amount of cognitive labor, then you get a proportionally small amount of cognitive labor for safety. I think the corresponding level of cognitive labor might actually be very large, especially in worlds where takeoff is relatively smaller. In general, a key question is \"how do the returns to cognitive labor compare between capabilities and alignment: how does the amount of cognitive labor needed to advance from X to Y capability level compare to the amount of cognitive labor needed to progress alignment from the point where AIs of X capability level are safe/aligned to the point where AIs of Y capability level are safe/aligned?\" This is basically a question about the relative returns to cognitive labor.\n\n---\n\nI'll try to do some initial analysis here (partially just to start off my thinking about this).\n\nI'll suppose that the AI company spends 5% of its resources on \"trying to reduce takeover risk alignment/safety work (x-safety)\" at each point in addition to whatever it spends on solving related problems which are commercially expedient and obviously incentivized (e.g. if training runs aren't working as well due to reward hacking the company's efforts to fix this aren't included in the 5% budget, though the 5% budget might subsidize some of this work). Initially, this includes time of human employees, but eventually human labor is irrelevant.\n\nFor simplicity, I'll assume there is no chance of spending more than 5% or slowing down more than this (when averaged over a every few development cycles, including once these development cycles speed up, short slow downs and spikes in spending are fine). In practice, I do think that empirical evidence and AIs warning that the situation is dangerous are very important, especially after handing off decision making to AIs (insofar as this happens).\n\nPrior to full automation, employees working on x-safety are accelerated by AI and then once x-safety work is fully automated, we can still think of the rate of progress as a multiplier on the \"without AI\" speed. However, there is a difference in that if AIs we hand off to are seriously misalignment we're ~totally fucked while this isn't true prior to this point.\n\nI'll talk about a specific level of capability: Capable enough to handoff all safety work (which is strictly more capable than full automated AI R&D, but maybe less capable than \"can dominate top human experts at ~everything\"). I'll call this level of capability \"DAI\" (Deferable AI).\n\nWe can then divide the problem into roughly two parts:\n\n- By the time we're at DAI, will we be able to align DAI (and also ensure these AIs are well elicited and have good epistemics/wisdom)? (As in, at least within some short period of DAI level capabilities.)\n- Conditional on successfully aligning DAI (including via \"lame\" prosaic techniques which aren't themselves very scalable), if we hand over to these AIs can they ongoingly ensure AIs remain aligned/safe given that capabilities keep going with 95% of resources?[^hope]\n\n[^hope]: In practice, I hope that these AIs might be able to advice AI company leadership to take a different path, but we're assuming that away and assuming that AI company leadership retains enough control to make these AIs spend 95% on capabilities.\n\nIt's a little hard for me to tell if what you're saying is a claim that the first part isn't going to work out or the second part isn't going to work out. I'll go through both mostly talking about \"is it very unlikely that we'll make it\".\n\n## Aligning DAI\n\nOn the first: we get moderate but not massive acceleration by DAI. Supposing DAI is made in late 2030 (5 years from now), then maybe we see superhuman coder a year before this and some moderate acceleration ([3x AI R&D labor acceleration](https://www.lesswrong.com/posts/LjgcRbptarrRfJWtR/a-breakdown-of-ai-capability-levels-focused-on-ai-r-and-d)) a year before that. Supposing moderately good but not great elicitation and that AIs aren't scheming, I think we slightly conservatively get an average of like 2.5x AI safety R&D acceleration between moderate acceleration and superhuman coder and like 4x acceleration on average between superhuman coder and DAI. (Initially this is mostly due to accelerating engineering / running experiments and then it gets more broad over time.) Ok, so we get roughly 2.5 years (post acceleration) to do research and study AIs in the capability range of [moderate acceleration to superhuman coder] and 4 years to study AIs between [superhuman coder and DAI]. This is as well as 3 years with AIs that aren't even capable of moderate acceleration. (Presumably the AI safety field is growing over this period?)\n\nNote that average acceleration over some period can be dominated by acceleration at the end if it's increasing over time.\n\n(I can explain where these numbers come from if this is helpful.)\n\nBut, what about the possibility of scheming? I think superhuman coder is only like [20% likely](https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming#P_scheming__under_various_scenarios__putting_aside_mitigations_) to be scheming (for a moderately strong notion of scheming, see [link](https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming)) by default and mitigations might somewhat lower this. DAI is like 40% likely to be scheming. So even if we fully YOLO scheming, on my views we're not totally screwed due to this alone. In practice, work could substantially lower these risks. If we had to guess, maybe I think that in practice the risk of scheming from DAI will be cut by a factor of 3 given 5% effort on well allocated safety and this scenario? Idk? So maybe we eat like 15% risk here? If allocated as well as I expect current frontier AI companies to allocate, I'd guess more like 25% risk? (shrug.) For now, I'll assume that if DAI is scheming we immediately lose (which is roughly true because we have to hand off quickly), but that superhuman coder scheming isn't fatal (which doesn't matter that much for the bottom line risk number anyway).\n\nOk, but it isn't sufficient to just ensure DAI isn't scheming, we have to also ensure it is aligned enough to hand off work and has good epistemics and is well elicited on hard to check tasks. This seems pretty hard given the huge rush, but it isn't obviously fucked IMO, especially given the extra years from acceleration. I have some draft writing on this which should hopefully be out somewhat soon. Maybe my view is 20% chance of failure given good allocation and roughly 60% chance of failure given the default allocation (which includes stuff like the safety team not actually handing off or not seriously working on this etc)?\n\nEdit: I now think 60% is too pessimistic, though I don't know what I think overall.\n\nMy current sense is that current technical AI x-safety work isn't very well targeted for ensuring DAI isn't scheming and making DAI safe to hand off to, but it also isn't pessimally targeted for this. I tenatively think the targeting will improve over time and that it has improved over the last few years.\n\n(In practice, the situation is messier and mostly more optimistic than this than this number appears because people will pursue plans other than handing off at DAI, there is the potential for slowing down, the takeoff might look different, trailing actors might have some influence, etc. Like, 60% chance of failing to handover to DAI is partially due to people not even targeting handover and their alternative plans would have some chance of working.)\n\nOverall, we're at like ~70% doom given default allocation and more like ~25% doom given an ideal allocation. Doesn't seem overwhemingly likely to be catastrophic so far given my views (though I agree it doesn't look good!). Not sure where you disagree.\n\n## Does aligned DAI suffice?\n\nAt each point, our aligned AIs have to align (or possibly temporarily control?) further AIs as the mad capabilities race using 95% of resources continues. How fast do things progress given algorithmic progress which is accelerated by AI R&D? Well, there is some chance things actually aren't that fast because of diminishing returns to algorithmic progress, some chance for decent amount of acceleration, and some chance we get wildly, wildly superhuman AIs in a few months (if AIs haven't taken over and put an end to the insanity by that level of capability).\n\nReally, we actually care about how the rate of capabilities progress compares to alignment, so things which just make the labor supply/efficacy bigger should mostly hit on both sides.\n\nOk, so how does capabilities progress compare to alignment? One really stupid guess is:\n\nIf we get the equivalent of 20 serial years of DAI-level labor (from >100k DAI level parallel agents given a proportional amount of compute) before +3 SDs over DAI we're fine because we have a scalable solution to alignment. Otherwise takeover. (This is somewhat more conservative than my actual view.)\n\nIn practice, there is some continuous alignment progress vs capabilities question.\n\nNaively, maybe DAI runs at like 40x human speed and we probably have enough parallel agents that 5% of the pool is >100k agents. So, 20 serial years takes 0.5 calender years. So naively, we're in trouble if we get +3 SDs within 0.5 years. I think this is like [65% likely](https://www.lesswrong.com/posts/hpjj4JgRw9akLMRu5/what-does-10x-ing-effective-compute-get-you), so we're in trouble so far. But before we get +3 SDs, we get to +1 SD and +2 SD as well increases to speed etc. Maybe the +1 SD AIs are all considered as good as like ~3x faster given compute bottlenecks and +2 SD is like 10x faster?  So, assuming progress in SD is roughly linear over time (probably wrong, but hopefully not too off), we get that we make it if we get +3 SD AIs in >~0.05 years which seems much more likely.\n\nThis is pretty shitty oversimplified modeling, but it overall makes me feel at-least-somewhat optimistic about recursive alignment outpacing recursive self-improvement given my views on takeoff speeds. Like maybe 80% chance things are fine?\n\n[Ok, I have to stop writing this comment and do something else]",
      "plaintextDescription": "Thanks for the nudge! I currently disagree with \"very unlikely\", but more importantly, I noticed that I haven't really properly analyzed the question of \"given how much cognitive labor is available between different capability levels, should we expect that alignment can keep up with capabilities if a small fraction (e.g. 5%) is ongoingly spent on alignment (in addition to whatever alignment-ish work is directly commercially expedient)\". I should spend more time thinking about this question and it seems plausible I'll end up updating towards thinking risk is substantially higher/lower on the basis of this. I think I was underestimating the case that even if AIs are reasonably aligned, it might just be seriously hard for them to improve alignment tech fast enough to keep up with capabilities (I wasn't ignoring this in my prior thinking, but I when I thought about some examples, the situation seemed worse than I was previously thinking), so I currently expect to update towards thinking risk is higher.\n\n----------------------------------------\n\n(At least somewhat rambly from here on.)\n\nThe short reason why I currently disagree: it seems pretty likely that we'll have an absolutely very large amount of cognitive labor (in parallel copies, serial speed, and quality) before we're cooked because improving algorithms will require more and more cognitive resources before it's too late. This depends on the takeoff dynamics. If you think that going from \"can full automate AI R&D\" to \"level of capability where we are totally cooked\" given a fixed amount of compute will take a small amount of cognitive labor, then you get a proportionally small amount of cognitive labor for safety. I think the corresponding level of cognitive labor might actually be very large, especially in worlds where takeoff is relatively smaller. In general, a key question is \"how do the returns to cognitive labor compare between capabilities and alignment: how does the amount of cognitive labor needed to adv"
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-09-22T02:29:19.912Z",
    "af": false
  },
  {
    "_id": "XqfMauRhpr4kCSGmP",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "B7pbKgE3W7Ha3qdQG",
    "topLevelCommentId": "Qx2sDtbgdC3qcmNiN",
    "contents": {
      "markdown": "> I mean, I also believe that if we solve the alignment problem, then we will no longer have an alignment problem, and I predict the same is true of Nate and Eliezer.\n\nBy \"superintelligence\" I mean \"systems which are qualititatively much smarter than top human experts\". (If Anyone Builds It, Everyone Dies seems to define ASI in a way that could include weaker levels of capability, but I'm trying to refer to what I see as the typical usage of the term.)\n\nSometimes, people say that \"aligning superintelligence is hard because it will be much smarter than us\". I agree, this seems like this makes aligning superintelligence much harder [for multiple reasons](https://www.lesswrong.com/posts/NDotm7oLHfR56g4sD/why-do-misalignment-risks-increase-as-ais-get-more-capable).\n\nCorrespondingly, I'm noting that if we can align earlier systems which are just capable enough to obsolete human labor (which IMO seems way easier than directly aligning wildly superhuman systems), these systems might be able to ongoingly align their successors. I wouldn't consider this \"solving the alignment problem\" because we instead just aligned a particular non-ASI system in a non-scalable way, in the same way I don't consider \"claude 4.0 opus is aligned enough to be pretty helpful and not plot takeover\" to be a solution to the alignment problem.\n\nPerhaps your view is \"obviously it's totally sufficient to align systems which are just capable enough to obsolete current human safety labor, so that's what I meant by 'the alignment problem'\". I don't personally think this is obvious given race dynamics and limited time (though I do think it's likely to suffice in practice). Minimally, people often seem to talk about aligning ASI (which I interpret to mean wildly superhuman AIs rather than human-ish level AIs).",
      "plaintextDescription": "> I mean, I also believe that if we solve the alignment problem, then we will no longer have an alignment problem, and I predict the same is true of Nate and Eliezer.\n\nBy \"superintelligence\" I mean \"systems which are qualititatively much smarter than top human experts\". (If Anyone Builds It, Everyone Dies seems to define ASI in a way that could include weaker levels of capability, but I'm trying to refer to what I see as the typical usage of the term.)\n\nSometimes, people say that \"aligning superintelligence is hard because it will be much smarter than us\". I agree, this seems like this makes aligning superintelligence much harder for multiple reasons.\n\nCorrespondingly, I'm noting that if we can align earlier systems which are just capable enough to obsolete human labor (which IMO seems way easier than directly aligning wildly superhuman systems), these systems might be able to ongoingly align their successors. I wouldn't consider this \"solving the alignment problem\" because we instead just aligned a particular non-ASI system in a non-scalable way, in the same way I don't consider \"claude 4.0 opus is aligned enough to be pretty helpful and not plot takeover\" to be a solution to the alignment problem.\n\nPerhaps your view is \"obviously it's totally sufficient to align systems which are just capable enough to obsolete current human safety labor, so that's what I meant by 'the alignment problem'\". I don't personally think this is obvious given race dynamics and limited time (though I do think it's likely to suffice in practice). Minimally, people often seem to talk about aligning ASI (which I interpret to mean wildly superhuman AIs rather than human-ish level AIs)."
    },
    "baseScore": 18,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-09-22T01:22:52.969Z",
    "af": false
  },
  {
    "_id": "gbw3dik8x8WwGonmL",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "c4XZeqtyjyLFjhCa4",
    "topLevelCommentId": "fZ5fz6BSnquBGxQJE",
    "contents": {
      "markdown": "(I mean, I think it's a substantial reason to think that \"literally everyone dies\" is considerably less likely and makes me not want to say stuff like \"everyone dies\", but I just don't think it implies much optimism exactly because the chance of death still seems pretty high and the value of the future is still lost. Like I don't consider \"misaligned AIs have full control and 80% of humans survive after a violent takeover\" to be a good outcome.)",
      "plaintextDescription": "(I mean, I think it's a substantial reason to think that \"literally everyone dies\" is considerably less likely and makes me not want to say stuff like \"everyone dies\", but I just don't think it implies much optimism exactly because the chance of death still seems pretty high and the value of the future is still lost. Like I don't consider \"misaligned AIs have full control and 80% of humans survive after a violent takeover\" to be a good outcome.)"
    },
    "baseScore": 17,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-09-22T01:11:30.544Z",
    "af": false
  },
  {
    "_id": "mKumcvvr45bqpDdPL",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "iJ5HvK2oHrequa3ZA",
    "topLevelCommentId": "QqCkofRQ7kRXQqJBf",
    "contents": {
      "markdown": "Why do I think the story involves a lot of discontinuity (relative to what I expect)?\n\n- Right at the start of the story, Sable has much higher levels of capability than Galvanic expects. It can confortably prove the Riemann Hypothesis even though Galvanic engineers are impressed by it proving some modest theorems. Generally, it seems like for a company to be impressed by a new AI's capabilities while it's actual capabilities are much higher probably requires a bunch of discontinuity (or requires AIs to ongoingly sandbag more and more each generation).\n- There isn't really any discussion of how the world has been changed by AI (beyond Galvanic developing (insufficient) countermeasures based on studying early systems) while Sable is seemingly competitive with top human experts or perhaps superhuman. For instance, it can prove the Riemann hypothesis with only maybe like ~$3 million in spending (assuming each GPU hour is like $2-4). It could be relatively much better at math (which seems totally plausible but not really how the story discusses it), but naively this implies the AI would be very useful for all kinds of stuff. If humans had somewhat weaker systems which were aligned enough to do useful safety work, these systems would potentially change the situation substantially. They might not (e.g. because these systems aren't really leveraged), but either way, the world could look totally different if we already had systems around this level of capability for a bit.\n- I'd expect the company would first scale up a version of parallel scaling which only works a bit (because it's still worth scaling up at this point) before scaling up a version which works very well. The story makes it seem like this is a single large breakthrough.\n- The company doesn't appear to run any serious evals on the AI before deploying it (making it easier to sandbag?) nor due they make any other real effort to measure the capabilities as far as we can tell. (E.g., measure the scaling law for parallel scaling, do science on it at smaller scale, etc.)\n- No other AI systems other than Sable do anything meaningful in the story at any point while I'd naively expect there would be other AI systems which are at a similar level of capability, especially given how long capabilities are halted in the story.\n\nI'm not trying to say this is totally impossible or something. I think this level of discontinuity is substantially more than I expect and substantially more than in AI 2027. (Idk, maybe like top 15% discontinuity while AI 2027 is like top 35% discontinuity, note that speed != discontinuity, though they are related.)\n\nI expect that many of these choices (e.g. not having many other AIs around) made the story less complex and easier to write and it seems kinda reasonable to pick a story which is easier to write.\n\nAlso, I'm not claiming that the situation would have been fine/safe with less discontinuity, in many cases this might just complicate the situation without particularly helping (though I do think less discontinuity would substantially reduce the risk overall). My overall point is just that the story does actually seem less realistic on this axis to me and this is related to why it seems more sci-fi (again, \"more sci-fi\" doesn't mean wrong).\n\n--- \n\nI roughly agree with your 3 bullets. The main thing is that I expect that you first find a kinda shitty version of parallel scaling before you find one so good it results in a big gain in capabilities. And you probably have to do stuff like tune hyperparameters and do other science before you want to scale it up. All this means that the advance would probably be somewhat more continuous. This doesn't mean it would be slow or safe, but it does change how things go and means that large unknown jumps in capability look less likely.\n\nOverall, I agree companies might find a new innovation and scale it up a bunch (and do this scaling quickly). I just think the default most likely picture looks somewhat different in a way which does actually make it somewhat less scary.",
      "plaintextDescription": "Why do I think the story involves a lot of discontinuity (relative to what I expect)?\n\n * Right at the start of the story, Sable has much higher levels of capability than Galvanic expects. It can confortably prove the Riemann Hypothesis even though Galvanic engineers are impressed by it proving some modest theorems. Generally, it seems like for a company to be impressed by a new AI's capabilities while it's actual capabilities are much higher probably requires a bunch of discontinuity (or requires AIs to ongoingly sandbag more and more each generation).\n * There isn't really any discussion of how the world has been changed by AI (beyond Galvanic developing (insufficient) countermeasures based on studying early systems) while Sable is seemingly competitive with top human experts or perhaps superhuman. For instance, it can prove the Riemann hypothesis with only maybe like ~$3 million in spending (assuming each GPU hour is like $2-4). It could be relatively much better at math (which seems totally plausible but not really how the story discusses it), but naively this implies the AI would be very useful for all kinds of stuff. If humans had somewhat weaker systems which were aligned enough to do useful safety work, these systems would potentially change the situation substantially. They might not (e.g. because these systems aren't really leveraged), but either way, the world could look totally different if we already had systems around this level of capability for a bit.\n * I'd expect the company would first scale up a version of parallel scaling which only works a bit (because it's still worth scaling up at this point) before scaling up a version which works very well. The story makes it seem like this is a single large breakthrough.\n * The company doesn't appear to run any serious evals on the AI before deploying it (making it easier to sandbag?) nor due they make any other real effort to measure the capabilities as far as we can tell. (E.g., measure the scaling law f"
    },
    "baseScore": 17,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-21T22:19:52.402Z",
    "af": false
  },
  {
    "_id": "mYTvwvbjNFS5ReHuz",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "eqwxjdDyXrJWNuqnA",
    "topLevelCommentId": "fZ5fz6BSnquBGxQJE",
    "contents": {
      "markdown": "> (b) it sounds like I'm willing to classify more things as \"death\" than you are.\n\nI don't think this matters much. I'm happy to consider non-consensual uploading to be death and I'm certainly happy to consider \"the humans are modified in some way they would find horrifying (at least on reflection)\" to be death. I think \"the humans are alive in the normal sense of alive\" is totally plausible and I expect some humans to be alive in the normal sense of alive in the majority of worlds where AIs takeover.\n\nMaking uploads is barely cheaper than literally keeping physical humans alive after AIs have fully solidified their power I think, maybe 0-3 OOMs more expensive or something, so I don't think non-consensual uploads are that much of the action. (I do think rounding humans up into shelters is relevant.)",
      "plaintextDescription": "> (b) it sounds like I'm willing to classify more things as \"death\" than you are.\n\nI don't think this matters much. I'm happy to consider non-consensual uploading to be death and I'm certainly happy to consider \"the humans are modified in some way they would find horrifying (at least on reflection)\" to be death. I think \"the humans are alive in the normal sense of alive\" is totally plausible and I expect some humans to be alive in the normal sense of alive in the majority of worlds where AIs takeover.\n\nMaking uploads is barely cheaper than literally keeping physical humans alive after AIs have fully solidified their power I think, maybe 0-3 OOMs more expensive or something, so I don't think non-consensual uploads are that much of the action. (I do think rounding humans up into shelters is relevant.)"
    },
    "baseScore": 12,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-21T18:05:44.705Z",
    "af": false
  },
  {
    "_id": "hcPkhjDyoEzaeqnu6",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "FeyHd6WxHPkXzw7gN",
    "topLevelCommentId": "fZ5fz6BSnquBGxQJE",
    "contents": {
      "markdown": "FWIW, I don't really consider my self to be responding to the book at all (in a way that is public or salient to your relevant audience) and my reasons for not signal boosting the book aren't really downstream of the content in the book in the way you describe. (More like, I feel sign uncertain about making You/Eliezer more prominant as representatives of the \"avoid AI takeover movement\" for a wide variety of reasons and think this effect dominates. And I'm not sure I want to be in the business of signal boosting books, though this is less relevant.)",
      "plaintextDescription": "FWIW, I don't really consider my self to be responding to the book at all (in a way that is public or salient to your relevant audience) and my reasons for not signal boosting the book aren't really downstream of the content in the book in the way you describe. (More like, I feel sign uncertain about making You/Eliezer more prominant as representatives of the \"avoid AI takeover movement\" for a wide variety of reasons and think this effect dominates. And I'm not sure I want to be in the business of signal boosting books, though this is less relevant.)"
    },
    "baseScore": 5,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-21T17:55:18.831Z",
    "af": false
  },
  {
    "_id": "ZrfFwZN57vGiRYfZ7",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "Q2fuLCxr6KA35oJHZ",
    "topLevelCommentId": "fZ5fz6BSnquBGxQJE",
    "contents": {
      "markdown": "I would say my summary for hope is more like:\n\n- It seems pretty likely to be doable (with lots of human-directed weak AI labor and/or controlled stronger AI labor) to use iterative and prosaic methods within roughly the current paradigm to sufficiently align AIs which are slightly superhuman. In particular, AIs which are capable enough to be better than humans at safety work (while being much faster and having other AI advantages), but not much more capable than this. This also requires doing a good job elicting capabilites and making the epistemics of these AIs reasonably good.\n  - Doable doesn't mean easy or going to happen by default.\n- If we succeeded in aligning these AIs and handing off to them, they would be in a decent position for other ongoing solving alignment (e.g. aligning a somewhat smarter successor which itself aligns its successor and so on or scalably solving alignment) and also in a decent position to buy more time for solving alignment.\n\nI don't think this is all of my hope, but if I felt much less optimistic about these pieces, that would substantially change my perspective.",
      "plaintextDescription": "I would say my summary for hope is more like:\n\n * It seems pretty likely to be doable (with lots of human-directed weak AI labor and/or controlled stronger AI labor) to use iterative and prosaic methods within roughly the current paradigm to sufficiently align AIs which are slightly superhuman. In particular, AIs which are capable enough to be better than humans at safety work (while being much faster and having other AI advantages), but not much more capable than this. This also requires doing a good job elicting capabilites and making the epistemics of these AIs reasonably good.\n   * Doable doesn't mean easy or going to happen by default.\n * If we succeeded in aligning these AIs and handing off to them, they would be in a decent position for other ongoing solving alignment (e.g. aligning a somewhat smarter successor which itself aligns its successor and so on or scalably solving alignment) and also in a decent position to buy more time for solving alignment.\n\nI don't think this is all of my hope, but if I felt much less optimistic about these pieces, that would substantially change my perspective."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-21T17:50:08.693Z",
    "af": false
  },
  {
    "_id": "SeePRjfZp2c2piZoj",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "hfMfey9waritrCm5L",
    "topLevelCommentId": "fZ5fz6BSnquBGxQJE",
    "contents": {
      "markdown": "To clarify my views on \"will misaligned AIs that succeed in seizing all power have a reasonable chance of keeping (most/many) humans alive\":\n\nI think this isn't very decision relevant and is not that important. I think AI takeover kills the majority of humans *in expectation* due to both the takeover itself and killing humans after (as as side effect of industrial expansion, eating the biosphere, etc.) and there is a substantial chance of literal every-single-human-is-dead extinction conditional on AI takeover (30%?). Regardless it destroys most of the potential value of the long run future and I care mostly about this.\n\nSo at least for me it isn't true that \"this is really the key hope held by the world's reassuring voices\". When I discuss how I think about AI risk, this mostly doesn't come up and when it does I might say something like \"AI takeover would probably kill most people and seems extremely bad overall\". Have you ever seen someone prominent pushing a case for \"optimism\" on the basis of causal trade with aliens / acaual trade?\n\nThe reason why I brought up this topic is because I think it's bad to make incorrect or weak arguments:\n\n- I think smart people will (correctly) notice these arguments seem motivated or weak and then on the basis of this epistemic spot check dismiss the rest. In argumentation, avoiding overclaiming has a lot of rhetorical benefits. I was using \"but will the AI actually kill everyone\" as an example of this. I think the other main case is \"before superintelligence, will we be able to get a bunch of help with alignment work?\" but there are other examples.\n- Worse, bad arguments/content result in negative polarization of somewhat higher context people who might otherwise have been somewhat sympathetic or at least indifferent. This is especially costly from the perspective of getting AI company employees to care. I get that you don't care (much) about AI company employees because you think that radical change is required for their to be any hope, but I think marginal increases in caring among AI company employees substantially reduce risk (though aren't close to sufficient for the situation being at all reasonable/safe).\n- Confidently asserted bad arguments and things people strongly disagree make it harder for people to join a coalition. Like, from an integrity perspective, I would need to caveat saying I agree with the book even though I do agree with large chunks of the book and the extent to which I feel the need to caveat this could be reduced. IDK how much you should care about this, but insofar as you care about people like me joining some push you're trying to make happen this sort of thing makes some difference.\n\nI do think this line of argumentation makes the title literally wrong even if I thought the probability of AI takeover was much higher. I'm not sure how much to care about this, but I do think it randomly imposes a bunch of costs to brand things as \"everyone dies\" when a substantial fraction of the coalition you might want to work with disagrees and it isn't a crux. Like, does the message punchyness outweight the costs here from your perspective? IDK.\n\n---\n\nResponding to some specific points:\n\n> a lot of my objection to superalignment type stuff is a combination of: \n\nI agree that automating alignment with AIs is pretty likely to go very poorly due to incompetence. I think this could go either way and further effort on trying to make this go better is a pretty cost-effective (in terms of using our labor etc) to marginally reduce doom, though it isn't going to result in a reasonable/safe situation.\n\n> that the real world doesn't look like any past predictions people made when they argued it'll all be okay because the future will handle things with dignity\n\nTo be clear, I don't think things will be OK exactly nor do I expect that much dignity, though I think I do expect more dignity than you do. My perspective is more like \"there seem like there are some pretty effective ways to reduce doom at the margin\" than \"we'll be fine because XYZ\".\n\n> my response to the trade arguments as I understand them is here plus in the footnotes here\n\nI don't think this seriously engages with the argument, though due to this footnote, I retract \"they don't talk at all about trade arguments for keeping humans alive\" (I edited my comment).\n\nAs far as [this section](https://ifanyonebuildsit.com/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive), I agree that it's totally fine to say \"everybody dies\" if it's overwhelmingly more likely everyone dies. I don't see how this responds to the argument that \"it's not overwhelming likely everyone dies because of acausal (and causal) trade\". I don't know how important this is, but I also don't know why you/Eliezer/MIRI feel like it's so important to argue against this as opposed to saying something like: \"AI takeover seems extremely bad and like it would at least kill billions of us. People disagree on exactly how likely vast numbers of humans dying as a result of AI takeover is, but we think it's at least substantial due to XYZ\". Is it just because you want to use the \"everybody dies\" part of the title? Fair enough I guess...\n\n> If humans met aliens that wanted to be left alone, it seems to me that we sure would peer in and see if they were doing any slavery, or any chewing agonizing tunnels through living animals, or etc. \n\nSure, but would the outcome for the aliens be as bad or worse than killing all of them from their perspective? I'm skeptical.",
      "plaintextDescription": "To clarify my views on \"will misaligned AIs that succeed in seizing all power have a reasonable chance of keeping (most/many) humans alive\":\n\nI think this isn't very decision relevant and is not that important. I think AI takeover kills the majority of humans in expectation due to both the takeover itself and killing humans after (as as side effect of industrial expansion, eating the biosphere, etc.) and there is a substantial chance of literal every-single-human-is-dead extinction conditional on AI takeover (30%?). Regardless it destroys most of the potential value of the long run future and I care mostly about this.\n\nSo at least for me it isn't true that \"this is really the key hope held by the world's reassuring voices\". When I discuss how I think about AI risk, this mostly doesn't come up and when it does I might say something like \"AI takeover would probably kill most people and seems extremely bad overall\". Have you ever seen someone prominent pushing a case for \"optimism\" on the basis of causal trade with aliens / acaual trade?\n\nThe reason why I brought up this topic is because I think it's bad to make incorrect or weak arguments:\n\n * I think smart people will (correctly) notice these arguments seem motivated or weak and then on the basis of this epistemic spot check dismiss the rest. In argumentation, avoiding overclaiming has a lot of rhetorical benefits. I was using \"but will the AI actually kill everyone\" as an example of this. I think the other main case is \"before superintelligence, will we be able to get a bunch of help with alignment work?\" but there are other examples.\n * Worse, bad arguments/content result in negative polarization of somewhat higher context people who might otherwise have been somewhat sympathetic or at least indifferent. This is especially costly from the perspective of getting AI company employees to care. I get that you don't care (much) about AI company employees because you think that radical change is required for their to be "
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-09-21T17:38:56.230Z",
    "af": false
  },
  {
    "_id": "YWfDQo35Ww5yKcBdj",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "Syc7j6nwbA6pxXZGw",
    "topLevelCommentId": "Qx2sDtbgdC3qcmNiN",
    "contents": {
      "markdown": "(Maybe this is obvious, but I thought I would say this just to be clear.)\n\n> I think \"anything like current techniques\" and \"anything like current understanding\" clearly set a very high bar for the difference. \"We made more progress on interpretability/etc at the current rates of progress\" fairly clearly doesn't count by the book's standards. \n\nSure, but I expect wildly more cognitive labor and effort if humans retain control and can effectively leverage earlier systems, not just \"more progress than we'd expect\". I agree the bar is above \"the progress we'd expect by default (given a roughly similar field size) in the next 10 years\", but I think things might get much more extreme due to handing off alignment work to AIs. I agree the book is intended to apply pretty broadly, but regardless of intention does it really apply to \"1 million AIs somewhat smarter than humans have spent 100 years each working on the problem (and coordinating etc?)\"? (I think the crux is more like \"can you actually safely get this alignment work out of these AIs\".)",
      "plaintextDescription": "(Maybe this is obvious, but I thought I would say this just to be clear.)\n\n> I think \"anything like current techniques\" and \"anything like current understanding\" clearly set a very high bar for the difference. \"We made more progress on interpretability/etc at the current rates of progress\" fairly clearly doesn't count by the book's standards.\n\nSure, but I expect wildly more cognitive labor and effort if humans retain control and can effectively leverage earlier systems, not just \"more progress than we'd expect\". I agree the bar is above \"the progress we'd expect by default (given a roughly similar field size) in the next 10 years\", but I think things might get much more extreme due to handing off alignment work to AIs. I agree the book is intended to apply pretty broadly, but regardless of intention does it really apply to \"1 million AIs somewhat smarter than humans have spent 100 years each working on the problem (and coordinating etc?)\"? (I think the crux is more like \"can you actually safely get this alignment work out of these AIs\".)"
    },
    "baseScore": 5,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-21T16:38:23.507Z",
    "af": false
  },
  {
    "_id": "WZYpnb7QekGiZgHwq",
    "postId": "kovCotfpTFWFXaxwi",
    "parentCommentId": "fAdhmkrHRyNKQ9NAy",
    "topLevelCommentId": "fAdhmkrHRyNKQ9NAy",
    "contents": {
      "markdown": "It's currently 25th under [new releases on amazon](https://www.amazon.com/gp/new-releases/books). So I think somewhere between borderline and unlikely for making it into the [top 15 NYT best seller list for non-fiction](https://www.nytimes.com/books/best-sellers/hardcover-nonfiction/)?",
      "plaintextDescription": "It's currently 25th under new releases on amazon. So I think somewhere between borderline and unlikely for making it into the top 15 NYT best seller list for non-fiction?"
    },
    "baseScore": 0,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-20T17:42:17.564Z",
    "af": false
  },
  {
    "_id": "QqCkofRQ7kRXQqJBf",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> I... really don't know what Scott expected a story that featured actual superintelligence to look like. I think the authors bent over backwards giving us one of the least-sci-fi stories you could possibly tell that includes superintelligence doing anything at all, without resorting to \"superintelligence just won't ever exist.\" \n\nWhat about literally the AI 2027 story which does involve superintelligence and Scott thinks doesn't sound \"unnecessarily dramatic\". I think AI 2027 seems much more intuitively plausible to me and it seems less \"sci-fi\" in this sense. (I'm not saying that \"less sci-fi\" is much evidence it's more likely to be true.)\n\n> The amount of \"fast takeoff\" seems like the amount of scaleup you'd expect if the graphs just kept going up the way they're currently going up, by approximately the same mechanisms they currently go up (i.e. some algorithmic improvements, some scaling). \n> \n> Sure, Galvanic would first run Sable on smaller amounts of compute. And... then they will run it on larger amounts of compute (and as I understand it, it'd be a new, surprising fact if they limited themselves to scaling up slowly/linearly rather than by a noticeable multiplier or order-of-magnitude. If I am wrong about current lab practices here, please link me some evidence).\n\nI think the amount of discontinuity in the story is substantially above the amount of discontinuity in more realistic-seeming-to-me stories like AI 2027 (which is also on the faster side of what I expect, like a top 20% takeoff in terms of speed). I don't think think extrapolating current trends predicts this much of a discontinuity.\n\n> If this story feels crazy to you, I want to remind you that [all possible views of the future are wild](https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/). Either some exponential graphs suddenly stop for unclear reasons, or some exponential graphs keep going and batshit crazy stuff can happen that your intuitions are not prepared for. You can believe option A if you want, but, it's not like \"the exponential graphs that have been consistent over hundreds of years suddenly stop\" is a viewpoint that you can safely point to as a \"moderate\" and claim to give the other guy burden of proof.\n> \n> You don't have the luxury of being the sort of moderate who doesn't have to believe something pretty crazy sounding here, one way or another. \n\nThe relevant moderates (that are likely to be reading this etc) are predicting an AI takeoff, so I don't think think \"exponential economic growth can't go on\" is a relevant objection.",
      "plaintextDescription": "> I... really don't know what Scott expected a story that featured actual superintelligence to look like. I think the authors bent over backwards giving us one of the least-sci-fi stories you could possibly tell that includes superintelligence doing anything at all, without resorting to \"superintelligence just won't ever exist.\" \n> \n>  \n\nWhat about literally the AI 2027 story which does involve superintelligence and Scott thinks doesn't sound \"unnecessarily dramatic\". I think AI 2027 seems much more intuitively plausible to me and it seems less \"sci-fi\" in this sense. (I'm not saying that \"less sci-fi\" is much evidence it's more likely to be true.)\n\n> The amount of \"fast takeoff\" seems like the amount of scaleup you'd expect if the graphs just kept going up the way they're currently going up, by approximately the same mechanisms they currently go up (i.e. some algorithmic improvements, some scaling). \n> \n> Sure, Galvanic would first run Sable on smaller amounts of compute. And... then they will run it on larger amounts of compute (and as I understand it, it'd be a new, surprising fact if they limited themselves to scaling up slowly/linearly rather than by a noticeable multiplier or order-of-magnitude. If I am wrong about current lab practices here, please link me some evidence).\n\nI think the amount of discontinuity in the story is substantially above the amount of discontinuity in more realistic-seeming-to-me stories like AI 2027 (which is also on the faster side of what I expect, like a top 20% takeoff in terms of speed). I don't think think extrapolating current trends predicts this much of a discontinuity.\n\n> If this story feels crazy to you, I want to remind you that all possible views of the future are wild. Either some exponential graphs suddenly stop for unclear reasons, or some exponential graphs keep going and batshit crazy stuff can happen that your intuitions are not prepared for. You can believe option A if you want, but, it's not like \"the exponential g"
    },
    "baseScore": 8,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-20T17:35:36.658Z",
    "af": false
  },
  {
    "_id": "FKLfPSLWrmDZddDdg",
    "postId": "JWH63Aed3TA2cTFMt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> I find it very strange that Collier claims that international compute monitoring would “tank the global economy.” What is the mechanism for this, exactly?\n\n>10% of the value of the S&P 500 is downstream of AI and the proposal is to ban further AI development. AI investment is a non-trivial fraction of current US GDP growth (20%?). I'd guess the proposal would cause a large market crash and a (small?) recession in the US; it's unclear if this is well described as \"tanking the global economy\".",
      "plaintextDescription": "> I find it very strange that Collier claims that international compute monitoring would “tank the global economy.” What is the mechanism for this, exactly?\n\n>10% of the value of the S&P 500 is downstream of AI and the proposal is to ban further AI development. AI investment is a non-trivial fraction of current US GDP growth (20%?). I'd guess the proposal would cause a large market crash and a (small?) recession in the US; it's unclear if this is well described as \"tanking the global economy\"."
    },
    "baseScore": 14,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2025-09-20T17:15:57.999Z",
    "af": false
  },
  {
    "_id": "fZ5fz6BSnquBGxQJE",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Given the [counteraguments](https://ifanyonebuildsit.com/5/wont-ais-care-at-least-a-little-about-humans), I don't see a reason to think this more than single-digit-percent likely to be especially relevant. (I can see >9% likelihood the AIs are \"nice enough that something interesting-ish happens\" but not >9% likelihood that we shouldn't think the outcome is still extremely bad. The people who think otherwise seem extremely motivatedly-cope-y to me).\n\nI think the arguments given in the online supplement for \"AIs will literally kill every single human\" fail to engage with the best counterarguments in a serious way. I get the sense that many people's complaints are of this form: the book does a bad job engaging with the strongest counterarguments in a way that is epistemically somewhat bad. (Idk if it violates group epistemic norms, but it seems like it is probably counterproductive. I guess this is most similar to complaint #2 in your breakdown.)\n\nSpecifically:\n\n*   They fail to engage with the details of \"how cheap is it actually for the AI to keep humans alive\" in [this section](https://ifanyonebuildsit.com/5/to-a-powerful-ai-wouldnt-preserving-humans-be-a-negligible-expense). Putting aside killing humans as part of a takeover effort, avoiding boiling the oceans (or eating the biosphere etc) maybe delays you for something like a week to a year. Each year costs you ~1/3 billionth of resources, so this is actually very cheap if you care in a scope-sensitive and patient way. Additionally, keeping humans alive through boiling the oceans might be extremely cheap, especially given very fast takeoff; this might lower costs to maybe more like 1/trillion. Regardless, this is much cheaper and much more salient than \"keeping a pile of 41 stones in your house\". (I'd guess you'd have to pay most American households more than a tenth of a penny to keep a pile of stones in their house.)\n*   They don't talk at all about trade arguments for keeping humans alive while these are a substantial fraction of the case, (edit:) aside from [this footnote](https://ifanyonebuildsit.com/5/wont-ai-find-us-fascinating-or-historically-important#ftnt179_ref) which doesn't really engage in a serious way[^eozuhwj2t6] (and is a footnote). ([This](https://ifanyonebuildsit.com/5/what-if-we-make-it-think-its-in-a-simulation) doesn't count.)\n*   [The argument](https://ifanyonebuildsit.com/5/wont-ais-care-at-least-a-little-about-humans) that \"humans wouldn't actually preserve the environment\" misses the relevant analogy which is more like \"humans come across some intelligent aliens who say they want to be left alone and leaving these aliens alone is pretty cheap from our perspective, but these aliens aren't totally optimized from our perspective, e.g. they have more suffering than we'd like in their lives\". This situation wouldn't result in us doing something to the aliens that they consider as bad as killing them all, so the type of kindness humans have is actually sufficient.\n\n> Note also that it's *very* expensive for the AI to not boil the oceans / etc as fast as possible, since that means losing a many galaxies worth of resources, so it seems like it's not enough to be \"very slightly\" nice – it has to be, like, pretty actively nice.\n\nFor a patient AI, it costs something like 1 / billion to 1/300 billion of resources which seems extremely cheap. E.g., for a person with a net worth of $1 million, it require them to spare a tenth of a penny to a thousandth of a penny. I think this counts as \"very slightly nice\"? It seems pretty misleading to describe this as \"very expensive\", though I agree the total amount of resources is large in a absolute sense.\n\n> There are some flavors of \"AI might be slightly nice\" that are interesting. But, they don't seem like it changes any of our decisions. It just makes us a bit more hopeful about the end result.\n\nFor the record, I agree with this.\n\n[^eozuhwj2t6]: For instance, why not think this results in a reasonable chance of the humans surviving in their normal physical bodies and being able to live the lives they want to live rather than being in an \"alien zoo\".",
      "plaintextDescription": "> Given the counteraguments, I don't see a reason to think this more than single-digit-percent likely to be especially relevant. (I can see >9% likelihood the AIs are \"nice enough that something interesting-ish happens\" but not >9% likelihood that we shouldn't think the outcome is still extremely bad. The people who think otherwise seem extremely motivatedly-cope-y to me).\n\nI think the arguments given in the online supplement for \"AIs will literally kill every single human\" fail to engage with the best counterarguments in a serious way. I get the sense that many people's complaints are of this form: the book does a bad job engaging with the strongest counterarguments in a way that is epistemically somewhat bad. (Idk if it violates group epistemic norms, but it seems like it is probably counterproductive. I guess this is most similar to complaint #2 in your breakdown.)\n\nSpecifically:\n\n * They fail to engage with the details of \"how cheap is it actually for the AI to keep humans alive\" in this section. Putting aside killing humans as part of a takeover effort, avoiding boiling the oceans (or eating the biosphere etc) maybe delays you for something like a week to a year. Each year costs you ~1/3 billionth of resources, so this is actually very cheap if you care in a scope-sensitive and patient way. Additionally, keeping humans alive through boiling the oceans might be extremely cheap, especially given very fast takeoff; this might lower costs to maybe more like 1/trillion. Regardless, this is much cheaper and much more salient than \"keeping a pile of 41 stones in your house\". (I'd guess you'd have to pay most American households more than a tenth of a penny to keep a pile of stones in their house.)\n * They don't talk at all about trade arguments for keeping humans alive while these are a substantial fraction of the case, (edit:) aside from this footnote which doesn't really engage in a serious way[1] (and is a footnote). (This doesn't count.)\n * The argument that \"huma"
    },
    "baseScore": 18,
    "voteCount": 14,
    "createdAt": null,
    "postedAt": "2025-09-20T17:06:14.337Z",
    "af": false
  },
  {
    "_id": "Qx2sDtbgdC3qcmNiN",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> The book is making a (relatively) narrow claim. \n> \n> You might *still* disagree with that claim. I think there are valid reasons to disagree, or at least assign significantly less confidence to the claim. \n> \n> But none of the reasons listed so far are disagreements with the thesis. And, remember, if the reason you disagree is because you think our understanding of AI will improve dramatically, or there will be a paradigm shift specifically away from \"unpredictably grown\" AI, this also isn't actually a disagreement with the sentence.\n\nThe authors clearly intend to make a pretty broad claim, not the more narrow claim you imply.\n\nThis feels like a motte and bailey where the motte is \"If you literally used something remotely like current scaled up methods without improved understanding to directly build superintelligence, everyone would die\" and the bailey is \"on the current trajectory, everyone will die if superintelligence is built without a miracle or a long (e.g. >15 year) pause\".\n\nI expect that *by default* superintelligence is built after a point where we have access to huge amounts of non-superintelligent cognitive labor so it's unlikely that we'll be using current methods and current understanding (unless humans have already lost control by this point, which seems totally plausible, but not overwhelmingly likely nor argued convincingly for by the book). Even just looking at capabilities, I think it's pretty likely that automated AI R&D will result in us operating in a totally different paradigm by the time we build superintelligence---this isn't to say this other paradigm will be safer, just that a narrow description of \"current techniques\" doesn't include the default trajectory.\n\nI think it's pretty clear the authors intend to include \"we ~hand off AI R&D and alignment to AIs developed in roughly the current paradigm which proceed with development\" as a special case of \"anything remotely like current techniques\" (as from my perspective it is the default trajectory). But, if these earlier AIs were well aligned (and wise and had reasonable epistemics), I think it's pretty unclear that the situation would go poorly and I'd guess it would go fine because these AIs would themselves develop much better alignment techniques. This is my main disagreement with the book.",
      "plaintextDescription": "> The book is making a (relatively) narrow claim. \n> \n> You might still disagree with that claim. I think there are valid reasons to disagree, or at least assign significantly less confidence to the claim. \n> \n> But none of the reasons listed so far are disagreements with the thesis. And, remember, if the reason you disagree is because you think our understanding of AI will improve dramatically, or there will be a paradigm shift specifically away from \"unpredictably grown\" AI, this also isn't actually a disagreement with the sentence.\n\nThe authors clearly intend to make a pretty broad claim, not the more narrow claim you imply.\n\nThis feels like a motte and bailey where the motte is \"If you literally used something remotely like current scaled up methods without improved understanding to directly build superintelligence, everyone would die\" and the bailey is \"on the current trajectory, everyone will die if superintelligence is built without a miracle or a long (e.g. >15 year) pause\".\n\nI expect that by default superintelligence is built after a point where we have access to huge amounts of non-superintelligent cognitive labor so it's unlikely that we'll be using current methods and current understanding (unless humans have already lost control by this point, which seems totally plausible, but not overwhelmingly likely nor argued convincingly for by the book). Even just looking at capabilities, I think it's pretty likely that automated AI R&D will result in us operating in a totally different paradigm by the time we build superintelligence---this isn't to say this other paradigm will be safer, just that a narrow description of \"current techniques\" doesn't include the default trajectory.\n\nI think it's pretty clear the authors intend to include \"we ~hand off AI R&D and alignment to AIs developed in roughly the current paradigm which proceed with development\" as a special case of \"anything remotely like current techniques\" (as from my perspective it is the default traject"
    },
    "baseScore": 40,
    "voteCount": 31,
    "createdAt": null,
    "postedAt": "2025-09-20T16:42:39.557Z",
    "af": false
  },
  {
    "_id": "LHD3mum4imKvxbSvB",
    "postId": "voEAJ9nFBAqau8pNN",
    "parentCommentId": "forxGaAcjf9s4TH45",
    "topLevelCommentId": "JuitdRH3a7mQWEDcA",
    "contents": {
      "markdown": "I think this comment is failing to engage with Rohin's perspective.\n\nRohin's claim presumably isn't that people shouldn't say anything that happens to be sensationalist, but instead that LW group epistemics have a huge issue with sensationalism bias.\n\n> There are like 5 x-risk-scene-people I can think offhand who seem like they might plausibly have dealt real damage via sensationalism, and a couple hundred people who I think dealt damage via not wanting to sound weird.\n\n\"plausibly have dealt real damage\" under your views or Rohin's views? Like I would have guessed that Rohin's view is that this book and associated discussion has itself done a bunch of damage via sensationalism (maybe he thinks the upsides are bigger, but this isn't a crux for ths claim). And, insofar as you cared about LW epistemics (which presumably you do), from Rohin's perspective this sort of thing is wrecking LW epistemics. I don't think the relative number of people matters that much relative to the costs of these biases, but regardless I'd guess Rohin disagrees about the quantity as well.\n\nMore generally, this feels like a total \"what-about-ism\". Regardless of whether \"things are okay, we can keep doing politics as usual, and none of us has to ever say anything socially scary\" bias is worse, it can still be the case that sensationalism is a massive issue. In my view, sensationalism (and something like purity testing / standard polarization dynamics) is much more of an issue for epistemics on LW (the main thing that Rohin was talking about) than \"no one has to do anything scary\" bias. In terms of the broader world, I don't have a strong view, but I'd guess the worst biases (in terms of their effect on understanding of AI risk) are something else entirely, though it's in the direction of \"we don't need to do anything\".",
      "plaintextDescription": "I think this comment is failing to engage with Rohin's perspective.\n\nRohin's claim presumably isn't that people shouldn't say anything that happens to be sensationalist, but instead that LW group epistemics have a huge issue with sensationalism bias.\n\n> There are like 5 x-risk-scene-people I can think offhand who seem like they might plausibly have dealt real damage via sensationalism, and a couple hundred people who I think dealt damage via not wanting to sound weird.\n\n\"plausibly have dealt real damage\" under your views or Rohin's views? Like I would have guessed that Rohin's view is that this book and associated discussion has itself done a bunch of damage via sensationalism (maybe he thinks the upsides are bigger, but this isn't a crux for ths claim). And, insofar as you cared about LW epistemics (which presumably you do), from Rohin's perspective this sort of thing is wrecking LW epistemics. I don't think the relative number of people matters that much relative to the costs of these biases, but regardless I'd guess Rohin disagrees about the quantity as well.\n\nMore generally, this feels like a total \"what-about-ism\". Regardless of whether \"things are okay, we can keep doing politics as usual, and none of us has to ever say anything socially scary\" bias is worse, it can still be the case that sensationalism is a massive issue. In my view, sensationalism (and something like purity testing / standard polarization dynamics) is much more of an issue for epistemics on LW (the main thing that Rohin was talking about) than \"no one has to do anything scary\" bias. In terms of the broader world, I don't have a strong view, but I'd guess the worst biases (in terms of their effect on understanding of AI risk) are something else entirely, though it's in the direction of \"we don't need to do anything\"."
    },
    "baseScore": 29,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-09-20T16:23:37.933Z",
    "af": false
  },
  {
    "_id": "QJiZyuZ87ovhqgq5X",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "8jG4ipDpgkEspq7W6",
    "topLevelCommentId": "fjo9AydRPTyurNaqv",
    "contents": {
      "markdown": "> Your first paragraph is an example of \"something that looks like coordination to not build ASI for quite a while\"! \"Several additional years\" is definitely \"quite a while\"! \n\nOk, if you count a several additional years as quite a while, then we're probably closer to agreement.\n\n---\n\nFor this scenario, I was imagining all these actions happen within 2 years of lead time. In practice, we should keep trying to buy additional lead time prior to it making sense to handoff to AIs and the AIs we handoff to will probably want to try to buy lead time (especially if there are strategies which are easier post handoff, e.g. due to leveraging labor from more powerful systems).\n\nI'm unsure about the difficulty of buying different amounts of lead time and it seems like it might be harder to buy lead time than to ongoingly ensure the alignment of later AIs. Eventually, we have to do some kind of a handoff and I think it's safer to do this handoff to AIs that aren't substantially more capable than top human experts in general purpose qualitative capabilties (like I think you want to handoff at roughly the minimum level of capability where the AIs are clearly capable enough to dominate humans, including at conceptually tricky work).",
      "plaintextDescription": "> Your first paragraph is an example of \"something that looks like coordination to not build ASI for quite a while\"! \"Several additional years\" is definitely \"quite a while\"!\n\nOk, if you count a several additional years as quite a while, then we're probably closer to agreement.\n\n----------------------------------------\n\nFor this scenario, I was imagining all these actions happen within 2 years of lead time. In practice, we should keep trying to buy additional lead time prior to it making sense to handoff to AIs and the AIs we handoff to will probably want to try to buy lead time (especially if there are strategies which are easier post handoff, e.g. due to leveraging labor from more powerful systems).\n\nI'm unsure about the difficulty of buying different amounts of lead time and it seems like it might be harder to buy lead time than to ongoingly ensure the alignment of later AIs. Eventually, we have to do some kind of a handoff and I think it's safer to do this handoff to AIs that aren't substantially more capable than top human experts in general purpose qualitative capabilties (like I think you want to handoff at roughly the minimum level of capability where the AIs are clearly capable enough to dominate humans, including at conceptually tricky work)."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-20T00:02:03.141Z",
    "af": false
  },
  {
    "_id": "H9rRrS4zJAzg7xJPG",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "g5FTYAzbEjnuNy3Rm",
    "topLevelCommentId": "fjo9AydRPTyurNaqv",
    "contents": {
      "markdown": "> I think at the very least you need to intervene to stop that feedback loop.\n\nThere's probably at least some disagreement here. I think even if you let takeoff proceed at the default rate with a small fraction (e.g. 5%) explicitly spent on reasonably targeted alignment work at each point (as in, 5% beyond what is purely commercially expedient), you have a reasonable chance of avoiding AI takeover (maybe 50% chance of misaligned AI takeover?). Some of this is due to the possibility of takeoff being relatively slower and more compute constrained (which you might think is very unlikely?). I also think that there is a decent chance that you get a higher fraction spent on safety after handing off to AIs or after getting advice from highly capable AIs even if this doesn't happen before this.\n\n> It again seems extremely unlikely that they would succeed at aligning a runaway intelligence explosion.\n\nI don't feel so confident--these AIs might have a decent amount of subjective time and total cognitive labor between each unit of increase in capabilities as the intelligence explosion continues such that they can keep things on track. Intuitively, capabilities might be more compute bottlenecked than alignment, so it should pull ahead if we can start with actually aligned (and wise) AIs (which is not easy to achieve to be clear!).\n\n> A lead time of >1 year does seem pretty unlikely at this point. My guess would be like 25% likely? So already this isn't going to work in 75% of worlds.\n\nI agree with around 25% likely.\n\n> This might allow us to get to something like safe ASI on the scale of single-digit years, but man, this just seems like such an insane risk to take, that I really hope we instead use the AI systems to coordinate a longer pause, which seems like a much easier task. \n\nI agree that coordinating a longer pause looks pretty good, but I'm not so sure about the relative feasibility given only the use of AIs that are somewhat more capable than top human experts (regardless of whether these AIs are running things). I think it might be much harder to buy 10 years of time than 2 years given the constraints at the time (including limited political will) and I'm not so sure aligning somewhat more powerful AIs will be harder (and then these somewhat more powerful AIs can align even more powerful AIs and this either bottoms out in a scalable solution to alignment or in powerful enough capabilities that they actually can buy more time).\n\n---\n\nOne general note: I do think that \"buying time along the way\" (either before handing off to AIs or after) is quite helpful for making the situation go well. However, I can also imagine worlds where things go fine and we didn't buy much/any time (especially if takeoff is naturally on the slower side).",
      "plaintextDescription": "> I think at the very least you need to intervene to stop that feedback loop.\n\nThere's probably at least some disagreement here. I think even if you let takeoff proceed at the default rate with a small fraction (e.g. 5%) explicitly spent on reasonably targeted alignment work at each point (as in, 5% beyond what is purely commercially expedient), you have a reasonable chance of avoiding AI takeover (maybe 50% chance of misaligned AI takeover?). Some of this is due to the possibility of takeoff being relatively slower and more compute constrained (which you might think is very unlikely?). I also think that there is a decent chance that you get a higher fraction spent on safety after handing off to AIs or after getting advice from highly capable AIs even if this doesn't happen before this.\n\n> It again seems extremely unlikely that they would succeed at aligning a runaway intelligence explosion.\n\nI don't feel so confident--these AIs might have a decent amount of subjective time and total cognitive labor between each unit of increase in capabilities as the intelligence explosion continues such that they can keep things on track. Intuitively, capabilities might be more compute bottlenecked than alignment, so it should pull ahead if we can start with actually aligned (and wise) AIs (which is not easy to achieve to be clear!).\n\n> A lead time of >1 year does seem pretty unlikely at this point. My guess would be like 25% likely? So already this isn't going to work in 75% of worlds.\n\nI agree with around 25% likely.\n\n> This might allow us to get to something like safe ASI on the scale of single-digit years, but man, this just seems like such an insane risk to take, that I really hope we instead use the AI systems to coordinate a longer pause, which seems like a much easier task.\n\nI agree that coordinating a longer pause looks pretty good, but I'm not so sure about the relative feasibility given only the use of AIs that are somewhat more capable than top human experts (regardles"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-19T23:48:49.993Z",
    "af": false
  },
  {
    "_id": "4JvznSDb9TLEgTo9L",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "iASXwYJdjZszBgahh",
    "topLevelCommentId": "fjo9AydRPTyurNaqv",
    "contents": {
      "markdown": "Sure, several. E.g.:\n\n- USG cares a decent amount and leading AI companies are on board, so they try to buy several additional years to work on safety.\n- We scale to roughly top human expert level while ensuring control.\n- Over time, we lower the risk of scheming at this level of capability through a bunch of empirical experiments and new interventions developed using a bunch of AI labor.\n- We relax our control measures and increasingly work on making AIs generally very trustworthy, including on hard-to-check open ended tasks. We do a bunch of studies of this.\n- This ends up not being that hard due to somewhat favorable generalization.\n- We handoff to AIs and they align their successors and so on.",
      "plaintextDescription": "Sure, several. E.g.:\n\n * USG cares a decent amount and leading AI companies are on board, so they try to buy several additional years to work on safety.\n * We scale to roughly top human expert level while ensuring control.\n * Over time, we lower the risk of scheming at this level of capability through a bunch of empirical experiments and new interventions developed using a bunch of AI labor.\n * We relax our control measures and increasingly work on making AIs generally very trustworthy, including on hard-to-check open ended tasks. We do a bunch of studies of this.\n * This ends up not being that hard due to somewhat favorable generalization.\n * We handoff to AIs and they align their successors and so on."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-19T18:05:31.394Z",
    "af": false
  },
  {
    "_id": "TDfAFxCcHkZ5LCgc3",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": "48bEwnxJJy5XR7Lsk",
    "topLevelCommentId": "fjo9AydRPTyurNaqv",
    "contents": {
      "markdown": ">  I have never heard of a remotely realistic-seeming story for how things will be OK, without something that looks like coordination to not build ASI for quite a while.\n\nI wonder if we should talk about this at some point. This perspective feels pretty wild to me and I don't immediately understand where the crux lives. \n\n- Do you think it will be extremely hard to avoid scheming in human-ish level AIs?\n- Do you think it will be extremely hard to get not-scheming-against-us human-ish level AIs aligned enough that handing over safety work to them is competitive with emulated humans (at the same speed and cost)?\n- Do you think that AIs trying hard to ongoingly ensure alignment and given some lead time will fail because alignment is much, much harder than capabilites? (E.g., full AGI can't align +1 SD AGI given 1 wall clock year, or +1 SD AGI can't align +2 SD AGI given 6 wall clock months, or so on.)\n- Maybe you're including \">1 years of lead time spent on safety\" under \"coordination to not build ASI for quite a while\" and you think this is extremely unlikely?",
      "plaintextDescription": "> I have never heard of a remotely realistic-seeming story for how things will be OK, without something that looks like coordination to not build ASI for quite a while.\n\nI wonder if we should talk about this at some point. This perspective feels pretty wild to me and I don't immediately understand where the crux lives.\n\n * Do you think it will be extremely hard to avoid scheming in human-ish level AIs?\n * Do you think it will be extremely hard to get not-scheming-against-us human-ish level AIs aligned enough that handing over safety work to them is competitive with emulated humans (at the same speed and cost)?\n * Do you think that AIs trying hard to ongoingly ensure alignment and given some lead time will fail because alignment is much, much harder than capabilites? (E.g., full AGI can't align +1 SD AGI given 1 wall clock year, or +1 SD AGI can't align +2 SD AGI given 6 wall clock months, or so on.)\n * Maybe you're including \">1 years of lead time spent on safety\" under \"coordination to not build ASI for quite a while\" and you think this is extremely unlikely?"
    },
    "baseScore": 22,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-19T17:14:05.756Z",
    "af": false
  },
  {
    "_id": "RzqspzrDmPuaxwbnM",
    "postId": "P4xeb3jnFAYDdEEXs",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> I'm quite sympathetic to something like the caveated version of the title\n\nPresumably, another problem with your caveated version of the title is that you don't expect literally everyone to die (at least not with high confidence) even if AIs take over.",
      "plaintextDescription": "> I'm quite sympathetic to something like the caveated version of the title\n\nPresumably, another problem with your caveated version of the title is that you don't expect literally everyone to die (at least not with high confidence) even if AIs take over."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-17T15:14:18.624Z",
    "af": false
  },
  {
    "_id": "fLKQFGvAbqkdemozr",
    "postId": "iEkksmar6mtuQtdsR",
    "parentCommentId": "zDyhyJtP4rehwjKFJ",
    "topLevelCommentId": "zDyhyJtP4rehwjKFJ",
    "contents": {
      "markdown": "> By contrast, some reasons to be skeptical that AI will be automating more than a few percent of the economy by 2033 (still absent AI R&D feedback loops and/or catastrophe):\n\nI currently expect substantial AI R&D acceleration from AIs which are capable of cheap and fast superhuman performance at arbitrary easy-and-cheap-to-check domains (especially if these AIs are very superhuman). Correspondingly I think \"absent AI R&D feedback loops\" might be doing a lot of the work here. Minimally, I think [full automation of research engineering](https://ai-2027.com/research/timelines-forecast#defining-a-superhuman-coder-sc) would yield a large acceleration (e.g. 3x faster AI progress), though this requires high performance on (some) non-formal domains. I think if AIs were (very) superhuman at easy (and cheap) to check stuff, you'd probably be able to use them to do a lot of coding tasks, some small scale research tasks that might transfer well enough, and there would be a decent chance of enough transfer to extend substantially beyond this.\n\nI think I still agree with the bottom line \"it's plausible that for several years in the late 2020s/early 2030s, we will have AI that is vastly superhuman at formal domains including math, but still underperforms humans at most white-collar jobs\". And I agree more strongly if you change \"underperforms humans at most white-collar jobs\" to \"can't yet fully automate AI R&D\".",
      "plaintextDescription": "> By contrast, some reasons to be skeptical that AI will be automating more than a few percent of the economy by 2033 (still absent AI R&D feedback loops and/or catastrophe):\n\nI currently expect substantial AI R&D acceleration from AIs which are capable of cheap and fast superhuman performance at arbitrary easy-and-cheap-to-check domains (especially if these AIs are very superhuman). Correspondingly I think \"absent AI R&D feedback loops\" might be doing a lot of the work here. Minimally, I think full automation of research engineering would yield a large acceleration (e.g. 3x faster AI progress), though this requires high performance on (some) non-formal domains. I think if AIs were (very) superhuman at easy (and cheap) to check stuff, you'd probably be able to use them to do a lot of coding tasks, some small scale research tasks that might transfer well enough, and there would be a decent chance of enough transfer to extend substantially beyond this.\n\nI think I still agree with the bottom line \"it's plausible that for several years in the late 2020s/early 2030s, we will have AI that is vastly superhuman at formal domains including math, but still underperforms humans at most white-collar jobs\". And I agree more strongly if you change \"underperforms humans at most white-collar jobs\" to \"can't yet fully automate AI R&D\"."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-16T16:21:23.341Z",
    "af": false
  },
  {
    "_id": "i8BMgs4zLv4asur8S",
    "postId": "iEkksmar6mtuQtdsR",
    "parentCommentId": "zDyhyJtP4rehwjKFJ",
    "topLevelCommentId": "zDyhyJtP4rehwjKFJ",
    "contents": {
      "markdown": "> Reliable informal-to-formal translation: solution verifiers need to be robust enough to avoid too much reward hacking, which probably requires natural language problems and solutions to be formalized to some degree (a variety of arrangements seem possible here, but it's hard to see how something purely informal can provide sufficiently scalable supervision, and it's hard to see how something purely formal can capture mathematicians' intuitions about what problems are interesting).\n\nDo you think OpenAI and GDM's recent results on IMO are driven by formalization in training (e.g. the AI formalizes and checks the proof) or some other verification strategy? I'd pretty strongly guess some other verification strategy that involves getting AIs to be able to (more) robustly check informal proofs. (Though maybe this verification ability was partially trained using formalization? I currently suspect not.) This also has the advantage of allowing for (massive amounts of) runtime search that operates purely using informal proofs.\n\nI think an important alternative to informal-to-formal translation is instead just getting very robust AI verification of informal proofs that scales sufficiently with capabilities. This is mostly how humans have gotten increasingly good at proving things in mathematics and I don't see a strong reason to think this is infeasible.\n\nIf superhuman math is most easily achieved by getting robust verification and this robust verification strategy generalizes to other non-formal but relatively easy to check domains (e.g. various software engineering tasks), then we'll also see high levels of capability in these other domains, including domains which are more relevant to AI R&D and the economy.\n\n",
      "plaintextDescription": "> Reliable informal-to-formal translation: solution verifiers need to be robust enough to avoid too much reward hacking, which probably requires natural language problems and solutions to be formalized to some degree (a variety of arrangements seem possible here, but it's hard to see how something purely informal can provide sufficiently scalable supervision, and it's hard to see how something purely formal can capture mathematicians' intuitions about what problems are interesting).\n\nDo you think OpenAI and GDM's recent results on IMO are driven by formalization in training (e.g. the AI formalizes and checks the proof) or some other verification strategy? I'd pretty strongly guess some other verification strategy that involves getting AIs to be able to (more) robustly check informal proofs. (Though maybe this verification ability was partially trained using formalization? I currently suspect not.) This also has the advantage of allowing for (massive amounts of) runtime search that operates purely using informal proofs.\n\nI think an important alternative to informal-to-formal translation is instead just getting very robust AI verification of informal proofs that scales sufficiently with capabilities. This is mostly how humans have gotten increasingly good at proving things in mathematics and I don't see a strong reason to think this is infeasible.\n\nIf superhuman math is most easily achieved by getting robust verification and this robust verification strategy generalizes to other non-formal but relatively easy to check domains (e.g. various software engineering tasks), then we'll also see high levels of capability in these other domains, including domains which are more relevant to AI R&D and the economy."
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-16T16:07:16.491Z",
    "af": false
  },
  {
    "_id": "46g5qbAhELqF7GHDX",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "wHMismXZp8BcctEFJ",
    "topLevelCommentId": "6ue8BPWrcoa2eGJdP",
    "contents": {
      "markdown": "- Anthropic reports SWE bench scores without reasoning which is some evidence it doesn't help (much) on this sort of task. (See e.g. the release blog post for 4 opus)\n- Anecdotal evidence\n\nProbably it would be more accurate to say \"doesn't seem to help much while it helps a lot for openai models\".",
      "plaintextDescription": " * Anthropic reports SWE bench scores without reasoning which is some evidence it doesn't help (much) on this sort of task. (See e.g. the release blog post for 4 opus)\n * Anecdotal evidence\n\nProbably it would be more accurate to say \"doesn't seem to help much while it helps a lot for openai models\"."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-12T05:10:18.283Z",
    "af": true
  },
  {
    "_id": "KtzEgxFoPFz5hWfZD",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Some people seem to think my timelines have shifted a bunch while they've only moderately changed.\n\nRelative to my views at the start of 2025, my median (50th percentile) for AIs fully automating AI R&D was pushed back by around 2 years—from something like Jan 2032 to Jan 2034. My 25th percentile has shifted similarly (though perhaps more importantly) from maybe July 2028 to July 2030. Obviously, my numbers aren't fully precise and vary some over time. (E.g., I'm not sure I would have quoted these exact numbers for this exact milestone at the start of the year; these numbers for the start of the year are partially reverse engineered from [this comment](https://www.lesswrong.com/posts/FG54euEAesRkSZuJN/ryan_greenblatt-s-shortform?commentId=iwodobEWjt9qwHbb2).)\n\nFully automating AI R&D is a pretty high milestone; my current numbers for something like \"AIs accelerate AI R&D as much as what would happen if employees ran 10x faster (e.g. by ~fully automating research engineering and some other tasks)\" are probably 50th percentile Jan 2032 and 25th percentile Jan 2029.[^given]\n\n[^given]: See [this comment](https://www.lesswrong.com/posts/FG54euEAesRkSZuJN/ryan_greenblatt-s-shortform?commentId=iwodobEWjt9qwHbb2) for the numbers I would have given for this milestone at the start of the year.\n\nI'm partially posting this so there is a record of my views; I think it's somewhat interesting to observe this over time. (That said, I don't want to anchor myself, which does seem like a serious downside. I should slide around a bunch and be somewhat incoherent if I'm updating as much as I should: my past views are always going to be somewhat obviously confused from the perspective of my current self.)\n\nWhile I'm giving these numbers, note that I think [Precise AGI timelines don't matter that much](https://www.lesswrong.com/posts/FG54euEAesRkSZuJN/ryan_greenblatt-s-shortform?commentId=eT6X2RxWEhskbRqiA).\n",
      "plaintextDescription": "Some people seem to think my timelines have shifted a bunch while they've only moderately changed.\n\nRelative to my views at the start of 2025, my median (50th percentile) for AIs fully automating AI R&D was pushed back by around 2 years—from something like Jan 2032 to Jan 2034. My 25th percentile has shifted similarly (though perhaps more importantly) from maybe July 2028 to July 2030. Obviously, my numbers aren't fully precise and vary some over time. (E.g., I'm not sure I would have quoted these exact numbers for this exact milestone at the start of the year; these numbers for the start of the year are partially reverse engineered from this comment.)\n\nFully automating AI R&D is a pretty high milestone; my current numbers for something like \"AIs accelerate AI R&D as much as what would happen if employees ran 10x faster (e.g. by ~fully automating research engineering and some other tasks)\" are probably 50th percentile Jan 2032 and 25th percentile Jan 2029.[1]\n\nI'm partially posting this so there is a record of my views; I think it's somewhat interesting to observe this over time. (That said, I don't want to anchor myself, which does seem like a serious downside. I should slide around a bunch and be somewhat incoherent if I'm updating as much as I should: my past views are always going to be somewhat obviously confused from the perspective of my current self.)\n\nWhile I'm giving these numbers, note that I think Precise AGI timelines don't matter that much.\n\n----------------------------------------\n\n 1. See this comment for the numbers I would have given for this milestone at the start of the year. ↩︎"
    },
    "baseScore": 32,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2025-09-10T20:42:01.652Z",
    "af": true
  },
  {
    "_id": "8TWMJknHhnxHfxSmX",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "HdPWGt54H26ZBiXpr",
    "topLevelCommentId": "iwodobEWjt9qwHbb2",
    "contents": {
      "markdown": "I've updated towards somewhat longer timelines again over the last 5 months. Maybe my 50th percentile for this milestone is now Jan 2032.",
      "plaintextDescription": "I've updated towards somewhat longer timelines again over the last 5 months. Maybe my 50th percentile for this milestone is now Jan 2032."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-10T16:57:28.138Z",
    "af": true
  },
  {
    "_id": "rmJhpqe3FFcEwJznx",
    "postId": "uRdJio8pnTqHpWa4t",
    "parentCommentId": "ZoZCD9MKC2QvyAa3c",
    "topLevelCommentId": "ZoZCD9MKC2QvyAa3c",
    "contents": {
      "markdown": "Some AI company employees with shorter timelines than me mostly. I also think that \"why I don't agree with X\" is a good prompt to express some deeper aspect of my models/views. It also makes a good reasonably engaging hook for a blog post.\n\nI might write some posts responding arguments for longer timelines that I disagree with if I feel like I have something interesting to say.",
      "plaintextDescription": "Some AI company employees with shorter timelines than me mostly. I also think that \"why I don't agree with X\" is a good prompt to express some deeper aspect of my models/views. It also makes a good reasonably engaging hook for a blog post.\n\nI might write some posts responding arguments for longer timelines that I disagree with if I feel like I have something interesting to say."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-10T16:53:09.838Z",
    "af": true
  },
  {
    "_id": "yEGLcvYmGHkepafCo",
    "postId": "RA7jb5h6sjnzivKv3",
    "parentCommentId": "3S4PTLxZssW7S5kxn",
    "topLevelCommentId": "ALwBknkiLbc4KtMNr",
    "contents": {
      "markdown": "Further discussion by Carl is [here](https://www.lesswrong.com/posts/RA7jb5h6sjnzivKv3/peterbarnett-s-shortform?commentId=EyBoP6qzmWNb99uPx).",
      "plaintextDescription": "Further discussion by Carl is here."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-06T23:21:17.439Z",
    "af": false
  },
  {
    "_id": "yHLNvYtfXi6dY5EZp",
    "postId": "DBn83cvA6PDeq8o5x",
    "parentCommentId": "KueCfhKrbPeesNbah",
    "topLevelCommentId": "KueCfhKrbPeesNbah",
    "contents": {
      "markdown": "See also [To be legible, evidence of misalignment probably has to be behavioral](https://www.lesswrong.com/posts/4QRvFCzhFbedmNfp4/to-be-legible-evidence-of-misalignment-probably-has-to-be).",
      "plaintextDescription": "See also To be legible, evidence of misalignment probably has to be behavioral."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-06T02:26:28.613Z",
    "af": false
  },
  {
    "_id": "F6rbnZ5JaAW4dECdJ",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "wRvJasfhGBsvzd5oG",
    "topLevelCommentId": "eT6X2RxWEhskbRqiA",
    "contents": {
      "markdown": "This is only somewhat related to what you were saying, but I do think 100 year medians vs 10 year medians does matter a bunch.",
      "plaintextDescription": "This is only somewhat related to what you were saying, but I do think 100 year medians vs 10 year medians does matter a bunch."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-04T22:16:34.661Z",
    "af": false
  },
  {
    "_id": "gYMjeLbuntdHeuaZw",
    "postId": "PBd7xPAh22y66rbme",
    "parentCommentId": "Y8tinYT65HjFaJ2tz",
    "topLevelCommentId": "A74RpoCmPpuEk476c",
    "contents": {
      "markdown": "> The Time article is materially wrong about a bunch of stuff\n\nAgreed which is why I noted this in my comment.[^foot] I think it's a bad sign that Anthropic seemingly actively sought out an article that ended up being wrong/misleading in a way which was convenient for Anthropic at the time and then didn't correct it.\n\n[^foot]: I said something weaker: \"For what it's worth, I think the implication of the article is wrong and the LTBT actually has very strong de jure power\", because I didn't see anything which is literally false as stated as opposed to being misleading. But you'd know better.\n\n> I really don't want to get into pedantic details, but there's no \"supposed to\" time for LTBT board appointments, I think you're counting from the first day they were legally able to appoint someone. Also https://www.anthropic.com/company lists five board members out of five seats, and four Trustees out of a maximum five. IMO it's fine to take a few months to make sure you've found the right person!\n\nFirst, I agree that there isn't a \"supposed to\" time, my wording here was sloppy, sorry about that.\n\nMy understanding was a that there was a long delay (e.g. much longer than a few months) between the LTBT being able to appoint a board member and actually appointing such a member and a long time where the LTBT only had 3 members. I think this long of a delay is somewhat concerning.\n\nMy understanding is that the LTBT could still decide one more seat (so that it determines a majority of the board). (Or maybe appoint 2 additional seats?) And that it has been able to do this for almost a year at this point. Maybe the LTBT thinks the current board composition is good such that appointments aren't needed, but the lack of any external AI safety expertise on the board or LTBT concerns me...\n\n> More broadly, the corporate governance discussions (not just about Anthropic) I see on LessWrong and in the EA community are very deeply frustrating, because almost nobody seems to understand how these structures normally function or why they're designed that way or the failure modes that occur in practise. Personally, I spent about a decade serving on nonprofit boards, oversight committes which appointed nonprofit boards, and set up the goverance for a for-profit company I founded.\n\nI certainly don't have particular expertise in corporate governance and I'd be interested in whether corporate governance experts who are unconflicted and very familiar with the AI situation think that the LTBT has the de facto power needed to govern the company through transformative AI. (And whether the public evidence should make me much less concerned about the LTBT than I would be about the OpenAI board.)\n\nMy view is that the normal functioning of a structure like the LTBT or a board would be dramatically insufficient for governing transformative AI (boards normally have a much weaker function in practice than the ostensible purposes of the LTBT and the Anthropic board), so I'm not very satisfied by \"the LTBT is behaving how a body of this sort would/should normally behave\".",
      "plaintextDescription": "> The Time article is materially wrong about a bunch of stuff\n\nAgreed which is why I noted this in my comment.[1] I think it's a bad sign that Anthropic seemingly actively sought out an article that ended up being wrong/misleading in a way which was convenient for Anthropic at the time and then didn't correct it.\n\n> I really don't want to get into pedantic details, but there's no \"supposed to\" time for LTBT board appointments, I think you're counting from the first day they were legally able to appoint someone. Also https://www.anthropic.com/company lists five board members out of five seats, and four Trustees out of a maximum five. IMO it's fine to take a few months to make sure you've found the right person!\n\nFirst, I agree that there isn't a \"supposed to\" time, my wording here was sloppy, sorry about that.\n\nMy understanding was a that there was a long delay (e.g. much longer than a few months) between the LTBT being able to appoint a board member and actually appointing such a member and a long time where the LTBT only had 3 members. I think this long of a delay is somewhat concerning.\n\nMy understanding is that the LTBT could still decide one more seat (so that it determines a majority of the board). (Or maybe appoint 2 additional seats?) And that it has been able to do this for almost a year at this point. Maybe the LTBT thinks the current board composition is good such that appointments aren't needed, but the lack of any external AI safety expertise on the board or LTBT concerns me...\n\n> More broadly, the corporate governance discussions (not just about Anthropic) I see on LessWrong and in the EA community are very deeply frustrating, because almost nobody seems to understand how these structures normally function or why they're designed that way or the failure modes that occur in practise. Personally, I spent about a decade serving on nonprofit boards, oversight committes which appointed nonprofit boards, and set up the goverance for a for-profit company I fou"
    },
    "baseScore": 44,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2025-09-04T19:49:26.959Z",
    "af": false
  },
  {
    "_id": "3S4PTLxZssW7S5kxn",
    "postId": "RA7jb5h6sjnzivKv3",
    "parentCommentId": "ALwBknkiLbc4KtMNr",
    "topLevelCommentId": "ALwBknkiLbc4KtMNr",
    "contents": {
      "markdown": "People seem to be reacting to this as though it is bad news. Why? I'd guess the net harm caused by these investments is negligible and this seems like a reasonable earning to give strategy.",
      "plaintextDescription": "People seem to be reacting to this as though it is bad news. Why? I'd guess the net harm caused by these investments is negligible and this seems like a reasonable earning to give strategy."
    },
    "baseScore": 25,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2025-09-04T18:14:48.066Z",
    "af": false
  },
  {
    "_id": "jJcqoRxRkfTP5iBG8",
    "postId": "HsLWpZ2zad43nzvWi",
    "parentCommentId": "6ccfz7BNjK8c28FdQ",
    "topLevelCommentId": "6ccfz7BNjK8c28FdQ",
    "contents": {
      "markdown": "Agentic software engineering mostly, I don't think Genie matters.",
      "plaintextDescription": "Agentic software engineering mostly, I don't think Genie matters."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-04T15:15:58.034Z",
    "af": false
  },
  {
    "_id": "fK4KEyDv4YxzoQQnz",
    "postId": "PBd7xPAh22y66rbme",
    "parentCommentId": "kCSbLieAKrwCqJ4pi",
    "topLevelCommentId": "A74RpoCmPpuEk476c",
    "contents": {
      "markdown": "I certainly agree that the LTBT has de jure control (or as you say \"formal control\").\n\nBy \"strong control\" I meant more precisely something like: \"lots of influence in practice, e.g. the influence of Anthropic leadership is comparable to the influence that the LTBT itself is exerting in practice over appointments or comparable (though probably less) to the influence that (e.g.) Sam Altman has had over recent board appointments at OpenAI\". Perhaps \"it seems like they have a bunch of control\" would have been a more accurate way to put things.\n\nI think it would be totally unsurprising for the LTBT to have de jure power but not that much de facto power (given the influence of Anthropic leadership) and from the outside it sure looks like this is the case at the moment.\n\nSee [this Time article](https://time.com/6983420/anthropic-structure-openai-incentives/), which was presumably explicitly sought out by Anthropic to reassure investors in the aftermath of the OpenAI board crisis, in which Brian Israel (at the time general counsel at Anthropic) is paraphrased as repeatedly saying (to investors) \"what happened at OpenAI can't happen to us\". The article (again, likely explicitly sought out by Anthropic as far as I can tell) also says \"it also means that the LTBT ultimately has a limited influence on the company: while it will eventually have the power to select and remove a majority of board members, those members will in practice face similar incentives to the rest of the board.\" For what it's worth, I think the implication of the article is wrong and the LTBT actually has very strong de jure power (optimistically, the journalist misinterpreted Brian Israel and wrote a misleading article), but it sure seems like Anthropic leadership wanted to create the impression that the power of the LTBT is limited to reassure shareholders (which does actually weaken the LTBT: the power of institutions is partially based on perception, see e.g. the OpenAI board).\n\nI find the board appointments of the LTBT to not be reassuring; these hires seem unlikely to result in serious oversight of the company due to insufficient expertise and not being dedicated full-time board members. I also don't find it reassuring that these hires were made far after when they were supposed to be made and that the LTBT hasn't filled its empty seats. (At least based on public information.)\n\nAll these concerns wouldn't be a big deal if this was a normal company rather than a company aiming to build AGI: probably the single largest danger to humanity as well as the most dangerous and important technology ever (as I expect people at Anthropic would agree).\n\n(See also my discussion of the LTBT [in this comment](https://www.lesswrong.com/posts/6tjHf5ykvFqaNCErH/anthropic-s-responsible-scaling-policy-and-long-term-benefit?commentId=NyqcvZifqznNGKxdT), though I think I say strictly more here.)\n\nI could imagine the LTBT stepping up to take on a more serious oversight role and it seems plausible this will happen in the future, but as it stands public evidence makes it look like the de facto power being exerted by the LTBT is very limited. It's hard for me to have much confidence either way with my limited knowledge.\n\nTo be clear, my view is that this situation is substantially the fault of current LTBT members (who in my view should probably think of governing Anthropic as their top priority and do this full time).\n\nHere are some (possibly costly) actions that Anthropic or the LTBT could take which would (partially) reassure me:\n\n- Designate a member of the alignment science team to report periodically to the LTBT directly. Ideally, this person would be employed by the LTBT rather than by Anthropic (e.g. Anthropic can't fire them) and wouldn't have equity so they are less financially conflicted. It should be public who this is. This could be someone on the alignment stress testing sub-team. I think DMZ is the person with the position at Anthropic that is most naturally suited to do this. I have other specific candidates in mind and could share privately on request.\n- The LTBT generally acquires more full-time staff who are independent from the company.\n- Dario states internally in a clear way that he wouldn't aggressively maneuver against the board (or the LTBT) if they were trying to remove him or otherwise do something he disagreed with. And, that Anthropic employees shouldn't join in efforts to undermine the (de jure) power of the board if this happened. This wouldn't be verifiable externally (unless Dario said this publicly), but I do think it meaningfully ties Dario's hands (because a major source of power Dario has is strong employee loyalty). As far as I know, Dario could have already done this, but I'm skeptical this has happened on priors.\n- Some member(s) of the LTBT become full time on the job of being an LTBT member and spend a bunch of time talking to employees and external experts etc. This would ideally be a new LTBT member who has domain expertise. Substantially more time would also help.\n- The hiring process for new board members by the LTBT is changed to enforce strong separation between the LTBT and the existing board/leadership via not providing any information to the existing board or Anthropic leadership until the hire is decided. This seems very costly and I'm not sure I'd recommend this, but it would address my particular concerns. I think this could be a good choice if the LTBT had independent staff and full-time LTBT members.\n\nTo be clear, my view is that Anthropic is currently overall the best governed/managed company trying to build AGI, but this is due to my views about Dario and other Anthropic executives (which are partially based on connections and private knowledge) rather than due to the LTBT. And I don't think \"best governed/managed AGI company\" is a very high bar.",
      "plaintextDescription": "I certainly agree that the LTBT has de jure control (or as you say \"formal control\").\n\nBy \"strong control\" I meant more precisely something like: \"lots of influence in practice, e.g. the influence of Anthropic leadership is comparable to the influence that the LTBT itself is exerting in practice over appointments or comparable (though probably less) to the influence that (e.g.) Sam Altman has had over recent board appointments at OpenAI\". Perhaps \"it seems like they have a bunch of control\" would have been a more accurate way to put things.\n\nI think it would be totally unsurprising for the LTBT to have de jure power but not that much de facto power (given the influence of Anthropic leadership) and from the outside it sure looks like this is the case at the moment.\n\nSee this Time article, which was presumably explicitly sought out by Anthropic to reassure investors in the aftermath of the OpenAI board crisis, in which Brian Israel (at the time general counsel at Anthropic) is paraphrased as repeatedly saying (to investors) \"what happened at OpenAI can't happen to us\". The article (again, likely explicitly sought out by Anthropic as far as I can tell) also says \"it also means that the LTBT ultimately has a limited influence on the company: while it will eventually have the power to select and remove a majority of board members, those members will in practice face similar incentives to the rest of the board.\" For what it's worth, I think the implication of the article is wrong and the LTBT actually has very strong de jure power (optimistically, the journalist misinterpreted Brian Israel and wrote a misleading article), but it sure seems like Anthropic leadership wanted to create the impression that the power of the LTBT is limited to reassure shareholders (which does actually weaken the LTBT: the power of institutions is partially based on perception, see e.g. the OpenAI board).\n\nI find the board appointments of the LTBT to not be reassuring; these hires seem unlikely "
    },
    "baseScore": 67,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2025-09-04T15:10:33.804Z",
    "af": false
  },
  {
    "_id": "n2nYiyiY8YZSLmzAa",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "sD3SMnhHMAox22wsC",
    "topLevelCommentId": "eT6X2RxWEhskbRqiA",
    "contents": {
      "markdown": "For people who are comparatively advantaged at this, it seems good to try to make the case for this in a variety of different ways. One place to start is to try to convince relatively soft target audiences like me (who's sympathetic but disagrees) by e.g. posting on LW and then go somewhere from here.\n\nI think it's a rough task, but ultimately worth trying.",
      "plaintextDescription": "For people who are comparatively advantaged at this, it seems good to try to make the case for this in a variety of different ways. One place to start is to try to convince relatively soft target audiences like me (who's sympathetic but disagrees) by e.g. posting on LW and then go somewhere from here.\n\nI think it's a rough task, but ultimately worth trying."
    },
    "baseScore": 5,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-04T00:21:05.916Z",
    "af": false
  },
  {
    "_id": "eT6X2RxWEhskbRqiA",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "# Precise AGI timelines don't matter that much.\n\nWhile I do spend some time discussing AGI timelines (and I've written [some](https://www.lesswrong.com/posts/HsLWpZ2zad43nzvWi/trust-me-bro-just-one-more-rl-scale-up-this-one-will-be-the) [posts](https://www.lesswrong.com/posts/2ssPfDpdrjaM2rMbn/my-agi-timeline-updates-from-gpt-5-and-2025-so-far-1) [about it](https://www.lesswrong.com/posts/FG54euEAesRkSZuJN/ryan_greenblatt-s-shortform?commentId=6ue8BPWrcoa2eGJdP) recently), I don't think moderate quantitative differences in AGI timelines matter that much for deciding what to do[^richard]. For instance, having a 15-year median rather than a 6-year median doesn't make that big of a difference. That said, I do think that moderate differences in the chance of very short timelines (i.e., less than 3 years) matter more: going from a 20% chance to a 50% chance of full AI R&D automation within 3 years should potentially make a substantial difference to strategy.[^pol]\n\n[^richard]: I've seen Richard Ngo make this point before, though I couldn't find where he did this. More generally, this isn't a very original point; I just think it's worth making given that I've been talking about timelines recently.\n\n[^pol]: I also think that the chance that very powerful AI happens under this presidential administration is action-relevant for policy.\n\nAdditionally, my guess is that the most productive way to engage with discussion around timelines is mostly to not care much about resolving disagreements, but then when there appears to be a large chance that timelines are very short (e.g., >25% in <2 years) it's worthwhile to try hard to argue for this.[^views] I think takeoff speeds are much more important to argue about when making the case for AI risk.\n\n[^views]: You could have views such that you expect to never be >25% confident in <2-year timelines until it's basically too late. For instance, maybe you expect very fast takeoff driven by a single large algorithmic advance. Under this view, I think arguing about the details of timelines looks even less good and you should mostly make the case for risk independently of this, perhaps arguing \"it seems like AI could emerge quickly and unexpectedly, so we need to act now\".\n\nI do think that having somewhat precise views is helpful for some people in doing relatively precise prioritization within people already working on safety, but this seems pretty niche.\n\nGiven that I don't think timelines are that important, why have I been writing about this topic? This is due to a mixture of: I find it relatively quick and easy to write about timelines, my commentary is relevant to the probability of very short timelines (which I do think is important as discussed above), a bunch of people seem interested in timelines regardless, and I do think timelines matter some.\n\nConsider reflecting on whether you're overly fixated on details of timelines.",
      "plaintextDescription": "Precise AGI timelines don't matter that much.\nWhile I do spend some time discussing AGI timelines (and I've written some posts about it recently), I don't think moderate quantitative differences in AGI timelines matter that much for deciding what to do[1]. For instance, having a 15-year median rather than a 6-year median doesn't make that big of a difference. That said, I do think that moderate differences in the chance of very short timelines (i.e., less than 3 years) matter more: going from a 20% chance to a 50% chance of full AI R&D automation within 3 years should potentially make a substantial difference to strategy.[2]\n\nAdditionally, my guess is that the most productive way to engage with discussion around timelines is mostly to not care much about resolving disagreements, but then when there appears to be a large chance that timelines are very short (e.g., >25% in <2 years) it's worthwhile to try hard to argue for this.[3] I think takeoff speeds are much more important to argue about when making the case for AI risk.\n\nI do think that having somewhat precise views is helpful for some people in doing relatively precise prioritization within people already working on safety, but this seems pretty niche.\n\nGiven that I don't think timelines are that important, why have I been writing about this topic? This is due to a mixture of: I find it relatively quick and easy to write about timelines, my commentary is relevant to the probability of very short timelines (which I do think is important as discussed above), a bunch of people seem interested in timelines regardless, and I do think timelines matter some.\n\nConsider reflecting on whether you're overly fixated on details of timelines.\n\n----------------------------------------\n\n 1. I've seen Richard Ngo make this point before, though I couldn't find where he did this. More generally, this isn't a very original point; I just think it's worth making given that I've been talking about timelines recently. ↩︎\n\n 2. I also t"
    },
    "baseScore": 93,
    "voteCount": 42,
    "createdAt": null,
    "postedAt": "2025-09-03T22:35:36.258Z",
    "af": true
  },
  {
    "_id": "nXCSkxsBkGKF57c88",
    "postId": "PBd7xPAh22y66rbme",
    "parentCommentId": "wAZPZDRFe4om5EBAL",
    "topLevelCommentId": "A74RpoCmPpuEk476c",
    "contents": {
      "markdown": "[Feel free not to respond / react with the \"not worth getting into\" emoji\"]\n\n> Severe or willful violation of our RSP, or misleading the public about it.\n\nShould this be read as \"[Severe or willful violation of our RSP] or [misleading the public about the RSP]\" or should this be read as \"[Severe or willful violation of our RSP] or [Severe or willful misleading the public about the RSP]\".\n\nIn my views/experience, I'd say there are instances where the public and (perhaps more strongly) Anthropic employees have been misled about the RSP somewhat willfully (e.g., there's an obvious well known important misconception that is convenient for Anthropic leadership and that wasn't corrected), though I guess I wouldn't consider this to be a severe violation.\n\n> If the LTBT was abolished without a good replacement.\n\nCurious about how you'd relate to the \"the LTBT isn't applying any meaningful oversight, Anthropic leadership has strong control over board appointments, and this isn't on track to change (and Anthropic leadership isn't really trying to change this)\". I'd say this is the current status quo. This is kind of a tricky thing to do well, but it doesn't really seem from the outside like Anthropic is actually trying on this. (Which is maybe a reasonable choice, because idk if the LTBT was ever really that important given realistic constraints, but you see to think it is important.)",
      "plaintextDescription": "[Feel free not to respond / react with the \"not worth getting into\" emoji\"]\n\n> Severe or willful violation of our RSP, or misleading the public about it.\n\nShould this be read as \"[Severe or willful violation of our RSP] or [misleading the public about the RSP]\" or should this be read as \"[Severe or willful violation of our RSP] or [Severe or willful misleading the public about the RSP]\".\n\nIn my views/experience, I'd say there are instances where the public and (perhaps more strongly) Anthropic employees have been misled about the RSP somewhat willfully (e.g., there's an obvious well known important misconception that is convenient for Anthropic leadership and that wasn't corrected), though I guess I wouldn't consider this to be a severe violation.\n\n> If the LTBT was abolished without a good replacement.\n\nCurious about how you'd relate to the \"the LTBT isn't applying any meaningful oversight, Anthropic leadership has strong control over board appointments, and this isn't on track to change (and Anthropic leadership isn't really trying to change this)\". I'd say this is the current status quo. This is kind of a tricky thing to do well, but it doesn't really seem from the outside like Anthropic is actually trying on this. (Which is maybe a reasonable choice, because idk if the LTBT was ever really that important given realistic constraints, but you see to think it is important.)"
    },
    "baseScore": 44,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2025-09-03T21:55:30.488Z",
    "af": false
  },
  {
    "_id": "CAooquHpfDAcZPSk3",
    "postId": "cxuzALcmucCndYv4a",
    "parentCommentId": "wMwx3c3tPmzvic22d",
    "topLevelCommentId": "jhCCPGFvrAz2cRKLk",
    "contents": {
      "markdown": "[https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version#2_1__Action_counterfactuals](https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version#2_1__Action_counterfactuals), [https://agentfoundations.org/item?id=1399](https://agentfoundations.org/item?id=1399), [https://www.alignmentforum.org/posts/5bd75cc58225bf06703753d4/two-major-obstacles-for-logical-inductor-decision-theory?commentId=fJGDzhKy2FQQpPqKn](https://www.alignmentforum.org/posts/5bd75cc58225bf06703753d4/two-major-obstacles-for-logical-inductor-decision-theory?commentId=fJGDzhKy2FQQpPqKn)",
      "plaintextDescription": "https://www.lesswrong.com/posts/i3BTagvt3HbPMx6PN/embedded-agency-full-text-version#2_1__Action_counterfactuals, https://agentfoundations.org/item?id=1399, https://www.alignmentforum.org/posts/5bd75cc58225bf06703753d4/two-major-obstacles-for-logical-inductor-decision-theory?commentId=fJGDzhKy2FQQpPqKn"
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-03T18:43:47.322Z",
    "af": false
  },
  {
    "_id": "Gk9483vf5EAE9ssPQ",
    "postId": "HsLWpZ2zad43nzvWi",
    "parentCommentId": "ABXXdTBNmyEtwyChg",
    "topLevelCommentId": "R9Awe6uWkC2QEGo6G",
    "contents": {
      "markdown": "- I expect lower cost per rollout on average due to AI companies doing RL on a bunch of smaller tasks and from companies not necessarily using tons of reasoning tokens on most envs. Also, API prices are marked up relative to what companies actually pay on compute which can easily add a factor of 3. If we are just looking at the hardest agentic software engineering environments, then this closes the gap a decent amount.\n- I expect spending on RL enviroments to be more like 10x lower than RL training compute rather than similar (and I wouldn't be surprised by a large gap) because it's hard to massively scale up spending on RL envs effectively in a short period of time while we already have an scaled up industrial process for buying more compute.\n\nI'm more sympathetic to \"companies will spend this much on some high quality RL envs\" than \"the typical RL env will be very expensive\", but I think some disagreement remains.",
      "plaintextDescription": " * I expect lower cost per rollout on average due to AI companies doing RL on a bunch of smaller tasks and from companies not necessarily using tons of reasoning tokens on most envs. Also, API prices are marked up relative to what companies actually pay on compute which can easily add a factor of 3. If we are just looking at the hardest agentic software engineering environments, then this closes the gap a decent amount.\n * I expect spending on RL enviroments to be more like 10x lower than RL training compute rather than similar (and I wouldn't be surprised by a large gap) because it's hard to massively scale up spending on RL envs effectively in a short period of time while we already have an scaled up industrial process for buying more compute.\n\nI'm more sympathetic to \"companies will spend this much on some high quality RL envs\" than \"the typical RL env will be very expensive\", but I think some disagreement remains."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-03T15:25:01.532Z",
    "af": true
  },
  {
    "_id": "m3595XS3kthA2ZrYt",
    "postId": "HsLWpZ2zad43nzvWi",
    "parentCommentId": "ZZhDibq8ALdgzEQrY",
    "topLevelCommentId": "ZZhDibq8ALdgzEQrY",
    "contents": {
      "markdown": "Yeah, revenue trends generally seem less robust regardless. (It doesn't look like there is a consistent longer running trend except maybe over the last 2 years. I'd also expect revenue to be less stable in general.)",
      "plaintextDescription": "Yeah, revenue trends generally seem less robust regardless. (It doesn't look like there is a consistent longer running trend except maybe over the last 2 years. I'd also expect revenue to be less stable in general.)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-03T14:29:18.757Z",
    "af": false
  },
  {
    "_id": "XLzhJFmb4Yi7yduEG",
    "postId": "HsLWpZ2zad43nzvWi",
    "parentCommentId": "R9Awe6uWkC2QEGo6G",
    "topLevelCommentId": "R9Awe6uWkC2QEGo6G",
    "contents": {
      "markdown": "Thanks, I wasn't aware of this post. (I think it overstates the level of spending we'll see on the average RL env within a year by maybe 10x or more, but I agree directionally.)",
      "plaintextDescription": "Thanks, I wasn't aware of this post. (I think it overstates the level of spending we'll see on the average RL env within a year by maybe 10x or more, but I agree directionally.)"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-03T14:04:06.381Z",
    "af": true
  },
  {
    "_id": "PycsNnNXbZhsyCQvS",
    "postId": "fdCaCDfstHxyPmB9h",
    "parentCommentId": "v2kytzaT5sCgBjLsx",
    "topLevelCommentId": "v2kytzaT5sCgBjLsx",
    "contents": {
      "markdown": "There is a natural sense in which AI progress is exponential: capabilities are increasing at a rate which involves exponentially increasing impact (as measured by e.g. economic value).",
      "plaintextDescription": "There is a natural sense in which AI progress is exponential: capabilities are increasing at a rate which involves exponentially increasing impact (as measured by e.g. economic value)."
    },
    "baseScore": 15,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-02T00:57:54.367Z",
    "af": false
  },
  {
    "_id": "LtBizcFB2rFTrmExH",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "5er28NDwBMPcwi97D",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "Hmm, I think what I said was about half wrong and I want to retract my point.\n\nThat said, I think much of the relevant questions are overlapping (like, \"how do we expect the future to generally go?\", \"why/how is AI risky?\", \"how fast will algorithmic progress go at various points?) and I interpret this post as just talking about the effect on epistemics around the overlapping questions (regardless of whether you'd expect moderates to mostly be working in domains with better feedback loops).\n\nThis isn't that relevant for your main point, but I also think the biggest question for radicals in practice is mostly: How can we generate massive public/government support for radical action on AI? ",
      "plaintextDescription": "Hmm, I think what I said was about half wrong and I want to retract my point.\n\nThat said, I think much of the relevant questions are overlapping (like, \"how do we expect the future to generally go?\", \"why/how is AI risky?\", \"how fast will algorithmic progress go at various points?) and I interpret this post as just talking about the effect on epistemics around the overlapping questions (regardless of whether you'd expect moderates to mostly be working in domains with better feedback loops).\n\nThis isn't that relevant for your main point, but I also think the biggest question for radicals in practice is mostly: How can we generate massive public/government support for radical action on AI?"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-31T14:18:03.619Z",
    "af": false
  },
  {
    "_id": "jtgKFFPsRTbobF328",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "z6mZR4zjaTXApHChG",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "> On a more object level, my main critique of the post is that almost all of the bullet points are even more true of, say, working as a physicist.\n\nBut (in the language of the post) both moderates and radicals are working in the epistemic domain not some unrelated domain. It's not that moderates and radicals are trying to answer different questions (and the questions moderates are answering are epistemically easier like physics). There are some differences in the most relevant questions, but I don't think this is a massive effect.",
      "plaintextDescription": "> On a more object level, my main critique of the post is that almost all of the bullet points are even more true of, say, working as a physicist.\n\nBut (in the language of the post) both moderates and radicals are working in the epistemic domain not some unrelated domain. It's not that moderates and radicals are trying to answer different questions (and the questions moderates are answering are epistemically easier like physics). There are some differences in the most relevant questions, but I don't think this is a massive effect."
    },
    "baseScore": 2,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-30T01:15:23.938Z",
    "af": false
  },
  {
    "_id": "dqHpeQFsnhRSz6XAt",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "XzTS9Mt2FSvjYu6ik",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "I wasn't talking in retrospect, but I meant something might larger than $1 billion by strong incentives and I really mean very specifically APS systems at the time when they are feasible to build.\n\nThe 10% would come from other approaches/architectures ending up being surprisingly better at the point when people could build APS systems. (E.g., you don't need your AIs to have \"the models they use in making plans represent with reasonable accuracy the causal upshot of gaining and maintaining different forms of power over humans and the real-world environment\".)\n\nIn further consideration I might be more like 95%, but 90% doesn't seem crazy to me depending on the details of the operationalization.\n\nI would put very high probabilities on \"people would pay >$50 billion for a strong APS system right now\", so we presumably agree on that.\n\nIt's really key to my perspective here that by the time that we can build APS systems, maybe something else which narrowly doesn't meet this definition has come around and looks more competitive. There is something messy here because maybe there are strong incentives to build APS systems eventually, but this occurs substantially after full automation of the whole economy or similar by other systems. I was trying to exclude cases where substantially after human intellectual labor is totally obsolete, APS systems are strongly incentivized as this case is pretty different. (And other factors like \"maybe we'll have radical superbabies before we can build APS systems\" factor in too, though again this is very sensitive to operationalization.)",
      "plaintextDescription": "I wasn't talking in retrospect, but I meant something might larger than $1 billion by strong incentives and I really mean very specifically APS systems at the time when they are feasible to build.\n\nThe 10% would come from other approaches/architectures ending up being surprisingly better at the point when people could build APS systems. (E.g., you don't need your AIs to have \"the models they use in making plans represent with reasonable accuracy the causal upshot of gaining and maintaining different forms of power over humans and the real-world environment\".)\n\nIn further consideration I might be more like 95%, but 90% doesn't seem crazy to me depending on the details of the operationalization.\n\nI would put very high probabilities on \"people would pay >$50 billion for a strong APS system right now\", so we presumably agree on that.\n\nIt's really key to my perspective here that by the time that we can build APS systems, maybe something else which narrowly doesn't meet this definition has come around and looks more competitive. There is something messy here because maybe there are strong incentives to build APS systems eventually, but this occurs substantially after full automation of the whole economy or similar by other systems. I was trying to exclude cases where substantially after human intellectual labor is totally obsolete, APS systems are strongly incentivized as this case is pretty different. (And other factors like \"maybe we'll have radical superbabies before we can build APS systems\" factor in too, though again this is very sensitive to operationalization.)"
    },
    "baseScore": 3,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-29T18:07:05.475Z",
    "af": false
  },
  {
    "_id": "zvd2AygxkCNSZrf8F",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "CiRNK66CJEbmZyZXP",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "I agree with this in principle, but think that doing a good job of noting major failings of prominent moderates in the current environment would look very different than Eliezer's comment and requires something stronger than just giving examples of some moderates which seem incorrect to Eliezer.\n\nAnother way to put this is that I think citing a small number of anecdotes in defense of a broader world view is [a dangerous thing to do](https://www.lesswrong.com/posts/DSzpr8Y9299jdDLc9/cardiologists-and-chinese-robbers) and not attaching this to the argument in the post is even more dangerous. I think it's more dangerous when the description of the anecdotes is sneering and misleading. So, when using this epistemically dangerous tool, I think there is a higher burden of doing a good job which isn't done here.\n\nOn the specifics here, I think Carlsmith's report is unrepresentative for a bunch of reasons. I think Bioanchors is representative (though I don't think it looks fucking nuts in retrospect).\n\nThis is putting aside the fact that this doesn't engage with the arguments in the post at all beyond effectively reacting to the title.",
      "plaintextDescription": "I agree with this in principle, but think that doing a good job of noting major failings of prominent moderates in the current environment would look very different than Eliezer's comment and requires something stronger than just giving examples of some moderates which seem incorrect to Eliezer.\n\nAnother way to put this is that I think citing a small number of anecdotes in defense of a broader world view is a dangerous thing to do and not attaching this to the argument in the post is even more dangerous. I think it's more dangerous when the description of the anecdotes is sneering and misleading. So, when using this epistemically dangerous tool, I think there is a higher burden of doing a good job which isn't done here.\n\nOn the specifics here, I think Carlsmith's report is unrepresentative for a bunch of reasons. I think Bioanchors is representative (though I don't think it looks fucking nuts in retrospect).\n\nThis is putting aside the fact that this doesn't engage with the arguments in the post at all beyond effectively reacting to the title."
    },
    "baseScore": 16,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-08-29T16:24:09.095Z",
    "af": false
  },
  {
    "_id": "Zg7RNyF9iQrm3qQs5",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "zznnKPKZsHisLqeLE",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "I think williawa's characterization of how people reacted to bioanchors basically matches my experience and I'm skeptical of the claim that OpenPhil was very politically controlling of EA with respect to timelines.\n\nAnd, I agree with the claim that that Eliezer often implies people interpreted bioanchors in some way they didn't. (I also think bioanchors looks pretty reasonable in retrospect, but this is a separate claim.)",
      "plaintextDescription": "I think williawa's characterization of how people reacted to bioanchors basically matches my experience and I'm skeptical of the claim that OpenPhil was very politically controlling of EA with respect to timelines.\n\nAnd, I agree with the claim that that Eliezer often implies people interpreted bioanchors in some way they didn't. (I also think bioanchors looks pretty reasonable in retrospect, but this is a separate claim.)"
    },
    "baseScore": 3,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-29T16:01:54.953Z",
    "af": false
  },
  {
    "_id": "PcEzyXPibLyzLhPDk",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "AudP8djw4goyjzCie",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "> Carlsmith's Multiple Stage Fallacy risk estimate of 5% that involved only an 80% chance anyone would even try to build agentic AI?\n\nThis is false as stated. The report says:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3c6147b1ab9fea2069400bc0ed90dcf14a70506c3aa5de6f.png)\n\nThe corresponding footnote 179 is:\n\n> As a reminder, APS systems are ones with: (a) Advanced capability: they outperform the best humans on some set of tasks which when performed at an advanced level grant significant power in today’s world (tasks like scientific research, business/military/political strategy, engineering, hacking, and social persuasion/manipulation); (b) Agentic planning: they make and execute plans, in pursuit of objectives, on the basis of models of the world; and (c) Strategic awareness: the models they use in making plans represent with reasonable accuracy the causal upshot of gaining and maintaining different forms of power over humans and the real-world environment.\n\nStrong incentives isn't the same as \"anyone would try to build\" and \"agentic AI\" isn't the same as APS systems (which has a much more specific and stronger definition!).\n\nI'd personally put more like 90% on the claim (and it might depend a lot on what you mean by strong incentives).\n\nTo be clear, I agree with the claim that Carlsmith's report suffers from multi-stage fallacy (e.g., even without strong incentives to build APS systems, you can easily get AI takeover on my views) and is importantly wrong (and I thought so at the time), but your specific claim about the report here is incorrect.",
      "plaintextDescription": "> Carlsmith's Multiple Stage Fallacy risk estimate of 5% that involved only an 80% chance anyone would even try to build agentic AI?\n\nThis is false as stated. The report says:\n\nThe corresponding footnote 179 is:\n\n> As a reminder, APS systems are ones with: (a) Advanced capability: they outperform the best humans on some set of tasks which when performed at an advanced level grant significant power in today’s world (tasks like scientific research, business/military/political strategy, engineering, hacking, and social persuasion/manipulation); (b) Agentic planning: they make and execute plans, in pursuit of objectives, on the basis of models of the world; and (c) Strategic awareness: the models they use in making plans represent with reasonable accuracy the causal upshot of gaining and maintaining different forms of power over humans and the real-world environment.\n\nStrong incentives isn't the same as \"anyone would try to build\" and \"agentic AI\" isn't the same as APS systems (which has a much more specific and stronger definition!).\n\nI'd personally put more like 90% on the claim (and it might depend a lot on what you mean by strong incentives).\n\nTo be clear, I agree with the claim that Carlsmith's report suffers from multi-stage fallacy (e.g., even without strong incentives to build APS systems, you can easily get AI takeover on my views) and is importantly wrong (and I thought so at the time), but your specific claim about the report here is incorrect."
    },
    "baseScore": 29,
    "voteCount": 18,
    "createdAt": null,
    "postedAt": "2025-08-29T15:56:37.133Z",
    "af": false
  },
  {
    "_id": "9xFftQqJFT4v7cZLS",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "u9ajvjNKEqaavYghL",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "This seems very wrong to me from my experience in 2022 (though maybe the situation was very different in 2021? Or maybe there is some other social circle that I wasn't exposed to which had these properties?). ",
      "plaintextDescription": "This seems very wrong to me from my experience in 2022 (though maybe the situation was very different in 2021? Or maybe there is some other social circle that I wasn't exposed to which had these properties?)."
    },
    "baseScore": 1,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-29T15:47:54.638Z",
    "af": false
  },
  {
    "_id": "GSmDKjaWtn6ukefrG",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "AudP8djw4goyjzCie",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "Note that this post is arguing that there are some specific epistemic advantages of working as a moderate, not that moderates are always correct or that there aren't epistemic disadvantages to being a moderate. I don't think \"there exist moderates which seem very incorrect to me\" is a valid response to the post similarly to how \"there exist radicals which seem very incorrect to me\" wouldn't be a valid argument for the post.\n\nThis is independent from the point [Buck notes](https://www.lesswrong.com/posts/9MaTnw5sWeQrggYBG/an-epistemic-advantage-of-working-as-a-moderate?commentId=wfNMofrg4f25zAAwg) that the label moderate as defined in the post doesn't apply in 2020.",
      "plaintextDescription": "Note that this post is arguing that there are some specific epistemic advantages of working as a moderate, not that moderates are always correct or that there aren't epistemic disadvantages to being a moderate. I don't think \"there exist moderates which seem very incorrect to me\" is a valid response to the post similarly to how \"there exist radicals which seem very incorrect to me\" wouldn't be a valid argument for the post.\n\nThis is independent from the point Buck notes that the label moderate as defined in the post doesn't apply in 2020."
    },
    "baseScore": 23,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2025-08-29T15:46:04.045Z",
    "af": false
  },
  {
    "_id": "Qjjrtsk37G4FRNzjF",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": "9ZhJa92gYnbT4FBph",
    "topLevelCommentId": "LoqpxdbsbGz2NbjEK",
    "contents": {
      "markdown": "> Regarding GPT-4, I believe the estimate was that it has 1.8 trillion parameters, which if shared weights are used may not precisely correspond to connections or FLOPs.\n\nFor standard LLM architectures, forward pass FLOPs are $\\approx 2 \\cdot \\mathrm{parameters}$ (because of the multiply and accumulate for each matmul param). It could be that GPT-4 has some non-standard architecture where this is false, but I doubt it.\n\nSo, yeah we agree here, I was just noting that connection == FLOP (roughly).\n\n> What is your basis for this intuition? [...] what makes you think that the brain is likely to do the worse job of training its available weights, or that many synapses are \"mostly untrained\"?\n\nThe brain is purely local which makes training all the parameters efficiently much harder, my understanding is that in at least the vision focused part of the brain there is a bunch of use of randomly initialized filters, and I seem to recall some argument made somewhere (by Steven Byrnes?) that the effective number of parameters was much lower. Sorry I can't say more here.",
      "plaintextDescription": "> Regarding GPT-4, I believe the estimate was that it has 1.8 trillion parameters, which if shared weights are used may not precisely correspond to connections or FLOPs.\n\nFor standard LLM architectures, forward pass FLOPs are ≈2⋅parameters (because of the multiply and accumulate for each matmul param). It could be that GPT-4 has some non-standard architecture where this is false, but I doubt it.\n\nSo, yeah we agree here, I was just noting that connection == FLOP (roughly).\n\n> What is your basis for this intuition? [...] what makes you think that the brain is likely to do the worse job of training its available weights, or that many synapses are \"mostly untrained\"?\n\nThe brain is purely local which makes training all the parameters efficiently much harder, my understanding is that in at least the vision focused part of the brain there is a bunch of use of randomly initialized filters, and I seem to recall some argument made somewhere (by Steven Byrnes?) that the effective number of parameters was much lower. Sorry I can't say more here."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-28T17:57:00.546Z",
    "af": false
  },
  {
    "_id": "rvtbAunGpsjfkouDe",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": "jmDfbmthkyhCHvWd2",
    "topLevelCommentId": "wiNa6JfLEB4sFxFzC",
    "contents": {
      "markdown": "> It just suggests (to me) that they might be farther from that milestone than their performance on crystal-intelligence-friendly tasks would imply.\n\nI basically agree, but we can more directly attempt extrapolations (e.g. METR horizon length) and I put more weight on this.\n\nI also find it a bit silly when people say \"AIs are very good at competition programming, so surely they must soon be able to automate SWE\" (a thing I have seen at least some semi-prominent frontier AI company employees imply). That said, I think AIs being good at competitive programming is substantially not based on better cystalized intelligence and is instead based on this being easier to train for with RL and easier to scale up inference compute on.",
      "plaintextDescription": "> It just suggests (to me) that they might be farther from that milestone than their performance on crystal-intelligence-friendly tasks would imply.\n\nI basically agree, but we can more directly attempt extrapolations (e.g. METR horizon length) and I put more weight on this.\n\nI also find it a bit silly when people say \"AIs are very good at competition programming, so surely they must soon be able to automate SWE\" (a thing I have seen at least some semi-prominent frontier AI company employees imply). That said, I think AIs being good at competitive programming is substantially not based on better cystalized intelligence and is instead based on this being easier to train for with RL and easier to scale up inference compute on."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-28T16:28:24.932Z",
    "af": false
  },
  {
    "_id": "YPxQWJ8rgvafgg5JP",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": "5yT5JhdQhqddcbfv9",
    "topLevelCommentId": "4AckMBy3dgS6txZi8",
    "contents": {
      "markdown": "> Anyway, I think we may be agreeing on the main point here: my suggestion that LLMs solve FrontierMath problems \"the wrong way\", and your point about depth arguably being more important than breadth, seem to be pointing in the same direction.\n\nYep, though it's worth distinguishing between LLMs *often* solving FrontierMath problems the \"wrong way\" and *always* solving them the \"wrong way\". My understanding is that they don't always solve them the \"wrong way\" (at least for Tier 1/2 problems rather than Tier 3 problems), so you should (probably) be strictly more impressed than you would be if you only know that LLMs solved X% of problems the \"right way\".",
      "plaintextDescription": "> Anyway, I think we may be agreeing on the main point here: my suggestion that LLMs solve FrontierMath problems \"the wrong way\", and your point about depth arguably being more important than breadth, seem to be pointing in the same direction.\n\nYep, though it's worth distinguishing between LLMs often solving FrontierMath problems the \"wrong way\" and always solving them the \"wrong way\". My understanding is that they don't always solve them the \"wrong way\" (at least for Tier 1/2 problems rather than Tier 3 problems), so you should (probably) be strictly more impressed than you would be if you only know that LLMs solved X% of problems the \"right way\"."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-28T16:25:32.186Z",
    "af": false
  },
  {
    "_id": "W2HiDgz4abZXESqoW",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": "E4rfEPzneg7mdKx3c",
    "topLevelCommentId": "b9F9J3vxMrnHN574p",
    "contents": {
      "markdown": "> Say more? At https://arcprize.org/leaderboard, I see \"Stem Grad\" at 98% on ARC-AGI-1, and the highest listed AI score is 75.7% for \"o3-preview (Low)\". I vaguely recall seeing a higher reported figure somewhere for some AI model, but not 98%.\n\nBy \"can beat humans\", I mean AIs are well within the human range, probably somewhat better than the average/median human in the US at ARC-AGI-1. In [this study](https://arxiv.org/abs/2409.01374v1), humans get 65% right on the public evaluation set.\n\n> This post states that the \"average human\" scores 60% on ARC-AGI-2, though I was unable to verify the details (it claims to be a linkpost for an article which does not seem to contain that figure). Personally I tried 10-12 problems when the test was first launched, and IIRC I missed either 1 or 2.\n\nI'm skeptical, I bet mturkers do worse. This is very similar to the score that was found for humans for ARC-AGI-1 which is much easier from my understanding [this study](https://arxiv.org/abs/2409.01374v1).\n\nBy \"hard for humans\", I just mean that it takes substantially effort even for somewhat smart humans, I don't mean that humans can't do it.",
      "plaintextDescription": "> Say more? At https://arcprize.org/leaderboard, I see \"Stem Grad\" at 98% on ARC-AGI-1, and the highest listed AI score is 75.7% for \"o3-preview (Low)\". I vaguely recall seeing a higher reported figure somewhere for some AI model, but not 98%.\n\nBy \"can beat humans\", I mean AIs are well within the human range, probably somewhat better than the average/median human in the US at ARC-AGI-1. In this study, humans get 65% right on the public evaluation set.\n\n> This post states that the \"average human\" scores 60% on ARC-AGI-2, though I was unable to verify the details (it claims to be a linkpost for an article which does not seem to contain that figure). Personally I tried 10-12 problems when the test was first launched, and IIRC I missed either 1 or 2.\n\nI'm skeptical, I bet mturkers do worse. This is very similar to the score that was found for humans for ARC-AGI-1 which is much easier from my understanding this study.\n\nBy \"hard for humans\", I just mean that it takes substantially effort even for somewhat smart humans, I don't mean that humans can't do it."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-28T16:22:29.425Z",
    "af": false
  },
  {
    "_id": "qR6YjoGpMmxBsFD5t",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": "LoqpxdbsbGz2NbjEK",
    "topLevelCommentId": "LoqpxdbsbGz2NbjEK",
    "contents": {
      "markdown": "[@Steven Byrnes](https://www.lesswrong.com/users/steve2152?mention=user) might have takes on this.",
      "plaintextDescription": "@Steven Byrnes might have takes on this."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-27T21:33:29.377Z",
    "af": false
  },
  {
    "_id": "QMiMFMAKqgnSweiQG",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "In general, this post seems to make a bunch of claims that LLMs have specific large qualitative barriers relative to humans, but these claims seem mostly unsupported. The evidence as far as I can tell seems more consistent with LLMs being weaker in a bunch of specific quantitative ways which are improving over time. For instance, LLMs can totally do continuous learning or consolidate memory, it's just that the best methods for this work pretty poorly. (But plausibly are still within the human range for most/many relevant tasks.)",
      "plaintextDescription": "In general, this post seems to make a bunch of claims that LLMs have specific large qualitative barriers relative to humans, but these claims seem mostly unsupported. The evidence as far as I can tell seems more consistent with LLMs being weaker in a bunch of specific quantitative ways which are improving over time. For instance, LLMs can totally do continuous learning or consolidate memory, it's just that the best methods for this work pretty poorly. (But plausibly are still within the human range for most/many relevant tasks.)"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-27T21:22:20.538Z",
    "af": false
  },
  {
    "_id": "LoqpxdbsbGz2NbjEK",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> The best public estimate is that GPT-4 has 1.8 trillion “parameters”, meaning that its neural network has that many connections. In the two and a half years since it was released, it’s not clear that any larger models have been deployed (GPT-4.5 and Grok 3 might be somewhat larger).  \n>   \n> The human brain is far more complex than this; the most common estimate is 100 trillion connections, and each connection is probably considerably more complex than the connections in current neural networks. In other words, the brain has far more information storage capacity than any current artificial neural network.  \n>   \n> Which leads to the question: how the hell do LLMs manage to learn and remember so many more raw facts than a person[^crpxvjw6rz6]?\n\nI don't have any expertise in neuroscience, but I think this is somewhat confused:\n\n*   Probably the effective number of parameters in the human brain is actually lower than 100 trillion because many of these \"parameters\" are basically randomly initialized or mostly untrained. (Or are trained very slowly/weakly.) The brain can't use a global learning algorithm, so it might effectively use parameters much less efficiently.\n*   It's a bit confusing to describe GPT-4 as having 1.8 trillion connections as 1.8 trillion is the number of floating point operations (roughly) not the number of neurons. In general, the analogy between the human brain and LLMs is messy because a single brain neuron probably has far fewer learned parameters than a LLM neuron, but plausibly somewhat more than a single floating point number.",
      "plaintextDescription": "> The best public estimate is that GPT-4 has 1.8 trillion “parameters”, meaning that its neural network has that many connections. In the two and a half years since it was released, it’s not clear that any larger models have been deployed (GPT-4.5 and Grok 3 might be somewhat larger).\n> \n> The human brain is far more complex than this; the most common estimate is 100 trillion connections, and each connection is probably considerably more complex than the connections in current neural networks. In other words, the brain has far more information storage capacity than any current artificial neural network.\n> \n> Which leads to the question: how the hell do LLMs manage to learn and remember so many more raw facts than a person[4]?\n\nI don't have any expertise in neuroscience, but I think this is somewhat confused:\n\n * Probably the effective number of parameters in the human brain is actually lower than 100 trillion because many of these \"parameters\" are basically randomly initialized or mostly untrained. (Or are trained very slowly/weakly.) The brain can't use a global learning algorithm, so it might effectively use parameters much less efficiently.\n * It's a bit confusing to describe GPT-4 as having 1.8 trillion connections as 1.8 trillion is the number of floating point operations (roughly) not the number of neurons. In general, the analogy between the human brain and LLMs is messy because a single brain neuron probably has far fewer learned parameters than a LLM neuron, but plausibly somewhat more than a single floating point number."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-27T21:19:36.613Z",
    "af": false
  },
  {
    "_id": "wiNa6JfLEB4sFxFzC",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> To put it another way: compared to people, large language models seem to be superhuman in crystallized knowledge, which seems to be masking shortcomings in fluid intelligence. Is that a dead end, great for benchmarks but bad for a lot of work in the real world?\n\nYou seem to imply that AIs aren't improving on fluid intelligence. Why do you think this? I'd guess that AIs will just eventually have sufficiently high fluid intelligence while still compensating with (very) superhuman crystallized knowledge (like an older experienced human professional).\n\nIf fluid intelligence wasn't improving, this would be a dead end, but if there is some pipeline which is improving fluid intelligence (quickly), then I don't see a particular reasons to think that high crystallized knowledge is a reason for *discounting* AI.",
      "plaintextDescription": "> To put it another way: compared to people, large language models seem to be superhuman in crystallized knowledge, which seems to be masking shortcomings in fluid intelligence. Is that a dead end, great for benchmarks but bad for a lot of work in the real world?\n\nYou seem to imply that AIs aren't improving on fluid intelligence. Why do you think this? I'd guess that AIs will just eventually have sufficiently high fluid intelligence while still compensating with (very) superhuman crystallized knowledge (like an older experienced human professional).\n\nIf fluid intelligence wasn't improving, this would be a dead end, but if there is some pipeline which is improving fluid intelligence (quickly), then I don't see a particular reasons to think that high crystallized knowledge is a reason for discounting AI."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-27T21:08:14.221Z",
    "af": false
  },
  {
    "_id": "4AckMBy3dgS6txZi8",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> AIs have been demonstrating what arguably constitutes superhuman performance on FrontierMath, a set of extremely difficult mathematical problems.\n\nAIs aren't superhuman on frontier math. I'd guess that Terry Tao with 8 hours per problem (and internet access) is much better than current AIs. (Especially after practicing on some of the problems etc.)\n\nAt a more basic level, this superhumanness would substantially be achieved by broadness/generality rather than by being superhuman within some field (which is arguably less important/impactful). Like, if you compared AIs to a group of humans who are pretty good at this type of math, the humans would probably also destroy the AI.",
      "plaintextDescription": "> AIs have been demonstrating what arguably constitutes superhuman performance on FrontierMath, a set of extremely difficult mathematical problems.\n\nAIs aren't superhuman on frontier math. I'd guess that Terry Tao with 8 hours per problem (and internet access) is much better than current AIs. (Especially after practicing on some of the problems etc.)\n\nAt a more basic level, this superhumanness would substantially be achieved by broadness/generality rather than by being superhuman within some field (which is arguably less important/impactful). Like, if you compared AIs to a group of humans who are pretty good at this type of math, the humans would probably also destroy the AI."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-27T21:02:44.491Z",
    "af": false
  },
  {
    "_id": "G8CwrsqbvJzv8EDiL",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Is sample-efficient learning a singularly important step on the path to AGI?\n\nAlmost definitionally, learning as efficiently as top humans would suffice for AGI. (You could just train the AI on way more data/compute and it would be superhuman.)\n\nAIs will probably reach milestones like full automation of AI R&D before matching top human sample efficiency in broad generality (though they might be better in some/many cases).",
      "plaintextDescription": "> Is sample-efficient learning a singularly important step on the path to AGI?\n\nAlmost definitionally, learning as efficiently as top humans would suffice for AGI. (You could just train the AI on way more data/compute and it would be superhuman.)\n\nAIs will probably reach milestones like full automation of AI R&D before matching top human sample efficiency in broad generality (though they might be better in some/many cases)."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-27T20:59:07.376Z",
    "af": false
  },
  {
    "_id": "b9F9J3vxMrnHN574p",
    "postId": "uAbbEz4p6tcsENaRz",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> But if this were true, you’d think they’d be able to handle ARC-AGI puzzles (see the example image just above)\n\nIn a footnote you note that models do well on ARC-AGI-1, but I think you're description of the situation is misleading:\n\n*   AIs trained on the training set of ARC-AGI (and given a bunch of compute) can *beat* humans on ARC-AGI-1.\n*   The example puzzle you show for ARC-AGI is one of the easiest puzzles; AIs have been able to succeed at this for a while. The ones AIs get wrong are typically ones which are very large (causing difficulties with perception) or which are actually hard for humans.\n*   ARC-AGI-2 isn't easy for humans. It's hard for humans and AIs probably do similarly to random humans (e.g. mturkers) given a limited period.\n*   ARC-AGI-3 is much more \"perception\" loaded than ARC-AGI-2/ARC-AGI-1 due to being structured as a video game (which implicitly means you have to process and understand a long series of frames). I expect this is why AIs struggle.\n\nOverall, I think LLMs *do* handle ARC-AGI puzzles. They are well within the human range for ARC-AGI-1/2 and their failures are pretty often perception failures.\n\nFair enough if your objection is that the level of sample efficiency on this type of task for typical humans isn't sufficient. (I agree.)\n\n> Maybe they’re only good at picking up ideas from an example, *if they’d already learned that idea during their original training?* In other words, maybe in-context learning is helpful at jogging their memory, but not for teaching new concepts.\n\nMy view is that LLMs are generally qualitatively dumber than the most capable humans in a bunch of ways (including ability to learn new things), but that this is improving over time. Thiere isn't some dictomy between \"sample efficient learning\" and not. I think you'll struggle to find tasks where AIs haven't been improving by following the heuristic \"what haven't AIs already learned\" (though AIs do gain an advantage by knowing lots of stuff, they are also improving at all kinds of stuff).",
      "plaintextDescription": "> But if this were true, you’d think they’d be able to handle ARC-AGI puzzles (see the example image just above)\n\nIn a footnote you note that models do well on ARC-AGI-1, but I think you're description of the situation is misleading:\n\n * AIs trained on the training set of ARC-AGI (and given a bunch of compute) can beat humans on ARC-AGI-1.\n * The example puzzle you show for ARC-AGI is one of the easiest puzzles; AIs have been able to succeed at this for a while. The ones AIs get wrong are typically ones which are very large (causing difficulties with perception) or which are actually hard for humans.\n * ARC-AGI-2 isn't easy for humans. It's hard for humans and AIs probably do similarly to random humans (e.g. mturkers) given a limited period.\n * ARC-AGI-3 is much more \"perception\" loaded than ARC-AGI-2/ARC-AGI-1 due to being structured as a video game (which implicitly means you have to process and understand a long series of frames). I expect this is why AIs struggle.\n\nOverall, I think LLMs do handle ARC-AGI puzzles. They are well within the human range for ARC-AGI-1/2 and their failures are pretty often perception failures.\n\nFair enough if your objection is that the level of sample efficiency on this type of task for typical humans isn't sufficient. (I agree.)\n\n> Maybe they’re only good at picking up ideas from an example, if they’d already learned that idea during their original training? In other words, maybe in-context learning is helpful at jogging their memory, but not for teaching new concepts.\n\nMy view is that LLMs are generally qualitatively dumber than the most capable humans in a bunch of ways (including ability to learn new things), but that this is improving over time. Thiere isn't some dictomy between \"sample efficient learning\" and not. I think you'll struggle to find tasks where AIs haven't been improving by following the heuristic \"what haven't AIs already learned\" (though AIs do gain an advantage by knowing lots of stuff, they are also improving at"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-27T20:56:39.806Z",
    "af": false
  },
  {
    "_id": "uEjpFo9HZWmXfqH52",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "5xoynkgbr6PSLeeAc",
    "topLevelCommentId": "bRToWiJyoMgcpfvmw",
    "contents": {
      "markdown": "Relevant metrics of performance are roughly linear in log-compute when compute is utilized effectively in the current paradigm for training frontier models.\n\nFrom my perspective it looks like performance has been steadily advancing as you scale up compute and other resources.\n\n(This isn't to say that pretraining hasn't had lower returns recently, but you made a stronger claim.)",
      "plaintextDescription": "Relevant metrics of performance are roughly linear in log-compute when compute is utilized effectively in the current paradigm for training frontier models.\n\nFrom my perspective it looks like performance has been steadily advancing as you scale up compute and other resources.\n\n(This isn't to say that pretraining hasn't had lower returns recently, but you made a stronger claim.)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-24T23:40:00.475Z",
    "af": false
  },
  {
    "_id": "AibCcFWtvx8ezxecM",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "hTZFkH3iLfQBNkK8x",
    "topLevelCommentId": "bRToWiJyoMgcpfvmw",
    "contents": {
      "markdown": "And [the whole exchange on nitter for those who don't like going on x/twitter](https://nitter.net/ESYudkowsky/status/1959641016388227294#m).",
      "plaintextDescription": "And the whole exchange on nitter for those who don't like going on x/twitter."
    },
    "baseScore": 15,
    "voteCount": 14,
    "createdAt": null,
    "postedAt": "2025-08-24T19:03:08.257Z",
    "af": true
  },
  {
    "_id": "pESut55iNurmTnhPT",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "irkJ5kbSqf3mb7Dey",
    "topLevelCommentId": "bRToWiJyoMgcpfvmw",
    "contents": {
      "markdown": "You seem to think that imitation resulted in LLMs quickly saturating on an S-curve, but relevant metrics (e.g. [time-horizon](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) seem like they smoothly advance without a clear reduction in slope from the regime where pretraining was rapidly being scaled up (e.g. up to and through GPT-4) to after (in fact, the slope seems somewhat higher).\n\nPresumably you think some qualitative notion of intelligence (which is hard to measure) has slowed down?\n\nMy view is that basically everything is progressing relatively smoothly and there isn't anything which is clearly stalled in a robust way.",
      "plaintextDescription": "You seem to think that imitation resulted in LLMs quickly saturating on an S-curve, but relevant metrics (e.g. time-horizon seem like they smoothly advance without a clear reduction in slope from the regime where pretraining was rapidly being scaled up (e.g. up to and through GPT-4) to after (in fact, the slope seems somewhat higher).\n\nPresumably you think some qualitative notion of intelligence (which is hard to measure) has slowed down?\n\nMy view is that basically everything is progressing relatively smoothly and there isn't anything which is clearly stalled in a robust way."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-24T19:01:52.131Z",
    "af": false
  },
  {
    "_id": "um6HoKhompjshamvZ",
    "postId": "6eguLiC2QP399GrQE",
    "parentCommentId": "XHJvoWCCunapuYHoY",
    "topLevelCommentId": "p3vwj8efnK6uAZpA5",
    "contents": {
      "markdown": "More generally, I recommend using the reduce or hidden feature on tags you don't like. I have:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3e33ab26c60107c7938c39086e176ab1078b80ca9de8e85c.png)",
      "plaintextDescription": "More generally, I recommend using the reduce or hidden feature on tags you don't like. I have:"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-23T20:59:32.411Z",
    "af": false
  },
  {
    "_id": "XHJvoWCCunapuYHoY",
    "postId": "6eguLiC2QP399GrQE",
    "parentCommentId": "p3vwj8efnK6uAZpA5",
    "topLevelCommentId": "p3vwj8efnK6uAZpA5",
    "contents": {
      "markdown": "You can filter for things matching the \"AI\" tag. This will include a bunch of posts by people who aren't on the alignment forum and thus can't cross post there, but I don't think there is a better way to filter.\n\nAnother option would be to browse on the alignment forum and then add a browser short cut for editing the url to be lesswrong.com. So, you'd open the post then use the shortcut.",
      "plaintextDescription": "You can filter for things matching the \"AI\" tag. This will include a bunch of posts by people who aren't on the alignment forum and thus can't cross post there, but I don't think there is a better way to filter.\n\nAnother option would be to browse on the alignment forum and then add a browser short cut for editing the url to be lesswrong.com. So, you'd open the post then use the shortcut."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-23T20:55:18.689Z",
    "af": false
  },
  {
    "_id": "TFKqBrmkc6NqyXHP3",
    "postId": "6eguLiC2QP399GrQE",
    "parentCommentId": "p3vwj8efnK6uAZpA5",
    "topLevelCommentId": "p3vwj8efnK6uAZpA5",
    "contents": {
      "markdown": "Related: in my ideal world there would be a wrapper version of LessWrong which is like the alignment forum (just focused on transformative AI) but where anyone can post. By default, I'd probably end up recommending people interested in AI go to this because the other content on lesswrong isn't relevant to them.\n\nOne proposal for this:\n\n- Use a separate url (e.g. aiforum.com or you could give up on the alignment forum as is and use that existing url).\n- This is a shallow wrapper on LW in the same way the alignment forum is, but anyone can post.\n- All posts tagged with AI are crossposted (and can maybe be de-crossposted by a moderator if it's not actually relevant). (And if you post using the separate url, it automatically is also on LW and is always tagged with AI.)\n- Maybe you add some mechanism for tagging quick takes or manually cross posting them (similar to how you can cross post quick takes to alignment forum now).\n- Ideally the home page of the website default to having some more visual emphasis on key research as well as key explanations/intros rather than as much focus on latest events.",
      "plaintextDescription": "Related: in my ideal world there would be a wrapper version of LessWrong which is like the alignment forum (just focused on transformative AI) but where anyone can post. By default, I'd probably end up recommending people interested in AI go to this because the other content on lesswrong isn't relevant to them.\n\nOne proposal for this:\n\n * Use a separate url (e.g. aiforum.com or you could give up on the alignment forum as is and use that existing url).\n * This is a shallow wrapper on LW in the same way the alignment forum is, but anyone can post.\n * All posts tagged with AI are crossposted (and can maybe be de-crossposted by a moderator if it's not actually relevant). (And if you post using the separate url, it automatically is also on LW and is always tagged with AI.)\n * Maybe you add some mechanism for tagging quick takes or manually cross posting them (similar to how you can cross post quick takes to alignment forum now).\n * Ideally the home page of the website default to having some more visual emphasis on key research as well as key explanations/intros rather than as much focus on latest events."
    },
    "baseScore": 30,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2025-08-23T20:53:29.890Z",
    "af": false
  },
  {
    "_id": "H4HLm9LpvXzJiJSAg",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "24xdXjXxMyQvsbfG5",
    "topLevelCommentId": "9yr7HwYzeeW8Nqq2N",
    "contents": {
      "markdown": "To be clear, Buck's view is that it is a very bad outcome if a token population is kept alive (e.g., all/most currently alive humans) but (misaligned) AIs control the vast majority of resources. And, he thinks most of the badness is due to the loss of the vast majority of resources.\n\nHe didn't say \"and this would be fine\" or \"and I'm enthusiastic about this outcome\", he was just making a local validity point and saying you weren't effectively addressing the comment you were responding to.\n\n(I basically agree with the missing mood point, if I was writing the same comment Buck wrote, I would have more explicitly noted the loss of value and my agreements.)",
      "plaintextDescription": "To be clear, Buck's view is that it is a very bad outcome if a token population is kept alive (e.g., all/most currently alive humans) but (misaligned) AIs control the vast majority of resources. And, he thinks most of the badness is due to the loss of the vast majority of resources.\n\nHe didn't say \"and this would be fine\" or \"and I'm enthusiastic about this outcome\", he was just making a local validity point and saying you weren't effectively addressing the comment you were responding to.\n\n(I basically agree with the missing mood point, if I was writing the same comment Buck wrote, I would have more explicitly noted the loss of value and my agreements.)"
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-21T15:35:24.096Z",
    "af": false
  },
  {
    "_id": "EYjZ6GWzBPfhjAbnr",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "EtQyzKcKDfyizX6zq",
    "topLevelCommentId": "9yr7HwYzeeW8Nqq2N",
    "contents": {
      "markdown": "> Don't we feel gratitude and warmth and empathy and care-for-the-monkey's-values such that we're willing to make small sacrifices on their behalf?\n\nPeople do make small sacrifices on behalf of monkeys? Like >1 / billion of human resources are spent on doing things for monkeys (this is just >$100k per year). And, in the case of AI takeover, 1 / billion could easily suffice to avoid literal human extinction (with some chance of avoiding mass fatalities due to AI takeover). This isn't to say that after AI takeover humans would have much control over the future or that the situation wouldn't be very bad on my views (or on the views of most people at least on reflection). Like, even if some (or most/all) humans survive it's still an x-risk if we lose control over the longer run future.\n\nLike I agree with the claim that people care very little about the interests of monkeys and don't let them slow them down in the slightest. But, the exact amount of caring humans exhibit probably would suffice for avoiding literal extinction in the case of AIs.\n\nI think your response is \"sure, but AIs won't care at all\":\n\n> You have to be careful with the metaphor, because it can lead people to erroneously assuming that an AI would be at least that nice, which is not at all obvious or likely for various reasons (that you can read about in the book when it comes out in September!). \n\nAgree that it's not obvious and I think I tenatively expect AIs that takeover are less \"nice\" in this way than humans are. But, I think it's pretty likely (40%?) they are \"nice\" enough to care about humans some tiny amount that suffices for avoiding extinction (while also not having specific desires about what to do with humans that interfere with this) and there is also the possibility of (acausal) trade resulting in human survival. In aggregate, I think these make extinction less likely than not. (But don't mean that the value of the future isn't (mostly) lost.)",
      "plaintextDescription": "> Don't we feel gratitude and warmth and empathy and care-for-the-monkey's-values such that we're willing to make small sacrifices on their behalf?\n\nPeople do make small sacrifices on behalf of monkeys? Like >1 / billion of human resources are spent on doing things for monkeys (this is just >$100k per year). And, in the case of AI takeover, 1 / billion could easily suffice to avoid literal human extinction (with some chance of avoiding mass fatalities due to AI takeover). This isn't to say that after AI takeover humans would have much control over the future or that the situation wouldn't be very bad on my views (or on the views of most people at least on reflection). Like, even if some (or most/all) humans survive it's still an x-risk if we lose control over the longer run future.\n\nLike I agree with the claim that people care very little about the interests of monkeys and don't let them slow them down in the slightest. But, the exact amount of caring humans exhibit probably would suffice for avoiding literal extinction in the case of AIs.\n\nI think your response is \"sure, but AIs won't care at all\":\n\n> You have to be careful with the metaphor, because it can lead people to erroneously assuming that an AI would be at least that nice, which is not at all obvious or likely for various reasons (that you can read about in the book when it comes out in September!).\n\nAgree that it's not obvious and I think I tenatively expect AIs that takeover are less \"nice\" in this way than humans are. But, I think it's pretty likely (40%?) they are \"nice\" enough to care about humans some tiny amount that suffices for avoiding extinction (while also not having specific desires about what to do with humans that interfere with this) and there is also the possibility of (acausal) trade resulting in human survival. In aggregate, I think these make extinction less likely than not. (But don't mean that the value of the future isn't (mostly) lost.)"
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-21T15:32:28.021Z",
    "af": false
  },
  {
    "_id": "gu99bhTJec9YrkZpk",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "gFQ9L2z5kmjXhFbqE",
    "topLevelCommentId": "9yr7HwYzeeW8Nqq2N",
    "contents": {
      "markdown": "> This rhymes with what Paul Christiano and his various interlocutors (e.g. Buck and Ryan above) think, but I think you've put forward a much weaker version of it than they do.\n\nTo be clear, I'm not at all expecting ASI to \"self-align\", \"develop values that are benevolent towards us\", or to pursue \"cooperation, appeasement and general benevolence\".\n\n(I think you understand my view, after all, you just said \"rhyme\", not agree. Regardless, clarifying here.)\n\nWhat I think is:\n\n- Misaligned AIs which takeover probably won't cause literal human extinction (though large numbers of people might die in the takeover and literal extinction is totally plausible). This takeover would still be extremely bad in expectation for currently living humans and very bad (in my views) from a longtermist perspective (as in, bad for the long run future and acausal interactions).\n- We might be able to make [trades/deals with earlier misaligned AIs](https://www.lesswrong.com/posts/psqkwsKrKHCfkhrQx/making-deals-with-early-schemers) that are pretty helpful (and good for both us and AIs).\n- If the first ASI is misaligned with arbitrary ambitious aims, AI takeover is likely (at least if there aren't reasonably competitive AIs which are pretty aligned).",
      "plaintextDescription": "> This rhymes with what Paul Christiano and his various interlocutors (e.g. Buck and Ryan above) think, but I think you've put forward a much weaker version of it than they do.\n\nTo be clear, I'm not at all expecting ASI to \"self-align\", \"develop values that are benevolent towards us\", or to pursue \"cooperation, appeasement and general benevolence\".\n\n(I think you understand my view, after all, you just said \"rhyme\", not agree. Regardless, clarifying here.)\n\nWhat I think is:\n\n * Misaligned AIs which takeover probably won't cause literal human extinction (though large numbers of people might die in the takeover and literal extinction is totally plausible). This takeover would still be extremely bad in expectation for currently living humans and very bad (in my views) from a longtermist perspective (as in, bad for the long run future and acausal interactions).\n * We might be able to make trades/deals with earlier misaligned AIs that are pretty helpful (and good for both us and AIs).\n * If the first ASI is misaligned with arbitrary ambitious aims, AI takeover is likely (at least if there aren't reasonably competitive AIs which are pretty aligned)."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-21T15:11:00.333Z",
    "af": false
  },
  {
    "_id": "4499W7vbZEcip8sq6",
    "postId": "qBbnXtt9zFWXMrP4M",
    "parentCommentId": "P2BnANj8jtk8bJiYG",
    "topLevelCommentId": "P2BnANj8jtk8bJiYG",
    "contents": {
      "markdown": "From my perspective, the main case for working on safeguards is transfer to misalignment risk. (Both in terms of transferring of the methods and the policy/politics that this work maybe helps with transferring.)\n\nI don't have a strong view about how much marginal work on the safeguards team transfers to misalignment risk, but I think the case for substantial transfer (e.g. comparable to that of working on the alignment science team doing somewhat thoughtful work) is reasonable, especially if the person is explicitly focused on transferring to misalignment or is working on developing directly misalignment relevant safeguards.\n\nBy default, I'd guess that people interested in mitigating misalignment risk should by default (as in, in the absence of a strong comparative adavantage or particularly exciting relevant project) prefer working on the alignment science team at Anthropic (just comparing options at Anthropic) or something else that is more directly targeted at misalignment risk.",
      "plaintextDescription": "From my perspective, the main case for working on safeguards is transfer to misalignment risk. (Both in terms of transferring of the methods and the policy/politics that this work maybe helps with transferring.)\n\nI don't have a strong view about how much marginal work on the safeguards team transfers to misalignment risk, but I think the case for substantial transfer (e.g. comparable to that of working on the alignment science team doing somewhat thoughtful work) is reasonable, especially if the person is explicitly focused on transferring to misalignment or is working on developing directly misalignment relevant safeguards.\n\nBy default, I'd guess that people interested in mitigating misalignment risk should by default (as in, in the absence of a strong comparative adavantage or particularly exciting relevant project) prefer working on the alignment science team at Anthropic (just comparing options at Anthropic) or something else that is more directly targeted at misalignment risk."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-21T04:48:39.756Z",
    "af": false
  },
  {
    "_id": "hGoSqv93wnCk3YHGg",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "6ue8BPWrcoa2eGJdP",
    "topLevelCommentId": "6ue8BPWrcoa2eGJdP",
    "contents": {
      "markdown": "I wrote an update [here](https://www.lesswrong.com/posts/2ssPfDpdrjaM2rMbn/my-agi-timeline-updates-from-gpt-5-and-2025-so-far-1).",
      "plaintextDescription": "I wrote an update here."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-20T16:15:25.517Z",
    "af": true
  },
  {
    "_id": "yT76QcTwxwj8GBALu",
    "postId": "RnKmRusmFpw7MhPYw",
    "parentCommentId": "B3bM9xh7wBtLyHAYF",
    "topLevelCommentId": "iAwEZCk5RWi5jKwjb",
    "contents": {
      "markdown": "METR [found that](https://metr.org/blog/2025-08-12-research-update-towards-reconciling-slowdown-with-time-horizons/) 0/4 PRs which passed test cases and they reviewed were also acceptable to review. This was for 3.7 sonnet on large open source repos with default infrastructure.\n\nThe rate at which PRs passed test cases was also low, but if you're focusing on the PR being viable to merge conditional on passing test cases, the \"0/4\" number is what you want. (And this is consistent with 10% or some chance of 35% of PRs being mergable conditional on passing test cases, we don't have a very large sample size here.)\n\nI don't think this is much evidence that AI can't sometimes write acceptable PRs in general and there examples of AIs doing this. On small projects I've worked on, AIs from a long time ago have written a big chunk of code ~zero-shot. Anecdotally, I've heard of people having success with AIs completing tasks zero-shot. I don't know what you mean by \"PR\" that doesn't include this.",
      "plaintextDescription": "METR found that 0/4 PRs which passed test cases and they reviewed were also acceptable to review. This was for 3.7 sonnet on large open source repos with default infrastructure.\n\nThe rate at which PRs passed test cases was also low, but if you're focusing on the PR being viable to merge conditional on passing test cases, the \"0/4\" number is what you want. (And this is consistent with 10% or some chance of 35% of PRs being mergable conditional on passing test cases, we don't have a very large sample size here.)\n\nI don't think this is much evidence that AI can't sometimes write acceptable PRs in general and there examples of AIs doing this. On small projects I've worked on, AIs from a long time ago have written a big chunk of code ~zero-shot. Anecdotally, I've heard of people having success with AIs completing tasks zero-shot. I don't know what you mean by \"PR\" that doesn't include this."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-19T20:57:09.867Z",
    "af": false
  },
  {
    "_id": "3hnDrKT3qwKwuyy4i",
    "postId": "RnKmRusmFpw7MhPYw",
    "parentCommentId": "B4hrtjPY5vTbPAuSA",
    "topLevelCommentId": "iAwEZCk5RWi5jKwjb",
    "contents": {
      "markdown": "> Aside: On my model, LLMs are not on track to hit any walls. They will keep getting better at the things they've been getting better at, at the same pace, for as long as the inputs to the process (compute, data, data progress, algorithmic progress) keep scaling at the same rate. My expectation is instead that they're just not going towards AGI, so \"no walls in their way\" doesn't matter; and that they will run out of fuel before the cargo cult of them becomes Singularity-tier transformative.\n\nOk, but surely there has to be something they aren't getting better at (or are getting better at too slowly). Under your model they have to hit a wall in this sense.\n\nI think [your main view](https://www.lesswrong.com/posts/oKAFFvaouKKEhbBPm/a-bear-case-my-predictions-regarding-ai-progress) is that LLMs won't ever complete actually hard tasks and current benchmarks just aren't measuring actually hard tasks or have other measurement issues? This seems inconsistent with saying they'll just keep getting better though unless your hypothesizing truely insane benchmark flaws right?\n\nLike, if they stop improving at <1 month horizon lengths (as you say immediately above the text I quoted) that is clearly a case of LLMs hitting a wall right? I agree that compute and resources running out could cause this, but it's notable that we expect ~1 month in not that long, like only ~3 years at the current rate. ",
      "plaintextDescription": "> Aside: On my model, LLMs are not on track to hit any walls. They will keep getting better at the things they've been getting better at, at the same pace, for as long as the inputs to the process (compute, data, data progress, algorithmic progress) keep scaling at the same rate. My expectation is instead that they're just not going towards AGI, so \"no walls in their way\" doesn't matter; and that they will run out of fuel before the cargo cult of them becomes Singularity-tier transformative.\n\nOk, but surely there has to be something they aren't getting better at (or are getting better at too slowly). Under your model they have to hit a wall in this sense.\n\nI think your main view is that LLMs won't ever complete actually hard tasks and current benchmarks just aren't measuring actually hard tasks or have other measurement issues? This seems inconsistent with saying they'll just keep getting better though unless your hypothesizing truely insane benchmark flaws right?\n\nLike, if they stop improving at <1 month horizon lengths (as you say immediately above the text I quoted) that is clearly a case of LLMs hitting a wall right? I agree that compute and resources running out could cause this, but it's notable that we expect ~1 month in not that long, like only ~3 years at the current rate."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-19T17:18:01.707Z",
    "af": false
  },
  {
    "_id": "vQnayNPcDTd9Pv4kN",
    "postId": "RnKmRusmFpw7MhPYw",
    "parentCommentId": "eidho3DxPzQN8B5vN",
    "topLevelCommentId": "iAwEZCk5RWi5jKwjb",
    "contents": {
      "markdown": "> I think if this were right LLMs would already be useful for software engineering and able to make acceptable PRs.\n\nI think LLMs can be useful for software engineering and can sometimes write acceptable PRs. (I've very clearly seen both of these first hand.) Maybe you meant something slightly weaker, like \"AIs would be able to write acceptable PRs at a rate of >1/10 on large open source repos\"? I think this is already probably true, at least with some scaffolding and inference time compute. Note that METR's recent results were on 3.7 sonnet.",
      "plaintextDescription": "> I think if this were right LLMs would already be useful for software engineering and able to make acceptable PRs.\n\nI think LLMs can be useful for software engineering and can sometimes write acceptable PRs. (I've very clearly seen both of these first hand.) Maybe you meant something slightly weaker, like \"AIs would be able to write acceptable PRs at a rate of >1/10 on large open source repos\"? I think this is already probably true, at least with some scaffolding and inference time compute. Note that METR's recent results were on 3.7 sonnet."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-19T17:08:45.490Z",
    "af": false
  },
  {
    "_id": "krMWsHLYuAPG3A7Dr",
    "postId": "mXa66dPR8hmHgndP5",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Are you looking at mean squared error in log space? I expect this will be more meaningful and less dominated by the last few points.",
      "plaintextDescription": "Are you looking at mean squared error in log space? I expect this will be more meaningful and less dominated by the last few points."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-19T13:25:31.275Z",
    "af": false
  },
  {
    "_id": "iXpamMBCtE8JRgRFe",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "kpptyWTFrfx3Re4dh",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "Ok, sound it sounds like your view is \"indeed if we got ~totally aligned AIs capable of fully automating safety work (but not notably more capable than the bare minimum requirement for this), we'd probably fine (even if there is still a small fraction of effort spent on safety) and the crux is earlier than this\".\n\nIs this right? If so, it seems notable if the problem can be mostly reduced to sufficiently aligning (still very capable) human-ish level AIs and handing off to these systems (which don't have the scariest properties of an ASI from an alignment perspective).\n\n---\n\n> I think your position is that techniques which are likely to descend from current techniques could work well-enough for roughly human-level systems, and that's where I encounter this sense of circularity.\n\nI'd say my position is more like:\n\n- **Scheming might just not happen**: It's basically a toss up whether systems at this level of capability would end up scheming \"by default\" (as in, without active effort researching preventing scheming and just work motivated by commercial utility along the way). Maybe I'm at ~40% scheming for such systems, though [the details alter my view a lot](https://www.lesswrong.com/posts/aEguDPoCzt3287CCD/how-will-we-update-about-scheming).\n- **The rest of the problem if we assume no scheming doesn't obviously seem that hard**: It's unclear how hard it will be to make non-scheming AIs of the capability level discussed above be sufficiently aligned for the strong sense of alignment I discussed above. I think it's unlikely that the default course gets us there, but it seems pretty plausible to me that modest effort along the way does. It just requires some favorable generalization of the sort that doesn't seem that surprising and we'll have some AI labor along the way to help. And, for this part of the problem, we totally can get multiple tries and study things pretty directly with empiricism using behavioral tests (though we're still depending on some cleverness and transfer as we can't directly verify the things we ultimately want the AI to do).\n- **Further prosaic effort seems helpful for both avoiding scheming and the rest of the problem**: I don't see strong arguments for thinking that at the level of capability we're discussing scheming will be intractable to prosaic methods or experimentation. I can see why this might happen and I can certainly imagine worlds where no on really tries. Similarly, I don't see a strong argument for further effort on relatively straightforward methods can't help a bunch in getting you sufficiently aligned systems (supposing they aren't scheming): we can measure what we want somewhat well with a bunch of effort and I can imagine many things which could make a pretty big difference (again, this isn't to say that this effort will happen in practice).\n\nThis isn't to say that I can't imagine worlds where pretty high effort and well orchestrated prosaic iteration totally fails. This seems totally plausible, especially given how fast this might happen, so risks seem high. And, it's easy for me to imagine ways the world could be such that relatively prosaic methods and iteration is ~doomed without much more time than we can plausibly hope for, it's just that these seem somewhat unlikely in aggregate to me.\n\nSo, I'd be pretty skeptical of someone claiming that the risk of this type of approach would be <3% (without at the very least preserving the optionality for a long pause during takeoff depending on empirical evidence), but I don't see a case for thinking \"it would be very surprising or wild if prosaic iteration sufficed\".",
      "plaintextDescription": "Ok, sound it sounds like your view is \"indeed if we got ~totally aligned AIs capable of fully automating safety work (but not notably more capable than the bare minimum requirement for this), we'd probably fine (even if there is still a small fraction of effort spent on safety) and the crux is earlier than this\".\n\nIs this right? If so, it seems notable if the problem can be mostly reduced to sufficiently aligning (still very capable) human-ish level AIs and handing off to these systems (which don't have the scariest properties of an ASI from an alignment perspective).\n\n----------------------------------------\n\n> I think your position is that techniques which are likely to descend from current techniques could work well-enough for roughly human-level systems, and that's where I encounter this sense of circularity.\n\nI'd say my position is more like:\n\n * Scheming might just not happen: It's basically a toss up whether systems at this level of capability would end up scheming \"by default\" (as in, without active effort researching preventing scheming and just work motivated by commercial utility along the way). Maybe I'm at ~40% scheming for such systems, though the details alter my view a lot.\n * The rest of the problem if we assume no scheming doesn't obviously seem that hard: It's unclear how hard it will be to make non-scheming AIs of the capability level discussed above be sufficiently aligned for the strong sense of alignment I discussed above. I think it's unlikely that the default course gets us there, but it seems pretty plausible to me that modest effort along the way does. It just requires some favorable generalization of the sort that doesn't seem that surprising and we'll have some AI labor along the way to help. And, for this part of the problem, we totally can get multiple tries and study things pretty directly with empiricism using behavioral tests (though we're still depending on some cleverness and transfer as we can't directly verify the things we ulti"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-14T01:04:28.323Z",
    "af": false
  },
  {
    "_id": "rL6DT6SNZhj6yP2PK",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "dowuyzxhvSfcrjHoa",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "Suppose that we ended up with AIs that were ~perfectly aligned (to what the company/project that trained these AIs wanted) which had capabilities that dominate top human experts in AI R&D, AI safety research, philosophy, and most digital research fields in general. These AIs are sufficiently aligned that they try hard to do the right thing even on hard-to-check, conceptual, and open-ended tasks. Let's say these AIs aren't much more capable than needed for this capability bar (and the capability profile works out such that these AIs aren't qualitatively wildly superhuman as seems likely to me).\n\nThese AIs are sufficiently aligned and capable that they dominate humans at doing safety work and have better epistemics than groups of human experts.\n\nI think the situation would probably be fine if the AI company tasked these AIs with proceeding with AI development with 20% of resources spent on alignment/safety of future models and 80% on advancing capabilities.\n\nI claim that either you have to think that takeoff is very fast or that alignment is very difficult to think things are likely to go wrong given this situation:\n\n- At the point of handoff, you maybe have ~3 million AI labor years / year and each instance maybe runs at effectively 50x speed. (Obviously the error bars here are very wide.) 20% goes to alignment.\n- So, holding this level of capability fixed, in 2 months, you get the equivalent of ~8 years of work from 10k people at the level of top human experts. In 6 months, you get 25 years of work from 10k people. This works occurs with pretty limited compute per person-year, but anything purely conceptual/theoretical can be worked on for a very long time.\n- But, also capabilities are improving over this period, so you actually get substantially more labor than this, as long as the level of alignment can be preserved.\n- At some point, AIs end up scheming if you haven't done enough additional work by this point. But, it seems pretty likely you'd still be safe after enough AI progress that it's as though the AI's are 5x faster (taking into account qualitative intelligence improvements, I just mean that the AIs are as productive as roughly 5x faster versions of our prior AIs).\n- So, within a short period (e.g. 2 months) after this additional AI progress (such that it's as though the AIs are 5x faster), you have an insane amount of alignment work done.\n- You could just think takeoff is very fast, or that alignment is very compute bottlenecked.\n\nThese AIs might also advise different actions than an 80/20 split to be clear! Like trying to buy lead time to spend on alignment.\n\nThis overall makes me pretty optimistic about scenarios where we reach this level of alignment in these not-yet-ASI level systems which sounds like a clear disagreement with your perspective. I don't think this is all of the disagreement, but it might drive a bunch of it.\n\n(To be clear, I think this level of alignment could totally fail to happen, but we seem to disagree even given this!)",
      "plaintextDescription": "Suppose that we ended up with AIs that were ~perfectly aligned (to what the company/project that trained these AIs wanted) which had capabilities that dominate top human experts in AI R&D, AI safety research, philosophy, and most digital research fields in general. These AIs are sufficiently aligned that they try hard to do the right thing even on hard-to-check, conceptual, and open-ended tasks. Let's say these AIs aren't much more capable than needed for this capability bar (and the capability profile works out such that these AIs aren't qualitatively wildly superhuman as seems likely to me).\n\nThese AIs are sufficiently aligned and capable that they dominate humans at doing safety work and have better epistemics than groups of human experts.\n\nI think the situation would probably be fine if the AI company tasked these AIs with proceeding with AI development with 20% of resources spent on alignment/safety of future models and 80% on advancing capabilities.\n\nI claim that either you have to think that takeoff is very fast or that alignment is very difficult to think things are likely to go wrong given this situation:\n\n * At the point of handoff, you maybe have ~3 million AI labor years / year and each instance maybe runs at effectively 50x speed. (Obviously the error bars here are very wide.) 20% goes to alignment.\n * So, holding this level of capability fixed, in 2 months, you get the equivalent of ~8 years of work from 10k people at the level of top human experts. In 6 months, you get 25 years of work from 10k people. This works occurs with pretty limited compute per person-year, but anything purely conceptual/theoretical can be worked on for a very long time.\n * But, also capabilities are improving over this period, so you actually get substantially more labor than this, as long as the level of alignment can be preserved.\n * At some point, AIs end up scheming if you haven't done enough additional work by this point. But, it seems pretty likely you'd still be safe af"
    },
    "baseScore": 9,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-13T01:36:08.144Z",
    "af": false
  },
  {
    "_id": "cdvowJr7Hth8XAhDD",
    "postId": "GAJbegsvnd85hX3eS",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> More concretely, my median is that AI research will be automated by the end of 2028, and AI will be better than humans at >95% of current intellectual labor by the end of 2029.\n\nIt seems like you think AI research will be (fully?) automated once AIs are at 80% reliability on 1 month long benchmarkable / easy-to-check SWE tasks. (Or within 1 year of 50% reliability on 1 month tasks.) Isn't this surprisingly fast? I'd guess there is a decently large gap between 1 month tasks and \"fully automated AI R&D\". Partial automation will speed things up, but will it speed things up this much?",
      "plaintextDescription": "> More concretely, my median is that AI research will be automated by the end of 2028, and AI will be better than humans at >95% of current intellectual labor by the end of 2029.\n\nIt seems like you think AI research will be (fully?) automated once AIs are at 80% reliability on 1 month long benchmarkable / easy-to-check SWE tasks. (Or within 1 year of 50% reliability on 1 month tasks.) Isn't this surprisingly fast? I'd guess there is a decently large gap between 1 month tasks and \"fully automated AI R&D\". Partial automation will speed things up, but will it speed things up this much?"
    },
    "baseScore": 11,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-12T22:46:53.499Z",
    "af": false
  },
  {
    "_id": "atovvQQ94AaCuqp7H",
    "postId": "kgb58RL88YChkkBNf",
    "parentCommentId": "u9KQ5QCxzwnobkrB4",
    "topLevelCommentId": "jTh5oyAACvwK5fTrb",
    "contents": {
      "markdown": "> By what mechanism? This feels like 'we get a pause' or 'there's a wall'.\n\nI just meant that takeoff isn't that fast so we have like >0.5-1 year at a point where AIs are at least very helpful for safety work (if reasonably elicited) which feels plausible to me. The duration of \"AIs could fully automate safety (including conceptual stuff) if well elicited+aligned but aren't yet scheming due to this only occuring later in capabilites and takeoff being relatively slower\" feels like it could be non-trivial in my views.\n\nI don't think this involves either a pause or a wall. (Though some fraction of the probability does come from actors intentionally spending down lead time.)\n\n> I'm unsure which way to read this\n\nI meant it's generally helpful and would have been helpful here for this specific issue, so mostly 2, but also some of 1. I'm not sure if there are other specific places where the piece overclaims (aside from other claims about takeoff speeds elsewhere). I do think this piece reads kinda poorly to my eyes in terms of it's overall depiction of the situation with AI in a way that maybe comes across poorly to an ML audience, but idk how much this matters. (I'm probably not going to prioritize looking for issues in this particular post atm beyond what I've already done : ).)\n\n> The thing I am confident in is \"if superintelligence tomorrow, then we're cooked\". I expect to remain confident in something like this for a thousand tomorrows at the very least, maybe many times that.\n\nThis is roughly what I meant by \"you are actually uncertain (which I am, but you aren't)\", but my description was unclear. I meant like \"you are confident in doom in the current regime (as in, >80% rather than <=60%) without a dramatic change that could occur over some longer duration\". TBC, I don't mean to imply that being relatively uncertain about doom is somehow epistemically superior.",
      "plaintextDescription": "> By what mechanism? This feels like 'we get a pause' or 'there's a wall'.\n\nI just meant that takeoff isn't that fast so we have like >0.5-1 year at a point where AIs are at least very helpful for safety work (if reasonably elicited) which feels plausible to me. The duration of \"AIs could fully automate safety (including conceptual stuff) if well elicited+aligned but aren't yet scheming due to this only occuring later in capabilites and takeoff being relatively slower\" feels like it could be non-trivial in my views.\n\nI don't think this involves either a pause or a wall. (Though some fraction of the probability does come from actors intentionally spending down lead time.)\n\n> I'm unsure which way to read this\n\nI meant it's generally helpful and would have been helpful here for this specific issue, so mostly 2, but also some of 1. I'm not sure if there are other specific places where the piece overclaims (aside from other claims about takeoff speeds elsewhere). I do think this piece reads kinda poorly to my eyes in terms of it's overall depiction of the situation with AI in a way that maybe comes across poorly to an ML audience, but idk how much this matters. (I'm probably not going to prioritize looking for issues in this particular post atm beyond what I've already done : ).)\n\n> The thing I am confident in is \"if superintelligence tomorrow, then we're cooked\". I expect to remain confident in something like this for a thousand tomorrows at the very least, maybe many times that.\n\nThis is roughly what I meant by \"you are actually uncertain (which I am, but you aren't)\", but my description was unclear. I meant like \"you are confident in doom in the current regime (as in, >80% rather than <=60%) without a dramatic change that could occur over some longer duration\". TBC, I don't mean to imply that being relatively uncertain about doom is somehow epistemically superior."
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-11T05:31:34.254Z",
    "af": false
  }
]