[
  {
    "_id": "Zcg9idTyY5rKMtYwo",
    "postId": "SbAofYCgKkaXReDy4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "**Why the focus on wise AI advisors? (Metapost**[^jjkppxnxn9l]**Â with FAQ ğŸ“šğŸ™‹ğŸ»â€â™‚ï¸, Informal**[^fpemp7m5g1f]**Â Edition ğŸ•º, Working Draft ğŸ› ï¸)**\n================================================================================================================================================\n\n+++ **About this PostÂ ** \\- ğŸ”—:Â  [`ğŸ•ºwiseaiadvisors.com`](http://www.wiseaiadvisors.com) *,Â * `~**ğŸ•´ï¸Formal Edition (coming soon)**~`\n\n**This post is still in draft**, so any feedback would be greatly appreciated ğŸ™. It'll be posted as a full, proper Less Wrong/EA Forum/Alignment forum post, as opposed to just a short-form, when it's ready ğŸŒ±ğŸŒ¿ğŸŒ³.\n\n[`Â âœ‰ï¸ PM via LW profileÂ `](https://www.lesswrong.com/users/chris_leong) Â \n\n+++ **ğŸ“² QR Code**\n\n![QR code](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4b7763283db1ffc2edcad94e5e422775dfb9c45d5f81555.webp)\n\n\n\n\n+++\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\nThis post is a collaboration between Chris Leong (primary author) and Christopher Clay (editor), written in the voice of Chris Leong.\n\nWe have worked very hard on this[^v1c3p35za3]Â and we hope you find it to be of some use. Despite this work, it will most likely contain enough flaws that we will be at least somewhat embarrassed at having written at least parts of it and yet at some point a post has to go out into world to fend for itself.\n\nThere's no guarantee that this post will have the kind of impact that we deeply wish for, or even any achieve anything at all... And yet - regardless[^1o1w2qhebrt]Â \\- it is an honour to serve[^soymig3bk6]Â especially at this dark hour when AGI looms on the horizon and doubt has begun to creep into the hearts of men[^7uu5r9iyvrp]Â ğŸ«¡. The sheer scale of the problem feels overwhelming at times, and yet... we do what we can[^db0uqr4co4].\n\nThis post[^phthi2c7tmi]Â doubles[^j5fcv0ia4b]Â as both as serious attempt to produce an original \"alignment proposal\"[^07gb5uq7naii]Â and a passion project[^n8q4p9itm8o]. The AI safety community has spend nearly two and a half decades trying to convince people to take AI risks seriously. Whilst there have been [significant](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023) [successes](https://safe.ai/work/statement-on-ai-risk), these have also [fallen short](https://www.transformernews.ai/p/paris-ai-summit-failure)[^54slr2rt88c]. Undoubtedly this is incredibly naive[^gvunovlt2mc], but perhaps passion and authenticity[^0e7h2m5mcwxf]Â can succeed where arguments and logic have failed âœ¨ğŸŒ .Â \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/13b6a7f3dfe6980a617b42d696c1a7cc15168b8840e220bf.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n**Acknowledgements:**\n---------------------\n\nThe initial draft of this post was produced during the [10th edition of AI Safety Camp](https://www.aisafety.camp/#h.4tl51mi26u2m). Thanks to my team: Matthew Hampton, Richard Kroon, and Chris Cooper, for their feedback. Unlike most other AI Safety Camp projects, which focus on a single, unitary project, we were more of a research collective with each person pursuing their own individual projects.\n\nI am continuing development during the 2025 Summer Edition of the [Cambridge ERA: AI Fellowship](https://erafellowship.org/) with an eye to eventually produce a more formal output. Thanks to my research manager, Peter Gebauer and my mentor, Prof. David Manley.\n\nI also greatly appreciate feedback provided by many others[^c6alwvabodu], including: Jonathan Kummerfeld.\n\n\n\n\n++++++ **âœ’ï¸ Selected Quotes:** [`**ğŸ•µï¸**`](https://www.lesswrong.com/posts/SbAofYCgKkaXReDy4/chris_leong-s-shortform?view=postCommentsNew&postId=SbAofYCgKkaXReDy4&commentId=nSADfv5LmodMKrj5L)\n\n<table style=\"border:4px solid hsl(0, 0%, 0%)\"><tbody><tr><td style=\"border-style:none;text-align:center\">But the moral considerations, Doctor...<br><br>Did you and the other scientists not stop to consider the implications of what you were creating? â€” <strong>Roger Robb</strong><br><br>When you see something that is technically sweet, you go ahead and do it and you argue about what to do about it only after you have had your technical success. That is the way it was with the atomic bomb<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"16\" data-footnote-id=\"mfip7m73mqq\" role=\"doc-noteref\" id=\"fnrefmfip7m73mqq\"><sup><a href=\"#fnmfip7m73mqq\">[16]</a></sup></span>â€” <strong>Oppenheimer</strong></td></tr><tr><td style=\"border-style:none;text-align:center\">â¦</td></tr><tr><td style=\"border-style:none;text-align:center\">There are moments in the history of science, where you have a group of scientists look at their creation and just say, you know: â€˜What have we done?... Maybe it's great, maybe it's bad, but what have we done? &nbsp;â€” <strong>Sam Altman</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"17\" data-footnote-id=\"z6b96rz3izh\" role=\"doc-noteref\" id=\"fnrefz6b96rz3izh\"><sup><a href=\"#fnz6b96rz3izh\">[17]</a></sup></span></td></tr><tr><td style=\"border-style:none;text-align:center\">â¦</td></tr><tr><td style=\"border-style:none;text-align:center\">Urgent: get collectively wiser - <strong>Yoshua Bengio, AI \"Godfather\"</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"18\" data-footnote-id=\"x18prkmell\" role=\"doc-noteref\" id=\"fnrefx18prkmell\"><sup><a href=\"#fnx18prkmell\">[18]</a></sup></span><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"19\" data-footnote-id=\"zgmg02kaeoa\" role=\"doc-noteref\" id=\"fnrefzgmg02kaeoa\"><sup><a href=\"#fnzgmg02kaeoa\">[19]</a></sup></span></td></tr></tbody></table>\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5fb57b7c057b577bb3507ade36c84417aafece4257201484.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\nWe stand at a crucial moment in the history of our species. Fueled by technological progress, our power has grown so great that for the first time in humanityâ€™s long history, we have the capacity to destroy ourselvesâ€”severing our entire future and everything we could become.\n\nYet humanityâ€™s wisdom has grown only falteringly, if at all, and lags dangerously behind. Humanity lacks the maturity, coordination and foresight necessary to avoid making mistakes from which we could never recover. As the gap between our power and our wisdom grows, our future is subject to an ever-increasing level of risk. This situation is unsustainable. So over the next few centuries[^jzpd115xfjs], humanity will be tested: it will either act decisively to protect itself and its long-term potential, or, in all likelihood, this will be lost forever â€” **Toby Ord,** *The Precipice*[^2nxmmdl2c84]\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\nWe have created a Star Wars civilization, with Stone Age emotions, medieval institutions, and godlike technology â€” **Edward O. Wilson**, *The Social Conquest of Earth*[^5tjuz2uesxw]\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\nBefore the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything and the immaturity of our conduct â€” **Nick Bostrom**, Founder of the Future of Humanity Institute, *Superintelligence*[^lazgfieyhti]\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\nIf we continue to accumulate only power and not wisdom, we will surely destroy ourselves â€” **Carl Sagan**, *Pale Blue Dot*[^50qt1zbgixv]\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\nNever has humanity had such power over itself, yet nothing ensures that it will be used wisely, particularly when we consider how it is currently being usedâ€¦There is a tendency to believe that every increase in power means â€œan increase of â€˜progressâ€™ itself â€, an advance in â€œsecurity, usefulness, welfare and vigour; â€¦an assimilation of new values into the stream of cultureâ€, as if reality, goodness and truth automatically flow from technological and economic power as such. â€” **Pope Francis**, *Laudato si'*[^jbnzpzrhqm]\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\nThe fundamental test is how wisely we will guide this transformation â€“ how we minimize the risks and maximize the potential for good â€” **AntÃ³nio Guterres**, Secretary-General of the United Nations[^wj9aaiu9ky]\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\nOur future is a race between the growing power of our technology and the wisdom with which we use it. Letâ€™s make sure that wisdom wins â€” **Stephen Hawking**, *Brief Answers to the Big Questions*[^6uvrmixpsg]\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\n[`ğŸ Additional quotes (from Life Itself)Â `](https://lifeitself.org/learn/wisdom-gap)\n\n![A digital painting of a toddler sitting in a sandbox at dusk, wearing a crumpled paper crown and holding a glowing scepter topped with a miniature Earth. The child looks fascinated and innocent. Surrounding them are plastic toy tanks, a rocket, a dump truck, and a small shovel, softly illuminated by the warm glow of the scepter. The background is dark, evoking a sense of quiet gravity.](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/38f866971870df4623a5d5f10b5973fe864e6ffa7565d153.png)\n\n\n\n\n+++\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"background-color:#fef3e6;border-style:none;text-align:center\"><blockquote><p><i>In light of the potential of AI development to feed back into itself...</i></p><p><i>if increases in capabilities don't lead to an equivalent increase in wisdom...</i></p><h3><i><strong>our capabilities are likely to far exceed our ability to handle</strong></i><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"28\" data-footnote-id=\"faq358rz42w\" role=\"doc-noteref\" id=\"fnreffaq358rz42w\"><sup><a href=\"#fnfaq358rz42w\">[28]</a></sup></span><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"29\" data-footnote-id=\"n91qdvd9u2\" role=\"doc-noteref\" id=\"fnrefn91qdvd9u2\"><sup><a href=\"#fnn91qdvd9u2\">[29]</a></sup></span></h3></blockquote></td></tr></tbody></table>\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\n### **ğŸ”® Preview**\n\n<table style=\"border:3px solid hsl(0, 0%, 0%)\"><tbody><tr><td style=\"background-color:#f2edfa\"><strong>ğŸ§¨ The Defining Challenge</strong>: <i>Given the rapid speed of AI progress, massive strategic uncertainty and wide range of potential societal-scale vulnerabilities</i><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"30\" data-footnote-id=\"5uuw3mc1psx\" role=\"doc-noteref\" id=\"fnref5uuw3mc1psx\"><sup><a href=\"#fn5uuw3mc1psx\">[30]</a></sup></span>... <i>we face a growing gap between our capabilities and our wisdom</i><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"31\" data-footnote-id=\"jxddqbmy5t\" role=\"doc-noteref\" id=\"fnrefjxddqbmy5t\"><sup><a href=\"#fnjxddqbmy5t\">[31]</a></sup></span>...</td></tr><tr><td style=\"background-color:#dcd0f2;text-align:justify\"><strong>ğŸ“Œ </strong>ğ™¿ğšğ™¸ğ™¼ğ™°ğšğšˆ ğ™²ğ™»ğ™°ğ™¸ğ™¼<strong>: </strong>In light of this, wise AI advisors aren't just an <i><strong>urgent</strong></i><strong> research priority</strong>,<strong> but </strong><i><strong>absolutely critical</strong></i><strong> (â‰ï¸)</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"32\" data-footnote-id=\"hegisbxg6nv\" role=\"doc-noteref\" id=\"fnrefhegisbxg6nv\"><sup><a href=\"#fnhegisbxg6nv\">[32]</a></sup></span></td></tr></tbody></table>\n\n+++ **ğŸ“–** ğ™±ğ™°ğš‚ğ™¸ğ™² ğšƒğ™´ğšğ™¼ğ™¸ğ™½ğ™¾ğ™»ğ™¾ğ™¶ğšˆ **\\- Wise AI advisors? Wisdom?** *(I recommend skipping initially)* ğŸ™\n\nÂ **â¬‡ï¸ I suggest initially skipping this section** and focusing on the core argument for now[^o6fi0wr2k0c]Â â¬‡ï¸\n\n+++ **Wise AI Advisors?Â **\n\nDo you mean?:  \nâ€¢ AIs trained to provide advice to humans âœ…  \nâ€¢ AIs trained to act wisely in the world âŒ  \nâ€¢ Humans trained to provide wise advise about AI âŒ\n\nMost work on *Wise AI* is focused on the question of how AI could learn to act wisely in the world[^w9irh1p2hkr], however, I'm more interested in the former as it allows humans to compensate for the weaknesses in the AIs[^ydl8ctv68iq].Â \n\nEven though *Wise AI Advisers* doesn't refer to humans, I am primarily interested in how *Wise AI Advisors* could be deployed as part of a cybernetic system.\n\nTraining humans to be wiser would help with this project:\n\nâ€¢ Wiser humans can train wiser AI  \nâ€¢ When we combine AI and humans into a cybernetic system, wiser humans will both be better able to elicit capabilities from the AI and also better able to plug any gaps in the AIs wisdom.\n\n\n\n\n++++++ **What do you mean by wisdom?Â **\n\nFor the purposes of the following argument, I'd encourage you to first consider this in relation to how you conceive of wisdom rather than worrying too much about how I conceive of wisdom. Two main reasons:\n\nâ€¢ I suspect this reduces the chance of losing someone partway through the argument because they conceive of wisdom slightly differently than I do[^nusqvx99ufs].  \nâ€¢ I believe that there are many different types of wisdom that are useful for steering the world in a positive direction and many perspectives on wisdom that are worth investigating. I'm encouraging readers to first consider this argument in relation to their own understanding of wisdom in order to increase the diversity of approaches pursued.\n\n*Even though I'd encourage you to read the core argument first, if you really want to hear more about how I conceive of wisdom right now, you can scroll down to the Clarifications section to find out more about what I believe.* ğŸ”œğŸ§˜ or â¬‡ï¸â¬‡ï¸â¬‡ï¸ğŸ˜”**â­ My core argument... in a single sentence â˜ï¸â­**\n\n\n\n\n+++\n\n\n++++++ **ğŸ˜® Absolutely critical? That's a strong claim! -**  **ğŸ›¡ï¸**[^eeb22p5hq1q]\n\nYes, Â I've[^qsh0j77epwp]Â intentionally chosen a high bar for what I'll be arguing for in this post. I believe that there's a strong case to be made that the gameboard looks much less promising without them[^w90s7saq4is][^3gphbn8unn5].  \n  \nDon't worry, Â I'll be addressing a *long* list of objections[^jcvwxbjkszm]Â **âš”ï¸ğŸŒ**. Let's go ğŸª‚!\n\n\n\n\n+++\n\n<table style=\"background-color:hsl(240, 60%, 97%);border:3px solid #dcd0f2\"><tbody><tr><td style=\"background-color:#f2edfa;border-style:none;padding:4px\"><p><strong>â˜ Three Key Advantages:</strong></p><ol><li>âœ³ï¸ğŸ—ºï¸ â€” This is <strong>complementary with almost any plan</strong> for making AGI go well.</li><li>ğŸ†“ğŸ’ â€” The <strong>opportunity cost is minimal</strong>. Financially, much of this work could be pursued as a startup and I expect non-profit projects to appeal to new funders. Talentwise, I expect pursuing this direction to (counter-intuitively) increase the effective talent available for other directions by drawing more talent into the space<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"42\" data-footnote-id=\"e2qrep2h1up\" role=\"doc-noteref\" id=\"fnrefe2qrep2h1up\"><sup><a href=\"#fne2qrep2h1up\">[42]</a></sup></span>.</li><li><strong>ğŸ¦‹ğŸŒªï¸ â€” </strong>Even modest boosts in wisdom could be significant. <strong>Single decisions often shape the course of history.</strong></li></ol><p><i>(See the main post for more detail.)</i></p></td></tr></tbody></table>\n\n+++ Five additional benefits\n\n**ğŸš¨ğŸ’ª**ğŸ¦¾Â  **â€”** 1: This approach **scales with increases in capabilities**.  \n2) Forget marginal improvements, wisdom tech **could provide the missing piece** for another strategy, by allowing us to pursue it in a wiser manner.  \n3) Wisdom technology is likely **favourable from a differential technology perspective**. Many actors are only reckless because they're unwise.  \n4) **Even a small coalition** using these advisors to refine strategy, improve coordination and engage in non-manipulative persuasion **could significantly shift the course of civilisation over time**.  \n5) Suppose all else fails: training up **a bunch of folks with both a deep understanding of the alignment problem and a strong understanding of wisdom seems incredibly useful**.\n\n\n\n\n+++\n\n*Please note: This post is still a work in progress. Some sections have undergone more editing and refinement than others. Some bits may be inconsistent, such as if I'm in the middle of making a change. As this is just a draft, I may not end up endorsing all the claims made. Feedback is greatly appreciated* ğŸ™.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"background-color:#fef3e6;border-style:none;text-align:center\"><blockquote><p><i>Given the stakes, we need every advantage we can get...</i></p><h3><i>we can't afford to leave anything on the table</i><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"43\" data-footnote-id=\"odckrfr0lxq\" role=\"doc-noteref\" id=\"fnrefodckrfr0lxq\"><sup><a href=\"#fnodckrfr0lxq\">[43]</a></sup></span><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"44\" data-footnote-id=\"10pqzu117irp\" role=\"doc-noteref\" id=\"fnref10pqzu117irp\"><sup><a href=\"#fn10pqzu117irp\">[44]</a></sup></span></h3></blockquote></td></tr></tbody></table>\n\n**â­ My** ğ™²ğ™¾ğšğ™´ ğ™°ğšğ™¶ğš„ğ™¼ğ™´ğ™½ğšƒ... **in a single sentence â˜ï¸â­**\n------------------------------------------------------------------\n\n<table style=\"background-color:hsl(240, 60%, 97%);border:3px solid #dcd0f2\"><tbody><tr><td style=\"background-color:#f2edfa;border-style:none;padding:4px;text-align:center\"><h3><i><strong>Unaided human wisdom is vastly insufficient for the task before us..</strong></i></h3><p><i>of navigating an entire series of highly uncertain and deeply contested decisions</i></p><p><i>where a single mistake could prove ruinous</i></p><p><i>with a greatly compressed timeline</i></p></td></tr></tbody></table>\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n### **â­ ... or in less than three minutes**[^fuhaiw8462]**â±ï¸â­**\n\n<table style=\"background-color:hsl(210, 76%, 92%);border:2px dashed #FFB74D\"><tbody><tr><td style=\"background-color:#d8e9ff;border-style:none;text-align:justify\"><i>...if you use the <strong>bold text</strong> to skim</i> until the <a href=\"https://www.lesswrong.com/posts/SbAofYCgKkaXReDy4/chris_leong-s-shortform?view=postCommentsNew&amp;postId=SbAofYCgKkaXReDy4&amp;commentId=LPvF5xG9qjLER3Gju\"><strong>amber lantern section divider</strong></a><strong> ğŸ˜‰</strong></td></tr></tbody></table>\n\nÂ  *Two useful framings:*\n\n<table><tbody><tr><td style=\"background-color:#f8e0c2\">ğšƒ ğ™· ğ™´<strong>â€ƒğŸ…‚ğŸ…„ğŸ……â€ƒ</strong>ğšƒ ğš ğ™¸ ğ™° ğ™³ <i>â€“ speed, uncertainty, vulnerability</i>ğŸƒ<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"46\" data-footnote-id=\"cpn6zczbhj\" role=\"doc-noteref\" id=\"fnrefcpn6zczbhj\"><sup><a href=\"#fncpn6zczbhj\">[46]</a></sup></span><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"47\" data-footnote-id=\"67amztdujec\" role=\"doc-noteref\" id=\"fnref67amztdujec\"><sup><a href=\"#fn67amztdujec\">[47]</a></sup></span></td></tr><tr><td style=\"background-color:#fef3e6\"><p><i>Going through them in reverse order:</i><br>&nbsp;</p><p>ğŸ…… ğš„ ğ™» ğ™½ ğ™´ ğš ğ™° ğ™± ğ™¸ ğ™» ğ™¸ ğšƒ ğšˆ<i> â€“</i> <strong>ğŸŒŠ</strong><i><strong>ğŸš£</strong></i>:</p><p><strong>The development of advanced AI technologies will have a </strong><i><strong>massive</strong></i><strong> impact on society&nbsp;given the </strong><i><strong>essentially infinite</strong></i><strong> ways to deploy such a general technology. There are lots of ways this could go well, and lots of ways w</strong><s><strong>e all die</strong></s><strong> this could go e</strong><i><strong>xtremely</strong></i><strong> poorly.</strong></p><blockquote><p>Catastrophic malfunctions, permanent dictatorships, AI-designed pandemics, mass cyberattacks, information warfare, war machines lacking any mercy, the list goes on...<br><br><i>Worse:</i>&nbsp;<a href=\"https://www.lesswrong.com/posts/dDDi9bZm6ELSXTJd9/intent-aligned-ai-systems-deplete-human-agency-the-need-for\"><code> Someone&nbsp;</code></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/D7PumeYTDPfBTp3i7/the-waluigi-effect-mega-post\"><code> Will&nbsp;</code></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/zi6SsECs5CCEyhAop/gpt-4o-is-an-absurd-sycophant\"><code> <i>Always</i>&nbsp;</code></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/ifechgnJRtJdduFGC/emergent-misalignment-narrow-finetuning-can-produce-broadly\"><code> Invent&nbsp;</code></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/cGcwQDKAKbQ68BGuR/subliminal-learning-llms-transmit-behavioral-traits-via\"><code> A&nbsp;</code></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/tr6hxia3T8kYqrKm5/the-stakes-of-ai-moral-status\"><code> New&nbsp;</code></a>&nbsp;<a href=\"https://www.alignmentforum.org/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem\"><code> Way&nbsp;</code></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/Mak2kZuTq8Hpnqyzb/the-intelligence-curse\"><code> That&nbsp;</code></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness\"><code> Disaster&nbsp;</code></a>&nbsp;<a href=\"https://www.lesswrong.com/posts/6kBMqrK9bREuGsrnd/ai-enabled-coups-a-small-group-could-use-ai-to-seize-power-1\"><code> Could&nbsp;</code></a> <a href=\"https://www.lesswrong.com/posts/wSEPrKkLmnwxFBkFD/ai-takeoff-and-nuclear-war\"><code>Strike</code></a> &nbsp;<a href=\"https://www.lesswrong.com/w/black-swans\"><code> ...&nbsp;</code></a>&nbsp;<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"48\" data-footnote-id=\"0zz8widkd88\" role=\"doc-noteref\" id=\"fnref0zz8widkd88\"><sup><a href=\"#fn0zz8widkd88\">[48]</a></sup></span><br><br><i>The kicker</i>: Threat models can <i>interact</i> (in many, many ways). Technological unemployment breaking politics. Loss of control + automated warfare. AI-enabled theft of specialised biological models...</p></blockquote><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><p><i>In more detail...</i></p></summary><div class=\"detailsBlockContent\"><p>i) At this stage I'm not claiming any particular timelines.</p><p>I believe it's likely to be <s>absurdly</s> quite fast, but I don't make this claim this until we get to Speed.</p><p>I suspect that often when people doubt this claim, they've implicitly assumed that I was talking about the short or medium term, rather than the long term ğŸ¤”. After all, the claim that there are many ways that AI could plausible lead to dramatic benefits or harms over the next 50 or 100 years feels like an <i>extremely</i> robust claim. There are many things that a true artificial general intelligence <i>could </i>do. It's mainly just a question of how long it takes to develop the technology.</p><p>ii) \"We only have to be lucky once, you have to be lucky every time\" - the IRA on the offense-defense balance. Unfortunately, if there's one thing computers are good at, it's persistence.</p><p>iii) \"Mossad is much more clever and powerful than novices implicitly imagine a \"superintelligence\" will be; in the sense that, when novices ask themselves what a \"superintelligence\" will be able to do, they fall well short of the actual Mossad.\" - Eliezer Yudkowsky</p></div></details><p>ğŸ…„ ğ™½ ğ™² ğ™´ ğš ğšƒ ğ™° ğ™¸ ğ™½ ğšƒ ğšˆ â€“ <strong>ğŸŒ…ğŸ’¥:</strong></p><p><strong>We have </strong><i><strong>massive</strong></i><strong> disagreement on what we expect the development of AI, let alone the best strategy</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"49\" data-footnote-id=\"b8zxurs0z8v\" role=\"doc-noteref\" id=\"fnrefb8zxurs0z8v\"><sup><a href=\"#fnb8zxurs0z8v\">[49]</a></sup></span>. <strong>Making the wrong call could prove </strong><i><strong>catastrophic.</strong></i><strong>&nbsp;</strong></p><blockquote><p><i>Strategically</i>: Accelerate or pause? What's the offence-defence balance? Do we need global unity or to win the arms race? Who (or what) should we be aligning AIs to? Can AI \"do our alignment homework\" for us?</p><p><br><i>Situationally</i><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"50\" data-footnote-id=\"lnirj25c27l\" role=\"doc-noteref\" id=\"fnreflnirj25c27l\"><sup><a href=\"#fnlnirj25c27l\">[50]</a></sup></span><i>:</i> Will LLM's scale AGI or are they merely a dead-end? Should we expect <a href=\"https://ai-2027.com/\">AGI in 2027</a> or is it more than a decade away? Will AI create jobs and cure economic malaise or will it crush, destroy and obliviate them? Will the masses embrace as the key to their salvation or call for a Butlerian Jihad?<br><br><i>The kicker: </i>We're facing these issues at an extremly bad time <i>â€“ </i>when both trust and society's epistemic infrastructure is crumbling. Even if our task were epistemically easy, we might still fail.</p></blockquote><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><p><i>In more detail...</i></p></summary><div class=\"detailsBlockContent\"><p>i) A lot of this uncertainty just seems inherently really hard to resolve. Predicting the future is hard.</p><p>ii) However hard this is to resolve in theory, it's worse in practise. Instead of an objective search for the truth, these discussions are distorted by all these different factors including money, social status and the need for meaning.</p><p>iii) <i>More on the kicker: </i>We're seeing increasing polarisation, less trust in media and experts<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"51\" data-footnote-id=\"ga8bmkazsy\" role=\"doc-noteref\" id=\"fnrefga8bmkazsy\"><sup><a href=\"#fnga8bmkazsy\">[51]</a></sup></span>&nbsp;and AI stands to make this worse. This is not where we want to be starting from and who knows how long this might take to resolve?</p></div></details><p>ğŸ…‚ ğ™¿ ğ™´ ğ™´ ğ™³ &nbsp;â€“ ğŸ˜±<strong>â³:</strong></p><p><strong>AI Is developing </strong><i><strong>incredibly</strong></i><strong> rapidly... We have limited time to act and to figure out how to act.</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"52\" data-footnote-id=\"z2r37ct4sk\" role=\"doc-noteref\" id=\"fnrefz2r37ct4sk\"><sup><a href=\"#fnz2r37ct4sk\">[52]</a></sup></span>.</p><blockquote><p><i>No wall! </i>See <a href=\"https://arcprize.org/blog/oai-o3-pub-breakthrough\">o3 crushing the ARC challenge</a>, the <a href=\"https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/\">automation of expontentially longer coding tasks</a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"53\" data-footnote-id=\"4o4ceb7a29\" role=\"doc-noteref\" id=\"fnref4o4ceb7a29\"><sup><a href=\"#fn4o4ceb7a29\">[53]</a></sup></span>, <a href=\"https://x.com/polynoamial/status/1946478249187377206\">LLMs demonstrating IMO gold medal maths ability</a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"54\" data-footnote-id=\"evlbzg6lpak\" role=\"doc-noteref\" id=\"fnrefevlbzg6lpak\"><sup><a href=\"#fnevlbzg6lpak\">[54]</a></sup></span>, <a href=\"https://time.com/7203729/ai-evaluations-safety/\">widespread and rapid benchmark saturation</a>, <a href=\"https://arxiv.org/pdf/2503.23674\">the Turing Test being passed</a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"55\" data-footnote-id=\"6mrrj9yx5bn\" role=\"doc-noteref\" id=\"fnref6mrrj9yx5bn\"><sup><a href=\"#fn6mrrj9yx5bn\">[55]</a></sup></span>&nbsp;and society shrugging its shoulders (â€¼ï¸)</p><p><br><i>Worse</i>: We may <i>already</i> be witnessing an AI arms race. Stopping this may simply not be possible <i>â€“ </i>forget about AGI, an <i>inconcievably</i> large prize <i>â€“ </i>many see winning as a matter of <i>survival</i>.<br><br><i>The kicker: </i>It's entirely plausible that as more of the development process is automated, that AI actually <i>accelerates</i>. That we look back at the current rate of progress and <i>laugh</i> about how we used to consider it fast.</p></blockquote><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><p><i>In more detail...</i></p></summary><div class=\"detailsBlockContent\"><p>i) Even if timelines aren't short, we might still be in trouble if the take-off speed is fast. Unfortunately, humanity is not very good at preparing for abstract, speculative-seeming threats ahead of time.</p><p>ii) Even if neither timelines nor take-off speeds are fast in an <i>absolute</i> sense, we might still expect disaster if they are fast in a <i>relative</i> sense. Governance - especially global governance - tends to proceed rather slowly. Even though it can happen much faster when there's a crisis, sometimes problems need to be solved ahead of time and once you're in them it's too late. As an example, once an AI induced pandemic is spreading, you may have already lost.</p><p>iii) Even if neither timelines and take-off speed are fast in an absolute or relative sense, it's just hard on a fundamental level to regulate or control agents that are capable of acting far faster than humans, especially if we develop agents that are adaptive.</p></div></details></td></tr><tr><td style=\"background-color:#fef3e6\"><p><strong>Reflections - Why the SUV Triad is Fucking</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"56\" data-footnote-id=\"gj8k7rnd8f\" role=\"doc-noteref\" id=\"fnrefgj8k7rnd8f\"><sup><a href=\"#fngj8k7rnd8f\">[56]</a></sup></span><strong>&nbsp;Scary</strong></p><ul><li>The speed of development makes this problem <i>much </i>harder. Even if alignment were easy and governance didn't require anything special, we could still fail because it's been decided that we <i>have</i> to race as fast as possible.</li><li>Even if a threat can't led to catastrophe, it can still distract us from those that can. It's hard to avoid catastrophe when we don't know where to focus our efforts âšªğŸª™âš«.</li><li>Many of the threats constitute civilisational-level risk <i>by themselves. </i>We could successful navigate all the other threats, but simply drop the ball <i>once</i> and all that could be <i>for naught</i>.</li></ul><p>Big if true: It may even <a href=\"https://www.lesswrong.com/posts/SbAofYCgKkaXReDy4/chris_leong-s-shortform?commentId=WXNdpMefwCBZtcPf6#fnq6v04h7nhi\">present a reason</a> (draft) to expect Disaster-By-Default â€¼ï¸</p><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><p><i>In more detail... - TODO:</i></p></summary><div class=\"detailsBlockContent\"><p>i) In an adversarial environment, your vulnerability is determined by the domain where you're most vulnerable. Your weakest link. Worse, this applies recursively. Your vulnerability within a domain is determined by the kind of attack you're least able to withstand.</p><p>ii) Civilisation has limited co-ordination capacity. A committee can only cover so many issues in a given time. Global co-ordination is incredibly difficult to achieve - but it's possible. However, one at a time is a lot easier than dozens. ğŸ˜…â³ğŸ’£ğŸ’£ğŸ’£</p><p>iii) <strong>Talk about resiliance being hard and challenges with general agents.</strong></p></div></details></td></tr></tbody></table>\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">âš˜</td></tr></tbody></table>\n\n<table style=\"background-color:#fff2e6;border-style:none\"><tbody><tr><td style=\"background-color:#f8e0c2;border-style:double\">ğšƒ ğ™· ğ™´ &nbsp; ğš† ğ™¸ ğš‚ ğ™³ ğ™¾ ğ™¼ â€” [ğ™² ğ™° ğ™¿ ğ™° ğ™± ğ™¸ ğ™» ğ™¸ ğšƒ ğ™¸ ğ™´ ğš‚] &nbsp; ğ™¶ ğ™° ğ™¿<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"57\" data-footnote-id=\"4v7remohl8t\" role=\"doc-noteref\" id=\"fnref4v7remohl8t\"><sup><a href=\"#fn4v7remohl8t\">[57]</a></sup></span>&nbsp;<i>â€“ </i>ğŸ“‰ğŸ“ˆ</td></tr><tr><td style=\"background-color:#fef3e6;border-style:double\"><p>&lt;TODO: Feels like there's some more implicit subclaims here I need to address&gt;<br>&nbsp;</p><p><strong>Subclaim 1: As humanity gain access to more power, we need more wisdom in order to navigate it</strong></p><p><br>This claim is almost a cliche, but is it true?</p><p>&nbsp;</p><p>I think that it is, at least in the case of AI:</p><ul><li><strong>The more powerful a technology is, the greater the cost of accidentally \"dropping a ball\"</strong></li><li><strong>As a general technology, the more powerful AI becomes, the more balls there are to drop</strong></li><li><strong>As AI becomes more powerful, we venture further and further away from past experience</strong> (we could even call this the societal training distribution)&nbsp;</li></ul><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><p><i>In more detail...</i></p></summary><div class=\"detailsBlockContent\"><p>Objection: AI can help us reduce our chance of dropping a ball<br><br>Response: I agree, that's why I'm proposing this research direction, but I don't think this happens by default.<br><br>One possibility is that we create strongly aligned general defender agents (seems unlikely) that we just give mostly free reign (very risky).</p><p><br>Or we need AI that helps us make wise decisions ahead of time (where to allocate resources, where to concretrate oversight), which is much harder to train for than bio/cyber/manipulation capabilities.</p><p>&nbsp;</p><p>Question: What about society's natural process of accumulating wisdom?</p><p><br>Let's take a step back. If we construe wisdom broadly, then we end up with two kinds of wisdom<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"whg0hyhxfc8\" role=\"doc-noteref\" id=\"fnrefwhg0hyhxfc8\"><sup><a href=\"#fnwhg0hyhxfc8\">[3]</a></sup></span>:</p><ul><li>Hindsight: Wisdom gained through hard, bitter experience</li><li>Foresight: Being able to figure out what needs to be done even without direct experience</li></ul><p><br>The slower the rate of development, the more we can lean on hindsight, the faster it is, then more we need to lean on foresight.<br>&nbsp;</p><p>Our timeline seems to be fast, so unfortunately it seems that we will need to rely heavily on foresight, instead of society's natural process of accumulating metis (or practical wisdom) through trial and error.</p></div></details><p><strong>Subclaim 2: By default, increases in capabilities don't result in increases in wisdom</strong></p><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><p><i>In more detail...</i></p></summary><div class=\"detailsBlockContent\"><p>Objection: AI can help us reduce our chance of dropping a ball<br><br>Response: I agree, that's why I'm proposing this research direction, but I don't think this happens by default.<br><br>One possibility is that we create strongly aligned general defender agents (seems unlikely) that we just give mostly free reign (very risky).</p><p><br>Or we need AI that helps us make wise decisions ahead of time (where to allocate resources, where to concretrate oversight), which is much harder to train for than bio/cyber/manipulation capabilities.</p><p>&nbsp;</p><p>Question: What about society's natural process of accumulating wisdom?</p><p><br>Let's take a step back. If we construe wisdom broadly, then we end up with two kinds of wisdom<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"whg0hyhxfc8\" role=\"doc-noteref\" id=\"fnrefwhg0hyhxfc8\"><sup><a href=\"#fnwhg0hyhxfc8\">[3]</a></sup></span>:</p><ul><li>Hindsight: Wisdom gained through hard, bitter experience</li><li>Foresight: Being able to figure out what needs to be done even without direct experience</li></ul><p><br>The slower the rate of development, the more we can lean on hindsight, the faster it is, then more we need to lean on foresight.<br>&nbsp;</p><p>Our timeline seems to be fast, so unfortunately it seems that we will need to rely heavily on foresight, instead of society's natural process of accumulating metis (or practical wisdom) through trial and error.</p></div></details></td></tr></tbody></table>\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">âš˜</td></tr></tbody></table>\n\n<table style=\"background-color:#fff2e6;border-style:none\"><tbody><tr><td style=\"background-color:#f8e0c2;border-style:double\"><strong>How</strong><i><strong> </strong></i><strong>do we address the SUV Triad and the Wisdom Gap?&nbsp;</strong></td></tr><tr><td style=\"background-color:#fef3e6;border-style:double\"><p><strong>I propose that the most straightforward way to address this is to train wise AI advisors. But what about the alternatives?:</strong></p><ol><li><strong>Perhaps we need to buy time, that is to pause? As previously discussed, there are incredibly strong forces pushing against this. Even if a pause is necessarily, it seems unlikely that we could persuade enough actors to see this, agree on a robust mechanism and unpause at the appropriate time without some kind of major increase in wisdom.</strong></li><li><strong>Given the number of different threats and how fast they're coming at us, whack-a-mole simply isn't going to cut it</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"58\" data-footnote-id=\"msdo4n1exzg\" role=\"doc-noteref\" id=\"fnrefmsdo4n1exzg\"><sup><a href=\"#fnmsdo4n1exzg\">[58]</a></sup></span><strong>. We need general solutions.</strong></li><li><strong>Whilst we can and should be developing human wisdom as fast as possible, this process tends to be slow. I don't believe that this will cut it by itself. In any case, I expect increases in human wisdom and increases in AI wisdom to be strongly complementary.</strong></li></ol><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><p><i>&nbsp;In more detail...</i></p></summary><div class=\"detailsBlockContent\"><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png/w_110 110w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png/w_220 220w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png/w_330 330w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png/w_440 440w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png/w_550 550w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png/w_660 660w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png/w_770 770w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png/w_880 880w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png/w_990 990w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/07ff12cacf2dbb244aba337f9475f064312d64094ef500b2.png/w_1068 1068w\"></figure><p>TODO: This diagram still needs to be updated. I don't want the alternatives I address to look like just a random list. Providing a diagram that shows how we could try to operate on different points helps make this seem more legible.</p><p>&nbsp;</p><p>TODO: Address more exotic responses.</p></div></details></td></tr></tbody></table>\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">âš˜</td></tr></tbody></table>\n\n*In light of this:*\n\n<table style=\"border:3px solid hsl(0, 0%, 0%)\"><tbody><tr><td style=\"background-color:#dcd0f2;text-align:justify\"><h3><strong>ğŸ“Œ I believe that AI is much more likely to go well for humanity if we develop wise AI advisors</strong></h3></td></tr><tr><td style=\"background-color:#f2edfa\"><p><strong>&nbsp; &nbsp;</strong> I am skeptical of the main alternatives</p><p><strong>&nbsp; &nbsp;&nbsp;</strong>I am serious about making this happen...</p><p>&nbsp; &nbsp; If you are serious about this too, please:&nbsp;<a href=\"https://www.lesswrong.com/users/chris_leong\"><code> âœ‰ï¸ message me&nbsp;</code></a><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"59\" data-footnote-id=\"eg9jv9u2b4\" role=\"doc-noteref\" id=\"fnrefeg9jv9u2b4\"><sup><a href=\"#fneg9jv9u2b4\">[59]</a></sup></span>.</p></td></tr></tbody></table>\n\n+++ **â˜ Or for a More Formal Analysis (Using Three Different Frameworks) â€” TODO**\n\n**Importance-Tractability-Neglectedness**  \nÂ \n\nThis is a standard EA framework for considering *cause areas*. Wise AI is broad enough that I consider it reasonable to analyse it as a cause area.Â \n\n| Â  | Definition | Score | Reason |\n| --- | --- | --- | --- |\n| Importance | Â  | Â  | Â  |\n| Tractability | Â  | Â  | Â  |\n| Neglectedess | Â  | Â  | Â  |\n| OVERALL | Â  | Â  | Â  |\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n**Safety-Freedom-Value**\n\nThis framework is designed to appeal more to startup/open-source folks. These folks are more likely to put significant weight on the social value a technology provides and on freedom beyond mere utility.\n\n| Â  | Definition | Score | Reason |\n| --- | --- | --- | --- |\n| Safety | Â  | Â  | Â  |\n| Positive Use Cases | Â  | Â  | Â  |\n| \\* Freedom (as intrinsic good) | Â  | Â  | Â  |\n| OVERALL | Â  | Â  | Â  |\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n**Searching for Solutions**  \nÂ \n\nThis framework is designed for folk who think current techniques are unlikely to work.\n\n| Â  | Definition | Score | Reason |\n| --- | --- | --- | --- |\n| Survival value | How much does the technique directly contribute to our chance of survival? | Â  | Â  |\n| Information Value | Â  | Â  | Â  |\n| Unlocked Optionality | Â  | Â  | Â  |\n| Acceleration | Â  | Â  | Â  |\n| \\* Malicious use cases | Â  | Â  | Â  |\n| OVERALL | Â  | Â  | Â  |\n\n\n\n\n++++++ **â˜ I'm sold! How can I get involved? ğŸ¥³ğŸ**\n\nAs I said, if you're serious, pleaseÂ  [`**âœ‰ï¸** PM me`](https://www.lesswrong.com/users/chris_leong) . If you think you might be serious, but need to talk it through, please reach out as well.\n\nI'd be useful for me to know your background and how you think you could contribute. Maybe tell you a few facts about yourself, what interests you and drop a link to your LinkedIn profile?\n\nIf you scroll down, you'll see that I've answered some more questions about getting involved, but I'll include some useful links here as well:\n\nâ€¢Â  [`List of potentially useful projects`](https://www.lesswrong.com/posts/24RvfBgyt72XzfEYS/potentially-useful-projects-in-wise-ai) : for those who want to know what could be done concretely. Scroll down further for a list of project lists â¬‡ï¸.  \nâ€¢Â  [`Resources for founding an AI safety startup or non-profit`](https://www.lesswrong.com/w/ai-safety-and-entrepreneurship) : since I believe there should be multiple organisations pursuing this agenda  \nâ€¢Â  [`Resources for getting started in AI Safety more broadly`](https://www.aisafety.com/) : since some of these resources might be useful here as well\n\n\n\n\n+++\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n### **ğŸ¤” Clarifications**\n\n*Reminder: If you're looking for a definition of wise AI advisors, it's at the* ğŸ” *of the page .*\n\n+++ **I've read through your argument and substituted my own understanding of wisdom. Now that** ~**you've wasted my time**~ **I've done this, perhaps you could clarify how you think about wisdom? âœ…ğŸ™**\n\n*Sure. I've written at the top of this post* â¬†ï¸ *why I often try to dodge this question initially. But given that you've made it this far, I'll share some thoughts. Just don't over-update on my views* ğŸ˜‚.\n\nGiven how rapidly AI is developing, I suspect that we're unlikely to resolve the millennia-long philosophical debate about the true nature of wisdom before AGI is built. Therefore, I suggest that we instead sidestep this question by identifying specific capabilities related to wisdom that might useful for steering the world in a positive direction.\n\nI'd suggest that examples might include: the ability to make wise strategic decisions, non-manipulation persuasion and the ability to find win-wins. I'll try to write up a longer list in the future.\n\nSome of these capabilities will be more or less useful for steering the world in positive directions. On the other hand, negative externalities such as accelerating timelines or enabling malicious actors.\n\nMy goal is to figure out which capabilities to prioritise by balancing the benefits against the costs and then incorporating feasibility.\n\nIt may be the case that some people decide that wisdom requires different capabilities than those I end up deciding are important. As long as they pick a capability that isn't net-negative, I don't see that as bad. In fact, I see different people pursuing different understandings of wisdom as adding robustness to different world models.\n\n\n\n\n++++++ **Â If wisdom ultimately breaks down into specific capabilities, why not simply talk about these capabilities and avoid using a vague concept like wisdom? ğŸ™…â€â™‚ï¸ğŸŒ**\n\nSo the question is: \"Why do I want to break wisdom down into separate capabilities instead of choosing a unitary definition of wisdom and attempting to train that into an AI system?\"\n\nFirstly, I think the chance of us being able to steer the world towards a positive direction is much higher if we're able to combine multiple capabilities together, so it makes sense to have a handle for the broader project, in addition to handles for individual sub-projects. I believe that techniques designed for one capability will often carry over to other capabilities, as will the challenges, and having a larger handle makes it easier to make these connections. I also think there's a chance that these capabilities amplify each other (as per the final few paragraphs ofÂ  [`Imagining and Building Wise Machines`](https://arxiv.org/pdf/2411.02478)[^crtqmhn7yak]Â by Johnson, Bengio, Grossmann, et al).  \n  \nSecondly, I believe we should be aiming to increase both human wisdom and AI wisdom simultaneously. In particular, I believe it's important to increase the wisdom of folks creating AI systems and that this will then prove useful for a wide variety of specific capabilities that we might wish to train.  \n  \nFinally, I'm interested in investigating this frame as part of a more ambitious plan to solve alignment on a principled level. Instead of limiting the win condition to building an AI that always (competently) acts in line with human values, the wise AI Advisors frame broadens it such that the AI only needs to inspire humans to make the right decision. It's hard to know in advance whether this reframing will be any easier, but even if it doesn't help, I have a strong intuition that understanding why it doesn't help would shed light on the barriers to solving the core alignment problem.\n\n\n\n\n++++++ **Weren't you focused on wise AI advisors via Imitation Learning before? ğŸ¯**\n\nYep, I was focused on it before. I now see that goal as overly narrow. The goal is to produce wise AI Advisors via any means. I think that [Imitation Learning is underrated](https://aiimpacts.org/an-overview-of-obvious-approaches-to-training-wise-ai-advisors/), but there are lots of other approaches that are worth exploring as well.\n\n\n\n\n+++\n\n### **âœ‹ ObjectionsÂ **\n\n+++ **Isn't this argument a bit pessimistic? ğŸ™…â€â™‚ï¸âš–ï¸. I prefer to be optimistic. ğŸ‘Œ**\n\nOptimism isn't about burying your head in the sand and ignoring the massive challenges facing us. That's denialism. Optimism is about rolling up your sleeves and doing what needs to be done.\n\nThe nice thing about this proposal from an optimistic standpoint is that, assuming there is a way for AI to go well for humanity, then it seems natural to expect that there is some way to leverage AI to help us find it[^k76ez251czp].\n\nAdditionally, the argument for developing wise AI advisors isn't in any way contingent on a pessimistic view of the world. Even if you think AI is likely to go well by default, wise AI advisors could still be of great assistance for making things go even better. For example, facilitating negotiations between powers, navigating the safety-openness tradeoff and minimising any transitional issues.\n\n\n\n\n++++++ **But I don't think timelines are short. They could be long. Like more than a decade. ğŸ‘ŒğŸ¤·â€â™‚ï¸**\n\nShort timelines add force to the argument I've made above, but they aren't at all a necessary component[^04xxgeaaee8g].\n\nEven if AI will be developed over decades rather than years, there's still enough different challenges and key decisions that unaugmented human wisdom is unlikely to be sufficient.\n\nIn fact, my proposal might even work better over long timelines, as it provides more time for AI advisers to help steer the world in a positive direction[^2bv3miwbtfv].\n\n\n\n\n++++++ **Don't companies have a commercial incentive to train wise AI by default? ğŸ¤**\n\nI'm extremely worried about the incentives created by a general chatbot product. The average user is low-context and this creates an incentive towards [sychophancy](https://openai.com/index/sycophancy-in-gpt-4o/) ğŸ¤¥.  \n  \nI believe that a product aimed at providing advice for critically important decisions would provide better incentives, even it were created by the same company.\n\nFurthermore, given the potential for short timelines, it seems extremely risky ğŸ° to rely purely on the profit motive, especially since there is a much stronger profit motive to pursue capabilities ğŸ’°ğŸ’°ğŸ’°. A few months' delay could easily mean that a wise AI advisor isn't available for a crucial decision ğŸšŒğŸ’¨. Humanity has probably already missed having such advisors available to assist us during a number of key decision points ğŸ˜­.\n\n\n\n\n++++++ **Won't this just be used by malicious actors? Doesn't this just accelerate capabilities? ğŸ¤”âŒ**\n\nI expect both the benefits and the externalities to vary hugely by capability. I expect some to be positive, some to be negative and some to be extremely hard to determine. More work is required to figure out which capabilities are best from a [differential technology development](https://michaelnotebook.com/dtd/) perspective.\n\n*I understand that this answer might be frustrating, but I think it's worth sharing these ideas even though i haven't yet had time to run this analysis. I have aÂ * [`list of projects`](https://www.lesswrong.com/posts/24RvfBgyt72XzfEYS/potentially-useful-projects-in-wise-ai) *that I hope will prove fairly robust, despite all the uncertainty.*\n\n\n\n\n++++++ **Is there value in wisdom given that wisdom is often illegible and this makes it non-verifiable? âœ”ï¸ğŸ’¯**\n\nOscar Delany makes this argument inÂ  [`Tentatively against making AIs 'wise'`](https://forum.effectivealtruism.org/posts/tuAnrhy2BExGkwTKE/tentatively-against-making-ais-wise) (runner up in theÂ  [`AI Impact Essay Competition`](https://blog.aiimpacts.org/p/winners-of-the-essay-competition) ).\n\nThis will depend on your definition of wisdom.\n\nI admit that this tends to be a problem with how I tend to conceive of wisdom, however I will note thatÂ  [`Imagining and Building Wise Machines`](https://arxiv.org/pdf/2411.02478) ( [`summary`](https://www.lesswrong.com/posts/euAMyQAQWTYyWZW8Z/summary-imagining-and-building-wise-machines-the-centrality) ) takes the opposite stance - the wisdom, conceived of as metacognition - can actually assist with explainability.\n\nBut let's assume that this is a problem. How much of a problem is it?\n\nI suspect that this varies significantly by actor. There's a reasonable argument that public institutions shouldn't be using such tools for reasons of justice. However, these arguments have much less force when it comes to private actors.\n\nEven for private actors, it makes sense to use more legible techniques as much as possible, but I don't think this will be sufficient for all decisions. In particular, I don't think objective reasoning is sufficient for navigating the key decisions facing society in the transition to advanced AI.\n\nBut I also want to push back against the non-verifiability. You can do things like run a pool of advisors and only take dramatic action if more than a certain proportion agree, plus you can do testing, even advanced things like [latent adversarial testing](https://www.lesswrong.com/posts/mynHvr3ZnYDsk2tGH/latent-adversarial-training-lat-improves-the-representation). It's not as verifiable as we'd like, but it's not like we're completely helpless here.\n\nThere will also be ways to combine wise AI advisors with more legible systems.\n\n\n\n\n++++++ **I'm mostly worried about inner alignment. Does this proposal address this? ğŸ¤âœ”ï¸**\n\nInner alignment is an extremely challenging issue. If only we had some... wise AI advisors to help us navigate this problem.\n\n\"But this doesn't solve the problem, this assumes that these advisors are aligned themselves\": Indeed, that is a concern. However, I suspect that the wise AI advisors approach has less exposure to these kinds of risks as it allows us to achieve certain goals at a lower base model capability level.\n\nâ€¢ Firstly, wise people don't always have huge amounts of intellectual firepower. So I believe that we will be able to achieve a lot without necessarily using the most powerful models.  \nâ€¢ Secondly, the approach of combined human-AI teams allows the humans to compensate for any weaknesses present in the AIs.\n\nIn summary, this approach might help in two ways: by reducing exposure and advising us on how to navigate the issue.\n\n\n\n\n+++\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n### **ğŸŒ… What makes this agenda so great?**\n\nThanks for asking ğŸ¥³. Strangely enough I was looking for an excuse to hit a bunch of my key talking points ğŸ˜‚. Here's my top three:\n\n<table style=\"background-color:#fef3e6\"><tbody><tr><td style=\"border-style:none\"><p><br>i:<strong> ğŸ¤ğŸ§­ğŸŒ Imagine a coalition of organisations</strong> attempting to steer the world towards positive outcomes<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"64\" data-footnote-id=\"9x9nhn6ilx\" role=\"doc-noteref\" id=\"fnref9x9nhn6ilx\"><sup><a href=\"#fn9x9nhn6ilx\">[64]</a></sup></span>, <strong>with wise AI advisors helping them refine strategy, improve coordination and engage in non-manipulative persuasion</strong>. How much much do you think this could shift our trajectory? It seems to me that even a small coalition could make a significant difference over time.<br>&nbsp;</p><blockquote><p><strong>Q: </strong>If this existed, would your organisation want to be part of such a coalition?<br><br><i>Stronger, but more speculative </i><strong>ğŸ¤œğŸ¤›: </strong>Forget a small coalition<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"65\" data-footnote-id=\"g17me1dynkd\" role=\"doc-noteref\" id=\"fnrefg17me1dynkd\"><sup><a href=\"#fng17me1dynkd\">[65]</a></sup></span>, what if these wise AI advisors were able to help an initial coalition cultivate a much larger set of allies? Surely this would greatly increase the impact<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"66\" data-footnote-id=\"4eyc2jpzl2o\" role=\"doc-noteref\" id=\"fnref4eyc2jpzl2o\"><sup><a href=\"#fn4eyc2jpzl2o\">[66]</a></sup></span>?</p></blockquote></td></tr><tr><td style=\"border-style:none;padding:0px;text-align:center\">â‰</td></tr><tr><td style=\"border-style:none\"><p><br>ii: ğŸ†“ğŸ’âœ³ï¸ Wise AI advisors are most likely <strong>complementary with almost any plan</strong> for making AGI go well<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"67\" data-footnote-id=\"nw9j9obwnrr\" role=\"doc-noteref\" id=\"fnrefnw9j9obwnrr\"><sup><a href=\"#fnnw9j9obwnrr\">[67]</a></sup></span>. I expect that much of this work could take the form of a startup and that non-profits working in this space would appeal to new funders, such that there's <strong>very little opportunity cost</strong> in terms of pursuing this direction.<br>&nbsp;</p><blockquote><p><strong>Q: </strong>How could wise AI advisors augment your favoured strategy?</p><p>&nbsp;</p><p><i>Stronger, but more speculative </i>ğŸ•µï¸ğŸ§©: Forget marginal improvements, <strong>wisdom tech could provide the missing piece for another strategy.</strong> It seems quite likely that there are strategies where we'd inevitably fail with purely raw, unaided humans trying to pull it off<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"68\" data-footnote-id=\"2pnutp0bsqr\" role=\"doc-noteref\" id=\"fnref2pnutp0bsqr\"><sup><a href=\"#fn2pnutp0bsqr\">[68]</a></sup></span>, but where we could scrape through if we deployed (wise) AI advisors in the right way.&nbsp;</p></blockquote></td></tr><tr><td style=\"border-style:none;padding:0px;text-align:center\">â‰</td></tr><tr><td style=\"border-style:none\"><p><br>iii:<strong> ğŸ§‘â€ğŸ”¬ğŸ§­ğŸŒ… </strong>Wisdom tech seems favourable from a <a href=\"https://michaelnotebook.com/dtd/\">differential technology development</a> perspective. <strong>Most AI technology differentially advantages \"reckless and unwise\"</strong> <strong>since \"responsible &amp; wise\" actors</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"69\" data-footnote-id=\"t1hzunqf8pq\" role=\"doc-noteref\" id=\"fnreft1hzunqf8pq\"><sup><a href=\"#fnt1hzunqf8pq\">[69]</a></sup></span><strong>&nbsp;need more time to figure out how to deploy</strong> a technology. There's a limit to how much we can speed up human processes, but wise AI advisors could likely reduce this time lag further<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"70\" data-footnote-id=\"o18fnks9cyd\" role=\"doc-noteref\" id=\"fnrefo18fnks9cyd\"><sup><a href=\"#fno18fnks9cyd\">[70]</a></sup></span>. There's also a reasonable chance than <strong>wisdom tech allows \"reckless and unwise\" actors to realise their own foolishness</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"71\" data-footnote-id=\"s8y91xgk2j\" role=\"doc-noteref\" id=\"fnrefs8y91xgk2j\"><sup><a href=\"#fns8y91xgk2j\">[71]</a></sup></span>. For those who favour openness, wisdom tech might be safer to distribute.<br>&nbsp;</p><blockquote><p>&nbsp;<strong>Q: </strong>What aspect of wisdom would be most beneficial from a differential technology development perspective?</p><p><i>Stronger, but more speculative </i>ğŸ¦‰ğŸ’¥: Forget nudging technological development around the edges, if intelligence/capabilities tech differentially advantages irresponsible and unwise actors due to its very nature, <strong>perhaps pursuing an intelligence explosion is actualy a fatal mistake</strong>? <strong>Could pursuing a </strong><a href=\"https://aiimpacts.org/some-preliminary-notes-on-the-promise-of-a-wisdom-explosion/\"><strong>wisdom explosion</strong></a><strong> instead break this trend</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"72\" data-footnote-id=\"5g9p7o2gllf\" role=\"doc-noteref\" id=\"fnref5g9p7o2gllf\"><sup><a href=\"#fn5g9p7o2gllf\">[72]</a></sup></span>?</p></blockquote><p>&nbsp;</p></td></tr></tbody></table>\n\nğŸ Not persuaded yet? Here's three more bonus points :\n\n*   ğŸ¦‹ğŸŒªï¸: **Even modest boosts in wisdom could be significant. Single decisions often shape the course of history**. Altering the right decision might be the difference between avoiding and experiencing a catastrophe. Similarly, there are individual actors who could dramatically change the strategic situation by deciding to start acting responsibly.\n*   ğŸš§ğŸŒ±: **Progress on the core part of the alignment problem seems to have stalled. They say insanity is doing the same thing over and over again without success, so perhaps we should be looking for new framings?** The wisdom framing seems quite underexplored and potentially fruitful to me[^jp6d03wlda]. It's not at all obvious that we want our technology to be intelligent more than we want it to be wise[^kar5n6c7ooo]. Furthermore, considering how to steer the world using cybernetic systems involving both humans and AI provides a generalisation of the alignment problem. **Maybe neither of these frames will ultimately make sense**, but even if that's the case, I suspect that **understanding precisely why they aren't fruitful would be valuable**[^zf8veqike3c].\n*   ğŸ¦â€â¬›ğŸ¦‰: Even if the most ambitious parts of this proposal fail on a concrete level, **training up a bunch of folks with both a deep understanding of the alignment problem** and wisdom seems pretty darn valuable. Even if we don't know exactly what these people might do, it **feels like a robustly useful skillset** for the community to be developing[^3ykcny8ifoz].\n\nI believe that this proposal is particularly promising because it has so many different plausible theories of change. It's hard to know in advance what assumptions will or will not pan out\n\n*You might also be interested in my draft postÂ * [`N Stories of Impact for Wise AI Advisors ğŸ—ï¸`](https://docs.google.com/document/d/15-n-EYeyAsbf2TETCQS9jHHAOwmy-TOW3ubVf_z5DvU/edit?tab=t.0) *which attempts to cleanly seperate out the various possible theories of impact.*\n\n+++ **â˜ I wasn't sold before, but I am now. How can I get involved? ğŸ¥³ğŸ**\n\nIf you're serious, pleaseÂ  [`**âœ‰ï¸** PM me`](https://www.lesswrong.com/users/chris_leong) . If you think you might be serious, but need to talk it through, please reach out as well.\n\nI'd be useful for me to know your background and how you think you could contribute. Maybe tell you a few facts about yourself, what interests you and drop a link to your LinkedIn profile?\n\nIf you scroll down, you'll see that I've answered some more questions about getting involved, but I'll include some useful links here as well:\n\nâ€¢Â  [`List of potentially useful projects`](https://www.lesswrong.com/posts/24RvfBgyt72XzfEYS/potentially-useful-projects-in-wise-ai) : for those who want to know what could be done concretely. Scroll down further for a list of project lists â¬‡ï¸.  \nâ€¢Â  [`Resources for founding an AI safety startup or non-profit`](https://www.lesswrong.com/w/ai-safety-and-entrepreneurship) : since I believe there should be multiple organisations pursuing this agenda  \nâ€¢Â  [`Resources for getting started in AI Safety more broadly`](https://www.aisafety.com/) : since some of these resources might be useful here as well\n\n\n\n\n+++\n\n<table style=\"background-color:rgb(255, 255, 255);border-color:rgb(0, 0, 0)\"><tbody><tr><td style=\"background-color:#a3c3e6;border-color:rgb(217, 217, 217);border-style:double;padding:0.4em;text-align:justify\"><strong>âœ‹ But this is still all so abstract? ğŸ§˜ğŸ‘©â€ğŸ¼</strong></td></tr><tr><td style=\"background-color:#d8e9ff;border-color:rgb(217, 217, 217);border-style:double;padding:0.4em\"><p>Now that we've clarified some of the possible theories of impact, the next section will delve more into specific approaches and projects, including listing some (hopefully) <a href=\"https://www.lesswrong.com/posts/24RvfBgyt72XzfEYS/potentially-useful-projects-in-wise-ai\">robustly beneficial projects</a> that could be pursued in this space.</p><p>I also intend for some of my future work to be focused on making things more concrete. As an example, I'm hoping to spend some time during my ERA fellowship attempting to clarify what kinds of wisdom are most important for steering the world in positive directions.</p><p>So whilst I think it is important to make this proposal more concrete, I'm not going to rush. Doing things well is often better than doing them as fast as possible. It took a long time for AI safety to move from abstract theoretical discussions to concrete empirical research and I expect it'll also take some time for these ideas to mature<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"77\" data-footnote-id=\"b39jysqn1r4\" role=\"doc-noteref\" id=\"fnrefb39jysqn1r4\"><sup><a href=\"#fnb39jysqn1r4\">[77]</a></sup></span>.</p></td></tr></tbody></table>\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n### **ğŸŒ± What can I do? Is this viable? Is this useful?**\n\n+++ **â˜ Do you have specific projects that might be useful? -Â ** [`**ğŸ“šMain List**`](https://www.lesswrong.com/posts/24RvfBgyt72XzfEYS/potentially-useful-projects-in-wise-ai) or expand for other lists\n\nYes, I created a list:Â  [`Potentially Useful Projects in Wise AI`](https://www.lesswrong.com/posts/24RvfBgyt72XzfEYS/potentially-useful-projects-in-wise-ai) . It contains a variety of projects from ones that would be marginally helpful to incredibly ambitious moonshots.\n\n**What other project lists exist?**\n\nHere are some project lists (or larger resources containing a project list) for wise AI or related areas:\n\nâ€¢Â  [`AI Impacts Essay Competition`](https://blog.aiimpacts.org/p/essay-competition-on-the-automation#footnote-anchor-1-143374374) : Covers the automation of philosophy and wisdom.\n\nâ€¢Â  [`Fellowship on AI for Human Reasoning - Future of Life Foundation`](https://www.flf.org/fellowship) : AI tools for coordination and epistemics\n\nâ€¢Â  [`AI for Epistemics - Benjamin Todd`](https://80000hours.org/2024/05/project-idea-ai-for-epistemics/) \\- He writes: \"The ideal founding team would cover the bases of: (i) forecasting / decision-making expertise (ii) AI expertise (iii) product and entrepreneurial skills and (iv) knowledge of an initial user-type. Though bear in mind that if you have a gap in one of these areas now, you could probably fill it within a year\" and then provides a list of projects.\n\nâ€¢Â  [`Project ideas: Epistemics - Forethought`](https://www.forethought.org/research/project-ideas-epistemics) \\- focuses on improving epistemics in general, not just AI solutions.\n\n\n\n\n++++++ **Do you have any advice for creating a startup in this space? -Â ** [`**ğŸ“šResources**`](https://www.lesswrong.com/w/ai-safety-and-entrepreneurship)\n\nSee  theÂ  [`AI Safety & Entrepreneurship wiki page`](https://www.lesswrong.com/w/ai-safety-and-entrepreneurship) for resources including articles, incubation programs, fiscal sponsorship and funding.\n\n\n\n\n++++++ **Is it really useful to make AI incrementally wiser? âœ”ï¸ğŸ’¯**\n\nAI being incrementally wiser might still be the difference between making a correct or incorrect decision at a key point.\n\nOften several incremental improvements stack on top of each other, leading to a major advance.\n\nFurther, we can ask AI advisors to advise us on more ambitious projects to train wise AI. And the wiser our initial AI advisors are, the more likely this is to go well. That is, improvements that are initially incremental might be able to be leveraged to gain further improvements.\n\nIn the best case, this might kick off a (strong) [positive feedback cycle (aka wisdom explosion)](https://aiimpacts.org/some-preliminary-notes-on-the-promise-of-a-wisdom-explosion/).\n\n\n\n\n++++++ **Sure, you can make incremental progress. But is it really possible to train an AI to be wise in any deep way? ğŸ¤âœ”ï¸ğŸ¤·**\n\nPossibly ğŸ¤·.\n\nI'm not convinced it's harder than any of the other ambitious alignment agendas and we won't know how far we can go without giving it a serious effort. Is training an AI to be wise really harder than aligning it? If anything, it seems like a less stringent requirement.  \n  \nCompare:  \nâ€¢ Ambitious mechanistic interpretability aims to perfectly understand how a neural network works at the level of individual weights  \nâ€¢ Agent foundations attempting to truly understand what concepts like agency, optimisation, decisions are values are at a fundamental level  \nâ€¢ Davidad's Open Agency architecture attempting train AI's that come with proof certificates that an AI has less than a certain probability of having unwanted side-effects\n\nIs it obvious that any of these are easier than training a truly wise AI advisor? I can't answer for you, but it isn't obvious to me.\n\nGiven the stakes, I think it is worth pursuing ambitious agendas anyway. Even if you think timelines are short, it's hard to justify holding a probability approaching 100%, so it makes sense for folk to be pursuing plans on different timelines.\n\n\n\n\n++++++ **I understand your point about all ambitious alignment proposals being extremely challenging, but do you have any specific ideas for training AI to be deeply wise (even if speculative)? âœ…**\n\n*This section provides a high-level overview of some of my more speculative ideas. Whilst these ideas are very far from fully developed, I nonetheless thought it was worthwhile sharing an early version of them. That said, I'm very much on the set of \"let a hundred flowers bloom\". Whilst I think the approach I propose is promising, I also think it's much more likely than not that someone else comes up with an even better idea.*\n\nI believe that imitation learning is greatly underrated. I have a strong intuition that using the standard ML approach for training wisdom will fail because of the traditional Goodhart's law reasons, except that it'll be worse because wisdom is such a fuzzy thing (Who the hell knows what wisdom really is?).\n\nI feel that training a deeply wise AI system requires an objective function that we can optimise hard on. This naturally draws me towards imitation learning. It has its flaws, but it certainly seems like we could optimise much harder on an imitation function than with attempting to train wisdom directly.\n\nNow, imitation learning is often thought of as weak and there's certainly some truth in this when we're just talking about an initial imitation model. However, this isn't a cap on the power of imitation learning, only a floor. There's nothing stopping us from training a bunch of such models and using amplification techniques such as debate, trees of agents or even [iterated distillation and amplification](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616). I expect there to be many other such techniques for amplifying your initial imitation models.\n\nSeeÂ  [`An Overview of \"Obvious\" Approaches to Training Wise AI Advisors`](https://aiimpacts.org/an-overview-of-obvious-approaches-to-training-wise-ai-advisors/) for more information. This post was the first half of my entry into the AI Impacts Essay Competition on the Automation of Wisdom and Philosophy which [placed third](https://aiimpacts.org/an-overview-of-obvious-approaches-to-training-wise-ai-advisors/).\n\n\n\n\n++++++ **Why mightn't I want to work on this?**\n\nI think it's important to consider whether you might not be the right person to make progress on this.\n\nI mean â€” wisdom? Who the hell knows what that is?\n\nI expect most people who try to gain traction on this to just end up confusing themselves and others. So, you probably shouldn't work on this unless you're a particularly good fit.\n\nOn the other hand, there are very strong selection effects where those who are fools think they are wise and those who are wise doubt their own wisdom.\n\nI wish I had a good answer here. All I can say is to avoid both arrogance and performative modesty when reflecting on this question.\n\n\n\n\n++++++ **Many of the methods you've proposed are non-embodied. Doesn't wisdom require embodiment? ğŸ¤”âŒğŸ¤·**\n\nThere's a very simplistic version argument for this that [proves too much](https://www.lesswrong.com/posts/G5eMM3Wp3hbCuKKPE/proving-too-much): people have used this claim to argue that LLMs were never going to be able to learn to reason, whilst o3 seems to have [conclusively disproven this](https://arcprize.org/blog/oai-o3-pub-breakthrough). So I don't think that the standard version of this argument works.  \n  \nIt's also worth noting that in robotics, there are examples of [zero-shot transfer to the real world](https://www.reddit.com/r/singularity/comments/1kim2ec/jim_fan_says_nvidia_trained_humanoid_robots_to/). Even if wisdom isn't directly analogous, this suggests that large amounts of real-world experience might be less crucial than it appears at first glance.\n\nAll this said, I find it plausible that a more sophisticated version of this argument might post a greater challenge. Even if this were the case, I see no reason why we couldn't combine both embodied and non-embodied methods. So even if there was a more sophisticated version that ultimately turned out to be true, this still wouldn't demonstrated that research into non-embodied methods was pointless.\n\n\n\n\n+++\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n### **âœï¸ Conclusion:**\n\n**Coming soon ğŸ˜‰.**\n\n+++ **â˜ Okay, you've convinced me now. Any chance you could repeat how to get involved so I don't have to scroll up? âœ…ğŸ“š**\n\nSure ğŸ˜Š. Round ~1ï¸âƒ£~2ï¸âƒ£ ğŸ””, here we go ğŸª‚!\n\nAs I said, if you're serious, pleaseÂ  [`**âœ‰ï¸** PM me`](https://www.lesswrong.com/users/chris_leong) . If you think you might be serious, but need to talk it through, please reach out as well.\n\nI'd be useful for me to know your background and how you think you could contribute. Maybe tell you a few facts about yourself, what interests you and drop a link to your LinkedIn profile?\n\nUseful link:\n\nâ€¢Â  [`List of potentially useful projects`](https://www.lesswrong.com/posts/24RvfBgyt72XzfEYS/potentially-useful-projects-in-wise-ai) : for those who want to know what could be done concretely. Scroll down further for a list of project lists â¬‡ï¸.  \nâ€¢Â  [`Resources for founding an AI safety startup or non-profit`](https://www.lesswrong.com/w/ai-safety-and-entrepreneurship) : since I believe there should be multiple organisations pursuing this agenda  \nâ€¢Â  [`Resources for getting started in AI Safety more broadly`](https://www.aisafety.com/) : since some of these resources might be useful here as well\n\n\n\n\n+++\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n### **ğŸ“˜ Appendix:**\n\n+++ **Who are you? ğŸ‘‹**\n\nHi, I'm Chris (Leong). I've studied maths, computer science, philosophy and psychology. I've been interested in AI safety for nearly a decade and I've participated in quite a large number of related opportunities.\n\n**With regards to the intersection of AI safety and wisdom:**\n\nI won third prize in theÂ  [`AI Impacts Competition on the Automation of Wisdom and Philosophy`](https://www.lesswrong.com/posts/hiuTzNBqG2EYg6qM5/winners-of-the-essay-competition-on-the-automation-of-wisdom) . It's divided into two parts:\n\nâ€¢Â  [`An Overview of â€œObviousâ€ Approaches to Training Wise AI Advisors`](https://aiimpacts.org/an-overview-of-obvious-approaches-to-training-wise-ai-advisors/)\n\nâ€¢Â  [`Some Preliminary Notes on the Promise of a Wisdom Explosion`](https://aiimpacts.org/some-preliminary-notes-on-the-promise-of-a-wisdom-explosion/)\n\nI recently ran anÂ  [`AI Safety Camp project on Wise AI Advisors`](https://www.aisafety.camp/#h.4tl51mi26u2m) .\n\nMore recently, I was invited to continue my research as a [ERA Cambridge Fellow](https://erafellowship.org/) in Technical AI Governance.\n\nFeel free to connect with me onÂ  [`LinkedIn`](https://www.linkedin.com/in/casebash/) (ideally mentioning your interest in wise AI advisors so I know to accept) orÂ  [`**âœ‰ï¸** PM me`](https://www.lesswrong.com/users/chris_leong) . Â \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\nHi, I'm Christopher Clay. I was previously a Non-Trivial Fellow and I participated in the Global Challenges Project.\n\n\n\n\n++++++ **Does anyone else think wise AI or wise AI advisors are important? âœ”ï¸ğŸ’¯**\n\nYes, here are a few:\n\n**Imagining and Building Wise Machines: The centrality of AI metacognitionÂ **\n\n[`Â original paperÂ `](https://arxiv.org/pdf/2411.02478)Â  [`summary`](https://www.lesswrong.com/posts/euAMyQAQWTYyWZW8Z/summary-imagining-and-building-wise-machines-the-centrality)\n\n**Authors:** Yoshua Bengio, Igor Grossmann, Melanie Mitchell and Samuel Johnson[^juibjpdlbp]Â et al\n\n> Wise metacognition can lead to a virtuous cycle in AI, just as it does in humans. We may not know precisely what form wise AI will takeâ€”but it must surely be preferable to folly.\n\nThe paper mostly focuses on wise AI agents, however, not exclusively so:\n\n> Prof. Grossmann (personal correspondence): \"I like the idea of wise advisors. I don't think the argument in our paper is against it -it all depends on how humans will use the technology (and there are several papers on the role of metacognition for discerning when to rely on decision-aids/AI advisors, too).\"\n\n**AI Impacts Automation of Wisdom and Philosophy CompetitionÂ **\n\n**Organised by:** Owen Cotton-Barrett  \n**Judges:** Andreas StuhlmÃ¼ller, Linh Chi Nguyen, Bradford Saad and David Manley[^bn3g0btbhcu]\n\n[`Â competition announcementÂ `](https://blog.aiimpacts.org/p/essay-competition-on-the-automation)Â  [`Wise AI Wednesdays post`](https://www.alignmentforum.org/posts/WBMcKgrTpmniTK8HG/untitled-draft-bgxq)Â  [`winners and judge's comments`](https://www.lesswrong.com/posts/hiuTzNBqG2EYg6qM5/winners-of-the-essay-competition-on-the-automation-of-wisdom)[^zjoz3gawj2p]\n\n> AI is likely to automate more and more categories of thinking with time.\n> \n> By default, the direction the world goes in will be a result of the choices people make, and these choices will be informed by the best thinking available to them. People systematically make better, wiser choices when they understand more about issues, and when they are advised by deep and wise thinking.\n> \n> Advanced AI will reshape the world, and create many new situations with potentially high-stakes decisions for people to make. To what degree people will understand these situations well enough to make wise choices remains to be seen. To some extent this will depend on how much good human thinking is devoted to these questions; but at some point it will probably depend crucially on how advanced, reliable, and widespread the automation of high-quality thinking about novel situations is[^orupv2e418s]\n\n**Beyond Artificial Intelligence (AI): Exploring Artificial Wisdom (AW)**  \n**Authors:** Dilip V Jest, Sarah A Graham, Tanya T Nguyen, Colin A Depp, Ellen E Lee, Ho-Cheol Kim\n\n[`Â paperÂ `](https://pubmed.ncbi.nlm.nih.gov/32583762/)\n\n> The ultimate goal of artificial intelligence (AI) is the development of technology that is best able to serve humanity and this will require advancements that go beyond the basic components of general intelligence. The term â€œintelligenceâ€ does not best represent the technological needs of advancing society, because intelligence alone does not guarantee well-being either for individuals or societies. It is not intelligence, but wisdom, that is associated with greater well-being, happiness, health, and perhaps even longevity of the individual and the society. Thus, the future need in technology is for artificial wisdom (AW)\n\n\n\n\n++++++ **What do you see as the strongest counterarguments against this general direction? ğŸ”œğŸ™**\n\nI've discussed several possible counter-arguments above, but I'm going to hold off publicly posting about which ones seem strongest to me for now. I'm hoping this will increase diversity of thought and [nerdsnipe](https://xkcd.com/356/) people into helping red-team my proposal for me ğŸ§‘â€ğŸ”¬ğŸ‘©â€ğŸ”¬ğŸ¯. Sometimes with feedback there's a risk where if you say, \"I'm particularly worried about these particular counter-arguments\" you can redirect too much of the discussion onto those particular points, at the expense of other, perhaps stronger criticisms.\n\n\n\n\n++++++ **â˜ Do you have any recommendations for further reading? âœ…ğŸ“š**\n\nI've created a weekly post series on Less Wrong calledÂ  [`Wise AI Wednesdays`](https://www.lesswrong.com/s/JqH7ZkqSjKrTk9CpH) .\n\n**Motivation**\n\n*â€˜AI for societal upliftâ€™ as a path to victory* \\- [`LW Post`](https://www.lesswrong.com/posts/kvZyCJ4qMihiJpfCr/ai-for-societal-uplift-as-a-path-to-victory): Examines the conditions in which a \"societal uplift\" - epistemics + co-ordination + institutional steering - might or might not lead to positive outcomes\n\n*N Stories of Impact for Wise AI Advisors* \\- [`Draft ğŸ—ï¸`](https://docs.google.com/document/d/15-n-EYeyAsbf2TETCQS9jHHAOwmy-TOW3ubVf_z5DvU/edit?tab=t.0) : Different stories about how wise AI advisors could be useful for having a positive impact on the world.\n\n**Artificial Wisdom**\n\n*Imagining and building wise machines: The centrality of AI metacognition by Johnson, Karimi, Bengio, et al.* **-** [`paper`](https://arxiv.org/pdf/2411.02478) ,Â  [`summary`](https://www.lesswrong.com/posts/euAMyQAQWTYyWZW8Z/summary-imagining-and-building-wise-machines-the-centrality) : This paper argues that wisdom involves two kinds of strategies (task-level strategies & metacognitive strategies). Since current AI is pretty good at the former, they argue that we should pursue the latter as a path to increasing AI wisdom.\n\n*Finding the Wisdom to Build Safe AI by Gordon Seidoh Worley* -Â  [`LW post`](https://www.lesswrong.com/posts/pchAuTJhfKJS2s8it/finding-the-wisdom-to-build-safe-ai) : Seidoh talks about his own journey in becoming wiser through Zen and outlines a plan for building wise AI. In particular, he argues that it will be hard to produce wise AI without having a wise person to evaluate it.\n\n*Designing Artificial Wisdom: The Wise Workflow Research Organisation by Jordan Arel* -Â  [`EA forum post`](https://forum.effectivealtruism.org/posts/z5pKtbjpK3wZWbSmk/designing-artificial-wisdom-the-wise-workflow-research) : Jordan proposes mapping the workflows within an organisation that is researching a topic like AI safety or existential risk. AI could be used to automate or augment parts of their work. This proportion would increase over time, with the hope being that this would eventually allow us to fully bootstrap an artificially wise system.\n\n*Should we just be building more datasets? by Gabriel Recchia* -Â  [`Substack`](https://thediscontinuity.substack.com/p/should-we-just-be-building-more-datasets) : Argues that an underrated way of increasing the wisdom of AI systems would be building more datasets (whilst also acknowledging the risks).\n\n*Tentatively Against Making AIs 'wise' by Oscar Delany* -Â  [`EA forum post`](https://forum.effectivealtruism.org/posts/tuAnrhy2BExGkwTKE/tentatively-against-making-ais-wise) : This articles argues that insofar as wisdom is conceived of as being more intuitive than carefully reasoned pursuing AI wisdom would be a mistake as we need AI reasoning to be transparent. I've included this because it seems valuable to have at least one critical article.\n\n**Neighbouring Areas of Research**\n\n*What's Important In \"AI for Epistemics\"?* *by Lukas Finnveden -Â * [`Forethought`](https://www.forethought.org/research/whats-important-in-ai-for-epistemics) : AI for Epistemics is a subtly different but overlapping area. It is close enough that this article is worth reading. It provides an overview of why you might want to work on this, heuristics for good interventions and concrete projects.\n\n*AI for AI Safety by Joe Carlsmith* ( [`LW post`](https://www.lesswrong.com/posts/F3j4xqpxjxgQD3xXh/ai-for-ai-safety) ): Provides a strategic analysis of why AI for AI safety is important whether it's for making direct safety progress, evaluating risks, restraining capabilities or improving \"backdrop capacity\". Great diagrams.\n\n*AI Tools for Existential Security* *by Lizka Vaintrob and Owen Cotton-Barratt --Â * [`Forethought`](https://www.forethought.org/research/ai-tools-for-existential-security) : Discusses how applications of AI can be used to reduce existential risks and suggests strategic implications.\n\n*Not Superintelligence*: *Supercoordination* -Â  [`forum post`](https://sofiechan.com/p/2513)[^6kkzb5xhbu]: This article suggests that software-mediated supercoordination could be beneficial for steering the world in positive directions, but also identifies the possibility of this ending up as a \"horrorshow\".\n\n**Human Wisdom**\n\n*Stanford Encyclopedia of Philosophy Article on Wisdom by Sharon Ryan* -Â  [`SEP article`](https://plato.stanford.edu/entries/wisdom/) : SEP articles tend to be excellent, but also long and complicated. In contrast, this article maintains the excellence while being short and accessible.\n\n*Thirty Years of Psychology Wisdom Research: What We Know About the Correlates of an Ancient Concept by Dong, Weststrate and Fournier* -Â  [`paper`](https://journals.sagepub.com/doi/10.1177/17456916221114096) : Provides  an excellent overview of how different groups within psychology view wisdom.\n\n*The Quest for Artificial Wisdom by Sevilla -Â * [`paper`](https://link.springer.com/article/10.1007/s00146-012-0390-6) : This article outlines how wisdom is viewed in the Contemplative Sciences discipline. It has some discussion of how to apply this to AI, but much of this discussion seems outdated in light of the deep learning paradigm.\n\n**Applications to Governance**\n\nğŸ† *Wise AI support for government decision-making by Ashwin* -Â  [`Substack`](https://www.expectedsurprise.com/p/wise-ai-support-for-government-decision) (Prize winning entry in theÂ  [`AI Impacts Automation of Wisdom and Philosophy Competition`](https://blog.aiimpacts.org/p/winners-of-the-essay-competition) ): This article convinced me that it isn't too early to start trying to engage the government on wise AI. In particular, Ashwin considers the example of automating the Delphi process. He argues that even though you might begin by automating parts of the process, over time you could expand beyond this, for example, by helping the organisers figure out what questions they should be asking.\n\n**Some of my own work**:\n\nğŸ† My third prize-winning entry in theÂ  [`AI Impacts Automation of Wisdom and Philosophy Competition`](https://blog.aiimpacts.org/p/winners-of-the-essay-competition) (split into two parts):\n\n[`Some Preliminary Notes on the Promise of a Wisdom Explosion`](https://aiimpacts.org/some-preliminary-notes-on-the-promise-of-a-wisdom-explosion/) : Defines a wisdom explosion as a recursive self-improvement feedback loop that enhances wisdom, unlike intelligence as per the more traditional intelligence explosion. Argues that wisdom tech is safer from a differential technology perspective.\n\n[`An Overview of \"Obvious\" Approaches to Training Wise AI Advisors`](https://aiimpacts.org/an-overview-of-obvious-approaches-to-training-wise-ai-advisors/) : Compares four different high-level approaches to training wise AI: direct training, imitation learning, attempting to understand what wisdom is at a deep principled level, the scattergun approach. One of the competition judges wrote: \"I can imagine this being a handy resource to look at when thinking about how to train wisdom, both as a starting point, a refresher, and to double-check that one hasnâ€™t forgotten anything important\".\n\n\n\n\n++++++ **â˜ What are some projects that exist in this space? â€” TODO**\n\n[`Â Automating the Delphi Method for Safety CasesÂ `](https://www.aipolicybulletin.org/articles/making-the-case-how-can-we-know-when-ai-is-safe-7nglg) â€” Philip Fox, Ketana Krishna, Tuneer Mondal, Ben Smith, Alejandro Tlaie Â â€” Ashwin and Michaelah Gertz-Billingsley proposed automating the Delphi Mehod as a foot-in-the door for getting the government to use wise AI. Well, turns out these folk at Arcadia Impact have done it!\n\n\n\n\n+++\n\n[^jjkppxnxn9l]: â‡¢ \"What's an metapost?\" â€” Oh, I just made that term up. Essentially, it's just a megapost, but instead of being one long piece of text, it uses collapsible sections to keep the core post lightweight ğŸª¶. Â In other words, it's both a megapost and a minipost at the same time ğŸ˜‰.â‡¢ \"But people will confuse this with a meta post?\" â€” No space. Simple ğŸ¤·â€â™‚ï¸.â‡¢ This term happens to also be appropriate in a second sense; some aspects are inspired by metamodernism. \n\n[^fpemp7m5g1f]: â‡¢ I jokingly refer to this as the \"Party Edition\" as it's designed to be shared at informal social gatherings ğŸ•º.â‡¢ \"Party edition? Why the hell would you make a party edition!?\" â€” I could you tell a story about how this will actually be impactful (HPMoR; Siliconversations; spreading ideas by going from person to person being massively underrated), but at the end of the day, I wanted to make it, so I made it ğŸ¤·â€â™‚ï¸.â‡¢ Â \"But what is it?\" â€” I've handcrafted this version for more casual contexts. I've tried to be more authentic and I hope you find it more engaging. That said, these are serious issues, so I'm also creating a \"serious edition\" for situations for contexts where sharing a post like this might come off as disrespectful ğŸ©. Otherwise: ğŸ¥³.â‡¢ \"That makes sense, but are the lame jokes really necessary?\" â€” Â Yes, they are most necessary. \"When the novice attained the rank of grad student, he took the name Bouzo and would only discuss rationality while wearing a clown suit\" ğŸ¤¡ğŸ© \n\n[^v1c3p35za3]: \"Why put so much effort into a Less Wrong post?\" â€” ğŸŸ¥ğŸ‰ \n\n[^1o1w2qhebrt]: \"Regardless\" â€” Yes. One must imagine Sisyphus happy... â¤ï¸ğŸ¦‰. \n\n[^soymig3bk6]: â‡¢ Â ğŸ”¥ğŸ‘€ğŸ â€” I remember reading about how, in The Lord of the Rings, they even took the time to inscribe some writing inside someone's armor that could never be seen on camera. I found this inspiring; as a demonstration of what true dedication to art looks like â€” Â â–¶ï¸â‡¢ Complete tangent but... \n\n[^7uu5r9iyvrp]: â‡¢ \"I see in your eyes the same fear that would take the heart of me. A day may come when the courage of men fails, when we forsake our friends and break all bonds of fellowship, but it is not this day. An hour of wolves and shattered shields, when the age of men comes crashing down, but it is not this day! This day we fight!! By all that you hold dear on this good Earth, I bid you stand\" ğŸ’ğŸŒ‹ğŸ›¡ï¸ â€” â–¶ï¸â‡¢ \"If we as humans ever face extinction via alien invasion, I nominate Aragorn to come to life and lead mankind\" â€” @jlop6822 (Youtube comment) \n\n[^db0uqr4co4]: \"If I see a situation pointed south, I can't ignore it. Sometimes I wish I could\" â€” Steve Rogers ğŸ›¡ï¸ğŸ‡ºğŸ‡¸ \n\n[^phthi2c7tmi]: It's really just this version that is the passion project. The \"Formal Version\" is necessary and the right version for certain contexts, but that format also loses something ğŸ’”. \n\n[^j5fcv0ia4b]: \"We will call this discourse, oscillating between a modern enthusiasm and a postmodern irony, metamodernism... New generations of artists increasingly abandon the aesthetic precepts of deconstruction, parataxis, and pastiche in favor of aesth-ethical notions of reconstruction, myth, and metaxis... History, it seems, is moving rapidly beyond its all too hastily proclaimed end\" â€” Â Notes on metamodernism \n\n[^07gb5uq7naii]: I'm using the the term \"alignment proposal\" in an extremely general sense, particularly \"how we could save the world\", rather than the narrow sense of achieving good outcomes by specifically aligning models. That said, wise AI advisors could assist with aligning models and they also provide us with a generalised version of the alignment problem (discussed later in this post) ğŸŒ â†’ ğŸŒ. \n\n[^n8q4p9itm8o]: \"It is only as an aesthetic phenomenon that existence and the world are eternally justified\" â€” â¤ï¸ğŸ¦‰ğŸ”¨. \n\n[^54slr2rt88c]: Reality does not grade on a curve ğŸš€ğŸ°ğŸ’¥ \n\n[^gvunovlt2mc]: Is naiviety always bad? Is there some kind of universal rule? Or are there exceptions? Perhaps there are situations where a certain form of naiviety can form a self-fulfilling prophecy (or hyperstition) ğŸ”®ğŸª. \n\n[^0e7h2m5mcwxf]: \"Of all that is written, I love only what a man has written with his blood. Write with blood, and you will experience that blood is spirit\" â€” â¤ï¸ğŸ¦‰ğŸ”¨. \n\n[^c6alwvabodu]: Unfortunately, I've forgotten the names of many people who provided me with useful feedback ğŸ˜”. \n\n[^mfip7m73mqq]: ğŸ‡ºğŸ‡¸ğŸš€ â€” ğŸ†: ğŸ“½ï¸ğŸ­ğŸ¦¾ğŸ¬ğŸï¸ğŸ¥ğŸ¼ â€” â–¶ï¸ \n\n[^z6b96rz3izh]: Commenting on the parallels between the Manhattan project and AI on This Past Weekend with Theo Von â˜¢ï¸ğŸ¤–. \n\n[^x18prkmell]: Source: NeurIPSâ€™2019 workshop on Fairness and Ethics, Vancouver, BC, On the Wisdom Race, December 13th, 2019 ğŸ¦‰ğŸï¸ğŸ \n\n[^zgmg02kaeoa]: One reason why this arrangement of quotes amuses me is because the Sam Altman quote was actually in response to the host specifically asking Sam about what he thought about Yoshua Bengio's concerns ğŸ˜‚. \n\n[^jzpd115xfjs]: \"Centuries\" â€” I wish ğŸ˜­. \n\n[^2nxmmdl2c84]: ğŸ§—â€â™€ï¸ \n\n[^5tjuz2uesxw]: ğŸ—¿ğŸ‘‘â˜¢ï¸ \n\n[^lazgfieyhti]: ğŸ‘¦ğŸ’£ğŸ’¥ \n\n[^50qt1zbgixv]: ğŸ“º: ğŸªğŸ”­ \n\n[^jbnzpzrhqm]: â›ª: ğŸ¤²ğŸ¥£ â†’ ğŸ¦ğŸŒ± \n\n[^wj9aaiu9ky]: ğŸ‡ºğŸ‡³: ğŸ‘¨â€ğŸ’», â­â­â­â­â­ \n\n[^6uvrmixpsg]: ğŸ‘¨â€ğŸ”¬ğŸ‘¨â€ğŸ¦¼: ğŸŒ¬ï¸ğŸ•³ï¸ \n\n[^faq358rz42w]: Salvia Ego Ipse, Philosopher of AI, Liber magnus, sapiens et praetiosus philosophiae Ã· \n\n[^n91qdvd9u2]: \"But what if an equivalent increase in wisdom turns out to be impossible?\" â€” This is left as an exercise to the reader ğŸ˜±. \n\n[^5uuw3mc1psx]: I refer to this as the SUV Triad ğŸ«° \n\n[^jxddqbmy5t]: â‡¢ I refer to this the Wisdom-Capabilities Gap or simply the Wisdom Gap.â‡¢ \"Does it make sense to assume that greater capabilities require greater wisdom to handle?\" Â â€” Â Yes, I believe it does. I'll argue for this in the Core Argument ğŸ”œ. \n\n[^hegisbxg6nv]: I may still decide to weaken this claim. This is a draft post after all ğŸ™. \n\n[^o6fi0wr2k0c]: â‡¢ \"Then why is this section first?\": You might find it hard to believe, but there's actually folks who'd freak out if I didn't do this ğŸ”±ğŸ«£. Yep... ğŸ¤·â€â™‚ï¸â‡¢ I recommend diving straight into the core argument, and seeing if you agree with it when you substitute your own understanding of what wisdom is. I promise you ğŸ’, I'm not doing this arbitrarily. However, if you really want to read the definitions first, then you should feel free to just go for it ğŸ§˜ğŸ‘Œ. \n\n[^w9irh1p2hkr]: For example, see a and b. \n\n[^ydl8ctv68iq]: I also believe that this increases our chances of being able to set up a positive feedback loop ğŸ¤ğŸ¤ğŸ¤ of increasing wisdom (aka \"wisdom explosion\" ğŸ¦‰ğŸ†) compared to strategies that don't leverage humans. But I don't want to derail the discussion by delving into this right now ğŸ‡ğŸ•³ï¸. \n\n[^nusqvx99ufs]: I'd like to think of myself as being the kind of person who's sensible enough to avoid getting derailed by small or irrelevant details ğŸ¤ğŸ¤. Nonetheless, I can recall countless a few times when I've failed at this in the past ğŸš‡ğŸ’¥. Honestly, I wouldn't be surprised if many people were in the same boat ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸ§‘â€ğŸ¤â€ğŸ§‘ğŸŒğŸ¤. \n\n[^eeb22p5hq1q]: I'm using emojis to provide a visual indication of my answer without needing to expand the section ğŸ¦¸ğŸ©». \n\n[^qsh0j77epwp]: This post is being written collaboratively by Chris Leong (primary author) and Christopher Clay (second author) in the voice of Chris Leong ğŸ¤”ğŸ¤âœï¸. The blame for any cringe in the footnotes can be pinned on Chris Leong ğŸ¯. \n\n[^w90s7saq4is]: â‡¢ I believe that good communication involves being as open about the language game â¤ï¸ğŸ¦‰ being being played as possible ğŸ«¶.Given this, I want to acknowledge that this post is not intended to deliver a perfectly balanced analysis, but rather to lay out an optimistic caseÂ for wise AI advisors ğŸŒ…. In fact, feel free to think of it as a kind of manifesto ğŸ“–ğŸšªğŸ“œğŸ“Œ.I suspect that many, if not most, readers of Less Wrong will be worried that this must necessarily come at the expense of truth-seeking ğŸ§˜. In fact, I probably would have agree with this not too long ago. However, recently I've been resonating a lot more with the idea that there's value in all kinds of language games, so long as they are deployed in the right context and pursued in the right way ğŸ¤”[83]. Most communication shouldn't be a manifesto, but the optimal number of manifestos isn't zero (â—).â‡¢ Stone-cold rationality is important, but so is the ability to dream ğŸ˜´ğŸ’­ğŸ‘. It's important to be able to see clearly, but also to lay out a vision ğŸ”­âœ¨. These are in tension, sure, but not in contradiction âš–ï¸. Synthesis is hard, but not impossible ğŸ§—.â‡¢ If you're interested in further discussion, see the next footnote... \n\n[^3gphbn8unn5]: â‡¢ \"Two footnotes on the exact same point? This is madness!\": No, this is Sparta! ğŸŒâ›°ï¸ğŸ°ğŸ•³ï¸Spartans don't quit and neither do we ğŸ’ª. So here it is: Why it makes sense to lay out an optimistic case, Round ğŸ””ğŸ””.Who doesn't love a second bite at the apple? ğŸ‘¸ğŸ»ğŸğŸ’€ğŸ˜´â‡¢ Adopting a lens of optimism unlocks creativity by preventing you from discarding ideas too early (conversely, a lens of pessimism aids with red-teaming by preventing you from discarding objections too easily). It increases the chance that bad ideas make it through your initial filter, but these can always be filtered further down the line. I believe that such a lens is the right choice for now, given that this area of thoughtspace is quite neglected. Exploration is a higher priority than filtration ğŸŒ¬ï¸â›µğŸŒ±.â‡¢ \"But you're biasing people toward optimism\": I take this seriously ğŸ˜”ğŸ¤”. At the same time, I don't believe that readers need to be constantly handheld lest they form the wrong inference. Surely, it's worth risking some small amount of bias to avoid coming off as overly paternalistic?Should texts be primarily written for the benefit of those who can't apply critical thinking? To be honest, that idea feels rather strange to me ğŸ™ƒ. I think it's important to primarily write for the benefit of your target audience and I don't think that the readers I care about most are just going to absorb the text wholesale, without applying any further skepticism or cognitive processing ğŸ¤”. Trusting the reader has risks, sure, but I believe there are many people for whom this is both warranted and deserved ğŸ§­â›µğŸŒ. \n\n[^jcvwxbjkszm]: \"Why all the emojis?\": All blame belongs to Chris Leong ğŸ¯.\"Not who deserves to be shot, but why are you using them?\": A few different reasons: they let me bring back some of the expressiveness that's possible in verbal conversation, but which is typically absent in written communication ğŸ¥¹ğŸ˜±, and striking a more casual tone helps differentiate it from any more academic or rigorous treatment (ğŸ˜‰) that might hypothetically come later ğŸ¤ ğŸ›¤ï¸ğŸ“ and they help keep the reader interested ğŸ.But honestly: I mostly just feel drawn to the challenge. The frame \"both/and\" has been resonating a lot with me recently and one such way is that I've been feeling drawn towards trying to produce content that manages to be both fun and serious at the same time ğŸ¥³âš–ï¸ğŸ©. \n\n[^e2qrep2h1up]: I'm starting to wonder if this point is actually true. This is a NOTE TO MYSELF to review this claim. \n\n[^odckrfr0lxq]: \"Why can't we afford to leave anything on the table?\" - The task we face is extremely challenging, the stakes sky high, it's extremely unclear what would have to happen for AI to go well, we have no easy way on gaining clarity on what would have to happen and we're kind of in an AI arms race which basically precludes us from fully thinking everything through ğŸ˜…â³â°.Â  \n\n[^10pqzu117irp]: \"But what about prioritisation?\" â€” Think Wittgenstein. I'm asserting the need for a mindset, a way of being ğŸ§˜ğŸƒâ€â™‚ï¸ğŸ‹ï¸.\"But I don't understand you\" Â â€” If a lion could speak, we could not understand him â¤ï¸ğŸ¦‰ğŸ•µï¸. \n\n[^fuhaiw8462]: This section is currently being edited, so it may take more than 3 minutes. Sorry ğŸ™! \n\n[^cpn6zczbhj]: I selected this name because it's easy to say and remember, but in my heart it'll always be the MST/MSU/MSV trifecta (minimally spare time, massive strategic uncertainty, many scary vulnerabilities ğŸ’”ğŸš¢ğŸŒŠğŸ¶. I greatly appreciate Will MacAskill's recommendation that I simplify the name ğŸ™. \n\n[^67amztdujec]: I don't think I'm exactly arguing anything super original or unprecedented here ğŸ˜­ğŸ˜±, but I don't know if I've ever really seen anyone really go hard on this shape of argument before ğŸŒŠğŸŒŠğŸŒŠ. \n\n[^0zz8widkd88]: Some of these risks could directly cause catastrophe. Others might indirectly lead to this via undermining vital societal infrastructure such that we are then exposed to other threats ğŸ•·ï¸ğŸ•¸ï¸.Â  \n\n[^b8zxurs0z8v]: One of the biggest challenges here is that AI throws so many civilisational scale challenges towards us at once. Humanity can deal with civilisational scale challenges, but global co-ordination is hard and when there's so many different issues to deal with its hard to give each problem the proper attention it deserves ğŸ˜…ğŸŒ«ï¸ğŸ’£ğŸ’£ğŸ’£. \n\n[^lnirj25c27l]: I'm intentionally painting these questions as more of a binary than they actually are to help illustrate the range of opinions. \n\n[^ga8bmkazsy]: Some of this is deserved, imho ğŸ¤·â€â™‚ï¸. \n\n[^z2r37ct4sk]: In many ways, the figuring out how to act part of the problem is the much harder component ğŸ§­ğŸŒ«ï¸ğŸª¨. Given perfect co-ordination between nation states, these issues could almost certainly be resolved quite quickly, but given there's no agreement on what needs to be done and politics being the mind-killer, I wouldn't hold your breadth... ğŸ˜¯ğŸ˜¶ğŸŒğŸš€ğŸš€ğŸš€ \n\n[^4o4ceb7a29]: The exponential increase in task-length actually generalises beyond coding. \n\n[^evlbzg6lpak]: What's more impressive is how it was solved: \"Why am I excited about IMO results we just published: - we did very little IMO-specific work, we just keep training general models - all natural language proofs - no evaluation harness We needed a new research breakthrough and @alexwei_ and team delivered\"https://x.com/MillionInt/status/1946551400365994077 ğŸ¤¯ \n\n[^6mrrj9yx5bn]: Some people have complained that the students used in this experiment were underqualified as judges, but I'm sure there'll be a follow-up next year that addresses this issue ğŸ¤·â€â™‚ï¸. \n\n[^gj8k7rnd8f]: â‡¢ Clark: \"Sir, this is a Less Wrong. You can't swear here\" â€” In order for a film to get a 12A or PG-13 rating, it cannot contain gratuitous use of profanity. One 'f#@k' is allowed, anymore and it automatically becomes a 15 or R rated\" â€” See, PG-13.â‡¢ Jax: \"Censorship is terrible and the belief that swearing hurts kids is utterly absurd. Do do you think they've forgotten what it was like as a kid?\" â€” Not taking sides, but restrictions aren't always negative. Sometimes limiting a resource encourages people to use that resource to maximum effect ğŸ¯. \n\n[^4v7remohl8t]: See Life Itself's article on the \"wisdom gap\" which is the terminology I prefer in informal contexts. 10 Billion uses the term capabilities-wisdom gap, which I've reversed in order to emphasis wisdom. I prefer this version for more formal contexts ğŸ“–ğŸ˜•. \n\n[^msdo4n1exzg]: I don't want to dismiss the value of addressing specific issues in terms of buying us time ğŸ”„. \n\n[^eg9jv9u2b4]: I recommend finishing the post first (don't feel obligated to expand all the sections) ğŸ™. \n\n[^crtqmhn7yak]: This paper argues that robustness, explainability and cooperation all facilitate each other ğŸ¤ğŸ’ª. In case the link to wisdom is confusing (ğŸ¦‰â“), the authors argue that metacognition is both key to wisdom and key to these capabilities â˜ï¸. \n\n[^k76ez251czp]: This argument could definitely be more rigorous, however, this response is addressed to optimists and this it is especially likely to be true if we adopt an optimistic stance on technical and social alignment. So I feel fine leaving it as is ğŸ¤ğŸ™. \n\n[^04xxgeaaee8g]: âš ï¸ If timelines are extremely short, then many of my proposals might become irrelevant because there won't be time to develop them ğŸ˜­â³. We might be limited to simple things like tweaking the system prompt or developing a new user interface, as opposed to some of the more ambitious approaches that might require changing how we train our models. However, there's a lot of uncertainty with timelines, so more ambitious projects might make sense anyway. ğŸ¤”ğŸŒ«ï¸ğŸ¤ğŸ¤·.Â  \n\n[^2bv3miwbtfv]: \"If your proposal might even work better over long timelines, why does your core argument focus on short timelines ğŸ¤”â‰ï¸\": There are multiple arguments for developing wise AI advisors and I wanted to focus on one that was particularly legible ğŸ”¤ğŸ™. \n\n[^9x9nhn6ilx]: Andrew Critch has labelled this - \"pivoting\" civilisation across multiple acts across multiple persons and institutions - a pivotal process. He compares this favourably to the concept of a pivotal act - \"pivoting\" civilisation in a single action - on the basis of it being easier, safer and more legitimate ğŸ—ºï¸ğŸŒ¬ï¸â›µï¸â›µï¸â›µï¸ğŸŒ. \n\n[^g17me1dynkd]: I say \"forget X\" partly in jest. My point is that if there's a realistic chance of realising stronger possibility, then it would likely be much more significant than the weaker possibility ğŸ‘ï¸âš½. \n\n[^4eyc2jpzl2o]: I think there's a strong case that if these advisers can help you gain allies at all, they can help you gain many allies ğŸºğŸª‚ğŸª‚ğŸª‚. \n\n[^nw9j9obwnrr]: In So You Want To Make Marginal Progress..., John Wentworth argues that \"when we don't already know how to solve the main bottleneck(s)... a solution must generalize to work well with the whole wide class of possible solutions to other subproblems... (else) most likely, your contribution will not be small; it will be completely worthless\" ğŸ—‘ï¸â€¼ï¸. \n\n[^2pnutp0bsqr]: My default assumption is that people will be making further use of AI advice going forwards. But that doesn't contradict my key point, that certain strategies may become viable if we \"go hard on\" producing wise AI advisors ğŸ—ºï¸ğŸ”“ğŸŒ„. \n\n[^t1hzunqf8pq]: \"But \"reckless and unwise\" and \"responsible & wise\" aren't the only possible combinations!\" - True ğŸ‘, but they co-occur sufficiently often that I think this is okay for a high-level analysis ğŸ¤”ğŸ¤ğŸ™. \n\n[^o18fnks9cyd]: They could also speed up the ability of malicious and \"reckless & unwise\" actors to engage in destructive actions, however I still think such advisors would most likely to improve the comparative situation since many defences and precautions can be put in place before they are needed ğŸ¤”ğŸ¤ğŸ’ª. \n\n[^s8y91xgk2j]: This argument is developed further in Some Preliminary Notes on the Promise of a Wisdom Explosion ğŸ¦‰ğŸ†ğŸ“„. \n\n[^5g9p7o2gllf]: I agree it's highly unlikely that we could convince all the major labs to pursue a wisdom explosion instead of an intelligence explosion ğŸ™‡. Nonetheless, I don't think this renders the question irrelevant â˜ï¸. Let's suppose (ğŸ˜´ğŸ’­) it really were the case that the transition to AGI was would most likely go well if society had decided to pursue a wisdom explosion rather than an intelligence explosion â€¼ï¸. I don't know, but that seems like it'd be the kind of thing that would be pretty darn useful to know ğŸ¤·â€â™‚ï¸. My experience with a lot of things is that it's a lot easier to find additional solutions than it is to find the first. In other words, solutions aren't just valuable for their potential to be implemented, but because of what they tell us about the shape of the problem ğŸ—ï¸ğŸšªğŸŒ. \n\n[^jp6d03wlda]: How can we tell which framings are likely to be fruitful and ones are likely to be not ğŸ‘©â€ğŸ¦°ğŸğŸğŸŒ³? This isn't an easy question, but my intuition is that a framing more likely to be worth pursuing if they lead to questions that feel like they should (â“) be being asked, but have been neglected due to unconscious framing effects ğŸ™ˆğŸ–¼ï¸. In contrast, a framing is less likely to be fruitful if it asks questions that seem interesting prima facie, but for which the best researchers within the existing paradigm have good answers for, even if most researchers do not ğŸœï¸âœ¨. \n\n[^kar5n6c7ooo]: I acknowledge that \"intelligence\" is only rather loosely connect with the capabilities AI systems have in practise. Factors like prestige, economic demand and tractability play a larger role in terms of what capabilities are developed than the name used to describe the area of research. Nonethless, I think it'd be hard to argue that the framing effect hasn't exterted any influence. So I think it is worthwhile understanding how this may have shaped the field and whether this influence has been for the best ğŸ•µï¸ğŸ¤”.Â  \n\n[^zf8veqike3c]: A common way that an incorrect paradigm can persist is if existing researchers don't have good answers to particular questions, but see them as unimportant ğŸ™„ğŸ‘‰. \n\n[^3ykcny8ifoz]: I have an intuition that it's much more valuable for the AI safety and governance community to be generating talent with a distinct skillset/strengths than to just be recruiting more folk like those we already have ğŸ§©. As we attract more people with the same skillset we likely experience decreasing marginal returns due to the highest impact roles already being filled ğŸ™…â€â™‚ï¸ğŸ“‰. \n\n[^b39jysqn1r4]: That said, given how fast AI development is going, I'm hoping the process of concretisation can be significantly sped up. Fortunately, I think it'll be easier to do it this time as I've already seen what the process looked like for alignment ğŸ”­ğŸš€ğŸ¤.Â  \n\n[^juibjpdlbp]: First author ğŸ¥‡. \n\n[^bn3g0btbhcu]: Wei Dai who was originally announced as a judge withdrew ğŸ•³ï¸. \n\n[^zjoz3gawj2p]: Including me ğŸ†ğŸ€ğŸ˜Š. \n\n[^orupv2e418s]: They write: \"The precise opinions expressed in this post should not be taken as institutional views of AI Impacts, but as approximate views of the competition organizers\" â˜ï¸ğŸ‘Œ. \n\n[^6kkzb5xhbu]: Linking to this article does not constitute an endorsement of Sofiechan or any other views shared there. Unfortunately, I am not aware of other discussions of this concept ğŸ˜­ğŸ¤·, so I will keep this link in this post temporarily ğŸ™ğŸ”œ.Â  \n\n[^aiqi80ztv7r]: You could even say: with the right person, and to the right degree, and at the right time, and for the right purpose, and in the right way â¤ï¸ğŸ¦‰.",
      "plaintextDescription": "Why the focus on wise AI advisors? (Metapost[1]Â with FAQ ğŸ“šğŸ™‹ğŸ»â€â™‚ï¸, Informal[2]Â Edition ğŸ•º, Working Draft ğŸ› ï¸)\nAbout this PostÂ  - ğŸ”—:Â  ğŸ•ºwiseaiadvisors.com ,Â  ğŸ•´ï¸Formal Edition (coming soon)Â \n\nThis post is still in draft, so any feedback would be greatly appreciated ğŸ™. It'll be posted as a full, proper Less Wrong/EA Forum/Alignment forum post, as opposed to just a short-form, when it's ready ğŸŒ±ğŸŒ¿ğŸŒ³.\n\nÂ âœ‰ï¸ PM via LW profileÂ  Â \n\nğŸ“² QR Code\n\n\nThis post is a collaboration between Chris Leong (primary author) and Christopher Clay (editor), written in the voice of Chris Leong.\n\nWe have worked very hard on this[3]Â and we hope you find it to be of some use. Despite this work, it will most likely contain enough flaws that we will be at least somewhat embarrassed at having written at least parts of it and yet at some point a post has to go out into world to fend for itself.\n\nThere's no guarantee that this post will have the kind of impact that we deeply wish for, or even any achieve anything at all... And yet - regardless[4]Â - it is an honour to serve[5]Â especially at this dark hour when AGI looms on the horizon and doubt has begun to creep into the hearts of men[6]Â ğŸ«¡. The sheer scale of the problem feels overwhelming at times, and yet... we do what we can[7].\n\nThis post[8]Â doubles[9]Â as both as serious attempt to produce an original \"alignment proposal\"[10]Â and a passion project[11]. The AI safety community has spend nearly two and a half decades trying to convince people to take AI risks seriously. Whilst there have been significant successes, these have also fallen short[12]. Undoubtedly this is incredibly naive[13], but perhaps passion and authenticity[14]Â can succeed where arguments and logic have failed âœ¨ğŸŒ .Â \n\n\nAcknowledgements:\nThe initial draft of this post was produced during the 10th edition of AI Safety Camp. Thanks to my team: Matthew Hampton, Richard Kroon, and Chris Cooper, for their feedback. Unlike most other AI Safety Camp projects, which focus on a single, "
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-02-10T16:54:00.795Z",
    "af": false
  },
  {
    "_id": "4iitWAM6wmRPAv3oy",
    "postId": "f46GtgMzcJy5TmSmq",
    "parentCommentId": "hQw4TExPFaupHfAoX",
    "topLevelCommentId": "hQw4TExPFaupHfAoX",
    "contents": {
      "markdown": "At it's worst it can be, but I'd encourage you to reflect on the second quote:\n\n> Society would remember the Holocaust differently if there were no survivors to tell the story, but only data, records and photographs. The stories of victims and survivors weave together the numbers to create a truth that is tangible to the human experienceâ€¦",
      "plaintextDescription": "At it's worst it can be, but I'd encourage you to reflect on the second quote:\n\n> Society would remember the Holocaust differently if there were no survivors to tell the story, but only data, records and photographs. The stories of victims and survivors weave together the numbers to create a truth that is tangible to the human experienceâ€¦"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-14T18:46:52.168Z",
    "af": false
  },
  {
    "_id": "fNcJo66u4jEsN52bg",
    "postId": "ffKE9ohuuSFZDepeZ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Great post!\n\nI really appreciate proposals that are both pragmatic and ambitious; and this post is both!\n\nI guess the closest thing there is to a CEA for AI Safety is [Kairos](https://kairos-project.org/). However, they decided to focus explicitly on student groups[^h0x3blwyv5s].\n\n[^h0x3blwyv5s]: SPAR isn't limited to students, but it is very much in line with this by providing, \"research mentorship for early-career individuals in AI safety\".",
      "plaintextDescription": "Great post!\n\nI really appreciate proposals that are both pragmatic and ambitious; and this post is both!\n\nI guess the closest thing there is to a CEA for AI Safety is Kairos. However, they decided to focus explicitly on student groups[1].\n\n 1. ^\n    SPAR isn't limited to students, but it is very much in line with this by providing, \"research mentorship for early-career individuals in AI safety\"."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-04T00:44:44.500Z",
    "af": false
  },
  {
    "_id": "8sFvznWNYfRZr7N6F",
    "postId": "JkrkzXQiPwFNYXqZr",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I think he's clearly had a narrative he wanted to spin and he's being very defensive here.\n\nIf I wanted to steelman his position, I would do so as follows (low-confidence and written fairly quickly):\n\n1) I expect he believes his framing and that he feels fairly confident in it because most of the people he respects also adopt this framing.\n2) In so far as his own personal views make it into the article, I expect he believes that he's engaging in a socially acceptable amount of editorializing. In fact, I expect he believes that editorializing the article in this way is more socially responsible than not, likely due to the role of journalism being something along the lines of \"critiquing power\".\n3) Further, whilst I expect he wouldn't universally endorse \"being socially acceptable among journalists\" as guaranteeing that something is moral, he'd likely defend it as a strongly reliable heuristic, such that it would take pretty strong arguments to justify departing from this.\n4) Whilst he likely endorses some degree of objectivity (in terms of getting facts correct), I expect that he also sees neutrality as overrated by old school journalists. I expect he believes that it limits the ability of jouralists to steer the world towards positive outcomes. That is, more of as a consideration that can be overriden, rather than a rule.",
      "plaintextDescription": "I think he's clearly had a narrative he wanted to spin and he's being very defensive here.\n\nIf I wanted to steelman his position, I would do so as follows (low-confidence and written fairly quickly):\n\n 1. I expect he believes his framing and that he feels fairly confident in it because most of the people he respects also adopt this framing.\n 2. In so far as his own personal views make it into the article, I expect he believes that he's engaging in a socially acceptable amount of editorializing. In fact, I expect he believes that editorializing the article in this way is more socially responsible than not, likely due to the role of journalism being something along the lines of \"critiquing power\".\n 3. Further, whilst I expect he wouldn't universally endorse \"being socially acceptable among journalists\" as guaranteeing that something is moral, he'd likely defend it as a strongly reliable heuristic, such that it would take pretty strong arguments to justify departing from this.\n 4. Whilst he likely endorses some degree of objectivity (in terms of getting facts correct), I expect that he also sees neutrality as overrated by old school journalists. I expect he believes that it limits the ability of jouralists to steer the world towards positive outcomes. That is, more of as a consideration that can be overriden, rather than a rule."
    },
    "baseScore": 5,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-18T22:40:50.326Z",
    "af": false
  },
  {
    "_id": "ukfzDMBgMJKunmkvx",
    "postId": "JkrkzXQiPwFNYXqZr",
    "parentCommentId": "zDJ83BuhFJ9ecGMj8",
    "topLevelCommentId": "zDJ83BuhFJ9ecGMj8",
    "contents": {
      "markdown": "I almost agreed voted this â€” then read the comments below â€” and disagreed voted this instead.",
      "plaintextDescription": "I almost agreed voted this â€” then read the comments below â€” and disagreed voted this instead."
    },
    "baseScore": 1,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-18T22:24:51.265Z",
    "af": false
  },
  {
    "_id": "b8RAqywDaranw4pbs",
    "postId": "RCDEFhCLcifogLwEm",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Fascinating work. I'm keen to hear more about the belief set of this opposing cluster.",
      "plaintextDescription": "Fascinating work. I'm keen to hear more about the belief set of this opposing cluster."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-15T00:08:46.985Z",
    "af": false
  },
  {
    "_id": "cJ3FSHhGghpbDHWjc",
    "postId": "YKhteXYywLgNtKkYF",
    "parentCommentId": "oLubstoFeiicgvjTg",
    "topLevelCommentId": "oLubstoFeiicgvjTg",
    "contents": {
      "markdown": "You're misunderstanding the language game.",
      "plaintextDescription": "You're misunderstanding the language game."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-14T17:39:01.429Z",
    "af": false
  },
  {
    "_id": "Qwmgfkd8ojGBaqt4L",
    "postId": "g6rpo6hshodRaaZF3",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Do you think Wiki pages might be less important with LLM's these days? Also, I just don't end up on Wiki pages as often, I'm wondering if Google stopped prioritizing it so heavily.",
      "plaintextDescription": "Do you think Wiki pages might be less important with LLM's these days? Also, I just don't end up on Wiki pages as often, I'm wondering if Google stopped prioritizing it so heavily."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-14T16:43:00.545Z",
    "af": false
  },
  {
    "_id": "j9GepQHjDJra7ZFuw",
    "postId": "5hApNw5f7uG8RXxGS",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Is there any chance you could define what you mean by \"open agency\"? Do you essentially mean \"distributed agency\"?",
      "plaintextDescription": "Is there any chance you could define what you mean by \"open agency\"? Do you essentially mean \"distributed agency\"?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-06T13:57:58.358Z",
    "af": true
  },
  {
    "_id": "LMSAFyCszLZTprJvj",
    "postId": "SbAofYCgKkaXReDy4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "**Placeholder for an experimental art project â€” Under construction ğŸš§**[^9t7xhkxfgbv]\n=====================================================================================\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"background-color:#fef3e6;border-style:none\"><blockquote><p><i>Anything can be art, it might just be bad art</i> â€” Millie Florence</p></blockquote></td></tr></tbody></table>\n\n+++ ***Art in the Age of the Internet***\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"background-color:#fef3e6;border-style:none\"><blockquote><p><i>The medium is the message</i> â€” Marshall McLuhan, Media Theorist</p></blockquote></td></tr></tbody></table>\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"background-color:#fef3e6;border-style:none\"><blockquote><p><i>Hypertext is not a technology, it is a way of thinking</i> â€” ChatGPT 5<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"2\" data-footnote-id=\"fas5dwq0iic\" role=\"doc-noteref\" id=\"fnreffas5dwq0iic\"><sup><a href=\"#fnfas5dwq0iic\">[2]</a></sup></span></p></blockquote></td></tr></tbody></table>\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"background-color:#fef3e6;border-style:none\"><blockquote><p><i>Writing is the process of reducing a tapestry of interconnections to a narrow sequence. This is, in a sense, illicit. This is a wrongful compression of what should spread out, and todayâ€™s computers, theyâ€™ve betrayed that â€” </i>Ted Nelson, founder of Project Xanadu<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"860euzcaox4\" role=\"doc-noteref\" id=\"fnref860euzcaox4\"><sup><a href=\"#fn860euzcaox4\">[3]</a></sup></span><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"4\" data-footnote-id=\"n1ugqr9gel\" role=\"doc-noteref\" id=\"fnrefn1ugqr9gel\"><sup><a href=\"#fnn1ugqr9gel\">[4]</a></sup></span></p></blockquote></td></tr></tbody></table>\n\n\n\n\n\n\n+++\n\nğ•¯ğ–”ğ–”ğ–’\\\\(^\\textbf{ØŸ}\\\\)\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"background-color:#f2edfa;border-style:none;padding:4px;text-align:center\"><p>ğ’½ğ‘œğ“Œ ğ“‰ğ‘œ ğ’·ğ‘’ğ‘”ğ’¾ğ“ƒ? ğ“Œğ’½ğ’¶ğ“‰ ğ’¶ğ’·ğ‘œğ“Šğ“‰ ğ’¶ğ“‰ <i>ğ•¿ğ–ğ–Š ğ•°ğ–“ğ–‰?</i><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"5\" data-footnote-id=\"9z89tno5jsa\" role=\"doc-noteref\" id=\"fnref9z89tno5jsa\"><sup><a href=\"#fn9z89tno5jsa\">[5]</a></sup></span></p><p><i>ğ•¿ğ–ğ–Š ğ•°ğ–“ğ–‰? ğ•šğ•¤ ğ•šğ•¥ ğ•£ğ•–ğ•’ğ•ğ•ğ•ª ğ•¿ğ–ğ–Š ğ•°ğ–“ğ–‰?</i></p><p><i>ğ“ğ‘’ğ“ˆ. ğ’¾ğ“‰ ğ’¾ğ“ˆ ğ•¿ğ–ğ–Š ğ•°ğ–“ğ–‰. ğ‘œğ“‡ ğ“‚ğ’¶ğ“ğ’·ğ‘’<strong> ğ’¯ğ’½â„¯ ğµâ„¯â„Šğ’¾ğ“ƒğ“ƒğ’¾ğ“ƒâ„Š</strong></i>.<br><i>ğ“Œğ’½ğ’¶ğ“‰ğ‘’ğ“‹ğ‘’ğ“‡ ğ“‰ğ’½ğ‘’ ğ’¸ğ’¶ğ“ˆğ‘’, ğ’¾ğ“‰ ğ’¾ğ“ˆ <strong>ğ’¶ğ“ƒ ğ‘’ğ“ƒğ’¹</strong>.</i><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"6\" data-footnote-id=\"qd6rw1t3uu\" role=\"doc-noteref\" id=\"fnrefqd6rw1t3uu\"><sup><a href=\"#fnqd6rw1t3uu\">[6]</a></sup></span></p></td></tr></tbody></table>\n\n<table style=\"border:4px solid hsl(0, 0%, 0%)\"><tbody><tr><td style=\"border-color:hsl(0, 0%, 0%)\"><h3><strong>Ilya: The AI scientist shaping the world</strong></h3></td></tr><tr><td style=\"background-color:#fef3e6;border-style:none;padding:4px\"><blockquote><p><i>Now AI is a great thing, because AI will solve all the problems that we have today. It will solve employment, it will solve disease, it will solve poverty, but it will also create new problems...</i><br><br><i>The problem of fake new is going to be a million times worse, cyber attacks will become much more extreme, we will have totally automated AI weapons. I think AI has the potential to create infinity stable dictatorships...</i></p></blockquote></td></tr><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"7\" data-footnote-id=\"upeynzp9je8\" role=\"doc-noteref\" id=\"fnrefupeynzp9je8\"><sup><a href=\"#fnupeynzp9je8\">[7]</a></sup></span></td></tr><tr><td style=\"border-style:none;padding:0px\"><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=9iqn1HhFJ6c\"><div><iframe src=\"https://www.youtube.com/embed/9iqn1HhFJ6c\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure></td></tr><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr><tr><td style=\"background-color:#fef3e6;border-style:none\"><blockquote><p><i>I feel technology is a force of nature...</i></p><p><i>Because the way I imagine it is that there is an avalanche, like there is an avalanche of AGI development. Imagine this huge unstoppable force...</i></p><p><i>And I think it's pretty likely the entire surface of the earth will be covered with solar panels and data centers.</i></p></blockquote></td></tr><tr><td style=\"border-style:none\"><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png/w_350 350w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png/w_700 700w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png/w_1050 1050w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png/w_1400 1400w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png/w_1750 1750w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png/w_2100 2100w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png/w_2450 2450w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png/w_2800 2800w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png/w_3150 3150w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/033b9aca1d0c03b419eb44806931b2f1e8f3af835cbc0f57.png/w_3456 3456w\"></figure></td></tr><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr><tr><td style=\"background-color:#fef3e6\"><blockquote><p><i>The future will be good for the AIs regardless, it would be nice if it were good for humans as well</i></p></blockquote></td></tr><tr><td style=\"border-style:none\"><figure class=\"image\"><img src=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png\" srcset=\"https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png/w_210 210w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png/w_420 420w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png/w_630 630w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png/w_840 840w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png/w_1050 1050w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png/w_1260 1260w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png/w_1470 1470w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png/w_1680 1680w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png/w_1890 1890w, https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/69e62e6744f1013f135408dfaefea3dab8f25e3e51501746.png/w_2082 2082w\"></figure></td></tr><tr><td style=\"border-style:none;text-align:center\">â¦</td></tr><tr><td style=\"border-style:none;text-align:center\">â¦</td></tr><tr><td style=\"border-style:none;text-align:center\">â¦</td></tr><tr><td style=\"border-style:none\"><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><p><i><strong>Journal</strong></i></p></summary><div class=\"detailsBlockContent\"><p>&nbsp;</p></div></details></td></tr></tbody></table>\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"background-color:#fef3e6;border-style:none;text-align:center\"><blockquote><p>ğ— ğ—¶ğ˜ğ—¶ğ—´ğ—®ğ˜ğ—¶ğ—»ğ—´ ğ˜ğ—µğ—² ğ—¿ğ—¶ğ˜€ğ—¸ ğ—¼ğ—³ ğ—²ğ˜…ğ˜ğ—¶ğ—»ğ—°ğ˜ğ—¶ğ—¼ğ—» ğ—³ğ—¿ğ—¼ğ—º ğ—”ğ—œ ğ˜€ğ—µğ—¼ğ˜‚ğ—¹ğ—± ğ—¯ğ—² ğ—® ğ—´ğ—¹ğ—¼ğ—¯ğ—®ğ—¹ ğ—½ğ—¿ğ—¶ğ—¼ğ—¿ğ—¶ğ˜ğ˜† ğ—®ğ—¹ğ—¼ğ—»ğ—´ğ˜€ğ—¶ğ—±ğ—² ğ—¼ğ˜ğ—µğ—²ğ—¿ ğ˜€ğ—¼ğ—°ğ—¶ğ—²ğ˜ğ—®ğ—¹-ğ˜€ğ—°ğ—®ğ—¹ğ—² ğ—¿ğ—¶ğ˜€ğ—¸ğ˜€ ğ˜€ğ˜‚ğ—°ğ—µ ğ—®ğ˜€ ğ—½ğ—®ğ—»ğ—±ğ—²ğ—ºğ—¶ğ—°ğ˜€ ğ—®ğ—»ğ—± ğ—»ğ˜‚ğ—°ğ—¹ğ—²ğ—®ğ—¿ ğ˜„ğ—®ğ—¿.</p></blockquote></td></tr><tr><td style=\"background-color:#fef3e6;border-style:none\">Geoffry Hinton, Yoshua Bengio, Demis Hassabis, Sam Altman, Dario Amodei, Bill Gates, Ily Sutskever...&nbsp;</td></tr></tbody></table>\n\n<table style=\"border:4px solid hsl(0, 0%, 0%)\"><tbody><tr><td><h3>There's No Rule That Says We'll Make It â€” Rob Miles</h3></td></tr><tr><td style=\"border-style:none\"><figure class=\"media\"><div data-oembed-url=\"https://www.youtube.com/watch?v=JD_iA7imAPs\"><div><iframe src=\"https://www.youtube.com/embed/JD_iA7imAPs\" allow=\"autoplay; encrypted-media\" allowfullscreen=\"\"></iframe></div></div></figure></td></tr></tbody></table>\n\n+++ **More**\n\n<table style=\"border:4px solid hsl(0, 0%, 0%)\"><tbody><tr><td><h3><a href=\"https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy\">MIRI announces new \"Death With Dignity\" strategy, April 2nd, 2022</a></h3></td></tr><tr><td style=\"border-style:none\"><p>Well, let's be frank here. &nbsp;MIRI didn't solve AGI alignment and at least knows that it didn't. &nbsp;Paul Christiano's incredibly complicated schemes have no chance of working in real life before DeepMind destroys the world. &nbsp;Chris Olah's transparency work, at current rates of progress, will at best let somebody at DeepMind give a highly speculative warning about how the current set of enormous inscrutable tensors, inside a system that was recompiled three weeks ago and has now been training by gradient descent for 20 days, might possibly be planning to start trying to deceive its operators.</p><p>Management will then ask what they're supposed to do about that.</p><p>Whoever detected the warning sign will say that there isn't anything known they can do about that. &nbsp;Just because you can see the system might be planning to kill you, doesn't mean that there's any known way to build a system that won't do that. &nbsp;Management will then decide not to shut down the project - because it's not certain that the intention was really there or that the AGI will really follow through, because other AGI projects are hard on their heels, because if all those gloomy prophecies are true then there's nothing anybody can do about it anyways. &nbsp;Pretty soon that troublesome error signal will vanish.</p><p>When Earth's prospects are that far underwater in the basement of the <a href=\"https://intelligence.org/2017/11/26/security-mindset-and-the-logistic-success-curve/\">logistic success curve</a>, it may be hard to feel motivated about continuing to fight, since doubling our chances of survival will only take them from 0% to 0%.</p><p>That's why I would suggest reframing the problem - especially on an emotional level - to helping humanity <i>die with dignity,</i> or rather, since even this goal is realistically unattainable at this point, <i>die with slightly more dignity than would otherwise be counterfactually obtained...</i></p></td></tr></tbody></table>\n\n\n\n\n\n\n+++\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n### **Three Quotes on Transformative Technology**\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;text-align:center\">But the moral considerations, Doctor...<br><br>Did you and the other scientists not stop to consider the implications of what you were creating? â€” <strong>Roger Robb</strong><br><br>When you see something that is technically sweet, you go ahead and do it and you argue about what to do about it only after you have had your technical success. That is the way it was with the atomic bombâ€” <strong>Oppenheimer</strong></td></tr><tr><td style=\"border-style:none;text-align:center\">â¦</td></tr><tr><td style=\"border-style:none;text-align:center\">There are moments in the history of science, where you have a group of scientists look at their creation and just say, you know: â€˜What have we done?... Maybe it's great, maybe it's bad, but what have we done? &nbsp;â€” <strong>Sam Altman</strong></td></tr><tr><td style=\"border-style:none;text-align:center\">â¦</td></tr><tr><td style=\"border-style:none;text-align:center\">Urgent: get collectively wiser - <strong>Yoshua Bengio, AI \"Godfather\"</strong></td></tr></tbody></table>\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5fb57b7c057b577bb3507ade36c84417aafece4257201484.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\n+++ **âœ’ï¸ Selected Quotes:**\n\nWe stand at a crucial moment in the history of our species. Fueled by technological progress, our power has grown so great that for the first time in humanityâ€™s long history, we have the capacity to destroy ourselvesâ€”severing our entire future and everything we could become.\n\nYet humanityâ€™s wisdom has grown only falteringly, if at all, and lags dangerously behind. Humanity lacks the maturity, coordination and foresight necessary to avoid making mistakes from which we could never recover. As the gap between our power and our wisdom grows, our future is subject to an ever-increasing level of risk. This situation is unsustainable. So over the next few centuries, humanity will be tested: it will either act decisively to protect itself and its long-term potential, or, in all likelihood, this will be lost forever â€” **Toby Ord,** *The Precipice*\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\nWe have created a Star Wars civilization, with Stone Age emotions, medieval institutions, and godlike technology â€” **Edward O. Wilson**, *The Social Conquest of Earth*\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\nBefore the prospect of an intelligence explosion, we humans are like small children playing with a bomb. Such is the mismatch between the power of our plaything and the immaturity of our conduct â€” **Nick Bostrom**, Founder of the Future of Humanity Institute, *Superintelligence*\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\nIf we continue to accumulate only power and not wisdom, we will surely destroy ourselves â€” **Carl Sagan**, *Pale Blue Dot*\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/284f638b19003945147effcd861ebeb2b3207bc7612029bb.png)\n\nNever has humanity had such power over itself, yet nothing ensures that it will be used wisely, particularly when we consider how it is currently being usedâ€¦There is a tendency to believe that every increase in power means â€œan increase of â€˜progressâ€™ itself â€, an advance in â€œsecurity, usefulness, welfare and vigour; â€¦an assimilation of new values into the stream of cultureâ€, as if reality, goodness and truth automatically flow from technological and economic power as such. â€” **Pope Francis**, *Laudato si'*\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\nThe fundamental test is how wisely we will guide this transformation â€“ how we minimize the risks and maximize the potential for good â€” **AntÃ³nio Guterres**, Secretary-General of the United Nations\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\nOur future is a race between the growing power of our technology and the wisdom with which we use it. Letâ€™s make sure that wisdom wins â€” **Stephen Hawking**, *Brief Answers to the Big Questions*\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\n![A digital painting of a toddler sitting in a sandbox at dusk, wearing a crumpled paper crown and holding a glowing scepter topped with a miniature Earth. The child looks fascinated and innocent. Surrounding them are plastic toy tanks, a rocket, a dump truck, and a small shovel, softly illuminated by the warm glow of the scepter. The background is dark, evoking a sense of quiet gravity.](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/38f866971870df4623a5d5f10b5973fe864e6ffa7565d153.png)\n\n\n\n\n++++++ **â¤ï¸â€ğŸ”¥ Desires**\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"background-color:#f2edfa;border-style:none;padding:4px;text-align:center\"><p><i>ğ“ˆğ‘œğ“‚ğ‘’ğ“‰ğ’¾ğ“‚ğ‘’ğ“ˆ ğ¼ ğ’¿ğ“Šğ“ˆğ“‰ ğ“Œğ’¶ğ“ƒğ“‰ ğ“‰ğ‘œ ğ“‚ğ’¶ğ“€â„¯ <strong>ğ’œğ“‡ğ“‰</strong></i></p><p><i>ğ•¥ğ•™ğ•–ğ•Ÿ ğ•ğ•’ğ•œğ•– ğ•šğ•¥</i></p><p><i>ğ’·ğ“Šğ“‰ ğ“‰ğ’½â„¯ ğ“Œğ‘œğ“‡ğ“ğ’¹ <strong>ğ’©ğ¸ğ¸ğ’Ÿğ’®</strong> <strong>ğ’®ğ’¶ğ“‹ğ’¾ğ“ƒâ„Š</strong>...</i></p><p><i>ğ•ªğ• ğ•¦ ğ•”ğ•’ğ•Ÿ <strong>ğ“ˆğ’¶ğ“‹â„¯</strong> ğ•šğ•¥?</i></p><p><i>ğ¼... ğ¼ ğ’¸ğ’¶ğ“ƒ <strong>ğ’¯ğ“‡ğ“</strong>...</i></p></td></tr></tbody></table>\n\n<table style=\"border:2px solid hsl(0, 0%, 0%)\"><tbody><tr><td style=\"text-align:center\"><a href=\"https://forum.effectivealtruism.org/posts/AjxqsDmhGiW9g8ju6/effective-altruism-in-the-garden-of-ends\">Effective altruism in the garden of ends</a></td></tr><tr><td style=\"border-style:none\"><blockquote><p>No â€“ I will eat, sleep, and drink well to feel alive; so too will I love and dance as well as help.</p></blockquote></td></tr></tbody></table>\n\n\n\n\n\n\n+++\n\n### **Hope**\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\">â¦</td></tr></tbody></table>\n\n### **Scraps**\n\n+++ ***Ilya Sutskever***\n\n> â€œIt had taken Sutskever years to be able to put his finger on Altmanâ€™s pattern of behaviorâ€”how OpenAIâ€™s CEO would tell him one thing, then say another and act as if the difference was an accident. â€˜Oh, I must have misspoken,â€™ Altman would say. Sutskever felt that Altman was dishonest and causing chaos, which would be a problem for any CEO, but especially for one in charge of such potentially civilization-altering technology.â€\n\n[**The Optimist, Keach Hagey**](https://www.openaifiles.org/the-optimist)\n\n> Ilya Sutskever, once widely regarded as perhaps the most brilliant mind at OpenAI, voted in his capacity as a board member last November to remove Sam Altman as CEO. The move was unsuccessful, in part because Sutskever reportedly bowed to pressure from his colleagues and reversed his vote. After those fateful events, Sutskever disappeared from OpenAIâ€™s offices so noticeably that memes began circulating online asking what had happened to him. Finally, in May, Sutskever announced he had stepped down from the company.\n\n[**Time 100 AI 2024**](https://time.com/7012869/ilya-sutskever-2/)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bf08a729b3a4d140beadde6ef54e1c79bea2df9cab291998.png)\n\n[**Twitter**](https://x.com/ilyasut/status/1790517455628198322)\n\n> We approach safety and capabilities in tandem, as technical problems to be solved through revolutionary engineering and scientific breakthroughs. We plan to advance capabilities as fast as possible while making sure our safety always remains ahead.\n> \n> This way, we can scale in peace.\n> \n> Our singular focus means no distraction by management overhead or product cycles, and our business model means safety, security, and progress are all insulated from short-term commercial pressures.\n\n[Safe Superintelligence Inc.](https://ssi.inc/)\n\n\n\n\n+++\n\n[^9t7xhkxfgbv]: â‡¢ Note to self: My previous project had too much meta-commetary and this may have undermined the sincerity, so I should probably try to minimise meta-commentary.â‡¢ \"You're going to remove this in the final version, right?\" â€” Maybe. \n\n[^fas5dwq0iic]: \"But you can't quote ChatGPT ğŸ˜ !\" - Internet Troll Ã· \n\n[^860euzcaox4]: \"I would say the flaw of Xanadu's UI was treating transclusion as 'horizontal' and side-by-side\" â€” Gwern ğŸ™ƒ \n\n[^n1ugqr9gel]: \"StretchText is a hypertext feature that has not gained mass adoption in systems like the World Wide Web... StretchText is similar to outlining, however instead of drilling down lists to greater detail, the current node is replaced with a newer node\" - WikipediaThis â€˜stretchingâ€™ to increase the amount of writing, or contracting to decrease it gives the feature its name. This is analogous to zooming in to get more detail.Ted Nelson coined the term c.â€‰1967.Conceptually, StretchText is similar to existing hypertexts system where a link provides a more descriptive or exhaustive explanation of something, but there is a key difference between a link and a piece of StretchText. A link completely replaces the current piece of hypertext with the destination, whereas StretchText expands or contracts the content in place. Thus, the existing hypertext serves as context.â‡¢ \"This isn't a proper implementation of StretchText\" Â â€” Indeed. \n\n[^9z89tno5jsa]: In defence of Natural Language DSLs Â â€” Â Connor Leahy \n\n[^qd6rw1t3uu]: Did this conversation really happen? â€” ç©† \n\n[^upeynzp9je8]: â‡¢ \"Sooner or later, everything old is new again\" â€” Stephen Kingâ‡¢ \"Therefore if any man be in Christ, he is a new creature: old things are passed away; behold, all things have become new.\" â€” 2 Corinthians 5:17",
      "plaintextDescription": 