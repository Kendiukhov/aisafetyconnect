[
  {
    "_id": "Zks9mFqFDLEQXuopv",
    "postId": "6KcP7tEe5hgvHbrSF",
    "parentCommentId": "5xhwL7xKTkC7TKzzu",
    "topLevelCommentId": "5xhwL7xKTkC7TKzzu",
    "contents": {
      "markdown": "I'm dropping this because it probably won't change conclusions much and on the margin, new more realistic evals seem like they would add more realism per unit of effort. The code is [open-source](https://github.com/METR/cross-domain-horizon) and if anyone writes the PR for me, I'll probably merge it.",
      "plaintextDescription": "I'm dropping this because it probably won't change conclusions much and on the margin, new more realistic evals seem like they would add more realism per unit of effort. The code is open-source and if anyone writes the PR for me, I'll probably merge it."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-26T22:30:50.607Z",
    "af": false
  },
  {
    "_id": "DGhxoLwcGpbCEeKv4",
    "postId": "5Ewo7u5xpGkQ6y4NJ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Ontological cluelessness seems poorly defined to me.\n\nThe notion of \"concept\" or \"knowledge\" does not seem especially basic given that philosophers have tried to define them for thousands of years and several different answers are given today, so it doesn't make sense to declare we have ontological cluelessness if we're wrong about them. What about \"five\" or \"sodium chloride\"? These are known to be extremely useful concepts in making sense of the world around us, and often rediscovered by humans, neural nets, and other systems, so it seems **highly unlikely** they would need to be revised to get an \"accurate understanding\" of 5+2 or the chemical properties of salt.\n\nHowever, if the criterion is \"an extreme upending of what we thought the cosmos was made of, the basic principles by which it operated, and the ways to make sense of those principles.\", it seems **highly likely** that we will have at least one extreme shift in what we think the cosmos is made of, given that relativity and QM have incompatible ontologies; there will also necessarily be new basic principles, given that physics steadily marches towards more and more general and abstruse math.",
      "plaintextDescription": "Ontological cluelessness seems poorly defined to me.\n\nThe notion of \"concept\" or \"knowledge\" does not seem especially basic given that philosophers have tried to define them for thousands of years and several different answers are given today, so it doesn't make sense to declare we have ontological cluelessness if we're wrong about them. What about \"five\" or \"sodium chloride\"? These are known to be extremely useful concepts in making sense of the world around us, and often rediscovered by humans, neural nets, and other systems, so it seems highly unlikely they would need to be revised to get an \"accurate understanding\" of 5+2 or the chemical properties of salt.\n\nHowever, if the criterion is \"an extreme upending of what we thought the cosmos was made of, the basic principles by which it operated, and the ways to make sense of those principles.\", it seems highly likely that we will have at least one extreme shift in what we think the cosmos is made of, given that relativity and QM have incompatible ontologies; there will also necessarily be new basic principles, given that physics steadily marches towards more and more general and abstruse math."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-26T02:00:47.121Z",
    "af": false
  },
  {
    "_id": "3KhqodHvp9z4WppGt",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "Hk8LYnbddhjRHhtfy",
    "topLevelCommentId": "itguzw6umXEGBd2zj",
    "contents": {
      "markdown": "The idea is they're printing money, not just borrowing it, which in the extreme would cause hyperinflation (and is equivalent to default since debt is in nominal dollars). It probably seems less bad though.",
      "plaintextDescription": "The idea is they're printing money, not just borrowing it, which in the extreme would cause hyperinflation (and is equivalent to default since debt is in nominal dollars). It probably seems less bad though."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-19T03:46:28.574Z",
    "af": false
  },
  {
    "_id": "zLuqaTDm5yPCpsFtG",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "ybvubcSzHmiSPTWba",
    "topLevelCommentId": "itguzw6umXEGBd2zj",
    "contents": {
      "markdown": "agree, I originally wanted to make a list of all things that would be wildly unpopular but that toxoplasma of rage could theoretically make happen. But then it seemed more interesting to list things that could have more government or geopolitical relevance.",
      "plaintextDescription": "agree, I originally wanted to make a list of all things that would be wildly unpopular but that toxoplasma of rage could theoretically make happen. But then it seemed more interesting to list things that could have more government or geopolitical relevance.Â "
    },
    "baseScore": 5,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-18T22:22:47.344Z",
    "af": false
  },
  {
    "_id": "itguzw6umXEGBd2zj",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "US Government dysfunction and runaway political polarization bingo card. I don't expect any particular one of these to happen but it seems plausible that at least one of these will happen.\n\n*   A sanctuary city conducts armed patrols to oppose ICE raids, or the National Guard refuses a direct order from the president en masse\n*   Internal migration is de facto restricted for US citizens or green card holders\n*   For debt ceiling reasons, the US significantly defaults on its debt, stops Social Security payments, grounds flights, or issues a trillion-dollar coin\n*   US declares a neutral humanitarian NGO like the WHO a foreign terrorist organization\n*   A major news network (eg CNN) or social media site other than Tiktok (eg Facebook) loses licenses for ideological reasons\n*   A Democratic or Republican candidate for president, governor, or Congress is kept off a state ballot\n*   A major elected official or Cabinet member takes office while incarcerated\n*   Election issues on the scale of 1876, where Congress can't decide on the president until past January 20\n*   A state or local government establishes a 100% tax bracket or wealth tax\n*   The Fed chair is fired, or three board members\n*   A NCAA D1 college or pro sports league establishes a minimum quota for transgender athletes\n*   Existing solar/wind plants are decommissioned, or a state legally caps its percentage of solar energy (not just making it contingent on batteries or something practical)\n*   US votes for a UN resolution to condemn itself, e.g. for human rights abuses\n*   US withdraws from the UN, NATO, G7 or G20\n*   US allies boycott the 2028 Olympics\n*   Court packing; the Supreme Court has more than 9 justices\n*   Multiple Supreme Court justices are impeached",
      "plaintextDescription": "US Government dysfunction and runaway political polarization bingo card. I don't expect any particular one of these to happen but it seems plausible that at least one of these will happen.\n\n * A sanctuary city conducts armed patrols to oppose ICE raids, or the National Guard refuses a direct order from the president en masse\n * Internal migration is de facto restricted for US citizens or green card holders\n * For debt ceiling reasons, the US significantly defaults on its debt, stops Social Security payments, grounds flights, or issues a trillion-dollar coin\n * US declares a neutral humanitarian NGO like the WHO a foreign terrorist organization\n * A major news network (eg CNN) or social media site other than Tiktok (eg Facebook) loses licenses for ideological reasons\n * A Democratic or Republican candidate for president, governor, or Congress is kept off a state ballot\n * A major elected official or Cabinet member takes office while incarcerated\n * Election issues on the scale of 1876, where Congress can't decide on the president until past January 20\n * A state or local government establishes a 100% tax bracket or wealth tax\n * The Fed chair is fired, or three board members\n * A NCAA D1 college or pro sports league establishes a minimum quota for transgender athletes\n * Existing solar/wind plants are decommissioned, or a state legally caps its percentage of solar energy (not just making it contingent on batteries or something practical)\n * US votes for a UN resolution to condemn itself, e.g. for human rights abuses\n * US withdraws from the UN, NATO, G7 or G20\n * US allies boycott the 2028 Olympics\n * Court packing; the Supreme Court has more than 9 justices\n * Multiple Supreme Court justices are impeached"
    },
    "baseScore": 47,
    "voteCount": 25,
    "createdAt": null,
    "postedAt": "2025-09-18T20:42:17.749Z",
    "af": false
  },
  {
    "_id": "AABFPKA3HYDvus2AR",
    "postId": "i7JSL5awGFcSRhyGF",
    "parentCommentId": "nPNCDRDKpnkhbkQBr",
    "topLevelCommentId": "adS78sYv5wzumQPWe",
    "contents": {
      "markdown": "I do, though maybe not this extreme. Roughly every other day I bemoan the fact that AIs aren't misaligned yet (limiting the excitingness of my current research) and might not even be misaligned in future, before reminding myself our world is much better to live in than the alternative. I think there's not much else to do with a similar impact given how large even a 1% p(doom) reduction is. But I also believe that particularly good research now can trade 1:1 with crunch time.\n\nTheoretical work is just another step removed from the problem and should be viewed with at least as much suspicion.",
      "plaintextDescription": "I do, though maybe not this extreme. Roughly every other day I bemoan the fact that AIs aren't misaligned yet (limiting the excitingness of my current research) and might not even be misaligned in future, before reminding myself our world is much better to live in than the alternative. I think there's not much else to do with a similar impact given how large even a 1% p(doom) reduction is. But I also believe that particularly good research now can trade 1:1 with crunch time.\n\nTheoretical work is just another step removed from the problem and should be viewed with at least as much suspicion."
    },
    "baseScore": 13,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-18T17:41:46.712Z",
    "af": false
  },
  {
    "_id": "Sfg7Zbpk9tB2gLrkp",
    "postId": "cxuzALcmucCndYv4a",
    "parentCommentId": "iufzrFDx6gbtBnTje",
    "topLevelCommentId": "G7sJgxCADzBCgmnvJ",
    "contents": {
      "markdown": "> The Interwebs seem to indicate that that's only if you give it a laser spot to aim at, not with just GPS.\n\nGood catch.\n\nAgree grenade sized munitions won't damage buildings, I think the conversation is drifting between FPVs and other kinds of drones, and also between various settings, so I'll just state my beliefs.\n\n*   Fiber FPVs with 40km range and 2kg payload (either kamikaze or grenade/mine dropping), which can eventually be countered by a large number of short range guns if they fly at low altitude. It's not clear to me if the 40km range ones need to be larger\n*   Heavy bomber drones can be equipped with fiber (or Starlink for US allies) and carry 15kg+ payload, enough to damage buildings and sensitive industrial equipment. They can do this while flying above the range of small guns and need dedicated antiaircraft guns\n*   Fixed wing can carry even larger payloads with longer range and higher altitude, but are still pretty slow, except for the ones with jet engines\n*   Drones equipped with GPS will know their position to within ~10 meters like the GPS only variant of Excalibur. It seems possible to constrain the payload's horizontal velocity by 1 m/s on average, and the drop time from 1500m is 17 seconds, giving an error of 17 m. The overall error would be sqrt(10^2 + 17^2) = 20 m. If GPS is jammed, it's not obvious they can do the first part, but probably they can still use cameras or something\n*   All of the above are extremely threatening for both organized warfare and terrorism against an opponent without effective cheap air defense.\n*   Even with the next evolution of air defense including radar-acoustic fusion to find threats, the limited reliability of ~all types of existing air defense and large number of drone configurations makes me guess that drones will remain moderately threatening in some form. Given that Hezbollah was previously firing unguided rockets with CEP in the hundreds of meters, some kind of drone that can target with CEP around 20 meters could be more cost effective for them if they cannot procure thousands of cheap guided missiles. If they could drop six individual grenades on six people from a bomber drone even in the presence of air defense, that would be even more effective, but it seems unlikely\n*   Excalibur is made by the US, which has no incentive to reduce costs, and so its $70k price tag is more of a \"maximum the army is willing to pay\" situation. This is true to some extent with Skyranger so maybe someone motivated will build smart ammunition that costs $40 per round and make it cost effective.",
      "plaintextDescription": "> The Interwebs seem to indicate that that's only if you give it a laser spot to aim at, not with just GPS.\n\nGood catch.\n\nAgree grenade sized munitions won't damage buildings, I think the conversation is drifting between FPVs and other kinds of drones, and also between various settings, so I'll just state my beliefs.\n\n * Fiber FPVs with 40km range and 2kg payload (either kamikaze or grenade/mine dropping), which can eventually be countered by a large number of short range guns if they fly at low altitude. It's not clear to me if the 40km range ones need to be larger\n * Heavy bomber drones can be equipped with fiber (or Starlink for US allies) and carry 15kg+ payload, enough to damage buildings and sensitive industrial equipment. They can do this while flying above the range of small guns and need dedicated antiaircraft guns\n * Fixed wing can carry even larger payloads with longer range and higher altitude, but are still pretty slow, except for the ones with jet engines\n * Drones equipped with GPS will know their position to within ~10 meters like the GPS only variant of Excalibur. It seems possible to constrain the payload's horizontal velocity by 1 m/s on average, and the drop time from 1500m is 17 seconds, giving an error of 17 m. The overall error would be sqrt(10^2 + 17^2) = 20 m. If GPS is jammed, it's not obvious they can do the first part, but probably they can still use cameras or something\n * All of the above are extremely threatening for both organized warfare and terrorism against an opponent without effective cheap air defense.\n * Even with the next evolution of air defense including radar-acoustic fusion to find threats, the limited reliability of ~all types of existing air defense and large number of drone configurations makes me guess that drones will remain moderately threatening in some form. Given that Hezbollah was previously firing unguided rockets with CEP in the hundreds of meters, some kind of drone that can target with CEP around 20 meters co"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-17T01:41:41.042Z",
    "af": true
  },
  {
    "_id": "ZQ5nittov4u4KMZFq",
    "postId": "cxuzALcmucCndYv4a",
    "parentCommentId": "4WHtHNk2FaypEALpZ",
    "topLevelCommentId": "G7sJgxCADzBCgmnvJ",
    "contents": {
      "markdown": "Nor would I. In WWII bombers didn't even know where they were, but we have GPS now such that Excalibur guided artillery shells can get 1m CEP. And the US and possibly China can use Starlink and other constellations for localization even when GPS is jammed. I would guess 20m is easily doable with good sensing and dumb bombs, which would at least hit a building.",
      "plaintextDescription": "Nor would I. In WWII bombers didn't even know where they were, but we have GPS now such that Excalibur guided artillery shells can get 1m CEP. And the US and possibly China can use Starlink and other constellations for localization even when GPS is jammed. I would guess 20m is easily doable with good sensing and dumb bombs, which would at least hit a building."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-16T22:35:41.295Z",
    "af": true
  },
  {
    "_id": "F4rcdGAdxrG6gadth",
    "postId": "cxuzALcmucCndYv4a",
    "parentCommentId": "6C5c5fwJJEzDjTPTb",
    "topLevelCommentId": "G7sJgxCADzBCgmnvJ",
    "contents": {
      "markdown": "IMO it is too soon to tell whether drone defense will hold up to countercountermeasures.\n\n*   It's already very common for drones to drop grenades and they can theoretically do so from 1-2km up if you sacrifice precision.\n*   Once sound becomes the primary means of locating drones, I expect the UAS operators to do everything they can to mask the acoustic signature, including varying the sound profile through propeller design, making cheaper drones louder than high-end drones, and deployable acoustic decoys.\n*   Guns have short range so these only work to defend targets fairly close to the system. E.g. Ukraine's indigenous Sky Sentinel (12.7mm caliber) has a range of 1.5 km and a sufficiently large swarm of FPVs can overwhelm one anyway. For longer ranges, larger calibers are needed, and these have higher costs and lower rate of fire. [Skyranger](https://en.wikipedia.org/wiki/Skyranger_30) 30mm has a range of 3 km but the ammunition costs $hundreds per round.\n\nI agree that Israel will probably be less affected than larger, poorer countries, but given that drones have probably killed over 200,000 people in Ukraine even a small percentage of this would be a problem for Israel.",
      "plaintextDescription": "IMO it is too soon to tell whether drone defense will hold up to countercountermeasures.\n\n * It's already very common for drones to drop grenades and they can theoretically do so from 1-2km up if you sacrifice precision.\n * Once sound becomes the primary means of locating drones, I expect the UAS operators to do everything they can to mask the acoustic signature, including varying the sound profile through propeller design, making cheaper drones louder than high-end drones, and deployable acoustic decoys.\n * Guns have short range so these only work to defend targets fairly close to the system. E.g. Ukraine's indigenous Sky Sentinel (12.7mm caliber) has a range of 1.5 km and a sufficiently large swarm of FPVs can overwhelm one anyway. For longer ranges, larger calibers are needed, and these have higher costs and lower rate of fire. Skyranger 30mm has a range of 3 km but the ammunition costs $hundreds per round.\n\nI agree that Israel will probably be less affected than larger, poorer countries, but given that drones have probably killed over 200,000 people in Ukraine even a small percentage of this would be a problem for Israel."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-16T06:00:17.312Z",
    "af": true
  },
  {
    "_id": "ARNC35u7pZxZQZirR",
    "postId": "cYLyXvYk8n3GrFaAc",
    "parentCommentId": "z5FKSvNQYfyD25kLd",
    "topLevelCommentId": "z5FKSvNQYfyD25kLd",
    "contents": {
      "markdown": "My views which I have already shared in person:\n\n*   The reason old currency units were large is because they're derived from weights of silver (eg a pound sterling or the equivalent French system [dating back to Charlemagne](https://en.m.wikipedia.org/wiki/French_livre#History)), and the pound is a reasonably-sized base unit of weight, so the corresponding monetary unit was extremely large. There would be nothing wrong with having $1000 currency units again, it's just that we usually have inflation rather than deflation. In crypto none of the reasonably sized subdivisions have caught on and it seems tolerable to buy a sandwich for 0.0031 ETH if that were common.\n*   Currencies are redenominated after sufficient inflation only when the number of zeros on everything gets unwieldy. This requires replacing all cash and is a bad look because it's usually done after hyperinflation, so countries like South Korea haven't done it yet.\n*   The Iranian rial's exchange rate, actually around 1e-6 now, is so low partly due to sanctions, and is in the middle of redenomination from 10000 rial = 1 toman.\n*   When people make a new currency, they make it similar to the currencies of their largest trading partners for convenience, hence why so many are in the range of the USD, euro and yuan. Various regional status games change this but not by an order of magnitude, and it is conceivable to me that we could get a $20 base unit if they escalate a bit.",
      "plaintextDescription": "My views which I have already shared in person:\n\n * The reason old currency units were large is because they're derived from weights of silver (eg a pound sterling or the equivalent French system dating back to Charlemagne), and the pound is a reasonably-sized base unit of weight, so the corresponding monetary unit was extremely large. There would be nothing wrong with having $1000 currency units again, it's just that we usually have inflation rather than deflation. In crypto none of the reasonably sized subdivisions have caught on and it seems tolerable to buy a sandwich for 0.0031 ETH if that were common.\n * Currencies are redenominated after sufficient inflation only when the number of zeros on everything gets unwieldy. This requires replacing all cash and is a bad look because it's usually done after hyperinflation, so countries like South Korea haven't done it yet.\n * The Iranian rial's exchange rate, actually around 1e-6 now, is so low partly due to sanctions, and is in the middle of redenomination from 10000 rial = 1 toman.\n * When people make a new currency, they make it similar to the currencies of their largest trading partners for convenience, hence why so many are in the range of the USD, euro and yuan. Various regional status games change this but not by an order of magnitude, and it is conceivable to me that we could get a $20 base unit if they escalate a bit."
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-12T10:00:12.347Z",
    "af": false
  },
  {
    "_id": "oRCCHpsd8DXxb2SpS",
    "postId": "xgJuyvi3jGsmNAmg9",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I don't understand how the issue of validity in evals connects to the Gooder Regulator Theorem. It's certainly arbitrarily hard to measure a latent property of an AI agent that's buried an an arbitrarily complex causal diagram of the agent, but this is also true of any kind of science. The solution is to only measure simple things and validate them extensively.\n\nTo start, the playbook includes [methods from the social sciences](https://en.wikipedia.org/wiki/Validity_(statistics)). Are humans lying in a psychology experiment intending to deceive, or just socially conditioned into deception in these circumstances? We can discriminate using more experiments. Here's a list suggested by Claude.\n\n> **Internal Validity:**\n> \n> *   **Manipulation checks**: Verify participants understand the task and setting as intended\n> *   **Control conditions**: Include baseline conditions where lying provides no benefit\n> *   **Multiple measures**: Use both behavioral indicators (actual lies told) and physiological measures (stress responses, reaction times)\n> *   **Debrief interviews**: Post-experiment discussions to understand participant reasoning\n> \n> **Construct Validity:**\n> \n> *   **Operationalize \"lying\" clearly**: Distinguish between errors, omissions, and active false statements\n> *   **Measure awareness**: Test whether participants recognize they're providing false information\n> *   **Cross-validate**: Use multiple paradigms that elicit deception differently\n> \n> **External Validity:**\n> \n> *   **Vary contexts**: Test across different social settings and stakes\n> *   **Diverse samples**: Include participants from various cultural backgrounds where social norms differ\n> *   **Ecological validity**: Use scenarios that mirror real-world situations\n> \n> **Intentional deception should show:**\n> \n> *   Individual differences in moral reasoning correlation\n> *   Sensitivity to personal cost-benefit analysis\n> *   Greater cognitive effort signatures\n> *   Ability to inhibit when explicitly instructed\n> *   Conscious awareness and ability to justify\n> \n> **Socially conditioned behavior should show:**\n> \n> *   Stronger situation-specific activation\n> *   Less individual variation within similar social groups\n> *   Faster, more automatic responses\n> *   Resistance to conscious control\n> *   Difficulty articulating clear reasons\n\nLikewise, in evals we can do increasingly elaborate robustness checks where we change the prompt and setting and measure if the correlation is high, check that the model understand its behavior is deceptive, provide incentives to the model that should make its behavior change if it were intentionally deceptive, and so on. Much of the time, especially for propensity / alignment evals, we will find the evals are nonrobust and we'll be forced to weaken our claims.\n\nSometimes, we have an easier job. I don't expect capability evals to get vastly harder because many skills are verifiable and sandbagging is solvable. Also, AI evals are easier than human psychology experiments, because they're likely to be much cheaper and faster to run and more replicable.",
      "plaintextDescription": "I don't understand how the issue of validity in evals connects to the Gooder Regulator Theorem. It's certainly arbitrarily hard to measure a latent property of an AI agent that's buried an an arbitrarily complex causal diagram of the agent, but this is also true of any kind of science. The solution is to only measure simple things and validate them extensively.\n\nTo start, the playbook includes methods from the social sciences. Are humans lying in a psychology experiment intending to deceive, or just socially conditioned into deception in these circumstances? We can discriminate using more experiments. Here's a list suggested by Claude.\n\n> Internal Validity:\n> \n>  * Manipulation checks: Verify participants understand the task and setting as intended\n>  * Control conditions: Include baseline conditions where lying provides no benefit\n>  * Multiple measures: Use both behavioral indicators (actual lies told) and physiological measures (stress responses, reaction times)\n>  * Debrief interviews: Post-experiment discussions to understand participant reasoning\n> \n> Construct Validity:\n> \n>  * Operationalize \"lying\" clearly: Distinguish between errors, omissions, and active false statements\n>  * Measure awareness: Test whether participants recognize they're providing false information\n>  * Cross-validate: Use multiple paradigms that elicit deception differently\n> \n> External Validity:\n> \n>  * Vary contexts: Test across different social settings and stakes\n>  * Diverse samples: Include participants from various cultural backgrounds where social norms differ\n>  * Ecological validity: Use scenarios that mirror real-world situations\n> \n> Intentional deception should show:\n> \n>  * Individual differences in moral reasoning correlation\n>  * Sensitivity to personal cost-benefit analysis\n>  * Greater cognitive effort signatures\n>  * Ability to inhibit when explicitly instructed\n>  * Conscious awareness and ability to justify\n> \n> Socially conditioned behavior should show:\n> \n>  * Str"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-09T02:31:23.440Z",
    "af": false
  },
  {
    "_id": "sziLwLo9WBZxXcdj6",
    "postId": "PEuCkgD93rcnTYNrP",
    "parentCommentId": "odTKKABLmcxi9nGrz",
    "topLevelCommentId": "odTKKABLmcxi9nGrz",
    "contents": {
      "markdown": "[According to Wikipedia](https://en.wikipedia.org/wiki/Belarus%E2%80%93Poland_border_barrier) it seems to have worked well and not been expensive.\n\n> Poland began work on the 5.5-meter (18 foot) high steel wall topped with [barbed wire](https://en.wikipedia.org/wiki/Barbed_wire) at a cost of around 1.6 billion [zÅ](https://en.wikipedia.org/wiki/Polish_z%C5%82oty) (**US$407m**) \\[...\\] in the late summer of 2021. The barrier was completed on 30 June 2022.[^\\[3\\]^](https://en.wikipedia.org/wiki/Belarus%E2%80%93Poland_border_barrier#cite_note-ctv1-3) An electronic barrier \\[...\\] was added to the fence between November 2022 and early summer 2023 at a cost of **EUR 71.8 million**.[^\\[4\\]^](https://en.wikipedia.org/wiki/Belarus%E2%80%93Poland_border_barrier#cite_note-tvp1-4)\n> \n> \\[...\\] official border crossings with Belarus remained open, and the asylum process continued to function \\[...\\]\n> \n> [In 2024 \\[...\\] Exluding those who submitted applications at airports, there were 3,141 \\[asylum applications from\\] persons coming directly from the territory of Belarus, Russia or Ukraine.](https://asylumineurope.org/reports/country/poland/asylum-procedure/access-procedure-and-registration/access-territory-and-push-backs/)\n> \n> Since the fence was built, **illegal crossings have reduced to a trickle**; however, between August 2021 and February 2023, 37 bodies were found on both sides of the border; people have died mainly from hypothermia or drowning.[^\\[11\\]^](https://en.wikipedia.org/wiki/Belarus%E2%80%93Poland_border_barrier#cite_note-asy225-11)\n\nThe Greenberg article also suggests a reasonable tradeoff is being made in policy\n\n> Despite these fears, Duszczyk is convinced his approach is working. In a two-month period after the asylum suspension, illegal crossings from Belarus fell by 48% compared to the same period in 2024. At the same time, in all of 2024, there was one deathâout of 30,000 attempted crossingsâin Polish territory. There have been none so far in 2025. Duszczyk feels his humanitarian floor is holding.  \n>",
      "plaintextDescription": "According to Wikipedia it seems to have worked well and not been expensive.\n\n> Poland began work on the 5.5-meter (18 foot) high steel wall topped with barbed wire at a cost of around 1.6 billion zÅ (US$407m) [...] in the late summer of 2021. The barrier was completed on 30 June 2022.[3] An electronic barrier [...] was added to the fence between November 2022 and early summer 2023 at a cost of EUR 71.8 million.[4]\n> \n> [...] official border crossings with Belarus remained open, and the asylum process continued to function [...]\n> \n> In 2024 [...] Exluding those who submitted applications at airports, there were 3,141 [asylum applications from] persons coming directly from the territory of Belarus, Russia or Ukraine.\n> \n> Since the fence was built, illegal crossings have reduced to a trickle; however, between August 2021 and February 2023, 37 bodies were found on both sides of the border; people have died mainly from hypothermia or drowning.[11]\n\nThe Greenberg article also suggests a reasonable tradeoff is being made in policy\n\n> Despite these fears, Duszczyk is convinced his approach is working. In a two-month period after the asylum suspension, illegal crossings from Belarus fell by 48% compared to the same period in 2024. At the same time, in all of 2024, there was one deathâout of 30,000 attempted crossingsâin Polish territory. There have been none so far in 2025. Duszczyk feels his humanitarian floor is holding.\n> Â "
    },
    "baseScore": 11,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-08T23:02:03.117Z",
    "af": false
  },
  {
    "_id": "tXR66PxmaC9ezG6kn",
    "postId": "TiQGC6woDMPJ9zbNM",
    "parentCommentId": "Xe6j5rfgGjkPyGmtH",
    "topLevelCommentId": "PkeGvBroRwmhFq2bj",
    "contents": {
      "markdown": "Maybe you're reading some other motivations into them, but if we just list the concerns in the article only 2 out of 11 indicate they want protectionism. The rest of the items that apply to AI include threats to conservative Christian values, threats to other conservative policies, and things we can mostly agree on. This gives a lot to ally on, especially the idea that Silicon Valley should not be allowed unaccountable rule over humanity, and that we should avoid destroying everything to beat China. It seems like a more viable alliance than with the fairness and bias people; plus conservatives have way more power right now.\n\n*   **Mass unemployment**\n*   **\"UBI-based communism\"**\n*   Acceleration to âbeat Chinaâ forces sacrifice of a \"happier future for your children and grandchildren\"\n*   Suppression of conservative ideas by big tech eg algorithmic suppression, demonetization\n*   Various ways that tech destroys family values\n    *   Social media / AI addiction\n    *   Grok's \"hentai sex bots\"\n    *   Transhumanism as an affront to God and to \"human dignity and human flourishing\"\n    *   \"Tech assaulting the Judeo-Christian faith...\"\n*   Tech \"destroying humanity\"\n*   Tech atrophying the brains of their children in school and destroying critical thought in universities.\n*   Rule by unaccountable Silicon Valley elites lacking national loyalty.",
      "plaintextDescription": "Maybe you're reading some other motivations into them, but if we just list the concerns in the article only 2 out of 11 indicate they want protectionism. The rest of the items that apply to AI include threats to conservative Christian values, threats to other conservative policies, and things we can mostly agree on. This gives a lot to ally on, especially the idea that Silicon Valley should not be allowed unaccountable rule over humanity, and that we should avoid destroying everything to beat China. It seems like a more viable alliance than with the fairness and bias people; plus conservatives have way more power right now.\n\n * Mass unemployment\n * \"UBI-based communism\"\n * Acceleration to âbeat Chinaâ forces sacrifice of a \"happier future for your children and grandchildren\"\n * Suppression of conservative ideas by big tech eg algorithmic suppression, demonetization\n * Various ways that tech destroys family values\n   * Social media / AI addiction\n   * Grok's \"hentai sex bots\"\n   * Transhumanism as an affront to God and to \"human dignity and human flourishing\"\n   * \"Tech assaulting the Judeo-Christian faith...\"\n * Tech \"destroying humanity\"\n * Tech atrophying the brains of their children in school and destroying critical thought in universities.\n * Rule by unaccountable Silicon Valley elites lacking national loyalty."
    },
    "baseScore": 24,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-09-08T22:43:14.090Z",
    "af": false
  },
  {
    "_id": "FRZEvkmjAPphMtKtF",
    "postId": "Qdgo2jYAuFRMeMRJT",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'm curious about your sense of the path towards AI safety applications, if you have a more specific and/or opinionated view than the conclusion/discussion section.",
      "plaintextDescription": "I'm curious about your sense of the path towards AI safety applications, if you have a more specific and/or opinionated view than the conclusion/discussion section."
    },
    "baseScore": 14,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-04T06:12:49.358Z",
    "af": false
  },
  {
    "_id": "YTguq3EvPK6Lae4LH",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "CK4cToSbQquRHy8xr",
    "topLevelCommentId": "pxuQygJx8LqNEfm3G",
    "contents": {
      "markdown": "My view is that AIs are improving faster at research-relevant skills like SWE and math than they're increasing at misalignment (rate of bad behaviors like reward hacking, ease of eliciting an alignment faker, etc) or covert sabotage ability, such that we would need a discontinuity in both to get serious danger by 2x. There is as yet no scaling law for misalignment showing that it predictably gets worse when capabilities improve in practice.\n\nThe situation is not completely clear because we don't have good alignment evals and could get neuralese any year, but the data are pointing in that direction. I'm not sure about research taste as the benchmarks for that aren't very good. I'd change my mind here if we did see stagnation in research taste plus misalignment getting worse over time (not just sophistication of the bad things AIs do, but also frequency or egregiousness).",
      "plaintextDescription": "My view is that AIs are improving faster at research-relevant skills like SWE and math than they're increasing at misalignment (rate of bad behaviors like reward hacking, ease of eliciting an alignment faker, etc) or covert sabotage ability, such that we would need a discontinuity in both to get serious danger by 2x. There is as yet no scaling law for misalignment showing that it predictably gets worse when capabilities improve in practice.\n\nThe situation is not completely clear because we don't have good alignment evals and could get neuralese any year, but the data are pointing in that direction. I'm not sure about research taste as the benchmarks for that aren't very good. I'd change my mind here if we did see stagnation in research taste plus misalignment getting worse over time (not just sophistication of the bad things AIs do, but also frequency or egregiousness)."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-04T05:00:17.637Z",
    "af": false
  },
  {
    "_id": "9vFGxTaAtCXJLkugh",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "ZK5d6d7oiC7KbcRGP",
    "topLevelCommentId": "pxuQygJx8LqNEfm3G",
    "contents": {
      "markdown": "That is, you think alignment is so difficult that keeping humanity alive for 3 years is more valuable than the possibility of us solving alignment during the pause? Or that the AIs will sabotage the project in a way undetectable by management even if management is very paranoid about being sabotaged by any model that has shown prerequisite capabilities for it?",
      "plaintextDescription": "That is, you think alignment is so difficult that keeping humanity alive for 3 years is more valuable than the possibility of us solving alignment during the pause? Or that the AIs will sabotage the project in a way undetectable by management even if management is very paranoid about being sabotaged by any model that has shown prerequisite capabilities for it?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-03T22:50:49.886Z",
    "af": false
  },
  {
    "_id": "spMfsMN5BGcFpkivE",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "pxuQygJx8LqNEfm3G",
    "topLevelCommentId": "pxuQygJx8LqNEfm3G",
    "contents": {
      "markdown": "Ways this could be wrong:\n\n*   We can pause early (before AIs pose significant risk) at little cost\n*   We must pause early (AIs pose significant risk before they speed up research much). I think this is mostly ruled out by current evidence\n*   Safety research inherently has to be done by humans because it's less verifiable, even when capabilities is automated\n*   AI lab CEOs are good at management of safety research because their capabilities experience transfers (in this case I'd still much prefer Buck Shlegeris or Sam Altman over the US or China governments)\n*   It's easy to pause indefinitely once everyone realizes AIs are imminently dangerous, kind of like the current situation with nuclear\n*   Probably others I'm not thinking of",
      "plaintextDescription": "Ways this could be wrong:\n\n * We can pause early (before AIs pose significant risk) at little cost\n * We must pause early (AIs pose significant risk before they speed up research much). I think this is mostly ruled out by current evidence\n * Safety research inherently has to be done by humans because it's less verifiable, even when capabilities is automated\n * AI lab CEOs are good at management of safety research because their capabilities experience transfers (in this case I'd still much prefer Buck Shlegeris or Sam Altman over the US or China governments)\n * It's easy to pause indefinitely once everyone realizes AIs are imminently dangerous, kind of like the current situation with nuclear\n * Probably others I'm not thinking of"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-03T22:11:13.250Z",
    "af": false
  },
  {
    "_id": "Bvwawce6G7vMyMADJ",
    "postId": "FG54euEAesRkSZuJN",
    "parentCommentId": "aEZyXktJonCyvkop5",
    "topLevelCommentId": "6ue8BPWrcoa2eGJdP",
    "contents": {
      "markdown": "The data is pretty low-quality for that graph because the agents we used were inconsistent and Claude 3-level models could barely solve any tasks. Epoch has better data for SWE-bench Verified, which I converted to time horizon [here](https://metr.org/blog/2025-07-14-how-does-time-horizon-vary-across-domains/) and found to also be doubling every 4 months ish. Their elicitation is probably not as good for OpenAI as Anthropic models, but both are increasing at similar rates.",
      "plaintextDescription": "The data is pretty low-quality for that graph because the agents we used were inconsistent and Claude 3-level models could barely solve any tasks. Epoch has better data for SWE-bench Verified, which I converted to time horizon here and found to also be doubling every 4 months ish. Their elicitation is probably not as good for OpenAI as Anthropic models, but both are increasing at similar rates."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-03T20:51:55.905Z",
    "af": true
  },
  {
    "_id": "LaxetEYWpLz8LxThS",
    "postId": "cxuzALcmucCndYv4a",
    "parentCommentId": "jhCCPGFvrAz2cRKLk",
    "topLevelCommentId": "jhCCPGFvrAz2cRKLk",
    "contents": {
      "markdown": "What's the current best known solution to the 5 and 10 problem? I feel like actual agents have heuristic self-models rather than this cursed logical counterfactual thing and so there's no guarantee a solution exists. But I don't even know what formal properties we want, so I also don't know whether we have impossibility theorems, some properties have gone out of fashion, or people think it can still work.",
      "plaintextDescription": "What's the current best known solution to the 5 and 10 problem? I feel like actual agents have heuristic self-models rather than this cursed logical counterfactual thing and so there's no guarantee a solution exists. But I don't even know what formal properties we want, so I also don't know whether we have impossibility theorems, some properties have gone out of fashion, or people think it can still work.Â "
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-03T20:44:06.348Z",
    "af": false
  },
  {
    "_id": "pxuQygJx8LqNEfm3G",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "In a world with an AI pause, good management is **much** more important than the pause length. If the pause is government-mandated and happens late, I would prefer a 1-year pause with say Buck Shlegeris in charge over a 3-year pause with the average AI lab lead in charge.\n\nThe reason is that most of the safety research during the pause will be done by AIs. In a pause scenario where >50% of resources are temporarily dedicated to safety and >10x uplift is technically plausible, the speed of research is mainly limited by the level of AI we trust and our ability to understand the AI's findings. The world probably can't pause indefinitely, and it may not be desirable to do so. So what is the basic strategy for a finite AI pause?\n\n*   **Use superhuman AIs as that are smart as possible to do safety research, but not so capable that they're misaligned.** This may require avoiding use of latest gen models, or advancing capabilities and using the next gen only for safety.\n*   **Use an elaborate monitoring framework and incrementally apply the safety research generated** to increase the maximum capability level we can run at without being sabotaged when models are misaligned.\n*   **Prevent capability advances from being stolen**, so no rogue actor can break out and threaten DSA, forcing the pause to end.\n\nThe variance in AI speed will be huge as models get smarter and more efficient. Conditional on us needing a pause, I imagine that the situation will look something like: Agent-2 speeds up work by 10x and incurs 0.2%/year x-risk, Agent-3 has 20x speed and 1.0%/year x-risk, Agent-4 has 40x speed and 5.0%/year x-risk, and so on. This means there is huge value in **advancing capabilities in ways that that donât reduce safety**. For example, maybe alignment-specific training data doubles speed while preserving alignment, or a new architecture gets most of the benefits of âneuraleseâ without compromising monitorability.\n\nI expect that in a pause, good vs bad management could make a >10x difference in rate of safety research progress, with some of this coming from targeting the appropriate risk level, some from better handling ambiguous evidence about safety of current models, and some from differentially advancing safe capabilities. In comparison, the amount of political will and diplomacy that will be required to make a pause 10x longer is immense.\n\nPause length is still valuable inasmuch as it gives more time for management to understand the situation, but is not the most important factor in whether the pause actually succeeds at solving alignment.",
      "plaintextDescription": "In a world with an AI pause, good management is much more important than the pause length. If the pause is government-mandated and happens late, I would prefer a 1-year pause with say Buck Shlegeris in charge over a 3-year pause with the average AI lab lead in charge.\n\nThe reason is that most of the safety research during the pause will be done by AIs. In a pause scenario where >50% of resources are temporarily dedicated to safety and >10x uplift is technically plausible, the speed of research is mainly limited by the level of AI we trust and our ability to understand the AI's findings. The world probably can't pause indefinitely, and it may not be desirable to do so. So what is the basic strategy for a finite AI pause?\n\n * Use superhuman AIs as that are smart as possible to do safety research, but not so capable that they're misaligned. This may require avoiding use of latest gen models, or advancing capabilities and using the next gen only for safety.\n * Use an elaborate monitoring framework and incrementally apply the safety research generated to increase the maximum capability level we can run at without being sabotaged when models are misaligned.\n * Prevent capability advances from being stolen, so no rogue actor can break out and threaten DSA, forcing the pause to end.\n\nThe variance in AI speed will be huge as models get smarter and more efficient. Conditional on us needing a pause, I imagine that the situation will look something like: Agent-2 speeds up work by 10x and incurs 0.2%/year x-risk, Agent-3 has 20x speed and 1.0%/year x-risk, Agent-4 has 40x speed and 5.0%/year x-risk, and so on. This means there is huge value in advancing capabilities in ways that that donât reduce safety. For example, maybe alignment-specific training data doubles speed while preserving alignment, or a new architecture gets most of the benefits of âneuraleseâ without compromising monitorability.\n\nI expect that in a pause, good vs bad management could make a >10x difference in rat"
    },
    "baseScore": 31,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2025-09-03T19:56:55.576Z",
    "af": false
  },
  {
    "_id": "3PFTnzGXtrPZ65wJK",
    "postId": "PBd7xPAh22y66rbme",
    "parentCommentId": "HP5Hwxw6qfQetutEw",
    "topLevelCommentId": "tW3QAEfFy5AfTJyDb",
    "contents": {
      "markdown": "I have a better idea now what you intend. At risk of violating the \"Not worth getting into?\" react, I still don't think the title is as informative as it could be; summarizing on the object level would be clearer than saying their actions were similar to actions of \"moderate accelerationists\", which isn't a term you define in the post or try to clarify the connotations of.\n\nWho is a \"moderate communist\"? Hu Jintao, who ran the CCP but in a state capitalism way? Zohran Mamdani, because democratic socialism is sort of halfway to communism? It's an inherently vague term until defined, and so is \"moderate accelerationists\".\n\nI would be fine with the title if you explained it somewhere, with a sentence in the intro and/or conclusion like \"Anthropic have disappointingly acted as 'moderate accelerationists' who put at least as much resource into accelerating the development of AGI as ensuring it is safe\", or whatever version of this you endorse. As it is some readers, or at least I, have to think\n\n*   does Remmelt think that Anthropic's actions would also be taken by people who believe extinction by entropy-maximizing robots is only sort of bad?\n*   Or is it that Remmelt thinks that Anthropic is acting like a company who think the social benefits of speeding up AI could outweigh the costs?\n*   Or is the post trying to claim that ~half of Anthropic's actions sped up AI against their informal commitments?\n\nThis kind of triply recursive intention guessing is why I think the existing title is confusing.\n\nAlternatively, the title could be something different like \"Anthropic founders sped AI and abandoned many safety commitments\" or even \"Anthropic was not consistently candid about its priorities\". In any case it's not clear to me that it's worth changing vs making some kind of minor clarification.",
      "plaintextDescription": "I have a better idea now what you intend. At risk of violating the \"Not worth getting into?\" react, I still don't think the title is as informative as it could be; summarizing on the object level would be clearer than saying their actions were similar to actions of \"moderate accelerationists\", which isn't a term you define in the post or try to clarify the connotations of.\n\nWho is a \"moderate communist\"? Hu Jintao, who ran the CCP but in a state capitalism way? Zohran Mamdani, because democratic socialism is sort of halfway to communism? It's an inherently vague term until defined, and so is \"moderate accelerationists\".\n\nI would be fine with the title if you explained it somewhere, with a sentence in the intro and/or conclusion like \"Anthropic have disappointingly acted as 'moderate accelerationists' who put at least as much resource into accelerating the development of AGI as ensuring it is safe\", or whatever version of this you endorse. As it is some readers, or at least I, have to think\n\n * does Remmelt think that Anthropic's actions would also be taken by people who believe extinction by entropy-maximizing robots is only sort of bad?\n * Or is it that Remmelt thinks that Anthropic is acting like a company who think the social benefits of speeding up AI could outweigh the costs?\n * Or is the post trying to claim that ~half of Anthropic's actions sped up AI against their informal commitments?\n\nThis kind of triply recursive intention guessing is why I think the existing title is confusing.\n\nAlternatively, the title could be something different like \"Anthropic founders sped AI and abandoned many safety commitments\" or even \"Anthropic was not consistently candid about its priorities\". In any case it's not clear to me that it's worth changing vs making some kind of minor clarification."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-03T10:09:32.732Z",
    "af": false
  },
  {
    "_id": "Hy28AwukDvzoYAHds",
    "postId": "qtZ5CgDWe3BuiH8Wn",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'm writing a post about whether we'll have a Dyson Swarm reach Earth's current energy generation level before CA High-Speed Rail is complete. It seems plausible, if incompetence and nimbys delay CAHSR by a few years past ASI but space-based construction starts right after ASI.",
      "plaintextDescription": "I'm writing a post about whether we'll have a Dyson Swarm reach Earth's current energy generation level before CA High-Speed Rail is complete. It seems plausible, if incompetence and nimbys delay CAHSR by a few years past ASI but space-based construction starts right after ASI."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-02T14:48:42.151Z",
    "af": false
  },
  {
    "_id": "Cc9zEsxwjkS5FCuhG",
    "postId": "fdCaCDfstHxyPmB9h",
    "parentCommentId": "NksPrPW5KyZ2W5KNs",
    "topLevelCommentId": "v2kytzaT5sCgBjLsx",
    "contents": {
      "markdown": "In [many industries](https://en.wikipedia.org/wiki/Experience_curve_effects) cost decreases by some factor with every doubling of cumulative production. This is how solar eventually became economically viable.",
      "plaintextDescription": "In many industries cost decreases by some factor with every doubling of cumulative production. This is how solar eventually became economically viable."
    },
    "baseScore": 15,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-02T04:31:09.458Z",
    "af": false
  },
  {
    "_id": "tW3QAEfFy5AfTJyDb",
    "postId": "PBd7xPAh22y66rbme",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Title is confusing and maybe misleading, when I see \"accelerationists\" I think either e/acc or the idea that we should hasten the collapse of society in order to bring about a communist, white supremacist, or other extremist utopia. This is different from accelerating AI progress and, as far as I know, not the motivation of most people at Anthropic.",
      "plaintextDescription": "Title is confusing and maybe misleading, when I see \"accelerationists\" I think either e/acc or the idea that we should hasten the collapse of society in order to bring about a communist, white supremacist, or other extremist utopia. This is different from accelerating AI progress and, as far as I know, not the motivation of most people at Anthropic."
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-09-02T02:14:56.317Z",
    "af": false
  },
  {
    "_id": "usfnCBoBDoYCET5Mg",
    "postId": "bkjqfhKd8ZWHK9XqF",
    "parentCommentId": "5v8hkzg7XEEkjYKw3",
    "topLevelCommentId": "6pbZg7aQNSX8s4WB5",
    "contents": {
      "markdown": "I considered these factors. There are advantages too; the knapping subreddit also says it's easier to work, and with less dust the silicosis risk is lower. But the crucial one is it's much more exciting to make something molecularly sharp.\n\nThe cost seems acceptable because it's still lower than most hobbies. I'm mostly concerned that the required PPE will ruin the experience as quite a lot is recommended: gloves, mask, eye protection, good ventilation, a tarp to catch stray obsidian shards. Most of these are still needed for flint.\n\nMaybe this is why knapping isn't so popular compared to other crafts like knitting and woodworking; it seems to be both tedious and just hazardous enough to avoid for rich societies that place extremely high value on health. People don't persistence hunt gazelles for fun; they use guns or more likely play football and videogames.",
      "plaintextDescription": "I considered these factors. There are advantages too; the knapping subreddit also says it's easier to work, and with less dust the silicosis risk is lower. But the crucial one is it's much more exciting to make something molecularly sharp.\n\nThe cost seems acceptable because it's still lower than most hobbies. I'm mostly concerned that the required PPE will ruin the experience as quite a lot is recommended: gloves, mask, eye protection, good ventilation, a tarp to catch stray obsidian shards. Most of these are still needed for flint.\n\nMaybe this is why knapping isn't so popular compared to other crafts like knitting and woodworking; it seems to be both tedious and just hazardous enough to avoid for rich societies that place extremely high value on health. People don't persistence hunt gazelles for fun; they use guns or more likely play football and videogames."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-31T19:55:56.551Z",
    "af": false
  },
  {
    "_id": "6pbZg7aQNSX8s4WB5",
    "postId": "bkjqfhKd8ZWHK9XqF",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This post convinced me to buy 10 pounds of obsidian and basic knapping tools. I may give an update on whether knapping is fun and/or causes me severe injury, though I suspect that the learning curve is low hundreds of hours for proficiency and I'm unlikely to stick with it that long.",
      "plaintextDescription": "This post convinced me to buy 10 pounds of obsidian and basic knapping tools. I may give an update on whether knapping is fun and/or causes me severe injury, though I suspect that the learning curve is low hundreds of hours for proficiency and I'm unlikely to stick with it that long."
    },
    "baseScore": 28,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2025-08-30T01:25:29.959Z",
    "af": false
  },
  {
    "_id": "JMDtfLM3o7xvgZ4QE",
    "postId": "FkiiW2rnwAvNpEaiw",
    "parentCommentId": "YMstXnK2r9QXrsDFx",
    "topLevelCommentId": "2mBn78yxFyAariKpc",
    "contents": {
      "markdown": "Agree, especially from developing countries without a strong preexisting stance on AI, where choices could be less biased towards experts who already have lots of prestige, and more weighted on merits + lobbying.",
      "plaintextDescription": "Agree, especially from developing countries without a strong preexisting stance on AI, where choices could be less biased towards experts who already have lots of prestige, and more weighted on merits + lobbying."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-28T19:19:07.070Z",
    "af": false
  },
  {
    "_id": "sHwetaEWS9videxMy",
    "postId": "FkiiW2rnwAvNpEaiw",
    "parentCommentId": "2mBn78yxFyAariKpc",
    "topLevelCommentId": "2mBn78yxFyAariKpc",
    "contents": {
      "markdown": "Item 3 has some constraints on members (emphasis mine):\n\n> Requests the Secretary-General to launch a published criteria-based open call and to recommend a list of 40 members of the Panel to be appointed by the General Assembly in a time-bound manner, on the basis of their outstanding expertise in artificial intelligence and related fields, an interdisciplinary perspective, and **geographical and gender balance**, taking into account **candidacies from a broad representation of varying levels of technological development, including from developing countries**, with due consideration to nominations from Member States and with **no more than two selected candidates of the same nationality or affiliation** and no employees of the United Nations system;\n\nThis means only two each from US, UK, China, etc. I wonder what the geographic and gender balance will actually look like; these will significantly influence the average expertise type and influence of members.\n\nMy guess is that x-risk mitigation will not be the primary focus at first just because over half of the experts are American and British men and there are so many other interests to represent. Nor would industry be heavily represented because it skews too American (and the document mentioned CoIs, and the 7 goals of the Dialogue are mostly not about frontier capabilities). But in the long term, unless takeoff is fast, developing countries will realize the US is marching towards DSA and interesting things could happen.\n\nEdit: my guess for the composition would be similar to the existing [High-level Advisory Body on Artificial Intelligence](https://www.un.org/en/ai-advisory-body/members), with 39 members, of which\n\n*   19 men, 20 women\n*   15 academia/research (including 10 professors), 10 government, 4 from big tech and scaling labs, 10 other\n*   17 Responsibility / Safety / Policy oversight positions, 22 other positions\n*   Nationalities:\n    *   9 Americas (incl. 4 US), 11 Europe, 11 Asia (incl. 2 China), 1 Oceania, 5 Africa. They heavily overweighted Europe, which has 28% of the seats but only 9% of world population\n    *   19 high income countries, 24 LMIC\n    *   gpt5 isn't sure about some of the dual nationality cases though\n*   1 big name in x-risk (Jaan Tallinn)",
      "plaintextDescription": "Item 3 has some constraints on members (emphasis mine):\n\n> Requests the Secretary-General to launch a published criteria-based open call and to recommend a list of 40 members of the Panel to be appointed by the General Assembly in a time-bound manner, on the basis of their outstanding expertise in artificial intelligence and related fields, an interdisciplinary perspective, and geographical and gender balance, taking into account candidacies from a broad representation of varying levels of technological development, including from developing countries, with due consideration to nominations from Member States and with no more than two selected candidates of the same nationality or affiliation and no employees of the United Nations system;\n\nThis means only two each from US, UK, China, etc. I wonder what the geographic and gender balance will actually look like; these will significantly influence the average expertise type and influence of members.\n\nMy guess is that x-risk mitigation will not be the primary focus at first just because over half of the experts are American and British men and there are so many other interests to represent. Nor would industry be heavily represented because it skews too American (and the document mentioned CoIs, and the 7 goals of the Dialogue are mostly not about frontier capabilities). But in the long term, unless takeoff is fast, developing countries will realize the US is marching towards DSA and interesting things could happen.\n\nEdit: my guess for the composition would be similar to the existing High-level Advisory Body on Artificial Intelligence, with 39 members, of which\n\n * 19 men, 20 women\n * 15 academia/research (including 10 professors), 10 government, 4 from big tech and scaling labs, 10 other\n * 17 Responsibility / Safety / Policy oversight positions, 22 other positions\n * Nationalities:\n   * 9 Americas (incl. 4 US), 11 Europe, 11 Asia (incl. 2 China), 1 Oceania, 5 Africa. They heavily overweighted Europe, which has 28% of th"
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-28T03:04:18.693Z",
    "af": false
  },
  {
    "_id": "ts99XMeaSbPZFew8y",
    "postId": "98sCTsGJZ77WgQ6nE",
    "parentCommentId": "BirTiqtqmwY2JNzg5",
    "topLevelCommentId": "BirTiqtqmwY2JNzg5",
    "contents": {
      "markdown": "FWIW I feel like I get sufficient status reward for criticism and this moderation decision basically won't affect my behavior\n\n*   [This](https://www.lesswrong.com/posts/5CGNxadG3JRbGfGfg/notes-on-the-long-tasks-metr-paper-from-a-hcast-task?commentId=8uCFQfQtpkMqi8R5F) defended a paper where I was lead author, which got 8 million views on Twitter and was possibly the most important research output by my current employer, against criticism that it was p-hacking\n*   [This](https://www.lesswrong.com/posts/NBZvpcBx4ewqkdCdT/do-you-believe-in-hundred-dollar-bills-lying-on-the-ground-1?commentId=fargcXxr9ecc7vSQc) got me a bounty of $700 or so (which I think I declined or forgot about?) and citation in a follow-up post\n*   [This](https://www.lesswrong.com/posts/qgSEbLfZpH2Yvrdzm/i-tried-reproducing-that-lancet-study-about-usaid-cuts-so?commentId=gQuJZnvKLXAR4uFym) ratioed the OP by 3:1 and induced a thoughtful response by OP that helped me learn some nontrivial stats facts\n*   [This](https://www.lesswrong.com/posts/FijbeqdovkgAusGgz/grey-goo-is-unlikely?commentId=NvnvDGgkCK4otH7uE) got 73 karma and was the most important counterpoint to what I still think are mostly wrong and overrated views on nanotech\n*   [This](https://www.lesswrong.com/posts/6jytXo5HmR9HLvkzu/why-was-the-ai-alignment-community-so-unprepared-for-this?commentId=7mDrn8qWPBcXphPS2) got 70 karma and only took about an hour to write, and could have been 5 minutes if I were a better writer\n\nNow it's true that most of these comments are super long and high effort. But it's possible to get status reward for lower effort comments too, e.g. [this](https://www.lesswrong.com/posts/Zfik4xESDyahRALKk/yann-lecun-on-agi-and-ai-safety?commentId=wHSauCTzY3bc4gHm9), though it feels more like springing a \"gotcha\". Many of the examples of Said's critiques in the post at least seemed either deliberately inflammatory or unhelpful or targeted at some procedural point that isn't maximally relevant.\n\nAs for risking being wrong, [this](https://www.lesswrong.com/posts/HE3Styo9vpk7m8zi4/evhub-s-shortform?commentId=GtcufzdxyipTqASif) is the only \"bad\" recent comment of mine I can remember, and I think you have to be pretty risk averse to be totally discouraged from commenting. If 30% of my comments were wrong I would probably feel discouraged but if it were 15% I'd just be less confident or hedge more. Probably the main change I'll make is to shift away from this uncommon and very marginal type of comment that imposes costs on the author and might be wrong, to just downvote and move on.",
      "plaintextDescription": "FWIW I feel like I get sufficient status reward for criticism and this moderation decision basically won't affect my behavior\n\n * This defended a paper where I was lead author, which got 8 million views on Twitter and was possibly the most important research output by my current employer, against criticism that it was p-hacking\n * This got me a bounty of $700 or so (which I think I declined or forgot about?) and citation in a follow-up post\n * This ratioed the OP by 3:1 and induced a thoughtful response by OP that helped me learn some nontrivial stats facts\n * This got 73 karma and was the most important counterpoint to what I still think are mostly wrong and overrated views on nanotech\n * This got 70 karma and only took about an hour to write, and could have been 5 minutes if I were a better writer\n\nNow it's true that most of these comments are super long and high effort. But it's possible to get status reward for lower effort comments too, e.g. this, though it feels more like springing a \"gotcha\". Many of the examples of Said's critiques in the post at least seemed either deliberately inflammatory or unhelpful or targeted at some procedural point that isn't maximally relevant.\n\nAs for risking being wrong, this is the only \"bad\" recent comment of mine I can remember, and I think you have to be pretty risk averse to be totally discouraged from commenting. If 30% of my comments were wrong I would probably feel discouraged but if it were 15% I'd just be less confident or hedge more. Probably the main change I'll make is to shift away from this uncommon and very marginal type of comment that imposes costs on the author and might be wrong, to just downvote and move on."
    },
    "baseScore": 35,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-08-24T21:27:48.090Z",
    "af": false
  },
  {
    "_id": "DD987oQsAaDWEmEsR",
    "postId": "DALNiKBjMGMF8grme",
    "parentCommentId": "wwk9cG3XqghWWBmuS",
    "topLevelCommentId": "rNChn5r6a7XEcxmfW",
    "contents": {
      "markdown": "This man's modus ponens is definitely my modus tollens. It seems super cursed to use moral premises to answer metaphysics problems. In this argument, except for step 8, you can replace belief in free will with anything, and the argument says that determinism implies that any widely held belief is true.\n\n\"Ought implies can\" should be something that's true by construction of your moral system, rather than something you can just assert about an arbitrary moral system and use to derive absurd conclusions.",
      "plaintextDescription": "This man's modus ponens is definitely my modus tollens. It seems super cursed to use moral premises to answer metaphysics problems. In this argument, except for step 8, you can replace belief in free will with anything, and the argument says that determinism implies that any widely held belief is true.\n\n\"Ought implies can\" should be something that's true by construction of your moral system, rather than something you can just assert about an arbitrary moral system and use to derive absurd conclusions."
    },
    "baseScore": 11,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-22T03:53:07.915Z",
    "af": false
  },
  {
    "_id": "PYswRZB7xnAdh378a",
    "postId": "iLHe3vLur3NgrFPFy",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "How does your work differ from [*Forking Paths in Neural Text Generation*](https://openreview.net/forum?id=8RCmNLeeXx) (Bigelow et al.) from ICLR 2025?",
      "plaintextDescription": "How does your work differ from Forking Paths in Neural Text Generation (Bigelow et al.) from ICLR 2025?"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-19T00:59:59.882Z",
    "af": true
  },
  {
    "_id": "EsDHmbzE8GrjuhziZ",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "PJ3cznRtvojJ6quxH",
    "topLevelCommentId": "iwGgqsmpY6Tcex5je",
    "contents": {
      "markdown": "The big one is [Time on the Cross](https://en.wikipedia.org/wiki/Time_on_the_Cross:_The_Economics_of_American_Negro_Slavery) by Fogel. Wikipedia says\n\n> The book argued that slavery was an economically rational institution and that the economic exploitation of slaves was not as catastrophic as presumed \\[...\\]\n\nFrom what I can tell everyone thinks the conclusions around economic efficiency were true\n\n> Many in the historical community were impressed with the authors' application of [cliometrics](https://en.wikipedia.org/wiki/Cliometrics). In general historians and economists agree with the conclusion that slavery was efficient and economically viable but had more mixed attitudes towards the material well being of slaves.[^\\[5\\]^](https://en.wikipedia.org/wiki/Time_on_the_Cross:_The_Economics_of_American_Negro_Slavery#cite_note-5)",
      "plaintextDescription": "The big one is Time on the Cross by Fogel. Wikipedia says\n\n> The book argued that slavery was an economically rational institution and that the economic exploitation of slaves was not as catastrophic as presumed [...]\n\nFrom what I can tell everyone thinks the conclusions around economic efficiency were true\n\n> Many in the historical community were impressed with the authors' application of cliometrics. In general historians and economists agree with the conclusion that slavery was efficient and economically viable but had more mixed attitudes towards the material well being of slaves.[5]"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-18T19:20:08.805Z",
    "af": false
  },
  {
    "_id": "fYqMjv6Q7T4odjKHA",
    "postId": "cxuzALcmucCndYv4a",
    "parentCommentId": "d6FGkgseakEhQN3zG",
    "topLevelCommentId": "C8RJfs2DiMzPoGt7P",
    "contents": {
      "markdown": "Under the logistic methodology where we don't actually have long enough tasks to measure the 50% point, sure. But if we actually have years-long tasks, a true superhuman coder should be able to do them more reliably than humans, which is more than 50% if we filter the problem distribution to things humans can do with more than about 50% probability. There are other methodologies that I think are more meaningful, where it might also make sense to have the SC's time horizon be infinity.",
      "plaintextDescription": "Under the logistic methodology where we don't actually have long enough tasks to measure the 50% point, sure. But if we actually have years-long tasks, a true superhuman coder should be able to do them more reliably than humans, which is more than 50% if we filter the problem distribution to things humans can do with more than about 50% probability. There are other methodologies that I think are more meaningful, where it might also make sense to have the SC's time horizon be infinity."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-18T18:12:37.197Z",
    "af": false
  },
  {
    "_id": "kXW5rsrgqGPQ4kEFt",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "KwLJHZqwGab2BYvxw",
    "topLevelCommentId": "iwGgqsmpY6Tcex5je",
    "contents": {
      "markdown": "Various papers say this, and GPT5 [thinks it's true](https://chatgpt.com/share/68a36858-7d28-800a-9685-5e625ed054bb) even accounting for enforcement costs. As one would expect, it's not true if you also subtract things like health costs to enslaved people themselves.",
      "plaintextDescription": "Various papers say this, and GPT5 thinks it's true even accounting for enforcement costs. As one would expect, it's not true if you also subtract things like health costs to enslaved people themselves."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-18T18:00:12.026Z",
    "af": false
  },
  {
    "_id": "iwGgqsmpY6Tcex5je",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Sharecroppers in the US had lower labor productivity than 1860 slaves, due to factors like lower economies of scale and unwillingness to organize into gang labor systems. So when did Black Americans' wages catch up to what they should have been paid in 1860 (based on productivity and average free labor share of income)? GPT5 thinks not until [around 1900](https://chatgpt.com/share/68a2ca04-2310-800a-9758-3b2fa24a0c26), at which point the US GDP per capita had about doubled. This ignores the probable decrease in labor hours though.",
      "plaintextDescription": "Sharecroppers in the US had lower labor productivity than 1860 slaves, due to factors like lower economies of scale and unwillingness to organize into gang labor systems. So when did Black Americans' wages catch up to what they should have been paid in 1860 (based on productivity and average free labor share of income)? GPT5 thinks not until around 1900, at which point the US GDP per capita had about doubled. This ignores the probable decrease in labor hours though."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-18T06:50:47.901Z",
    "af": false
  },
  {
    "_id": "GtcufzdxyipTqASif",
    "postId": "HE3Styo9vpk7m8zi4",
    "parentCommentId": "PqLE6iMLSttiSuTjM",
    "topLevelCommentId": "n4aytCR9NBgNHhFpf",
    "contents": {
      "markdown": "One definition of \"safetywashing\" is, by analogy with [greenwashing](https://en.m.wikipedia.org/wiki/Greenwashing), a situation where\n\n*   (1) The desire for good PR drives companies to advertise their safety. This leads to no research, fake research, or research that is safety-themed but where few people involved actually care about x-risk, resulting in minimal expected safety impact.\n\nHendrycks has an alternate definition from a 2024 NeurIPS paper, where\n\n*   (2) a benchmark is safety-washed if it is advertised as a safety benchmark but highly correlated with general capabilities, and thus mostly incentivizes devs to improve general capabilities rather than safety. (Incidentally TruthfulQA is one of these, but this doesn't make all papers using it safetywashed)\n\nUnder neither definition is the Anthropic fellows program safetywashing. (1) is not true because at least 4/6 (I would argue around 5.25/6) of the research is actually targeted at safety, mentors care about safety, and there is likely positive expected impact. I will also note that in definition (1) at least, intent is necessary to the classification of safetywashing, so \"accidental safetywashing\" is about as much a non sequitur as \"accidental first-degree murder\".\n\nÂ (2) is not true even if you expand the definition to include advertising capabilities research as safety, because the one dual use paper still has safety applications and is advertised as capabilities! My main concerns with (2) are the dozens of benchmarks and papers since 2023 that use \"alignment\" as a buzzword, and attempts by companies to portray as safety techniques that would be necessary for their product anyway. This is why I think Ethan Perez sometimes mentoring dual-use papers or papers that end up making non-safety observations in the larger context of safety projects is neither a good example of safetywashing nor an important concern.",
      "plaintextDescription": "One definition of \"safetywashing\" is, by analogy with greenwashing, a situation where\n\n * (1) The desire for good PR drives companies to advertise their safety. This leads to no research, fake research, or research that is safety-themed but where few people involved actually care about x-risk, resulting in minimal expected safety impact.\n\nHendrycks has an alternate definition from a 2024 NeurIPS paper, where\n\n * (2) a benchmark is safety-washed if it is advertised as a safety benchmark but highly correlated with general capabilities, and thus mostly incentivizes devs to improve general capabilities rather than safety. (Incidentally TruthfulQA is one of these, but this doesn't make all papers using it safetywashed)\n\nUnder neither definition is the Anthropic fellows program safetywashing. (1) is not true because at least 4/6 (I would argue around 5.25/6) of the research is actually targeted at safety, mentors care about safety, and there is likely positive expected impact. I will also note that in definition (1) at least, intent is necessary to the classification of safetywashing, so \"accidental safetywashing\" is about as much a non sequitur as \"accidental first-degree murder\".\n\nÂ (2) is not true even if you expand the definition to include advertising capabilities research as safety, because the one dual use paper still has safety applications and is advertised as capabilities! My main concerns with (2) are the dozens of benchmarks and papers since 2023 that use \"alignment\" as a buzzword, and attempts by companies to portray as safety techniques that would be necessary for their product anyway. This is why I think Ethan Perez sometimes mentoring dual-use papers or papers that end up making non-safety observations in the larger context of safety projects is neither a good example of safetywashing nor an important concern."
    },
    "baseScore": 1,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-14T09:48:18.043Z",
    "af": false
  },
  {
    "_id": "PhytWaivxnE3pxchf",
    "postId": "cxuzALcmucCndYv4a",
    "parentCommentId": "C8RJfs2DiMzPoGt7P",
    "topLevelCommentId": "C8RJfs2DiMzPoGt7P",
    "contents": {
      "markdown": "I talked to the AI Futures team in person and shared roughly these thoughts:\n\n*   Time horizon as measured in the original paper is underspecified in several ways.\n*   Time horizon [varies by domain](https://www.lesswrong.com/posts/6KcP7tEe5hgvHbrSF/metr-how-does-time-horizon-vary-across-domains) and AI companies have multiple types of work. It is not clear if HCAST time horizon will be a constant factor longer than realistic time horizon, but it's a reasonable guess.\n*   As I see it, task lengths for time horizon should be something like the average amount of labor spent on each instance of a task by actual companies, and all current methodologies are approximations of this.\n*   To convert time horizon to speedup, you would need to estimate the [average labor involved in supervising an AI on a task](https://www.lesswrong.com/posts/Zr37dY5YPRT6s56jY/thomas-kwa-s-shortform#z5Riqicyjo8m3GD67) that would take a human X hours and the AI can do with reliability Y, which we currently don't have data on.\n*   As I see it, time horizon is in theory superexponential, as it has to go to infinity when we get AGI / superhuman coder. But the current data is not good enough to just fit a superexponential and get a timelines forecast. It could already be superexponential, or it could only go superexponential after time horizon hits 10 years.\n*   Cursor and the Claude Code team probably already have data that tracks the speed of generations, plus how long humans spend reading AI code, correcting AI mistakes, and supervising AI in other ways, that one could construct a better forecast from.\n*   It is also unclear what speedup an AI with infinite software time horizon would bring to AI R&D, because this would depend on its speed at doing existing tasks, how many useful novel tasks it invents that humans can't do, and ability to interface with non-software parts of the business.",
      "plaintextDescription": "I talked to the AI Futures team in person and shared roughly these thoughts:\n\n * Time horizon as measured in the original paper is underspecified in several ways.\n * Time horizon varies by domain and AI companies have multiple types of work. It is not clear if HCAST time horizon will be a constant factor longer than realistic time horizon, but it's a reasonable guess.\n * As I see it, task lengths for time horizon should be something like the average amount of labor spent on each instance of a task by actual companies, and all current methodologies are approximations of this.\n * To convert time horizon to speedup, you would need to estimate the average labor involved in supervising an AI on a task that would take a human X hours and the AI can do with reliability Y, which we currently don't have data on.\n * As I see it, time horizon is in theory superexponential, as it has to go to infinity when we get AGI / superhuman coder. But the current data is not good enough to just fit a superexponential and get a timelines forecast. It could already be superexponential, or it could only go superexponential after time horizon hits 10 years.\n * Cursor and the Claude Code team probably already have data that tracks the speed of generations, plus how long humans spend reading AI code, correcting AI mistakes, and supervising AI in other ways, that one could construct a better forecast from.\n * It is also unclear what speedup an AI with infinite software time horizon would bring to AI R&D, because this would depend on its speed at doing existing tasks, how many useful novel tasks it invents that humans can't do, and ability to interface with non-software parts of the business."
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-13T22:01:38.516Z",
    "af": true
  },
  {
    "_id": "E9uuA3YoBkxxCbJat",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "x7LmBj6REFDugEsCv",
    "topLevelCommentId": "x7LmBj6REFDugEsCv",
    "contents": {
      "markdown": "A [blog post](http://toughsf.blogspot.com/2025/05/the-laser-revolution-part-i-megawatt.html?m=1) argues that military megawatt class lasers could reach $100 million (inclusive of all fixed costs) by 2045, because laser cost is currently halving every 4 years. In good conditions this would be able to defeat almost arbitrary numbers of any kind of missile whether they can maneuver or not. But there are huge challenges:\n\n*   Adaptive optics and ultra-precise tracking tech to focus the beam to 20 cm diameter at 200 km, while slewing to track a missile at several degrees per second\n*   Countermeasures like the spinning the RVs to distribute heat\n*   Weather, especially thick clouds which seem completely impractical to burn through (against moving targets, this would basically mean vaporizing an entire plane of water several km^2 by 1 m^2)",
      "plaintextDescription": "A blog post argues that military megawatt class lasers could reach $100 million (inclusive of all fixed costs) by 2045, because laser cost is currently halving every 4 years. In good conditions this would be able to defeat almost arbitrary numbers of any kind of missile whether they can maneuver or not. But there are huge challenges:\n\n * Adaptive optics and ultra-precise tracking tech to focus the beam to 20 cm diameter at 200 km, while slewing to track a missile at several degrees per second\n * Countermeasures like the spinning the RVs to distribute heat\n * Weather, especially thick clouds which seem completely impractical to burn through (against moving targets, this would basically mean vaporizing an entire plane of water several km^2 by 1 m^2)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-13T03:29:52.996Z",
    "af": false
  },
  {
    "_id": "yCPmt6JN3ck8giXQA",
    "postId": "HE3Styo9vpk7m8zi4",
    "parentCommentId": "bCLJ3bELiSJKapCkC",
    "topLevelCommentId": "n4aytCR9NBgNHhFpf",
    "contents": {
      "markdown": "Not going to argue with the claim directly. However, it doesn't bother me if about 1/6 of projects have a final output that's not primarily about AI safety, and another 1/6 are dual-use for capabilities and safety. This actually seems like a good ratio for safety in the current research environment, given that\n\n*   research is hits-based\n*   research often ends up being about something different from the initial plan\n    *   Inverse scaling is a years-long initiative that started at FAR AI and is strongly related to safety. Would you expect Anthropic fellows that spend months working on it and find interesting phenomena not directly related to safety to just not publish, having nothing to show for their fellowship? This would also mean the safety observations don't get published.\n*   we have ~30 times more investment in AI capabilities than safety\n*   (edit: some) safety research heavily overlaps with capabilities\n\nIf we were in the middle of a 10-year AI pause and capabilities research were banned, I would feel differently. But capabilities externalities are not one of the biggest issues for safety research right now. Safetywashing, corporate buy-in to policies, and tractability of alignment research are all bigger.",
      "plaintextDescription": "Not going to argue with the claim directly. However, it doesn't bother me if about 1/6 of projects have a final output that's not primarily about AI safety, and another 1/6 are dual-use for capabilities and safety. This actually seems like a good ratio for safety in the current research environment, given that\n\n * research is hits-based\n * research often ends up being about something different from the initial plan\n   * Inverse scaling is a years-long initiative that started at FAR AI and is strongly related to safety. Would you expect Anthropic fellows that spend months working on it and find interesting phenomena not directly related to safety to just not publish, having nothing to show for their fellowship? This would also mean the safety observations don't get published.\n * we have ~30 times more investment in AI capabilities than safety\n * (edit: some) safety research heavily overlaps with capabilities\n\nIf we were in the middle of a 10-year AI pause and capabilities research were banned, I would feel differently. But capabilities externalities are not one of the biggest issues for safety research right now. Safetywashing, corporate buy-in to policies, and tractability of alignment research are all bigger."
    },
    "baseScore": 11,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-12T22:22:35.144Z",
    "af": false
  },
  {
    "_id": "x29uJx3Tc5xnmrn7L",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "7bwRtQyjYd2JjboRF",
    "topLevelCommentId": "x7LmBj6REFDugEsCv",
    "contents": {
      "markdown": "Yeah seems reasonable. I don't think the system will ever get to 99%+ accuracy until the defense has like ASI and nanobots; my claim is mostly that the economics could shift from MAD (where the attacker is heavily cost advantaged) to something like conventional war, where each side has the ability to inflict unacceptable losses on the other but must pay a similar cost to attack and also can't \"win\" with a first strike.\n\nIt's not obvious that decoys change the conclusion if the cost ratio is otherwise favorable enough for the defender. Suppose the interceptors cost only $30k each due to economies of scale. Decoys can be distinguished when reentry starts at about 2 minutes before impact, and if the interceptor's speed is 300 km/h (already possible with drones) it can cover 10 km in that time, about equal to the typical spread of decoys and warheads. Supposing the defender spends 5 drones per warhead x 10 warheads / missile, this would cost them $1.5 million, while the attacker has spent something like $30 million, the cost of a Trident II. For countervalue this could be defeated by the layered attack RussellThor mentioned unless there is a higher altitude cheap interceptor, but for counterforce the attacker needs several warheads per silo destroyed so the goal is achieved.\n\nI found both that article and [this one](https://www.globalsecurity.org/wmd/world/russia/mozyr-kaz.htm) on their more recent history a great read.",
      "plaintextDescription": "Yeah seems reasonable. I don't think the system will ever get to 99%+ accuracy until the defense has like ASI and nanobots; my claim is mostly that the economics could shift from MAD (where the attacker is heavily cost advantaged) to something like conventional war, where each side has the ability to inflict unacceptable losses on the other but must pay a similar cost to attack and also can't \"win\" with a first strike.\n\nIt's not obvious that decoys change the conclusion if the cost ratio is otherwise favorable enough for the defender. Suppose the interceptors cost only $30k each due to economies of scale. Decoys can be distinguished when reentry starts at about 2 minutes before impact, and if the interceptor's speed is 300 km/h (already possible with drones) it can cover 10 km in that time, about equal to the typical spread of decoys and warheads. Supposing the defender spends 5 drones per warhead x 10 warheads / missile, this would cost them $1.5 million, while the attacker has spent something like $30 million, the cost of a Trident II. For countervalue this could be defeated by the layered attack RussellThor mentioned unless there is a higher altitude cheap interceptor, but for counterforce the attacker needs several warheads per silo destroyed so the goal is achieved.\n\nI found both that article and this one on their more recent history a great read."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-11T21:44:29.215Z",
    "af": false
  },
  {
    "_id": "BGDbgYjEHQ9JvECfj",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "DybcGCygTLzK5i2TY",
    "topLevelCommentId": "x7LmBj6REFDugEsCv",
    "contents": {
      "markdown": "> Guided artillery, like Excalibur with muzzle velocities that can exceed 1000m/s and unit costs of <$100k can be at edge of space in ~30s, perhaps faster than a missile, with ramjet variants (Nammo etc) even faster (up to perhaps 1500m/s)\n\nThis is wild, I did not know that Excalibur had CEP under 1 meter or that there were artillery shells with solid-fueled ramjet engines.",
      "plaintextDescription": "> Guided artillery, like Excalibur with muzzle velocities that can exceed 1000m/s and unit costs of <$100k can be at edge of space in ~30s, perhaps faster than a missile, with ramjet variants (Nammo etc) even faster (up to perhaps 1500m/s)\n\nThis is wild, I did not know that Excalibur had CEP under 1 meter or that there were artillery shells with solid-fueled ramjet engines."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-09T17:23:15.739Z",
    "af": false
  },
  {
    "_id": "cTCFwJ7Ady42Egpv5",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "4KPLYAdHEHyebBa52",
    "topLevelCommentId": "x7LmBj6REFDugEsCv",
    "contents": {
      "markdown": "Against atmospheric jitter, we have historically used ground radar, but it's not clear to me this is even necessary depending on how much drone cameras improve. If the drone knows the exact position within 3 meters 0.5 second ahead of time (when the warhead is something like 2 km away), it won't have enough performance to steer into the warhead, but it can throw a 1 kg explosively formed penetrator laterally at 2 km/s, which it would need to time with 0.1 ms accuracy. This would put 100 grams in each possible 1 m^2 cross section, though I'm not sure if it would work when spread out. To defeat this the warhead would either have to steer in the upper atmosphere out of range of the EFP of any available drone, or jink faster than the EFP can aim.\n\nI thought that MIRVs were spin stabilized, but it looks like that's not true, so in theory you could mount grid fins on them. However, any retrofit would need to handle the reentry heating which is significantly more intense than on manned spacecraft; RVs have thick ablative heat shields.\n\nThe chain of nukes plan seems possible with or without grid fins, so whether MIRVs still have cost advantage depends on the max altitude of cheap-ish terminal interceptors, which I really have no idea about.",
      "plaintextDescription": "Against atmospheric jitter, we have historically used ground radar, but it's not clear to me this is even necessary depending on how much drone cameras improve. If the drone knows the exact position within 3 meters 0.5 second ahead of time (when the warhead is something like 2 km away), it won't have enough performance to steer into the warhead, but it can throw a 1 kg explosively formed penetrator laterally at 2 km/s, which it would need to time with 0.1 ms accuracy. This would put 100 grams in each possible 1 m^2 cross section, though I'm not sure if it would work when spread out. To defeat this the warhead would either have to steer in the upper atmosphere out of range of the EFP of any available drone, or jink faster than the EFP can aim.\n\nI thought that MIRVs were spin stabilized, but it looks like that's not true, so in theory you could mount grid fins on them. However, any retrofit would need to handle the reentry heating which is significantly more intense than on manned spacecraft; RVs have thick ablative heat shields.\n\nThe chain of nukes plan seems possible with or without grid fins, so whether MIRVs still have cost advantage depends on the max altitude of cheap-ish terminal interceptors, which I really have no idea about."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-09T17:17:15.717Z",
    "af": false
  },
  {
    "_id": "x7LmBj6REFDugEsCv",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Will nuclear ICBMs in their current form be obsolete soon? Here's the argument:\n\n*   ICBMs' military utility is to make the cost of intercepting them totally impractical for three reasons:\n    *   Intercepting a reentry vehicle requires another missile with higher performance-- the rule of thumb is an interceptor needs 3x larger acceleration than the incoming missile. This means interceptors cost something like $5-70 million each depending on range\n    *   An ICBM has enough range to target basically anywhere, either counterforce (enemy missile silos) or countervalue (cities) targets, so to have the entire country be protected from nuclear attack the US would need to have millions of interceptors compared to the current ~50.\n    *   One missile can split into up to 5-15 MIRV (multiple independent reentry vehicles) which carry their own warheads, thereby making the cost to the defender 5-15x larger. There can also be up to ~100 decoys, but these mostly fall away during reentry.\n*   RVs are extremely fast (~7 km/s of which ~3 km/s is downward velocity) but their path is totally predictable once they enter the boost phase, so the problem of intercepting them basically reduces to predicting exactly where they'll be, then putting *something* ~100 g or larger in their path. The RV's own kinetic energy will completely destroy it.\n*   My understanding is that sensing had historically worked like this. The launch is first detected by primitive satellites, but there are too many decoys to tell which are the warheads and the resolution is poor. Interceptors thus require extremely high performance, meaning expensive systems like thrusters for moving sideways at tens of g at the last minute, radar, and cooling systems for IR cameras; including all the fuel the interceptor masses thousands of times what it needs to.\n*   Sensing is quickly being solved. One new generation of satellites (HBTSS) can track warheads after their booster burns out (at which point they become much dimmer), and advertises [\"weapons quality track for fire control\"](https://www.northropgrumman.com/what-we-do/missile-defense/hypersonic-and-ballistic-tracking-space-sensor-satellites) even for hypersonic glide vehicles, which are much harder to track than ICBMs. The first two were just launched last year, but it would not surprise me if hundreds will be operational soon: US has well over 1000 Starshield satellites in orbit, many equipped with radar and infrared cameras. Through SpaceX, they have the ability to launch tens of satellites per month if they needed to. China doesn't have the boost capacity of SpaceX but they would still be able to launch 1000 sensing satellites.\n*   The ultimate cost floor for intercepting ICBMs in the limit of perfect sensing is interceptor drones. Ukraine and Russia are in the middle of a war where most casualties are caused by drones, providing extremely high incentives to develop [interceptor drones](https://thedefender.media/uk/2025/07/tekhnari-developed-mungoose-interceptor/), which already have a top speed of 300 km/h, max altitude of 5 km, and basic infrared cameras at a unit cost of $1k-10k. The idea is the space-based sensors know the missile's velocity well enough to locate the target within 100 meters with 15 minutes of warning, the drones launch to a position 3 km in front of the target and are guided into exact position by radar and their own sensors as the RVs reenter, with sufficiently low error that they never need to go above their top speed or accelerate more than 1 gee.\n*   The other thing keeping ballistic missile defense from being practical during the Cold War was treaties like the ABM treaty, but today there are basically no active nuclear treaties between Russia, the US, and China.\n*   The internet seems to think it's still really hard to do missile defense against ICBMs. It seems like this was true in the Cold War but we can no longer be confident it's true today after 40 years of peace, with Boeing and American drone manufacturers vastly less competent than SpaceX and Ukraine's/Russia's. My guess at why the meme has persisted is that it is highly threatening and destabilizing to advertise one's ICBM defense capabilities. But many officials are talking openly about the predictability of ballistic missiles, and there is an arms race for other hypersonics for a reason.\n\nMy current guess is that the [SM-6](https://en.wikipedia.org/wiki/RIM-174_Standard_ERAM) and [PAC-3](https://en.wikipedia.org/wiki/MIM-104_Patriot#PAC-3_MSE) missiles either are already reliable or would be if the US had more practice, and if sufficient incentive exists, the cost of terminal ballistic missile defense will gradually decline as sensing improves, from the SM-6's $5M unit cost to lower and lower cost missiles like $400k AIM-9X until one could defend every city and silo in the US or China with a fleet of 100,000 interceptor drones. This would last until there are enough hypersonic glide vehicles and cruise missiles to reestablish deterrence (because they can outmaneuver cheap interceptors). But MIRVs would never return to their previous state of heavily cost advantaging the attacker, because Russia's [state of the art ICBMs](https://en.wikipedia.org/wiki/RS-28_Sarmat) only claim to be able to carry 3 HGVs, vs 15 purely ballistic MIRVs. Also HGVs and HCMs are vulnerable in other ways.\n\nEdit: Another option pointed out by commenters is that ICBMs are retrofitted to carry reentry vehicles with limited maneuvering ability, which could happen faster.\n\nCurious to hear if I'm missing something major, as I'm by no means an expert here.",
      "plaintextDescription": "Will nuclear ICBMs in their current form be obsolete soon? Here's the argument:\n\n * ICBMs' military utility is to make the cost of intercepting them totally impractical for three reasons:\n   * Intercepting a reentry vehicle requires another missile with higher performance-- the rule of thumb is an interceptor needs 3x larger acceleration than the incoming missile. This means interceptors cost something like $5-70 million each depending on range\n   * An ICBM has enough range to target basically anywhere, either counterforce (enemy missile silos) or countervalue (cities) targets, so to have the entire country be protected from nuclear attack the US would need to have millions of interceptors compared to the current ~50.\n   * One missile can split into up to 5-15 MIRV (multiple independent reentry vehicles) which carry their own warheads, thereby making the cost to the defender 5-15x larger. There can also be up to ~100 decoys, but these mostly fall away during reentry.\n * RVs are extremely fast (~7 km/s of which ~3 km/s is downward velocity) but their path is totally predictable once they enter the boost phase, so the problem of intercepting them basically reduces to predicting exactly where they'll be, then putting something ~100 g or larger in their path. The RV's own kinetic energy will completely destroy it.\n * My understanding is that sensing had historically worked like this. The launch is first detected by primitive satellites, but there are too many decoys to tell which are the warheads and the resolution is poor. Interceptors thus require extremely high performance, meaning expensive systems like thrusters for moving sideways at tens of g at the last minute, radar, and cooling systems for IR cameras; including all the fuel the interceptor masses thousands of times what it needs to.\n * Sensing is quickly being solved. One new generation of satellites (HBTSS) can track warheads after their booster burns out (at which point they become much dimmer), and advertis"
    },
    "baseScore": 58,
    "voteCount": 32,
    "createdAt": null,
    "postedAt": "2025-08-09T03:25:30.902Z",
    "af": false
  },
  {
    "_id": "fKKyRZyD6awDEkoM2",
    "postId": "iEkksmar6mtuQtdsR",
    "parentCommentId": "tFscRdy6Ni2eGa2S3",
    "topLevelCommentId": "4aTHmdJkbmm6AFnpk",
    "contents": {
      "markdown": "Time horizon is currently defined as the human task length it takes to get a 50% success rate, fit with a logistic curve that goes from 0 to 100%. But we just did that because it was a good fit to the data. If METR finds evidence of label noise or tasks that are inherently hard to complete with more than human reliability, we can just fit a different logistic curve or switch methodologies (likely at the same time we upgrade our benchmark) so that time horizon more accurately reflects how much intellectual labor the AI can replace before needing human intervention.",
      "plaintextDescription": "Time horizon is currently defined as the human task length it takes to get a 50% success rate, fit with a logistic curve that goes from 0 to 100%. But we just did that because it was a good fit to the data. If METR finds evidence of label noise or tasks that are inherently hard to complete with more than human reliability, we can just fit a different logistic curve or switch methodologies (likely at the same time we upgrade our benchmark) so that time horizon more accurately reflects how much intellectual labor the AI can replace before needing human intervention."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-08T17:14:11.599Z",
    "af": false
  },
  {
    "_id": "n7tnPk9pqDciieczs",
    "postId": "tDkYdyJSqe3DddtK4",
    "parentCommentId": "mCGcdNWPLodFd9BoD",
    "topLevelCommentId": "mCGcdNWPLodFd9BoD",
    "contents": {
      "markdown": "The YouTube channel was banned last week for being suspected propaganda because he used to work for state media channel RT. This is pretty sad to me because the content was very informative with slight if any pro Russia bias. AFAIK the only place he posts now is telegram https://t.me/RealReporter_tg",
      "plaintextDescription": "The YouTube channel was banned last week for being suspected propaganda because he used to work for state media channel RT. This is pretty sad to me because the content was very informative with slight if any pro Russia bias. AFAIK the only place he posts now is telegram https://t.me/RealReporter_tg"
    },
    "baseScore": 10,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-07T07:22:56.781Z",
    "af": false
  },
  {
    "_id": "ybK45jbKk7HYqa4Lm",
    "postId": "rKmojEZ9qKwApjCfX",
    "parentCommentId": "BDpatgNcqqEvBz5Do",
    "topLevelCommentId": "BDpatgNcqqEvBz5Do",
    "contents": {
      "markdown": "What does it mean to prevent scheming on a tiny model? If we care about generalization performance rather than proving generalization bounds, to what distribution do we want the tiny model to generalize?",
      "plaintextDescription": "What does it mean to prevent scheming on a tiny model? If we care about generalization performance rather than proving generalization bounds, to what distribution do we want the tiny model to generalize?"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-06T06:58:54.900Z",
    "af": false
  },
  {
    "_id": "gQuJZnvKLXAR4uFym",
    "postId": "qgSEbLfZpH2Yvrdzm",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I don't think the evidence provided substantiates the conclusions in this post. Overall it looks like you've demonstrated this:\n\n*   They didn't commit obvious fraud (last digit test)\n*   If you remove all 14 of their control variables, this removes much of their statistical power, so the results are only marginally significant (p = 0.026981 overall, compare p < 0.01 in many independent categories in the original)\n*   If you remove all 14 of their control variables AND change from Poisson to negative binomial, the results are no longer significant. (However, if you include the control variables and use negative binomial the results are still significant: Web Table 12 in appendix)\n\nFurthermore you only examine all cause mortality, whereas the study examines deaths from specific diseases. Your replication will be less powered because even in non-high-income countries [most people die from things like heart disease and stroke](https://www.who.int/news-room/fact-sheets/detail/the-top-10-causes-of-death), reducing your power to detect even large changes in malaria or HIV deaths caused by USAID. Removing all 14 controls will kill your statistical power further, because the presence of things like \"adequate sanitation\" and \"health expenditure\" will definitely affect the mortality rate.\n\nFinally, in your conclusion, it makes no sense to say \"some of the choices they make to strengthen the result end up being counterproductive\"--this is weak evidence *against* p-hacking that to my mind cancels the weak evidence you found *for* p-hacking! This is super unvirtuous, don't just assume they were p-hacking and say your belief is confirmed when you found some weak evidence for and some weak evidence against.\n\nOff the top of my head, here are the results I'd want to see from someone skeptical of this study, in increasing order of effort. I agree these would be much easier if they shared their full data.\n\n*   Think about whether the causal story makes sense (is variance in USAID funding exogenous and random-ish?)\n*   Add year-based fixed effects\n*   Some kind of exploratory analysis to decide whether the negative binomial model is appropriate. I have never fit a negative binomial model but [here's what o3 suggests](https://chatgpt.com/share/68808bae-f73c-800a-8bbd-fe9f6ed02650)\n*   Use a Poisson model with robust standard errors\n*   Multiverse analysis where you pick 20 plausible control variables, control for 10 of them, and see how many are significant\n\n* * *\n\nNow some comments in no particular order:\n\nFirst thanks for including the equations, it makes the post a lot easier to understand.\n\n> when the worldâs foremost medical journal publishes work indicating that a policyâs results will be comparable in death count to historyâs [engineered mass-starvations](https://en.wikipedia.org/wiki/Holodomor) by yearâs end, Iâm going to be skeptical.\n\nI'm as skeptical of this as any other study, but we should beware the absurdity heuristic and instead think about the actual claimed mechanism. There are LOTS more people in LMICs in the world today than in 1932 Ukraine and we *know* that millions die from preventable diseases per year; we also know that one of the main goals of USAID is to reduce deaths from preventable diseases, and that spending enough resources on preventable diseases can basically eliminate them.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b0fb5902d366c0f061e9c363e2151215d1b4d975c8bbbafa.png)\n\n> Iâm going to be triply skeptical when similar Monte Carlo modeling has led to [inflated and incorrect death count prognostications in the past](https://statmodeling.stat.columbia.edu/2020/05/08/so-the-real-scandal-is-why-did-anyone-ever-listen-to-this-guy/).\n\nThere are at least two reasons I don't think these cases are similar.Â \n\n*   The link you shared is about inherently harder modeling than this Lancet study. Ferguson's model models the exponential spread of transmissible diseases, which inherently has orders of magnitude wide confidence intervals in the final number of deaths, especially for median vs worst case scenarios. The only thing these studies share is they're using the extremely common technique of Monte Carlo modeling, and they're about disease.\n*   The prediction that 500k people would die without a lockdown in the UK, and 2M in the US, isn't too far off, given that even with partial lockdowns + vaccination 232k died in the UK and 1.19M in the US, plus there was limited information at the time\n\n> Itâs way too much work for me to go find and recreate all of the 18(!) control variables they use.\n\nThey have 14 control variables not 18: 4 time based controls and 10 other controls.\n\n> The per year binning also torpedoes their results.Â \n\nThis is interesting, I'd like to see it with the full methodology\n\n> It looks a slightly better fit from a BIC perspective. The second quartile drops from significance and the third quartile is barely significant. Overall, this is still a much weaker result than what they report, but the binning itself doesnât change a lot about the regression.\n\nNote their `**` means p < 0.05 and their `***` means p < 0.01, which is different from your annotations\n\n> The weird thing about this paper is its relation to Regression 1. In my reproduction that doesnât have a gazillion controls, there is a significant relationship between per capita USAID spend and overall mortality. For whatever reason this wasnât enough and some of the choices they make to strengthen the result end up being counterproductive. In both the categorical binning issue and the choice to use Poisson, the alternative choices make the results null, which I consider to be fairly strong evidence of p-hacking.\n\nMy understanding is that when you replaced their experiment with a simpler, much less powerful experiment, then did a cursory sensitivity analysis, the results were sometimes significant and sometimes not significant. This is totally expected. Also it makes no sense to say \"some of the choices they make to strengthen the result end up being counterproductive\"--this is weak evidence *against* p-hacking that to my mind cancels the weak evidence you found *for* p-hacking! This is super unvirtuous, don't just assume they were p-hacking and say your belief is confirmed when you found some weak evidence for and some weak evidence against.\n\nAlso, the results are significant in so many categories that I disagree with p-hacking being likely here. If you aggregated the results in some principled way, you would get some crazy low value like 10^-50, so it can't be a multiple comparisons issue. As for the controls, they seem plausible on priors (I'm not familiar enough to know if they're standard), and seem to improve the predictive power of the model.",
      "plaintextDescription": "I don't think the evidence provided substantiates the conclusions in this post. Overall it looks like you've demonstrated this:\n\n * They didn't commit obvious fraud (last digit test)\n * If you remove all 14 of their control variables, this removes much of their statistical power, so the results are only marginally significant (p = 0.026981 overall, compare p < 0.01 in many independent categories in the original)\n * If you remove all 14 of their control variables AND change from Poisson to negative binomial, the results are no longer significant. (However, if you include the control variables and use negative binomial the results are still significant: Web Table 12 in appendix)\n\nFurthermore you only examine all cause mortality, whereas the study examines deaths from specific diseases. Your replication will be less powered because even in non-high-income countries most people die from things like heart disease and stroke, reducing your power to detect even large changes in malaria or HIV deaths caused by USAID. Removing all 14 controls will kill your statistical power further, because the presence of things like \"adequate sanitation\" and \"health expenditure\" will definitely affect the mortality rate.\n\nFinally, in your conclusion, it makes no sense to say \"some of the choices they make to strengthen the result end up being counterproductive\"--this is weak evidence against p-hacking that to my mind cancels the weak evidence you found for p-hacking! This is super unvirtuous, don't just assume they were p-hacking and say your belief is confirmed when you found some weak evidence for and some weak evidence against.\n\nOff the top of my head, here are the results I'd want to see from someone skeptical of this study, in increasing order of effort. I agree these would be much easier if they shared their full data.\n\n * Think about whether the causal story makes sense (is variance in USAID funding exogenous and random-ish?)\n * Add year-based fixed effects\n * Some kind of explorat"
    },
    "baseScore": 23,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-07-23T08:07:20.732Z",
    "af": false
  },
  {
    "_id": "pFk6Cw99kTnFRoPju",
    "postId": "anX4QrNjhJqGFvrBr",
    "parentCommentId": "gEgGNbDa2ZcEAG3DT",
    "topLevelCommentId": "mfHFAo9pp3rqcxPoG",
    "contents": {
      "markdown": "I don't think it's especially realistic but it seems like a good setting to test unlearning against for the sake of diversity / broader understanding of what we can do with AI safety techniques",
      "plaintextDescription": "I don't think it's especially realistic but it seems like a good setting to test unlearning against for the sake of diversity / broader understanding of what we can do with AI safety techniques"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-21T21:19:01.663Z",
    "af": false
  },
  {
    "_id": "Pe2NbGEqczBeFzGDr",
    "postId": "6KcP7tEe5hgvHbrSF",
    "parentCommentId": "dJK9ZZX765aP6ipwJ",
    "topLevelCommentId": "5xhwL7xKTkC7TKzzu",
    "contents": {
      "markdown": "The issue with Cybench is its difficulty annotations are \"first solve time\" which we don't know how to compare with median / average solve time among experts. If we get better data, it could be comparable.",
      "plaintextDescription": "The issue with Cybench is its difficulty annotations are \"first solve time\" which we don't know how to compare with median / average solve time among experts. If we get better data, it could be comparable."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-21T18:54:18.874Z",
    "af": false
  },
  {
    "_id": "5xhwL7xKTkC7TKzzu",
    "postId": "6KcP7tEe5hgvHbrSF",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Since publishing this we have gotten two comments:\n\n*   Someone pointed out that Mock AIME actually has human time annotations. We already had item-level agent success data from Epoch AI, so it joins GPQA Diamond in having high-quality data in both categories.\n*   In May, Palisade [found](https://arxiv.org/pdf/2505.19915) that the frontier time horizon on cyber offense CTFs is around 1 hour\n\nWe're looking into incorporating both of these if we find the data is compatible with ours!",
      "plaintextDescription": "Since publishing this we have gotten two comments:\n\n * Someone pointed out that Mock AIME actually has human time annotations. We already had item-level agent success data from Epoch AI, so it joins GPQA Diamond in having high-quality data in both categories.\n * In May, Palisade found that the frontier time horizon on cyber offense CTFs is around 1 hour\n\nWe're looking into incorporating both of these if we find the data is compatible with ours!"
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-21T18:37:26.689Z",
    "af": false
  },
  {
    "_id": "2pbQbrniMCf2qHrzn",
    "postId": "cxuzALcmucCndYv4a",
    "parentCommentId": "j6fTrCpzRajFHc27h",
    "topLevelCommentId": "j6fTrCpzRajFHc27h",
    "contents": {
      "markdown": "One possible way things could go is that models behave like human drug addicts, and don't crave reward until they have an ability to manipulate it easily/directly, but as soon as they do, lose all their other motivations and values and essentially become misaligned. In this world we might get\n\n*   Companies put lots of effort into keeping models from nascent reward hacking, because reward hacking behavior will quickly lead to more efficient reward hacking and lead to a reward-addicted model\n*   If the model becomes reward-addicted and the company can't control it, they might have to revert to an earlier checkpoint, costing 100s of millions\n*   If companies can control reward-addicted models, we might get some labs that try to keep models aligned for safety + to avoid the need for secure sandboxes, and some that develop misaligned models and invest heavily in secure sandboxes\n*   It's more likely we can tell the difference between aligned and reward-addicted models, because we can see behavioral differences out of distribution",
      "plaintextDescription": "One possible way things could go is that models behave like human drug addicts, and don't crave reward until they have an ability to manipulate it easily/directly, but as soon as they do, lose all their other motivations and values and essentially become misaligned. In this world we might get\n\n * Companies put lots of effort into keeping models from nascent reward hacking, because reward hacking behavior will quickly lead to more efficient reward hacking and lead to a reward-addicted model\n * If the model becomes reward-addicted and the company can't control it, they might have to revert to an earlier checkpoint, costing 100s of millions\n * If companies can control reward-addicted models, we might get some labs that try to keep models aligned for safety + to avoid the need for secure sandboxes, and some that develop misaligned models and invest heavily in secure sandboxes\n * It's more likely we can tell the difference between aligned and reward-addicted models, because we can see behavioral differences out of distribution"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-19T18:05:58.055Z",
    "af": true
  },
  {
    "_id": "mfHFAo9pp3rqcxPoG",
    "postId": "anX4QrNjhJqGFvrBr",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "One setting that might be useful to study is the one in [Grebe et al.](https://www.arxiv.org/abs/2504.21072), which I just saw at the ICML MUGen workshop. The idea is to insert a backdoor that points to the forget set; they study diffusion models but it should be applicable to LLMs too. It would be neat if UNDO or some variant can be shown to be robust to this-- I think it would depend on how much noising is needed to remove backdoors, which I'm not familiar with.",
      "plaintextDescription": "One setting that might be useful to study is the one in Grebe et al., which I just saw at the ICML MUGen workshop. The idea is to insert a backdoor that points to the forget set; they study diffusion models but it should be applicable to LLMs too. It would be neat if UNDO or some variant can be shown to be robust to this-- I think it would depend on how much noising is needed to remove backdoors, which I'm not familiar with."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-18T18:11:41.452Z",
    "af": true
  },
  {
    "_id": "cjWciZafJXBLTNrp7",
    "postId": "7xneDbsgj6yJDJMjK",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Monitoring seems crucial for the next safety case. My guess is that in couple of years we'll lose capability-based safety cases, and need to rely on a good understanding of the question \"how difficult are the tasks frontier agents can do covertly, without the human or AI monitor detecting its reasoning?\".\n\nHaving said this, it may be that in a year or two, we'll be able to translate neuralese or have a good idea of how much one can train against a monitor before CoT stops being transparent, so the current paradigm of faithful CoT is certainly not the only hope for monitoring.",
      "plaintextDescription": "Monitoring seems crucial for the next safety case. My guess is that in couple of years we'll lose capability-based safety cases, and need to rely on a good understanding of the question \"how difficult are the tasks frontier agents can do covertly, without the human or AI monitor detecting its reasoning?\".\n\nHaving said this, it may be that in a year or two, we'll be able to translate neuralese or have a good idea of how much one can train against a monitor before CoT stops being transparent, so the current paradigm of faithful CoT is certainly not the only hope for monitoring."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-17T05:35:33.304Z",
    "af": true
  },
  {
    "_id": "NYgAtngqKpNARkxKK",
    "postId": "6KcP7tEe5hgvHbrSF",
    "parentCommentId": "yiwN4h9Tpkjg5siBv",
    "topLevelCommentId": "yiwN4h9Tpkjg5siBv",
    "contents": {
      "markdown": "Yes, it's in the [repo](https://github.com/METR/cross-domain-horizon), in the data/scores and data/benchmarks folders.",
      "plaintextDescription": "Yes, it's in the repo, in the data/scores and data/benchmarks folders."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-16T22:53:05.644Z",
    "af": false
  },
  {
    "_id": "3qajgZAkhTrj9JbmB",
    "postId": "5NBf6xMNGzMb4osqC",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> It's interesting that task length is better correlated with release date than with training compute; I was not expecting that.\n\nIt's partially because we filtered for frontier models. If you had people train models of all different sizes that were compute-optimal architectures at their respective sizes, the correlation between time horizon and compute would plausibly be much better.",
      "plaintextDescription": "> It's interesting that task length is better correlated with release date than with training compute; I was not expecting that.\n\nIt's partially because we filtered for frontier models. If you had people train models of all different sizes that were compute-optimal architectures at their respective sizes, the correlation between time horizon and compute would plausibly be much better."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-14T07:04:53.489Z",
    "af": false
  },
  {
    "_id": "sJyGbpCYTc9dxZjA2",
    "postId": "fuSaKr6t6Zuh6GKaQ",
    "parentCommentId": "8jiceR8CjkSWjybcM",
    "topLevelCommentId": "8jiceR8CjkSWjybcM",
    "contents": {
      "markdown": "Cassidy Laidlaw published a great paper at [ICLR 2025](https://arxiv.org/abs/2403.03185) that proved (their Theorem 5.1) that (proxy reward - true reward) is bounded given a minimum proxy-true correlation and a maximum chi-squared divergence on the reference policy. Basically, chi-squared divergence works where KL divergence doesn't.\n\nUsing this in practice for alignment is still pretty restrictive-- the fact that the new policy canât be exponentially more likely to achieve **any** state than the reference policy means this will probably only be useful in cases where the reference policy is already intelligent/capable.",
      "plaintextDescription": "Cassidy Laidlaw published a great paper at ICLR 2025 that proved (their Theorem 5.1) that (proxy reward - true reward) is bounded given a minimum proxy-true correlation and a maximum chi-squared divergence on the reference policy. Basically, chi-squared divergence works where KL divergence doesn't.\n\nUsing this in practice for alignment is still pretty restrictive-- the fact that the new policy canât be exponentially more likely to achieve any state than the reference policy means this will probably only be useful in cases where the reference policy is already intelligent/capable."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-24T19:21:19.592Z",
    "af": true
  },
  {
    "_id": "haEvJKkdYEEkKQGhd",
    "postId": "PAYfmG2aRbdb74mEp",
    "parentCommentId": "B3LQLzxyNJTFHHKZc",
    "topLevelCommentId": "pFp3WoJ7RoQPwELDr",
    "contents": {
      "markdown": "Yes, in particular the concern about benchmark tasks being well-specified remains. We'll need both more data (probably collected from AI R&D tasks in the wild) and more modeling to get a forecast for overall speedup.\n\nHowever, I do think if we have a wide enough distribution of tasks, AIs outperform humans on all of them at task lengths that should imply humans spend 1/10th the labor, but AI R&D has not been automated yet, something strange needs to be happening. So looking at different benchmarks is partial progress towards understanding the gap between long time horizons on METR's task set and actual AI R&D uplift.",
      "plaintextDescription": "Yes, in particular the concern about benchmark tasks being well-specified remains. We'll need both more data (probably collected from AI R&D tasks in the wild) and more modeling to get a forecast for overall speedup.\n\nHowever, I do think if we have a wide enough distribution of tasks, AIs outperform humans on all of them at task lengths that should imply humans spend 1/10th the labor, but AI R&D has not been automated yet, something strange needs to be happening. So looking at different benchmarks is partial progress towards understanding the gap between long time horizons on METR's task set and actual AI R&D uplift."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-24T17:28:56.548Z",
    "af": false
  },
  {
    "_id": "3eNbBx5KxazTsyfJs",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "KaaSfntGEBBgadvrF",
    "topLevelCommentId": "KaaSfntGEBBgadvrF",
    "contents": {
      "markdown": "New graph with better data, formatting still wonky though. Colleagues say it reminds them of a subway map.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/238473ae61201f138ed2d55e0f6c49cd0f32e5db00a29986.png)\n\nWith individual question data from Epoch, and making an adjustment for human success rate (adjusted task length = avg human time / human success rate), AIME looks closer to the others, and it's clear that GPQA Diamond has saturated.",
      "plaintextDescription": "New graph with better data, formatting still wonky though. Colleagues say it reminds them of a subway map.\n\nWith individual question data from Epoch, and making an adjustment for human success rate (adjusted task length = avg human time / human success rate), AIME looks closer to the others, and it's clear that GPQA Diamond has saturated."
    },
    "baseScore": 10,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-06-19T05:05:31.977Z",
    "af": true
  },
  {
    "_id": "xgBAK9cvXGgBcYRbd",
    "postId": "8XHBaugB5S3r27MG9",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "What's an example of a claim it might be difficult/impossible to find a stable argument for?",
      "plaintextDescription": "What's an example of a claim it might be difficult/impossible to find a stable argument for?"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-18T02:12:03.664Z",
    "af": true
  },
  {
    "_id": "E3GCjeJgFPeZe3wmk",
    "postId": "tdQuoXsbW6LnxYqHx",
    "parentCommentId": "Yu2q6rBjdgjTZrb8C",
    "topLevelCommentId": "Pt2XA62epwJ99kT3G",
    "contents": {
      "markdown": "American drones are *very* expensive. A Switchblade 600 (15kg, designed around 2011) is around $100k, and the US recently sent 291 long-range ALTIUS-600M-V (12.2kg) to Taiwan for $300M indicating a unit cost of $1M. So $1T would only buy 1 million of the newer design, at least for export. Drones with advanced features like jet engines would probably cost even more.\n\nUkraine produced 2.2 million drones in 2024, and its 2025 production goal is 4.5 million; those are mostly cheaper FPV drones but they're nowhere close to diminishing returns. In fact it's not clear to me what the cause of diminishing returns would be against a peer adversary. Running out of targets that are targetable with drones? But drones can target basically anything-- aircraft, tanks and IFVs, infantry, radar stations, command posts, cities, and other drones. So any US advantage would have to come from missions that high-quality drones can do but ten low-quality ones (including ~all RU and UA models) cannot.",
      "plaintextDescription": "American drones are very expensive. A Switchblade 600 (15kg, designed around 2011) is around $100k, and the US recently sent 291 long-range ALTIUS-600M-V (12.2kg) to Taiwan for $300M indicating a unit cost of $1M. So $1T would only buy 1 million of the newer design, at least for export. Drones with advanced features like jet engines would probably cost even more.\n\nUkraine produced 2.2 million drones in 2024, and its 2025 production goal is 4.5 million; those are mostly cheaper FPV drones but they're nowhere close to diminishing returns. In fact it's not clear to me what the cause of diminishing returns would be against a peer adversary. Running out of targets that are targetable with drones? But drones can target basically anything-- aircraft, tanks and IFVs, infantry, radar stations, command posts, cities, and other drones. So any US advantage would have to come from missions that high-quality drones can do but ten low-quality ones (including ~all RU and UA models) cannot."
    },
    "baseScore": 13,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-06-15T02:38:23.368Z",
    "af": false
  },
  {
    "_id": "3iXXKP2z2WSsmrBjH",
    "postId": "tdQuoXsbW6LnxYqHx",
    "parentCommentId": "KcpEHPJeecKK8nwHk",
    "topLevelCommentId": "Pt2XA62epwJ99kT3G",
    "contents": {
      "markdown": "In the drone race, I think quantity is very important for several reasons:\n\n*   While things like tanks and artillery can only be useful as a complement to manpower, making quality the only way to increase effectiveness, militaries can effectively use a huge number of drones per human soldier, if they are either AI piloted or expended. Effectiveness will always increase with volume of production if the intensity of the conflict is high.\n*   American IFVs and tanks cost something like $100-$200/kg, and artillery shells something like $20/kg, but American drones range from $6,000 to $13k per kg. This means that at American costs, the US can only afford ~1% of the fires (by mass) delivered by drone as by artillery if it's investing equally in artillery and drones. There is a huge amount of room to cut costs and the US would need to do so to maximize effectiveness.\n*   Many key performance metrics of drones, like range and payload fraction, are limited by physics, basic materials science, and commodity hardware. US, China, Ukraine, and Russia will be using close to the same batteries and propellers.\n    *   However, quality could affect things like speed, accuracy, AI, and anti EW performance, so it might be more important when AI is more widely used and countermeasures like lasers and autoturrets are standard\n*   Russia is already cutting costs (e.g. making propellers out of wood rather than carbon) showing that on the current margin, quantity > quality.",
      "plaintextDescription": "In the drone race, I think quantity is very important for several reasons:\n\n * While things like tanks and artillery can only be useful as a complement to manpower, making quality the only way to increase effectiveness, militaries can effectively use a huge number of drones per human soldier, if they are either AI piloted or expended. Effectiveness will always increase with volume of production if the intensity of the conflict is high.\n * American IFVs and tanks cost something like $100-$200/kg, and artillery shells something like $20/kg, but American drones range from $6,000 to $13k per kg. This means that at American costs, the US can only afford ~1% of the fires (by mass) delivered by drone as by artillery if it's investing equally in artillery and drones. There is a huge amount of room to cut costs and the US would need to do so to maximize effectiveness.\n * Many key performance metrics of drones, like range and payload fraction, are limited by physics, basic materials science, and commodity hardware. US, China, Ukraine, and Russia will be using close to the same batteries and propellers.\n   * However, quality could affect things like speed, accuracy, AI, and anti EW performance, so it might be more important when AI is more widely used and countermeasures like lasers and autoturrets are standard\n * Russia is already cutting costs (e.g. making propellers out of wood rather than carbon) showing that on the current margin, quantity > quality."
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-06-15T00:53:41.337Z",
    "af": false
  },
  {
    "_id": "qFSFDkucXofycAyLk",
    "postId": "tdQuoXsbW6LnxYqHx",
    "parentCommentId": "EXdxbm22zM6GvArrx",
    "topLevelCommentId": "Pt2XA62epwJ99kT3G",
    "contents": {
      "markdown": "He also says that Chinese drones are low quality and Ukraine is slightly ahead of Russia.",
      "plaintextDescription": "He also says that Chinese drones are low quality and Ukraine is slightly ahead of Russia."
    },
    "baseScore": 1,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-14T23:57:50.762Z",
    "af": false
  },
  {
    "_id": "2x4ELncLuXesa9HjN",
    "postId": "anX4QrNjhJqGFvrBr",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Great paper, this is hopeful for unlearning being used in practice.\n\nI wonder if UNDO would stack with circuit discovery or some other kind of interpretability. Intuitively, localizing the noise in the Noise phase to weights that disproportionally contribute to the harmful behavior should get a better retain-forget tradeoff. It doesn't need to be perfect, just better than random, so it should be doable with current methods.",
      "plaintextDescription": "Great paper, this is hopeful for unlearning being used in practice.\n\nI wonder if UNDO would stack with circuit discovery or some other kind of interpretability. Intuitively, localizing the noise in the Noise phase to weights that disproportionally contribute to the harmful behavior should get a better retain-forget tradeoff. It doesn't need to be perfect, just better than random, so it should be doable with current methods."
    },
    "baseScore": 20,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-06-14T20:27:45.603Z",
    "af": true
  },
  {
    "_id": "9eW6FcuG7w6zDKqEY",
    "postId": "LZFffhpoXDmeNtEip",
    "parentCommentId": "z3sayy9n8gMFcdyv6",
    "topLevelCommentId": "z3sayy9n8gMFcdyv6",
    "contents": {
      "markdown": "It's not generally possible to cleanly separate assets into things valued for stage 1 reasons vs other reasons. You may claim these are edge cases but the world is full of edge cases:\n\n*   Apples are primarily valued on taste; other foods even more so (nutrition is more efficiently achieved via bitter greens, rice, beans, and vitamins than a typical Western diet). You can tell because Honeycrisp apples are much higher priced than lower-quality apples despite being nutritionally equivalent. But taste is highly subjective so value is actually based on weighted population average taste\n*   Fiat currency is valuable because the government demands taxes in fiat rather than apples, because this is vastly more efficient.\n*   Luxury brands, and therefore stocks of luxury brands, are mostly valuable because their brand is a status symbol; companies transition between their value being from a status symbol, positive reputation, other IP, physical capital, or other things\n*   Real estate in a city is valued because it's near people and infrastructure, in turn because everyone has settled on it as a Schelling point, because it had a port 80 years ago or something\n*   Mortgage-backed securities had higher value in 2007 because a credit rating agency's mathematical formula said they would be worth a larger number of dollars. The credit rating agency had incentives to say this but they were not the holder, and the holder believed them\n*   Many assets have higher value than others purely due to higher liquidity",
      "plaintextDescription": "It's not generally possible to cleanly separate assets into things valued for stage 1 reasons vs other reasons. You may claim these are edge cases but the world is full of edge cases:\n\n * Apples are primarily valued on taste; other foods even more so (nutrition is more efficiently achieved via bitter greens, rice, beans, and vitamins than a typical Western diet). You can tell because Honeycrisp apples are much higher priced than lower-quality apples despite being nutritionally equivalent. But taste is highly subjective so value is actually based on weighted population average taste\n * Fiat currency is valuable because the government demands taxes in fiat rather than apples, because this is vastly more efficient.\n * Luxury brands, and therefore stocks of luxury brands, are mostly valuable because their brand is a status symbol; companies transition between their value being from a status symbol, positive reputation, other IP, physical capital, or other things\n * Real estate in a city is valued because it's near people and infrastructure, in turn because everyone has settled on it as a Schelling point, because it had a port 80 years ago or something\n * Mortgage-backed securities had higher value in 2007 because a credit rating agency's mathematical formula said they would be worth a larger number of dollars. The credit rating agency had incentives to say this but they were not the holder, and the holder believed them\n * Many assets have higher value than others purely due to higher liquidity"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-06-04T01:08:19.023Z",
    "af": false
  },
  {
    "_id": "cZWcjhHMvEwCwDWHv",
    "postId": "RnKmRusmFpw7MhPYw",
    "parentCommentId": "o4EypZMi2BGGQerx8",
    "topLevelCommentId": "o4EypZMi2BGGQerx8",
    "contents": {
      "markdown": "I don't run the evaluations but probably we will; no timeframe yet though as we would need to do elicitation first. Claude's SWE-bench Verified scores suggest that it will be above 2 hours on the METR task set; the benchmarks are pretty similar apart from their different time annotations.",
      "plaintextDescription": "I don't run the evaluations but probably we will; no timeframe yet though as we would need to do elicitation first. Claude's SWE-bench Verified scores suggest that it will be above 2 hours on the METR task set; the benchmarks are pretty similar apart from their different time annotations."
    },
    "baseScore": 17,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-05-24T16:12:46.457Z",
    "af": false
  },
  {
    "_id": "TSL7BXhK3xAp5EjYx",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "fCYrjycYiHzHtgvkz",
    "topLevelCommentId": "KaaSfntGEBBgadvrF",
    "contents": {
      "markdown": "There is a decreasing curve of Gemini success probability vs average human time on questions in the benchmark, and the curve intersects 50% at roughly 110 minutes.\n\nBasically it's trying to measure the same quantity as the original paper (https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) but the numbers are less accurate since we have less data for these benchmarks.",
      "plaintextDescription": "There is a decreasing curve of Gemini success probability vs average human time on questions in the benchmark, and the curve intersects 50% at roughly 110 minutes.\n\nBasically it's trying to measure the same quantity as the original paper (https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) but the numbers are less accurate since we have less data for these benchmarks."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-21T18:07:34.506Z",
    "af": false
  },
  {
    "_id": "gPAvqmZQG4XLoasQW",
    "postId": "DFyoYHhbE8icgbTpe",
    "parentCommentId": "ivFXihTxueHCguA9M",
    "topLevelCommentId": "mt23nNfaMqf38q9mt",
    "contents": {
      "markdown": "oops, this was on my work account from which you can't make public links. Replaced the link with the prompt and beginning of o3 output.",
      "plaintextDescription": "oops, this was on my work account from which you can't make public links. Replaced the link with the prompt and beginning of o3 output."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-21T01:46:36.471Z",
    "af": false
  },
  {
    "_id": "GAtqMxj6krYJkXwQD",
    "postId": "DFyoYHhbE8icgbTpe",
    "parentCommentId": "5Jk4vJvPLe2srFFNe",
    "topLevelCommentId": "mt23nNfaMqf38q9mt",
    "contents": {
      "markdown": "o3 has the same conclusion with a slightly different prompt.Â \n\n> Read this comment exchange and come to a definitive conclusion about whether Garrett Baker is accurately representing Matthew. Focus on content rather than tone:\n\n> **Conclusion: Garrett is** ***not*** **accurately representing Matthewâs position.**  \n> Below is a pointâbyâpoint comparison that shows where Garrettâs paraphrases diverge from what Matthew is actually claiming (ignoring tone and focusing only on the content).",
      "plaintextDescription": "o3 has the same conclusion with a slightly different prompt.Â \n\n> Read this comment exchange and come to a definitive conclusion about whether Garrett Baker is accurately representing Matthew. Focus on content rather than tone:\n\nÂ \n\n> Conclusion: Garrett is not accurately representing Matthewâs position.\n> Below is a pointâbyâpoint comparison that shows where Garrettâs paraphrases diverge from what Matthew is actually claiming (ignoring tone and focusing only on the content)."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-21T01:34:21.384Z",
    "af": false
  },
  {
    "_id": "CA3dRnk9zRNb4DgAb",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "FeERxh9B7xxcukC8t",
    "topLevelCommentId": "KaaSfntGEBBgadvrF",
    "contents": {
      "markdown": "There was a unit conversion mistake, it should have been 80 minutes. Now fixed.\n\nBesides that, I agree with everything here; these will all be fixed in the final blog post. I already looked at one of the 30m-1h questions and it appeared to be doable in ~3 minutes with the ability to ctrl-f transcripts but would take longer without transcripts, unknown how long.\n\nIn the next version I will probably use the no-captions AI numbers and measure myself without captions to get a rough video speed multiplier, then possibly do better stats that separate out domains with strong human-time-dependent difficulty from domains without (like this and SWE-Lancer).",
      "plaintextDescription": "There was a unit conversion mistake, it should have been 80 minutes. Now fixed.\n\nBesides that, I agree with everything here; these will all be fixed in the final blog post. I already looked at one of the 30m-1h questions and it appeared to be doable in ~3 minutes with the ability to ctrl-f transcripts but would take longer without transcripts, unknown how long.\n\nIn the next version I will probably use the no-captions AI numbers and measure myself without captions to get a rough video speed multiplier, then possibly do better stats that separate out domains with strong human-time-dependent difficulty from domains without (like this and SWE-Lancer)."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-05-21T00:21:04.200Z",
    "af": true
  },
  {
    "_id": "DS5pshLdBBB5oACQd",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": "nqAbD42mkJnkX8Cfr",
    "topLevelCommentId": "KaaSfntGEBBgadvrF",
    "contents": {
      "markdown": "I would love to have Waymo data. It looks like it's [only available since September 2024](https://waymo.com/safety/impact/#downloads) so I'll still need to use Tesla for the earlier period. More critically they don't publish disengagement data, only crash/injury. There are Waymo claims of things like 1 disengagement every 17,000 miles but I don't believe them without a precise definition for what this number represents.",
      "plaintextDescription": "I would love to have Waymo data. It looks like it's only available since September 2024 so I'll still need to use Tesla for the earlier period. More critically they don't publish disengagement data, only crash/injury. There are Waymo claims of things like 1 disengagement every 17,000 miles but I don't believe them without a precise definition for what this number represents."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-05-20T22:55:32.348Z",
    "af": true
  },
  {
    "_id": "KaaSfntGEBBgadvrF",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "### Edit: Full post [here](https://www.lesswrong.com/posts/6KcP7tEe5hgvHbrSF/metr-how-does-time-horizon-vary-across-domains) with 9 domains and updated conclusions!\n\n### Cross-domain time horizon:Â \n\nWe know AI time horizons (human time-to-complete at which a model has a 50% success rate) on software tasks are currently ~1.5hr and doubling every 4-7 months, but what about other domains? Here's a preliminary result comparing METR's task suite (orange line) to benchmarks in other domains, all of which have some kind of grounding in human data:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/df3ebf754c807c03f6ae1b506ecd93ca9b7d32893dbc908e.png)\n\nObservations\n\n*   Time horizons on agentic computer use (OSWorld) is ~100x shorter than other domains. Domains like Tesla self-driving (tesla\\_fsd), scientific knowledge (gpqa), and math contests (aime), video understanding (video\\_mme), and software (hcast\\_r\\_s) all have roughly similar horizons.\n    *   My guess is this means models are good at taking in information from a long context but bad at acting coherently. Most work requires agency like OSWorld, which may be why AIs can't do the average real-world 1-hour task yet.\n    *   There are likely other domains that fall outside this cluster; these are just the five I examined\n    *   Note the original version had a unit conversion error that gave 60x too high horizons for video_mme; this has been fixed (thanks [@ryan_greenblatt](https://www.lesswrong.com/users/ryan_greenblatt?mention=user) )\n*   Rate of improvement varies significantly; math contests have improved ~50x in the last year but Tesla self-driving only 6x in 3 years.\n*   HCAST is middle of the pack in both.\n\nNote this is preliminary and uses a new methodology so there might be data issues. I'm currently writing up a full post!\n\nIs this graph believable? What do you want to see analyzed?\n\n*edit: fixed Video-MME numbers*",
      "plaintextDescription": "Edit: Full post here with 9 domains and updated conclusions!\n\n\nCross-domain time horizon:Â \nWe know AI time horizons (human time-to-complete at which a model has a 50% success rate) on software tasks are currently ~1.5hr and doubling every 4-7 months, but what about other domains? Here's a preliminary result comparing METR's task suite (orange line) to benchmarks in other domains, all of which have some kind of grounding in human data:\n\nObservations\n\n * Time horizons on agentic computer use (OSWorld) is ~100x shorter than other domains. Domains like Tesla self-driving (tesla_fsd), scientific knowledge (gpqa), and math contests (aime), video understanding (video_mme), and software (hcast_r_s) all have roughly similar horizons.\n   * My guess is this means models are good at taking in information from a long context but bad at acting coherently. Most work requires agency like OSWorld, which may be why AIs can't do the average real-world 1-hour task yet.\n   * There are likely other domains that fall outside this cluster; these are just the five I examined\n   * Note the original version had a unit conversion error that gave 60x too high horizons for video_mme; this has been fixed (thanks @ryan_greenblatt )\n * Rate of improvement varies significantly; math contests have improved ~50x in the last year but Tesla self-driving only 6x in 3 years.\n * HCAST is middle of the pack in both.\n\nNote this is preliminary and uses a new methodology so there might be data issues. I'm currently writing up a full post!\n\nIs this graph believable? What do you want to see analyzed?\n\nedit: fixed Video-MME numbers"
    },
    "baseScore": 123,
    "voteCount": 66,
    "createdAt": null,
    "postedAt": "2025-05-20T05:03:43.826Z",
    "af": true
  },
  {
    "_id": "NWZNy6TahnhnhHD3h",
    "postId": "bBX46cBpbruv2kJh5",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'd guess that a [cheaper, wall-mounted version of CleanAirKits/Airfanta](https://www.lesswrong.com/posts/Zr37dY5YPRT6s56jY/thomas-kwa-s-shortform?commentId=3Ed3suAhizEHpvyJC) would be a better product. It's just a box with fans and slots for filters, the installation labor is significantly lower, you get better aesthetics, and not everyone has 52 inch ceiling fans at a standardized mounting length already so the market is potentially much larger with a standalone device.  \n  \nThe problem with the ceiling fan is that it's not designed for static pressure, so its effectiveness at moving air through the filter will depend on contingent factors like the blade area ratio and distance from the ceiling (which determines how much filter area you can fit). 180 CFM @ 33 dB is better than the Coway but only matches a single box with ~7 PC fans on medium; you can do better with a custom designed ceiling fan but at that point the fan needs to be part of the product.",
      "plaintextDescription": "I'd guess that a cheaper, wall-mounted version of CleanAirKits/Airfanta would be a better product. It's just a box with fans and slots for filters, the installation labor is significantly lower, you get better aesthetics, and not everyone has 52 inch ceiling fans at a standardized mounting length already so the market is potentially much larger with a standalone device.\n\nThe problem with the ceiling fan is that it's not designed for static pressure, so its effectiveness at moving air through the filter will depend on contingent factors like the blade area ratio and distance from the ceiling (which determines how much filter area you can fit). 180 CFM @ 33 dB is better than the Coway but only matches a single box with ~7 PC fans on medium; you can do better with a custom designed ceiling fan but at that point the fan needs to be part of the product."
    },
    "baseScore": 15,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-05-12T02:18:16.901Z",
    "af": false
  },
  {
    "_id": "z5Riqicyjo8m3GD67",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "### **The uplift equation:**\n\nWhat is required for AI to provide net speedup to a software engineering project, when humans write higher quality code than AIs? It depends how it's used.\n\n**Cursor regime**\n\nIn this regime, similar to how I use Cursor agent mode, the human has to read every line of code the AI generates, so we can write:\n\n\\\\(\\text{Speedup factor} = \\frac{t_H}{t_{AI+H} }= \\frac{t_H}{t_{prompt} + t_{AIgen} + t_{check} + p_{fail} \\cdot t_H}\\\\)\n\nWhere\n\n*   \\\\(t_H\\\\)Â is the time for the human to write the code, either from scratch or after rejecting an AI suggestion\n*   \\\\(t_{AIgen}\\\\)Â is the time for the AI to generate the code in tokens per second.\n*   \\\\(t_{check}\\\\)Â is the time for the human to check the code, accept or reject it, and make any minor revisions, in order to bring the code quality and probability of bugs equal with human-written code (\\\\(\\Delta p_{bug} = 0\\\\))\n*   \\\\(p_{fail}\\\\)Â is the fraction of AI suggestions that are rejected entirely.\n\nNote this neglects other factors like code review time, code quality, bugs that aren't caught by the human, or enabling things the human can't do.\n\n**Autonomous regime**\n\nIn this regime the AI is reliable enough that the human doesn't check all the code for bugs, and instead eats the chance of costly bugs entering the codebase.\n\n\\\\(\\text{Speedup factor} = \\frac{t_H}{t_{AI} }= \\frac{t_H}{t_{prompt} + t_{AIgen} + \\Delta p_{bug} \\cdot t_{bugcost} + p_{fail} \\cdot t_H}\\\\)\n\n*   \\\\(\\Delta p_{bug}\\\\)Â is the added probability of a bug from the AI agent compared to a human\n*   \\\\(t_{bugcost}\\\\)Â is the expected cost of a bug, including revenue loss, compromising other projects, and time required to fix the bug. This can be higher thanÂ \\\\(t_H\\\\)\n\n\\\\(\\\\)**Verifiable regime**\n\nIf task success is cheaply verifiable e.g. if comprehensive tests are already written or other AIs can verify the code, then bugs are impossible except through reward hacking.\n\n\\\\(\\text{Speedup factor} = \\frac{t_H}{t_{AI} }= \\frac{t_H}{t_{prompt} + t_{AIgen} + p_{rewardhack}\\cdot t_{bugcost} + p_{fail} \\cdot t_H}\\\\)\n\nHereÂ \\\\(t_{prompt}\\\\)Â is lower than in the other regimes because the verifier can help the generator AI understand the task, andÂ \\\\(t_{AIgen}\\\\)Â can be faster too because you can parallelize.\n\n### Observations\n\nAlthoughÂ \\\\(\\\\)the AI is always faster than writing the code than the human, overall speedup is subject to Amdahl's law, i.e. limited by the slowest component. If AI generation is only 3x as fast as the human per line of code, speedup will never be faster than 3x. Even when AI mistake rate is low enough that we move to the autonomous regime, we still haveÂ \\\\(t_{prompt}\\\\)Â andÂ \\\\(t_{AIgen}\\\\), both of which can be significant fractions ofÂ \\\\(t_H\\\\)Â currently, especially for expert humans who are fast at writing code and projects that are few lines of code. Therefore I expect >5x speedups to overall projects only when (a) AIs write higher-quality code than humans, or (b) tasks are cheaply verifiable and reward hacking is rare, or (c) bothÂ \\\\(t_{prompt}\\\\)Â andÂ \\\\(t_{AIgen}\\\\)Â are much faster than in my current work.\n\nI made a simple Desmos model [here](https://www.desmos.com/calculator/owfn49fwrp).",
      "plaintextDescription": "The uplift equation:\nWhat is required for AI to provide net speedup to a software engineering project, when humans write higher quality code than AIs? It depends how it's used.\n\nCursor regime\n\nIn this regime, similar to how I use Cursor agent mode, the human has to read every line of code the AI generates, so we can write:\n\nSpeedup factor=tHtAI+H=tHtprompt+tAIgen+tcheck+pfailâtH\n\nWhere\n\n * tHÂ is the time for the human to write the code, either from scratch or after rejecting an AI suggestion\n * tAIgenÂ is the time for the AI to generate the code in tokens per second.\n * tcheckÂ is the time for the human to check the code, accept or reject it, and make any minor revisions, in order to bring the code quality and probability of bugs equal with human-written code (Îpbug=0)\n * pfailÂ is the fraction of AI suggestions that are rejected entirely.\n\nNote this neglects other factors like code review time, code quality, bugs that aren't caught by the human, or enabling things the human can't do.\n\nAutonomous regime\n\nIn this regime the AI is reliable enough that the human doesn't check all the code for bugs, and instead eats the chance of costly bugs entering the codebase.\n\nSpeedup factor=tHtAI=tHtprompt+tAIgen+Îpbugâtbugcost+pfailâtH\n\n * ÎpbugÂ is the added probability of a bug from the AI agent compared to a human\n * tbugcostÂ is the expected cost of a bug, including revenue loss, compromising other projects, and time required to fix the bug. This can be higher thanÂ tH\n\nVerifiable regime\n\nIf task success is cheaply verifiable e.g. if comprehensive tests are already written or other AIs can verify the code, then bugs are impossible except through reward hacking.\n\nSpeedup factor=tHtAI=tHtprompt+tAIgen+prewardhackâtbugcost+pfailâtH\n\nHereÂ tpromptÂ is lower than in the other regimes because the verifier can help the generator AI understand the task, andÂ tAIgenÂ can be faster too because you can parallelize.\n\n\nObservations\nAlthoughÂ the AI is always faster than writing the code than the hum"
    },
    "baseScore": 18,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-05-09T20:14:48.679Z",
    "af": false
  },
  {
    "_id": "hi33KnJdiLExuyfbM",
    "postId": "XHgK7iuhNuEdr44BN",
    "parentCommentId": "pen57vYWWqWCrxsgT",
    "topLevelCommentId": "pen57vYWWqWCrxsgT",
    "contents": {
      "markdown": "Why not require model organisms with known ground truth and see if the methods accurately reveal them, like in the paper? From the abstract of that paper:\n\n> Additionally, we argue for scientists using complex non-linear dynamical systems with known ground truth, such as the microprocessor as a validation platform for time-series and structure discovery methods.\n\nThis reduces the problem from covering all sources of doubt to making a sufficiently realistic model organism. This was our idea with [InterpBench](https://arxiv.org/abs/2407.14494), and I still find it plausible that with better execution one could iterate on it (or the probe, crosscoder, etc. equivalent) and make interpretability progress.",
      "plaintextDescription": "Why not require model organisms with known ground truth and see if the methods accurately reveal them, like in the paper? From the abstract of that paper:\n\n> Additionally, we argue for scientists using complex non-linear dynamical systems with known ground truth, such as the microprocessor as a validation platform for time-series and structure discovery methods.\n\nThis reduces the problem from covering all sources of doubt to making a sufficiently realistic model organism. This was our idea with InterpBench, and I still find it plausible that with better execution one could iterate on it (or the probe, crosscoder, etc. equivalent) and make interpretability progress."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-05-08T00:55:06.997Z",
    "af": false
  },
  {
    "_id": "8uCFQfQtpkMqi8R5F",
    "postId": "5CGNxadG3JRbGfGfg",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Author here. When constructing this paper, we needed an interpretable metric (time horizon), but this is not very data-efficient. We basically made the fewest tasks we could to get acceptable error bars, because high-quality task creation from scratch is very expensive. (We already spent over $150k baselining tasks, and more on the bounty and baselining infra.) Therefore we should expect that restricting to only 32 of the 170 tasks in the paper makes the error bars much wider; it roughly increases the error bars by a factor of sqrt(170/32) = 2.3.\n\nNow if these tasks were log-uniformly distributed from 1 second to 16 hours, we would still be able to compare these results to the rest of our dataset, but it's worse than that: all fully_private tasks are too difficult for models before GPT-4, so removing SWAA requires restricting the x axis to the 2 years since GPT-4. This is 1/3 of our x axis range, so it makes error bars on the trendline slope 3 times larger. Combined with the previous effect the error bars become 6.9 times larger than the main paper! So the analysis in this post, although it uses higher-quality data, basically throws away all the statistical power of the paper. I wish I had 170 high-quality private tasks but there was simply not enough time to construct them. Likewise, I wish we had baselines for all tasks, but sadly we will probably move to a different safety case framework before we get them.\n\nThough SWAA tasks have their own problems (eg many of them were written by Claude), they were developed in-house and are private, and it's unlikely that GPT-2 and GPT-3 would have a horizon like 10x different on a different benchmark. So for this check we really should not exclude them.\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">privacy_level</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">num_tasks</td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">fully_private</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">32</td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">public_problem</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">22</td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">easy_to_memorize</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">18</td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">public_solution</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">17</td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">semi_private</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">2</td></tr></tbody></table>\n\nWhen we include SWAA, the story is not too different from the original paper: doubling roughly 8 months. Note how large the error bars are.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e17b0c71e324b420d6b76141a2beafdaad0b30dd7da24629.png)\n\nWith only fully_private tasks and SWAA and RE-Bench, the extrapolated date of 1-month AI looks similar to the main paper, but the error bars are much larger; this is entirely driven by the small number of tasks (green bar).Â \n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/948dabcafbb6be07a691c0cfe4ec2f34239a3d85b048c20c.png)\n\nFor comparison, the main paper extrapolation\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bf68dde16913926f7872de945fdb8494ab374ad6d43a458d.png)\n\nWhen I exclude SWAA and RE-Bench too, the script refuses to even run, because sometimes the time horizon slope is negative, but when I suppress those errors and take the 2024-2025 trend we get 80% CI of early 2026 to mid-2047! This is consistent with the main result but pretty uninformative.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a4bac77cb0daa25a64931b4dafbbb45c58a6b710b706d6ef.png)\n\nYou can see that with only fully_private tasks (ignore the tasks under 1 minute; these are SWAA), it's hard to even tell whether longer tasks are more difficult in the <1 hour range (tiles plot). As such we should be suspicious of whether the time horizon metric works at all with so few tasks.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e437603ab4545cd4240b92eaab50bd55c40b3fe4b93a76bc.png)",
      "plaintextDescription": "Author here. When constructing this paper, we needed an interpretable metric (time horizon), but this is not very data-efficient. We basically made the fewest tasks we could to get acceptable error bars, because high-quality task creation from scratch is very expensive. (We already spent over $150k baselining tasks, and more on the bounty and baselining infra.) Therefore we should expect that restricting to only 32 of the 170 tasks in the paper makes the error bars much wider; it roughly increases the error bars by a factor of sqrt(170/32) = 2.3.\n\nNow if these tasks were log-uniformly distributed from 1 second to 16 hours, we would still be able to compare these results to the rest of our dataset, but it's worse than that: all fully_private tasks are too difficult for models before GPT-4, so removing SWAA requires restricting the x axis to the 2 years since GPT-4. This is 1/3 of our x axis range, so it makes error bars on the trendline slope 3 times larger. Combined with the previous effect the error bars become 6.9 times larger than the main paper! So the analysis in this post, although it uses higher-quality data, basically throws away all the statistical power of the paper. I wish I had 170 high-quality private tasks but there was simply not enough time to construct them. Likewise, I wish we had baselines for all tasks, but sadly we will probably move to a different safety case framework before we get them.\n\nThough SWAA tasks have their own problems (eg many of them were written by Claude), they were developed in-house and are private, and it's unlikely that GPT-2 and GPT-3 would have a horizon like 10x different on a different benchmark. So for this check we really should not exclude them.\n\nprivacy_levelnum_tasksfully_private32public_problem22easy_to_memorize18public_solution17semi_private2\n\nWhen we include SWAA, the story is not too different from the original paper: doubling roughly 8 months. Note how large the error bars are.\n\nWith only fully_private tasks an"
    },
    "baseScore": 50,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2025-05-06T22:31:33.081Z",
    "af": false
  },
  {
    "_id": "ZThGwM3uDJqBMCagQ",
    "postId": "s3NaETDujoxj4GbEm",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I ran the horizon length graph with pass@8 instead, and the increase between GPT-4 and o1 seems to be slightly smaller (could be noise), and also Claude 3.7 does worse than o1. This means the doubling rate for pass@8 may be slightly slower than for pass@1. However, if horizon length increase since 2023 were only due to RL, the improvement from pass@8 would be barely half as much as the improvement in pass@1. It's faster, which could be due to some combination of the following:\n\n*   o1 has a better base model than GPT-4\n*   HCAST is an example of \"emergent capabilities on long-horizon tasks\", unlike their non-agentic tasks\n*   RL helps more on HCAST *skills* than math or non-agentic coding\n*   RL helps more on larger models\n*   Some kind of nonlinearity in the relevant metrics\n\nThere are various problems with this graph (e.g. to follow the same methodology we should filter for models that are frontier on pass@8, not frontier on pass@1) but this was meant to be a quick and dirty check.\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">&nbsp;</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">GPT-4 horizon</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">o1 horizon</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Ratio</td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Pass@8</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">24 min</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">151 min</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">6.3x</td></tr><tr><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">Pass@1</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">5 min</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">39 min</td><td style=\"border-color:#000000;padding:5pt;vertical-align:top\">7.8x</td></tr></tbody></table>\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/3f0d168766b1f0e9703ed8cee00ba74ee1af6c002a82ab04.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/cfbb3f60d732ef946ab953c531983847b3658f5e82a6645d.png)",
      "plaintextDescription": "I ran the horizon length graph with pass@8 instead, and the increase between GPT-4 and o1 seems to be slightly smaller (could be noise), and also Claude 3.7 does worse than o1. This means the doubling rate for pass@8 may be slightly slower than for pass@1. However, if horizon length increase since 2023 were only due to RL, the improvement from pass@8 would be barely half as much as the improvement in pass@1. It's faster, which could be due to some combination of the following:\n\n * o1 has a better base model than GPT-4\n * HCAST is an example of \"emergent capabilities on long-horizon tasks\", unlike their non-agentic tasks\n * RL helps more on HCAST skills than math or non-agentic coding\n * RL helps more on larger models\n * Some kind of nonlinearity in the relevant metrics\n\nThere are various problems with this graph (e.g. to follow the same methodology we should filter for models that are frontier on pass@8, not frontier on pass@1) but this was meant to be a quick and dirty check.\n\nÂ GPT-4 horizono1 horizonRatioPass@824 min151 min6.3xPass@15 min39 min7.8x"
    },
    "baseScore": 33,
    "voteCount": 18,
    "createdAt": null,
    "postedAt": "2025-05-05T22:44:10.696Z",
    "af": false
  },
  {
    "_id": "GkpixFGxERpigGpsk",
    "postId": "s3NaETDujoxj4GbEm",
    "parentCommentId": "PBsvreBqFaRH2La6y",
    "topLevelCommentId": "PBsvreBqFaRH2La6y",
    "contents": {
      "markdown": "Agree, I'm pretty confused about this discrepancy. I can't rule out that it's just the \"RL can enable emergent capabilities\" point.",
      "plaintextDescription": "Agree, I'm pretty confused about this discrepancy. I can't rule out that it's just the \"RL can enable emergent capabilities\" point."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-05T21:27:31.345Z",
    "af": false
  },
  {
    "_id": "PLG9PZR5aiek4tudp",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "zvk3FbeMYX6zRyrFa",
    "topLevelCommentId": "zvk3FbeMYX6zRyrFa",
    "contents": {
      "markdown": "The dates used in our regression are the dates models were publicly released, not the dates we benchmarked them. If we use the latter dates, or the dates they were announced, I agree they would be more arbitrary.\n\nAlso, there is lots of noise in a time horizon measurement and it only displays any sort of pattern because we measured over many orders of magnitude and years. It's not very meaningful to extrapolate from just 2 data points; there are many reasons one datapoint could randomly change by a couple of months or factor of 2 in time horizon.Â \n\n*   Release schedules could be altered\n*   A model could be overfit to our dataset\n*   One model could play less well with our elicitation/scaffolding\n*   One company could be barely at the frontier, and release a slightly-better model right before the leading company releases a much-better model.\n\nAll of these factors are averaged out if you look at more than 2 models. So I prefer to see each model as evidence of whether the trend is accelerating or slowing down over the last 1-2 years, rather than an individual model being very meaningful.",
      "plaintextDescription": "The dates used in our regression are the dates models were publicly released, not the dates we benchmarked them. If we use the latter dates, or the dates they were announced, I agree they would be more arbitrary.\n\nAlso, there is lots of noise in a time horizon measurement and it only displays any sort of pattern because we measured over many orders of magnitude and years. It's not very meaningful to extrapolate from just 2 data points; there are many reasons one datapoint could randomly change by a couple of months or factor of 2 in time horizon.Â \n\n * Release schedules could be altered\n * A model could be overfit to our dataset\n * One model could play less well with our elicitation/scaffolding\n * One company could be barely at the frontier, and release a slightly-better model right before the leading company releases a much-better model.\n\nAll of these factors are averaged out if you look at more than 2 models. So I prefer to see each model as evidence of whether the trend is accelerating or slowing down over the last 1-2 years, rather than an individual model being very meaningful."
    },
    "baseScore": 10,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-05-03T19:43:38.210Z",
    "af": true
  },
  {
    "_id": "5qdbojEuLeDrWsGtJ",
    "postId": "ohgGyu78DXLMaASpE",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "o3 and o4-mini solve more than zero of the >1hr tasks that claude 3.7 got ~zero on, including some >4hr tasks that no previous models we tested have done well on, so it's not that models hit a wall at 1-4 hours. My guess is that the tasks they have been trained on are just more similar to HCAST tasks than RE-Bench tasks, though there are other possibilities.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/82ff20b3d0943c37e691b3ac26a29267e883406202c5b4a3.png)",
      "plaintextDescription": "o3 and o4-mini solve more than zero of the >1hr tasks that claude 3.7 got ~zero on, including some >4hr tasks that no previous models we tested have done well on, so it's not that models hit a wall at 1-4 hours. My guess is that the tasks they have been trained on are just more similar to HCAST tasks than RE-Bench tasks, though there are other possibilities."
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-05-03T05:39:26.052Z",
    "af": false
  },
  {
    "_id": "q5fLfkbooPihnsRhm",
    "postId": "bffJJvCC78LZjFa3Z",
    "parentCommentId": "TtvuZGaC3svKvNQFm",
    "topLevelCommentId": "TtvuZGaC3svKvNQFm",
    "contents": {
      "markdown": "Other metrics also point to drone-dominated and C&C dominated war. E.g. towed artillery is too vulnerable to counterbattery fire, and modern mobile artillery like [CAESAR](https://en.wikipedia.org/wiki/CAESAR_self-propelled_howitzer) must use \"shoot and scoot\" tactics-- it can fire 6 shells within two minutes of stopping and vacate before its last shell lands. But now drones [attack them while moving](https://battlemachines.org/2024/04/08/ukraine-wars-learnings-in-artillery/) too.",
      "plaintextDescription": "Other metrics also point to drone-dominated and C&C dominated war. E.g. towed artillery is too vulnerable to counterbattery fire, and modern mobile artillery like CAESAR must use \"shoot and scoot\" tactics-- it can fire 6 shells within two minutes of stopping and vacate before its last shell lands. But now drones attack them while moving too."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-05-01T20:12:23.260Z",
    "af": false
  },
  {
    "_id": "4cvRKAq8J46u3bRjD",
    "postId": "Zg5Lm4kfftxXgEKAQ",
    "parentCommentId": "zcDh3nHDjqcvsHSiu",
    "topLevelCommentId": "mm8bz6pRRDWHRuuyz",
    "contents": {
      "markdown": "Yes. RL will at least be *more* applicable to well-defined tasks. Some intuitions:\n\n*   In my everyday, the gap between well-defined task ability and working with the METR codebase is growing\n*   4 month doubling time is faster than the rate of progress in most other realistic or unrealistic domains\n*   Recent models really like to reward hack, suggesting that RL can cause some behaviors not relevant to realistic tasks\n\nThis trend will break at some point, eg when labs get better at applying RL to realistic tasks, or when RL hits diminishing returns, but I have no idea when",
      "plaintextDescription": "Yes. RL will at least be more applicable to well-defined tasks. Some intuitions:\n\n * In my everyday, the gap between well-defined task ability and working with the METR codebase is growing\n * 4 month doubling time is faster than the rate of progress in most other realistic or unrealistic domains\n * Recent models really like to reward hack, suggesting that RL can cause some behaviors not relevant to realistic tasks\n\nThis trend will break at some point, eg when labs get better at applying RL to realistic tasks, or when RL hits diminishing returns, but I have no idea when"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-04-25T01:01:30.781Z",
    "af": false
  },
  {
    "_id": "wRupM2t4hxnDdnous",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "GDM paper: [Evaluating the Goal-Directedness of Large Language Models](https://arxiv.org/html/2504.11844)\n\nTom Everitt, Rohin Shah, and others from GDM attempt to measure \"whether LLMs use their capabilities towards their given goal\". Unlike [previous work](https://arxiv.org/abs/2412.04758), their measure is not just rescaled task performance-- rather, an AI is goal-directed if it uses its capabilities effectively. A model that is not goal-directed when attempting a task will have capabilities but not properly use them. Thus, we can measure goal-directedness by comparing a model's actual performance to how it *should* perform if it were using its full capabilities on subtasks.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d1a90b4d16632aced59dd04dc9a9dd41d9ba6bc507b48eee.png)\n\nThis kind of measure could in principle be useful to measure:\n\n1.  prerequisites of scheming: maybe a model is not capable of scheming until it reaches some level of goal-directedness, and we can forecast when that happens\n2.  elicitation: a normally goal-directed model that is not trying on evals may be underelicited\n3.  sandbagging: if an AI unexpectedly has low goal-directedness on evals, this could be a warning sign for sandbagging.\n\nUnfortunately, it's mostly saturated already-- Gemini 2.0 and Claude 3.7 are over 95%. Even GPT-4 gets over 70%.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4c8cc1e0f98f56b4d324679f09538a18dfa789017da96b43.png)\n\nSo until we measure models on tasks where they are currently non-goal-directed, we can't tell whether future models are more goal-directed than Claude 3.7, precluding use (1). I also feel like scheming will require a qualitatively different kind of goal-directedness more related to instrumental reasoning. This measure seems potentially useful for measuring elicitation and sandbagging, though.\n\nAnother limitation is that this can only be measured on tasks that can be cleanly decomposed into subtasks, and whose performance is mathematically predictable from subtask performance. The algorithms they use to estimateÂ \\\\(\\mathbb E[R_{\\pi_c^*}]\\\\)Â (Appendix C) basically sample performance on every subtask and compute task performance from this.",
      "plaintextDescription": "GDM paper: Evaluating the Goal-Directedness of Large Language Models\n\nTom Everitt, Rohin Shah, and others from GDM attempt to measure \"whether LLMs use their capabilities towards their given goal\". Unlike previous work, their measure is not just rescaled task performance-- rather, an AI is goal-directed if it uses its capabilities effectively. A model that is not goal-directed when attempting a task will have capabilities but not properly use them. Thus, we can measure goal-directedness by comparing a model's actual performance to how it should perform if it were using its full capabilities on subtasks.\n\nThis kind of measure could in principle be useful to measure:\n\n 1. prerequisites of scheming: maybe a model is not capable of scheming until it reaches some level of goal-directedness, and we can forecast when that happens\n 2. elicitation: a normally goal-directed model that is not trying on evals may be underelicited\n 3. sandbagging: if an AI unexpectedly has low goal-directedness on evals, this could be a warning sign for sandbagging.\n\nUnfortunately, it's mostly saturated already-- Gemini 2.0 and Claude 3.7 are over 95%. Even GPT-4 gets over 70%.\n\nSo until we measure models on tasks where they are currently non-goal-directed, we can't tell whether future models are more goal-directed than Claude 3.7, precluding use (1). I also feel like scheming will require a qualitatively different kind of goal-directedness more related to instrumental reasoning. This measure seems potentially useful for measuring elicitation and sandbagging, though.\n\nAnother limitation is that this can only be measured on tasks that can be cleanly decomposed into subtasks, and whose performance is mathematically predictable from subtask performance. The algorithms they use to estimateÂ E[RÏâc]Â (Appendix C) basically sample performance on every subtask and compute task performance from this."
    },
    "baseScore": 17,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2025-04-21T09:40:15.356Z",
    "af": false
  },
  {
    "_id": "mm8bz6pRRDWHRuuyz",
    "postId": "Zg5Lm4kfftxXgEKAQ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Time horizon of o3 is ~1.5 hours vs Claude 3.7's 54 minutes, and it's [statistically significant](https://x.com/Kwathomas0/status/1912692988003917880) that it's above the long-term trend. It's been less than 2 months since the release of Claude 3.7. If time horizon continues doubling every 3.5 months as it has over the last year, we only have another 12 months until time horizon hits 16 hours and we are unable to measure it with HCAST.\n\nMy guess is that future model time horizon will double every 3-4 months for well-defined tasks (HCAST, RE-Bench, most automatically scorable tasks) that labs can RL on, while capability on more realistic tasks will follow the long-term 7-month doubling time.",
      "plaintextDescription": "Time horizon of o3 is ~1.5 hours vs Claude 3.7's 54 minutes, and it's statistically significant that it's above the long-term trend. It's been less than 2 months since the release of Claude 3.7. If time horizon continues doubling every 3.5 months as it has over the last year, we only have another 12 months until time horizon hits 16 hours and we are unable to measure it with HCAST.\n\nMy guess is that future model time horizon will double every 3-4 months for well-defined tasks (HCAST, RE-Bench, most automatically scorable tasks) that labs can RL on, while capability on more realistic tasks will follow the long-term 7-month doubling time."
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-04-17T02:41:55.815Z",
    "af": false
  },
  {
    "_id": "Fk8G79myPS3Cjqr3H",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Benchmark Readiness Level\n\nSafety-relevant properties should be ranked on a \"Benchmark Readiness Level\" (BRL) scale, inspired by NASA's [Technology Readiness Levels](https://www.nasa.gov/directorates/somd/space-communications-navigation-program/technology-readiness-levels/). At BRL 4, a benchmark exists; at BRL 6 the benchmark is highly valid; past this point the benchmark becomes increasingly robust against sandbagging. The definitions could look something like this:\n\n<table style=\"border-style:none\"><tbody><tr><td style=\"background-color:#356854;padding:2px 8px\">BRL</td><td style=\"background-color:#356854;border-bottom:1px solid #284e3f;border-right:1px solid #356854;border-top:1px solid #284e3f;padding:2px 8px\"><code>Definition</code></td><td style=\"background-color:#356854;border-bottom:1px solid #284e3f;border-right:1px solid #284e3f;border-top:1px solid #284e3f;padding:2px 8px\">Example</td></tr><tr><td style=\"background-color:#e67c73;border-bottom:1px solid #e67c73;border-left:1px solid #284e3f;border-right:1px solid #e67c73;padding:2px 8px;text-align:right\">1</td><td style=\"background-color:#ffffff;border-bottom:1px solid #f6f8f9;border-right:1px solid #ffffff;padding:2px 8px\"><code>Theoretical relevance to x-risk defined</code></td><td style=\"background-color:#ffffff;border-bottom:1px solid #f6f8f9;border-right:1px solid #284e3f;padding:2px 8px\">Adversarial competence</td></tr><tr><td style=\"background-color:#ec9c57;border-bottom:1px solid #ec9c57;border-left:1px solid #284e3f;border-right:1px solid #ec9c57;padding:2px 8px;text-align:right\">2</td><td style=\"background-color:#f6f8f9;border-bottom:1px solid #f6f8f9;border-right:1px solid #f6f8f9;padding:2px 8px\"><code>Property operationalized for frontier AIs and ASIs</code></td><td style=\"background-color:#f6f8f9;border-bottom:1px solid #f6f8f9;border-right:1px solid #284e3f;padding:2px 8px\">AI R&amp;D speedup;<br>Misaligned goals</td></tr><tr><td style=\"background-color:#f2bd3a;border-bottom:1px solid #f2bd3a;border-left:1px solid #284e3f;border-right:1px solid #f2bd3a;padding:2px 8px;text-align:right\">3</td><td style=\"background-color:#ffffff;border-bottom:1px solid #f6f8f9;border-right:1px solid #ffffff;padding:2px 8px\"><code>Behavior (or all parts) observed in artificial settings. Preliminary measurements exist, but may have large methodological flaws.</code></td><td style=\"background-color:#ffffff;border-bottom:1px solid #f6f8f9;border-right:1px solid #284e3f;padding:2px 8px\"><a href=\"https://openai.com/index/chain-of-thought-monitoring/\"><u>Reward hacking</u></a></td></tr><tr><td style=\"background-color:#f8de1d;border-bottom:1px solid #f8de1d;border-left:1px solid #284e3f;border-right:1px solid #f8de1d;padding:2px 8px;text-align:right\">4</td><td style=\"background-color:#f6f8f9;border-bottom:1px solid #f6f8f9;border-right:1px solid #f6f8f9;padding:2px 8px\"><code>Benchmark developed, but may measure different core skills from the ideal measure</code></td><td style=\"background-color:#f6f8f9;border-bottom:1px solid #f6f8f9;border-right:1px solid #284e3f;padding:2px 8px\">Cyber offense (CyBench)</td></tr><tr><td style=\"background-color:#ffff00;border-bottom:1px solid #d5ee22;border-left:1px solid #284e3f;border-right:1px solid #ffff00;padding:2px 8px;text-align:right\">5</td><td style=\"background-color:#ffffff;border-bottom:1px solid #f6f8f9;border-right:1px solid #ffffff;padding:2px 8px\"><code>Benchmark measures roughly what we want; superhuman range; methodology is documented and reproducible but may have validity concerns.</code></td><td style=\"background-color:#ffffff;border-bottom:1px solid #f6f8f9;border-right:1px solid #284e3f;padding:2px 8px\">Software<br>(HCAST++)</td></tr><tr><td style=\"background-color:#d5ee22;border-bottom:1px solid #abdd45;border-left:1px solid #284e3f;border-right:1px solid #d5ee22;padding:2px 8px;text-align:right\">6</td><td style=\"background-color:#f6f8f9;border-bottom:1px solid #f6f8f9;border-right:1px solid #f6f8f9;padding:2px 8px\"><code>\"Production quality\" high-validity benchmark. Strongly superhuman range; run on many frontier models; red-teamed for validity; represents all sub-capabilities. Portable implementation.</code></td><td style=\"background-color:#f6f8f9;border-bottom:1px solid #f6f8f9;border-right:1px solid #284e3f;padding:2px 8px\">&nbsp;</td></tr><tr><td style=\"background-color:#abdd45;border-bottom:1px solid #81cc67;border-left:1px solid #284e3f;border-right:1px solid #abdd45;padding:2px 8px;text-align:right\">7</td><td style=\"background-color:#ffffff;border-bottom:1px solid #f6f8f9;border-right:1px solid #ffffff;padding:2px 8px\"><code>Extensive validity checks against downstream properties; reasonable attempts (e.g. fine-tuning) to detect whether AIs are manipulating/sandbagging their scores.</code></td><td style=\"background-color:#ffffff;border-bottom:1px solid #f6f8f9;border-right:1px solid #284e3f;padding:2px 8px\">Knowledge<br>(MMLU with fine-tuning)</td></tr><tr><td style=\"background-color:#81cc67;border-bottom:1px solid #57bb8a;border-left:1px solid #284e3f;border-right:1px solid #81cc67;padding:2px 8px;text-align:right\">8</td><td style=\"background-color:#f6f8f9;border-bottom:1px solid #f6f8f9;border-right:1px solid #f6f8f9;padding:2px 8px\">&nbsp;</td><td style=\"background-color:#f6f8f9;border-bottom:1px solid #f6f8f9;border-right:1px solid #284e3f;padding:2px 8px\">-</td></tr><tr><td style=\"background-color:#57bb8a;border-bottom:1px solid #284e3f;border-left:1px solid #284e3f;border-right:1px solid #57bb8a;padding:2px 8px;text-align:right\">9</td><td style=\"background-color:#ffffff;border-bottom:1px solid #284e3f;border-right:1px solid #ffffff;padding:2px 8px\"><code>Benchmark has high validity to real-world contexts beyond lab settings. Can ensure accuracy even when measuring superintelligences with potentially deceptive capabilities.</code></td><td style=\"background-color:#ffffff;border-bottom:1px solid #284e3f;border-right:1px solid #284e3f;padding:2px 8px\">-</td></tr></tbody></table>\n\nHere's a draft list of properties we could apply the BRL to, including capabilities and propensity:\n\n*   General capabilities\n    *   Software ability (HCAST, others)\n    *   Knowledge (Humanity's Last Exam, others)\n*   Sandbagging\n    *   Situational awareness\n    *   Alignment faking\n    *   Sandbagging ability\n*   Monitorability\n    *   Steganographic collusion\n    *   Neuralese\n    *   Faithful CoT\n    *   Ability to monitor smarter AIs\n*   Research acceleration\n    *   % LoC written by AIs\n    *   AI R&D Uplift\n*   Escape risk\n    *   Adversarial competence\n    *   Self-exfiltration\n    *   Cyber offense\n*   Misalignment\n    *   Reward hacking\n    *   Misaligned goals\n*   Other Dangers\n    *   CBRN\n    *   Persuasion\n\nIs this doomed? Am I missing anything important?",
      "plaintextDescription": "Benchmark Readiness Level\n\nSafety-relevant properties should be ranked on a \"Benchmark Readiness Level\" (BRL) scale, inspired by NASA's Technology Readiness Levels. At BRL 4, a benchmark exists; at BRL 6 the benchmark is highly valid; past this point the benchmark becomes increasingly robust against sandbagging. The definitions could look something like this:\n\nBRLDefinitionExample1Theoretical relevance to x-risk definedAdversarial competence2Property operationalized for frontier AIs and ASIsAI R&D speedup;\nMisaligned goals3Behavior (or all parts) observed in artificial settings. Preliminary measurements exist, but may have large methodological flaws.Reward hacking4Benchmark developed, but may measure different core skills from the ideal measureCyber offense (CyBench)5Benchmark measures roughly what we want; superhuman range; methodology is documented and reproducible but may have validity concerns.Software\n(HCAST++)6\"Production quality\" high-validity benchmark. Strongly superhuman range; run on many frontier models; red-teamed for validity; represents all sub-capabilities. Portable implementation.Â 7Extensive validity checks against downstream properties; reasonable attempts (e.g. fine-tuning) to detect whether AIs are manipulating/sandbagging their scores.Knowledge\n(MMLU with fine-tuning)8Â -9Benchmark has high validity to real-world contexts beyond lab settings. Can ensure accuracy even when measuring superintelligences with potentially deceptive capabilities.-\n\nHere's a draft list of properties we could apply the BRL to, including capabilities and propensity:\n\n * General capabilities\n   * Software ability (HCAST, others)\n   * Knowledge (Humanity's Last Exam, others)\n * Sandbagging\n   * Situational awareness\n   * Alignment faking\n   * Sandbagging ability\n * Monitorability\n   * Steganographic collusion\n   * Neuralese\n   * Faithful CoT\n   * Ability to monitor smarter AIs\n * Research acceleration\n   * % LoC written by AIs\n   * AI R&D Uplift\n * Escape risk\n   * Adversar"
    },
    "baseScore": 14,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-04-09T21:09:43.334Z",
    "af": false
  },
  {
    "_id": "noF8PubgWD5iFmEYo",
    "postId": "Zr37dY5YPRT6s56jY",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Some versions of the [METR time horizon paper](https://www.lesswrong.com/posts/deesrjitvXM4xYGZd/metr-measuring-ai-ability-to-complete-long-tasks) from alternate universes:\n\n**Measuring AI Ability to Take Over Small Countries (idea by Caleb Parikh)**\n\nAbstract: Many are worried that AI will take over the world, but extrapolation from existing benchmarks suffers from a large distributional shift that makes it difficult to forecast the date of world takeover. We rectify this by constructing a suite of 193 realistic, diverse countries with territory sizes from 0.44 to 17 million km^2. Taking over most countries requires acting over a long time horizon, with the exception of France. Over the last 6 years, the land area that AI can successfully take over with 50% success rate has increased from 0 to 0 km^2, at the rate of 0 km^2 per year (95% CI 0.0-0.0 km^2/year); extrapolation suggests that AI world takeover is unlikely to occur in the near future. To address concerns about the narrowness of our distribution, we also study AI ability to take over small planets and asteroids, and find similar trends.\n\n**Measuring AI Ability to Worry About AI**\n\nAbstract: Since 2019, the amount of time LW has spent worrying about AI has doubled every seven months, and now constitutes the primary bottleneck to AI safety research. Automation of worrying would be transformative to the research landscape, but worrying includes several complex behaviors, ranging from simple fretting to concern, anxiety, perseveration, and existential dread, and so is difficult to measure. We benchmark the ability of frontier AIs to worry about common topics like disease, romantic rejection, and job security, and find that current frontier models such as Claude 3.7 Sonnet already outperform top humans, especially in existential dread. If these results generalize to worrying about AI risk, AI systems will be capable of autonomously worrying about their own capabilities by the end of this year, allowing us to outsource all our AI concerns to the systems themselves.\n\n**Estimating Time Since The Singularity**\n\nEarly work on the time horizon paper used a hyperbolic fit, which predicted that AGI (AI with an infinite time horizon) was reached last Thursday. \\[1\\] We were skeptical at first because the R^2 was extremely low, but [recent analysis by Epoch](https://x.com/tamaybes/status/1902537990062342547) suggested that AI already outperformed humans at a 100-year time horizon by about 2016. We have no choice but to infer that the Singularity has already happened, and therefore the world around us is a simulation. We construct a Monte Carlo estimate over dates since the Singularity and simulator intentions, and find that the simulation will likely be turned off in the next three to six months.\n\n\\[1\\]: This is true",
      "plaintextDescription": "Some versions of the METR time horizon paper from alternate universes:\n\nMeasuring AI Ability to Take Over Small Countries (idea by Caleb Parikh)\n\nAbstract: Many are worried that AI will take over the world, but extrapolation from existing benchmarks suffers from a large distributional shift that makes it difficult to forecast the date of world takeover. We rectify this by constructing a suite of 193 realistic, diverse countries with territory sizes from 0.44 to 17 million km^2. Taking over most countries requires acting over a long time horizon, with the exception of France. Over the last 6 years, the land area that AI can successfully take over with 50% success rate has increased from 0 to 0 km^2, at the rate of 0 km^2 per year (95% CI 0.0-0.0 km^2/year); extrapolation suggests that AI world takeover is unlikely to occur in the near future. To address concerns about the narrowness of our distribution, we also study AI ability to take over small planets and asteroids, and find similar trends.\n\nMeasuring AI Ability to Worry About AI\n\nAbstract: Since 2019, the amount of time LW has spent worrying about AI has doubled every seven months, and now constitutes the primary bottleneck to AI safety research. Automation of worrying would be transformative to the research landscape, but worrying includes several complex behaviors, ranging from simple fretting to concern, anxiety, perseveration, and existential dread, and so is difficult to measure. We benchmark the ability of frontier AIs to worry about common topics like disease, romantic rejection, and job security, and find that current frontier models such as Claude 3.7 Sonnet already outperform top humans, especially in existential dread. If these results generalize to worrying about AI risk, AI systems will be capable of autonomously worrying about their own capabilities by the end of this year, allowing us to outsource all our AI concerns to the systems themselves.\n\nEstimating Time Since The Singularity\n\nEarly work on t"
    },
    "baseScore": 80,
    "voteCount": 35,
    "createdAt": null,
    "postedAt": "2025-04-01T20:01:35.970Z",
    "af": true
  },
  {
    "_id": "3o48boGEREHWe7faZ",
    "postId": "gwKyHqe4CL6TZNQxp",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Quick list of reasons for me:\n\n*   I'm averse to attending mass protests myself because they make it harder to think clearly and I usually don't agree with everything any given movement stands for.\n*   Under my worldview, an unconditional pause is a much harder ask than is required to save most worlds if p(doom) is 14% (the number stated on the website). It seems highly impractical to implement compared to more common regulatory frameworks and is also super unaesthetic because I am generally pro-progress.\n*   The economic and political landscape around AI is complicated enough that agreeing with their stated goals is not enough; you need to agree with their theory of change.\n    *   Broad public movements require making alliances which can be harmful in the long term. Environmentalism turned anti-nuclear, a decades-long mistake which has accelerated climate change by years. PauseAI wants to include people who oppose AI on its present dangers, which makes me uneasy. What if the landscape changes such that the best course of action is contrary to PauseAI's current goals?\n*   I think [PauseAI's theory of change](https://pauseai.info/theory-of-change) is weak\n    *   From reading the website, they want to leverage protests, volunteer lobbying, and informing the public into an international treaty banning superhuman AI and a unilateral supply-chain pause. It seems hard for the general public to have significant influence over this kind of issue unless AI rises to the top issue for most Americans, since the current top issue is improving the economy, which directly conflicts with a pause.\n*   There are better theories of change\n    *   Strengthening RSPs into industry standards, then regulations.\n    *   Directly informing elites about the dangers of AI, rather than the general public.\n*   History (e.g. civil rights movement) shows that moderates not publicly endorsing radicals can result in a positive radical flank effect making moderates' goals easier to achieve.",
      "plaintextDescription": "Quick list of reasons for me:\n\n * I'm averse to attending mass protests myself because they make it harder to think clearly and I usually don't agree with everything any given movement stands for.\n * Under my worldview, an unconditional pause is a much harder ask than is required to save most worlds if p(doom) is 14% (the number stated on the website). It seems highly impractical to implement compared to more common regulatory frameworks and is also super unaesthetic because I am generally pro-progress.\n * The economic and political landscape around AI is complicated enough that agreeing with their stated goals is not enough; you need to agree with their theory of change.\n   * Broad public movements require making alliances which can be harmful in the long term. Environmentalism turned anti-nuclear, a decades-long mistake which has accelerated climate change by years. PauseAI wants to include people who oppose AI on its present dangers, which makes me uneasy. What if the landscape changes such that the best course of action is contrary to PauseAI's current goals?\n * I think PauseAI's theory of change is weak\n   * From reading the website, they want to leverage protests, volunteer lobbying, and informing the public into an international treaty banning superhuman AI and a unilateral supply-chain pause. It seems hard for the general public to have significant influence over this kind of issue unless AI rises to the top issue for most Americans, since the current top issue is improving the economy, which directly conflicts with a pause.\n * There are better theories of change\n   * Strengthening RSPs into industry standards, then regulations.\n   * Directly informing elites about the dangers of AI, rather than the general public.\n * History (e.g. civil rights movement) shows that moderates not publicly endorsing radicals can result in a positive radical flank effect making moderates' goals easier to achieve."
    },
    "baseScore": 12,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-03-30T21:38:49.448Z",
    "af": false
  },
  {
    "_id": "Bi5dLk9vEnbLcFfi8",
    "postId": "deesrjitvXM4xYGZd",
    "parentCommentId": "xQ7cW4WaiArDhchNA",
    "topLevelCommentId": "xQ7cW4WaiArDhchNA",
    "contents": {
      "markdown": "I basically agree with this. The reason the paper didn't include this kind of reasoning (only a paragraph about how AGI will have infinite horizon length) is we felt that making a forecast based on a superexponential trend would be too much speculation for an academic paper. (There is really no way to make one without heavy reliance on priors; does it speed up by 10% per doubling or 20%?) It wasn't necessary given the 2027 and 2029-2030 dates for 1-month AI derived from extrapolation already roughly bracketed our uncertainty.",
      "plaintextDescription": "I basically agree with this. The reason the paper didn't include this kind of reasoning (only a paragraph about how AGI will have infinite horizon length) is we felt that making a forecast based on a superexponential trend would be too much speculation for an academic paper. (There is really no way to make one without heavy reliance on priors; does it speed up by 10% per doubling or 20%?) It wasn't necessary given the 2027 and 2029-2030 dates for 1-month AI derived from extrapolation already roughly bracketed our uncertainty."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-03-30T20:38:56.442Z",
    "af": true
  },
  {
    "_id": "xMMrMCywTbbz6c6bT",
    "postId": "deesrjitvXM4xYGZd",
    "parentCommentId": "AKokBiGhgDjk8Ehno",
    "topLevelCommentId": "ynckjtFaLW95khFkE",
    "contents": {
      "markdown": "External validity is a huge concern, so we don't claim anything as ambitious as average knowledge worker tasks. In one sentence, my opinion is that our tasks suite is fairly representative of *well-defined, low-context, measurable software tasks that can be done without a GUI*. More speculatively, horizons on this are probably within a large (~10x) constant factor of horizons on most other software tasks. We have a lot more discussion of this in the paper, especially in heading 7.2.1 \"Systematic differences between our tasks and real tasks\". The [HCAST paper](https://metr.org/hcast.pdf) also has a better description of the dataset.\n\nWe didn't try to make the dataset a perfectly stratified sample of tasks meeting that description, but there is enough diversity in the dataset that I'm much more concerned about relevance of HCAST-like tasks to real life than relevance of HCAST to the universe of HCAST-like tasks.",
      "plaintextDescription": "External validity is a huge concern, so we don't claim anything as ambitious as average knowledge worker tasks. In one sentence, my opinion is that our tasks suite is fairly representative of well-defined, low-context, measurable software tasks that can be done without a GUI. More speculatively, horizons on this are probably within a large (~10x) constant factor of horizons on most other software tasks. We have a lot more discussion of this in the paper, especially in heading 7.2.1 \"Systematic differences between our tasks and real tasks\". The HCAST paper also has a better description of the dataset.\n\nWe didn't try to make the dataset a perfectly stratified sample of tasks meeting that description, but there is enough diversity in the dataset that I'm much more concerned about relevance of HCAST-like tasks to real life than relevance of HCAST to the universe of HCAST-like tasks."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-24T23:24:43.837Z",
    "af": false
  },
  {
    "_id": "kqKebfKDnMjDL7MCA",
    "postId": "deesrjitvXM4xYGZd",
    "parentCommentId": "SoSKmvD5fBtXALdv3",
    "topLevelCommentId": "xQ7cW4WaiArDhchNA",
    "contents": {
      "markdown": "Humans don't need 10x more memory per step nor 100x more compute to do a 10-year project than a 1-year project, so this is proof it isn't a hard constraint. It might need an architecture change but if the Gods of Straight Lines control the trend, AI companies will invent it as part of normal algorithmic progress and we will remain on an exponential / superexponential trend.",
      "plaintextDescription": "Humans don't need 10x more memory per step nor 100x more compute to do a 10-year project than a 1-year project, so this is proof it isn't a hard constraint. It might need an architecture change but if the Gods of Straight Lines control the trend, AI companies will invent it as part of normal algorithmic progress and we will remain on an exponential / superexponential trend."
    },
    "baseScore": 8,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-03-21T18:59:01.412Z",
    "af": false
  },
  {
    "_id": "LJeubAtSBWvy9rcn8",
    "postId": "deesrjitvXM4xYGZd",
    "parentCommentId": "AtpTmxyQNremn7SCn",
    "topLevelCommentId": "AtpTmxyQNremn7SCn",
    "contents": {
      "markdown": "Regarding 1 and 2, I basically agree that SWAA doesn't provide much independent signal. The reason we made SWAA was that models before GPT-4 got ~0% on HCAST, so we needed shorter tasks to measure their time horizon. 3 is definitely a concern and we're currently collecting data on open-source PRs to get a more representative sample of long tasks.",
      "plaintextDescription": "Regarding 1 and 2, I basically agree that SWAA doesn't provide much independent signal. The reason we made SWAA was that models before GPT-4 got ~0% on HCAST, so we needed shorter tasks to measure their time horizon. 3 is definitely a concern and we're currently collecting data on open-source PRs to get a more representative sample of long tasks."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-03-21T02:01:13.892Z",
    "af": false
  },
  {
    "_id": "mmycWuqnACmxaqz47",
    "postId": "deesrjitvXM4xYGZd",
    "parentCommentId": "buj5WB2fxodngMM8t",
    "topLevelCommentId": "buj5WB2fxodngMM8t",
    "contents": {
      "markdown": "> That bit at the end about \"time horizon of our average baseliner\" is a little confusing to me, but I understand it to mean \"if we used the 50% reliability metric on the humans we had do these tasks, our model would say humans can't reliably perform tasks that take longer than an hour\". Which is a pretty interesting point.\n\nThat's basically correct. To give a little more context for why we don't really believe this number, during data collection we were not really trying to measure the human success rate, just get successful human runs and measure their time. It was very common for baseliners to realize that finishing the task would take too long, give up, and try to collect speed bonuses on other tasks. This is somewhat concerning for biasing the human time-to-complete estimates, but much more concerning for this human time horizon measurement. So we don't claim the human time horizon as a result.",
      "plaintextDescription": "> That bit at the end about \"time horizon of our average baseliner\" is a little confusing to me, but I understand it to mean \"if we used the 50% reliability metric on the humans we had do these tasks, our model would say humans can't reliably perform tasks that take longer than an hour\". Which is a pretty interesting point.\n\nThat's basically correct. To give a little more context for why we don't really believe this number, during data collection we were not really trying to measure the human success rate, just get successful human runs and measure their time. It was very common for baseliners to realize that finishing the task would take too long, give up, and try to collect speed bonuses on other tasks. This is somewhat concerning for biasing the human time-to-complete estimates, but much more concerning for this human time horizon measurement. So we don't claim the human time horizon as a result."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-03-21T01:45:33.152Z",
    "af": false
  },
  {
    "_id": "K7bTXepdH7PbRmboP",
    "postId": "deesrjitvXM4xYGZd",
    "parentCommentId": "nidd8zevvHExytHL8",
    "topLevelCommentId": "nidd8zevvHExytHL8",
    "contents": {
      "markdown": "All models since at least GPT-3 have had this steep exponential decay \\[1\\], and the whole logistic curve has kept shifting to the right. The 80% success rate horizon has basically the same 7-month doubling time as the 50% horizon so it's not just an artifact of picking 50% as a threshold.\n\nClaude 3.7 isn't doing better on >2 hour tasks than o1, so it might be that the curve is compressing, but this might also just be noise or imperfect elicitation.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/71859fb920a70386e8ec91500fd4959a116c3d27330747af.png)\n\nRegarding the idea that autoregressive models would plateau at hours or days, it's plausible, and one point of evidence is that models are not really coherent over hundreds of steps (generations + uses of the Python tool) yet-- they do 1-2 hour tasks with ~10 actions, see section 5 of HCAST paper. On the other hand, current LLMs can learn a lot in-context and it's not clear there are limits to this. In our qualitative analysis we found evidence of increasing coherence, where o1 fails tasks due to repeating failed actions 6x less than GPT-4 1106.\n\nMaybe this could be tested by extracting ~1 hour tasks out of the hours to days long projects that we think are heavy in self-modeling, like planning. But we will see whether there's a plateau at the hours range in the next year or two anyway.\n\n\\[1\\] we don't have easy enough tasks that GPT-2 can do them with >50% success, so can't check the shape",
      "plaintextDescription": "All models since at least GPT-3 have had this steep exponential decay [1], and the whole logistic curve has kept shifting to the right. The 80% success rate horizon has basically the same 7-month doubling time as the 50% horizon so it's not just an artifact of picking 50% as a threshold.\n\nClaude 3.7 isn't doing better on >2 hour tasks than o1, so it might be that the curve is compressing, but this might also just be noise or imperfect elicitation.\n\nRegarding the idea that autoregressive models would plateau at hours or days, it's plausible, and one point of evidence is that models are not really coherent over hundreds of steps (generations + uses of the Python tool) yet-- they do 1-2 hour tasks with ~10 actions, see section 5 of HCAST paper. On the other hand, current LLMs can learn a lot in-context and it's not clear there are limits to this. In our qualitative analysis we found evidence of increasing coherence, where o1 fails tasks due to repeating failed actions 6x less than GPT-4 1106.\n\nMaybe this could be tested by extracting ~1 hour tasks out of the hours to days long projects that we think are heavy in self-modeling, like planning. But we will see whether there's a plateau at the hours range in the next year or two anyway.\n\n[1] we don't have easy enough tasks that GPT-2 can do them with >50% success, so can't check the shape"
    },
    "baseScore": 12,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-03-21T01:37:05.709Z",
    "af": false
  },
  {
    "_id": "TCipGBiyDw6fmpz44",
    "postId": "gXyMCnjrMfBbnYyZ4",
    "parentCommentId": "DRf2SkxbCrbnDo5Ks",
    "topLevelCommentId": "2JvXnFbwb3kZzSaxo",
    "contents": {
      "markdown": "It's expensive to construct and baseline novel tasks for this (we spent well over $100k on human baselines) so what we are able to measure in the future depends on whether we can harvest realistic tasks that naturally have human data. You could do a rough analysis on math contest problems, say assigning GSM8K and AIME questions lengths based on a guess of how long expert humans take, but the external validity concerns are worse than for software. For one thing, AIME has much harder topics than GSM8K (we tried to make SWAA not be artificially easier or harder than HCAST); for another, neither are particularly close to the average few minutes of a research mathematician's job.",
      "plaintextDescription": "It's expensive to construct and baseline novel tasks for this (we spent well over $100k on human baselines) so what we are able to measure in the future depends on whether we can harvest realistic tasks that naturally have human data. You could do a rough analysis on math contest problems, say assigning GSM8K and AIME questions lengths based on a guess of how long expert humans take, but the external validity concerns are worse than for software. For one thing, AIME has much harder topics than GSM8K (we tried to make SWAA not be artificially easier or harder than HCAST); for another, neither are particularly close to the average few minutes of a research mathematician's job."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-21T01:08:59.128Z",
    "af": false
  },
  {
    "_id": "cExKMTXNjJEa3HJCw",
    "postId": "deesrjitvXM4xYGZd",
    "parentCommentId": "9AHdJxuuGYxRqbj38",
    "topLevelCommentId": "WX7Gzski8RMwDf5f8",
    "contents": {
      "markdown": "The trend probably sped up in 2024. If the future trend follows the 2024--2025 trend, we get 50% reliability at 167 hours in 2027.",
      "plaintextDescription": "The trend probably sped up in 2024. If the future trend follows the 2024--2025 trend, we get 50% reliability at 167 hours in 2027."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-21T00:43:51.465Z",
    "af": false
  },
  {
    "_id": "2JvXnFbwb3kZzSaxo",
    "postId": "gXyMCnjrMfBbnYyZ4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Author here. My best guess is that by around the 1-month point, AIs will be automating large parts of both AI capabilities and empirical alignment research. Inferring anything more depends on many other beliefs.\n\nCurrently no one knows how hard the alignment problem is or what exactly good alignment research means-- it is the furthest-looking, least well-defined and least tractable of the subfields of AI existential safety. This means we don't know the equivalent task length of the alignment problem. Even more importantly, we only measured the AIs at software tasks and don't know what the trend is for other domains like math or law, it could be wildly different.\n\nWith that said, my current guess is that alignment will be sped up by AI slightly less than capabilities will be, success looks like building [deferrable AI](https://www.lesswrong.com/posts/TTFsKxQThrqgWeXYJ/how-might-we-safely-pass-the-buck-to-ai), and whether we succeed depends on whether the world dedicates more than X% \\[1\\] of AI research resources to relevant safety research than the exact software time horizon of the AIs involved, which is not directly applicable.\n\n\\[1\\] X is some unknown number probably between 0% and 65%",
      "plaintextDescription": "Author here. My best guess is that by around the 1-month point, AIs will be automating large parts of both AI capabilities and empirical alignment research. Inferring anything more depends on many other beliefs.\n\nCurrently no one knows how hard the alignment problem is or what exactly good alignment research means-- it is the furthest-looking, least well-defined and least tractable of the subfields of AI existential safety. This means we don't know the equivalent task length of the alignment problem. Even more importantly, we only measured the AIs at software tasks and don't know what the trend is for other domains like math or law, it could be wildly different.\n\nWith that said, my current guess is that alignment will be sped up by AI slightly less than capabilities will be, success looks like building deferrable AI, and whether we succeed depends on whether the world dedicates more than X% [1] of AI research resources to relevant safety research than the exact software time horizon of the AIs involved, which is not directly applicable.\n\n[1] X is some unknown number probably between 0% and 65%"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-21T00:42:16.482Z",
    "af": false
  },
  {
    "_id": "YQ8sfiB9ivj4J5YAA",
    "postId": "deesrjitvXM4xYGZd",
    "parentCommentId": "ynckjtFaLW95khFkE",
    "topLevelCommentId": "ynckjtFaLW95khFkE",
    "contents": {
      "markdown": "AIs (and humans) don't have 100% reliability at anything, so the graph tracks when AIs get a 50% success rate on our dataset, over all tasks and attempts. We also measure AI horizons at 80% success rate in the paper, and those are about 5x shorter. It's hard to measure much higher than 80% with our limited task suite, but if we could we would measure 95% and 99% as well.",
      "plaintextDescription": "AIs (and humans) don't have 100% reliability at anything, so the graph tracks when AIs get a 50% success rate on our dataset, over all tasks and attempts. We also measure AI horizons at 80% success rate in the paper, and those are about 5x shorter. It's hard to measure much higher than 80% with our limited task suite, but if we could we would measure 95% and 99% as well."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-03-21T00:06:59.393Z",
    "af": false
  },
  {
    "_id": "5ZB9thAApiNFjRb8E",
    "postId": "aCsYyLwKiNHxAzFB5",
    "parentCommentId": "RddhMQwa6q79hrBwo",
    "topLevelCommentId": "RddhMQwa6q79hrBwo",
    "contents": {
      "markdown": "> So the citation is an unreleased paper! That unreleased paper may make a splash, since (assuming this 7-month-doubling trend is not merely 1-2 years old) it strongly implies we really will find good solutions for turning LLMs agentic fairly soon.\n\nThe 7-month doubling trend we measured actually goes back to GPT-2 in 2019. Since 2024, the trend has been faster, doubling roughly every 3-4 months depending on how you measure, but we only have six 2024-25 models so error bars are wide and it's really unclear which trend will be predictive of the future.",
      "plaintextDescription": "> So the citation is an unreleased paper! That unreleased paper may make a splash, since (assuming this 7-month-doubling trend is not merely 1-2 years old) it strongly implies we really will find good solutions for turning LLMs agentic fairly soon.\n\nThe 7-month doubling trend we measured actually goes back to GPT-2 in 2019. Since 2024, the trend has been faster, doubling roughly every 3-4 months depending on how you measure, but we only have six 2024-25 models so error bars are wide and it's really unclear which trend will be predictive of the future."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-03-17T18:23:22.752Z",
    "af": false
  },
  {
    "_id": "Dj4EsMjDJeCG4QKPQ",
    "postId": "7wFdXj9oR8M9AiFht",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Important results from the paper as I see it:\n\n*   RL agents can learn to cheat in the wild, and CoT pressure doesn't prevent it.\n*   The slope of cheating increase and final cheating rate are both slightly lower when applying CoT pressure, but all of the cheating will go uncaught by the monitor.\n*   GPT-4o can zero-shot monitor o3-level agents with 95% accuracy when it has access to CoT, only 60% when it can only monitor actions.\n*   If there is no CoT pressure, RL agents telegraph their cheating, such that cheating precision and recall actually go *up* over time for a monitor that can only see CoT\n*   True utility (success rate counting cheating as failure) initially increases faster with CoT pressure than without, despite low initial levels of cheating in both.\n    *   I'm not sure why this is given they said the only change in reward related to CoT pressure is that the agent doesn't get reward when the monitor flags.\n*   If you allow rejection sampling, the utility curve is roughly similar between CoT pressure and no CoT pressure. This rhymes with the Scaling Laws for Reward Model Overoptimization result where utility is similar between RL and best-of-k\n\nSpeculation:\n\n*   The information that the model is cheating seems basically gone from the CoT, so training against a weak monitor and monitoring using a stronger monitor could be unreliable\n*   How much CoT pressure should you apply to maximize utility? Figure 4 suggests that if you do RL, cheating will happen at roughly the same point in training with or without CoT pressure (~0.3 epochs), which also coincides with the maximum utility point, so stop at the point where the non-CoT-pressure model starts to do detectable cheating.\n*   It is unclear how this applies to cheating / other malicious behaviors that are hard enough that they require CoT. My prior is that capabilities at hiding misaligned behaviors under CoT monitoring will improve at similar rates to capabilities at reasoning without CoT in other domains, although the presence of a monitor might specifically target the skill of obfuscating misaligned reasoning and make it happen sooner.",
      "plaintextDescription": "Important results from the paper as I see it:\n\n * RL agents can learn to cheat in the wild, and CoT pressure doesn't prevent it.\n * The slope of cheating increase and final cheating rate are both slightly lower when applying CoT pressure, but all of the cheating will go uncaught by the monitor.\n * GPT-4o can zero-shot monitor o3-level agents with 95% accuracy when it has access to CoT, only 60% when it can only monitor actions.\n * If there is no CoT pressure, RL agents telegraph their cheating, such that cheating precision and recall actually go up over time for a monitor that can only see CoT\n * True utility (success rate counting cheating as failure) initially increases faster with CoT pressure than without, despite low initial levels of cheating in both.\n   * I'm not sure why this is given they said the only change in reward related to CoT pressure is that the agent doesn't get reward when the monitor flags.\n * If you allow rejection sampling, the utility curve is roughly similar between CoT pressure and no CoT pressure. This rhymes with the Scaling Laws for Reward Model Overoptimization result where utility is similar between RL and best-of-k\n\nSpeculation:\n\n * The information that the model is cheating seems basically gone from the CoT, so training against a weak monitor and monitoring using a stronger monitor could be unreliable\n * How much CoT pressure should you apply to maximize utility? Figure 4 suggests that if you do RL, cheating will happen at roughly the same point in training with or without CoT pressure (~0.3 epochs), which also coincides with the maximum utility point, so stop at the point where the non-CoT-pressure model starts to do detectable cheating.\n * It is unclear how this applies to cheating / other malicious behaviors that are hard enough that they require CoT. My prior is that capabilities at hiding misaligned behaviors under CoT monitoring will improve at similar rates to capabilities at reasoning without CoT in other domains, although th"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-03-12T23:27:54.784Z",
    "af": true
  },
  {
    "_id": "JJdhF7mkjW6nfWJrA",
    "postId": "oKAFFvaouKKEhbBPm",
    "parentCommentId": "qPdiFgL8ivyzEbBdY",
    "topLevelCommentId": "jiMbyDh4uyhzj8Fsc",
    "contents": {
      "markdown": "I think we have two separate claims here:\n\n1.  Do technologies that have lots of resources put into their development generally improve discontinuously or by huge slope changes?\n2.  Do technologies often get displaced by technologies with a different lineage?\n\nI agree with your position on (2) here. But it seems like the claim in the post that sometime in the 2030s someone will make a single important architectural innovation that leads to takeover within a year mostly depends on (1), as it would require progress within that year to be comparable to all the progress from now until that year. Also you said the architectural innovation might be a slight tweak to the LLM architecture, which would mean it shares the same lineage.\n\nThe history of machine learning seems pretty continuous wrt advance prediction. In the Epoch graph, the line fit on loss of the best LSTM up to 2016 sees a slope change of less than 2x, whereas a hypothetical innovation that causes takeover within a year with not much progress in the intervening 8 years would be ~8x. So it seems more likely to me (conditional on 2033 timelines and a big innovation) that we get some architectural innovation which has a moderately different lineage in 2027, it overtakes transformers' performance in 2029, and afterward causes the rate of AI improvement to increase by something like 1.5x-2x.\n\n2 out of 3 of the technologies you listed probably have continuous improvement despite the lineage change\n\n*   1910-era cars were only a little better than horses, and the overall speed someone could travel long distances in the US probably increased in slope by <2x after cars due to things like road quality improvement before cars and improvements in ships and rail (though maybe railroads were a discontinuity, not sure)\n*   Before refrigerators we had low-quality refrigerators that would contaminate the ice with ammonia, and before that people shipped ice from Maine, so I would expect the cost/quality of refrigeration to have much less than an 8x slope change at the advent of mechanical refrigeration\n*   Only rockets were [actually a discontinuity](https://aiimpacts.org/historic-trends-in-long-range-military-payload-delivery/)\n\nTell me if you disagree.",
      "plaintextDescription": "I think we have two separate claims here:\n\n 1. Do technologies that have lots of resources put into their development generally improve discontinuously or by huge slope changes?\n 2. Do technologies often get displaced by technologies with a different lineage?\n\nI agree with your position on (2) here. But it seems like the claim in the post that sometime in the 2030s someone will make a single important architectural innovation that leads to takeover within a year mostly depends on (1), as it would require progress within that year to be comparable to all the progress from now until that year. Also you said the architectural innovation might be a slight tweak to the LLM architecture, which would mean it shares the same lineage.\n\nThe history of machine learning seems pretty continuous wrt advance prediction. In the Epoch graph, the line fit on loss of the best LSTM up to 2016 sees a slope change of less than 2x, whereas a hypothetical innovation that causes takeover within a year with not much progress in the intervening 8 years would be ~8x. So it seems more likely to me (conditional on 2033 timelines and a big innovation) that we get some architectural innovation which has a moderately different lineage in 2027, it overtakes transformers' performance in 2029, and afterward causes the rate of AI improvement to increase by something like 1.5x-2x.\n\n2 out of 3 of the technologies you listed probably have continuous improvement despite the lineage change\n\n * 1910-era cars were only a little better than horses, and the overall speed someone could travel long distances in the US probably increased in slope by <2x after cars due to things like road quality improvement before cars and improvements in ships and rail (though maybe railroads were a discontinuity, not sure)\n * Before refrigerators we had low-quality refrigerators that would contaminate the ice with ammonia, and before that people shipped ice from Maine, so I would expect the cost/quality of refrigeration to have "
    },
    "baseScore": 11,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-03-07T01:02:39.974Z",
    "af": false
  },
  {
    "_id": "wgtvF3LYDqYcACGNm",
    "postId": "oKAFFvaouKKEhbBPm",
    "parentCommentId": "35Nudg3BdPZQBfeXg",
    "topLevelCommentId": "jiMbyDh4uyhzj8Fsc",
    "contents": {
      "markdown": "> Easy verification makes for benchmarks that can quickly be cracked by LLMs. Hard verification makes for benchmarks that aren't used.\n\nAgree, this is one big limitation of the paper I'm working on at METR. The first two ideas you listed are things I would very much like to measure, and the third something I would like to measure but is much harder than any current benchmark given that university takes humans years rather than hours. If we measure it right, we could tell whether generalization is steadily improving or plateauing.",
      "plaintextDescription": "> Easy verification makes for benchmarks that can quickly be cracked by LLMs. Hard verification makes for benchmarks that aren't used.\n\nAgree, this is one big limitation of the paper I'm working on at METR. The first two ideas you listed are things I would very much like to measure, and the third something I would like to measure but is much harder than any current benchmark given that university takes humans years rather than hours. If we measure it right, we could tell whether generalization is steadily improving or plateauing."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-03-07T00:23:55.769Z",
    "af": false
  }
]