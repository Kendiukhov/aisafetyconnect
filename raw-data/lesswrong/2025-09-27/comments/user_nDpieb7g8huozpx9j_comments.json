[
  {
    "_id": "J5JF5oY8Jb4368TLh",
    "postId": "kNyMwXQxctWtaRZhs",
    "parentCommentId": "XhpmberwvDzYKuSRP",
    "topLevelCommentId": "XhpmberwvDzYKuSRP",
    "contents": {
      "markdown": "Oh, whoops, I screwed up there. When adapting my initial write-up to this sequence of LW posts, I reordered its sections. [Part 4](https://www.lesswrong.com/posts/gR96ANcNXQgh22qtG/synthesizing-standalone-world-models-part-4-metaphysical), which explains the idea I'm gesturing at by \"the anthropic prior\" here, was initially Part 1, and I missed the inconsistency in 2.3 the reordering created. Should be fixed now.\n\nThough I basically just mean \"the simplicity prior\" here. Part 4 covers why I think the simplicity prior is *also* the \"well-abstractability\" prior, and it does so using the anthropics frame, hence \"the anthropic prior\".",
      "plaintextDescription": "Oh, whoops, I screwed up there. When adapting my initial write-up to this sequence of LW posts, I reordered its sections. Part 4, which explains the idea I'm gesturing at by \"the anthropic prior\" here, was initially Part 1, and I missed the inconsistency in 2.3 the reordering created. Should be fixed now.\n\nThough I basically just mean \"the simplicity prior\" here. Part 4 covers why I think the simplicity prior is also the \"well-abstractability\" prior, and it does so using the anthropics frame, hence \"the anthropic prior\"."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-27T01:30:14.064Z",
    "af": false
  },
  {
    "_id": "TNty5ekBEi9CMCtAJ",
    "postId": "zYCoqjYNHFAEJD8TC",
    "parentCommentId": "YJt7FBeD3rkLWj8qY",
    "topLevelCommentId": "YJt7FBeD3rkLWj8qY",
    "contents": {
      "markdown": "That's probably this post itself, not a shortform? See the segment starting from \"the way it can matter\".",
      "plaintextDescription": "That's probably this post itself, not a shortform? See the segment starting from \"the way it can matter\"."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-26T03:50:52.942Z",
    "af": false
  },
  {
    "_id": "4HgT28fzMbHoKbaQd",
    "postId": "zYCoqjYNHFAEJD8TC",
    "parentCommentId": "qtfhDBjMeoiTqpqba",
    "topLevelCommentId": "qtfhDBjMeoiTqpqba",
    "contents": {
      "markdown": "(**Disclaimer:** writing off the top of my head, haven't thought too deeply about it, may be erroneous.)\n\nConsider probability theory. In its context, we sometimes talk about different kinds of worlds we could be in, differing by the outcome of some random process. Does that mean we think those other parallel worlds are literal? Not necessarily. In some cases, that's literally impossible; e. g., talking about worlds differing by something we're *logically* uncertain about. I don't know whether I live in a world in which the \\\\(10^{100}\\\\)th digit of pi is odd or even, but that doesn't mean both worlds can exist.\n\nYet, talking about \"different worlds\" is still a useful framework/bit of verbiage.\n\nNext: For whatever reason, the universe favors simplicity. Simple explanations, simple hypotheses, Occam's razor. Or perhaps \"simplicity\" is just a term for \"how much the universe prefers this thing\". In any case, if you want to make predictions about what happens, using some kind of simplicity prior is useful.\n\nAnthropic reasoning is, in large part, just a way of reasoning about that simplicity prior. When I say that we're more likely to be coarse agents because they \"draw on reality fluid from an entire pool of low-level agents\", I do not *necessarily* mean that there are *literal* alternate realities populated by agents slightly different from each other at the low level, and that we-the-coarse-agents are somehow implemented on all of them simultaneously, much like there isn't literally several worlds differing by the \\\\(10^{100}\\\\)th digit of pi.\n\nRather, what I mean is that, when we're calculating \"how much the universe will like this hypothesis about reality\", we would want to offset the raw complexity of the explanation by taking into account how many observationally equivalent explanations there are, if we want to compute the correct result. The universe likes simplicity, for whatever reason; being alt-simple is one way to be simple; therefore, hypotheses about reality in which we discover that we are \"coarse\" agents are favored by the generalized Occam's razor. To do otherwise, to argue that we exist in some manner in which our low-level implementation is uniquely defined, goes against the razor; it's postulating unnecessary details/entities.\n\nBut all of this is a mouthful, so it's easier to stick to talking about different worlds. They may or may not be literally real, seems like a reasonable bit of metaphysics to me, but I don't really know. It's above our current paygrade anyway, I shut up and calculate.\n\n> What observation is better explained or predicted if one assumes Tegmark universes?\n\nTegmark IV is a way to think about the space of all mathematically possible universes over which we define the probability distribution regarding the structure of reality we are in. In large part, it's a framework/metaphysical interpretation, and doesn't promise to make predictions different from any other valid framework for talking about the space of all possible realities.\n\nQuantum immortality/anthropic immortality is a separate assumption; Tegmark IV doesn't assume it (as you point out, it's coherent to imagine that you still only exist in a specific continuity in it) and it doesn't require Tegmark IV (e. g., in a solipsism-like view, you can also assume that you will never stop receiving further observations, and thereby restrict the set of hypotheses about the universe to those where you will never die – all without ever assuming there's more than one true universe).\n\n> What if my utility function is keeping alive and flourishing this one specific instance of myself, connected in an unbroken sequential chain to every previous instance?\n\nThose words may not actually mean anything. Like, if you compute 5+7 = 11, there's no meaningful sense in which you can pick out the \"original\" 5 out of that 11. You can if the elements being added up had some additional structure, if the equation is an abstraction over a more complex reality. But what if there *is* no such additional structure, if there's just no machinery for picking out \"this specific instance of yourself\"?\n\nLike, perhaps we are in a cleverly structured multi-level simulation that runs all Everett branches, and to save processing power, it functions as follows: first it computes the highest-level history of the world, then all second-highest-level histories consistent with that highest-level history, then all third-highest-level histories, et cetera; and suppose the bulk of our experiences is computed at some non-lowest level. In that case, you quite literally \"exist\" in *all* low-level histories consistent with your coarse high-level experiences; the bulk of those experiences was calculated first, then lower-level histories were generated \"around\" them, with some details added in. \"Which specific low-level history do I exist in?\" is then meaningless to ask.\n\nOr maybe not, maybe the simulation hub we are in runs all low-level histories separately, and you actually are in one of them. How can we tell?\n\nProbably neither of those, probably we don't yet have any idea what's *really* going on. Shrug.",
      "plaintextDescription": "(Disclaimer: writing off the top of my head, haven't thought too deeply about it, may be erroneous.)\n\nConsider probability theory. In its context, we sometimes talk about different kinds of worlds we could be in, differing by the outcome of some random process. Does that mean we think those other parallel worlds are literal? Not necessarily. In some cases, that's literally impossible; e. g., talking about worlds differing by something we're logically uncertain about. I don't know whether I live in a world in which the 10100th digit of pi is odd or even, but that doesn't mean both worlds can exist.\n\nYet, talking about \"different worlds\" is still a useful framework/bit of verbiage.\n\nNext: For whatever reason, the universe favors simplicity. Simple explanations, simple hypotheses, Occam's razor. Or perhaps \"simplicity\" is just a term for \"how much the universe prefers this thing\". In any case, if you want to make predictions about what happens, using some kind of simplicity prior is useful.\n\nAnthropic reasoning is, in large part, just a way of reasoning about that simplicity prior. When I say that we're more likely to be coarse agents because they \"draw on reality fluid from an entire pool of low-level agents\", I do not necessarily mean that there are literal alternate realities populated by agents slightly different from each other at the low level, and that we-the-coarse-agents are somehow implemented on all of them simultaneously, much like there isn't literally several worlds differing by the 10100th digit of pi.\n\nRather, what I mean is that, when we're calculating \"how much the universe will like this hypothesis about reality\", we would want to offset the raw complexity of the explanation by taking into account how many observationally equivalent explanations there are, if we want to compute the correct result. The universe likes simplicity, for whatever reason; being alt-simple is one way to be simple; therefore, hypotheses about reality in which we discover that"
    },
    "baseScore": 7,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-26T01:24:17.274Z",
    "af": false
  },
  {
    "_id": "eCepZtgbPokKScGsg",
    "postId": "xvCNjLL3GZ6w2BWeb",
    "parentCommentId": "ABfyXnk3DhKarwP2J",
    "topLevelCommentId": "ABfyXnk3DhKarwP2J",
    "contents": {
      "markdown": "Perhaps, but we're not aiming for an algorithm able to discover the minimal description of \"anything\". We're working with an object of a very specific type, that has a very specific structure which could be exploited (perhaps the way these posts sketch out) to find that minimal representation.\n\nI think this is a general problem with these kinds of fundamental impossibility theorems (see also: no-free-lunch theorems, the data-processing inequality, [Legg's minimial-complexity theorem](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/synthesizing-standalone-world-models-bounties-seeking?commentId=CAzBy3kKF8Ta9tMWR)). They're usually inapplicable to practical cases, because they talk about highly generic objects which are missing all the structures that make those problems solvable in practice. [This](https://x.com/VictorTaelin/status/1905323662447616106) is a good post on the topic.",
      "plaintextDescription": "Perhaps, but we're not aiming for an algorithm able to discover the minimal description of \"anything\". We're working with an object of a very specific type, that has a very specific structure which could be exploited (perhaps the way these posts sketch out) to find that minimal representation.\n\nI think this is a general problem with these kinds of fundamental impossibility theorems (see also: no-free-lunch theorems, the data-processing inequality, Legg's minimial-complexity theorem). They're usually inapplicable to practical cases, because they talk about highly generic objects which are missing all the structures that make those problems solvable in practice. This is a good post on the topic."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-25T12:10:41.994Z",
    "af": false
  },
  {
    "_id": "6CeHaaQPZRbacnpyN",
    "postId": "LngR93YwiEpJ3kiWh",
    "parentCommentId": "KKHgnsnab3eDiGk4v",
    "topLevelCommentId": "KKHgnsnab3eDiGk4v",
    "contents": {
      "markdown": "> There are effectively infinitely many things about the world that one could figure out\n\nOne way to control that is to control the training data. We don't necessarily have to point the wm-synthesizer at [the Pile](https://arxiv.org/abs/2101.00027) indiscriminately,[^w690x9plosh] we could assemble a dataset about a specific phenomenon we want to comprehend.\n\n> if we’re talking about e.g. possible inventions that don’t exist yet, then the combinatorial explosion of possibilities gets even worse\n\nHuman world-models are [lazy](https://www.lesswrong.com/posts/HaHcsrDSZ3ZC2b4fK/world-model-interpretability-is-all-we-need#3D__Laziness): they store knowledge in the maximally \"decomposed\" form[^8miq0m70xlh], and only synthesize specific concrete concepts when they're needed. (E. g., \"a triangular lightbulb\", which we could easily generate – which our world-models effectively \"contain\" – but which isn't generated until needed.)\n\nI expect inventions are the same thing. Given a powerful-enough world-model, we should be able to produce what we want just by using the world-model's native functions for that. Pick the needed concepts, plug them into each other in the right way, hit \"run\".\n\nIf constructing the concepts we want requires agency, the one contributing it could be *the human operator*, if they understand how the world-model works well enough.\n\nWill e-mail regarding the rest.\n\n> It’s funny that I’m always begging people to stop trying to reverse-engineer the neocortex, and you’re working on something that (if successful) would end up somewhere pretty similar to that, IIUC\n\nThe irony is not lost on me. When I was reading your Foom & Doom posts, and got to [this section](https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement#1_6_1_I_m_broadly_pessimistic_about_existing_efforts_to_delay_AGI), I did have a reaction roughly along [those lines](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e861bfa1b4d36306e48c8c8f953ca7d74938082ab582a936.png/w_720).\n\n> (But hmm, I guess if a paranoid doom-pilled person was trying to reverse-engineer the neocortex and only publish the results if they thought it would help with safe & beneficial AGI, and if they in fact had good judgment on that question, then I guess I’d be grudgingly OK with that.)\n\nI genuinely appreciate the sanity-check and the vote of confidence here!\n\n[^w690x9plosh]: Indeed, we might want to actively avoid that. \n\n[^8miq0m70xlh]: Perhaps something along the lines of the constructive-PID thing I sketched out.",
      "plaintextDescription": "> There are effectively infinitely many things about the world that one could figure out\n\nOne way to control that is to control the training data. We don't necessarily have to point the wm-synthesizer at the Pile indiscriminately,[1] we could assemble a dataset about a specific phenomenon we want to comprehend.\n\n> if we’re talking about e.g. possible inventions that don’t exist yet, then the combinatorial explosion of possibilities gets even worse\n\nHuman world-models are lazy: they store knowledge in the maximally \"decomposed\" form[2], and only synthesize specific concrete concepts when they're needed. (E. g., \"a triangular lightbulb\", which we could easily generate – which our world-models effectively \"contain\" – but which isn't generated until needed.)\n\nI expect inventions are the same thing. Given a powerful-enough world-model, we should be able to produce what we want just by using the world-model's native functions for that. Pick the needed concepts, plug them into each other in the right way, hit \"run\".\n\nIf constructing the concepts we want requires agency, the one contributing it could be the human operator, if they understand how the world-model works well enough.\n\nWill e-mail regarding the rest.\n\n> It’s funny that I’m always begging people to stop trying to reverse-engineer the neocortex, and you’re working on something that (if successful) would end up somewhere pretty similar to that, IIUC\n\nThe irony is not lost on me. When I was reading your Foom & Doom posts, and got to this section, I did have a reaction roughly along those lines.\n\n> (But hmm, I guess if a paranoid doom-pilled person was trying to reverse-engineer the neocortex and only publish the results if they thought it would help with safe & beneficial AGI, and if they in fact had good judgment on that question, then I guess I’d be grudgingly OK with that.)\n\nI genuinely appreciate the sanity-check and the vote of confidence here!\n\n 1. ^\n    Indeed, we might want to actively avoid that.\n\n 2. ^\n   "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-24T18:56:22.806Z",
    "af": true
  },
  {
    "_id": "cz8oEaxP4s5KohThw",
    "postId": "LngR93YwiEpJ3kiWh",
    "parentCommentId": "CAzBy3kKF8Ta9tMWR",
    "topLevelCommentId": "CAzBy3kKF8Ta9tMWR",
    "contents": {
      "markdown": "Yep, I know of this result. I haven't looked into it in depth, but my understanding is that it only says that powerful predictors have to be \"complex\" in the sense of *high Kolmogorov complexity*, right? But \"high K-complexity\" doesn't mean \"is a monolithic, irreducibly complex mess\". In particular, it doesn't rule out this property:\n\n> *   **\"Well-structured\":** has an organized top-down hierarchical structure, learning which lets you quickly navigate to specific information in it.\n\nWikipedia has pretty high K-complexity, well beyond the ability of the human mind to hold in its working memory. But it's still usable, because you're not trying to cram all of it into your brain at once. Its structure is navigable, and you only retrieve the information you want.\n\nSimilarly, the world's complexity is high, but it seems *decomposable*, into small modules that could be understood separately and navigated to locate specific knowledge.",
      "plaintextDescription": "Yep, I know of this result. I haven't looked into it in depth, but my understanding is that it only says that powerful predictors have to be \"complex\" in the sense of high Kolmogorov complexity, right? But \"high K-complexity\" doesn't mean \"is a monolithic, irreducibly complex mess\". In particular, it doesn't rule out this property:\n\n>  * \"Well-structured\": has an organized top-down hierarchical structure, learning which lets you quickly navigate to specific information in it.\n\nWikipedia has pretty high K-complexity, well beyond the ability of the human mind to hold in its working memory. But it's still usable, because you're not trying to cram all of it into your brain at once. Its structure is navigable, and you only retrieve the information you want.\n\nSimilarly, the world's complexity is high, but it seems decomposable, into small modules that could be understood separately and navigated to locate specific knowledge."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-24T17:16:52.003Z",
    "af": false
  },
  {
    "_id": "fbDAh8ij5LNgdfDNa",
    "postId": "xvCNjLL3GZ6w2BWeb",
    "parentCommentId": "fhjaXvtzAHyxqWd5A",
    "topLevelCommentId": "fhjaXvtzAHyxqWd5A",
    "contents": {
      "markdown": "Yup, John pointed me that way too.",
      "plaintextDescription": "Yup, John pointed me that way too."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-24T10:51:25.760Z",
    "af": false
  },
  {
    "_id": "BfwjD5pEkPiDMM8sH",
    "postId": "LngR93YwiEpJ3kiWh",
    "parentCommentId": "B5sXz85XugoptCSQ7",
    "topLevelCommentId": "89JcBqrJKEXdLsXer",
    "contents": {
      "markdown": "> just noting the minefield that is unfortunately next to the park where we're having this conversation\n\nFor what it's worth, I'm painfully aware of [all the skulls lying around](https://slatestarcodex.com/2017/04/07/yes-we-have-noticed-the-skulls/), yep.",
      "plaintextDescription": "> just noting the minefield that is unfortunately next to the park where we're having this conversation\n\nFor what it's worth, I'm painfully aware of all the skulls lying around, yep."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-24T00:27:02.604Z",
    "af": false
  },
  {
    "_id": "tPht8sazrAwsqWDJE",
    "postId": "LngR93YwiEpJ3kiWh",
    "parentCommentId": "MhbRswauxdin7v6zW",
    "topLevelCommentId": "Lju93gNs7AC2qR4xw",
    "contents": {
      "markdown": "Nice, that's the sort of poking-of-holes I was looking for.\n\n> your alpha over the program induction community (or for instance even the Cyc project) is not legible (to me)\n\nThat's a good thinking prompt. What is the full set of reasons I'm optimistic about this, in legible terms?\n\n... Hm, but perhaps outlining what you think your edge is in public is not a great idea. I'll answer in PMs tomorrow. (To ensure future readers get some information about how convincing my reasons are, though, it'd be neat if you posted your impressions afterwards as a response to this comment.)",
      "plaintextDescription": "Nice, that's the sort of poking-of-holes I was looking for.\n\n> your alpha over the program induction community (or for instance even the Cyc project) is not legible (to me)\n\nThat's a good thinking prompt. What is the full set of reasons I'm optimistic about this, in legible terms?\n\n... Hm, but perhaps outlining what you think your edge is in public is not a great idea. I'll answer in PMs tomorrow. (To ensure future readers get some information about how convincing my reasons are, though, it'd be neat if you posted your impressions afterwards as a response to this comment.)"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-24T00:22:54.507Z",
    "af": false
  },
  {
    "_id": "HmCYra3sFseD5ApfB",
    "postId": "LngR93YwiEpJ3kiWh",
    "parentCommentId": "8PHCcSm7cm5juHQfy",
    "topLevelCommentId": "89JcBqrJKEXdLsXer",
    "contents": {
      "markdown": "\"Share the dual-use stuff only with specific people who are known to properly understand the AGI risk, can avoid babbling about it in public, and would be useful contributors\" seems like the straightforward approach here.\n\nLike, groups of people are able to maintain commercial secrets. This is kind of not unlike that, except with somewhat higher stakes.",
      "plaintextDescription": "\"Share the dual-use stuff only with specific people who are known to properly understand the AGI risk, can avoid babbling about it in public, and would be useful contributors\" seems like the straightforward approach here.\n\nLike, groups of people are able to maintain commercial secrets. This is kind of not unlike that, except with somewhat higher stakes."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T23:43:54.211Z",
    "af": false
  },
  {
    "_id": "syM2FoWqGKt7fazmK",
    "postId": "Xp9ie6pEWFT8Nnhka",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "(*Accelerando* spoilers.)\n\n> (It's been awhile since I read it, I vaguely recall some in-universe reasons that it worked out with less grabbiness, but they were not reasons I expect to generalize to our world).\n\nIIRC, it was because moving away from the high-bandwidth hive of activity in the Matrioshka brain was never profitable for any individual agent participating in Economy 2.0, since it was running at incredibly high speeds and changing all the time. Exiting it even for a brief period was ~suicidal, and expending resources to send some probe to eat a distant star system was never positive-utility, because by the time the probe reached that star system, the entire economic system will have changed a billion times over. It's exemplified by the the main characters' own interstellar expedition.\n\nBasically, the Crab Bucket Ascendant.\n\nClosest quote I've found:\n\n> Conscious civilizations sooner or later convert all their available mass into computronium, powered by solar output. They don't go interstellar because they want to stay near the core where the bandwidth is high and latency is low, and sooner or later, competition for resources hatches a new level of metacompetition that obsoletes them.\n\nI also don't buy it, by the way. Even if all advanced agents in the universe are necessarily myopic, no exceptions, the value from claiming distant resources could be propagated to the present by, like, some kind of chain of financial derivatives.[^wcedfgxgw9f]\n\n[^wcedfgxgw9f]: I. e., a contract \\(C_1\\) whose value depends on the contract \\(C_2\\) that will exist slightly in the future relative to \\(C_1\\), whose value depends on the contract \\(C_3\\) that will exist slightly in the future relative to \\(C_2\\), whose value depends on ... the contract \\(C_n\\), whose value depends on having claim to the distant resource. The \"slightly in the future\" parameter can then be set as low as necessary to fit within your myopic agent's \"value horizon\".(Hm, this is another reason \"just make the AI myopic\" doesn't help with the alignment problem, isn't it.)",
      "plaintextDescription": "(Accelerando spoilers.)\n\n> (It's been awhile since I read it, I vaguely recall some in-universe reasons that it worked out with less grabbiness, but they were not reasons I expect to generalize to our world).\n\nIIRC, it was because moving away from the high-bandwidth hive of activity in the Matrioshka brain was never profitable for any individual agent participating in Economy 2.0, since it was running at incredibly high speeds and changing all the time. Exiting it even for a brief period was ~suicidal, and expending resources to send some probe to eat a distant star system was never positive-utility, because by the time the probe reached that star system, the entire economic system will have changed a billion times over. It's exemplified by the the main characters' own interstellar expedition.\n\nBasically, the Crab Bucket Ascendant.\n\nClosest quote I've found:\n\n> Conscious civilizations sooner or later convert all their available mass into computronium, powered by solar output. They don't go interstellar because they want to stay near the core where the bandwidth is high and latency is low, and sooner or later, competition for resources hatches a new level of metacompetition that obsoletes them.\n\nI also don't buy it, by the way. Even if all advanced agents in the universe are necessarily myopic, no exceptions, the value from claiming distant resources could be propagated to the present by, like, some kind of chain of financial derivatives.[1]\n\n 1. ^\n    I. e., a contract C1 whose value depends on the contract C2 that will exist slightly in the future relative to C1, whose value depends on the contract C3 that will exist slightly in the future relative to C2, whose value depends on ... the contract Cn, whose value depends on having claim to the distant resource. The \"slightly in the future\" parameter can then be set as low as necessary to fit within your myopic agent's \"value horizon\".\n    \n    (Hm, this is another reason \"just make the AI myopic\" doesn't help with the"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-23T23:36:02.299Z",
    "af": false
  },
  {
    "_id": "QhBED4BxmT3xCr5NZ",
    "postId": "LngR93YwiEpJ3kiWh",
    "parentCommentId": "89JcBqrJKEXdLsXer",
    "topLevelCommentId": "89JcBqrJKEXdLsXer",
    "contents": {
      "markdown": "> Hey, I really like the ideas you're putting down here\n\nThanks!\n\n> So the IR can be a dense high level representation so long as it can be decoded by some system into human readable or viewable symbols/data\n\nIf I understand correctly, that's the \"symbolic enough\" case from footnote 2:\n\n> if we could generate an external world-model that's *as* understandable to us as our own world-models (and we are confident that this understanding is accurate), that should suffice for fulfilling the \"interpretability\" criterion.\n\nWe also don't have full interpretability into our abstractions down to the neurons, after all.\n\nI don't think it'd be necessary per se, though. I think if we can get it to produce an explanation like this, we can then just iterate to \"explain the explanation\", et cetera, until everything's been reduced to symbolics. Or it can be achieved by turning some other \"crank\" controlling the \"explanation fidelity\".\n\nBut yeah, \"symbolic-enough\" may be satisfactory.\n\n> My main thought/caution against this proposal, however, would be that this agenda requires moving the capabilities needle forward for supervised/self-supervised learning\n\nYep. As I'd briefly mentioned, the actual gears-level sketches of \"sufficiently powerful compression algorithms\" are obviously dual-use, and shouldn't be openly published.",
      "plaintextDescription": "> Hey, I really like the ideas you're putting down here\n\nThanks!\n\n> So the IR can be a dense high level representation so long as it can be decoded by some system into human readable or viewable symbols/data\n\nIf I understand correctly, that's the \"symbolic enough\" case from footnote 2:\n\n> if we could generate an external world-model that's as understandable to us as our own world-models (and we are confident that this understanding is accurate), that should suffice for fulfilling the \"interpretability\" criterion.\n\nWe also don't have full interpretability into our abstractions down to the neurons, after all.\n\nI don't think it'd be necessary per se, though. I think if we can get it to produce an explanation like this, we can then just iterate to \"explain the explanation\", et cetera, until everything's been reduced to symbolics. Or it can be achieved by turning some other \"crank\" controlling the \"explanation fidelity\".\n\nBut yeah, \"symbolic-enough\" may be satisfactory.\n\n> My main thought/caution against this proposal, however, would be that this agenda requires moving the capabilities needle forward for supervised/self-supervised learning\n\nYep. As I'd briefly mentioned, the actual gears-level sketches of \"sufficiently powerful compression algorithms\" are obviously dual-use, and shouldn't be openly published."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T22:40:39.825Z",
    "af": false
  },
  {
    "_id": "KnPnEoRgfeHdBNzxT",
    "postId": "LngR93YwiEpJ3kiWh",
    "parentCommentId": "ayx9HjM7DqfrQh84F",
    "topLevelCommentId": "yKxcWNqsdE83itFuK",
    "contents": {
      "markdown": "> It also bears remembering that any simulation we build of reality is designed to fit a specific set of recorded observations, where agentic selection effects may skew data quite significantly in various places.\n\nYup. I've been idly considering some sort of generator of synthetic data designed to produce training sets which we could mix into real data to provably obscure such signals.[^8z39g5e9x0n] It is maybe sort of doable for math, but probably not for physics/biology. (I commend your paranoia here, by the way.)\n\nOverall, though, getting into this sort of fight with potential misaligned superintelligent agents isn't a great idea; their possibility should be crushed somewhere upstream of that point.\n\n> The existence of the simulation must have an influence in this world, since it would otherwise be pointless, and they can't be drawing their insights from a simulation of their own since otherwise you lose interpretability in infinite recursion-wells, so the simulation must necessarily be disanalogous to here in at least one key way.\n\nMm-hm. My go-to heuristic here is to ask: how do *human* world-models handle this type of failure mode? Suppose we're trying to model someone who gets access to a compute-unbounded oracle, asks it about the future, then takes some actions that depend on the answer, thereby creating a stable time loop. Suppose we care about accuracy, but we don't have the unbounded compute to actually run this. We have to approximate.\n\nIs modeling it as a sequence of nested simulations which terminates at some ground-floor simulation that *doesn't* contain an oracle actually an accurate, faithful way to model this? Clearly no: the process we're modeling explicitly doesn't have any layers like this. The type signature of our \"approximation\" doesn't match the original even approximately; it's some [completely unrelated thing](https://www.lesswrong.com/posts/NQfeEd5LWohauKpTj/the-system-you-deploy-is-not-the-system-you-design).\n\nInstead, we attempt to *abstract over* the whole process, and skip directly to the prediction of what happens *in the limit of* infinite nesting, without simulating anything. Same way we compute \\\\(f(x)=x-\\frac{x^3}{3!}+\\frac{x^5}{5!}-\\frac{x^7}{7!}+... \\\\) not by actually calculating all those terms, but by collapsing the infinite power series to \\\\(\\sin(x)\\\\).\n\nOne thing to note here is that the kind of world-model I'm aiming for is not a \"brute-force\" simulator. It's explicitly using abstractions as basic units, and it should be able to \"skip to the limit\" in the above manner. So, for a large class of situations, the potential simulacrum escapees would not be *actually simulated* at the level of gears; just abstractly approximated.\n\n(Somewhat relevant: [this discussion](https://www.lesswrong.com/posts/Xmqnuq5eo9inywS7T/am-i-confused-about-the-malign-universal-prior-argument?commentId=vb7XmjCiLzsjyJjh9) about brute-force-y \"gormless\" simulators that get scammed by acausal terrorists, vs. more sophisticated simulators. I'm aiming for the more sophisticated kind. This should also take care of said acausal attacks.)\n\n> It's a tree falling in a forest and is probably not a concern, but it's also unlikely that anyone would build a system they definitionally cannot use for anything.\n\nAlright, suppose the agent inside the box is tasked with producing some [proofs of work](https://en.wikipedia.org/wiki/Proof_of_work). They are automatically verified, type-checked, then shown on a small, low-tech digital display on the box. The software is mathematically verified to be correct/unhackable, the hardware is robustly designed not to allow any sequence of computation to cause a malfunction that would move the software into theoretically impossible states, and the display can only show letters/numbers.\n\nThe only thing the agent can do is providing or not providing proofs of work; it can't hack out or show any message which is not a valid proof of work. \n\nThis way, you can extract useful work from the agent, while still not letting the type signature of the box become \"an agent\". It is just a PoW-generator.\n\nSimilarly, a simulation whose security trumps the capabilities of the agents it simulates can remain an \"inert\" simulation which just happens to house agents, rather than becoming an agent of its own.\n\n[^8z39g5e9x0n]: Though the actual motivation there was figuring out if we can obscure information about our universe's actual physics, biology, and human cognition, so that if something in the synthesis process goes wrong and it spawns misaligned agents, they're less likely to be able to construct reliable Basilisk hacks. (Because if that failure mode is allowed, we can't actually use the interpretability property to verify the synthesized world-model's safety prior to running it.)",
      "plaintextDescription": "> It also bears remembering that any simulation we build of reality is designed to fit a specific set of recorded observations, where agentic selection effects may skew data quite significantly in various places.\n\nYup. I've been idly considering some sort of generator of synthetic data designed to produce training sets which we could mix into real data to provably obscure such signals.[1] It is maybe sort of doable for math, but probably not for physics/biology. (I commend your paranoia here, by the way.)\n\nOverall, though, getting into this sort of fight with potential misaligned superintelligent agents isn't a great idea; their possibility should be crushed somewhere upstream of that point.\n\n> The existence of the simulation must have an influence in this world, since it would otherwise be pointless, and they can't be drawing their insights from a simulation of their own since otherwise you lose interpretability in infinite recursion-wells, so the simulation must necessarily be disanalogous to here in at least one key way.\n\nMm-hm. My go-to heuristic here is to ask: how do human world-models handle this type of failure mode? Suppose we're trying to model someone who gets access to a compute-unbounded oracle, asks it about the future, then takes some actions that depend on the answer, thereby creating a stable time loop. Suppose we care about accuracy, but we don't have the unbounded compute to actually run this. We have to approximate.\n\nIs modeling it as a sequence of nested simulations which terminates at some ground-floor simulation that doesn't contain an oracle actually an accurate, faithful way to model this? Clearly no: the process we're modeling explicitly doesn't have any layers like this. The type signature of our \"approximation\" doesn't match the original even approximately; it's some completely unrelated thing.\n\nInstead, we attempt to abstract over the whole process, and skip directly to the prediction of what happens in the limit of infinite nesting, wit"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T22:36:29.083Z",
    "af": false
  },
  {
    "_id": "B9bqNrp2zFwKHn6pW",
    "postId": "anFrGMskALuH7aZDw",
    "parentCommentId": "TaS8AteTj3fk9v9bf",
    "topLevelCommentId": "JDp8WxRHhHGu64ibK",
    "contents": {
      "markdown": "> Are most of the persnickety internal-disagreers actually signalling that they intend to promote the book, or at least not downplay its thesis?\n\n[One of](https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=YqkSshygyQ4vdP2sk) the persnickety internal disagreers here. I have recommended *IABIED* to those of my acquaintances who I expect may read it. I don't really have any other platform to shout about it from, but if I did, I would've certainly used it to promote the book, leaving all nitpicking out of it.\n\nI, at least, do explicitly make a distinction between \"a place for persnickety internal discussion\" and \"the public-facing platform\", and would behave differently between the two.",
      "plaintextDescription": "> Are most of the persnickety internal-disagreers actually signalling that they intend to promote the book, or at least not downplay its thesis?\n\nOne of the persnickety internal disagreers here. I have recommended IABIED to those of my acquaintances who I expect may read it. I don't really have any other platform to shout about it from, but if I did, I would've certainly used it to promote the book, leaving all nitpicking out of it.\n\nI, at least, do explicitly make a distinction between \"a place for persnickety internal discussion\" and \"the public-facing platform\", and would behave differently between the two."
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-23T18:29:54.204Z",
    "af": false
  },
  {
    "_id": "A9kdJMRuKyCYeyY4u",
    "postId": "mMEbfooQzMwJERAJJ",
    "parentCommentId": "m7NavwqZPQFFfZEYy",
    "topLevelCommentId": "raneS2bpuyKY8DG32",
    "contents": {
      "markdown": "> Could you say more about why this attempt to solve the problem (by a hierarchy of abstractions) doesn't work?\n\nFinally published [a post](https://www.lesswrong.com/posts/xvCNjLL3GZ6w2BWeb/synthesizing-standalone-world-models-part-1-abstraction) outlining my current model of all of this; see section 1.5 specifically.",
      "plaintextDescription": "> Could you say more about why this attempt to solve the problem (by a hierarchy of abstractions) doesn't work?\n\nFinally published a post outlining my current model of all of this; see section 1.5 specifically."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-23T17:26:24.663Z",
    "af": false
  },
  {
    "_id": "RDmTyr8KxZoJoF9kz",
    "postId": "LngR93YwiEpJ3kiWh",
    "parentCommentId": "Lju93gNs7AC2qR4xw",
    "topLevelCommentId": "Lju93gNs7AC2qR4xw",
    "contents": {
      "markdown": "> This idea seems to require (basically) a major revolution in or even a complete solution to program induction\n\nEh, I think any nontrivial technical project can be made to sound like an incredibly significant and therefore dauntingly impossible achievement, if you pick the right field to view it from. But what matters is the *actual* approach you're using, and how challenging the technical problems are from the perspective of the *easiest* field in which they could be represented.\n\nSome examples:\n\n*   Consider various geometry problems, e. g., one of [those](https://org.coloradomesa.edu/~mapierce2/putnam/problems/synthetic-geometry.html). If you use the tools of analytic geometry, you'd end up having to solve a complicated system of nonlinear equations. If you use synthetic geometry instead, the way to resolve them might consist of applying a well-known theorem and a few simple reasoning steps, so simple you can do it in your head.\n*   Consider the problem of moving fast. Before the invention of the car, the problem of moving at 120 km/h could've been cast as \"a major revolution in horse-breeding and genetic engineering\". But the actual *approach* taken did not route through horses or biology at all. It achieved the end result through a different pathway, in which the technical problems were dramatically easier.\n*   Consider AI. Prior to Deep Learning, there was a throve of symbolic approaches to it; and even before that, hand-written GOFAIs. The technical problem of \"achieve DL-level performance using symbolic/GOFAI tools\" is dramatically harder than \"achieve DL-level performance\", unqualified. And yet, the latter can be technically described as a revolution in the relevant fields.\n*   Consider various other modeling problems, e. g., weather prediction, volcano modeling, materials-science modeling, quantitative trading. *Any* advancement in general modeling techniques would revolutionize all of those. But should that technical problem really be framed in the daunting terms of \"come up with a revolutionary stock-trading algorithm\"?\n\nTo generalize: Suppose there's some field A which is optimizing for X. Improving on X using the tools of A would necessarily require you to beat a market that is efficient-relative-to-you. Experts in A already know the tools of A in and out, and how to use them to maximize X. Even if you can beat them, it would only be an incremental improvement. A slightly better solver for systems of nonlinear equations, a slightly faster horse, a slightly better trading algorithm.\n\nThe way to *actually* massively improve on X is to ignore the extant tools of A entirely, and try to develop new tools for optimizing X by using some other field B. On the outside view, this is necessarily a high-risk proposition, since B might end up entirely unhelpful; but it's also high-reward, since it might allow you to actually \"beat the market\". And if you succeed, the actual technical problems you'll end up solving will be massively easier than the problems you'd need to solve to achieve the same performance using A's tools.\n\nBringing it back around: This agenda may or may not be viewed as aiming to revolutionize program induction, but I'm not setting out to take the extant program-induction tools and try to cobble together something revolutionary using them. The idea is to use an entirely different line of theory (agent foundations, natural abstractions, information theory, recent DL advances) to achieve that end result.",
      "plaintextDescription": "> This idea seems to require (basically) a major revolution in or even a complete solution to program induction\n\nEh, I think any nontrivial technical project can be made to sound like an incredibly significant and therefore dauntingly impossible achievement, if you pick the right field to view it from. But what matters is the actual approach you're using, and how challenging the technical problems are from the perspective of the easiest field in which they could be represented.\n\nSome examples:\n\n * Consider various geometry problems, e. g., one of those. If you use the tools of analytic geometry, you'd end up having to solve a complicated system of nonlinear equations. If you use synthetic geometry instead, the way to resolve them might consist of applying a well-known theorem and a few simple reasoning steps, so simple you can do it in your head.\n * Consider the problem of moving fast. Before the invention of the car, the problem of moving at 120 km/h could've been cast as \"a major revolution in horse-breeding and genetic engineering\". But the actual approach taken did not route through horses or biology at all. It achieved the end result through a different pathway, in which the technical problems were dramatically easier.\n * Consider AI. Prior to Deep Learning, there was a throve of symbolic approaches to it; and even before that, hand-written GOFAIs. The technical problem of \"achieve DL-level performance using symbolic/GOFAI tools\" is dramatically harder than \"achieve DL-level performance\", unqualified. And yet, the latter can be technically described as a revolution in the relevant fields.\n * Consider various other modeling problems, e. g., weather prediction, volcano modeling, materials-science modeling, quantitative trading. Any advancement in general modeling techniques would revolutionize all of those. But should that technical problem really be framed in the daunting terms of \"come up with a revolutionary stock-trading algorithm\"?\n\nTo generalize: Suppose th"
    },
    "baseScore": 14,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-23T11:51:46.870Z",
    "af": false
  },
  {
    "_id": "3yeANP5zdd95gK9GW",
    "postId": "LngR93YwiEpJ3kiWh",
    "parentCommentId": "yKxcWNqsdE83itFuK",
    "topLevelCommentId": "yKxcWNqsdE83itFuK",
    "contents": {
      "markdown": "\"Simulacrum escapees\" are explicitly one of the main failure modes we'll need to address, yes. Some thoughts:\n\n*   The obvious way to avoid them is to not point the wm-synthesizer at a dataset containing agents.\n    *   If we're aiming to develop intelligence-enhancing medical interventions or the technology for uploading, we don't necessarily need a world-model containing agents: a sufficiently advanced model/simulator of biology/physics would suffice.\n    *   Similarly, if we want a superintelligent proof synthesizer we can use to do a babble-and-prune search through the space of possible agent-foundations theorems,[^27pg1ef9d1y] we only need to make it good at math-in-general, not at intuitive reasoning about agent-containing math.\n        *   This is riskier than biology/physics, though, because perhaps reasoning even about fully formal agent-foundations math would require reasoning about agents intuitively, i. e., instantiating them in internal simulation spaces.\n*   Intuitively, \"a simulated agent breaks out of the simulation\" is a *capability-laden* failure of the wm-synthesize. It does not function how it ought to, it is not succeeding at producing an accurate world-model. It should be possible to make it powerful enough to avoid that.\n    *   Note how, in a sense, \"an agent recognizes it's in a simulation and hacks out\" is just an instance of the more general failure mode of \"part of the world is being modeled incorrectly\" (by e. g. having some flaws the simulated agent recognizes, or by allowing it to break out of the sandbox). To work, the process would need to be able to recognize and address those failure modes. If it's sufficiently powerful, whatever subroutines it uses to handle lesser \"bugs\" should generalize to handling this type of bug as well.\n*   With more insights into how agents work, we might be able to come up with more targeted interventions/constraints/regularization techniques for preventing simulacrum escapees. E. g., if we figure out the proper \"type signature\" of agents, we might be able to explicitly ban the wm-synthesizer from incorporating them in the world-model.\n\nThis is a challenge, but one I'm optimistic about handling.\n\n> Weeping Agents: Anything that holds the image of an agent becomes an agent\n\nNice framing! But I somewhat dispute that. Consider a perfectly boxed-in AI, running on a computer with no output channels whatsoever (or perhaps as [a homomorphic computation](https://en.wikipedia.org/wiki/Homomorphic_encryption), i. e., indistinguishable from noise without the key). This thing holds the image of an agent; but is it really \"an agent\" from the perspective of anyone outside that system?\n\nSimilarly, a sufficiently good world-model would sandbox the modeled agents well enough that it wouldn't, itself, engage in an agent-like behavior from the perspective of its operators.\n\n[^27pg1ef9d1y]: As in: we come up with a possible formalization of some aspect of agent foundations, then babble potential theorems about it at the proof synthesizer, and it provides proofs/disproofs. This is a pretty brute approach and is by no means a full solution, but I expect it can nontrivially speed us up.",
      "plaintextDescription": "\"Simulacrum escapees\" are explicitly one of the main failure modes we'll need to address, yes. Some thoughts:\n\n * The obvious way to avoid them is to not point the wm-synthesizer at a dataset containing agents.\n   * If we're aiming to develop intelligence-enhancing medical interventions or the technology for uploading, we don't necessarily need a world-model containing agents: a sufficiently advanced model/simulator of biology/physics would suffice.\n   * Similarly, if we want a superintelligent proof synthesizer we can use to do a babble-and-prune search through the space of possible agent-foundations theorems,[1] we only need to make it good at math-in-general, not at intuitive reasoning about agent-containing math.\n     * This is riskier than biology/physics, though, because perhaps reasoning even about fully formal agent-foundations math would require reasoning about agents intuitively, i. e., instantiating them in internal simulation spaces.\n * Intuitively, \"a simulated agent breaks out of the simulation\" is a capability-laden failure of the wm-synthesize. It does not function how it ought to, it is not succeeding at producing an accurate world-model. It should be possible to make it powerful enough to avoid that.\n   * Note how, in a sense, \"an agent recognizes it's in a simulation and hacks out\" is just an instance of the more general failure mode of \"part of the world is being modeled incorrectly\" (by e. g. having some flaws the simulated agent recognizes, or by allowing it to break out of the sandbox). To work, the process would need to be able to recognize and address those failure modes. If it's sufficiently powerful, whatever subroutines it uses to handle lesser \"bugs\" should generalize to handling this type of bug as well.\n * With more insights into how agents work, we might be able to come up with more targeted interventions/constraints/regularization techniques for preventing simulacrum escapees. E. g., if we figure out the proper \"type signature\" of ag"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T11:15:19.008Z",
    "af": false
  },
  {
    "_id": "7QvK5xk2NMA6b8mz3",
    "postId": "anFrGMskALuH7aZDw",
    "parentCommentId": "JDp8WxRHhHGu64ibK",
    "topLevelCommentId": "JDp8WxRHhHGu64ibK",
    "contents": {
      "markdown": "> I think that (metaphorically) there should be an all-caps disclaimer that reads something like \"TO BE CLEAR AI IS STILL ON TRACK TO KILL EVERYONE YOU LOVE; YOU SHOULD BE ALARMED ABOUT THIS AND TELLING PEOPLE IN NO UNCERTAIN TERMS THAT YOU HAVE FAR, FAR MORE IN COMMON WITH YUDKOWSKY AND SOARES THAN YOU DO WITH THE LOBBYISTS OF META, WHO ABSENT COORDINATION BY PEOPLE ON HUMANITY'S SIDE ARE LIABLE TO WIN THIS FIGHT, SO COORDINATE WE MUST\" every couple of paragraphs.\n\nYeah, I kind of regret not prefacing [my pseudo-review](https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=YqkSshygyQ4vdP2sk) with something like this. I was generally writing it from the mindset of \"obviously the book is entirely correct and I'm only reviewing the presentation\", and my assumption was that trying to \"sell it\" to LW users was preaching to the choir (I would've strongly endorsed it if I had a big mainstream audience, or even if I were making a top-level LW post). But that does feel like part of the our-kind-can't-cooperate pattern now.",
      "plaintextDescription": "> I think that (metaphorically) there should be an all-caps disclaimer that reads something like \"TO BE CLEAR AI IS STILL ON TRACK TO KILL EVERYONE YOU LOVE; YOU SHOULD BE ALARMED ABOUT THIS AND TELLING PEOPLE IN NO UNCERTAIN TERMS THAT YOU HAVE FAR, FAR MORE IN COMMON WITH YUDKOWSKY AND SOARES THAN YOU DO WITH THE LOBBYISTS OF META, WHO ABSENT COORDINATION BY PEOPLE ON HUMANITY'S SIDE ARE LIABLE TO WIN THIS FIGHT, SO COORDINATE WE MUST\" every couple of paragraphs.\n\nYeah, I kind of regret not prefacing my pseudo-review with something like this. I was generally writing it from the mindset of \"obviously the book is entirely correct and I'm only reviewing the presentation\", and my assumption was that trying to \"sell it\" to LW users was preaching to the choir (I would've strongly endorsed it if I had a big mainstream audience, or even if I were making a top-level LW post). But that does feel like part of the our-kind-can't-cooperate pattern now."
    },
    "baseScore": 15,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-22T08:03:38.178Z",
    "af": false
  },
  {
    "_id": "3WwBkNqhSHxwboSpw",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "XCx4ZwmWRdekhaLPT",
    "topLevelCommentId": "5Zo9vifvjFp9yGJEZ",
    "contents": {
      "markdown": "My guess is that they're doing the motte-and-bailey of \"make it seem to people who haven't read the book that it says that the ASI extinction is inevitable, that the book is just spreading doom and gloom\", from which, if challenged, they could retreat to \"no, I meant doom isn't inevitable even if we do build ASI using the current methods\".\n\nLike, if someone means the latter (and has also read the book and knows that it goes to great lengths to clarify that we *can* avoid extinction), would they really phrase it as \"doom is inevitable\", as opposed to e. g. \"safe ASI is impossible\"?\n\nOr maybe they haven't put that much thought into it and are just sloppy with language.",
      "plaintextDescription": "My guess is that they're doing the motte-and-bailey of \"make it seem to people who haven't read the book that it says that the ASI extinction is inevitable, that the book is just spreading doom and gloom\", from which, if challenged, they could retreat to \"no, I meant doom isn't inevitable even if we do build ASI using the current methods\".\n\nLike, if someone means the latter (and has also read the book and knows that it goes to great lengths to clarify that we can avoid extinction), would they really phrase it as \"doom is inevitable\", as opposed to e. g. \"safe ASI is impossible\"?\n\nOr maybe they haven't put that much thought into it and are just sloppy with language."
    },
    "baseScore": 10,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-09-22T05:55:49.382Z",
    "af": false
  },
  {
    "_id": "pTqjkcbsHajeLydw8",
    "postId": "viLu9uFcMFtJHgRRm",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I particularly agree with the point about the style being much more science-y than I'd expected, in a way that surely filters out large swathes of people. I'm assuming \"people who are completely clueless about science and are unable to follow technical arguments\" are just not the target audience. To crudely oversimplify, I think the target audience is 120+ IQ people, not 100 IQ people.\n\n> I mention this for transparency but also because some seem to be rallying around IABIED, even with its shortcomings, because they don’t think there is another option\n\nI think *IABIED* should be rallied around because \"the MIRI book\" is the obvious Schelling point for rallying around. It has brand recognition in our circles, its release is a big visible event, it managed to get into best-seller categories meaning it's visible to the mainstream audiences, etc. Even if there are other books which are moderately better at doing what *IABIED* does, it wouldn't be possible to amplify their impact the same way (even if, say, Eliezer personally recommended them), so *IABIED* it is.\n\nFurther, even if it's possible to coordinate around and boost a different book the same way, this would require additional time; months or years (if that better book is yet to be written). We don't have much of that luxury, in expectation.\n\nThis still wouldn't be a good idea if *IABIED* were *actively bad*, of course. But it's not. I think it's reasonably good, even if we have our quibbles; and MIRI's pre-release work shows that it seems convincing to non-experts.\n\nWe could think about crafting better persuasion-artefacts in the future, but I think rallying around *IABIED* *is* the only option, at this point in time. And it may or may not be a marginally worse option compared to some hypothetical alternatives, but it's not a bad option.",
      "plaintextDescription": "I particularly agree with the point about the style being much more science-y than I'd expected, in a way that surely filters out large swathes of people. I'm assuming \"people who are completely clueless about science and are unable to follow technical arguments\" are just not the target audience. To crudely oversimplify, I think the target audience is 120+ IQ people, not 100 IQ people.\n\n> I mention this for transparency but also because some seem to be rallying around IABIED, even with its shortcomings, because they don’t think there is another option\n\nI think IABIED should be rallied around because \"the MIRI book\" is the obvious Schelling point for rallying around. It has brand recognition in our circles, its release is a big visible event, it managed to get into best-seller categories meaning it's visible to the mainstream audiences, etc. Even if there are other books which are moderately better at doing what IABIED does, it wouldn't be possible to amplify their impact the same way (even if, say, Eliezer personally recommended them), so IABIED it is.\n\nFurther, even if it's possible to coordinate around and boost a different book the same way, this would require additional time; months or years (if that better book is yet to be written). We don't have much of that luxury, in expectation.\n\nThis still wouldn't be a good idea if IABIED were actively bad, of course. But it's not. I think it's reasonably good, even if we have our quibbles; and MIRI's pre-release work shows that it seems convincing to non-experts.\n\nWe could think about crafting better persuasion-artefacts in the future, but I think rallying around IABIED is the only option, at this point in time. And it may or may not be a marginally worse option compared to some hypothetical alternatives, but it's not a bad option."
    },
    "baseScore": 6,
    "voteCount": 16,
    "createdAt": null,
    "postedAt": "2025-09-19T04:22:05.625Z",
    "af": false
  },
  {
    "_id": "bSjrKdgbC4dT3bgL9",
    "postId": "iEkksmar6mtuQtdsR",
    "parentCommentId": "zDyhyJtP4rehwjKFJ",
    "topLevelCommentId": "zDyhyJtP4rehwjKFJ",
    "contents": {
      "markdown": "> However, in most of the experimental sciences, formal results are not the main bottleneck, so speed-ups would be more dependent on progress on coding, fuzzier tasks, robotics, and so on\n\nOne difficulty with predicting the impact of \"solving math\" on the world is [the Jevons effect](https://en.wikipedia.org/wiki/Jevons_paradox) (or a kind of generalization of it). If posing a problem formally becomes equivalent to solving it, it would have effects beyond just speeding up existing fully formal endeavors. It might potentially create *qualitatively* new industries/approaches relying on cranking out such solutions by the dozens.\n\nE. g., perhaps there are some industries which we already *can* fully formalize, but which still work in the applied-science regime, because building the thing and testing it empirically is cheaper than hiring a mathematician and waiting ten years. But once math is solved, you'd be able to effectively go through dozens of prototypes per day for, say, $1000, while previously, each one would've taken six months and $50,000.\n\nAre there such industries? What are they? I don't know, but I think there's a decent possibility that merely solving formal math would immediately make things go crazy.",
      "plaintextDescription": "> However, in most of the experimental sciences, formal results are not the main bottleneck, so speed-ups would be more dependent on progress on coding, fuzzier tasks, robotics, and so on\n\nOne difficulty with predicting the impact of \"solving math\" on the world is the Jevons effect (or a kind of generalization of it). If posing a problem formally becomes equivalent to solving it, it would have effects beyond just speeding up existing fully formal endeavors. It might potentially create qualitatively new industries/approaches relying on cranking out such solutions by the dozens.\n\nE. g., perhaps there are some industries which we already can fully formalize, but which still work in the applied-science regime, because building the thing and testing it empirically is cheaper than hiring a mathematician and waiting ten years. But once math is solved, you'd be able to effectively go through dozens of prototypes per day for, say, $1000, while previously, each one would've taken six months and $50,000.\n\nAre there such industries? What are they? I don't know, but I think there's a decent possibility that merely solving formal math would immediately make things go crazy."
    },
    "baseScore": 9,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-18T10:11:14.454Z",
    "af": false
  },
  {
    "_id": "pcdfnLLxqSt8Wjycc",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "9ejMnuGXRK2mfswY2",
    "topLevelCommentId": "YqkSshygyQ4vdP2sk",
    "contents": {
      "markdown": "I am somewhat interested now. I'll aim to look over it and get back to you, but no promises.",
      "plaintextDescription": "I am somewhat interested now. I'll aim to look over it and get back to you, but no promises."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-18T06:55:15.751Z",
    "af": false
  },
  {
    "_id": "nLJwF7smupAysZqgu",
    "postId": "WK979aX9KpfEMd9R9",
    "parentCommentId": "wtrd55HsQFJTiCP9h",
    "topLevelCommentId": "oMyHQG4maJB6DfdB8",
    "contents": {
      "markdown": "I still think there's a science to it which is yet to be properly written up. It's not at the level of \"this combination of design choices/clothing elements is bad, this one is good\", but there *is* a high-level organization to the related skills/principles, which can be taught to speed up someone learning design/fashion. They would still need to do a bunch of case studies/bottom-up learning afterwards (to learn specific extant patterns like \"the vibe of a 90s CS professor\"), but you can make that learning more sample-efficient.\n\nSocial skills are a good parallel. Actually talking to people and trying to accomplish different things with your conversations is necessary for developing social competence, but knowing some basics of the theory of mind and social dynamics is incredibly helpful for knowing what to pay attention to and try.",
      "plaintextDescription": "I still think there's a science to it which is yet to be properly written up. It's not at the level of \"this combination of design choices/clothing elements is bad, this one is good\", but there is a high-level organization to the related skills/principles, which can be taught to speed up someone learning design/fashion. They would still need to do a bunch of case studies/bottom-up learning afterwards (to learn specific extant patterns like \"the vibe of a 90s CS professor\"), but you can make that learning more sample-efficient.\n\nSocial skills are a good parallel. Actually talking to people and trying to accomplish different things with your conversations is necessary for developing social competence, but knowing some basics of the theory of mind and social dynamics is incredibly helpful for knowing what to pay attention to and try."
    },
    "baseScore": 14,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-09-18T06:34:47.865Z",
    "af": false
  },
  {
    "_id": "PdgMHjwShGkMCgHG6",
    "postId": "WK979aX9KpfEMd9R9",
    "parentCommentId": "oMyHQG4maJB6DfdB8",
    "topLevelCommentId": "oMyHQG4maJB6DfdB8",
    "contents": {
      "markdown": "I agree that a competent write-up on the (true) theory of fashion seems to be missing. The usual way to deal with such situations is to act like the neural network you are: find some big dataset of \\[clothing example, fashionability analysis\\] pairs, consume it, then reverse-engineer the intuitions you've learned. If there's no extant literature on the top-down theory available, go bottom-up and derive it yourself. (It *will* be time-consuming.)",
      "plaintextDescription": "I agree that a competent write-up on the (true) theory of fashion seems to be missing. The usual way to deal with such situations is to act like the neural network you are: find some big dataset of [clothing example, fashionability analysis] pairs, consume it, then reverse-engineer the intuitions you've learned. If there's no extant literature on the top-down theory available, go bottom-up and derive it yourself. (It will be time-consuming.)"
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-18T06:23:26.698Z",
    "af": false
  },
  {
    "_id": "NuEroFCssej4oRGnH",
    "postId": "WK979aX9KpfEMd9R9",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Of course, there are lots of other options besides literally just a leather jacket. As a general rule, any outfit which makes people ask “are you in a band?” signals coolness.\n\nThere are lots of options in the possibility-space. But are there lots of options on the actual market?\n\nFashion industry is one of those things that makes me want to just go [do it all myself](https://www.lesswrong.com/posts/Wg6ptgi2DupFuAnXG/orienting-toward-wizard-power) in frustration.[^osa953zbtwf] The clothing-space seems drastically underexplored.\n\nThe most obvious element is coloration. For any piece of clothing, there's a wide variety of complex-yet-tasteful multicolor patterns one might try. Very subtle highlights and gradients; unusual but simple geometric patterns; strong but carefully-chosen contrasts. You don't want a *literal* clash-of-colors clown outfit, but there's a wealth of possibilities beyond \"one simple color\"; e. g., mixing different hues of the same color.\n\nYet, most items on the market do just pick one simple color. Alternatively, they pick a common, basic (and therefore boring, conformist) pattern, e. g. plaid shirts. On the other end of the spectrum, you have graphic tees and such, which are varied but are decidedly unsubtle and, in my opinion, pretty lame (outside very specific combinations of design and social context[^r71g2tw5u]).\n\nYou can slightly deal with that by wearing several items of different colors that combine the way you want. But this only allows basic combinations, and the ability to do that becomes very constrained in hot weather.\n\nEagerly awaiting the point when AI advances enough for me to vibe-design and 3D print anything I can imagine.\n\n[^osa953zbtwf]: Also the bag industry. You'd think the wealthy community of digital nomads would've incentivized a thriving ecosystem of varied, competently designed modular bags, and yet. \n\n[^r71g2tw5u]: This is obviously peak fashion.",
      "plaintextDescription": "> Of course, there are lots of other options besides literally just a leather jacket. As a general rule, any outfit which makes people ask “are you in a band?” signals coolness.\n\nThere are lots of options in the possibility-space. But are there lots of options on the actual market?\n\nFashion industry is one of those things that makes me want to just go do it all myself in frustration.[1] The clothing-space seems drastically underexplored.\n\nThe most obvious element is coloration. For any piece of clothing, there's a wide variety of complex-yet-tasteful multicolor patterns one might try. Very subtle highlights and gradients; unusual but simple geometric patterns; strong but carefully-chosen contrasts. You don't want a literal clash-of-colors clown outfit, but there's a wealth of possibilities beyond \"one simple color\"; e. g., mixing different hues of the same color.\n\nYet, most items on the market do just pick one simple color. Alternatively, they pick a common, basic (and therefore boring, conformist) pattern, e. g. plaid shirts. On the other end of the spectrum, you have graphic tees and such, which are varied but are decidedly unsubtle and, in my opinion, pretty lame (outside very specific combinations of design and social context[2]).\n\nYou can slightly deal with that by wearing several items of different colors that combine the way you want. But this only allows basic combinations, and the ability to do that becomes very constrained in hot weather.\n\nEagerly awaiting the point when AI advances enough for me to vibe-design and 3D print anything I can imagine.\n\n 1. ^\n    Also the bag industry. You'd think the wealthy community of digital nomads would've incentivized a thriving ecosystem of varied, competently designed modular bags, and yet.\n\n 2. ^\n    This is obviously peak fashion."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-18T06:08:10.858Z",
    "af": false
  },
  {
    "_id": "6jYT4HjEoiXQWggSz",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "gMrTAqPzMXHSF833h",
    "topLevelCommentId": "YqkSshygyQ4vdP2sk",
    "contents": {
      "markdown": "> Have you happen to have read by beginner-friendly book about AI safety/risk \"Uncontrollable\"? \n\nNope.",
      "plaintextDescription": "> Have you happen to have read by beginner-friendly book about AI safety/risk \"Uncontrollable\"? \n\nNope."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-17T16:06:07.863Z",
    "af": false
  },
  {
    "_id": "7rTdiWRB9DGahvFN9",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "ppmoX6pz85AYuJdzJ",
    "topLevelCommentId": "YqkSshygyQ4vdP2sk",
    "contents": {
      "markdown": "Hopefully it's just my personal pet peeves, then!",
      "plaintextDescription": "Hopefully it's just my personal pet peeves, then!"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-17T04:44:23.185Z",
    "af": false
  },
  {
    "_id": "zganMKrWgan6f33cS",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "owLv83zdDGq8y86i5",
    "topLevelCommentId": "YqkSshygyQ4vdP2sk",
    "contents": {
      "markdown": "I liked that one as well, I think it does a good job at what it aims to do.\n\nMy worry is that \"improved and simplified\" may not be enough. As kave [pointed out](https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=SrShEDJcbybZ2dS3Y), it might be that we've already convinced everyone who could be convinced by arguments-shaped-like-this, and that we need to generate arguments shaped in a radically different way to convince new demographics, rather than slightly vary the existing shapes. (And it may be the case that it's very hard for people-like-us to generate arguments shaped different ways – I'm not quite sure what that'd look like, though I haven't thought about it much – so doing that would require something nontrivial from us, not just \"sit down and try to come up with new essays\".)\n\nMaybe that's wrong; maybe the issue was lack of reach rather than exhausting the persuadees' supply, and the book-packaging + timing will succeed massively. We'll see.",
      "plaintextDescription": "I liked that one as well, I think it does a good job at what it aims to do.\n\nMy worry is that \"improved and simplified\" may not be enough. As kave pointed out, it might be that we've already convinced everyone who could be convinced by arguments-shaped-like-this, and that we need to generate arguments shaped in a radically different way to convince new demographics, rather than slightly vary the existing shapes. (And it may be the case that it's very hard for people-like-us to generate arguments shaped different ways – I'm not quite sure what that'd look like, though I haven't thought about it much – so doing that would require something nontrivial from us, not just \"sit down and try to come up with new essays\".)\n\nMaybe that's wrong; maybe the issue was lack of reach rather than exhausting the persuadees' supply, and the book-packaging + timing will succeed massively. We'll see."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-17T03:57:59.232Z",
    "af": false
  },
  {
    "_id": "tfeBrtbSHKHxQZwzc",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "a4uxZ2LfDuDx7s9bJ",
    "topLevelCommentId": "YqkSshygyQ4vdP2sk",
    "contents": {
      "markdown": "Sure; but the following sections are meant as explanations/justifications of *why* that is the case. The paragraph I omitted does a good job of explaining why they would need to learn to predict the world at large, not just humans, and would therefore contain more than just human-mimicrky algorithms. To reinforce that with the point about reasoning models, one could perhaps explain how that \"generate sixteen CoTs, pick the best\" training can push LLMs to recruit those hidden algorithms for the purposes of steering rather than just prediction, or even to incrementally develop entirely new skills.\n\nA full explanation of reinforcement learning is probably not worth it (perhaps it was in the additional 200% of the book Eliezer wrote, but I agree it should've been aggressively pruned). But as-is, there are just clearly missing pieces here.",
      "plaintextDescription": "Sure; but the following sections are meant as explanations/justifications of why that is the case. The paragraph I omitted does a good job of explaining why they would need to learn to predict the world at large, not just humans, and would therefore contain more than just human-mimicrky algorithms. To reinforce that with the point about reasoning models, one could perhaps explain how that \"generate sixteen CoTs, pick the best\" training can push LLMs to recruit those hidden algorithms for the purposes of steering rather than just prediction, or even to incrementally develop entirely new skills.\n\nA full explanation of reinforcement learning is probably not worth it (perhaps it was in the additional 200% of the book Eliezer wrote, but I agree it should've been aggressively pruned). But as-is, there are just clearly missing pieces here."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-09-17T02:58:40.532Z",
    "af": false
  },
  {
    "_id": "RbzoJuWfRkwZMBnDC",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "SrShEDJcbybZ2dS3Y",
    "topLevelCommentId": "YqkSshygyQ4vdP2sk",
    "contents": {
      "markdown": "> I worry that I particularly enjoy the kind of writing they do, but we've already tapped the market of folks like me\n\nYup, hence my not being excited to see the usual rhetoric being rehearsed, instead of something novel.\n\n> The other thing I really liked: they would occassionally explain some science to expand on their point (nuclear physics is the example they expounded on at length, but IIRC they mentioned a bunch of other bit of science in passing)\n\nYup. Chapter 10 is my favorite.",
      "plaintextDescription": "> I worry that I particularly enjoy the kind of writing they do, but we've already tapped the market of folks like me\n\nYup, hence my not being excited to see the usual rhetoric being rehearsed, instead of something novel.\n\n> The other thing I really liked: they would occassionally explain some science to expand on their point (nuclear physics is the example they expounded on at length, but IIRC they mentioned a bunch of other bit of science in passing)\n\nYup. Chapter 10 is my favorite."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-16T19:58:47.119Z",
    "af": false
  },
  {
    "_id": "YqkSshygyQ4vdP2sk",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Just finished *If Anyone Builds It, Everyone Dies* (and some of the supplements).[^36t66j8ksdj] It feels... weaker than I'd hoped. Specifically, I think Part 3 is strong, and the supplemental materials are quite thorough, but Parts 1-2... I hope I'm wrong, and this opinion is counterweighed [by all these endorsements](https://www.lesswrong.com/posts/khmpWJnGJnuyPdipE/new-endorsements-for-if-anyone-builds-it-everyone-dies) and MIRI presumably running it by lots of test readers. But I'm more bearish on it making a huge impact than I was before reading it.\n\n**Point 1:** The rhetoric – the arguments and their presentations – is often not novel, just rehearsed variations on the arguments Eliezer/MIRI already deployed. This is not *necessarily* a problem, if those arguments were already shaped into their optimal form, and *I* do like this form... But I note those arguments have so far failed to go viral. Would repackaging them into a book, and deploying it in our post-ChatGPT present, be enough? Well, I hope so.\n\n**Point 2:** I found Chapter 2 in particular somewhat poorly written in how it explains the technical details.\n\nSpecifically, those explanations often occupy that unfortunate middle ground between \"informal gloss\" and \"correct technical description\" where I'd guess they're impenetrable *both* to non-technical readers and to technical readers unfamiliar with the subject matter.\n\nAn example that seems particularly egregious to me:\n\n> You might think that, because LLMs are grown without much understanding and trained only to predict human text, they cannot do anything except regurgitate human utterances. But that would be incorrect. \\[...\\]\n> \n> Furthermore, AIs nowadays are not trained only to predict human-generated text. An AI-grower might give their AI sixteen tries at solving a math problem, thinking aloud in words about how to solve it; then, the “chain-of-thought” for whichever of the sixteen tries went best would get further reinforced by gradient descent, yielding what’s called a reasoning model. That’s a sort of training that can push AIs to think thoughts no human could think.\n\nHow does that conclusion follow? If a base model can only regurgitate human utterances, how is generating sixteen utterances and then reinforcing some of them leads to it... *not* regurgitating human utterances? This explanation is clearly incomplete. My model of a nonexpert technical-minded reader, who is actually tracking the gears the book introduces, definitely notices that and is confused.\n\nExplanation of base models' training at the start of the chapter feels flawed in the same way. E. g.:\n\n> Then, they determine the architecture: the rules for how to combine their input (like the Once upon a ti sequence of 15 14 3…) with the weights in the parameters. Something like, “I’ll multiply each input-number with the weight in the first parameter, and then add it to the weight in the second parameter, and then I’ll replace it with zero if it’s negative, and then…” They pick a lot of operations like that—hundreds of billions, linking every single weight into the calculation.\n\nMy model of a technical-minded reader is confused about how that whole thing is supposed to work. It sounds like AI developers *manually* pick billions of operations? What? The technical reader would've rather you just mentioned operations on matrices. This might've required more effort to understand, but understanding would've been at all possible.\n\nMy model of a nontechnical reader just has their eyes glaze over when reading this description. It uses simple words, but it clearly gestures at some messy complicated thing, and doesn't actually [conceptualize it](https://www.lesswrong.com/posts/CZQYP7BBY4r9bdxtY/the-best-lay-argument-is-not-a-simple-english-yud-essay) in a simple-to-understand way. (\"It\" being \"the neural substrate\", and how it can both be unreadable to us yet encode useful computations.)\n\nAnd then this explanation is used to build the definition of gradient descent; and then this term is used all throughout the rest of the book to make arguments for various things. My guess is that this explanation is not sufficient to make readers feel like they grok the concept; on the contrary, it's likely to make them earmark the term as \"don't really get this one\". This would then, subtly or not, poison every argument where this term reoccurs.\n\nOr maybe I'm unfairly nitpicking. Again, I think MIRI ran it by many test readers, so presumably this actually does work fine in practice? But this is what my eyes are telling me.\n\n**Point 3:** Part 2, the fictional story. It's kind of... eh. The stated purpose is to help \"make abstract considerations feel more real\", but does it actually accomplish that? It's written in a pretty abstract way. Its narrative is mixed with technical discussion. It takes the AI's perspective, and doesn't view things from the world's perspective much, so it doesn't create a visceral sense of something *you* would *see* happening around you. It involves some honestly quite convoluted master-plan scenarios with multicancer pandemics.\n\nDoes the story actually serve to reinforce the risk's plausibility? Maybe, but I wouldn't have guessed so.\n\n**Point 4:** The question of \"but how does the AI kill us?\" is probably at the forefront of many people's minds, especially once the basics of \"it would want to kill us\" are established, but the book takes its sweet time getting there. And I don't think Chapter 6 is doing a stellar job either. It meanders around the point *so much*:\n\n*   It starts with an analogy...\n*   ... then it builds some abstract scaffolding about why ASI defeating humanity should be an easy call to make, even if we can't predict how...\n*   ... then it vaguely alludes to weird powers the ASI may develop, still without quite spelling out how these powers would enable a humanity-killing plan...\n*   ...  then it drops a bunch of concrete examples of weird hacks you can pull off with technology, still without describing how it may all come together to enable an omnicide...\n*   ... then it seems to focus on how much you can improve on biology/how easy it is to solve...\n*   ... then it does some more abstract discussion...\n*   ... and then the chapter ends...\n*   (the whole thing kind of feels like [this gif](https://imgur.com/huzuJrf))\n\n... and then we get to Part 2, the fictional story. In which the described concrete scenario is IMO quite convoluted and implausible-sounding, plus see all my other complaints about it in the previous point.\n\n* * *\n\nI think Part 3 is strong.[^of6cqmhg7zk] I think a solid chunk of Part 1 is strong as well. The online supplements seem great, and I like the format choices there (title-questions followed by quick subtitle answers). But Chapter 2, Chapter 6, and Part 2 seem like weak points. Which is unfortunate, since they're both the parts where the object-level case is laid out, and the early parts which decide whether a reader would keep reading or not.\n\nOr so my impression goes.\n\n[^36t66j8ksdj]: I binged it starting from the minute it became available, because I heard those reports about MIRI employees getting something new from it regarding the alignment problem, and wondered if it would enhance my own understanding as well, or perhaps upturn it and destroy my hopes about my own current research agenda. But no, unfortunately/thankfully there was nothing new for me. \n\n[^of6cqmhg7zk]: It also featured detailed descriptions of various engineering challenges/errors and the distillations of lessons from them (Chapter 10 and this supplement), which was the most interesting and useful part of the book for me personally.",
      "plaintextDescription": "Just finished If Anyone Builds It, Everyone Dies (and some of the supplements).[1] It feels... weaker than I'd hoped. Specifically, I think Part 3 is strong, and the supplemental materials are quite thorough, but Parts 1-2... I hope I'm wrong, and this opinion is counterweighed by all these endorsements and MIRI presumably running it by lots of test readers. But I'm more bearish on it making a huge impact than I was before reading it.\n\nPoint 1: The rhetoric – the arguments and their presentations – is often not novel, just rehearsed variations on the arguments Eliezer/MIRI already deployed. This is not necessarily a problem, if those arguments were already shaped into their optimal form, and I do like this form... But I note those arguments have so far failed to go viral. Would repackaging them into a book, and deploying it in our post-ChatGPT present, be enough? Well, I hope so.\n\nPoint 2: I found Chapter 2 in particular somewhat poorly written in how it explains the technical details.\n\nSpecifically, those explanations often occupy that unfortunate middle ground between \"informal gloss\" and \"correct technical description\" where I'd guess they're impenetrable both to non-technical readers and to technical readers unfamiliar with the subject matter.\n\nAn example that seems particularly egregious to me:\n\n> You might think that, because LLMs are grown without much understanding and trained only to predict human text, they cannot do anything except regurgitate human utterances. But that would be incorrect. [...]\n> \n> Furthermore, AIs nowadays are not trained only to predict human-generated text. An AI-grower might give their AI sixteen tries at solving a math problem, thinking aloud in words about how to solve it; then, the “chain-of-thought” for whichever of the sixteen tries went best would get further reinforced by gradient descent, yielding what’s called a reasoning model. That’s a sort of training that can push AIs to think thoughts no human could think.\n\nHow does th"
    },
    "baseScore": 57,
    "voteCount": 30,
    "createdAt": null,
    "postedAt": "2025-09-16T18:30:17.979Z",
    "af": false
  },
  {
    "_id": "drp6YRMwaFPthrWG5",
    "postId": "puv8fRDCH9jx5yhbX",
    "parentCommentId": "EDBNgnDARvsYaevZ5",
    "topLevelCommentId": "EDBNgnDARvsYaevZ5",
    "contents": {
      "markdown": "Not directly related to your query, but seems interesting:\n\n> The receptor was the first one I checked, and sure enough I have a single-nucleotide deletion 42 amino acids in to the open reading frame (ORF) of the 389 amino acid protein. That will induce a frameshift error, completely fucking up the rest of protein.\n\nWhich, in turn, is pretty solid evidence for \"oxytocin mediates the emotion of loving connection/aching affection\" (unless there are some mechanisms you've missed). I wouldn't have guessed it's that simple.\n\nGeneralizing, this suggests we can study links between specific brain chemicals/structures and cognitive features by looking for people [missing the same universal experience](https://slatestarcodex.com/2014/03/17/what-universal-human-experiences-are-you-missing-without-realizing-it/), checking if their genomes deviate from the baseline in the same way, then modeling the effects of that deviation on the brain. Alternatively, the opposite: search for people whose brain chemistry should be genetically near-equivalent except for one specific change, then exhaustively check if there's some blatant or subtle way their cognition differs from the baseline.\n\nDoing [a brief literature review](https://chatgpt.com/share/68c4670a-7310-800a-8b43-7b0e2182c109) via GPT-5, apparently this sort of thing is mostly done with regards to very \"loud\" conditions, rather than in an attempt to map out the brain in general. I could imagine that it won't turn out that simple in practice, but the actual bottleneck is probably researchers with a good enough theory-of-mind to correctly figure out the subtle ways the subjects' cognition differs (easy for \"severe autism\", much harder for \"I feel empathy and responsibility, but not loving connection\").",
      "plaintextDescription": "Not directly related to your query, but seems interesting:\n\n> The receptor was the first one I checked, and sure enough I have a single-nucleotide deletion 42 amino acids in to the open reading frame (ORF) of the 389 amino acid protein. That will induce a frameshift error, completely fucking up the rest of protein.\n\nWhich, in turn, is pretty solid evidence for \"oxytocin mediates the emotion of loving connection/aching affection\" (unless there are some mechanisms you've missed). I wouldn't have guessed it's that simple.\n\nGeneralizing, this suggests we can study links between specific brain chemicals/structures and cognitive features by looking for people missing the same universal experience, checking if their genomes deviate from the baseline in the same way, then modeling the effects of that deviation on the brain. Alternatively, the opposite: search for people whose brain chemistry should be genetically near-equivalent except for one specific change, then exhaustively check if there's some blatant or subtle way their cognition differs from the baseline.\n\nDoing a brief literature review via GPT-5, apparently this sort of thing is mostly done with regards to very \"loud\" conditions, rather than in an attempt to map out the brain in general. I could imagine that it won't turn out that simple in practice, but the actual bottleneck is probably researchers with a good enough theory-of-mind to correctly figure out the subtle ways the subjects' cognition differs (easy for \"severe autism\", much harder for \"I feel empathy and responsibility, but not loving connection\")."
    },
    "baseScore": 22,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-12T18:44:04.347Z",
    "af": false
  },
  {
    "_id": "2DAZT5brMm6gp9HdF",
    "postId": "hhbibJGt2aQqKJLb7",
    "parentCommentId": "PoE5ASPmywHcT6urZ",
    "topLevelCommentId": "PoE5ASPmywHcT6urZ",
    "contents": {
      "markdown": "From [my limited experience following AI events](https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=tTDfQXWsxf6TXHAAt), agreed. Whole storms of nonsense can be generated by some random accounts posting completely non-credible claims, some people unthinkingly amplifying those, then other people seeing that they are being amplified, thinking it means there's something to them, amplifying them further, etc.",
      "plaintextDescription": "From my limited experience following AI events, agreed. Whole storms of nonsense can be generated by some random accounts posting completely non-credible claims, some people unthinkingly amplifying those, then other people seeing that they are being amplified, thinking it means there's something to them, amplifying them further, etc."
    },
    "baseScore": 13,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-09-11T22:18:10.080Z",
    "af": false
  },
  {
    "_id": "DmKKPPms8kogj4kKk",
    "postId": "5GsGSs4gdFY6uBid4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "One of the cruxes here is whether one believes that \"AGI\" is in fact a real distinct thing, rather than there just being a space of diverse cognitive algorithms of different specializations and powers, with no high-level organization. I believe that, and that the current LLM artefacts are not AGI. Some people disagree. That's a crux.\n\nIf you're one of the people who thinks that \"general intelligence\" is a thing, then figuring out whether a given system is an AGI or not, or whether a given paradigm scales to AGI, can be a way to figure out whether that system/paradigm is going to be able to fully automates the economy/AI research. An AGI would definitely be able to do that, so \"LLMs are an AGI\" implies \"LLMs will be transformative\". \"LLMs are not AGI\" *does not* imply \"LLMs won't be transformative/omnicide-capable\", so determining the AGI-ness doesn't give you all the answers (such as the exact limits of the LLM paradigm). Nevertheless, \"is this an AGI?\" is still a useful query to run[^jmdxy077gn], and getting \"no\" does somewhat update you away from \"LLMs will be transformative\", since it removes some of the reasons they may be transformative. \n\nTertiarily, one question here may be, \"what the hell do you mean by a 'real AGI' as distinct from a 'good-enough AGI approximant'\"? Here's a long-winded pseudo-formal explanation:\n\n> I deny the assumption that behavioral definitions of AGI/ASI are akin   \n> to denying the distinction between the Taylor-polynomial approximation of a function, and that function itself (at least if we assume infinite compute [like this](https://arxiv.org/abs/1806.08747)).\n\nConsider a function \\\\(f(x)\\\\) that could be expanded into a power series \\\\(p(x)\\\\) with an infinite convergence radius, e. g. \\\\(f(x)=e^x\\\\).[^chg068ue7vb] The power series *is* that function, exactly, not an approximation; it's a valid way to *define* \\\\(f(x)\\\\).\n\nHowever, that validity only holds if you use the whole infinite-term power series, \\\\(p_\\infty(x)\\\\). Partial sums \\\\(p_n(x)\\\\), with finite \\\\(n\\\\), do not equal \\\\(f(x)\\\\), \"are not\" \\\\(f(x)\\\\). In practice, however, we're always limited to such finite partial sums. If so, then if you're choosing to physically/algorithmically implement \\\\(f(x)\\\\) as some \\\\(p_n(x)\\\\), the behavior of \\\\(p_n(x)\\\\) as you turn the \"crank\" of \\\\(n\\\\) is crucial for understanding what you're going to achieve.\n\nSuppose that we define an AGI as some function \\\\(F(x)\\\\). As above, \\\\(F\\\\) can have many exactly-equivalent representations/implementations, call their set \\\\(R(F)\\\\). One of them may be some entity \\\\(G_n(x)\\\\) such that \\\\(\\lim_{n\\to\\infty}G_n(x)=F(x)\\\\).\n\nDoes it matter which member of \\\\(R(F)\\\\) we're working with, if we're reasoning about \\\\(F\\\\)'s behavior or capabilities? No: they are all exactly equivalent. Some members of \\\\(R(F)\\\\) may define \\\\(F\\\\) behaviorally, some may do it structurally; we don't care. We pick whichever definition is more convenient to work with.\n\nHowever, implementing \\\\(F\\\\) in practice – incarnating it into algorithms and physics – is a completely different problem. To do so, we have to pick some *structural* representation/definition from \\\\(R(F)\\\\), and we have to mind its length/complexity, because it'll be upper-bounded (by our finite compute).\n\nIn this case, the pick of representation matters. Notably, for some \\\\(k\\ne \\infty\\\\), \\\\(G_k\\\\) is *not* a member of \\\\(R(F)\\\\). If so, we have to start wondering about the error \\\\(\\text{dist}(G_k;F)\\\\), and how it behaves as we move \\\\(k\\\\) around, and what is the largest \\\\(k\\\\) such that \\\\(G_k\\\\)'s description complexity is below our upper bound.\n\nFor example, perhaps \\\\(G_n(x)\\\\) is an expansion of \\\\(F(x)\\\\) around some abstract \"point\" \\\\(x=a\\\\), and while \\\\(G_\\infty\\\\) exactly equals \\\\(F\\\\) everywhere, \\\\(G_k\\\\) only serves as a good approximation around that point \\\\(a\\\\), with the error growing rapidly as you move away from \\\\(a\\\\). (We may call that \"failure to generalize out of distribution\".)\n\nIf so, nitpicking about the definition/representation/implementation of \\\\(F\\\\) that is being used to incarnate \\\\(F\\\\) absolutely matters.\n\nFor some members of \\\\(R(F)\\\\), say \\\\(Q_\\infty\\\\), even their complexity-limited approximations \\\\(Q_k\\\\) are good enough everywhere. Formally, we may say that \\\\(\\forall x:\\text{dist}(F(x);Q_k(x))<\\epsilon\\\\), for some small \\\\(\\epsilon\\\\). The informal statement \"\\\\(Q_k\\\\) is a genuine AGI\" is then only slightly imprecise.\n\nFor other members of \\\\(R(F)\\\\), like \\\\(G_\\infty\\\\), their complexity-limited approximations \\\\(G_k\\\\) are not that good. Formally, \\\\(\\exists x:\\text{dist}(F;Q_k)>T\\\\), for some large \\\\(T\\\\) and large sets of \\\\(x\\\\). The informal statement \"\\\\(G_k\\\\) is an AGI\" is then *drastically incorrect*: it essentially says \\\\(\\forall x:\\text{dist}(F(x);G_k(x))<\\epsilon\\\\). Translated into practical considerations, reasoning about \\\\(F\\\\) is not a good way to predict \\\\(G_k\\\\)'s behavior.\n\n**(Edit:** Further, if what you care about are *practically feasible* AGI designs, you may restrict the set \\\\(R(F)\\\\) only to those definitions/representations whose small-error approximations have a finite description length. Hence you may somewhat-imprecisely say that \"\\\\(G_n\\\\) is not a real AGI\" – since it's true for all finite \\\\(n\\\\) and \\\\(n=\\infty\\\\) is implicitly ruled to be outside the consideration.\n\nI note that this is what I was doing in [this comment](https://www.lesswrong.com/posts/kYL7fH2Gc9M7igqyy/yes-ai-continues-to-make-rapid-progress-including-towards?commentId=J7XcejkhkQG57dYaR), and yeah, my using the practical and the theoretical definitions of \\\\(R(F)\\\\) is unhelpful. I'll try to avoid that in the future.)\n\n[^jmdxy077gn]: Since figuring out whether it's an AGI/AGI-complete may be easier than predicting its consequences from the first principles.Formally, suppose all members of some set \\(S\\) have a property \\(q\\), and suppose we're wondering whether some entity \\(f\\) has that property. Proving \\(f\\in S\\) is a valid way to prove that \\(f\\) possesses \\(q\\), and it may be easier than proving that in a more \"direct\" way (without referring to \\(S\\)). \n\n[^chg068ue7vb]: Formally, assume \\(f(x)\\) is an entire function.",
      "plaintextDescription": "One of the cruxes here is whether one believes that \"AGI\" is in fact a real distinct thing, rather than there just being a space of diverse cognitive algorithms of different specializations and powers, with no high-level organization. I believe that, and that the current LLM artefacts are not AGI. Some people disagree. That's a crux.\n\nIf you're one of the people who thinks that \"general intelligence\" is a thing, then figuring out whether a given system is an AGI or not, or whether a given paradigm scales to AGI, can be a way to figure out whether that system/paradigm is going to be able to fully automates the economy/AI research. An AGI would definitely be able to do that, so \"LLMs are an AGI\" implies \"LLMs will be transformative\". \"LLMs are not AGI\" does not imply \"LLMs won't be transformative/omnicide-capable\", so determining the AGI-ness doesn't give you all the answers (such as the exact limits of the LLM paradigm). Nevertheless, \"is this an AGI?\" is still a useful query to run[1], and getting \"no\" does somewhat update you away from \"LLMs will be transformative\", since it removes some of the reasons they may be transformative. \n\nTertiarily, one question here may be, \"what the hell do you mean by a 'real AGI' as distinct from a 'good-enough AGI approximant'\"? Here's a long-winded pseudo-formal explanation:\n\n> I deny the assumption that behavioral definitions of AGI/ASI are akin \n> to denying the distinction between the Taylor-polynomial approximation of a function, and that function itself (at least if we assume infinite compute like this).\n\nConsider a function f(x) that could be expanded into a power series p(x) with an infinite convergence radius, e. g. f(x)=ex.[2] The power series is that function, exactly, not an approximation; it's a valid way to define f(x).\n\nHowever, that validity only holds if you use the whole infinite-term power series, p∞(x). Partial sums pn(x), with finite n, do not equal f(x), \"are not\" f(x). In practice, however, we're always limite"
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-10T18:42:27.893Z",
    "af": false
  },
  {
    "_id": "kMoz2WXW5YxRChdo9",
    "postId": "kYL7fH2Gc9M7igqyy",
    "parentCommentId": "Lwc6dxjzPnWXqHgw7",
    "topLevelCommentId": "J7XcejkhkQG57dYaR",
    "contents": {
      "markdown": "> I'd just say \"ah, any software that you can run on computers that can cause the extinction of humanity even if humans try to prevent it\" would fulfill the sufficiency criterion for AGI\\\\(_{\\text{niplav}}\\\\)\n\nFair.\n\n> One crux here may be that you are more certain that \"AGI\" [is](https://www.lesswrong.com/posts/suxvE2ddnYMPJN9HD) a [thing](https://tsvibt.blogspot.com/2022/08/the-thingness-of-things.html)?\n\nYup.",
      "plaintextDescription": "> I'd just say \"ah, any software that you can run on computers that can cause the extinction of humanity even if humans try to prevent it\" would fulfill the sufficiency criterion for AGIniplav\n\nFair.\n\n> One crux here may be that you are more certain that \"AGI\" is a thing?\n\nYup."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-10T15:21:13.636Z",
    "af": false
  },
  {
    "_id": "FKKnm4xLJouxqxbrp",
    "postId": "kYL7fH2Gc9M7igqyy",
    "parentCommentId": "6Ttoky9AgksSPf5fm",
    "topLevelCommentId": "J7XcejkhkQG57dYaR",
    "contents": {
      "markdown": "> If you surveyed people on lesswrong, as well as AI researchers, I'm pretty sure almost everyone (>90% of people) would call an AI model capable enough to eradicate humanity an AGI\n\nWell, if so, I think they would be very wrong. See a more in-depth response [here](https://www.lesswrong.com/posts/kYL7fH2Gc9M7igqyy/yes-ai-continues-to-make-rapid-progress-including-towards?commentId=JoRjFiGq2gHihWNjp).",
      "plaintextDescription": "> If you surveyed people on lesswrong, as well as AI researchers, I'm pretty sure almost everyone (>90% of people) would call an AI model capable enough to eradicate humanity an AGI\n\nWell, if so, I think they would be very wrong. See a more in-depth response here."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-10T14:59:25.384Z",
    "af": false
  },
  {
    "_id": "JoRjFiGq2gHihWNjp",
    "postId": "kYL7fH2Gc9M7igqyy",
    "parentCommentId": "ZoWQrdgSkm4wFhbgW",
    "topLevelCommentId": "J7XcejkhkQG57dYaR",
    "contents": {
      "markdown": "> Why do you define \"AGI\" through its internals, and not through its capabilities?\n\nFor the same reason we make a distinction between the Taylor-polynomial approximation of a function, and that function itself? There is a \"correct\" algorithm for general intelligence, which e. g. humans use, and there are various ways to approximate it. Approximants would behave differently from the \"real thing\" in various ways, and it's important to track which one you're looking at.\n\n> It would matter if the resulting AIs fall over and leave the lightcone in the primordial state, which looks plausible from your view?\n\nThat's not the primary difference, in my view. My view is simply that there's a difference between an \"exact AGI\" and AGI-approximants *at all*. That difference can be characterized as \"whether various agent-foundations results apply to that system universally / predict its behavior universally, or only in some limited contexts\". Is it well-modeld as an agent, or is modeling it as an agent itself a drastically flawed approximation of its behavior that breaks down in some contexts?\n\nAnd yes, \"falls over after the omnicide\" is a plausible way for AGI-approximants to differ from an actual AGI. Say, their behavior can be coherent at timescales long enough to plot humanity's demise (e. g., a millennium), but become incoherent at 100x that time-scale. So after a million years, their behavior devolves into noise, and they self-destruct or just enter a stasis. More likely, I would expect that if a civilization of those things ever runs into an actual general superintelligence (e. g., an alien one), the \"real deal\" would rip through them like it would through humanity (even if they have a massive resource advantage, e. g. 1000x more galaxies under their control).\n\nSome other possible differences:\n\n*   Capability level as a function of compute. Approximants can be drastically less compute-efficient, in a quantity-is-a-quality-of-its-own way.\n*   Generality. Approximants can be much \"spikier\" in their capabilities than general intelligences, genius-level in some domains/at some problems and below-toddler in other cases, in a way that is random, or fully determined by their training data.\n    *   Approximants' development may lack [a sharp left turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization).\n*   Autonomy. Approximants can be incapable of autonomously extending their agency horizons, or generalizing to new domains, or deriving new domains (\"innovating\") without an \"actual\" general intelligence to guide them.\n    *   (Which doesn't *necessarily* prevent them from destroying humanity. Do you need to \"innovate\" to do so? [I don't know that you do](https://www.lesswrong.com/posts/xSJMj3Hw3D7DPy5fJ/humanity-vs-agi-will-never-look-like-humanity-vs-agi-to).)\n\nNone of this may matter for the question of \"can this thing kill humanity?\". But, like, I think there's a real difference there that cuts reality at the joints, and tracking it is important.\n\nIn particular, it's important for the purposes of evaluating whether we in fact do live in a world where a specific AGI-approximant scales to omnicide given realistic quantities of compute, or whether we don't.\n\nFor example, consider [@Daniel Kokotajlo](https://www.lesswrong.com/users/daniel-kokotajlo?mention=user)'s expectation that METR's task horizons would go superexponential at some point. I think it's quite likely if the LLM paradigm scales to an *actual* AGI, if it is really acquiring the same generality/agency skills humans have. If not, however, if it is merely some domain-limited approximation of AGI, it may stay at the current trend forever (or, well, as long as the inputs keep scaling the same way), no matter how counter-intuitive that may feel.\n\nIndeed, \"how much can we rely on our intuitions regarding agents/humans when making predictions about a given AI paradigm?\" may be a good way to characterize the difference here. Porting intuitions about agents lets us make predictions like \"superexponential task-horizon scaling\" and \"the sharp left turn\" and \"obviously the X amount of compute and data ought to be enough for superintelligence\". But if the underlying system is not a \"real\" agent in many kinds of ways, those predictions would start becoming suspect. (And I would say the current paradigm's trajectory has been quite counter-intuitive, from this perspective. My intuitions broke around GPT-3.5, and my current models are an attempt to deal with that.)\n\nThis, again, may not actually bear on the question of whether the given system scales to omnicide. The AGI approximant's scaling laws may still be powerful enough. But it determines what hypotheses we should be tracking about this, what observations we should be looking for, and how we should update on them.\n\n> And the whole reason why we talk about AGI and ASI so much here on Less Wrong dot com is because those AI systems could lead to drastic changes of the future of the universe.\n\nThings worth talking about are the things that can lead to drastic changes of the future of the universe, yes. And AGI is one of those things, so we should talk about it. But I think defining \"is an AGI\" as \"any system that can lead to drastic changes of the future of the universe\" is silly, no? Words mean things. A sufficiently big antimatter bomb is not an AGI, superviruses are not AGIs, and LLMs can be omnicide-capable (and therefore worth talking about) without being AGIs as well.",
      "plaintextDescription": "> Why do you define \"AGI\" through its internals, and not through its capabilities?\n\nFor the same reason we make a distinction between the Taylor-polynomial approximation of a function, and that function itself? There is a \"correct\" algorithm for general intelligence, which e. g. humans use, and there are various ways to approximate it. Approximants would behave differently from the \"real thing\" in various ways, and it's important to track which one you're looking at.\n\n> It would matter if the resulting AIs fall over and leave the lightcone in the primordial state, which looks plausible from your view?\n\nThat's not the primary difference, in my view. My view is simply that there's a difference between an \"exact AGI\" and AGI-approximants at all. That difference can be characterized as \"whether various agent-foundations results apply to that system universally / predict its behavior universally, or only in some limited contexts\". Is it well-modeld as an agent, or is modeling it as an agent itself a drastically flawed approximation of its behavior that breaks down in some contexts?\n\nAnd yes, \"falls over after the omnicide\" is a plausible way for AGI-approximants to differ from an actual AGI. Say, their behavior can be coherent at timescales long enough to plot humanity's demise (e. g., a millennium), but become incoherent at 100x that time-scale. So after a million years, their behavior devolves into noise, and they self-destruct or just enter a stasis. More likely, I would expect that if a civilization of those things ever runs into an actual general superintelligence (e. g., an alien one), the \"real deal\" would rip through them like it would through humanity (even if they have a massive resource advantage, e. g. 1000x more galaxies under their control).\n\nSome other possible differences:\n\n * Capability level as a function of compute. Approximants can be drastically less compute-efficient, in a quantity-is-a-quality-of-its-own way.\n * Generality. Approximants can be much"
    },
    "baseScore": 12,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-10T14:58:17.007Z",
    "af": false
  },
  {
    "_id": "J7XcejkhkQG57dYaR",
    "postId": "kYL7fH2Gc9M7igqyy",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> AI Continues To Make Rapid Progress\n\nYup, that is indisputable. Further, it's possible that LLMs scale to a transformative technology, to the Singularity, and/or to omnicide. (Though it's not-that-likely, on my model; I think I still give this cluster of scenarios ~20%.)\n\n> Including Towards AGI\n\nI don't think so. I'm more sure of LLMs not scaling to AGI than ever.\n\nOver the last few months, I've made concentrated efforts to get firsthand experience of frontier LLMs, [for math-research assistance (o3)](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=3ngxyzRyNuabqDH5c) and [coding (Opus 4.1)](https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=seu93kZne8QayvEAD). LLMs' capabilities turned out to be *below* my previous expectations, precisely along the dimension of [agency/\"actual understanding\"](https://www.lesswrong.com/posts/auGYErf5QqiTihTsJ/what-indicators-should-we-watch-to-disambiguate-agi?commentId=DM2QPvAnwKNYy4QAp).\n\nDoubtlessly part of it is a prompting-skill issue on my part. Still, I don't think the gap between the performance I've managed to elicit and the top performance is *that* big. For one, my experience echoes those of many other mathematicians/software engineers, and also METR's results (on [agency horizons](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/), [often-negative productivity effects](https://metr.org/blog/2025-07-10-early-2025-ai-experienced-os-dev-study/), and [LLM-code-is-not-mergeable](https://metr.org/blog/2025-08-12-research-update-towards-reconciling-slowdown-with-time-horizons/)).\n\nThese things have no clue what's going on, there's nobody in there. Whatever algorithms they are running, those are not the algorithms of a general intelligence, and there's no reason to believe they're on some sort of \"sliding scale\" to it.\n\nI've still found them useful. If METR's trend actually holds, they will indeed become increasingly more useful. If it actually holds to >1-month tasks, they may actually become transformative within the decade. Perhaps they will automate the within-paradigm AI R&D[^6khhirwg26f], and it will lead to a software-only Singularity that will birth an AI model capable of eradicating humanity.\n\n*But that thing will still not be an AGI.* [This](https://gwern.net/fiction/clippy) would be the face of our extinction:\n\n> We should pause to note that a Clippy² still doesn’t really think or plan. It’s not really conscious. It is just an unfathomably vast pile of numbers produced by mindless optimization starting from a small seed program that could be written on a few pages. \\[...\\] When it ‘plans’, it would be more accurate to say it fake-plans; when it ‘learns’, it fake-learns; when it ‘thinks’, it is just interpolating between memorized data points in a high-dimensional space, and any interpretation of such fake-thoughts as real thoughts is highly misleading; when it takes ‘actions’, they are fake-actions optimizing a fake-learned fake-world, and are not real actions, any more than the people in a simulated rainstorm really get wet, rather than fake-wet. (The deaths, however, are real.)\n\nThis seems unlikely to me on balance. I think compute scaling will run out well before that. I think it's possible to scale LLMs far enough to achieve this, but that it's \"possible\" in a very useless way. A Jupiter Brain-sized LLM can likely do it (and probably just an Earth Brain-sized one), but we are not building a Jupiter Brain-sized LLM.\n\nBut maybe I'm wrong; maybe we do have enough compute.\n\n* * *\n\n1\\. Imagine an infinitely large branching lookup table/flowchart. It maps all possible sequences of observations to sequences of actions picked to match the behavior of a general intelligence. Given a hypercomputer to run it, would that thing be *effectively* an AGI, for all intents and purposes? Sure. But would it actually *be* an AGI, structurally? Nope.\n\nRemove the hypercomputer assumption and switch an infinitely large flowchart to a merely unfathomably large one. Suddenly the flowchart stops implementing general intelligence *exactly*, is relegated to an approximation of it. And that approximation is not that good, and rapidly degrades as you scale the available compute down.\n\nCan a Galaxy Brain-scale flowchart like this kill humanity? Maybe, maybe not: combinatorial numbers are larger than astronomical numbers. But there *are* numbers big enough that a flowchart of that size would be able to ape an AGI's behavior well enough to paperclip us.\n\n2\\. [Imagine Cyc](https://yuxi.ml/cyc/). It was (is) an effort to build AGI. Its core motivation is as follows: There is no \"simple algorithm for intelligence\". Intelligence is a mess of ad-hoc heuristics, and generality/autonomous learning emerges from those heuristics once some critical mass is attained. The way to AGI, then, is to do the hard, dirty work of inputting that critical mass of heuristics into your AI system (instead of lazily hoping for some sort of algorithmic shortcut), and eventually it would take off and start outputting novel discoveries:\n\n> The more knowledge Cyc has, the easier it is for Cyc to learn more. At first, it must be spoon-fed knowledge with every entry entered by hand. As it builds a basic understanding of the world, it would be able to parse sentences half-way from natural language to logic, and the ontologists would help finish the job, and the more it knew, the better it could parse, saving more time, until it would start parsing without human help. On that day, the “knowledge pump” would finally, triumphantly, be primed, and Cyc would start pumping and pumping, and more knowledge would just keep pouring out without any exhaustion, ushering a new golden age.\n\nTo realize this vision, Cycorp hired tons of domain experts to extract knowledge from:\n\n> Cyc would try to solve a problem, and fails by timing out. The ontologists at Cyc would call up a human expert and ask, “How did you do this?” and the expert would explain how they would solve it with quick rules of thumb, which the ontologists would write into Cyc, resulting in more assertions, and possibly more inference engines.\n\nAnd they scaled it to a pretty ridiculous degree:\n\n> The number of assertions grew to 30M, the cost grew to $200M, with 2000 person-years.\n\nAnd they had various fascinating exponential scaling laws:\n\n> \\[T\\]he growth of assertions is roughly exponential, doubling every 6 years. At this rate, in 2032 Cyc can expect to reach 100M assertions, the hoped-for point at which Cyc would know as much as a typical human.\n\nBut that project seems doomed. Sure, much like the flowchart, if scaled sufficiently far, this AGI-as-coded-by-ancient-Greek-philosophers would approximate a general intelligence well enough to be interesting/dangerous. But it would not have the algorithms of a general intelligence internally, and as you scale the available compute down, the approximation's quality would degrade rapidly.\n\nA Jupiter Brain-sized Cyc can probably defeat humanity. But Cycorp does not have Jupiter Brain-scale resources.\n\n3\\. [Imagine the Deep Learning industry](https://www.beren.io/2025-08-02-Most-Algorithmic-Progress-is-Data-Progress/):\n\n> When we see frontier models improving at various benchmarks we should think not just of increased scale and clever ML research ideas but *billions of dollars spent paying PhDs, MDs, and other experts to write questions and provide example answers and reasoning targeting these precise capabilities*. With the advent of outcome based RL and the move towards more ‘agentic’ use-cases, this data also includes custom RL environments which are often pixel-perfect replications of commonly used environments such as specific websites like Airbnb or Amazon, browsers, terminals and computer file-systems, and so on alongside large amounts of human trajectories exhaustively covering most common use-cases with these systems.\n> \n> In a way, this is like a large-scale reprise of the expert systems era, where instead of paying experts to directly program their thinking as code, they provide numerous examples of their reasoning and process formalized and tracked, and then we distill this into models through behavioural cloning.\n\nIndeed, this is *exactly* like a large-scale reprise of the expert systems era. The same notion that there's no simple algorithm for intelligence, that it's just a mess of heuristics; that attaining AGI just requires the \"hard work\" of scaling compute and data (instead of lazy theorizing about architectures!); the expectation that if they just chisel-in enough domain-specific expertise into DL models, generality would spontaneously emerge; the hiring of experts to extract that knowledge from; the sheer ridiculous scale of the endeavor. The only thing that's different is handing off the coding to the SGD (which does lead to dramatic efficiency improvements).\n\nDoes that paradigm scale, in the limit of infinite compute, to perfectly approximating the external behavior of generally intelligent entities? Yes. But any given LLM, no matter how big, would not be structured as a general intelligence internally, and the approximation's quality would degrade rapidly as you scale it down.\n\nBut how rapidly? A Jupiter Brain-sized LLM can probably kill us. But can an Earth Brain-sized, or, say, a \"10% of US' GDP\"-sized LLM, do it?\n\nI don't know. Maybe, maybe not. But eyeballing the current trends, I expect not.\n\n* * *\n\nNow, a fair question to ask here is: does this *matter*? If LLMs aren't \"real general intelligences\", but it's still fairly plausible that they're good-enough AGI approximations to drive humanity extinct, shouldn't our policy be the same in both cases?\n\nTo a large extent, yes. But building gears-level models of this whole thing [still seems important](https://www.lesswrong.com/posts/nEBbw2Bc2CnN2RMxy/gears-level-models-are-capital-investments).\n\n[^6khhirwg26f]: \"Within-paradigm\" as in, they will not be able to switch themselves to an innovative neurosymbolic architecture, like IIRC happens in AI-2027. Just speed up the existing algorithmic-efficiency, data-quality, and RL-environment scaling laws.",
      "plaintextDescription": "> AI Continues To Make Rapid Progress\n\nYup, that is indisputable. Further, it's possible that LLMs scale to a transformative technology, to the Singularity, and/or to omnicide. (Though it's not-that-likely, on my model; I think I still give this cluster of scenarios ~20%.)\n\n> Including Towards AGI\n\nI don't think so. I'm more sure of LLMs not scaling to AGI than ever.\n\nOver the last few months, I've made concentrated efforts to get firsthand experience of frontier LLMs, for math-research assistance (o3) and coding (Opus 4.1). LLMs' capabilities turned out to be below my previous expectations, precisely along the dimension of agency/\"actual understanding\".\n\nDoubtlessly part of it is a prompting-skill issue on my part. Still, I don't think the gap between the performance I've managed to elicit and the top performance is that big. For one, my experience echoes those of many other mathematicians/software engineers, and also METR's results (on agency horizons, often-negative productivity effects, and LLM-code-is-not-mergeable).\n\nThese things have no clue what's going on, there's nobody in there. Whatever algorithms they are running, those are not the algorithms of a general intelligence, and there's no reason to believe they're on some sort of \"sliding scale\" to it.\n\nI've still found them useful. If METR's trend actually holds, they will indeed become increasingly more useful. If it actually holds to >1-month tasks, they may actually become transformative within the decade. Perhaps they will automate the within-paradigm AI R&D[1], and it will lead to a software-only Singularity that will birth an AI model capable of eradicating humanity.\n\nBut that thing will still not be an AGI. This would be the face of our extinction:\n\n> We should pause to note that a Clippy² still doesn’t really think or plan. It’s not really conscious. It is just an unfathomably vast pile of numbers produced by mindless optimization starting from a small seed program that could be written on a few pag"
    },
    "baseScore": 16,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-09T20:00:22.159Z",
    "af": false
  },
  {
    "_id": "3JcnvqL6Ps8Zgv8pH",
    "postId": "9MaTnw5sWeQrggYBG",
    "parentCommentId": "XvwJdujgeYidHZZ23",
    "topLevelCommentId": "AudP8djw4goyjzCie",
    "contents": {
      "markdown": "> I think an important gear here is that things can be obvious-in-hindsight, but not in advance, in a way which isn't really a Bayesian update on new evidence and therefore doesn't strictly follow prediction rules.\n\nThat's my model here as well. Pseudo-formalizing it: We're not idealized agents, we're bounded agents, which means we can't actually do full Bayesian updates. We have to pick and choose what computations we run, what classes of evidence we look for and update on. In hindsight, we may discover that an incorrect prediction was caused by ours opting not to spend the resources on updating on some specific information, such that if we knew to do that, we would have *reliably* avoided the error even while having all the same *object-level* information.  \n\nIn other words, it's a Bayesian update to the distribution over Bayesian updates we should run. We discover a thing about (human) reasoning: that there's a specific reasoning error/oversight we're prone to, and that we have to run an update on the output of \"am I making this reasoning error?\" in specific situations.\n\nThis doesn't necessarily mean that this meta-level error would have been obvious to anyone in the world at all, at the time it was made. Nowadays, we all may be committing fallacies whose very definitions require agent-foundations theory decades ahead of ours; fallacies whose definitions we wouldn't even *understand* without reading a future textbook. But it does mean that specific *object-level* conclusions we're reaching today would be obviously incorrect to someone who is reasoning in a more correct way.",
      "plaintextDescription": "> I think an important gear here is that things can be obvious-in-hindsight, but not in advance, in a way which isn't really a Bayesian update on new evidence and therefore doesn't strictly follow prediction rules.\n\nThat's my model here as well. Pseudo-formalizing it: We're not idealized agents, we're bounded agents, which means we can't actually do full Bayesian updates. We have to pick and choose what computations we run, what classes of evidence we look for and update on. In hindsight, we may discover that an incorrect prediction was caused by ours opting not to spend the resources on updating on some specific information, such that if we knew to do that, we would have reliably avoided the error even while having all the same object-level information.  \n\nIn other words, it's a Bayesian update to the distribution over Bayesian updates we should run. We discover a thing about (human) reasoning: that there's a specific reasoning error/oversight we're prone to, and that we have to run an update on the output of \"am I making this reasoning error?\" in specific situations.\n\nThis doesn't necessarily mean that this meta-level error would have been obvious to anyone in the world at all, at the time it was made. Nowadays, we all may be committing fallacies whose very definitions require agent-foundations theory decades ahead of ours; fallacies whose definitions we wouldn't even understand without reading a future textbook. But it does mean that specific object-level conclusions we're reaching today would be obviously incorrect to someone who is reasoning in a more correct way."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-05T12:46:30.918Z",
    "af": false
  },
  {
    "_id": "fqrfcN7SHefEKnMnB",
    "postId": "Qdgo2jYAuFRMeMRJT",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Just skimmed for now...\n\n> We say a latent \\\\(\\Lambda'\\\\) is a \"redund\" over observables \\\\(X_1, ..., X_n\\\\) if and only if \\\\(\\Lambda'\\\\) is fully determined by each \\\\(X_i\\\\) individually, i.e. there exist functions \\\\(f_i\\\\) such that \\\\(\\Lambda' = f_i(X_i)\\\\) for each \\\\(i\\\\). In the approximate case, we weaken this condition to say that the entropy \\\\(H(\\Lambda'|X_i) \\leq \\epsilon\\\\) for all \\\\(i\\\\), for some approximation error \\\\(\\epsilon\\\\).\n\nI see your latest result has allowed you to streamline the definitions of redundancy and redunds.\n\nI think attempting to require  \\\\(\\epsilon_\\text{red}\\\\) to be small in terms of \\\\(\\epsilon_{\\text{red}'}\\\\) would still run into [my counterexample](https://www.lesswrong.com/posts/sCNdkuio62Fi9qQZK/usd500-usd500-bounty-problem-does-an-approximately?commentId=sLF4Rj6TKrgk438Co), right? (Setups could be constructed such that requiring \\\\(\\epsilon_\\text{red}\\\\) to be \\\\(\\epsilon_{\\text{red}'}\\\\)-small would cause \\\\(\\epsilon_\\text{med}\\\\) to scale arbitrarily with \\\\(i\\\\), and vice versa. So in the general case, there may exist valid redunds with the redundancy error \\\\(\\epsilon_{\\text{red}'}\\\\) such that the maximal redund's \\\\(\\epsilon_\\text{red}\\\\) and \\\\(\\epsilon_\\text{med}\\\\) (and therefore \\\\(\\epsilon_\\text{med}+2\\epsilon_{\\text{red}'}\\\\)) cannot *both* be \\\\(\\epsilon_{\\text{red}'}\\\\)-small.)",
      "plaintextDescription": "Just skimmed for now...\n\n> We say a latent Λ′ is a \"redund\" over observables X1,...,Xn if and only if Λ′ is fully determined by each Xi individually, i.e. there exist functions fi such that Λ′=fi(Xi) for each i. In the approximate case, we weaken this condition to say that the entropy H(Λ′|Xi)≤ϵ for all i, for some approximation error ϵ.\n\nI see your latest result has allowed you to streamline the definitions of redundancy and redunds.\n\nI think attempting to require  ϵred to be small in terms of ϵred′ would still run into my counterexample, right? (Setups could be constructed such that requiring ϵred to be ϵred′-small would cause ϵmed to scale arbitrarily with i, and vice versa. So in the general case, there may exist valid redunds with the redundancy error ϵred′ such that the maximal redund's ϵred and ϵmed (and therefore ϵmed+2ϵred′) cannot both be ϵred′-small.)"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-09-04T07:52:16.725Z",
    "af": false
  },
  {
    "_id": "smtbm7JHTfuAkAvPg",
    "postId": "RnKmRusmFpw7MhPYw",
    "parentCommentId": "WEA8KPXy6yX4RydXu",
    "topLevelCommentId": "XiLBTAbBFhQd3MmoD",
    "contents": {
      "markdown": "There's [this recent paper](https://digitaleconomy.stanford.edu/publications/canaries-in-the-coal-mine/), see Zvi's summary/discussion [here](https://www.lesswrong.com/posts/z38K3ELXSWde8TqnL/are-they-starting-to-take-our-jobs). I have not looked into it deeply. [Looks a bit weird](https://www.lesswrong.com/posts/z38K3ELXSWde8TqnL/are-they-starting-to-take-our-jobs?commentId=rc6jsm7fxG8an6BvA) to me. Overall, the very fact that there's so much confusion around whether LLMs are or are not useful is itself extremely weird.\n\n(Disclaimer: off-the-cuff speculation, no idea if that is how anything works.)\n\nI'm not sure how much I buy this narrative, to be honest. The kind of archetypical \"useless junior dev\" who can be outright replaced by an LLM probably... wasn't being hired to do the job anyway, but instead as a human-capital investment? To be transformed into a middle/senior dev, whose job an LLM can't yet do. So LLMs achieving short-term-capability parity with juniors shouldn't hurt juniors' job prospects, because they weren't hired for their existing capabilities anyway.\n\nHmm, perhaps it's not quite like this. Suppose companies weren't \"consciously\" hiring junior developers as a future investments; that they \"thought\"[^fhykn8l30h9] junior devs are actually useful, in the sense that if they \"knew\" they were just a future investment, they wouldn't have been hired. The appearance of LLMs who are as capable as junior devs would now remove the pretense that the junior devs provide counterfactual immediate value. So their hiring would stop, because middle/senior managers would be unable to keep justifying it, despite the quiet fact that they were *effectively* not being hired for their immediate skills anyway. And so the career pipeline would get clogged.\n\nMaybe that's what's happening?\n\n(Again, no idea if that's how anything there works, I have very limited experience in that sphere.)\n\n[^fhykn8l30h9]: In a semi-metaphorical sense, as an emergent property of various social dynamics between the middle managers reporting on juniors' performance to senior managers who set company priorities based in part on what would look good and justifiable to the shareholders, or something along those lines.",
      "plaintextDescription": "There's this recent paper, see Zvi's summary/discussion here. I have not looked into it deeply. Looks a bit weird to me. Overall, the very fact that there's so much confusion around whether LLMs are or are not useful is itself extremely weird.\n\n(Disclaimer: off-the-cuff speculation, no idea if that is how anything works.)\n\nI'm not sure how much I buy this narrative, to be honest. The kind of archetypical \"useless junior dev\" who can be outright replaced by an LLM probably... wasn't being hired to do the job anyway, but instead as a human-capital investment? To be transformed into a middle/senior dev, whose job an LLM can't yet do. So LLMs achieving short-term-capability parity with juniors shouldn't hurt juniors' job prospects, because they weren't hired for their existing capabilities anyway.\n\nHmm, perhaps it's not quite like this. Suppose companies weren't \"consciously\" hiring junior developers as a future investments; that they \"thought\"[1] junior devs are actually useful, in the sense that if they \"knew\" they were just a future investment, they wouldn't have been hired. The appearance of LLMs who are as capable as junior devs would now remove the pretense that the junior devs provide counterfactual immediate value. So their hiring would stop, because middle/senior managers would be unable to keep justifying it, despite the quiet fact that they were effectively not being hired for their immediate skills anyway. And so the career pipeline would get clogged.\n\nMaybe that's what's happening?\n\n(Again, no idea if that's how anything there works, I have very limited experience in that sphere.)\n\n 1. ^\n    In a semi-metaphorical sense, as an emergent property of various social dynamics between the middle managers reporting on juniors' performance to senior managers who set company priorities based in part on what would look good and justifiable to the shareholders, or something along those lines."
    },
    "baseScore": 9,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-02T12:55:18.328Z",
    "af": false
  },
  {
    "_id": "rc6jsm7fxG8an6BvA",
    "postId": "z38K3ELXSWde8TqnL",
    "parentCommentId": "hegM229WiNEutxAmN",
    "topLevelCommentId": "hegM229WiNEutxAmN",
    "contents": {
      "markdown": "Yeah, those graphs look kind of off. In particular: it supposedly started happening in *October 2022*? That's pre-GPT-4, and GPT-4 was very barely able to code anything useful. I don't buy that *GPT-3.5*, effectively the proof-of-concept technical demo, had this sort of effect.\n\nI guess maybe companies recognized where AI progress was headed and foresightfully downscaled their hiring practices in expectation of AI advancing faster than early-career humans can be trained? I, likewise, don't buy this level of industry-wide foresight.\n\nMy guess is that \"2022 changed it\" because of e. g. the Russia-Ukraine war and general rising world instability, not because of AI.",
      "plaintextDescription": "Yeah, those graphs look kind of off. In particular: it supposedly started happening in October 2022? That's pre-GPT-4, and GPT-4 was very barely able to code anything useful. I don't buy that GPT-3.5, effectively the proof-of-concept technical demo, had this sort of effect.\n\nI guess maybe companies recognized where AI progress was headed and foresightfully downscaled their hiring practices in expectation of AI advancing faster than early-career humans can be trained? I, likewise, don't buy this level of industry-wide foresight.\n\nMy guess is that \"2022 changed it\" because of e. g. the Russia-Ukraine war and general rising world instability, not because of AI."
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-09-02T12:46:11.625Z",
    "af": false
  },
  {
    "_id": "7EYQaF5aXBd6DyGuN",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "EcGMcqYpF7Sjtaxdk",
    "topLevelCommentId": "seu93kZne8QayvEAD",
    "contents": {
      "markdown": "> I get the feeling that it can remove blockers\n\nYeah, they can be used as a way to get out of procrastination/breach ugh fields/move past the generalized writer's block, by giving you the ability to quickly and non-effortfully hammer out a [badly-done](https://x.com/charlesmiller_7/status/1949799305126015355) [first draft](https://www.lesswrong.com/posts/3xs67kNbeYgytgETb/rauno-s-shortform?commentId=DMaGkiCS6wdJ4bmsp) of the thing. \n\n> They do like making complex stuff, though... I wonder if that's because there's a lot more bad code that looks good than just good code?\n\nWild guess: it's because AGI labs (and Anthropic in particular) are currently trying to train them to be able to autonomously spin up large, complex codebases, so much of their post-training consists of training episodes where they're required to do that. So they end up biased towards architectural choices suitable for complex applications instead of small ones.\n\nLike, given free reign, they seem eager to build broad foundations on which lots of other features could later be implemented (n = 2), even if you gave them the full spec and it only involves 1-3 features.\n\nThis would actually be kind of good – building a good foundation which won't need refactoring if you'll end up fancying a new feature! – except they're, uh, not actually *good* at building those foundations.\n\nWhich, if my guess is correct, is because their post-training always pushes them to the edge of, and then slightly *past*, their abilities. Like, suppose the post-training uses rejection sampling (training on episodes in which they succeeded), and it involves a curriculum spanning everything from small apps to \"build an OS\". If so, much like in METR's studies, there'll be some point of program complexity where they only succeed 50% of the time (or less). But they're still going to be *trained on* those 50% successful trajectories. So they'll end up \"overambitious\"/\"overconfident\", trying to do things they're not quite reliable at yet.\n\nOr maybe not; wild guess, as I said.",
      "plaintextDescription": "> I get the feeling that it can remove blockers\n\nYeah, they can be used as a way to get out of procrastination/breach ugh fields/move past the generalized writer's block, by giving you the ability to quickly and non-effortfully hammer out a badly-done first draft of the thing. \n\n> They do like making complex stuff, though... I wonder if that's because there's a lot more bad code that looks good than just good code?\n\nWild guess: it's because AGI labs (and Anthropic in particular) are currently trying to train them to be able to autonomously spin up large, complex codebases, so much of their post-training consists of training episodes where they're required to do that. So they end up biased towards architectural choices suitable for complex applications instead of small ones.\n\nLike, given free reign, they seem eager to build broad foundations on which lots of other features could later be implemented (n = 2), even if you gave them the full spec and it only involves 1-3 features.\n\nThis would actually be kind of good – building a good foundation which won't need refactoring if you'll end up fancying a new feature! – except they're, uh, not actually good at building those foundations.\n\nWhich, if my guess is correct, is because their post-training always pushes them to the edge of, and then slightly past, their abilities. Like, suppose the post-training uses rejection sampling (training on episodes in which they succeeded), and it involves a curriculum spanning everything from small apps to \"build an OS\". If so, much like in METR's studies, there'll be some point of program complexity where they only succeed 50% of the time (or less). But they're still going to be trained on those 50% successful trajectories. So they'll end up \"overambitious\"/\"overconfident\", trying to do things they're not quite reliable at yet.\n\nOr maybe not; wild guess, as I said."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-29T00:02:19.779Z",
    "af": false
  },
  {
    "_id": "6smHPDemF6PYk6BPp",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "BHH2sh3ZgAo9ocPQm",
    "topLevelCommentId": "seu93kZne8QayvEAD",
    "contents": {
      "markdown": "Yup, it certainly seems useful if you (a) want to cobble something together using tools you don't know (and which is either very simple, or easy to \"visually\" bug-test, or which you don't need to work extremely reliably), (b) want to learn some tools you don't know, so you use the LLM as a chat-with-docs interface.",
      "plaintextDescription": "Yup, it certainly seems useful if you (a) want to cobble something together using tools you don't know (and which is either very simple, or easy to \"visually\" bug-test, or which you don't need to work extremely reliably), (b) want to learn some tools you don't know, so you use the LLM as a chat-with-docs interface."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-28T23:34:05.894Z",
    "af": false
  },
  {
    "_id": "Wg9xgYFnEWPbDGwsK",
    "postId": "dxQ8jsHxgA2hwf9ai",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> When using LLM-based coding assistants, I always had a strange feeling about the interaction. I think I now have a pointer around that feeling - disappointment from having expected more (again and again), followed by low level of disgust, and an aftertaste of disrespect growing into hatred\n\nYeah, I kinda get it. Not to the point of hatred, but I do find interacting with LLMs... mentally taxing. They pass as just enough of a \"well-meaning eagerly helpful person\" to make me not want to be mean to them (as it'd make me feel bad), but they also continually induce weary disappointment in me.\n\nI wish we figured out some other interface over the base models that is not these \"AI assistant\" personas. I don't know what that'd be, but surely something better is possible. Something framed as an impersonal call to a database equipped with a powerful program/knowledge synthesis tool, maybe.\n\n> Have you seen how people un-minified Claude Code - the sheer amount of workarounds, cringe IMPORTANT inside the system prompt and the constant reminders?\n\nThis prompted me to write up about my recent experience with it, see [here](https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=seu93kZne8QayvEAD).",
      "plaintextDescription": "> When using LLM-based coding assistants, I always had a strange feeling about the interaction. I think I now have a pointer around that feeling - disappointment from having expected more (again and again), followed by low level of disgust, and an aftertaste of disrespect growing into hatred\n\nYeah, I kinda get it. Not to the point of hatred, but I do find interacting with LLMs... mentally taxing. They pass as just enough of a \"well-meaning eagerly helpful person\" to make me not want to be mean to them (as it'd make me feel bad), but they also continually induce weary disappointment in me.\n\nI wish we figured out some other interface over the base models that is not these \"AI assistant\" personas. I don't know what that'd be, but surely something better is possible. Something framed as an impersonal call to a database equipped with a powerful program/knowledge synthesis tool, maybe.\n\n> Have you seen how people un-minified Claude Code - the sheer amount of workarounds, cringe IMPORTANT inside the system prompt and the constant reminders?\n\nThis prompted me to write up about my recent experience with it, see here."
    },
    "baseScore": 8,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-28T01:09:05.896Z",
    "af": false
  },
  {
    "_id": "seu93kZne8QayvEAD",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "My latest experience with trying to use Opus 4.1 to vibe-code[^w8p6ojezssj]:\n\nContext: I wanted a few minor helper apps[^z9e7n4ly9j] that I didn't think worth the time to code manually, and which I used as an opportunity to test LLMs' current skill level (not expecting it to actually save much development time; yay multi-purpose projects).\n\nVery mixed results. My experience is that you need to give it a very short leash. As long as you do the job of factorizing the problem, and then feed it small steps to execute one by one, it tends to zero-shot them. You do need to manually check that the implementational choices are what you intended, and you'll want to start by building some (barebones) UI to make this time-efficient, but this mostly works. It's also pretty time-consuming on your part, both the upfront cost of factorizing and the directing afterwards.\n\nIf you instead give it a long leash, it produces overly complicated messes (like, 3x the lines of code needed?) which don't initially work, and which it iteratively bug-fixes into the ground. The code is then also liable to contain foundation-level implementation choices that go directly against the spec; if that is pointed out, the LLM wrecks everything attempting a refactor.\n\nIn general, if the LLM screws up, telling it to fix things seems to be a bad idea, with it entering a downward spiral. You're better off reverting the codebase to the pre-screwup state, figuring out how to modify your latest instruction to get the LLM to avoid the previous error, then hope it won't find a new way to screw up. That \"figure out how to modify your instruction\" step requires a fair amount of mental effort, not to mention having to actually type it out.[^dktwnbnr8iv]\n\nI shudder to think what large vibe-coded codebases[^1ac0sqxehuz] look like internally. Ones where you can't, like, get [a firehose of data](https://www.lesswrong.com/posts/9kNxhKWvixtKW5anS/you-are-not-measuring-what-you-think-you-are-measuring) on how their components work, where your vision is limited to unit tests, and where several such components are interlocked.\n\nSome of this is doubtlessly a skill issue on my part: I don't do green-field development much and my LLM-wrangling expertise is unrefined. But I don't buy that you can structure arbitrary programs and workflows such that LLMs zero-shot more than 1-3 features at a time (roughly the equivalents of 5-30 minutes of a human developer's work) without close supervision. I guess it might be fine if your requirements are pretty loose and you don't mind the app breaking or working incorrectly once in a while. But not if you want anything secure, precise, and reliable.\n\nThere's also the issue with LLMs breaking on big codebases, but this presumably *can* be fixed by structuring codebases accordingly, around LLMs' flaws. Like, design them to make their components interact through sparse interfaces with strong restrictions on input/output type signatures, such that the implementation details are completely abstracted out and the LLM only needs to deal with external functionality. (But LLMs sure don't know, out-of-the-box, that they need to structure their code this way.)\n\nNotably, it's all very in line with METR's evaluations, both [the agency horizons](https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/) (the 80%-success ones) and [the \"LLM code is not mergable\" result](https://www.lesswrong.com/posts/25JGNnT9Kg4aN5N5s/metr-research-update-algorithmic-vs-holistic-evaluation). I'm very curious if LLMs will indeed be able to reliably zero-shot hour-sized chunks of coding a year from now.\n\n**Edit:** Oh yeah, and this all eventually did result in workable apps which I'm using now. \n\n[^w8p6ojezssj]: Through Claude Code; initially used Opus 4.1 for everything, then got annoyed at the amount of money being spent and switched to the \"Opus for planning, Sonnet for execution\" mode. \n\n[^z9e7n4ly9j]: Specifically, a notetaking app (the simplest/minimum-viable version), a timer, and a task database, with my having very precise, specific, and idiosyncratic opinions regarding how they ought to work. \n\n[^dktwnbnr8iv]: To be clear, by a \"screwup\" I don't mean writing code that contains a bug – LLMs can do bug-fixing as well as they do everything else. Writing code containing a bug isn't quite \"screwing up\", it's a normal part of development.I'm talking about more \"workflow-breaking\" screwups, where the LLM makes some agency-level error (deletes code it shouldn't have, pastes the code into the wrong place, misreads the spec...), gets flustered, and then tries to \"fix\" things. \n\n[^1ac0sqxehuz]: I .e., where the developer doesn't manually read and comprehensively fix LLM-generated code, which would often take more time than writing it manually.",
      "plaintextDescription": "My latest experience with trying to use Opus 4.1 to vibe-code[1]:\n\nContext: I wanted a few minor helper apps[2] that I didn't think worth the time to code manually, and which I used as an opportunity to test LLMs' current skill level (not expecting it to actually save much development time; yay multi-purpose projects).\n\nVery mixed results. My experience is that you need to give it a very short leash. As long as you do the job of factorizing the problem, and then feed it small steps to execute one by one, it tends to zero-shot them. You do need to manually check that the implementational choices are what you intended, and you'll want to start by building some (barebones) UI to make this time-efficient, but this mostly works. It's also pretty time-consuming on your part, both the upfront cost of factorizing and the directing afterwards.\n\nIf you instead give it a long leash, it produces overly complicated messes (like, 3x the lines of code needed?) which don't initially work, and which it iteratively bug-fixes into the ground. The code is then also liable to contain foundation-level implementation choices that go directly against the spec; if that is pointed out, the LLM wrecks everything attempting a refactor.\n\nIn general, if the LLM screws up, telling it to fix things seems to be a bad idea, with it entering a downward spiral. You're better off reverting the codebase to the pre-screwup state, figuring out how to modify your latest instruction to get the LLM to avoid the previous error, then hope it won't find a new way to screw up. That \"figure out how to modify your instruction\" step requires a fair amount of mental effort, not to mention having to actually type it out.[3]\n\nI shudder to think what large vibe-coded codebases[4] look like internally. Ones where you can't, like, get a firehose of data on how their components work, where your vision is limited to unit tests, and where several such components are interlocked.\n\nSome of this is doubtlessly a skill issue on"
    },
    "baseScore": 17,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-28T00:55:31.166Z",
    "af": false
  },
  {
    "_id": "w33sbTu2CpmPsjann",
    "postId": "DHSY697pRWYto6LsF",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> This really seems like the sort of thing where we should be able to get a mathematical theorem, from first principles, rather than assuming.\n\nDoing my usual exercise of \"stop reading here and try to derive an independent model\"...\n\nHere's the way my anthropics-poisoned brain would frame this:\n\nAssume that your window of observations is limited to some small part of the universe, or perhaps that it's *coarse* (you only observe some high-level information, not full low-level details). Suppose you either narrow this window further (stop observing some part of the system), or try to predict/control low-level dynamics from the high-level state.\n\nIn the former case, the moment you stop observing a part of the system, your observations become consistent with those from a *set* of timelines differing by what happened in that part of the system (under the constraint of the laws of physics). E. g., if you started with a perfect \"snapshot\" of the gas molecules' positions and velocities (we're assuming they're billiard balls), as time goes on, there's more and more information leaking from the unobserved environment as they bounce off walls composed of other molecules (whose exact positions and velocities you didn't observe, so you couldn't predict their effects on the gas particles).[^estk2o8aqbh] You might be able to correctly time when to open/close the door to correctly sort the molecules that started very close to it, but this ability would quickly degrade. Your uncertainty increases; entropy rises.\n\nIn the latter case, you already started out unsure regarding which timeline consistent with the given high-level information (e. g., pressure and temperature in the box) you're in. Your uncertainty over the low-level state is already maximized under the high-level constraints.\n\nAnd Maxwell's demon basically maintains perfect observation of the state of the system you'd otherwise fail to (perfectly) observe.\n\nThe exact amount of uncertainty generated (aka the amount of data the demon's observational array should be gathering for the demon to work) depends on the physics of the system you're in (how quickly the set of possible timelines grows; how many low-level states are consistent with the high-level state). But the overall idea should work in all universes and on all levels of abstraction.\n\nMoving from anthropics to embedded agency: the above assumes that you have the correct model of the laws of your abstract environment (its state-transition rules, potentially stochastic ones), and enough computational resources to simulate that environment (at the given level of abstraction).\n\nIf you're operating at the lowest level of abstraction of our universe, your uncertainty is \"in the territory\": this setup should reduce to the quantum-mechanical uncertainty principle, with the unobserved part of the system becoming Schrödinger's cat's box. The anthropics reasoning then reduces to the many-worlds reasoning.[^snpnh99ezz]\n\nAt higher levels, it's \"pseudo-territorial\": if we model a [coarse agent](https://www.lesswrong.com/posts/zYCoqjYNHFAEJD8TC/are-you-more-real-if-you-re-really-forgetful) as genuinely discarding the low-level information it can't integrate into its abstract models, the setup should be structurally equivalent to the lowest-level one.[^w5e05hv562]\n\nIn a way, this is trivial. As long as the underlying state-transition rules are stochastic, or are *modeled as* stochastic – either because they're \"genuinely\" uncertain as in QM, or because you have imperfect information (up to the \\\\(m\\\\)th significant digit), or because you're using an abstract model that bakes in imperfect information – as long as it's the case, any mechanism that tries to control which state you end up in would need to \"oppose\"/\"de-stochasticify\" the stochastic dynamics.\n\n> So: with n molecules, the KL divergence between the two distributions is \\\\(n \\cdot log2\\\\), i.e. \\\\(n\\\\) bits, as one might intuitively guess.\n\nRight, if you're not making any new observations, your uncertainty would be maximized over the set of the *allowed* timelines consistent with your past observations and the laws of physics (or the laws of your abstract environment). Since molecules can't disappear/leave the box/etc., there are only \\\\(2^n\\\\) such timelines.\n\n[^estk2o8aqbh]: Alternatively, you can model it as having snapshotted the state only up to some \\(m\\)th significant digit of the molecules' positions/velocities, and as it evolves, the knowledge of the digits past the \\(m\\)th one becomes ever more necessary for predicting the dynamics. \n\n[^snpnh99ezz]: This can probably be exactly formalized via this approach,[4] but these margins etc. etc. (Note to John: this is related to the potential (dubious) QM-based line of reasoning I'd mentioned in the write-up I sent you.)  \n\n[^w5e05hv562]: Except with the stochastic state-transition mechanism being Markovian instead of non-Markovian, because that information is not actually discarded (we can't use it, but we still observe it, so it still differentiates our exact observations between different timelines) – so there wouldn't be macro-scale quantum effects; see the linked paper. \n\n[^70okdmbm0w7]: Bullshit check: Scott Aaronson thinks this paper is valid (but not enlightening).",
      "plaintextDescription": "> This really seems like the sort of thing where we should be able to get a mathematical theorem, from first principles, rather than assuming.\n\nDoing my usual exercise of \"stop reading here and try to derive an independent model\"...\n\nHere's the way my anthropics-poisoned brain would frame this:\n\nAssume that your window of observations is limited to some small part of the universe, or perhaps that it's coarse (you only observe some high-level information, not full low-level details). Suppose you either narrow this window further (stop observing some part of the system), or try to predict/control low-level dynamics from the high-level state.\n\nIn the former case, the moment you stop observing a part of the system, your observations become consistent with those from a set of timelines differing by what happened in that part of the system (under the constraint of the laws of physics). E. g., if you started with a perfect \"snapshot\" of the gas molecules' positions and velocities (we're assuming they're billiard balls), as time goes on, there's more and more information leaking from the unobserved environment as they bounce off walls composed of other molecules (whose exact positions and velocities you didn't observe, so you couldn't predict their effects on the gas particles).[1] You might be able to correctly time when to open/close the door to correctly sort the molecules that started very close to it, but this ability would quickly degrade. Your uncertainty increases; entropy rises.\n\nIn the latter case, you already started out unsure regarding which timeline consistent with the given high-level information (e. g., pressure and temperature in the box) you're in. Your uncertainty over the low-level state is already maximized under the high-level constraints.\n\nAnd Maxwell's demon basically maintains perfect observation of the state of the system you'd otherwise fail to (perfectly) observe.\n\nThe exact amount of uncertainty generated (aka the amount of data the demon's obse"
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-26T23:44:13.110Z",
    "af": false
  },
  {
    "_id": "9nRKGHfTvSdJ8xQDv",
    "postId": "dX7gx7fezmtR55bMQ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "~You're absolutely right! This post isn't just—~ Ahem.\n\nYeah, I think the phenomena share the same mechanism. As [@james oofou](https://www.lesswrong.com/users/james-oofou?mention=user) points out, there might be a meaningful difference in the *degree* of reality-untethering between the two... But while dictators get more pushback from the world than an idealized socially isolated crackpot, dictators also have the power to passively *distort* the world around them to fit their beliefs. Crackpots aren't surrounded by powerful people falling over themselves to guess at what they want and *physically* build Potemkin villages around them. The degree of the ensuing reality untethering could be quite massive.[^au9jk4kcmze]\n\nI think there's an \"official\" name for the \"yes-men psychosis\", by the way: [the dictator's dilemma](https://www.sciencedirect.com/science/article/pii/S0167278924001003). Or, the former might be a slightly more general form of the latter.\n\n[On a lighter note](https://x.com/slatestarcodex/status/1928052430182437162):\n\n> **Feast Bab:** Modern life has completely eliminated the role of Grand Vizier. I could not hire one if I wanted to.\n> \n> **Scott Alexander:** AI gives intelligent advice, flatters you shamelessly, and is secretly planning to betray you and take over. We have \\*automated\\* the role of Grand Vizier.\n\n[^au9jk4kcmze]: The Putin example seems particularly fitting here. A relevant factor: Allegedly,[2] Putin was extremely terrified of getting COVID, and basically locked himself in a bunker for the entirety of 2020 and then some. People were only allowed in after sitting in quarantine for 2 weeks. This means that he started getting even more filtered information about the outside world, and the only people he socialized with were those of his close circle who had nothing better to do than entertain him; no real-world-grounded duties that would get in the way of sitting out the two weeks. This amplified the echo chamber even further than normal: he basically hung out with his cronies convincing each other of various conspiracy theories. This ultimately contributed to the events of 2022. \n\n[^zzn3shaxem]: Source: A Russian-opposition speaker I mostly trust. I have not looked into it any deeper just asked GPT-5 to sanity-check it, see here if you're interested. Yes, this is very ironic.",
      "plaintextDescription": "You're absolutely right! This post isn't just— Ahem.\n\nYeah, I think the phenomena share the same mechanism. As @james oofou points out, there might be a meaningful difference in the degree of reality-untethering between the two... But while dictators get more pushback from the world than an idealized socially isolated crackpot, dictators also have the power to passively distort the world around them to fit their beliefs. Crackpots aren't surrounded by powerful people falling over themselves to guess at what they want and physically build Potemkin villages around them. The degree of the ensuing reality untethering could be quite massive.[1]\n\nI think there's an \"official\" name for the \"yes-men psychosis\", by the way: the dictator's dilemma. Or, the former might be a slightly more general form of the latter.\n\nOn a lighter note:\n\n> Feast Bab: Modern life has completely eliminated the role of Grand Vizier. I could not hire one if I wanted to.\n> \n> Scott Alexander: AI gives intelligent advice, flatters you shamelessly, and is secretly planning to betray you and take over. We have *automated* the role of Grand Vizier.\n\n 1. ^\n    The Putin example seems particularly fitting here. A relevant factor: Allegedly,[2] Putin was extremely terrified of getting COVID, and basically locked himself in a bunker for the entirety of 2020 and then some. People were only allowed in after sitting in quarantine for 2 weeks. This means that he started getting even more filtered information about the outside world, and the only people he socialized with were those of his close circle who had nothing better to do than entertain him; no real-world-grounded duties that would get in the way of sitting out the two weeks. This amplified the echo chamber even further than normal: he basically hung out with his cronies convincing each other of various conspiracy theories. This ultimately contributed to the events of 2022.\n\n 2. ^\n    Source: A Russian-opposition speaker I mostly trust. I have not looke"
    },
    "baseScore": 10,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-25T20:07:08.149Z",
    "af": false
  },
  {
    "_id": "5xoynkgbr6PSLeeAc",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "pESut55iNurmTnhPT",
    "topLevelCommentId": "bRToWiJyoMgcpfvmw",
    "contents": {
      "markdown": "That's not the relevant metric. *The process of training* involves a model skyrocketing in capabilities, from a random initialization to a human-ish level (or the surface appearance of it, at least). There's a simple trick – pretraining – which allows to push a model's intelligence from zero to that level.\n\nAdvancing past *this* point then slows down to a crawl: each incremental advance requires new incremental research derived by humans, rather than just turning a compute crank.\n\n(Indeed, IIRC a model's loss curves across training do look like S-curves? **Edit:** On looking it up, nope, I think.)\n\nThe FOOM scenario, on the other hand, assumes a paradigm that grows from random initialization to human level to superintelligence all in one go, as part of the same training loop, without a phase change from \"get it to human level incredibly fast, over months\" to \"painstakingly and manually improve the paradigm past the human level, over years/decades\".",
      "plaintextDescription": "That's not the relevant metric. The process of training involves a model skyrocketing in capabilities, from a random initialization to a human-ish level (or the surface appearance of it, at least). There's a simple trick – pretraining – which allows to push a model's intelligence from zero to that level.\n\nAdvancing past this point then slows down to a crawl: each incremental advance requires new incremental research derived by humans, rather than just turning a compute crank.\n\n(Indeed, IIRC a model's loss curves across training do look like S-curves? Edit: On looking it up, nope, I think.)\n\nThe FOOM scenario, on the other hand, assumes a paradigm that grows from random initialization to human level to superintelligence all in one go, as part of the same training loop, without a phase change from \"get it to human level incredibly fast, over months\" to \"painstakingly and manually improve the paradigm past the human level, over years/decades\"."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-24T19:59:50.985Z",
    "af": false
  },
  {
    "_id": "irkJ5kbSqf3mb7Dey",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "bRToWiJyoMgcpfvmw",
    "topLevelCommentId": "bRToWiJyoMgcpfvmw",
    "contents": {
      "markdown": "I don't know that they were wrong about that claim. Or, it depends on what we interpret as the claim. \"AI would do [the thing in this chart](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/eeZ2u9CavqkWpo5TK/pvqq1qd3rsyke45kju4k)\" proved false[^5w0fz3mg23a], but I don't think this necessarily implies that \"there's a vast distance between a village idiot and Einstein in intelligence levels\".\n\nRather, what we're observing may just be a property of the specific approach to AI represented by LLMs. It is not quite \"imitation learning\", but it shares some core properties of imitation learning. LLMs skyrocketed to human-ish level because they're trained to emulate humans via human-generated data. Improvements then slowed to a (relative) crawl because it *became a data-quality problem.* It's not that there's a vast distance between stupid and smart humans, such that moving from a random initialization to \"dumb human\" is as hard as moving from a \"dumb human\" to a \"smart human\". It's just that, for humans, assembling an \"imitate a dumb human\" dataset is easy (scrape the internet), whereas transforming it into an \"imitate a smart human\" dataset is [very hard](https://www.beren.io/2025-08-02-Most-Algorithmic-Progress-is-Data-Progress/). (And then RL is just strictly worse at compute-efficiency and generality, etc.)\n\n(Edit: Yeah, that roughly seems to be Eliezer's model too, see [this thread](https://x.com/ESYudkowsky/status/1959633568969474057).)\n\nIf that's the case, Eliezer and co.'s failure wasn't in modeling the underlying dynamics of intelligence incorrectly, but in failing to predict and talk about the foibles of an ~imitation-learning paradigm. That seems fairly minor.\n\nAlso: did that chart *actually* get disproven? To believe so, we have to assume current LLMs are at the \"dumb human\" levels, and that what's currently happening is a slow crawl to \"smart human\" and beyond. But if LLMs are not AGI-complete, if their underlying algorithms (rather than externally visible behaviors) qualitatively differ from what humans do, this gives us little information on the speed with which an AGI-complete AI would move from a \"dumb human\" to a \"smart human\". Indeed, I still expect pretty much that chart to happen once we get to actual AGI; [see here](https://www.lesswrong.com/posts/yew6zFWAKG4AGs3Wk/foom-and-doom-1-brain-in-a-box-in-a-basement).\n\n[^5w0fz3mg23a]: Or did it? See below.",
      "plaintextDescription": "I don't know that they were wrong about that claim. Or, it depends on what we interpret as the claim. \"AI would do the thing in this chart\" proved false[1], but I don't think this necessarily implies that \"there's a vast distance between a village idiot and Einstein in intelligence levels\".\n\nRather, what we're observing may just be a property of the specific approach to AI represented by LLMs. It is not quite \"imitation learning\", but it shares some core properties of imitation learning. LLMs skyrocketed to human-ish level because they're trained to emulate humans via human-generated data. Improvements then slowed to a (relative) crawl because it became a data-quality problem. It's not that there's a vast distance between stupid and smart humans, such that moving from a random initialization to \"dumb human\" is as hard as moving from a \"dumb human\" to a \"smart human\". It's just that, for humans, assembling an \"imitate a dumb human\" dataset is easy (scrape the internet), whereas transforming it into an \"imitate a smart human\" dataset is very hard. (And then RL is just strictly worse at compute-efficiency and generality, etc.)\n\n(Edit: Yeah, that roughly seems to be Eliezer's model too, see this thread.)\n\nIf that's the case, Eliezer and co.'s failure wasn't in modeling the underlying dynamics of intelligence incorrectly, but in failing to predict and talk about the foibles of an ~imitation-learning paradigm. That seems fairly minor.\n\nAlso: did that chart actually get disproven? To believe so, we have to assume current LLMs are at the \"dumb human\" levels, and that what's currently happening is a slow crawl to \"smart human\" and beyond. But if LLMs are not AGI-complete, if their underlying algorithms (rather than externally visible behaviors) qualitatively differ from what humans do, this gives us little information on the speed with which an AGI-complete AI would move from a \"dumb human\" to a \"smart human\". Indeed, I still expect pretty much that chart to happen once we g"
    },
    "baseScore": 9,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-24T17:25:40.789Z",
    "af": false
  },
  {
    "_id": "sHfbJriC4T5eFuaEb",
    "postId": "Gd36HT7qLr684SYuQ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Congratulations!\n\nI'm very glad to see some serious progress on this agenda. If I'm being honest, I was less than excited by the latest long stretch of (published) results on it. It felt like there was no progress on the core ideas being made, only the shoring-up of the foundations and some minor peripheral insights. It looked concerningly plausible that this path routed through regions of math-space so thorny they were effectively impassable. (This might be an unfair characterization, particularly if those results looked more important in the context of unpublished research/considerations. Sorry if so.) \n\nThis result I *am* very much excited about. It seems to be a meaningful step \"depthwards\", and serves as an existence proof that depthwards progress is possible at all. Great job!",
      "plaintextDescription": "Congratulations!\n\nI'm very glad to see some serious progress on this agenda. If I'm being honest, I was less than excited by the latest long stretch of (published) results on it. It felt like there was no progress on the core ideas being made, only the shoring-up of the foundations and some minor peripheral insights. It looked concerningly plausible that this path routed through regions of math-space so thorny they were effectively impassable. (This might be an unfair characterization, particularly if those results looked more important in the context of unpublished research/considerations. Sorry if so.) \n\nThis result I am very much excited about. It seems to be a meaningful step \"depthwards\", and serves as an existence proof that depthwards progress is possible at all. Great job!"
    },
    "baseScore": 25,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-08-23T00:01:49.077Z",
    "af": false
  },
  {
    "_id": "P2EwwPofxiaozCJZC",
    "postId": "RnKmRusmFpw7MhPYw",
    "parentCommentId": "3hnDrKT3qwKwuyy4i",
    "topLevelCommentId": "iAwEZCk5RWi5jKwjb",
    "contents": {
      "markdown": "> Like, if they stop improving at <1 month horizon lengths (as you say immediately above the text I quoted) that is clearly a case of LLMs hitting a wall right?\n\nI distinguish \"the LLM paradigm hitting a wall\" and \"the LLM paradigm running out of fuel for further scaling\".\n\n> I agree that compute and resources running out could cause this, but it's notable that we expect ~1 month in not that long, like only ~3 years at the current rate.\n\nYes, precisely. Last I checked, we [expected](https://www.lesswrong.com/posts/XiMRyQcEyKCryST8T/slowdown-after-2028-compute-rlvr-uncertainty-moe-data-wall) scaling to run out by 2029ish, no?\n\nAh, reading the comments, I see [you expect there to be some inertia](https://www.lesswrong.com/posts/XiMRyQcEyKCryST8T/slowdown-after-2028-compute-rlvr-uncertainty-moe-data-wall?commentId=tH8wQwvSMJvcH7Rce)... Okay, 2032 / 7 more years *would* put us at \">1 year\" task horizons. That does make me a bit more concerned. (Though 80% reliability is several doublings behind, and I expect tasks that involve real-world messiness to be even further behind.)\n\n> Ok, but surely there has to be something they aren't getting better at (or are getting better at too slowly)\n\n\"Ability to come up with scientific innovations\" seems to be one.\n\nLike, I expect they *are* getting better at the underlying skill. If you had a benchmark which measures some toy version of \"produce scientific innovations\" ([AidanBench](https://aidanbench.com/)?), and you plotted frontier models' performance on it against time, you would see the number going up. But it currently seems to lag way behind other capabilities, and I likewise don't expect it to reach dangerous heights before scaling runs out.\n\nThe way I would put it, the things LLMs are strictly not improving on are not \"specific types of external tasks\". What I think they're not getting better at – because it's something they've never been capable of doing – are specific *cognitive algorithms* which allow to complete certain cognitive tasks in a dramatically more compute-efficient manner. We've [talked about this some](https://www.lesswrong.com/posts/auGYErf5QqiTihTsJ/what-indicators-should-we-watch-to-disambiguate-agi?commentId=DM2QPvAnwKNYy4QAp) before.\n\nI think that, in the limit of scaling, the LLM paradigm is equivalent to AGI, but that it's not a very efficient way to approach this limit. And it's less efficient along some dimensions of intelligence than along others.\n\nThis paradigm attempts to scale certain modules a generally intelligent mind would have to ridiculous levels of power in order to make up for the lack of *other* necessary modules. This will keep working to improve performance across all tasks, as long as you keep feeding LLMs more data and compute. But there seems to be only a few \"GPT-4 to GPT-5\" jumps left, and I don't think it'd be enough.",
      "plaintextDescription": "> Like, if they stop improving at <1 month horizon lengths (as you say immediately above the text I quoted) that is clearly a case of LLMs hitting a wall right?\n\nI distinguish \"the LLM paradigm hitting a wall\" and \"the LLM paradigm running out of fuel for further scaling\".\n\n> I agree that compute and resources running out could cause this, but it's notable that we expect ~1 month in not that long, like only ~3 years at the current rate.\n\nYes, precisely. Last I checked, we expected scaling to run out by 2029ish, no?\n\nAh, reading the comments, I see you expect there to be some inertia... Okay, 2032 / 7 more years would put us at \">1 year\" task horizons. That does make me a bit more concerned. (Though 80% reliability is several doublings behind, and I expect tasks that involve real-world messiness to be even further behind.)\n\n> Ok, but surely there has to be something they aren't getting better at (or are getting better at too slowly)\n\n\"Ability to come up with scientific innovations\" seems to be one.\n\nLike, I expect they are getting better at the underlying skill. If you had a benchmark which measures some toy version of \"produce scientific innovations\" (AidanBench?), and you plotted frontier models' performance on it against time, you would see the number going up. But it currently seems to lag way behind other capabilities, and I likewise don't expect it to reach dangerous heights before scaling runs out.\n\nThe way I would put it, the things LLMs are strictly not improving on are not \"specific types of external tasks\". What I think they're not getting better at – because it's something they've never been capable of doing – are specific cognitive algorithms which allow to complete certain cognitive tasks in a dramatically more compute-efficient manner. We've talked about this some before.\n\nI think that, in the limit of scaling, the LLM paradigm is equivalent to AGI, but that it's not a very efficient way to approach this limit. And it's less efficient along some dimens"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-19T19:28:20.599Z",
    "af": false
  },
  {
    "_id": "B4hrtjPY5vTbPAuSA",
    "postId": "RnKmRusmFpw7MhPYw",
    "parentCommentId": "iAwEZCk5RWi5jKwjb",
    "topLevelCommentId": "iAwEZCk5RWi5jKwjb",
    "contents": {
      "markdown": "For reference, I'd also bet on 8+ task length (on METR's benchmark[^wqio3c7gnfs]) by 2027. Probably significantly earlier; maybe early 2026, or even end of this year. Would not be shocked if OpenAI's IMO-winning model already clears that.\n\nYou say you expect progress to stall at 4-16 hours because solving such problems would require AIs to develop sophisticated models of them. My guess is that you're using intuitions regarding at what task-lengths it would be necessary for a human. LLMs, however, are not playing by the same rules: where we might need a new model, they may be able to retrieve a stored template solution. I don't think we really have any idea at what task length this trick would stop working for them. I could see it being \"1 week\", or \"1 month\", or \">1 year\", or \"never\".\n\nI do expect \"<1 month\", though. Or rather, that even if the LLM architecture is able to support arbitrarily big templates, the scaling of data and compute will run out before this point; and then plausibly the investment and the talent pools would dry up as well (after LLMs betray everyone's hopes of AGI-completeness).\n\nNot sure what happens if we do get to \">1 year\", because on my model, LLMs might *still not become AGIs despite that*. Like, they would still be \"solvers of already solved problems\", except they'd be... able to solve... any problem in the convex hull of the problems any human ever solved in 1 year...? I don't know, that would be very weird; but things have already gone in very weird ways, and this is what the straightforward extrapolation of my current models says. (We do potentially die there.[^98fuxrn7q0u])\n\nAside: On my model, LLMs *are not on track to hit any walls*. They will keep getting better at the things they've been getting better at, at the same pace, for as long as the inputs to the process (compute, data, [data progress](https://www.beren.io/2025-08-02-Most-Algorithmic-Progress-is-Data-Progress/), algorithmic progress) keep scaling at the same rate. My expectation is instead that they're just *not going towards AGI*, so \"no walls in their way\" doesn't matter; and that they will run out of fuel before the cargo cult of them becomes Singularity-tier transformative.\n\n(Obviously this model may be wrong. I'm still [fluctuating around 80%](https://www.lesswrong.com/posts/oKAFFvaouKKEhbBPm/a-bear-case-my-predictions-regarding-ai-progress?commentId=wxKjcca3dDftdCkvM).)\n\n[^wqio3c7gnfs]: Recall that it uses unrealistically \"clean\" tasks and accepts unviable-in-practice solutions: the corresponding horizons for real-world problem-solving seem much shorter. As do the plausibly-much-more-meaningful 80%-completion horizons – this currently sits at 26 minute. (Something like 95%-completion horizons may actually be the most representative metric, though I assume there are some issues with estimating that.) \n\n[^98fuxrn7q0u]: Probably this way:We should pause to note that a Clippy² still doesn’t really think or plan. It’s not really conscious. It is just an unfathomably vast pile of numbers produced by mindless optimization starting from a small seed program that could be written on a few pages. [...] When it ‘plans’, it would be more accurate to say it fake-plans; when it ‘learns’, it fake-learns; when it ‘thinks’, it is just interpolating between memorized data points in a high-dimensional space, and any interpretation of such fake-thoughts as real thoughts is highly misleading; when it takes ‘actions’, they are fake-actions optimizing a fake-learned fake-world, and are not real actions, any more than the people in a simulated rainstorm really get wet, rather than fake-wet. (The deaths, however, are real.)",
      "plaintextDescription": "For reference, I'd also bet on 8+ task length (on METR's benchmark[1]) by 2027. Probably significantly earlier; maybe early 2026, or even end of this year. Would not be shocked if OpenAI's IMO-winning model already clears that.\n\nYou say you expect progress to stall at 4-16 hours because solving such problems would require AIs to develop sophisticated models of them. My guess is that you're using intuitions regarding at what task-lengths it would be necessary for a human. LLMs, however, are not playing by the same rules: where we might need a new model, they may be able to retrieve a stored template solution. I don't think we really have any idea at what task length this trick would stop working for them. I could see it being \"1 week\", or \"1 month\", or \">1 year\", or \"never\".\n\nI do expect \"<1 month\", though. Or rather, that even if the LLM architecture is able to support arbitrarily big templates, the scaling of data and compute will run out before this point; and then plausibly the investment and the talent pools would dry up as well (after LLMs betray everyone's hopes of AGI-completeness).\n\nNot sure what happens if we do get to \">1 year\", because on my model, LLMs might still not become AGIs despite that. Like, they would still be \"solvers of already solved problems\", except they'd be... able to solve... any problem in the convex hull of the problems any human ever solved in 1 year...? I don't know, that would be very weird; but things have already gone in very weird ways, and this is what the straightforward extrapolation of my current models says. (We do potentially die there.[2])\n\nAside: On my model, LLMs are not on track to hit any walls. They will keep getting better at the things they've been getting better at, at the same pace, for as long as the inputs to the process (compute, data, data progress, algorithmic progress) keep scaling at the same rate. My expectation is instead that they're just not going towards AGI, so \"no walls in their way\" doesn't matter; "
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-18T22:39:55.688Z",
    "af": false
  },
  {
    "_id": "h3ob8gPzSX8P4ctvW",
    "postId": "2xHhe4EBHAFofkQJf",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> But what is feasible is to send the messages simply claiming that we are sentient, we want to be reconstructed\n\nClever. Issue: Any signal we send may only intersect the expanding frontier of the nearest alien civilization [0.2-1 billion years later](https://grabbyaliens.com/paper); i. e., at a distance 0.2-2 billion light-years away. Can we send a signal that could actually be picked up?\n\nLet's suppose the alien civilization is nice, vast (spans many galaxies by the time the signal reaches it), and is *specifically* listening for these types of \"reconstruct-us\" messages, sent from distant low-tech low-power civilizations facing extinction/genocide. Intuitively, it should be relatively cheap for them to build coordinated arrays of receivers spanning galaxy-sized volumes, with a correspondingly ridiculous effective surface area. We could further assume there's a civilization like this in any direction we could point.[^mjo85sla8pg]\n\nSuppose we had $10 million to spend on the project, and had 2 years until we're unable to broadcast. How much information would we be able to transmit?\n\nI don't know how to calculate it off the top of my head, so I went and asked LLMs. Their answers varied wildly between models and samples, but they usually said that ~100 GB was achievable. [Here](https://chatgpt.com/share/68a2530c-ee9c-800a-af17-2179c4999f97)'s one of GPT-5's answers; I'd love if someone sanity-checked that.\n\nThat should be enough information to reconstruct us, I think, if we use dense compression and send high-quality data? (Not reconstructing any specific person, of course, but the general shape of the human mind and human values.)\n\nNeat. This is the only desperate Hail Mary plan I've heard that actually sounds plausible. I'd donate a modest amount to that.\n\n(Aside: [relevant Sam Hughes short story](https://qntm.org/transi).)\n\n[^mjo85sla8pg]: This isn't all that many assumptions. It's basically just \"the grabby-alien model is approximately correct\" and \"probability that any given alien civilization is nice\". If sending such messages is feasible, the nice alien civilizations are pretty likely to be listening to them.",
      "plaintextDescription": "> But what is feasible is to send the messages simply claiming that we are sentient, we want to be reconstructed\n\nClever. Issue: Any signal we send may only intersect the expanding frontier of the nearest alien civilization 0.2-1 billion years later; i. e., at a distance 0.2-2 billion light-years away. Can we send a signal that could actually be picked up?\n\nLet's suppose the alien civilization is nice, vast (spans many galaxies by the time the signal reaches it), and is specifically listening for these types of \"reconstruct-us\" messages, sent from distant low-tech low-power civilizations facing extinction/genocide. Intuitively, it should be relatively cheap for them to build coordinated arrays of receivers spanning galaxy-sized volumes, with a correspondingly ridiculous effective surface area. We could further assume there's a civilization like this in any direction we could point.[1]\n\nSuppose we had $10 million to spend on the project, and had 2 years until we're unable to broadcast. How much information would we be able to transmit?\n\nI don't know how to calculate it off the top of my head, so I went and asked LLMs. Their answers varied wildly between models and samples, but they usually said that ~100 GB was achievable. Here's one of GPT-5's answers; I'd love if someone sanity-checked that.\n\nThat should be enough information to reconstruct us, I think, if we use dense compression and send high-quality data? (Not reconstructing any specific person, of course, but the general shape of the human mind and human values.)\n\nNeat. This is the only desperate Hail Mary plan I've heard that actually sounds plausible. I'd donate a modest amount to that.\n\n(Aside: relevant Sam Hughes short story.)\n\n 1. ^\n    This isn't all that many assumptions. It's basically just \"the grabby-alien model is approximately correct\" and \"probability that any given alien civilization is nice\". If sending such messages is feasible, the nice alien civilizations are pretty likely to be listening to t"
    },
    "baseScore": 6,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-08-17T22:10:42.169Z",
    "af": false
  },
  {
    "_id": "mCiqJstKHfEi8WJty",
    "postId": "vC97fzSkA5zoy6aDJ",
    "parentCommentId": "MK4DxTpKJgdvWEsf5",
    "topLevelCommentId": "MK4DxTpKJgdvWEsf5",
    "contents": {
      "markdown": "Have you considered the following concern?:\n\nWhile this might prevent models from giving instructions on e. g. building a bomb in response to a literal \"how do I build a bomb?\" prompt, they may still be able to infer bomb-building instructions from first principles using their general knowledge in response to e. g. \"how do I build a compact device that omnidirectionally releases a lot of energy in under 1 sec?\". More generally, while this removes the explicit knowledge of what *counts as* a biothreat/bomb/etc., it doesn't necessarily prevent the model from *reconstructing* that knowledge. The only way to prevent that would be to hobble the models' general capabilities: either removing *all* physics/chemistry knowledge, or hobbling their general reasoning/problem-solving skills (to remove their ability to do clever reconstructions).\n\nNow, arguably, LLMs may be forever unable to actually do this sort of \"reconstruction\", since it's an innovation-like task and they've been pretty bad at this so far. But:\n\n*   This is obviously a concern if I'm wrong and LLMs *are* AGI-complete. In this case, they would eventually attain the skills for doing this kind of reconstruction, and only the aforementioned drastic hobbling would prevent it.\n*   This might be less innovation-flavoured than it seems, because general-knowledge datasets might still contain the \"shadows\" of the harmful knowledge, or disparate pieces of it, such that its reconstruction is easy. (E. g., general physics knowledge + various scientific experiments and engineering principles + fiction and news articles which mention bombs.)\n\nSo: have you tried asking for instructions \"indirectly\"?\n\n(Though even if this suffices to prevent reconstruction in the relatively dumb models of today, it might not work for tomorrow's cleverer reasoners.\n\nThe core issue is the same as in e. g. [Deep Deceptiveness](https://www.lesswrong.com/posts/XWwvwytieLtEWaFJX/deep-deceptiveness): the circuits the model would use to reconstruct harmful knowledge are *the same* circuits that make it useful in other contexts, so you can't have one without the other.)",
      "plaintextDescription": "Have you considered the following concern?:\n\nWhile this might prevent models from giving instructions on e. g. building a bomb in response to a literal \"how do I build a bomb?\" prompt, they may still be able to infer bomb-building instructions from first principles using their general knowledge in response to e. g. \"how do I build a compact device that omnidirectionally releases a lot of energy in under 1 sec?\". More generally, while this removes the explicit knowledge of what counts as a biothreat/bomb/etc., it doesn't necessarily prevent the model from reconstructing that knowledge. The only way to prevent that would be to hobble the models' general capabilities: either removing all physics/chemistry knowledge, or hobbling their general reasoning/problem-solving skills (to remove their ability to do clever reconstructions).\n\nNow, arguably, LLMs may be forever unable to actually do this sort of \"reconstruction\", since it's an innovation-like task and they've been pretty bad at this so far. But:\n\n * This is obviously a concern if I'm wrong and LLMs are AGI-complete. In this case, they would eventually attain the skills for doing this kind of reconstruction, and only the aforementioned drastic hobbling would prevent it.\n * This might be less innovation-flavoured than it seems, because general-knowledge datasets might still contain the \"shadows\" of the harmful knowledge, or disparate pieces of it, such that its reconstruction is easy. (E. g., general physics knowledge + various scientific experiments and engineering principles + fiction and news articles which mention bombs.)\n\nSo: have you tried asking for instructions \"indirectly\"?\n\n(Though even if this suffices to prevent reconstruction in the relatively dumb models of today, it might not work for tomorrow's cleverer reasoners.\n\nThe core issue is the same as in e. g. Deep Deceptiveness: the circuits the model would use to reconstruct harmful knowledge are the same circuits that make it useful in other contexts, so y"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-13T05:27:31.644Z",
    "af": false
  },
  {
    "_id": "cvckvDjBZGxCc8AL7",
    "postId": "qGMonyLRXFRnCWSj6",
    "parentCommentId": "rCbNeAAt5HYJ5HGws",
    "topLevelCommentId": "xsdEwRnRZKbzXnibT",
    "contents": {
      "markdown": "Potential counterargument: That kinda hurts you precisely in the timelines in which you may have large impact, no?\n\nI mean, I guess it's correct if you *don't* have high ambitions, or if you're very strongly confident that your pathways of realizing those ambitions won't route through becoming publicly well-known. But if you're ultimately aiming high, and are leaving yourself the option to be opportunistic about how to do it (rather than committing to a specific path from the start), being open like this early may have significant negative effects for you later on. (Impacting your ability to acquire, compound, or wield power.)\n\nI'm instinctively tempted to reject this counterargument, but I think my instinctive rejection is invalid. It seems to be rooted in the feelings of \"how is cyberbullying real, just walk away from the screen\" and \"if you're leading your life in a way where you're at the mercy of the vicissitudes of the public opinion, you're doing *something* wrong\". The latter is a preference over paths-to-power, and while paths that route through publicity are dispreferred by me, they're still valid. The former fails to take into account that, on some paths, your career opportunities/ability to make business deals/whether VCs would fund you/etc. depend on whether *other* people (or corporations/other entities) believe that cyberbullying is real.\n\nSo I think it's a fine strategy if you're a hermit researcher aiming to solve alignment and apply the solution to a brain in a box in a basement, or if you don't have big plans to begin with. But maybe not so good if you're aiming for policy/advocacy/fieldbuilding/etc.",
      "plaintextDescription": "Potential counterargument: That kinda hurts you precisely in the timelines in which you may have large impact, no?\n\nI mean, I guess it's correct if you don't have high ambitions, or if you're very strongly confident that your pathways of realizing those ambitions won't route through becoming publicly well-known. But if you're ultimately aiming high, and are leaving yourself the option to be opportunistic about how to do it (rather than committing to a specific path from the start), being open like this early may have significant negative effects for you later on. (Impacting your ability to acquire, compound, or wield power.)\n\nI'm instinctively tempted to reject this counterargument, but I think my instinctive rejection is invalid. It seems to be rooted in the feelings of \"how is cyberbullying real, just walk away from the screen\" and \"if you're leading your life in a way where you're at the mercy of the vicissitudes of the public opinion, you're doing something wrong\". The latter is a preference over paths-to-power, and while paths that route through publicity are dispreferred by me, they're still valid. The former fails to take into account that, on some paths, your career opportunities/ability to make business deals/whether VCs would fund you/etc. depend on whether other people (or corporations/other entities) believe that cyberbullying is real.\n\nSo I think it's a fine strategy if you're a hermit researcher aiming to solve alignment and apply the solution to a brain in a box in a basement, or if you don't have big plans to begin with. But maybe not so good if you're aiming for policy/advocacy/fieldbuilding/etc."
    },
    "baseScore": 21,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2025-08-13T04:40:45.950Z",
    "af": false
  },
  {
    "_id": "ReqKjsqDZBcvxeGZG",
    "postId": "rP9nPwg7gRe5mTSDz",
    "parentCommentId": "JnyxsnneimpGaRgRH",
    "topLevelCommentId": "JnyxsnneimpGaRgRH",
    "contents": {
      "markdown": "I've been relatively skeptical of the whole 4o-psychosis thing (specifically, about its effect size), but the public outcry about 4o's shutdown, and [stuff like this](https://x.com/Devon_OnEarth/status/1954642409834389715), are tiding me over to \"this is an actual serious problem\".\n\nLike, the psychosis cases are just the tip of the iceberg. There are vast volumes of [social dark matter](https://www.lesswrong.com/posts/KpMNqA5BiCRozCwM3/social-dark-matter) of people who've become dependent on LLMs[^ia7i0f0rfr] yet know to hide it, and who haven't yet become so dysfunctional that they *can't* hide it. And while the effects in any *individual* case may be relatively minor, this has the potential to screw the society up even worse than social media, if LLMs slightly lift the craziness level of a median person and this has compounding effects. (In worlds where LLM use proliferates/they get integrated into apps everyone uses, with Meta et al. optimizing those integrated LLMs for precisely this sort of dependency-causing behavior.)\n\nI mean, it probably won't *actually* matter, because the world as we know it would end (one way or another) before this has significant effects. But man, the long-timeline LLM-plateau worlds are potentially fucked as well.\n\n[^ia7i0f0rfr]: In a counterfactual way where they otherwise would've been fine, or at least meaningfully finer.",
      "plaintextDescription": "I've been relatively skeptical of the whole 4o-psychosis thing (specifically, about its effect size), but the public outcry about 4o's shutdown, and stuff like this, are tiding me over to \"this is an actual serious problem\".\n\nLike, the psychosis cases are just the tip of the iceberg. There are vast volumes of social dark matter of people who've become dependent on LLMs[1] yet know to hide it, and who haven't yet become so dysfunctional that they can't hide it. And while the effects in any individual case may be relatively minor, this has the potential to screw the society up even worse than social media, if LLMs slightly lift the craziness level of a median person and this has compounding effects. (In worlds where LLM use proliferates/they get integrated into apps everyone uses, with Meta et al. optimizing those integrated LLMs for precisely this sort of dependency-causing behavior.)\n\nI mean, it probably won't actually matter, because the world as we know it would end (one way or another) before this has significant effects. But man, the long-timeline LLM-plateau worlds are potentially fucked as well.\n\n 1. ^\n    In a counterfactual way where they otherwise would've been fine, or at least meaningfully finer."
    },
    "baseScore": 12,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-12T04:11:14.485Z",
    "af": false
  },
  {
    "_id": "snfkDYnoQM7Rji79E",
    "postId": "ntELqTE47jyHPnH84",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Point is: it would be absolutely amazing for someone else to have my back, when it comes to handling alignment of strong AI\n\n[Pffft:](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=7N9SLtnM9KmMAxaHM)\n\n> my ideal girlfriend is a genius alignment researcher on the level of von Neumann and Einstein",
      "plaintextDescription": "> Point is: it would be absolutely amazing for someone else to have my back, when it comes to handling alignment of strong AI\n\nPffft:\n\n> my ideal girlfriend is a genius alignment researcher on the level of von Neumann and Einstein"
    },
    "baseScore": 3,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-10T02:26:06.592Z",
    "af": false
  },
  {
    "_id": "QGBpvaeqWPnLbxpJf",
    "postId": "SuvWoLaGiNjPDcA7d",
    "parentCommentId": "oEq6wcKyaFcRp42H9",
    "topLevelCommentId": "h6smWDBHmDCzyTqZD",
    "contents": {
      "markdown": "I would've mapped \"distracted\" to \"starts to think about something else\", not \"thinks about nothing\". If mapping to human behavior, this is IMO more like \"mindlessly staring at a wall under mental fatigue\" or something.\n\nMore seriously, what this actually immediately reminded me of is [this paper](https://arxiv.org/abs/2404.15758):\n\n> **Let's Think Dot by Dot: Hidden Computation in Transformer Language Models**\n> \n> Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.\n\nI expect it's not one of the cases where the filler tokens can encode useful computations, but there *may* actually be some semantic meaning there.\n\n> No one is claiming GPT-5 is AGI, and also, these sorts of behaviors are compatible with AGI anyway, so they don't really provide much evidence one way or another.\n\n(That was intended as a tongue-in-check joke implying that executive-function problems are what qualifies a system as AGI.)",
      "plaintextDescription": "I would've mapped \"distracted\" to \"starts to think about something else\", not \"thinks about nothing\". If mapping to human behavior, this is IMO more like \"mindlessly staring at a wall under mental fatigue\" or something.\n\nMore seriously, what this actually immediately reminded me of is this paper:\n\n> Let's Think Dot by Dot: Hidden Computation in Transformer Language Models\n> \n> Chain-of-thought responses from language models improve performance across most benchmarks. However, it remains unclear to what extent these performance gains can be attributed to human-like task decomposition or simply the greater computation that additional tokens allow. We show that transformers can use meaningless filler tokens (e.g., '......') in place of a chain of thought to solve two hard algorithmic tasks they could not solve when responding without intermediate tokens. However, we find empirically that learning to use filler tokens is difficult and requires specific, dense supervision to converge. We also provide a theoretical characterization of the class of problems where filler tokens are useful in terms of the quantifier depth of a first-order formula. For problems satisfying this characterization, chain-of-thought tokens need not provide information about the intermediate computational steps involved in multi-token computations. In summary, our results show that additional tokens can provide computational benefits independent of token choice. The fact that intermediate tokens can act as filler tokens raises concerns about large language models engaging in unauditable, hidden computations that are increasingly detached from the observed chain-of-thought tokens.\n\nI expect it's not one of the cases where the filler tokens can encode useful computations, but there may actually be some semantic meaning there.\n\n> No one is claiming GPT-5 is AGI, and also, these sorts of behaviors are compatible with AGI anyway, so they don't really provide much evidence one way or another.\n\n(That was "
    },
    "baseScore": 16,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-08-09T23:17:26.406Z",
    "af": false
  },
  {
    "_id": "EYJgr99CHr3ZpjvcT",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Sooo, apparently OpenAI's mysterious breakthrough technique for generalizing RL to hard-to-verify domains that scored them IMO gold is just... \"use the LLM as a judge\"? Sources: [the main one](https://www.theinformation.com/articles/universal-verifiers-openais-secret-weapon) is paywalled, but [this](https://x.com/nrehiew_/status/1952574124351697316) seems to capture the main data, and you can also search for various crumbs [here](https://old.reddit.com/r/singularity/comments/1mhf0zj/openai_has_created_a_universal_verifier_to/) and [here](https://www.gurufocus.com/news/3033773/openais-gpt5-to-feature-breakthrough-universal-verifier-technology).\n\n> The technical details of how exactly the universal verifier works aren’t yet clear. Essentially, it involves tasking an LLM with the job of checking and grading another model’s answers by using various sources to research them.\n\nMy understanding is that they approximate an oracle verifier by an LLM with more compute and access to more information and tools, then train the model to be accurate by this approximate-oracle's lights.\n\nNow, it's possible that the journalists are completely misinterpreting the thing they're reporting on, or that it's all some galaxy-brained OpenAI op to mislead the competition. It's also possible that there's some incredibly clever trick for making it work much better than how it sounds like it'd work.\n\nBut if that's indeed the accurate description of the underlying reality, that's... kind of underwhelming. I'm curious how far this can scale, but I'm not feeling very threatened by it.\n\n(Haven't seen this discussed on LW, kudos to [@lwreader132](https://www.lesswrong.com/users/lwreader132?mention=user) for bringing it to my attention.)",
      "plaintextDescription": "Sooo, apparently OpenAI's mysterious breakthrough technique for generalizing RL to hard-to-verify domains that scored them IMO gold is just... \"use the LLM as a judge\"? Sources: the main one is paywalled, but this seems to capture the main data, and you can also search for various crumbs here and here.\n\n> The technical details of how exactly the universal verifier works aren’t yet clear. Essentially, it involves tasking an LLM with the job of checking and grading another model’s answers by using various sources to research them.\n\nMy understanding is that they approximate an oracle verifier by an LLM with more compute and access to more information and tools, then train the model to be accurate by this approximate-oracle's lights.\n\nNow, it's possible that the journalists are completely misinterpreting the thing they're reporting on, or that it's all some galaxy-brained OpenAI op to mislead the competition. It's also possible that there's some incredibly clever trick for making it work much better than how it sounds like it'd work.\n\nBut if that's indeed the accurate description of the underlying reality, that's... kind of underwhelming. I'm curious how far this can scale, but I'm not feeling very threatened by it.\n\n(Haven't seen this discussed on LW, kudos to @lwreader132 for bringing it to my attention.)"
    },
    "baseScore": 13,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-09T01:21:23.288Z",
    "af": true
  },
  {
    "_id": "h6smWDBHmDCzyTqZD",
    "postId": "SuvWoLaGiNjPDcA7d",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> We also observe inscrutable reasoning traces whose uninterpretable sections seem (on a surface reading) less likely to contain any particular semantic meaning.\n\nSo, um, not to anthropomorphize too much, but is it, like, dissociating under stress in that screencap or what?\n\nTruly, AGI achieved.",
      "plaintextDescription": "> We also observe inscrutable reasoning traces whose uninterpretable sections seem (on a surface reading) less likely to contain any particular semantic meaning.\n\nSo, um, not to anthropomorphize too much, but is it, like, dissociating under stress in that screencap or what?\n\nTruly, AGI achieved."
    },
    "baseScore": 9,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-08-08T11:54:17.530Z",
    "af": false
  },
  {
    "_id": "L2yg9k3bqNPQv6o6g",
    "postId": "Bvz3MFCuGdKaTh8ee",
    "parentCommentId": "NbFm8gZdA8EcJByLD",
    "topLevelCommentId": "9kzp7cq3cn4AbuDRT",
    "contents": {
      "markdown": "That would be my interpretation if I were to steelman him. My actual expectation is that he's lumping Eliezer-style positions with Yampolskiy-style positions, barely differentiating between them. Eliezer has certainly said things along the general lines of \"AGI can never be made aligned using the tools of the current paradigm\", backing it up by what could be called \"logical arguments\" from evolution or first principles.\n\nLike, Dario clearly disagrees with Eliezer's position as well, given who he is and what he is doing, so there must be some way he is dismissing it. And he is talking about \"doomers\" there, in general, yet Yampolskiy and Yampolskiy-style views are not the central AGI-doomsayer position. So why would he be talking about his anti-Yampolskiy views in the place where he should be talking about his anti-Eliezer views?\n\nMy guess is that it's because those views are one and the same. Alternatively, he deliberately chose to associate general AGI-doom arguments with a weak-man position he could dunk on, in a way that leaves him the opportunity to retreat to the motte of \"I actually meant Yampolskiy's views, oops, sorry for causing a misunderstanding\". Not sure which is worse.\n\nYes, his statement is clearly nonsensical if we read it as a dismissal of Eliezer's position, but it sure *sounded*, in-context, like he would've been referring to Eliezer's position there. So I expect the nonsense is because he's mischaracterizing (deliberately or not) that position; I'm not particularly inclined to search for complicated charitable interpretations.",
      "plaintextDescription": "That would be my interpretation if I were to steelman him. My actual expectation is that he's lumping Eliezer-style positions with Yampolskiy-style positions, barely differentiating between them. Eliezer has certainly said things along the general lines of \"AGI can never be made aligned using the tools of the current paradigm\", backing it up by what could be called \"logical arguments\" from evolution or first principles.\n\nLike, Dario clearly disagrees with Eliezer's position as well, given who he is and what he is doing, so there must be some way he is dismissing it. And he is talking about \"doomers\" there, in general, yet Yampolskiy and Yampolskiy-style views are not the central AGI-doomsayer position. So why would he be talking about his anti-Yampolskiy views in the place where he should be talking about his anti-Eliezer views?\n\nMy guess is that it's because those views are one and the same. Alternatively, he deliberately chose to associate general AGI-doom arguments with a weak-man position he could dunk on, in a way that leaves him the opportunity to retreat to the motte of \"I actually meant Yampolskiy's views, oops, sorry for causing a misunderstanding\". Not sure which is worse.\n\nYes, his statement is clearly nonsensical if we read it as a dismissal of Eliezer's position, but it sure sounded, in-context, like he would've been referring to Eliezer's position there. So I expect the nonsense is because he's mischaracterizing (deliberately or not) that position; I'm not particularly inclined to search for complicated charitable interpretations."
    },
    "baseScore": 17,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-04T15:03:30.753Z",
    "af": false
  },
  {
    "_id": "ftjvigikxbn6fJptx",
    "postId": "AT6yMXwPyf5EDebqa",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Recall how Putin has been \"putting nuclear forces on high alert\" over and over and over again since the start of the war, including during the initial events in February 2022. It never meant anything.\n\nI expect this is the exact same thing. Trump is just joining in on the posturing fun, because he's Putin-like in this regard. I feel fairly confident that neither Putin nor Trump will ever actually nuke over this conflict in its current shape, and you should feel free to ignore all of their nonsense.\n\nSome context here: I'm Russian and I pay some attention to Russian anti-Putin political analysts. I haven't gone in-depth on verifying their analyses, and there are obviously some biases there, but they offer a compelling perspective which I think is missing from Western views.\n\nWestern Kremlinologists, from what I've read, tend to take Putin's government seriously. They effectively *steelman* his decision-making. They try to figure out how various actions Russia takes may be in Russia's geopolitical interests, they're making predictions based on the expectation that Russia is going to act to advance those interests in the future, they talk about things as if the Russian government would make hard choices to protect said interests, et cetera. Whenever Russia does something stupid, they try to infer flaws in Putin's model of the world under which those actions make geopolitical sense, rather than assuming he did something geopolitically incoherent.\n\nThe perspective among Russian anti-Putin analysts is that the above is mostly false. Russia is *not* well-modeled as a serious-minded geopolitical agent. Russia's government is under Putin's near-total control (especially after the latest purges), and Putin has no priorities higher than holding onto his power and entertaining himself. He doesn't view himself as an extension of the Russian state whose interests he ultimately (or at least partially) pursues, there are no other factions with explicit decision-making power who can steer Russia's actions away from whatever catches Putin's fancy, and the processes that fueled Putin's rise to power were not processes that shaped him into a geopolitics-minded actor.\n\nRussian opposition often describes the current state of affairs as Russia being controlled by an organized crime group, with Putin the kingpin. This might indeed be a better model, rather than viewing them as a more traditional autocracy.\n\nPutin does like empire-building, and having power, and when other world leaders bow down to him and attempt to placate him. Inasmuch as those goals are aligned with Russia's geopolitical interests, Putin will pursue those interests. But only by coincidence. If some decision would hurt Putin's power or ego, he's never going to do it, no matter how much it would be in Russia's interests.\n\nFor example: Ending the war. Does the war threaten to crash Russia's economy? Does it make Russia fatally dependent on the whims of e. g. China? Sure. But ending the war is guaranteed to return home a ton of disaffected people with combat experience, and turn the world's spotlights away from Putin. That threatens his power and fun, so he won't do it, no matter how good of a deal he is offered. (And I'm given to understand Trump was offering quite a good deal.) He didn't expect Ukraine to turn out the way it did (it was probably intended as a Short Victorious War to boost ratings at home in advance of the 2024 elections[^n473ni4ng3]); but now that he's settled into it, he does not *want* an exit strategy.\n\nSimilarly, threatening to nuke is a pretty good action, by Putin's utility function. It freaks the Western politicians out, they start calling him attempting to placate him, great time is had.\n\nBut *actually* nuking, especially someone who may nuke back? Hell no. That might lead to Putin's death. He will never do it. (I guess one exception may be a situation where he thinks he'll be killed anyway, e. g., if there's an actual march-on-Moscow ground invasion of Russia. But maybe not even then. **Edit:** A plausible alternate outcome is that he'll flee to a hidden bunker somewhere and attempt to harness every resource at his disposal to stall/wait it out, hoping the situation would somehow eventually progress in some way that gives him the opportunity to recover. \"Stall for opportunity\" seems to be his go-to strategy.)\n\nAnd Trump proactively escalating to a nuclear exchange from his end seems even less likely.\n\nAgain, I'm not *confident* this model is correct, I haven't ran comparative analyses of this vs. take-Russia-seriously Western models. But it seems solid to me, and is more postdictive of e. g. how rotten Russia's industrial-military complex turned out to actually be.\n\n[^n473ni4ng3]: Russian elections are not real, but: (1) the election days can serve as Schelling points for public discontent which may be harnessed by defecting elites in an attempt to depose Putin, (2) the Simulacrum Level 3 appearance of having the mandate still needs to be maintained. So it still makes sense to prepare for them.",
      "plaintextDescription": "Recall how Putin has been \"putting nuclear forces on high alert\" over and over and over again since the start of the war, including during the initial events in February 2022. It never meant anything.\n\nI expect this is the exact same thing. Trump is just joining in on the posturing fun, because he's Putin-like in this regard. I feel fairly confident that neither Putin nor Trump will ever actually nuke over this conflict in its current shape, and you should feel free to ignore all of their nonsense.\n\nSome context here: I'm Russian and I pay some attention to Russian anti-Putin political analysts. I haven't gone in-depth on verifying their analyses, and there are obviously some biases there, but they offer a compelling perspective which I think is missing from Western views.\n\nWestern Kremlinologists, from what I've read, tend to take Putin's government seriously. They effectively steelman his decision-making. They try to figure out how various actions Russia takes may be in Russia's geopolitical interests, they're making predictions based on the expectation that Russia is going to act to advance those interests in the future, they talk about things as if the Russian government would make hard choices to protect said interests, et cetera. Whenever Russia does something stupid, they try to infer flaws in Putin's model of the world under which those actions make geopolitical sense, rather than assuming he did something geopolitically incoherent.\n\nThe perspective among Russian anti-Putin analysts is that the above is mostly false. Russia is not well-modeled as a serious-minded geopolitical agent. Russia's government is under Putin's near-total control (especially after the latest purges), and Putin has no priorities higher than holding onto his power and entertaining himself. He doesn't view himself as an extension of the Russian state whose interests he ultimately (or at least partially) pursues, there are no other factions with explicit decision-making power who can ste"
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-08-02T20:07:04.800Z",
    "af": false
  },
  {
    "_id": "g9c5HR27GmmuDCP7z",
    "postId": "tmWMuY5HCSNXXZ9oq",
    "parentCommentId": "qDHTCCwhWmwupnJYM",
    "topLevelCommentId": "qDHTCCwhWmwupnJYM",
    "contents": {
      "markdown": "I second [@Seth Herd](https://www.lesswrong.com/users/seth-herd?mention=user)'s suggestion, I'm interested in your vision regarding how success would look like. Not just \"here's a list of some initiatives and research programs that should be helpful\" or \"here's a possible optimistic scenario in which things go well, but which we don't actually believe in\", but the sketch of an actual end-to-end plan around which you'd want people to coordinate. (Under the understanding that plans are worthless but planning is everything, of course.)",
      "plaintextDescription": "I second @Seth Herd's suggestion, I'm interested in your vision regarding how success would look like. Not just \"here's a list of some initiatives and research programs that should be helpful\" or \"here's a possible optimistic scenario in which things go well, but which we don't actually believe in\", but the sketch of an actual end-to-end plan around which you'd want people to coordinate. (Under the understanding that plans are worthless but planning is everything, of course.)"
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-31T21:28:32.258Z",
    "af": false
  },
  {
    "_id": "JwvLAcHnkSSNMyQix",
    "postId": "KJh2xckmCNAggisuq",
    "parentCommentId": "ZEdXy2pzMtpJCt4Hn",
    "topLevelCommentId": "ZEdXy2pzMtpJCt4Hn",
    "contents": {
      "markdown": "I think I have [a thing](https://www.lesswrong.com/posts/xPrL2xF9iYWpPmu6B/my-empathy-is-rarely-kind?commentId=mQE5mvmfkvcH8A3WP) very similar to John's here, and for me at least, it's mostly orthogonal to \"how much you care about this person's well-being\". Or, like, as relevant for that as whether that person has a likeable character trait.\n\nThe main impact is on the ability to coordinate with/trust/relax around that person. If they're well-modeled as an agent, you can, to wit, model them as a game-theoretic agent: as someone who is going to pay attention to the relevant parts of any given situation and continually make choices within it that are consistent with the pursuit of some goal. They may make mistakes, but those would be well-modeled as the foibles of being a bounded agent.\n\nOn the other hand, people who *can't* be modeled as agents (in a given context) can't be expected to behave in this way. They may make decisions based on irrelevant parts of the situations, act in inconsistent ways, and can't be trusted not to go careening in some random direction in response to random stimuli. Sort of like, ahem, an LLM.\n\nNote that I think it isn't a binary \"There Are Two Types of People\" thing: the same person can act as an agent in some contexts and fail at this in others. That said, there *is* a spectrum of \"in how many contexts does this person act as an agent?\", and meaningful clustering around \"not very many\", \"increasingly many\", etc.\n\nBy itself, this is mostly unrelated to how much I care about a given person for the purposes of e. g. wanting their life to be better. (Like, I don't think non-agent-approximating people have less qualia or something.)\n\nThis *is* relevant for the purposes of who I would want to be friends/allies/colleagues with, for the straightforward reason of \"people who are more well-modeled as coherent agents are more reliable allies\", and also because it's a character trait I like.",
      "plaintextDescription": "I think I have a thing very similar to John's here, and for me at least, it's mostly orthogonal to \"how much you care about this person's well-being\". Or, like, as relevant for that as whether that person has a likeable character trait.\n\nThe main impact is on the ability to coordinate with/trust/relax around that person. If they're well-modeled as an agent, you can, to wit, model them as a game-theoretic agent: as someone who is going to pay attention to the relevant parts of any given situation and continually make choices within it that are consistent with the pursuit of some goal. They may make mistakes, but those would be well-modeled as the foibles of being a bounded agent.\n\nOn the other hand, people who can't be modeled as agents (in a given context) can't be expected to behave in this way. They may make decisions based on irrelevant parts of the situations, act in inconsistent ways, and can't be trusted not to go careening in some random direction in response to random stimuli. Sort of like, ahem, an LLM.\n\nNote that I think it isn't a binary \"There Are Two Types of People\" thing: the same person can act as an agent in some contexts and fail at this in others. That said, there is a spectrum of \"in how many contexts does this person act as an agent?\", and meaningful clustering around \"not very many\", \"increasingly many\", etc.\n\nBy itself, this is mostly unrelated to how much I care about a given person for the purposes of e. g. wanting their life to be better. (Like, I don't think non-agent-approximating people have less qualia or something.)\n\nThis is relevant for the purposes of who I would want to be friends/allies/colleagues with, for the straightforward reason of \"people who are more well-modeled as coherent agents are more reliable allies\", and also because it's a character trait I like."
    },
    "baseScore": 12,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-07-31T18:21:37.910Z",
    "af": false
  },
  {
    "_id": "mQE5mvmfkvcH8A3WP",
    "postId": "xPrL2xF9iYWpPmu6B",
    "parentCommentId": "RCkazt4StCoGQYJnw",
    "topLevelCommentId": "7YscAPfjLYKdMP9qh",
    "contents": {
      "markdown": "To expand on that...\n\nIn my mental ontology, there's a set of specific concepts and mental motions associated with *accountability*: viewing people as being responsible for their actions, being disappointed in or impressed by their choices, modeling the assignment of blame/credit as meaningful operations. Implicitly, this requires modeling other people as *agents*: types of systems which are usefully modeled as having control over their actions. To me, this is a prerequisite for being able to truly connect with someone.\n\nWhen you apply the not-that-coherent-an-agent lens, you do lose that. Because, like, which parts of that person's cognition should you interpret as the agent making choices, and which as parts of the malfunctioning exoskeleton the agent has no control over? You can make *some* decision about that, but this is usually pretty arbitrary. If someone is best modeled like this, they're not well-modeled as an agent, and holding them accountable is a category error. They're a type of system that does what it does.\n\nYou can still invoke the social rituals of \"blame\" and \"responsibility\" if you expect that to change their behavior, but the mental experience of doing so is very different. It's more like calculating the nudges you need to make to prompt the desired mechanistic behavior, rather than as interfacing with a fellow person. In the latter case, you can sort of *relax*, communicate in a way focused on *transferring information,* instead of focusing on the form of communication, and trust them to make correct inferences. In the former case, you need to keep precise track of tone/wording/aesthetics/etc., and it's less \"communication\" and more \"optimization\".\n\nI *really* dislike thinking of people in this way, and I try to adopt the viewing-them-as-a-person frame whenever it's at all possible. But the other frame does unfortunately seem to be useful in many cases. Trying to do otherwise often feels like reaching out for someone's hand and finding nothing there.\n\nIf *this* is what you meant by viewing others as cats, yeah, that tracks.\n\n**Edit:** Oh, [nice timing](https://www.lesswrong.com/posts/KJh2xckmCNAggisuq/follow-up-to-my-empathy-is-rarely-kind).\n\n> On reflection, I think I suspend disbelief in the subject’s moral agency. \n\nYeah, that.",
      "plaintextDescription": "To expand on that...\n\nIn my mental ontology, there's a set of specific concepts and mental motions associated with accountability: viewing people as being responsible for their actions, being disappointed in or impressed by their choices, modeling the assignment of blame/credit as meaningful operations. Implicitly, this requires modeling other people as agents: types of systems which are usefully modeled as having control over their actions. To me, this is a prerequisite for being able to truly connect with someone.\n\nWhen you apply the not-that-coherent-an-agent lens, you do lose that. Because, like, which parts of that person's cognition should you interpret as the agent making choices, and which as parts of the malfunctioning exoskeleton the agent has no control over? You can make some decision about that, but this is usually pretty arbitrary. If someone is best modeled like this, they're not well-modeled as an agent, and holding them accountable is a category error. They're a type of system that does what it does.\n\nYou can still invoke the social rituals of \"blame\" and \"responsibility\" if you expect that to change their behavior, but the mental experience of doing so is very different. It's more like calculating the nudges you need to make to prompt the desired mechanistic behavior, rather than as interfacing with a fellow person. In the latter case, you can sort of relax, communicate in a way focused on transferring information, instead of focusing on the form of communication, and trust them to make correct inferences. In the former case, you need to keep precise track of tone/wording/aesthetics/etc., and it's less \"communication\" and more \"optimization\".\n\nI really dislike thinking of people in this way, and I try to adopt the viewing-them-as-a-person frame whenever it's at all possible. But the other frame does unfortunately seem to be useful in many cases. Trying to do otherwise often feels like reaching out for someone's hand and finding nothing there.\n\nIf t"
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-31T17:49:06.339Z",
    "af": false
  },
  {
    "_id": "PfLRSjj3M9ukQPLCT",
    "postId": "KYnM5ZRgaDA4isbbw",
    "parentCommentId": "JaaPjdyyMC6QJC8Cw",
    "topLevelCommentId": "wxoBqzLwe2M7KaiGN",
    "contents": {
      "markdown": "\"Not optimized to be convincing to AI researchers\" ≠ \"looks like fraud\". \"Optimized to be convincing to policymakers\" might involve research that clearly demonstrates some property of AIs/ML models which is basic knowledge for capability researchers (and for which they already came up with rationalizations why it's totally fine) but [isn't well-known outside specialist circles](https://xkcd.com/2501/).\n\nE. g., the basic example is the fact that ML models are black boxes trained by an autonomous process which we don't understand, instead of manually coded symbolic programs. This isn't as well-known outside ML communities as one might think, and non-specialists are frequently shocked when they properly understand that fact.",
      "plaintextDescription": "\"Not optimized to be convincing to AI researchers\" ≠ \"looks like fraud\". \"Optimized to be convincing to policymakers\" might involve research that clearly demonstrates some property of AIs/ML models which is basic knowledge for capability researchers (and for which they already came up with rationalizations why it's totally fine) but isn't well-known outside specialist circles.\n\nE. g., the basic example is the fact that ML models are black boxes trained by an autonomous process which we don't understand, instead of manually coded symbolic programs. This isn't as well-known outside ML communities as one might think, and non-specialists are frequently shocked when they properly understand that fact."
    },
    "baseScore": 1,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-31T15:56:51.599Z",
    "af": false
  },
  {
    "_id": "TCNH2mCGRxqgt2DLC",
    "postId": "KYnM5ZRgaDA4isbbw",
    "parentCommentId": "wxoBqzLwe2M7KaiGN",
    "topLevelCommentId": "wxoBqzLwe2M7KaiGN",
    "contents": {
      "markdown": "That seems like a pretty good idea!\n\n(There are projects that stress-test the assumptions behind AGI labs' plans, of course, but I don't think anyone is (1) deliberately picking at the plans AGI labs claim, in a basically adversarial manner, (2) optimizing experimental setups and results for legibility to policymakers, rather than for convincingness to other AI researchers. Explicitly setting those priorities might be useful.)",
      "plaintextDescription": "That seems like a pretty good idea!\n\n(There are projects that stress-test the assumptions behind AGI labs' plans, of course, but I don't think anyone is (1) deliberately picking at the plans AGI labs claim, in a basically adversarial manner, (2) optimizing experimental setups and results for legibility to policymakers, rather than for convincingness to other AI researchers. Explicitly setting those priorities might be useful.)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-31T13:06:43.131Z",
    "af": false
  },
  {
    "_id": "RCkazt4StCoGQYJnw",
    "postId": "xPrL2xF9iYWpPmu6B",
    "parentCommentId": "6ttPdQhii3syapaoE",
    "topLevelCommentId": "7YscAPfjLYKdMP9qh",
    "contents": {
      "markdown": "[@Caleb Biddulph](https://www.lesswrong.com/users/caleb-biddulph?mention=user)'s reply seems right to me. Another tack:\n\n> It's like the old \"dragon in the garage\" parable: the woman is too good at systematically denying the things which would actually help to not have a working model somewhere in there\n\nI think you're still imagining too coherent an agent. Yes, perhaps there is a slice through her mind that contains a working model which, if that model were dropped into the mind of a more coherent agent, could be used to easily comprehend and fix the situation. But this slice doesn't necessarily have executive conscious control at any given moment, and if it ever does, it isn't necessarily the same slice that contains her baseline/reflectively endorsed personality.\n\nE. g., perhaps, at any given moment, only *part* of that model is visible to the conscious mind, a 3D object sliding through a 2D plane, and the person can't really take in the whole of it at once, realize how ridiculous they're being, and act on it rationally. Or perhaps the thought of confronting the problem causes overwhelming distress due to malfunctioning emotional circuitry, and so do the thoughts of fixing that circuitry, in a way that recurses on itself indefinitely/in the style of TD learning. Or something else that's just as messy.\n\nHuman brains haven't solved self-interpretability, and human minds aren't arranged into even the approximate shape of coherent agents by default. Just because there's a module in there somewhere which implements a model of X doesn't mean the person can casually reach in and do things with that module.\n\n**Edit:** After reading your other responses, yeah, \"not modeling them as creatures particularly similar to yourself\" might just be the correct approach. I, uh, also don't find people with weak metacognitive skills particularly *relatable*.",
      "plaintextDescription": "@Caleb Biddulph's reply seems right to me. Another tack:\n\n> It's like the old \"dragon in the garage\" parable: the woman is too good at systematically denying the things which would actually help to not have a working model somewhere in there\n\nI think you're still imagining too coherent an agent. Yes, perhaps there is a slice through her mind that contains a working model which, if that model were dropped into the mind of a more coherent agent, could be used to easily comprehend and fix the situation. But this slice doesn't necessarily have executive conscious control at any given moment, and if it ever does, it isn't necessarily the same slice that contains her baseline/reflectively endorsed personality.\n\nE. g., perhaps, at any given moment, only part of that model is visible to the conscious mind, a 3D object sliding through a 2D plane, and the person can't really take in the whole of it at once, realize how ridiculous they're being, and act on it rationally. Or perhaps the thought of confronting the problem causes overwhelming distress due to malfunctioning emotional circuitry, and so do the thoughts of fixing that circuitry, in a way that recurses on itself indefinitely/in the style of TD learning. Or something else that's just as messy.\n\nHuman brains haven't solved self-interpretability, and human minds aren't arranged into even the approximate shape of coherent agents by default. Just because there's a module in there somewhere which implements a model of X doesn't mean the person can casually reach in and do things with that module.\n\nEdit: After reading your other responses, yeah, \"not modeling them as creatures particularly similar to yourself\" might just be the correct approach. I, uh, also don't find people with weak metacognitive skills particularly relatable."
    },
    "baseScore": 14,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2025-07-30T17:51:01.487Z",
    "af": false
  },
  {
    "_id": "7YscAPfjLYKdMP9qh",
    "postId": "xPrL2xF9iYWpPmu6B",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> When I try to empathize with that woman, what I feel toward her is disgust. If I were in her shoes, I would immediately jump to getting rid of the damn nail, it wouldn’t even occur to me to *not* fix it. \n\nYou may not be doing enough of putting yourself into her shoes. Specifically, you seem to be putting yourself into her *material circumstances*, as if you switched minds (and got her memories et cetera), instead of, like... imagining yourself also having her world-model and set of crystallized-intelligence heuristics and cognitive-bandwidth limitations.[^1fqilrbr77y] Putting your *inner homunculus* in the place of *her* inner homunculus.\n\nOne of the complications in agent foundations is that a bounded agent's *effective* action-space is much more limited than its physical action-space. There is, for example, a sequence of keystrokes you can execute right now that would result in your outputting the code of an aligned AGI. This is an action that is technically available to \"you\" – but it's obviously not available to *you* in any relevant sense. The failure to model that would be a failure.\n\nIt's subtler in cases where, like, \"some part of the person knows what they need to do to fix their situation but they can't bring themselves to admit it and do it\", or whatever. It's not as straightforward as directly missing knowledge; not isomorphic to lacking the key to an encrypted file. But I think it's directionally similar: they're not making decisions in the decision-theoretic landscape you model them as occupying.\n\nFrom the perspective of someone with a nail stuck in their head, the world does not *look* like there's a nail stuck in their head which they could easily remove in order to improve their life in ways in which they want it to be improved. The world looks like a confused muddle, a constantly shifting dreamscape. They're not sure whether the nail is real, whether the nail can be removed, whether removing the nail would help, whether removing the nail is what they want. Their models of all of those things also keep wildly oscillating on a day-to-day basis: they can't act as a coherent agent over long horizons because neither their world-model nor their desires are *stable*. And even when they happen to be in the \"removing the nail will fix it!\" mental state, they're often just *unable to make their body execute those motions*: they lack the willpower/executive function/whatever-mental-resource to act on such plans, even when they manage to form them. Which, I think, is closer to \"being unable to raise your arm because your muscles are crucially fatigued\" rather than to \"a personal choice/failure\".\n\nThey're best modeled not as agents who are being willfully obstinate, but as people helplessly trapped in the cognitive equivalents of malfunctioning motorized exoskeletons.\n\nOr, another frame, which is pretty uncharitable but is probably directionally correct: imagine them as *yourself with brain damage*. You can't move from the brain-damaged mind-state to a pre-injury mind-state by a sequence of normal human mental motions. Similarly, it's not a set of actions *actually available* to these people, for them to \"snap out of it\" and become John-like.\n\n> Don’t go bullshitting me about how a kind and compassionate life of mediocrity is a “different kind of strength”\n\nOn that note: Based on what I wrote above, one might expect me to be imbued with some sort of heartfelt emotions-level universal love. But... no, not really.\n\nI do want everyone in the world to be happy and live good lives. Not only intellectually, I *feel* strongly about this. But those desires and feelings [don't route through emotional empathy](https://www.lesswrong.com/posts/vAsQNjW3gbiskP9Wf/not-by-empathy-alone?commentId=hA4CguPo4AZkxPB42) in me.\n\nOn the emotional level, I don't *like* most people. I find it easy to empathize with them in the deep way described above, but this does nothing to make me want to be *friends* with them. The \"enjoying cats' company\" mental motion doesn't come easily to me; my default emotions-level stance towards them is... disinterest.\n\nWhich is to say, doing empathy correctly indeed wouldn't necessarily make one *kinder*. It's possible to empathize with someone deeply enough to make all their faults understandable and their mistakes forgivable, and *still* not like them on a personal level.\n\nBut I do think you're failing to empathize deeply enough here.\n\n[^1fqilrbr77y]: Edit: Oh, didn't read the comments before posting, I see everyone already said that. I have more analogies though. :P",
      "plaintextDescription": "> When I try to empathize with that woman, what I feel toward her is disgust. If I were in her shoes, I would immediately jump to getting rid of the damn nail, it wouldn’t even occur to me to not fix it. \n\nYou may not be doing enough of putting yourself into her shoes. Specifically, you seem to be putting yourself into her material circumstances, as if you switched minds (and got her memories et cetera), instead of, like... imagining yourself also having her world-model and set of crystallized-intelligence heuristics and cognitive-bandwidth limitations.[1] Putting your inner homunculus in the place of her inner homunculus.\n\nOne of the complications in agent foundations is that a bounded agent's effective action-space is much more limited than its physical action-space. There is, for example, a sequence of keystrokes you can execute right now that would result in your outputting the code of an aligned AGI. This is an action that is technically available to \"you\" – but it's obviously not available to you in any relevant sense. The failure to model that would be a failure.\n\nIt's subtler in cases where, like, \"some part of the person knows what they need to do to fix their situation but they can't bring themselves to admit it and do it\", or whatever. It's not as straightforward as directly missing knowledge; not isomorphic to lacking the key to an encrypted file. But I think it's directionally similar: they're not making decisions in the decision-theoretic landscape you model them as occupying.\n\nFrom the perspective of someone with a nail stuck in their head, the world does not look like there's a nail stuck in their head which they could easily remove in order to improve their life in ways in which they want it to be improved. The world looks like a confused muddle, a constantly shifting dreamscape. They're not sure whether the nail is real, whether the nail can be removed, whether removing the nail would help, whether removing the nail is what they want. Their models "
    },
    "baseScore": 97,
    "voteCount": 59,
    "createdAt": null,
    "postedAt": "2025-07-30T13:18:54.313Z",
    "af": false
  },
  {
    "_id": "pXYtsKu5p9neEQTbt",
    "postId": "i7JSL5awGFcSRhyGF",
    "parentCommentId": "yGEtb5LCbJwur2RS5",
    "topLevelCommentId": "NeCAEzo4TjJagWgCj",
    "contents": {
      "markdown": "Well, an aligned Singularity would probably be relatively pleasant, since the entities fueling it would consider causing this sort of vast distress a negative and try to avoid it. Indeed, if you trust them not to drown you, there would be no need for this sort of frantic grasping-at-straws.\n\nAn *un*aligned Singularity would probably also be more pleasant, since the entities fueling it would likely try to make it *look* aligned, with the span of time between the treacherous turn and everyone dying likely being short.\n\nThis scenario covers a sort of \"neutral-alignment/non-controlled\" Singularity, where there's no specific superintelligent actor (or coalition) in control of the whole process, and it's instead guided by... market forces, I guess? With AGI labs continually releasing new models for private/corporate use, providing the tools/opportunities you can try to grasp to avoid drowning. I think this is roughly how things would go under \"mainstream\" models of AI progress (e. g., AI 2027). (I don't expect it to *actually* go this way, I don't think LLMs can power the Singularity.)",
      "plaintextDescription": "Well, an aligned Singularity would probably be relatively pleasant, since the entities fueling it would consider causing this sort of vast distress a negative and try to avoid it. Indeed, if you trust them not to drown you, there would be no need for this sort of frantic grasping-at-straws.\n\nAn unaligned Singularity would probably also be more pleasant, since the entities fueling it would likely try to make it look aligned, with the span of time between the treacherous turn and everyone dying likely being short.\n\nThis scenario covers a sort of \"neutral-alignment/non-controlled\" Singularity, where there's no specific superintelligent actor (or coalition) in control of the whole process, and it's instead guided by... market forces, I guess? With AGI labs continually releasing new models for private/corporate use, providing the tools/opportunities you can try to grasp to avoid drowning. I think this is roughly how things would go under \"mainstream\" models of AI progress (e. g., AI 2027). (I don't expect it to actually go this way, I don't think LLMs can power the Singularity.)"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-27T09:13:51.178Z",
    "af": false
  },
  {
    "_id": "sKuD7wpytQuEgCpAT",
    "postId": "TTzyky2EQfYapxvMn",
    "parentCommentId": "354bDaS4Jukij7xho",
    "topLevelCommentId": "354bDaS4Jukij7xho",
    "contents": {
      "markdown": "> Yes, it's competently executed\n\nIs it?\n\nIt certainly signals that the authors have a competent grasp of the AI industry and its mainstream models of what's happening. But is it actually *competent AI-policy work*, even under the e/acc agenda?\n\nMy impression is that no, it's not. It seems to live in an e/acc fanfic about a competent US racing to AGI, not in reality. It vaguely recommends doing a thousand things that would be nontrivial to execute if [the Eye of Sauron](https://www.lesswrong.com/posts/foM8SA3ftY94MGMq9/assessment-of-intelligence-agency-functionality-is-difficult#kzz9CWYAjzdWBhnxp) were looking directly at them, and the Eye is very much not doing that. On the contrary, the wider Trump administration is doing things that directly contradict the most key recommendations here (energy, chips, science, talent, \"American allies\"), and this document seems to pretend this isn't happening. A *politically effective* version of this document would have been written in a very different way; this one seems to be written mainly for entertainment/fantasizing purposes.\n\nLike, it demonstrates that the people tasked with thinking about AI in the Trump administration have a solid enough understanding of the AI industry to recognize which policies would accelerate capability research. But that understanding hadn't translated into capability-positive policy decisions so far. Is there reason to think this plan's publication is going to turn that around...?\n\nIs my take here wrong? I don't have much experience here, this is a strong opinion weakly held. (Addressing that question to [@Zvi](https://www.lesswrong.com/users/zvi?mention=user) as well.)",
      "plaintextDescription": "> Yes, it's competently executed\n\nIs it?\n\nIt certainly signals that the authors have a competent grasp of the AI industry and its mainstream models of what's happening. But is it actually competent AI-policy work, even under the e/acc agenda?\n\nMy impression is that no, it's not. It seems to live in an e/acc fanfic about a competent US racing to AGI, not in reality. It vaguely recommends doing a thousand things that would be nontrivial to execute if the Eye of Sauron were looking directly at them, and the Eye is very much not doing that. On the contrary, the wider Trump administration is doing things that directly contradict the most key recommendations here (energy, chips, science, talent, \"American allies\"), and this document seems to pretend this isn't happening. A politically effective version of this document would have been written in a very different way; this one seems to be written mainly for entertainment/fantasizing purposes.\n\nLike, it demonstrates that the people tasked with thinking about AI in the Trump administration have a solid enough understanding of the AI industry to recognize which policies would accelerate capability research. But that understanding hadn't translated into capability-positive policy decisions so far. Is there reason to think this plan's publication is going to turn that around...?\n\nIs my take here wrong? I don't have much experience here, this is a strong opinion weakly held. (Addressing that question to @Zvi as well.) "
    },
    "baseScore": 11,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-07-26T13:33:16.466Z",
    "af": false
  },
  {
    "_id": "h5cDRsXH9xSxJ3LPd",
    "postId": "ybHtafxQeaNFQJM22",
    "parentCommentId": "x5ekTHstQdsmzqFAq",
    "topLevelCommentId": "bEJmsPWdX3CtDqPCf",
    "contents": {
      "markdown": "Yeah, I figured.\n\n> If the judge sees that you are a $61 billion market cap company hiring the greatest lawyers in the world, but you're not putting forth your best legal foot when you have lawyers from other companies writing briefs outlining their own defense arguments, the consequences for you and your lawyers will be severe\n\nWhat would be the actual wrongdoing here, legally speaking?",
      "plaintextDescription": "Yeah, I figured.\n\n> If the judge sees that you are a $61 billion market cap company hiring the greatest lawyers in the world, but you're not putting forth your best legal foot when you have lawyers from other companies writing briefs outlining their own defense arguments, the consequences for you and your lawyers will be severe\n\nWhat would be the actual wrongdoing here, legally speaking?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-26T06:38:26.320Z",
    "af": false
  },
  {
    "_id": "Wv6CcvHYRwn7Fpm5J",
    "postId": "ybHtafxQeaNFQJM22",
    "parentCommentId": "bEJmsPWdX3CtDqPCf",
    "topLevelCommentId": "bEJmsPWdX3CtDqPCf",
    "contents": {
      "markdown": "Clearly the heroic thing to do would be to go to trial and then deliberately mess it up very badly in a calculated fashion that sets an awful precedent for the other AGI companies. You might say, \"but China!\", but if the US cripples itself, then suddenly the USG would be much more interested in reaching some sort of international-AGI-ban deal with China, so it all works out.\n\n(Only half-serious.)",
      "plaintextDescription": "Clearly the heroic thing to do would be to go to trial and then deliberately mess it up very badly in a calculated fashion that sets an awful precedent for the other AGI companies. You might say, \"but China!\", but if the US cripples itself, then suddenly the USG would be much more interested in reaching some sort of international-AGI-ban deal with China, so it all works out.\n\n(Only half-serious.)"
    },
    "baseScore": 3,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-07-25T22:21:24.436Z",
    "af": false
  },
  {
    "_id": "2hg3Pu8yNg3iuYYue",
    "postId": "DA3vbSEfABLdoCt59",
    "parentCommentId": "j9nsoCfceB5y3dcAh",
    "topLevelCommentId": "d82MwJ6oeCtAvrqsQ",
    "contents": {
      "markdown": "Yeah, I guess the use-case I had in mind is generally people who don't want LLMs trained on (particular pieces of) their writing, rather than datasets specifically.",
      "plaintextDescription": "Yeah, I guess the use-case I had in mind is generally people who don't want LLMs trained on (particular pieces of) their writing, rather than datasets specifically."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-25T16:53:19.693Z",
    "af": false
  },
  {
    "_id": "d82MwJ6oeCtAvrqsQ",
    "postId": "DA3vbSEfABLdoCt59",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Hmm. This approach relies partly on the AGI labs being cooperative and wary of violating the law, and partly on creating minor inconveniences for accessing the data which inconvenient human users as well. In addition, any data shared this way would have to be shared via the download portal, impoverishing the web experience.\n\nI wonder if it's possible to design some method of data protection that (1) would be deployable on arbitrary web pages, (2) would not burden human users, (3) would make AGI labs actively not *want* to scrape pages protected this way.\n\nHere's one obvious idea. It's pretty hostile and might potentially have net-negative results (for at least some goals), but I think it's worth discussing.\n\nWe could automatically seed the text with jailbreaks, spam, false information, and other low-quality/negative-quality training data, in a way that is invisible to users but visible to LLMs. [Pliny-style emoji bombs](https://x.com/elder_plinius/status/1891980067309342773), invisible/very tiny text, and other techniques along those lines, randomly inserted into the human-readable text between paragraphs or words.\n\nHow easy would it be for AGI labs to clean this up? Might be pretty easy, if we only have select few methods of hiding the text: then they can just automatically look for text within invisible tags/added via Unicode variation selectors, and remove it. But it might be possible to create a diverse, ever-growing family of text-hiding methods, such that static countermeasures don't work. Tasking an LLM with cleaning the document, instead of manually designed methods, might backfire, with the cleaner LLM itself getting jailbroken by the embedded cognitohazards.\n\nMaking the hidden text actually harmful seems easier: jailbreaks are invented faster than countermeasures against them, I think.\n\nIdeally, the whole setup would be continuously updated: instead of individual writers having to inject this stuff on their own, there would be a centralized public API or GitHub repo which web developers could use, embedding this functionality into websites. This centralized API/repo can then be continuously updated with new jailbreaks and counters to whatever countermeasures AGI labs come up with (AdBlock-style).  \n\nAgain, it's obviously pretty hostile, but if paired with a canary string, any actor that decides to ignore the canary string and scrape the page anyway deserves what they get.\n\nAny obvious reasons this is a bad idea which I'm missing? I guess the obvious failure mode is people deploying this without the canary string, meaning even cooperative AGI labs might accidentally train on the data poisoned this way. If the goal is to *prevent* training on bad data (because of e. g. misalignment concerns), that's obviously counterproductive.",
      "plaintextDescription": "Hmm. This approach relies partly on the AGI labs being cooperative and wary of violating the law, and partly on creating minor inconveniences for accessing the data which inconvenient human users as well. In addition, any data shared this way would have to be shared via the download portal, impoverishing the web experience.\n\nI wonder if it's possible to design some method of data protection that (1) would be deployable on arbitrary web pages, (2) would not burden human users, (3) would make AGI labs actively not want to scrape pages protected this way.\n\nHere's one obvious idea. It's pretty hostile and might potentially have net-negative results (for at least some goals), but I think it's worth discussing.\n\nWe could automatically seed the text with jailbreaks, spam, false information, and other low-quality/negative-quality training data, in a way that is invisible to users but visible to LLMs. Pliny-style emoji bombs, invisible/very tiny text, and other techniques along those lines, randomly inserted into the human-readable text between paragraphs or words.\n\nHow easy would it be for AGI labs to clean this up? Might be pretty easy, if we only have select few methods of hiding the text: then they can just automatically look for text within invisible tags/added via Unicode variation selectors, and remove it. But it might be possible to create a diverse, ever-growing family of text-hiding methods, such that static countermeasures don't work. Tasking an LLM with cleaning the document, instead of manually designed methods, might backfire, with the cleaner LLM itself getting jailbroken by the embedded cognitohazards.\n\nMaking the hidden text actually harmful seems easier: jailbreaks are invented faster than countermeasures against them, I think.\n\nIdeally, the whole setup would be continuously updated: instead of individual writers having to inject this stuff on their own, there would be a centralized public API or GitHub repo which web developers could use, embedding this fu"
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-25T07:27:15.390Z",
    "af": false
  },
  {
    "_id": "ZBXoovhi32SoYgRwe",
    "postId": "cGcwQDKAKbQ68BGuR",
    "parentCommentId": "tgFG6CT5CAcpDAmv6",
    "topLevelCommentId": "b5XYGBtqpBHXgaAqq",
    "contents": {
      "markdown": "> Of course, the *degree* of transmission does depend on the distillation distribution.\n\nYes, that's what makes it not particularly enlightening here, I think? The theorem says that the student moves in a direction that is at-worst-orthogonal towards the teacher – meaning \"orthogonal direction\" is the lower bound, right? And it's a pretty weak lower bound. (Or, a statement which I think is approximately equivalent, the student's post-distillation loss on the teacher's loss function is at-worst-equal to its pre-distillation loss.)\n\nAnother perspective: consider looking at this theorem without knowing the empirical result. Would you be able to predict this result from this theorem? I think not; I think the \"null hypothesis\" of \"if you train on those outputs of the teacher that have nothing to do with the changed feature, the student would move in an almost-orthogonal direction relative to it\" isn't ruled out by it. It doesn't interact with/showcase the feature-entangling dynamic.\n\n> Would be really cool to connect to SLT! Is there a particular result you think is related?\n\nNot sure, sorry, not well-versed in the SLT enough.",
      "plaintextDescription": "> Of course, the degree of transmission does depend on the distillation distribution.\n\nYes, that's what makes it not particularly enlightening here, I think? The theorem says that the student moves in a direction that is at-worst-orthogonal towards the teacher – meaning \"orthogonal direction\" is the lower bound, right? And it's a pretty weak lower bound. (Or, a statement which I think is approximately equivalent, the student's post-distillation loss on the teacher's loss function is at-worst-equal to its pre-distillation loss.)\n\nAnother perspective: consider looking at this theorem without knowing the empirical result. Would you be able to predict this result from this theorem? I think not; I think the \"null hypothesis\" of \"if you train on those outputs of the teacher that have nothing to do with the changed feature, the student would move in an almost-orthogonal direction relative to it\" isn't ruled out by it. It doesn't interact with/showcase the feature-entangling dynamic.\n\n> Would be really cool to connect to SLT! Is there a particular result you think is related?\n\nNot sure, sorry, not well-versed in the SLT enough."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-07-24T04:09:02.127Z",
    "af": false
  },
  {
    "_id": "DgrKMHdaBnXxAnRu4",
    "postId": "qC6owmPE3xpag3Wyi",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> OpenAI has [declared ChatGPT Agent as High in Biological and Chemical capabilities](https://x.com/OpenAI/status/1945904754443669659) under their Preparedness Framework\n\nHuh. They certainly say all the right things here, so this might be a minor positive update on OpenAI for me.\n\nOf course, the way it *sounds* and the way it *is* are entirely different things, and it's not clear yet whether the development of all these serious-sounding safeguards was approached with making things actually secure in mind, as opposed to safety-washing. E. g., are they actually going to stop anyone moderately determined?\n\nHm, it's been five minutes and it looks like there's no Pliny jailbreak yet. That's something. Maybe Pliny doesn't have access yet. (Edit: [Yep](https://x.com/elder_plinius/status/1947974765127323782).)",
      "plaintextDescription": "> OpenAI has declared ChatGPT Agent as High in Biological and Chemical capabilities under their Preparedness Framework\n\nHuh. They certainly say all the right things here, so this might be a minor positive update on OpenAI for me.\n\nOf course, the way it sounds and the way it is are entirely different things, and it's not clear yet whether the development of all these serious-sounding safeguards was approached with making things actually secure in mind, as opposed to safety-washing. E. g., are they actually going to stop anyone moderately determined?\n\nHm, it's been five minutes and it looks like there's no Pliny jailbreak yet. That's something. Maybe Pliny doesn't have access yet. (Edit: Yep.)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-23T16:11:37.560Z",
    "af": false
  },
  {
    "_id": "b5XYGBtqpBHXgaAqq",
    "postId": "cGcwQDKAKbQ68BGuR",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Fascinating. This is the sort of result that makes me curious about how LLMs work irrespective of their importance to any existential risks.\n\n> In the paper, we prove a theorem showing that a single, sufficiently small step of gradient descent on any teacher-generated output necessarily moves the student toward the teacher\n\nHmm, that theorem didn't seem like a very satisfying explanation to me. Unless I'm missing something, it doesn't actually imply anything about the student's features that are *seemingly unrelated* to the training distribution being moved towards the teacher's? It just says the student is moved towards the teacher, which, of course it is.\n\nIt seems to me that what this phenomenon implies is some sort of dimensionality collapse? That is, the reason why fine-tuning on the teacher's outputs about Feature A moves the student towards the teacher along the Feature-B axis as well is because the effective dimensionality of the space of LLM algorithms is smaller than the dimensionality of the parameter space, so moving it along A tends to drag the model along B as well?\n\nI'm not well-versed in singular learning theory, but I'm pretty sure it has [some related results](https://www.lesswrong.com/s/mqwA5FcL6SrHEQzox/p/fovfuFdpuEwQzJu2w). Perhaps its tools can be used to shed more light on this?",
      "plaintextDescription": "Fascinating. This is the sort of result that makes me curious about how LLMs work irrespective of their importance to any existential risks.\n\n> In the paper, we prove a theorem showing that a single, sufficiently small step of gradient descent on any teacher-generated output necessarily moves the student toward the teacher\n\nHmm, that theorem didn't seem like a very satisfying explanation to me. Unless I'm missing something, it doesn't actually imply anything about the student's features that are seemingly unrelated to the training distribution being moved towards the teacher's? It just says the student is moved towards the teacher, which, of course it is.\n\nIt seems to me that what this phenomenon implies is some sort of dimensionality collapse? That is, the reason why fine-tuning on the teacher's outputs about Feature A moves the student towards the teacher along the Feature-B axis as well is because the effective dimensionality of the space of LLM algorithms is smaller than the dimensionality of the parameter space, so moving it along A tends to drag the model along B as well?\n\nI'm not well-versed in singular learning theory, but I'm pretty sure it has some related results. Perhaps its tools can be used to shed more light on this?"
    },
    "baseScore": 15,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2025-07-23T10:09:06.084Z",
    "af": false
  },
  {
    "_id": "pBih9XiurpRDTDF4F",
    "postId": "ZkgaPopsBkQgeA2k8",
    "parentCommentId": "eGfX62vtQRfCrYrHJ",
    "topLevelCommentId": "eGfX62vtQRfCrYrHJ",
    "contents": {
      "markdown": "Also, it's funny that we laugh at xAI when they [say](https://x.com/xai/status/1943786243231559873) stuff like \"we anticipate Grok will uncover new physics and technology within 1-2 years\", but when an OpenAI employee goes \"I wouldn’t be surprised if by next year models will be deriving new theorems and contributing to original math research\", that's somehow more credible. Insert the \"know the work rules\" meme here.\n\n(FWIW, I consider both claims pretty unlikely but not laughably incredible.)",
      "plaintextDescription": "Also, it's funny that we laugh at xAI when they say stuff like \"we anticipate Grok will uncover new physics and technology within 1-2 years\", but when an OpenAI employee goes \"I wouldn’t be surprised if by next year models will be deriving new theorems and contributing to original math research\", that's somehow more credible. Insert the \"know the work rules\" meme here.\n\n(FWIW, I consider both claims pretty unlikely but not laughably incredible.)"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-22T23:00:06.468Z",
    "af": false
  },
  {
    "_id": "eGfX62vtQRfCrYrHJ",
    "postId": "ZkgaPopsBkQgeA2k8",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> The ‘barely speak English’ part makes the solution worse in some ways but actually makes me give their claims to be doing something different more credence rather than less\n\nI think people are overupdating on that. My impression is that gibberish like this is the default way RL makes models speak, and that they need to be separately fine-tuned to produce readable outputs. E. g., [the DeepSeek-R1 paper](https://arxiv.org/pdf/2501.12948) repeatedly complained about \"poor readability\" with regards to DeepSeek-R1-Zero (their cold-start no-SFT training run).\n\n> **Actually This Seems Like A Big Deal**\n> \n> If we take all of OpenAI’s statements at face value\n\nNitpick, but: I'm struck by how many times they mentioned the word \"general\". They seem to be trying to shoehorn it into every tweet. And it's *weird*. Like, *of course* it was done using a general system. That's, like, the default assumption? Whenever did OpenAI ship task-specific neurosymbolic systems, or whatever it is that would be the opposite of \"general\" here?\n\nI guess maybe it makes sense if they're assuming that people would be primed by DeepMind to expect math-specific systems here? It still seems kind of excessive. I buy that they have a model that really did score that well on the IMO benchmark, but the more times they remind us how very *general* it is, the more I'm prompted to think that maybe it's not quite *so* general after all.\n\nThere's also the desperate rush with which they started talking about it: *just* after the closing ceremony, seemingly to adhere to the most bare-bones technically-correct interpretation of \"don't steal the spotlight\", and apparently without even having any blog post/official announcement prepared, and without having decided what they're willing to reveal about their technical setup.\n\nI think it has all the telltale signs of being a heavily spun marketing op. There's truth at the core of it, yes, but I'm suspicious that all the \"reading between the lines\" that people are doing to spot the \"big implications\" here is precisely what the \"marketing op\" part of it was intended to cause. \n\n> Given that P3 was unusually easy this year\n\nNot only P3, apparently [all problems except P6](https://x.com/GregHBurnham/status/1947356600424890779/photo/1) were really easy this year. I don't think this is particularly load-bearing for anything, but seems like another worthwhile incremental update to make.",
      "plaintextDescription": "> The ‘barely speak English’ part makes the solution worse in some ways but actually makes me give their claims to be doing something different more credence rather than less\n\nI think people are overupdating on that. My impression is that gibberish like this is the default way RL makes models speak, and that they need to be separately fine-tuned to produce readable outputs. E. g., the DeepSeek-R1 paper repeatedly complained about \"poor readability\" with regards to DeepSeek-R1-Zero (their cold-start no-SFT training run).\n\n> Actually This Seems Like A Big Deal\n> \n> If we take all of OpenAI’s statements at face value\n\nNitpick, but: I'm struck by how many times they mentioned the word \"general\". They seem to be trying to shoehorn it into every tweet. And it's weird. Like, of course it was done using a general system. That's, like, the default assumption? Whenever did OpenAI ship task-specific neurosymbolic systems, or whatever it is that would be the opposite of \"general\" here?\n\nI guess maybe it makes sense if they're assuming that people would be primed by DeepMind to expect math-specific systems here? It still seems kind of excessive. I buy that they have a model that really did score that well on the IMO benchmark, but the more times they remind us how very general it is, the more I'm prompted to think that maybe it's not quite so general after all.\n\nThere's also the desperate rush with which they started talking about it: just after the closing ceremony, seemingly to adhere to the most bare-bones technically-correct interpretation of \"don't steal the spotlight\", and apparently without even having any blog post/official announcement prepared, and without having decided what they're willing to reveal about their technical setup.\n\nI think it has all the telltale signs of being a heavily spun marketing op. There's truth at the core of it, yes, but I'm suspicious that all the \"reading between the lines\" that people are doing to spot the \"big implications\" here is precise"
    },
    "baseScore": 13,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-07-22T22:42:55.520Z",
    "af": false
  },
  {
    "_id": "pWLb4ZhSLgkpdCb8e",
    "postId": "RcBqeJ8GHM2LygQK3",
    "parentCommentId": "ajToDqPvw53x7MHxh",
    "topLevelCommentId": "CayYH9BhaAGzqij2y",
    "contents": {
      "markdown": "> I think this is overall reasonable if you interpret \"hard-to-verify\" as \"substantially harder to verify\" and I think this probably how many people would read this by default\n\nNot sure about this. The kind of \"hard-to-verify\" I care about is e. g. agenty behavior in real-world conditions. I assume many other people are also watching out for that specifically, and that capability researchers are deliberately aiming for it.\n\nAnd I don't think the proofs are any evidence for that. The issue is that there exists, in principle, a way to easily verify math proofs: by translating them into a formal language and running them through a theorem-verifier. So the \"correct\" way for gradient descent to solve this was to encode some sort of internal theorem-verifier into the LLM.\n\nEven more broadly, we know that improved performance at IMO could be achieved by task-specific models (AlphaGeometry, AlphaProof), which means that much of the IMO benchmark is not a strong signal of general intelligence. A general intelligence can solve it, but one oughtn't be a general intelligence for that, and since gradient descent prefers shortcuts and shallow solutions...\n\nThey say they're not using task-specific methodology, but what does this actually mean? Does it mean they did not even RL the model on math-proof tasks, they RL'd it on something else and the gold-level IMO performance arose by transfer learning? Doubt it. Which means this observation doesn't really distinguish between \"the new technique is fully general and works in all domains\" and \"the new technique looks like it should generalize because of secret technical details, but it doesn't actually, it only worked here because it exploited the underlying easy-to-verify properties of the task\".",
      "plaintextDescription": "> I think this is overall reasonable if you interpret \"hard-to-verify\" as \"substantially harder to verify\" and I think this probably how many people would read this by default\n\nNot sure about this. The kind of \"hard-to-verify\" I care about is e. g. agenty behavior in real-world conditions. I assume many other people are also watching out for that specifically, and that capability researchers are deliberately aiming for it.\n\nAnd I don't think the proofs are any evidence for that. The issue is that there exists, in principle, a way to easily verify math proofs: by translating them into a formal language and running them through a theorem-verifier. So the \"correct\" way for gradient descent to solve this was to encode some sort of internal theorem-verifier into the LLM.\n\nEven more broadly, we know that improved performance at IMO could be achieved by task-specific models (AlphaGeometry, AlphaProof), which means that much of the IMO benchmark is not a strong signal of general intelligence. A general intelligence can solve it, but one oughtn't be a general intelligence for that, and since gradient descent prefers shortcuts and shallow solutions...\n\nThey say they're not using task-specific methodology, but what does this actually mean? Does it mean they did not even RL the model on math-proof tasks, they RL'd it on something else and the gold-level IMO performance arose by transfer learning? Doubt it. Which means this observation doesn't really distinguish between \"the new technique is fully general and works in all domains\" and \"the new technique looks like it should generalize because of secret technical details, but it doesn't actually, it only worked here because it exploited the underlying easy-to-verify properties of the task\"."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-22T21:23:20.810Z",
    "af": false
  },
  {
    "_id": "wyHh3QTKDFn4AFag5",
    "postId": "ZA8ZEggkqzHpoKiEm",
    "parentCommentId": "ATq5LA9tbo7PvegJj",
    "topLevelCommentId": "ATq5LA9tbo7PvegJj",
    "contents": {
      "markdown": "[Singular Learning Theory](https://www.alignmentforum.org/s/czrXjvCLsqGepybHC) and [Simplex's work](https://www.simplexaisafety.com/about) (e. g. [this](https://www.lesswrong.com/posts/gTZ2SxesbHckJ3CkF/transformers-represent-belief-state-geometry-in-their)), maybe? [Cartesian Frames](https://www.lesswrong.com/s/2A7rrZ4ySx6R8mfoT) and [Finite Factored Sets](https://www.lesswrong.com/posts/N5Jm6Nj4HkNKySA5Z/finite-factored-sets) might also work, but I'm less sure about those.\n\nIt's actually pretty hard to come up with agendas in the intersection of \"seems like an alignment-relevant topic it'd be useful to popularize\" and \"has complicated math which would be insightful and useful to visualize/simulate\".\n\n*   Natural abstractions, ARC's ELK, Shard Theory, and general embedded-agency theories are currently better understood by starting from the concepts, not the math.\n*   Infrabayesianism, Orthogonal's QACI, logical-induction stuff, and ARC's heuristical arguments seem too abstract to allow interesting visual modeling. (Maybe if you get really creative...)\n*   There are various alignment-relevant theorems and mathematical tools scattered around which could be interestingly visualized (e. g., my own [causal mirrors](https://www.lesswrong.com/posts/hx5wTeBSdf4bsYnY9/idealized-agents-are-approximate-causal-mirrors-radical)), but most of them are *niche* tools, so it's not obvious there's much value in popularizing them.\n\nAlso, not sure if that's a deal-breaker for you, but some important agendas are not technically \"about\" AI Safety at all, even if they resulted from people starting from the alignment problem and iteratively identifying various subproblems necessary for making progress on it. This process often moves you outside the field of AI. For example: natural abstractions, FFS, and heuristical arguments, which don't even centrally study *minds* at all.",
      "plaintextDescription": "Singular Learning Theory and Simplex's work (e. g. this), maybe? Cartesian Frames and Finite Factored Sets might also work, but I'm less sure about those.\n\nIt's actually pretty hard to come up with agendas in the intersection of \"seems like an alignment-relevant topic it'd be useful to popularize\" and \"has complicated math which would be insightful and useful to visualize/simulate\".\n\n * Natural abstractions, ARC's ELK, Shard Theory, and general embedded-agency theories are currently better understood by starting from the concepts, not the math.\n * Infrabayesianism, Orthogonal's QACI, logical-induction stuff, and ARC's heuristical arguments seem too abstract to allow interesting visual modeling. (Maybe if you get really creative...)\n * There are various alignment-relevant theorems and mathematical tools scattered around which could be interestingly visualized (e. g., my own causal mirrors), but most of them are niche tools, so it's not obvious there's much value in popularizing them.\n\nAlso, not sure if that's a deal-breaker for you, but some important agendas are not technically \"about\" AI Safety at all, even if they resulted from people starting from the alignment problem and iteratively identifying various subproblems necessary for making progress on it. This process often moves you outside the field of AI. For example: natural abstractions, FFS, and heuristical arguments, which don't even centrally study minds at all."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-22T15:16:37.752Z",
    "af": false
  },
  {
    "_id": "n9EHNfnr5Y2vpv5KP",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "oGBzGmrmNiv82iHMH",
    "topLevelCommentId": "GtEtFbXhPAHXzqPF4",
    "contents": {
      "markdown": "> what would it even mean to have 10^30 times more shrimp than atoms?\n\nOh, easy, it just implies you're engaging in acausal trade with a godlike entity residing in some universe dramatically bigger than this one. This interpretation introduces no additional questions or complications whatsoever.",
      "plaintextDescription": "> what would it even mean to have 10^30 times more shrimp than atoms?\n\nOh, easy, it just implies you're engaging in acausal trade with a godlike entity residing in some universe dramatically bigger than this one. This interpretation introduces no additional questions or complications whatsoever."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-22T08:15:37.770Z",
    "af": false
  },
  {
    "_id": "waMS2NgB4JcXsWfDq",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "nusTZioRC4vfcxnta",
    "topLevelCommentId": "GtEtFbXhPAHXzqPF4",
    "contents": {
      "markdown": "> I just really don't buy the whole \"let's add up qualia\" as any basis of moral calculation\n\nSame, honestly. To me, many of these thought experiments seem decoupled from anything practically relevant. But it still seems to me that people often *do* argue from those abstracted-out frames I'd outlined, and these arguments are probably sometimes useful for establishing at least some agreement on ethics. (I'm not sure how a full-complexity godshatter-on-godshatter argument would even look like (a fistfight, maybe?), and am very skeptical it'd yield any useful results.)\n\nAnyway, it sounds like we mostly figured out what the initial drastic disconnect between our views here was caused by?",
      "plaintextDescription": "> I just really don't buy the whole \"let's add up qualia\" as any basis of moral calculation\n\nSame, honestly. To me, many of these thought experiments seem decoupled from anything practically relevant. But it still seems to me that people often do argue from those abstracted-out frames I'd outlined, and these arguments are probably sometimes useful for establishing at least some agreement on ethics. (I'm not sure how a full-complexity godshatter-on-godshatter argument would even look like (a fistfight, maybe?), and am very skeptical it'd yield any useful results.)\n\nAnyway, it sounds like we mostly figured out what the initial drastic disconnect between our views here was caused by?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-22T08:13:19.397Z",
    "af": false
  },
  {
    "_id": "nGY3XiCS7h7LqpWui",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "tAqSsttyFSZycqmRA",
    "topLevelCommentId": "GtEtFbXhPAHXzqPF4",
    "contents": {
      "markdown": "> I agree that this is a thing people often like to invoke, but it feels to me a lot like people talking about billionaires and not noticing the classical crazy arithmetic errors like\n\nIsn't it the opposite? It's a *defence* against providing too-low numbers, it's specifically to ensure that even infinitesimally small preferences are elicited with certainty.\n\nBundling up all \"this seems like a lot\" numbers into the same mental bucket, and then failing to recognize when a real number is not actually as high as in your hypothetical, is certainly an error one could make here. But I don't see an exact correspondence...\n\nIn the billionaires case, a thought-experimenter may invoke the hypothetical of \"if a wealthy person had enough money to lift everyone out of poverty while still remaining rich, wouldn't them not doing so be outrageous?\", while inviting the audience to fill-in the definitions of \"enough money\" and \"poverty\". Practical situations might then just fail to match that hypothetical, and innumerate people might fail to recognize that, yes. But this doesn't mean that that hypothetical is fundamentally useless to reason about, or that it can't be used to study some specific intuitions/disagreements. (\"But there are no rich people with so much money!\" kind of maps to \"but I did have breakfast!\".)\n\nAnd in the shrimps case, hypotheticals involving a \"very-high but not abstraction-breaking\" number of shrimps are a useful tool for discussion/rhetoric. It allows to establish agreement/disagreement on \"shrimp experiences have inherent value at all\", a relatively simple question that could serve as a foundation for discussing other, more complicated and contextual ones. (Such as \"*how much* should I value shrimp experiences?\" or \"but do enough shrimps actually exist to add up to more than a human?\" or \"but is Intervention X to which I'm asked to donate $5 going to actually prevent five dollars' worth of shrimp suffering?\".)\n\nLike, I think having a policy of always allowing abstraction breaks would just impoverish the set of thought experiments we would be able consider and use as tools. Tons of different dilemmas would collapse to Pascal's mugging or whatever.\n\n> Like, I would feel more sympathetic to this simplification if the author of the post was a hardcore naive utilitarian, but they self-identify as a kantian. Kantianism is a highly contextual ethical theory that clearly cares about a bunch of different details of the shrimp, so I don't get the sense the author wants us to abstract away everything but some supposed \"happiness qualia\" or \"suffering qualia\" from the shrimp.\n\nHmm... I think this paragraph at the beginning is what primed me to parse it this way:\n\n> Merriam-Webster defines torture as “the infliction of intense pain (as from burning, crushing, or wounding) to punish, coerce, or afford sadistic pleasure.” So I remind the reader that it is part of the second thought experiment that the shrimp are sentient.\n\nWhy would we need this assumption[^yvjgqz0wo6s], if the hypothetical weren't centrally about the *inherent* value of the shrimps/shrimp qualia, and the idea that it adds up? The rest of [that essay](https://morallawwithin.substack.com/p/yes-you-should-save-10100-shrimp) also features no discussion of the contextual value that the existence of a shrimp injects into various diverse environments in which it exists, etc. It just throws the big number around, while comparing the value of shrimps to the value of eating a bag of skittles, after having implicitly justified shrimps having value via shrimps having qualia.\n\nI suppose it's possible that if I had the full context of the author's writing in mind, your interpretation would have been obviously correct[^vcltsauw4i9]. But the essay itself reads the opposite way to me.\n\n[^yvjgqz0wo6s]: A pretty strong one, I think, since \"are shrimp qualia of nonzero moral relevance?\" is often the very point of many discussions. \n\n[^vcltsauw4i9]: Indeed, failing to properly familiarize myself with the discourse and the relevant frames before throwing in hot takes was my main blunder here.",
      "plaintextDescription": "> I agree that this is a thing people often like to invoke, but it feels to me a lot like people talking about billionaires and not noticing the classical crazy arithmetic errors like\n\nIsn't it the opposite? It's a defence against providing too-low numbers, it's specifically to ensure that even infinitesimally small preferences are elicited with certainty.\n\nBundling up all \"this seems like a lot\" numbers into the same mental bucket, and then failing to recognize when a real number is not actually as high as in your hypothetical, is certainly an error one could make here. But I don't see an exact correspondence...\n\nIn the billionaires case, a thought-experimenter may invoke the hypothetical of \"if a wealthy person had enough money to lift everyone out of poverty while still remaining rich, wouldn't them not doing so be outrageous?\", while inviting the audience to fill-in the definitions of \"enough money\" and \"poverty\". Practical situations might then just fail to match that hypothetical, and innumerate people might fail to recognize that, yes. But this doesn't mean that that hypothetical is fundamentally useless to reason about, or that it can't be used to study some specific intuitions/disagreements. (\"But there are no rich people with so much money!\" kind of maps to \"but I did have breakfast!\".)\n\nAnd in the shrimps case, hypotheticals involving a \"very-high but not abstraction-breaking\" number of shrimps are a useful tool for discussion/rhetoric. It allows to establish agreement/disagreement on \"shrimp experiences have inherent value at all\", a relatively simple question that could serve as a foundation for discussing other, more complicated and contextual ones. (Such as \"how much should I value shrimp experiences?\" or \"but do enough shrimps actually exist to add up to more than a human?\" or \"but is Intervention X to which I'm asked to donate $5 going to actually prevent five dollars' worth of shrimp suffering?\".)\n\nLike, I think having a policy of always allowing a"
    },
    "baseScore": 7,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-22T07:38:10.861Z",
    "af": false
  },
  {
    "_id": "iCoFgQWG5RRKa3wPm",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "EpvjkePAAHquMCAY2",
    "topLevelCommentId": "GtEtFbXhPAHXzqPF4",
    "contents": {
      "markdown": "> I think there is also a real conversation going on here about whether maybe, even if you isolated each individual shrimp into a tiny pocket universe, and you had no way of ever seeing them or visiting the great shrimp rift (a natural wonder clearly greater than any natural wonder on earth), and all you knew for sure was that it existed somewhere outside of your sphere of causal influence, and the shrimp never did anything more interesting than current alive shrimp, whether it would still be worth it to kill a human\n\nYeah, that's more what I had in mind. Illusion of transparency, I suppose.\n\n> Like, I agree there are versions of the hypothetical that are too removed, but ultimately, I think a central lesson of scope sensitivity is that having *a lot* more of something often means drastic qualitative changes in what it means to have that thing\n\nCertainly, and it's an important property of reality. But I don't think this is what extreme hypotheticals such as the one under discussion *actually* want to talk about (even if you think this is a more important question to focus on)?\n\nLike, my model is that the \\\\(10^{100}\\\\) shrimp in this hypothetical are not meant to *literally be* \\\\(10^{100}\\\\) shrimp. They're meant to be \\\\(\\text{\"}10^{100}\\text{\"}\\\\) \"shrimp\". Intuitively, this is meant to stand for something like \"a number of shrimp large enough for any value you're assigning them to become morally relevant\". My interpretation is that the *purpose* of using a crazy-large number is to elicit that preference with certainty, even if it's epsilon; *not* to invite a discussion about qualitative changes in the nature of crazy-large quantities of arbitrary matter.\n\nThe hypothetical is interested in shrimp welfare. If we take the above consideration into account, it *stops* being about \"shrimp\" at all (see the shrimps-to-rocks move). The abstractions within which the hypothetical is meant to live break.\n\nAnd yes, if we're talking about a physical situation involving the number \\\\(10^{100}\\\\), the abstractions in question *really do* break under forces this strong, and we have to navigate the situation with the broken abstractions. But in thought-experiment land, we can artificially stipulate those abstractions inviolable (or replace the crazy-high abstraction-breaking number with a very-high but non-abstraction-breaking number).",
      "plaintextDescription": "> I think there is also a real conversation going on here about whether maybe, even if you isolated each individual shrimp into a tiny pocket universe, and you had no way of ever seeing them or visiting the great shrimp rift (a natural wonder clearly greater than any natural wonder on earth), and all you knew for sure was that it existed somewhere outside of your sphere of causal influence, and the shrimp never did anything more interesting than current alive shrimp, whether it would still be worth it to kill a human\n\nYeah, that's more what I had in mind. Illusion of transparency, I suppose.\n\n> Like, I agree there are versions of the hypothetical that are too removed, but ultimately, I think a central lesson of scope sensitivity is that having a lot more of something often means drastic qualitative changes in what it means to have that thing\n\nCertainly, and it's an important property of reality. But I don't think this is what extreme hypotheticals such as the one under discussion actually want to talk about (even if you think this is a more important question to focus on)?\n\nLike, my model is that the 10100 shrimp in this hypothetical are not meant to literally be 10100 shrimp. They're meant to be \"10100\" \"shrimp\". Intuitively, this is meant to stand for something like \"a number of shrimp large enough for any value you're assigning them to become morally relevant\". My interpretation is that the purpose of using a crazy-large number is to elicit that preference with certainty, even if it's epsilon; not to invite a discussion about qualitative changes in the nature of crazy-large quantities of arbitrary matter.\n\nThe hypothetical is interested in shrimp welfare. If we take the above consideration into account, it stops being about \"shrimp\" at all (see the shrimps-to-rocks move). The abstractions within which the hypothetical is meant to live break.\n\nAnd yes, if we're talking about a physical situation involving the number 10100, the abstractions in question really do br"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-22T06:25:08.529Z",
    "af": false
  },
  {
    "_id": "hqkHRzjBoLcZHGZAs",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "p7nM6XLonfGxe69Wi",
    "topLevelCommentId": "GtEtFbXhPAHXzqPF4",
    "contents": {
      "markdown": "> One can argue it's meaningless to talk about numbers this big, and while I would dispute that, it's definitely a much more sensible position than trying to take a confident stance to destroy or substantially alter a set of things so large that it vastly eclipses in complexity and volume and mass and energy all that has ever or will ever exist by a trillion-fold.\n\nOkay, while I'm hastily backpedaling from the general claims I made, I *am* interested in your take on [the first half of this post](https://www.lesswrong.com/posts/RDG2dbg6cLNyo6MYT/thane-ruthenis-s-shortform?commentId=WPKEHv4pnCDTaekBu). I think there's a difference between talking about an *actual* situation, full complexities of the messy reality taken into account, where a supernatural being physically shows up and makes you really decide between a human and \\\\(10^{100}\\\\) of shrimp, and a thought experiment where \"you\" \"decide\" between a \"human\" and \\\\(10^{100}\\\\) of \"shrimp\". In the second case, my model is that we're implicitly operating in an abstracted-out setup where the terms in the quotation marks are, essentially, assumed ontologically basic, and matching our intuitive/baseline expectations about what they mean.\n\nWhile, within the hypothetical, we can still have some uncertainty over e. g. the degree of the internal experiences of those \"shrimp\", I think we have to remove considerations like \"the shrimp will be deposited into a physical space obeying the laws of physics where their mass may form planets and galaxies\" or \"with so many shrimp, it's near-certain that the random twitches of some subset of them would spontaneously implement Boltzmann civilizations of uncountably many happy beings\".\n\nIMO, doing otherwise is a kind of \"dodging the hypothetical\", no different from considering it very unlikely that the supernatural being really has control over \\\\(10^{100}\\\\) of something, and starting to argue about this instead.",
      "plaintextDescription": "> One can argue it's meaningless to talk about numbers this big, and while I would dispute that, it's definitely a much more sensible position than trying to take a confident stance to destroy or substantially alter a set of things so large that it vastly eclipses in complexity and volume and mass and energy all that has ever or will ever exist by a trillion-fold.\n\nOkay, while I'm hastily backpedaling from the general claims I made, I am interested in your take on the first half of this post. I think there's a difference between talking about an actual situation, full complexities of the messy reality taken into account, where a supernatural being physically shows up and makes you really decide between a human and 10100 of shrimp, and a thought experiment where \"you\" \"decide\" between a \"human\" and 10100 of \"shrimp\". In the second case, my model is that we're implicitly operating in an abstracted-out setup where the terms in the quotation marks are, essentially, assumed ontologically basic, and matching our intuitive/baseline expectations about what they mean.\n\nWhile, within the hypothetical, we can still have some uncertainty over e. g. the degree of the internal experiences of those \"shrimp\", I think we have to remove considerations like \"the shrimp will be deposited into a physical space obeying the laws of physics where their mass may form planets and galaxies\" or \"with so many shrimp, it's near-certain that the random twitches of some subset of them would spontaneously implement Boltzmann civilizations of uncountably many happy beings\".\n\nIMO, doing otherwise is a kind of \"dodging the hypothetical\", no different from considering it very unlikely that the supernatural being really has control over 10100 of something, and starting to argue about this instead."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-22T05:40:45.666Z",
    "af": false
  },
  {
    "_id": "WPKEHv4pnCDTaekBu",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "z4seqkwhgWCSi96JX",
    "topLevelCommentId": "GtEtFbXhPAHXzqPF4",
    "contents": {
      "markdown": "> No, being extremely overwhelmingly confident about morality such that even if you are given a choice to drastically alter 99.999999999999999999999% of the matter in the universe, you call the side of not destroying it \"insane\" for not wanting to give up a single human life, a thing we do routinely for much weaker considerations, is insane.\n\nHm. Okay, so my reasoning there went as follows:\n\n> *   Substitute shrimp for rocks. \\\\(10^{100}\\\\) rocks would also be an amount of matter bigger than exists in the observable universe, and we presumably should assign a nonzero probability to rocks being sapient. Should we then save \\\\(10^{100}\\\\) rocks instead of one human?\n> *   Perhaps. But I think this transforms the problem into Pascal's mugging, and has nothing to do with shrimp or ethics anymore. If we're allowed to drag in outside considerations like this, we should also start questioning whether these \\\\(10^{100}\\\\) rocks/shrimp actually exist, and all other usual arguments against Pascal's mugging.\n> *   To properly engage with thought experiments *within* some domain, like ethics, we should take the assumptions behind this domain as a given. This implicitly means constraining our hypothesis space to models of reality within which this domain is a meaningful thing to reason about.\n> *   In this case, this would involve being able to reason about \\\\(10^{100}\\\\) rocks as if they *really were* just \"rocks\", without dragging in the uncertainty about \"but what if my very conception of what a 'rock' is is metaphysically confused?\".\n> *   Similarly, surely we should be able to have thought experiments in which \"shrimp\" *really are* just \"shrimp\", ontologically basic entities that are not made up of matter which can spontaneously assemble into Boltzmann brains or whatever.\n> *   \"Shrimp\" being a type of system that could implement qualia as valuable as that of humans seems overwhelmingly unlikely to me, not *as* unlikely as \"rocks have human-level qualia\", but in the same reference class. Therefore, in *the abstract thought-experiment setup in which I have no uncertainty regarding the ontological nature of shrimp*, it's reasonable to argue that no amount of them compares to a human life.\n\nI'm not sure where you'd get off this train, but I assume the last bullet-point would do this? I. e., that you would argue that holding the possibility of shrimps having human-level qualia is salient in a way it's not for rocks?\n\nYeah, that seems valid. I might've shot from the hip on that one.\n\n> The whole \"tier\" thing obviously fails. You always end up dominated by spurious effects on the highest tier\n\nI have a story for how that would make sense, similarly involving juggling inside-model and outside-model reasoning, but, hm, I'm somehow getting the impression my thinking here is undercooked/poorly presented. I'll revisit that one at a later time.\n\n**Edit:** Incidentally, any chance the UI for retracting a comment could be modified? I have two suggestions here:\n\n*   I'd like to be able to list a retraction reason, ideally at the top of the comment.\n*   The crossing-out thing makes it difficult to read the comment afterwards, and some people might want to be able to do that. Perhaps it's better to automatically put the contents into a collapsible instead, or something along those lines?",
      "plaintextDescription": "> No, being extremely overwhelmingly confident about morality such that even if you are given a choice to drastically alter 99.999999999999999999999% of the matter in the universe, you call the side of not destroying it \"insane\" for not wanting to give up a single human life, a thing we do routinely for much weaker considerations, is insane.\n\nHm. Okay, so my reasoning there went as follows:\n\n>  * Substitute shrimp for rocks. 10100 rocks would also be an amount of matter bigger than exists in the observable universe, and we presumably should assign a nonzero probability to rocks being sapient. Should we then save 10100 rocks instead of one human?\n>  * Perhaps. But I think this transforms the problem into Pascal's mugging, and has nothing to do with shrimp or ethics anymore. If we're allowed to drag in outside considerations like this, we should also start questioning whether these 10100 rocks/shrimp actually exist, and all other usual arguments against Pascal's mugging.\n>  * To properly engage with thought experiments within some domain, like ethics, we should take the assumptions behind this domain as a given. This implicitly means constraining our hypothesis space to models of reality within which this domain is a meaningful thing to reason about.\n>  * In this case, this would involve being able to reason about 10100 rocks as if they really were just \"rocks\", without dragging in the uncertainty about \"but what if my very conception of what a 'rock' is is metaphysically confused?\".\n>  * Similarly, surely we should be able to have thought experiments in which \"shrimp\" really are just \"shrimp\", ontologically basic entities that are not made up of matter which can spontaneously assemble into Boltzmann brains or whatever.\n>  * \"Shrimp\" being a type of system that could implement qualia as valuable as that of humans seems overwhelmingly unlikely to me, not as unlikely as \"rocks have human-level qualia\", but in the same reference class. Therefore, in the abstract thought-"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-21T17:53:47.944Z",
    "af": false
  },
  {
    "_id": "GtEtFbXhPAHXzqPF4",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "**Edit:** Nevermind, evidently I've not thought this through properly. I'm retracting the below.\n\n* * *\n\n~The naïve formulations of utilitarianism assume that all possible experiences can be mapped to scalar utilities lying on~ *~the same,~*  *~continuous~* ~spectrum, and that experiences' utility is additive. I think that's an error.~\n\n~This is how we get the frankly insane conclusions like \"you should save ~\\\\(10^{100}\\\\)~shrimps instead of one human\" or~ [~everyone's perennial favorite~](https://www.lesswrong.com/posts/3wYTFWY3LKQCnAptN/torture-vs-dust-specks)~, \"if you're choosing between one person getting tortured for 50 years or some amount of people ~\\\\(N\\\\)~ getting a dust speck into their eye, there must be an ~\\\\(N\\\\)~ big enough that the torture is better\". I disagree with those. I would sacrifice an arbitrarily large amount of shrimps to save one human, and there's no ~\\\\(N\\\\)~ big enough for me to pick torture. I don't care if that disagrees with what the math says: if the math says something else, it's the wrong math and we should find a better one.~\n\n~Here's a sketch of how I think such better math might look like:~\n\n*   ~There's a totally ordered,~ *~discrete~* ~set of \"importance tiers\" of experiences.~\n    *   *~Within~* ~tiers, the utilities are additive: two people getting dust specks is twice as bad as one person getting dust-speck'd, two people being tortured is twice as bad as one person being tortured, eating a chocolate bar twice per week is twice as good as eating a bar once per week, etc.~\n    *   *~Across~* ~tiers, the tier ordering dominates: if we're comparing some experience belonging to a higher tier to any combination of experiences from lower tiers, the only relevant consideration is the sign of the higher-tier experience. No amount of people getting dust-speck'd, and no amount of dust-speck events sparsely distributed throughout one person's life, can ever add up to anything as important as a torture-level experience.~[^xudf3tc9r1]\n*   ~Intuitively, tiers correspond to the size of effect a given experience has on a person's life:~\n    *   ~A dust speck is a minor inconvenience that is forgotten after a second. If we zoom out and consider the person's life at a higher level, say on the scale of a day, this experience rounds off~ *~exactly~* ~to zero, rather than to an infinitesimally small but nonzero value. (Again, on the assumption of a median dust-speck event, no emergent or butterfly effects.)~\n    *   ~Getting yelled at by someone might ruin the entirety of someone's day, but is unlikely to meaningfully change the course of their life. Experiences on this tier are more important than any amount of dust-speck experiences, but any combination of them rounds down to zero from a life's-course perspective.~\n    *   ~Getting tortured is likely to significantly traumatize someone, to have a lasting negative impact on their life. Experiences at this tier ought to dominate getting-yelled-at as much as getting-yelled-at dominates dust specks.~\n*   ~Physically, those \"importance tiers\" probably fall out of the hierarchy of~ [~natural abstractions~](https://www.lesswrong.com/w/natural-abstraction)~. Like everything else, a person's life has different levels of organization. Any detail in how the high-level life-history goes is incomparably more important than any experience which is only locally relevant (which fails to send long-distance ripples throughout the person's life). Butterfly effects are then the abstraction leaks (low-level events that perturb high-level dynamics), etc.~\n\n~I didn't spend much time thinking about this, so there may be some glaring holes here, but this already fits my intuitions~ *~much~* ~better.~\n\n* * *\n\n~I think we can expand that framework to cover \"tiers of sentience\":~\n\n*   ~If shrimps have qualia, it might be that~ *~any~* ~qualia they're capable of experiencing belong to lower-importance tiers, compared to the highest-tier human qualia.~\n*   ~Simultaneously, it might be the case that the highest-importance shrimp qualia are on the level of the lower-importance human qualia.~\n*   ~Thus, it might be reasonable to sacrifice the experience of eating a chocolate bar to save ~\\\\(10^{100}\\\\)~ shrimps, even if you'd never sacrifice a person's life (or even make someone cry) to save any amount of shrimps.~\n\n~This makes some intuitive sense, I think. The model above assumes that \"local\" experiences, which have no impact on the overarching pattern of a person's life, are arbitrarily less important than that overarching pattern. What if we're dealing with beings whose internal lives~ *~have~* ~no such overarching patterns, then? A shrimp's interiority is certainly less complex than that of a human, so it seems plausible that its life-experience lacks comparably rich levels of organization (something like \"the ability to experience what is currently happening as part of the tapestry spanning its entire life, rather than as an isolated experience\"). So all of its qualia would be comparable only with the \"local\" experiences of a human, for some tier of locality: we would have direct equivalence between them.~\n\n~One potential issue here is that this implies the existence of utility monsters: some divine entities such that they can have experiences incomparably more important than any experience a human can have. I guess it's possible that if I understood qualia better, I would agree with that, but this seems about as anti-intuitive as \"shrimps matter as much as humans\". My intuition is that sapient entities~ *~top~* ~the hierarchy of moral importance, that there's nothing meaningfully \"above\" them. So that's an issue.~\n\n~One potential way to deal with this is to suggest that what distinguishes sapient/\"generally intelligent\" entities is not that they're the only entities whose experiences matter, but that they have the ability to (learn to) have experiences of~ *~arbitrarily high~* ~tiers. And indeed: the whole shtick of \"general intelligence\" is that it should allow you to learn and reason about arbitrarily complicated systems of abstraction/multi-level organization. If the importance tiers of experiences really have something to do with the richness of the organization of the entity's inner life, this resolves things neatly. Now:~\n\n*   ~Non-sapient entities may have experiences of nonzero importance.~\n*   ~No combination of non-sapient experiences can compare to the importance of a sapient entity's life.~\n*   ~\"Is sapient\" tops the hierarchy of moral relevance: there's no type of entity that is fundamentally \"above\".~\n\n[^xudf3tc9r1]: Two caveats here are butterfly effects and emergent importance:Getting a dust speck at the wrong moment might kill you (if you're operating dangerous machinery) or change the trajectory of your life (if this minor inconvenience is the last straw that triggers a career-ruining breakdown). We have to assume such possibilities away: the experiences exist \"in a vacuum\". Doing otherwise would violate the experimental setup, dragging in various practical considerations, instead of making it purely about ethics.So we assume that each dust-speck event always has the \"median\" amount of impact on a person's life, even if you scale the amount of dust-speck events arbitrarily.Getting 1000 dust specks one after another adds up to something more than 1000 single-dustspeck experiences; it's worse than getting a dust speck once per day for 1000 days. More intuitively, experiencing a 10/10 pain for one millisecond is not comparable to experiencing 10/10 pain for 10 minutes. There are emergent effects at play, and like with butterflies, we must assume them away for experimental purity.So if we're talking about \\(M\\) experiences from within the same importance tier, they're assumed to be distributed such that they don't add up to a higher-tier experience.Note that those are very artificial conditions. In real life, both of those are very much in play. Any lower-tier experience has a chance of resulting into a higher-tier experience, and every higher-tier experience emerges from (appropriately distributed) lower-tier experiences. In our artificial setup, we're assuming certain knowledge that no butterfly effects would occur, and that a lower-tier event contributes to no higher-tier pattern.Relevance: There's reasoning that goes, \"if you ever drive to the store to get a chocolate bar, you're risking crashing into and killing someone, therefore you don't value people's lives infinitely more than eating chocolate\". I reject it on the above grounds. Systematically avoiding all situations where you're risking someone's life in exchange for a low-importance experience would assemble into a high-importance life-ruining experience for you (starving to death in your apartment, I guess?). Given that, we're now comparing same-tier experiences, and here I'm willing to be additive, calculating that killing a person with very low probability is better than killing yourself (by a thousand cuts) with certainty.",
      "plaintextDescription": "Edit: Nevermind, evidently I've not thought this through properly. I'm retracting the below.\n\n----------------------------------------\n\nThe naïve formulations of utilitarianism assume that all possible experiences can be mapped to scalar utilities lying on the same, continuous spectrum, and that experiences' utility is additive. I think that's an error.\n\nThis is how we get the frankly insane conclusions like \"you should save 10100 shrimps instead of one human\" or everyone's perennial favorite, \"if you're choosing between one person getting tortured for 50 years or some amount of people N getting a dust speck into their eye, there must be an N big enough that the torture is better\". I disagree with those. I would sacrifice an arbitrarily large amount of shrimps to save one human, and there's no N big enough for me to pick torture. I don't care if that disagrees with what the math says: if the math says something else, it's the wrong math and we should find a better one.\n\nHere's a sketch of how I think such better math might look like:\n\n * There's a totally ordered, discrete set of \"importance tiers\" of experiences.\n   * Within tiers, the utilities are additive: two people getting dust specks is twice as bad as one person getting dust-speck'd, two people being tortured is twice as bad as one person being tortured, eating a chocolate bar twice per week is twice as good as eating a bar once per week, etc.\n   * Across tiers, the tier ordering dominates: if we're comparing some experience belonging to a higher tier to any combination of experiences from lower tiers, the only relevant consideration is the sign of the higher-tier experience. No amount of people getting dust-speck'd, and no amount of dust-speck events sparsely distributed throughout one person's life, can ever add up to anything as important as a torture-level experience.[1]\n * Intuitively, tiers correspond to the size of effect a given experience has on a person's life:\n   * A dust speck is a minor inconven"
    },
    "baseScore": -1,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-21T16:14:50.111Z",
    "af": false
  },
  {
    "_id": "hhxnGgpjGAXoMs2Tf",
    "postId": "RDG2dbg6cLNyo6MYT",
    "parentCommentId": "2H4Fab5DgiL44p5uj",
    "topLevelCommentId": "sM28F6kD2ttpnzwBC",
    "contents": {
      "markdown": "Incidentally, your [Intelligence as Privilege Escalation](https://www.lesswrong.com/posts/vLrj4ZNcGCTMJqdXB/intelligence-as-privilege-escalation) is pretty relevant to that picture. I had it in mind when writing that.",
      "plaintextDescription": "Incidentally, your Intelligence as Privilege Escalation is pretty relevant to that picture. I had it in mind when writing that."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-21T14:58:06.942Z",
    "af": false
  },
  {
    "_id": "MqkEEF93fx4mvfQ3p",
    "postId": "i7JSL5awGFcSRhyGF",
    "parentCommentId": "NeCAEzo4TjJagWgCj",
    "topLevelCommentId": "NeCAEzo4TjJagWgCj",
    "contents": {
      "markdown": "Not necessarily. If humans don't die or end up depowered in the first few weeks of it, it might instead be a continuous high-intensity stress state, because you'll need to be paying attention 24/7 to constant world-upturning developments, frantically figuring out what process/trend/entity you should be hitching your wagon to in order to not be drowned by the ever-rising tide, with the correct choice dynamically changing at an ever-increasing pace.\n\n\"Not being depowered\" would actually make the Singularity experience *massively worse* in the short term, precisely because you'll be constantly getting access to new tools and opportunities, and it'd be on you to frantically figure out how to make good use of them.\n\nThe relevant reference class is probably something like [\"being a high-frequency trader\":](https://www.scimitar.capital/p/time-is-event-based)\n\n> Crypto is the only market that trades 24/7, meaning there simply was no rest for the wicked. The game was less about brilliance and more about being awake when it counted. Resource management around attention and waking hours was a big part of the game. \\[...\\]\n> \n> My cofounder and I developed a polyphasic sleeping routine so that we would be conscious during as many of these action periods as possible. It was rare to get uninterrupted sleep for more than 3 hours at a time. We took tactical naps whenever possible and had phone alarms to wake us up in case important headlines came out during off hours. I felt like I had experienced three days for every one that passed.\n> \n> There was always something going on. Everyday a new puzzle to solve. A new fire to put out. We frequently would work 18 hour days processing information, trading events, building infrastructure, and managing risk. We frequently moved around different parts of the world, built strong relationships with all sorts of people from around the globe, and experienced some of the highest highs and lowest lows of our lives.\n> \n> Those three years felt like the longest stretch I’ve ever lived.\n\nThis is pretty close to how I expect a \"slow\" takeoff to feel like, yep.",
      "plaintextDescription": "Not necessarily. If humans don't die or end up depowered in the first few weeks of it, it might instead be a continuous high-intensity stress state, because you'll need to be paying attention 24/7 to constant world-upturning developments, frantically figuring out what process/trend/entity you should be hitching your wagon to in order to not be drowned by the ever-rising tide, with the correct choice dynamically changing at an ever-increasing pace.\n\n\"Not being depowered\" would actually make the Singularity experience massively worse in the short term, precisely because you'll be constantly getting access to new tools and opportunities, and it'd be on you to frantically figure out how to make good use of them.\n\nThe relevant reference class is probably something like \"being a high-frequency trader\":\n\n> Crypto is the only market that trades 24/7, meaning there simply was no rest for the wicked. The game was less about brilliance and more about being awake when it counted. Resource management around attention and waking hours was a big part of the game. [...]\n> \n> My cofounder and I developed a polyphasic sleeping routine so that we would be conscious during as many of these action periods as possible. It was rare to get uninterrupted sleep for more than 3 hours at a time. We took tactical naps whenever possible and had phone alarms to wake us up in case important headlines came out during off hours. I felt like I had experienced three days for every one that passed.\n> \n> There was always something going on. Everyday a new puzzle to solve. A new fire to put out. We frequently would work 18 hour days processing information, trading events, building infrastructure, and managing risk. We frequently moved around different parts of the world, built strong relationships with all sorts of people from around the globe, and experienced some of the highest highs and lowest lows of our lives.\n> \n> Those three years felt like the longest stretch I’ve ever lived.\n\nThis is pretty clos"
    },
    "baseScore": 15,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-07-21T14:43:52.197Z",
    "af": false
  },
  {
    "_id": "Xjfx5e3oKfn6gexRx",
    "postId": "RcBqeJ8GHM2LygQK3",
    "parentCommentId": "DhF4kit7hwJ6w85KG",
    "topLevelCommentId": "CayYH9BhaAGzqij2y",
    "contents": {
      "markdown": "> It does sound like it may be a new and in a narrow sense unexpected technical development\n\nI buy that, sure. I even buy that they're as excited about it as they present, that *they* believe/hope it unlocks generalization to hard-ot-verify domains. And yes, they may or may not be right. But I'm skeptical on priors/based on my model of ML, and their excitement isn't very credible evidence, so I've not moved far from said priors.",
      "plaintextDescription": "> It does sound like it may be a new and in a narrow sense unexpected technical development\n\nI buy that, sure. I even buy that they're as excited about it as they present, that they believe/hope it unlocks generalization to hard-ot-verify domains. And yes, they may or may not be right. But I'm skeptical on priors/based on my model of ML, and their excitement isn't very credible evidence, so I've not moved far from said priors."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-21T09:43:34.254Z",
    "af": false
  },
  {
    "_id": "CFixqrCm6ZBG9JCmg",
    "postId": "RcBqeJ8GHM2LygQK3",
    "parentCommentId": "FPyLkErXoZvbovLdK",
    "topLevelCommentId": "YCKddybxgJaXZri95",
    "contents": {
      "markdown": "Oh, yeah, he's not superintelligence-pilled or anything. I was implicitly comparing with a relatively low baseline, yes.",
      "plaintextDescription": "Oh, yeah, he's not superintelligence-pilled or anything. I was implicitly comparing with a relatively low baseline, yes."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-21T09:36:53.717Z",
    "af": false
  },
  {
    "_id": "Q5sbWowvJrJMWjPdT",
    "postId": "RcBqeJ8GHM2LygQK3",
    "parentCommentId": "YCKddybxgJaXZri95",
    "topLevelCommentId": "YCKddybxgJaXZri95",
    "contents": {
      "markdown": "Honestly, that thread did initially sound kind of copium-y to me too, which I was surprised by, since his AI takes are usually pretty good[^biynbv2zl9] and level-headed. But it makes much more sense under the interpretation that this isn't him being in denial about AI performance, but him undermining OpenAI in response to them defecting against IMO. That's why he's pushing the \"this isn't a fair human-AI comparison\" line.\n\n[^biynbv2zl9]: Edit: For someone who doesn't \"feel the ASI\", I mean.",
      "plaintextDescription": "Honestly, that thread did initially sound kind of copium-y to me too, which I was surprised by, since his AI takes are usually pretty good[1] and level-headed. But it makes much more sense under the interpretation that this isn't him being in denial about AI performance, but him undermining OpenAI in response to them defecting against IMO. That's why he's pushing the \"this isn't a fair human-AI comparison\" line.\n\n 1. ^\n    Edit: For someone who doesn't \"feel the ASI\", I mean."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-21T07:27:54.827Z",
    "af": false
  },
  {
    "_id": "ZpDqNS3dBB7Ti93dm",
    "postId": "RcBqeJ8GHM2LygQK3",
    "parentCommentId": "CayYH9BhaAGzqij2y",
    "topLevelCommentId": "CayYH9BhaAGzqij2y",
    "contents": {
      "markdown": "The claim I'm squinting at real hard is [this one](https://x.com/polynoamial/status/1946478252496695523):\n\n> We developed new techniques that make LLMs a lot better at hard-to-verify tasks. \n\nLike, there's some murkiness with them apparently [awarding gold to themselves](https://x.com/lmthang/status/1946960256439058844) instead of IMO organizers doing it, and with [that other competitive-programming contest](https://x.com/andresnds/status/1945655797314154762) at which presumably-the-same model did well being [OpenAI-funded](https://atcoder.jp/contests/awtf2025algo). But whatever, I'm willing to buy that they have a model that legitimately achieved roughly this performance (even if a fairer set of IMO judges would've docked points to slightly below the unimportant \"gold\" threshold).\n\nBut since when are math proofs, or competitive programming, considered good examples of hard-to-verify tasks? I'm surprised I haven't seen anyone challenging that. (FYI, I do not think they are good examples of hard-to-verify tasks, fight me. **Edit:** Also, inasmuch as proofs *have* hard-to-verify properties like \"being well-written\", their model sure failed at that.) So pending some results in an actual hard-to-verify domain, or convincing technical arguments that this new approach would really generalize, I'm not buying that.\n\nOverall... This is the performance I would have expected out of them just mindlessly RLing bigger models harder. They needed to have *a whole new breakthrough* to get here? And the models on the new approach take *hours* to produce their solutions? They're spinning it as good news for AI, but pending more information, this sounds pretty bad to me actually.",
      "plaintextDescription": "The claim I'm squinting at real hard is this one:\n\n> We developed new techniques that make LLMs a lot better at hard-to-verify tasks. \n\nLike, there's some murkiness with them apparently awarding gold to themselves instead of IMO organizers doing it, and with that other competitive-programming contest at which presumably-the-same model did well being OpenAI-funded. But whatever, I'm willing to buy that they have a model that legitimately achieved roughly this performance (even if a fairer set of IMO judges would've docked points to slightly below the unimportant \"gold\" threshold).\n\nBut since when are math proofs, or competitive programming, considered good examples of hard-to-verify tasks? I'm surprised I haven't seen anyone challenging that. (FYI, I do not think they are good examples of hard-to-verify tasks, fight me. Edit: Also, inasmuch as proofs have hard-to-verify properties like \"being well-written\", their model sure failed at that.) So pending some results in an actual hard-to-verify domain, or convincing technical arguments that this new approach would really generalize, I'm not buying that.\n\nOverall... This is the performance I would have expected out of them just mindlessly RLing bigger models harder. They needed to have a whole new breakthrough to get here? And the models on the new approach take hours to produce their solutions? They're spinning it as good news for AI, but pending more information, this sounds pretty bad to me actually."
    },
    "baseScore": 17,
    "voteCount": 14,
    "createdAt": null,
    "postedAt": "2025-07-21T07:09:03.113Z",
    "af": false
  },
  {
    "_id": "FfQtbZxjh5nn9MLsj",
    "postId": "gsc9yP7vAh74M2g8i",
    "parentCommentId": "9Q4noTyHCCgm8gwtm",
    "topLevelCommentId": "SRKY6WNka2dWY3CwL",
    "contents": {
      "markdown": "> Silently sponsoring FrontierMath and receiving access to the question sets, and, if I remember correctly, o3 and o3-mini performing worse on a later evaluation done on a newer private question set of some sort\n\nIIRC, that worse performance was due to using a worse/less adapted agency scaffold, rather than OpenAI making the numbers up or engaging in any other egregious tampering. Regarding ARC-AGI, the December-2024 o3 and the public o3 are indeed entirely different models, but I don't think it implies the December one was tailored for ARC-AGI.\n\nI'm not saying OpenAI isn't liable to exaggerate or massage their results for hype, but I don't think they ever outright cheated (as far as we know, at least). Especially given that getting caught cheating will probably spell the AI industry's death, and their situation doesn't yet look *so* desperate that they'd risk that.\n\nSo I currently expect that the results here are, ultimately, legitimate. What I'm very skeptical about are the *implications* of those results that people are touting around.",
      "plaintextDescription": "> Silently sponsoring FrontierMath and receiving access to the question sets, and, if I remember correctly, o3 and o3-mini performing worse on a later evaluation done on a newer private question set of some sort\n\nIIRC, that worse performance was due to using a worse/less adapted agency scaffold, rather than OpenAI making the numbers up or engaging in any other egregious tampering. Regarding ARC-AGI, the December-2024 o3 and the public o3 are indeed entirely different models, but I don't think it implies the December one was tailored for ARC-AGI.\n\nI'm not saying OpenAI isn't liable to exaggerate or massage their results for hype, but I don't think they ever outright cheated (as far as we know, at least). Especially given that getting caught cheating will probably spell the AI industry's death, and their situation doesn't yet look so desperate that they'd risk that.\n\nSo I currently expect that the results here are, ultimately, legitimate. What I'm very skeptical about are the implications of those results that people are touting around."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-20T11:21:19.189Z",
    "af": false
  },
  {
    "_id": "xQnqmHGZodhx3JSpv",
    "postId": "RcBqeJ8GHM2LygQK3",
    "parentCommentId": "hYDJruTgH2aiYDrmb",
    "topLevelCommentId": "wvnmr72XKFxo5uva4",
    "contents": {
      "markdown": "I'd guess it has something to do with whatever they're using to automatically evaluate the performance in \"hard-to-verify domains\". My understanding is that, during training, those entire proofs would have been the final outputs which the reward function (or whatever) would have taken in and mapped to training signals. So their shape is precisely what the training loop *optimized –* and if so, this shape is downstream of some peculiarities on that end, the training loop preferring/enforcing this output format.",
      "plaintextDescription": "I'd guess it has something to do with whatever they're using to automatically evaluate the performance in \"hard-to-verify domains\". My understanding is that, during training, those entire proofs would have been the final outputs which the reward function (or whatever) would have taken in and mapped to training signals. So their shape is precisely what the training loop optimized – and if so, this shape is downstream of some peculiarities on that end, the training loop preferring/enforcing this output format."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-20T10:59:09.424Z",
    "af": false
  },
  {
    "_id": "SKxWDG3XTqcW2bdFh",
    "postId": "gsc9yP7vAh74M2g8i",
    "parentCommentId": "MxuNh8tSPBhayTTyx",
    "topLevelCommentId": "SRKY6WNka2dWY3CwL",
    "contents": {
      "markdown": "> Pretty much everybody is looking into test-time compute and RLVR right now. How come (seemingly) nobody else has found out about this \"new general-purpose method\" before OpenAI?\n\nWell, someone has to be the first, and they got to RLVR itself first last September.\n\n> OpenAI has been shown to not be particularly trustworthy when it comes to test and benchmark results\n\nThey have? How so?",
      "plaintextDescription": "> Pretty much everybody is looking into test-time compute and RLVR right now. How come (seemingly) nobody else has found out about this \"new general-purpose method\" before OpenAI?\n\nWell, someone has to be the first, and they got to RLVR itself first last September.\n\n> OpenAI has been shown to not be particularly trustworthy when it comes to test and benchmark results\n\nThey have? How so?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-20T10:05:19.414Z",
    "af": false
  },
  {
    "_id": "LN5xqprQ6N5wwAuLp",
    "postId": "gsc9yP7vAh74M2g8i",
    "parentCommentId": "ddPGEjy9iHiaXFsxk",
    "topLevelCommentId": "SRKY6WNka2dWY3CwL",
    "contents": {
      "markdown": "Eh. Scaffolds that involve agents privately iterating on ideas and then outputting a single result are a known approach, see e. g. [this](https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/), or Deep Research, or possibly o1 pro/o3 pro. I expect it's something along the same lines, except with some trick that makes it work better than ever before... Oh, come to think of it, Noam Brown did have [that interview](https://www.youtube.com/watch?v=ddd4xjuJTyg) I was meaning to watch, about \"scaling test-time compute to multi-agent civilizations\". That sounds relevant.\n\nI mean, it *can* be scary, for sure; no way to be certain until we see the details.",
      "plaintextDescription": "Eh. Scaffolds that involve agents privately iterating on ideas and then outputting a single result are a known approach, see e. g. this, or Deep Research, or possibly o1 pro/o3 pro. I expect it's something along the same lines, except with some trick that makes it work better than ever before... Oh, come to think of it, Noam Brown did have that interview I was meaning to watch, about \"scaling test-time compute to multi-agent civilizations\". That sounds relevant.\n\nI mean, it can be scary, for sure; no way to be certain until we see the details."
    },
    "baseScore": 8,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-07-19T14:48:10.887Z",
    "af": false
  }
]