[
  {
    "_id": "EeK3DoPtdNxfphkJG",
    "postId": "ETNhvutPHZKMvk3E9",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> I may never stop finding it funny the extent to which Trump will seek out the one thing we know definitively is going badly, then and choose that to lie and brag about.\n\nWe're not the target audience for those posts. He's telling everyone who's kissing his ass that this is a topic he wants them to lie about as well.",
      "plaintextDescription": "> I may never stop finding it funny the extent to which Trump will seek out the one thing we know definitively is going badly, then and choose that to lie and brag about.\n\nWe're not the target audience for those posts. He's telling everyone who's kissing his ass that this is a topic he wants them to lie about as well."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-15T16:18:13.142Z",
    "af": false
  },
  {
    "_id": "dstrwgTDSYXfAB6qD",
    "postId": "Sqrt2FC3c45QpX3Au",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> If we were doing a better job catching cheaters presumably people would be doing it less?\n\nSeems trivially easy for an idiot to start an account, lose some games, get frustrated, and start asking a chess bot what to do so that they can rescue a bad position / show that bastard what's what / vicariously enjoy winning. I expect that crowd to be the majority of cheaters, and for them to be essentially inelastic to the probability with which they get caught (the first time).",
      "plaintextDescription": "> If we were doing a better job catching cheaters presumably people would be doing it less?\n\nSeems trivially easy for an idiot to start an account, lose some games, get frustrated, and start asking a chess bot what to do so that they can rescue a bad position / show that bastard what's what / vicariously enjoy winning. I expect that crowd to be the majority of cheaters, and for them to be essentially inelastic to the probability with which they get caught (the first time)."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-23T19:20:49.327Z",
    "af": false
  },
  {
    "_id": "i5xYABe4QLP3XoeNN",
    "postId": "JkrkzXQiPwFNYXqZr",
    "parentCommentId": "zDJ83BuhFJ9ecGMj8",
    "topLevelCommentId": "zDJ83BuhFJ9ecGMj8",
    "contents": {
      "markdown": "I believe that Cade knows perfectly well what everyone has been saying for years; he's being disingenuous because the object level doesn't matter to him, and the only important thing is ensuring that these weirdos don't get status. He's never once engaged on simulacrum level 1 with this community.",
      "plaintextDescription": "I believe that Cade knows perfectly well what everyone has been saying for years; he's being disingenuous because the object level doesn't matter to him, and the only important thing is ensuring that these weirdos don't get status. He's never once engaged on simulacrum level 1 with this community."
    },
    "baseScore": 31,
    "voteCount": 20,
    "createdAt": null,
    "postedAt": "2025-08-17T23:53:41.556Z",
    "af": false
  },
  {
    "_id": "LjCSmqcfDfDpA87mM",
    "postId": "gQyphPbaLHBMJoghD",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "At long last, Elon Musk has created the Yandere Simulator from the classic short story Don't Create The Yandere Simulator, Among Other Things",
      "plaintextDescription": "At long last, Elon Musk has created the Yandere Simulator from the classic short story Don't Create The Yandere Simulator, Among Other Things"
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-09T23:29:40.020Z",
    "af": false
  },
  {
    "_id": "tYeMNX63sAcBDYanD",
    "postId": "tv6KfHitijSyKCr6v",
    "parentCommentId": "6X6sSNwgtXXxH9aSb",
    "topLevelCommentId": "6X6sSNwgtXXxH9aSb",
    "contents": {
      "markdown": "On several of these I think you might be confounding the positively correlated variables \"has heard of this question\" and \"is on the autism spectrum\", the latter of which is anticorrelated with the kind of behavior you want (intuitive, empathetic listening without needing to ask a bunch of explicit questions). I find it unlikely that asking the question makes people *worse* at the behavior you want.",
      "plaintextDescription": "On several of these I think you might be confounding the positively correlated variables \"has heard of this question\" and \"is on the autism spectrum\", the latter of which is anticorrelated with the kind of behavior you want (intuitive, empathetic listening without needing to ask a bunch of explicit questions). I find it unlikely that asking the question makes people worse at the behavior you want."
    },
    "baseScore": 4,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-08T18:38:54.539Z",
    "af": false
  },
  {
    "_id": "RkYSjtoHdHSt68QDY",
    "postId": "FBZmnmbDBqhmhGhby",
    "parentCommentId": "r42tMKfEsYirWYzJ4",
    "topLevelCommentId": "r42tMKfEsYirWYzJ4",
    "contents": {
      "markdown": "Zvi is arguing \"X implies Y\" here. Zvi happens to believe Y but disbelieve X; however, he is writing to people who think \"X and not-Y\", in order to nudge them to support Y.\n\nHere X = it is good for the US to build superintelligence fast, before China does, and Y = we should have some diffusion rules making it harder for China to catch up to the USA. \n\nZvi believes Z = nobody should be building superintelligence soon, and believes Z implies Y, but it is useful to show that X implies Y as well.",
      "plaintextDescription": "Zvi is arguing \"X implies Y\" here. Zvi happens to believe Y but disbelieve X; however, he is writing to people who think \"X and not-Y\", in order to nudge them to support Y.\n\nHere X = it is good for the US to build superintelligence fast, before China does, and Y = we should have some diffusion rules making it harder for China to catch up to the USA. \n\nZvi believes Z = nobody should be building superintelligence soon, and believes Z implies Y, but it is useful to show that X implies Y as well."
    },
    "baseScore": 14,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-05-17T07:17:16.067Z",
    "af": false
  },
  {
    "_id": "NrD74ETnrZAbRHGG5",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Question for [@Scott Garrabrant](https://www.lesswrong.com/users/scott-garrabrant?mention=user), [@TsviBT](https://www.lesswrong.com/users/tsvibt?mention=user), [@Andrew_Critch](https://www.lesswrong.com/users/andrew_critch?mention=user), [@So8res](https://www.lesswrong.com/users/so8res?mention=user), [@jessicata](https://www.lesswrong.com/users/jessica-liu-taylor?mention=user), and anyone else who knows the answer: the logical inductor constructed in [the paper](https://arxiv.org/abs/1609.03543) is not merely computable but also primitive recursive, right? \n\nSeems obvious to me (because the fixed price point is approximated, etc), but I want to be sure I'm not missing something.",
      "plaintextDescription": "Question for @Scott Garrabrant, @TsviBT, @Andrew_Critch, @So8res, @jessicata, and anyone else who knows the answer: the logical inductor constructed in the paper is not merely computable but also primitive recursive, right? \n\nSeems obvious to me (because the fixed price point is approximated, etc), but I want to be sure I'm not missing something."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-04T21:37:18.914Z",
    "af": true
  },
  {
    "_id": "pbjf2hDPyH77PQNPr",
    "postId": "vpjDibeusXvQq4TCu",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "To be fair, I also think it was a correct decision to focus on the Democratic Party when advocating for sane AI notkilleveryoneism policy before the 2024 election, because that was playing to our outs. Trump will do on AI whatever he is personally paid to do, and the accelerationists have more money; anyone sane but unable/unwilling to bribe Trump has no influence whatsoever with him in this administration.\n\n(I went around saying the above before the election, by the way.)\n\n(Sorry for the mindkilling, it's relevant on the object level.)",
      "plaintextDescription": "To be fair, I also think it was a correct decision to focus on the Democratic Party when advocating for sane AI notkilleveryoneism policy before the 2024 election, because that was playing to our outs. Trump will do on AI whatever he is personally paid to do, and the accelerationists have more money; anyone sane but unable/unwilling to bribe Trump has no influence whatsoever with him in this administration.\n\n(I went around saying the above before the election, by the way.)\n\n(Sorry for the mindkilling, it's relevant on the object level.)"
    },
    "baseScore": 30,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2025-04-30T18:24:37.525Z",
    "af": false
  },
  {
    "_id": "xhDdJvjp9oDiK2pGg",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "\\[EDIT: Never mind, this is just Kleene's *second* recursion theorem!\\]\n\nQuick question about Kleene's recursion theorem:\n\nLet's say F is a computable function from ℕ^N to ℕ. Is there a single computable function X from ℕ^N to ℕ such that\n\nX = F(X, y\\_2,..., y\\_N) **for all y\\_2,...,y\\_N in ℕ**\n\n(taking the X within F as the binary code of X in a fixed encoding) or do there need to be additional conditions?",
      "plaintextDescription": "[EDIT: Never mind, this is just Kleene's second recursion theorem!]\n\nQuick question about Kleene's recursion theorem:\n\nLet's say F is a computable function from ℕ^N to ℕ. Is there a single computable function X from ℕ^N to ℕ such that\n\nX = F(X, y_2,..., y_N) for all y_2,...,y_N in ℕ\n\n(taking the X within F as the binary code of X in a fixed encoding) or do there need to be additional conditions?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-04-01T18:48:48.931Z",
    "af": true
  },
  {
    "_id": "k2xEiW5JjRtdaQq2z",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": "CgjKcnJmznEGTPxEi",
    "topLevelCommentId": "CgjKcnJmznEGTPxEi",
    "contents": {
      "markdown": "My current candidate definitions, with some significant issues in the footnotes:\n\nA **fair environment** is a probabilistic function \\\\(F(x_1,...,x_N)=[X_1,...,X_N]\\\\) from an array of actions to an array of payoffs. \n\nAn **agent** \\\\(A\\\\) is a random variable \n\n\\\\(A(F,A_1,..., A_{i-1},A_i=A,A_{i+1},...,A_N)\\\\) \n\nwhich takes in a fair environment \\\\(F\\\\)[^v8qe4hrv1fd] and a list of agents (including itself), and outputs a mixed strategy over its available actions in \\\\(F\\\\). [^b498knpuloj]\n\nA **fair agent** is one whose mixed strategy is a function of subjective probabilities[^yoon1wwm15] that it assigns to \\[the actions of some finite collection of agents in fair environments, where any agents not appearing in the original problem must themselves be fair\\]. \n\nFormally, if \\\\(A\\\\) is a fair agent in with a subjective probability estimator \\\\(P\\\\), \\\\(A\\\\)'s mixed strategy in a fair environment \\\\(F\\\\),\n\n\\\\(A(F,A_1,..., A_{i-1},A_i=A,A_{i+1},...,A_N)\\\\)\n\nshould depend only on a finite collection of \\\\(A\\\\)'s subjective probabilities about outcomes \n\n\\\\(\\{P(F_k(A_1,...,A_N,B_1,...B_M))=[X_1,...,X_{N+M}]\\}_{k=1}^K\\\\) \n\nfor a set of fair environments \\\\(F_1,...,F_K\\\\) and an additional set of fair[^99exe3f94kb] agents[^6a5o5hvwt1l] \\\\(B_1,...,B_M\\\\) if needed (note that not all agents need to appear in all environments). \n\nA **fair problem** is a fair environment with one designated player, where all other agents are fair agents.\n\n[^v8qe4hrv1fd]: I might need to require every \\(F\\) to have a default action \\(d_F\\), so that I don't need to worry about axiom-of-choice issues when defining an agent over the space of all fair environments. \n\n[^b498knpuloj]: I specified a probabilistic environment and mixed strategies because I think there should be a unique fixed point for agents, such that this is well-defined for any fair environment \\(F\\). (By analogy to reflective oracles.) But I might be wrong, or I might need further restrictions on \\(F\\). \n\n[^yoon1wwm15]: Grossly underspecified. What kinds of properties are required for subjective probabilities here? You can obviously cheat by writing BlueEyedBot into your probability estimator. \n\n[^99exe3f94kb]: This is an infinite recursion, of course. It works if we require each \\(B_m\\) to have a strictly lower complexity in some sense than \\(A\\) (e.g. the rank of an agent is the largest number \\(K\\) of environments it can reason about when making any decision, and each \\(B_m\\) needs to be lower-rank than \\(A\\)), but I worry that's too strong of a restriction and would exclude some well-definable and interesting agents. \n\n[^6a5o5hvwt1l]: Does the fairness requirement on the \\(B_m\\) suffice to avert the MetaBlueEyedBot problem in general? I'm unsure.",
      "plaintextDescription": "My current candidate definitions, with some significant issues in the footnotes:\n\nA fair environment is a probabilistic function F(x1,...,xN)=[X1,...,XN] from an array of actions to an array of payoffs. \n\nAn agent A is a random variable \n\nA(F,A1,...,Ai−1,Ai=A,Ai+1,...,AN) \n\nwhich takes in a fair environment F[1] and a list of agents (including itself), and outputs a mixed strategy over its available actions in F. [2]\n\nA fair agent is one whose mixed strategy is a function of subjective probabilities[3] that it assigns to [the actions of some finite collection of agents in fair environments, where any agents not appearing in the original problem must themselves be fair]. \n\nFormally, if A is a fair agent in with a subjective probability estimator P, A's mixed strategy in a fair environment F,\n\nA(F,A1,...,Ai−1,Ai=A,Ai+1,...,AN)\n\nshould depend only on a finite collection of A's subjective probabilities about outcomes \n\n{P(Fk(A1,...,AN,B1,...BM))=[X1,...,XN+M]}Kk=1 \n\nfor a set of fair environments F1,...,FK and an additional set of fair[4] agents[5] B1,...,BM if needed (note that not all agents need to appear in all environments). \n\nA fair problem is a fair environment with one designated player, where all other agents are fair agents.\n\n 1. ^\n    I might need to require every F to have a default action dF, so that I don't need to worry about axiom-of-choice issues when defining an agent over the space of all fair environments.\n\n 2. ^\n    I specified a probabilistic environment and mixed strategies because I think there should be a unique fixed point for agents, such that this is well-defined for any fair environment F. (By analogy to reflective oracles.) But I might be wrong, or I might need further restrictions on F.\n\n 3. ^\n    Grossly underspecified. What kinds of properties are required for subjective probabilities here? You can obviously cheat by writing BlueEyedBot into your probability estimator.\n\n 4. ^\n    This is an infinite recursion, of course. It works if we r"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-03-25T05:04:13.587Z",
    "af": true
  },
  {
    "_id": "CgjKcnJmznEGTPxEi",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "**How do you formalize the definition of a decision-theoretically fair problem, even when abstracting away the definition of an agent as well as embedded agency? **\n\nI've failed to find anything in our literature.\n\nIt's simple to define a fair *environment*, given those abstractions: a function E from an array of actions to an array of payoffs, with no reference to any other details of the non-embedded agents that took those actions and received those payoffs.\n\nHowever, fair problems are more than just fair environments: we want a definition of a fair problem (and fair agents) under which, among other things:\n\n*   The classic Newcomb's Problem against Omega, with certainty or with 1% random noise: **fair**\n*   Omega puts $1M in the box iff it predicts that the player *consciously endorses* one-boxing, regardless of what it predicts the player will *actually do* (e.g. misunderstand the instructions and take a different action than they endorsed): **unfair**\n*   Prisoner's Dilemma between two agents who base their actions on not only each others' predicted actions in the current environment, but also their predicted actions in *other* defined-as-fair dilemmas: **fair**\n    *   For example, PrudentBot will cooperate with you if it deduces that you will cooperate with it and *also* that you would defect against DefectBot, because it wants to exploit CooperateBots).\n*   Prisoner's Dilemma between two agents who base their actions on each others' predicted actions in defined-as-unfair dilemmas: **unfair**\n    *   It would let us smuggle in unfairness from other dilemmas; e.g. if BlueEyedBot only tries Löbian cooperation against agents with blue eyes, and MetaBlueEyedBot only tries Löbian cooperation against agents that predictably cooperate with BlueEyedBot, then the Prisoner's Dilemma against MetaBlueEyedBot should count as unfair.\n\nModal combat doesn't need to worry about this, because all the agents in it are fair-by-construction.\n\nYeah, I know, it's about a decade late to be asking this question.",
      "plaintextDescription": "How do you formalize the definition of a decision-theoretically fair problem, even when abstracting away the definition of an agent as well as embedded agency? \n\nI've failed to find anything in our literature.\n\nIt's simple to define a fair environment, given those abstractions: a function E from an array of actions to an array of payoffs, with no reference to any other details of the non-embedded agents that took those actions and received those payoffs.\n\nHowever, fair problems are more than just fair environments: we want a definition of a fair problem (and fair agents) under which, among other things:\n\n * The classic Newcomb's Problem against Omega, with certainty or with 1% random noise: fair\n * Omega puts $1M in the box iff it predicts that the player consciously endorses one-boxing, regardless of what it predicts the player will actually do (e.g. misunderstand the instructions and take a different action than they endorsed): unfair\n * Prisoner's Dilemma between two agents who base their actions on not only each others' predicted actions in the current environment, but also their predicted actions in other defined-as-fair dilemmas: fair\n   * For example, PrudentBot will cooperate with you if it deduces that you will cooperate with it and also that you would defect against DefectBot, because it wants to exploit CooperateBots).\n * Prisoner's Dilemma between two agents who base their actions on each others' predicted actions in defined-as-unfair dilemmas: unfair\n   * It would let us smuggle in unfairness from other dilemmas; e.g. if BlueEyedBot only tries Löbian cooperation against agents with blue eyes, and MetaBlueEyedBot only tries Löbian cooperation against agents that predictably cooperate with BlueEyedBot, then the Prisoner's Dilemma against MetaBlueEyedBot should count as unfair.\n\nModal combat doesn't need to worry about this, because all the agents in it are fair-by-construction.\n\nYeah, I know, it's about a decade late to be asking this question."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-03-25T02:06:36.076Z",
    "af": true
  },
  {
    "_id": "mHJRftQZYiKjqmmwS",
    "postId": "qYPHryHTNiJ2y6Fhi",
    "parentCommentId": "sFGGdoenAuFChEJwy",
    "topLevelCommentId": "sFGGdoenAuFChEJwy",
    "contents": {
      "markdown": "Over the past three years, as my timelines have shortened and my hopes for alignment or coordination have dwindled, I've switched over to consumption. I just make sure to keep a long runway, so that I could pivot if AGI progress is somehow halted or sputters out on its own or something.",
      "plaintextDescription": "Over the past three years, as my timelines have shortened and my hopes for alignment or coordination have dwindled, I've switched over to consumption. I just make sure to keep a long runway, so that I could pivot if AGI progress is somehow halted or sputters out on its own or something."
    },
    "baseScore": 6,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-02-13T01:19:30.513Z",
    "af": false
  },
  {
    "_id": "hJSZ4e7kiNRbyL925",
    "postId": "3S4nyoNEEuvNsbXt8",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "The fault does not lie with Jacob, but wow, this post aged like an open bag of bread.",
      "plaintextDescription": "The fault does not lie with Jacob, but wow, this post aged like an open bag of bread."
    },
    "baseScore": 19,
    "voteCount": 20,
    "createdAt": null,
    "postedAt": "2024-12-06T06:57:31.638Z",
    "af": false
  },
  {
    "_id": "9sKAoEbXZBfd75dh9",
    "postId": "LaYNLeqwSSuPLouXH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I suggest a fourth default question for these reading groups:\n\nHow did this post age?",
      "plaintextDescription": "I suggest a fourth default question for these reading groups:\n\nHow did this post age?"
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-12-03T22:36:30.758Z",
    "af": false
  },
  {
    "_id": "SYEbvpuHfrYdxrBto",
    "postId": "HsznWM9A7NiuGsp28",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Soon the two are lost in a maze of words defined in other words, the problem that Steven Harnad once [described](http://www.ecs.soton.ac.uk/%7Eharnad/Papers/Harnad/harnad90.sgproblem.html) as trying to learn Chinese from a Chinese/Chinese dictionary.\n\nOf course, it turned out that LLMs do this just fine, thank you.",
      "plaintextDescription": "> Soon the two are lost in a maze of words defined in other words, the problem that Steven Harnad once described as trying to learn Chinese from a Chinese/Chinese dictionary.\n\nOf course, it turned out that LLMs do this just fine, thank you."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-10-14T00:55:49.293Z",
    "af": false
  },
  {
    "_id": "m4BkEFNEXdik2kRBz",
    "postId": "WBw8dDkAWohFjWQSk",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> intensional terms\n\nShould probably link to [Extensions and Intensions](https://www.lesswrong.com/posts/HsznWM9A7NiuGsp28/extensions-and-intensions); not everyone reads these posts in order.",
      "plaintextDescription": "> intensional terms\n\nShould probably link to Extensions and Intensions; not everyone reads these posts in order."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-10-14T00:46:54.300Z",
    "af": false
  },
  {
    "_id": "hrZ5CzmFGeLKTGMh8",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": "HWakXzyz4zjZzumQR",
    "topLevelCommentId": "xzyfS77dq9yTHd8PA",
    "contents": {
      "markdown": "Mati [described himself as a TPM since September 2023 (after being PM support since April 2022)](https://www.linkedin.com/in/matiroy/), and Andrei [described himself as a Research Engineer](https://www.linkedin.com/in/aalexaa/) from April 2023 to March 2024. Why do you believe either was not a FTE at the time?\n\nAnd while failure to sign isn't proof of lack of desire to sign, the two are heavily correlated—otherwise it would be incredibly unlikely for the small Superalignment team to have so many members who signed late or not at all.",
      "plaintextDescription": "Mati described himself as a TPM since September 2023 (after being PM support since April 2022), and Andrei described himself as a Research Engineer from April 2023 to March 2024. Why do you believe either was not a FTE at the time?\n\nAnd while failure to sign isn't proof of lack of desire to sign, the two are heavily correlated—otherwise it would be incredibly unlikely for the small Superalignment team to have so many members who signed late or not at all."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-07T18:38:45.509Z",
    "af": false
  },
  {
    "_id": "xzyfS77dq9yTHd8PA",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "With the sudden simultaneous exits of Mira Murati, Barret Zoph, and Bob McGrew, I thought I'd update [my tally of the departures](https://www.lesswrong.com/posts/pJgwg5Tn4v3KSpKAL/orthonormal-s-shortform?commentId=dgYqM5PkzDZNCmwZn) from OpenAI, collated with how quickly the ex-employee had signed the loyalty letter to Sam Altman last November. \n\nThe letter was leaked at [505 signatures](https://www.documentcloud.org/documents/24172246-letter-to-the-openai-board-google-docs), 667 signatures, and finally [702 signatures](https://gwern.net/doc/reinforcement-learning/openai/2023-openai.pdf); in the end, it was reported that 737 of 770 employees signed. Since then, I've been able to verify 56 departures of people who were full-time employees (as far as I can tell, contractors were not allowed to sign, but all FTEs were).\n\nI still think I'm missing some, so these are lower bounds (modulo any mistakes I've made).\n\nHeadline numbers:\n\n*   Attrition for the 505 OpenAI employees who signed before the letter was first leaked: at least 24/505 = 4.8%\n*   Attrition for the next 197 to sign (it was leaked again at 667 signatures, and one last time at 702): at least 13/197 = 6.6%\n*   Attrition for the (reported) 68 who had not signed by the last leak: at least 19/68 = **27.9%**.\n\nReportedly, 737 out of the 770 signed in the end, and many of the Superalignment team chose not to sign at all.\n\n**Below are my current tallies of some notable subsets. Please comment with any corrections!**\n\nPeople from the Superalignment team who never signed as of the 702 leak (including some policy/governance people who seem to have been closely connected) and are now gone:\n\n*   Carroll Wainwright\n*   Collin Burns\n*   Cullen O'Keefe\n*   Daniel Kokotajlo\n*   Jan Leike (though he did separately Tweet that the board should resign)\n*   Jeffrey Wu\n*   Jonathan Uesato\n*   Leopold Aschenbrenner\n*   Mati Roy\n*   William Saunders\n*   Yuri Burda\n\nPeople from the Superalignment team (and close collaborators) who did sign before the final leak but are now gone:\n\n*   Jan Hendrik Kirchner (signed between 668 and 702)\n*   Steven Bills (signed between 668 and 702)\n*   John Schulman (signed between 506 and 667)\n*   Sherry Lachman (signed between 506 and 667)\n*   Ilya Sutskever (signed by 505)\n*   Pavel Izmailov (signed by 505)\n*   Ryan Lowe (signed by 505)\n*   Todor Markov (signed by 505)\n\nOthers who didn't sign as of the 702 leak (some of whom may have just been AFK for the wrong weekend, though I doubt that was true of Karpathy) and are now gone:\n\n*   Andrei Alexandru (Research Engineer)\n*   Andrej Karpathy (Co-Founder)\n*   Austin Wiseman (Finance/Accounting)\n*   Girish Sastry (Policy)\n*   Jay Joshi (Recruiting)\n*   Katarina Slama (Member of Technical Staff)\n*   Lucas Negritto (Member of Technical Staff, then Developer Community Ambassador)\n*   Zarina Stanik (Marketing)\n\nNotable other ex-employees:\n\n*   Barrett Zoph (VP of Research, Post-Training; signed by 505)\n*   Bob McGrew (Chief Research Officer; signed by 505)\n*   Chris Clark (Head of Nonprofit and Strategic Initiatives; signed by 505)\n*   Diane Yoon (VP of People; signed by 505)\n*   Gretchen Krueger (Policy; signed by 505; posted a [significant Twitter thread](https://x.com/GretchenMarina/status/1793403475260551517) at the time she left)\n*   Mira Murati (CTO; signed by 505)",
      "plaintextDescription": "With the sudden simultaneous exits of Mira Murati, Barret Zoph, and Bob McGrew, I thought I'd update my tally of the departures from OpenAI, collated with how quickly the ex-employee had signed the loyalty letter to Sam Altman last November. \n\nThe letter was leaked at 505 signatures, 667 signatures, and finally 702 signatures; in the end, it was reported that 737 of 770 employees signed. Since then, I've been able to verify 56 departures of people who were full-time employees (as far as I can tell, contractors were not allowed to sign, but all FTEs were).\n\nI still think I'm missing some, so these are lower bounds (modulo any mistakes I've made).\n\nHeadline numbers:\n\n * Attrition for the 505 OpenAI employees who signed before the letter was first leaked: at least 24/505 = 4.8%\n * Attrition for the next 197 to sign (it was leaked again at 667 signatures, and one last time at 702): at least 13/197 = 6.6%\n * Attrition for the (reported) 68 who had not signed by the last leak: at least 19/68 = 27.9%.\n\nReportedly, 737 out of the 770 signed in the end, and many of the Superalignment team chose not to sign at all.\n\nBelow are my current tallies of some notable subsets. Please comment with any corrections!\n\nPeople from the Superalignment team who never signed as of the 702 leak (including some policy/governance people who seem to have been closely connected) and are now gone:\n\n * Carroll Wainwright\n * Collin Burns\n * Cullen O'Keefe\n * Daniel Kokotajlo\n * Jan Leike (though he did separately Tweet that the board should resign)\n * Jeffrey Wu\n * Jonathan Uesato\n * Leopold Aschenbrenner\n * Mati Roy\n * William Saunders\n * Yuri Burda\n\nPeople from the Superalignment team (and close collaborators) who did sign before the final leak but are now gone:\n\n * Jan Hendrik Kirchner (signed between 668 and 702)\n * Steven Bills (signed between 668 and 702)\n * John Schulman (signed between 506 and 667)\n * Sherry Lachman (signed between 506 and 667)\n * Ilya Sutskever (signed by 505)\n * Pavel Izmai"
    },
    "baseScore": 81,
    "voteCount": 38,
    "createdAt": null,
    "postedAt": "2024-10-01T04:14:17.860Z",
    "af": true
  },
  {
    "_id": "Fnt6hpJgLDhmws5ro",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": "dgYqM5PkzDZNCmwZn",
    "topLevelCommentId": "dgYqM5PkzDZNCmwZn",
    "contents": {
      "markdown": "EDIT: On reflection, I made this a [full Shortform post](https://www.lesswrong.com/posts/pJgwg5Tn4v3KSpKAL/orthonormal-s-shortform?commentId=xzyfS77dq9yTHd8PA).\n\nWith the sudden simultaneous exits of Mira Murati, Barret Zoph, and Bob McGrew, I thought I'd do a more thorough scan of the departures. I still think I'm missing some, so these are lower bounds (modulo any mistakes I've made).\n\nHeadline numbers:\n\n*   Attrition for the 505 OpenAI employees who signed before the letter was first leaked: at least 24/505 = 4.8%\n*   Attrition for the next 197 to sign (it was leaked again at 667 signatures, and one last time at 702): at least 13/197 = 6.6%\n*   Attrition for the (reported) 68 who had not signed by the last leak: at least 19/68 = **27.9%**.\n\nReportedly, 737 out of the 770 signed in the end, and many of the Superalignment team chose not to sign at all.\n\n**Below are my current tallies of some notable subsets. Please comment with any corrections!**\n\nPeople from the Superalignment team who never signed as of the 702 leak (including some policy/governance people who seem to have been closely connected) and are now gone:\n\n*   Carroll Wainwright\n*   Collin Burns\n*   Cullen O'Keefe\n*   Daniel Kokotajlo\n*   Jan Leike (though he did separately Tweet that the board should resign)\n*   Jeffrey Wu\n*   Jonathan Uesato\n*   Leopold Aschenbrenner\n*   Mati Roy\n*   William Saunders\n*   Yuri Burda\n\nPeople from the Superalignment team (and close collaborators) who did sign before the final leak but are now gone:\n\n*   Jan Hendrik Kirchner (signed between 668 and 702)\n*   Steven Bills (signed between 668 and 702)\n*   John Schulman (signed between 506 and 667)\n*   Sherry Lachman (signed between 506 and 667)\n*   Ilya Sutskever (signed by 505)\n*   Pavel Izmailov (signed by 505)\n*   Ryan Lowe (signed by 505)\n*   Todor Markov (signed by 505)\n\nOthers who didn't sign as of the 702 leak (some of whom may have just been AFK for the wrong weekend, though I doubt that was true of Karpathy) and are now gone:\n\n*   Andrei Alexandru (Research Engineer)\n*   Andrej Karpathy (Co-Founder)\n*   Austin Wiseman (Finance/Accounting)\n*   Girish Sastry (Policy)\n*   Jay Joshi (Recruiting)\n*   Katarina Slama (Member of Technical Staff)\n*   Lucas Negritto (Member of Technical Staff, then Developer Community Ambassador)\n*   Zarina Stanik (Marketing)\n\nNotable other ex-employees:\n\n*   Barrett Zoph (VP of Research, Post-Training; signed by 505)\n*   Bob McGrew (Chief Research Officer; signed by 505)\n*   Chris Clark (Head of Nonprofit and Strategic Initiatives; signed by 505)\n*   Diane Yoon (VP of People; signed by 505)\n*   Gretchen Krueger (Policy; signed by 505; posted a [significant Twitter thread](https://x.com/GretchenMarina/status/1793403475260551517) at the time)\n*   Mira Murati (CTO; signed by 505)",
      "plaintextDescription": "EDIT: On reflection, I made this a full Shortform post.\n\nWith the sudden simultaneous exits of Mira Murati, Barret Zoph, and Bob McGrew, I thought I'd do a more thorough scan of the departures. I still think I'm missing some, so these are lower bounds (modulo any mistakes I've made).\n\nHeadline numbers:\n\n * Attrition for the 505 OpenAI employees who signed before the letter was first leaked: at least 24/505 = 4.8%\n * Attrition for the next 197 to sign (it was leaked again at 667 signatures, and one last time at 702): at least 13/197 = 6.6%\n * Attrition for the (reported) 68 who had not signed by the last leak: at least 19/68 = 27.9%.\n\nReportedly, 737 out of the 770 signed in the end, and many of the Superalignment team chose not to sign at all.\n\nBelow are my current tallies of some notable subsets. Please comment with any corrections!\n\nPeople from the Superalignment team who never signed as of the 702 leak (including some policy/governance people who seem to have been closely connected) and are now gone:\n\n * Carroll Wainwright\n * Collin Burns\n * Cullen O'Keefe\n * Daniel Kokotajlo\n * Jan Leike (though he did separately Tweet that the board should resign)\n * Jeffrey Wu\n * Jonathan Uesato\n * Leopold Aschenbrenner\n * Mati Roy\n * William Saunders\n * Yuri Burda\n\nPeople from the Superalignment team (and close collaborators) who did sign before the final leak but are now gone:\n\n * Jan Hendrik Kirchner (signed between 668 and 702)\n * Steven Bills (signed between 668 and 702)\n * John Schulman (signed between 506 and 667)\n * Sherry Lachman (signed between 506 and 667)\n * Ilya Sutskever (signed by 505)\n * Pavel Izmailov (signed by 505)\n * Ryan Lowe (signed by 505)\n * Todor Markov (signed by 505)\n\nOthers who didn't sign as of the 702 leak (some of whom may have just been AFK for the wrong weekend, though I doubt that was true of Karpathy) and are now gone:\n\n * Andrei Alexandru (Research Engineer)\n * Andrej Karpathy (Co-Founder)\n * Austin Wiseman (Finance/Accounting)\n * Girish Sas"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-01T03:19:29.720Z",
    "af": false
  },
  {
    "_id": "qHjEQvyTbkHpSpSd6",
    "postId": "5qbcmKdfWc7vskrRD",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> CDT agents respond well to threats\n\nMight want to rephrase this as \"CDT agents give in to threats\"",
      "plaintextDescription": "> CDT agents respond well to threats\n\nMight want to rephrase this as \"CDT agents give in to threats\""
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-09-28T00:19:38.138Z",
    "af": false
  },
  {
    "_id": "TaBHaQLDA2a2CZv8T",
    "postId": "awKbxtfFAfu7xDXdQ",
    "parentCommentId": "ZyyKndtMtDPJJGHaR",
    "topLevelCommentId": "ZyyKndtMtDPJJGHaR",
    "contents": {
      "markdown": "This is weirdly meta.",
      "plaintextDescription": "This is weirdly meta."
    },
    "baseScore": 5,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-09-02T01:19:05.717Z",
    "af": false
  },
  {
    "_id": "nboNJrcy2foRhHir7",
    "postId": "2tKbDKGLXtEHGtGLn",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> If families are worried about the cost of groceries, they should welcome this price discrimination. The AI will realize you are worried about costs. It will offer you prime discounts to win your business. It will know you are willing to switch brands to get discounts, and use this to balance inventory.\n> \n> Then it will go out and charge other people more, because they can afford to pay. Indeed, this is highly progressive policy. The wealthier you are, the more you will pay for groceries. What’s not to love?\n\nA problem is that this is not only a tax on indifference, but also a tax on innumeracy and on lack of leisure time. Those who don't know how to properly comparison shop are likely to be less wealthy, not more; same with those who don't have the spare time to go to more than one store.",
      "plaintextDescription": "> If families are worried about the cost of groceries, they should welcome this price discrimination. The AI will realize you are worried about costs. It will offer you prime discounts to win your business. It will know you are willing to switch brands to get discounts, and use this to balance inventory.\n> \n> Then it will go out and charge other people more, because they can afford to pay. Indeed, this is highly progressive policy. The wealthier you are, the more you will pay for groceries. What’s not to love?\n\nA problem is that this is not only a tax on indifference, but also a tax on innumeracy and on lack of leisure time. Those who don't know how to properly comparison shop are likely to be less wealthy, not more; same with those who don't have the spare time to go to more than one store."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-08-26T20:41:28.532Z",
    "af": false
  },
  {
    "_id": "7LgBvpwLdwRsw33hz",
    "postId": "2ne9taAPiGqoTLXJJ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Re: experience machine, Past Me would have refused it and Present Me would take it. The difference is due to a major (and seemingly irreversible) deterioration in my wellbeing several years ago, but not *only* because that makes the real world less enjoyable. \n\nAgency is another big reason to refuse the experience machine; if I think I can make a difference in the base-level world, I feel a moral responsibility towards it. But I experience significantly less agency now (and project less agency in the future), so that factor is diminished for me.\n\nThe main factor that's still operative is epistemics: I would much rather my beliefs be accurate than be deceived about the world. But it's hard for that to outweigh the unhappiness at this point.\n\nSo if a lot of people would choose the Experience Machine, that suggests they are some combination of unhappy, not confident in their agency, and not obsessed with their epistemics. (Which does, I think, operationalize your \"something is very wrong\".)",
      "plaintextDescription": "Re: experience machine, Past Me would have refused it and Present Me would take it. The difference is due to a major (and seemingly irreversible) deterioration in my wellbeing several years ago, but not only because that makes the real world less enjoyable. \n\nAgency is another big reason to refuse the experience machine; if I think I can make a difference in the base-level world, I feel a moral responsibility towards it. But I experience significantly less agency now (and project less agency in the future), so that factor is diminished for me.\n\nThe main factor that's still operative is epistemics: I would much rather my beliefs be accurate than be deceived about the world. But it's hard for that to outweigh the unhappiness at this point.\n\nSo if a lot of people would choose the Experience Machine, that suggests they are some combination of unhappy, not confident in their agency, and not obsessed with their epistemics. (Which does, I think, operationalize your \"something is very wrong\".)"
    },
    "baseScore": 7,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2024-08-26T04:42:01.988Z",
    "af": false
  },
  {
    "_id": "e9dsupATQEbTqDYzM",
    "postId": "awKbxtfFAfu7xDXdQ",
    "parentCommentId": "rCvPJaxt6wRB63ZeD",
    "topLevelCommentId": "rCvPJaxt6wRB63ZeD",
    "contents": {
      "markdown": "1.  Thanks—I didn't recall the content of Yglesias' tweet, and I'd noped out of sorting through his long feed. I suspect Yglesias didn't understand *why* the numbers were weird, though, and people who read his tweet were even less likely to get it. And most significantly, he tries to draw a conclusion from a spurious fact!\n2.  Allowing explicitly conditional markets with a different fee structure (ideally, all fees refunded on the counterfactual markets) could be an interesting public service on Manifold's part.\n3.  The only part of my tone that worries me in retrospect is that I should have done more to indicate that *you personally* were trying to do a good thing, and I'm criticizing the deference to conditional markets rather than criticizing your actions. I'll see if I can edit the post to improve on that axis.\n4.  I think we still differ on that. Even though the numbers for the main contenders were just a few points apart, there was massive jockeying to put certain candidates at the top end of that range, because relative position is what viewers noticed.",
      "plaintextDescription": " 1. Thanks—I didn't recall the content of Yglesias' tweet, and I'd noped out of sorting through his long feed. I suspect Yglesias didn't understand why the numbers were weird, though, and people who read his tweet were even less likely to get it. And most significantly, he tries to draw a conclusion from a spurious fact!\n 2. Allowing explicitly conditional markets with a different fee structure (ideally, all fees refunded on the counterfactual markets) could be an interesting public service on Manifold's part.\n 3. The only part of my tone that worries me in retrospect is that I should have done more to indicate that you personally were trying to do a good thing, and I'm criticizing the deference to conditional markets rather than criticizing your actions. I'll see if I can edit the post to improve on that axis.\n 4. I think we still differ on that. Even though the numbers for the main contenders were just a few points apart, there was massive jockeying to put certain candidates at the top end of that range, because relative position is what viewers noticed."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-08-26T04:05:20.706Z",
    "af": false
  },
  {
    "_id": "doRpbgchpbhvPF3ZB",
    "postId": "awKbxtfFAfu7xDXdQ",
    "parentCommentId": "HehBPPwbWGLinMdaN",
    "topLevelCommentId": "HehBPPwbWGLinMdaN",
    "contents": {
      "markdown": "I'm really impressed with your grace in writing this comment (as well as the one you wrote on the market itself), and it makes me feel better about Manifold's public epistemics.",
      "plaintextDescription": "I'm really impressed with your grace in writing this comment (as well as the one you wrote on the market itself), and it makes me feel better about Manifold's public epistemics."
    },
    "baseScore": 19,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2024-08-09T04:54:02.871Z",
    "af": false
  },
  {
    "_id": "8gNhSxvyFFigaFayE",
    "postId": "awKbxtfFAfu7xDXdQ",
    "parentCommentId": "xQFoszxdYtEnBAhni",
    "topLevelCommentId": "xQFoszxdYtEnBAhni",
    "contents": {
      "markdown": "Yes, and I gained some easy mana from such markets; but the market that got the most attention by far was the intrinsically flawed conditional market.",
      "plaintextDescription": "Yes, and I gained some easy mana from such markets; but the market that got the most attention by far was the intrinsically flawed conditional market."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-08-06T18:41:44.948Z",
    "af": false
  },
  {
    "_id": "NCxXHTibRQ63KyooY",
    "postId": "awKbxtfFAfu7xDXdQ",
    "parentCommentId": "u6fov45oB4XKeD54G",
    "topLevelCommentId": "u6fov45oB4XKeD54G",
    "contents": {
      "markdown": "Real-money markets do have stronger incentives for sharps to scour for arbitrage, so the 1/1/26 market would have been more likely to be noticed before months had gone by.\n\nHowever (depending on the fee structure for resolving N/A markets), real-money markets have *even stronger* incentives for sharps to stay away entirely from spurious conditional markets, since they'd be throwing away cash and not just Internet points. Never ever *ever* cite out-of-the-money conditional markets.",
      "plaintextDescription": "Real-money markets do have stronger incentives for sharps to scour for arbitrage, so the 1/1/26 market would have been more likely to be noticed before months had gone by.\n\nHowever (depending on the fee structure for resolving N/A markets), real-money markets have even stronger incentives for sharps to stay away entirely from spurious conditional markets, since they'd be throwing away cash and not just Internet points. Never ever ever cite out-of-the-money conditional markets."
    },
    "baseScore": 8,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2024-08-06T17:00:07.104Z",
    "af": false
  },
  {
    "_id": "ra6Fs4TXvpRHK7b99",
    "postId": "awKbxtfFAfu7xDXdQ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Broke: Prediction markets are an aggregation of opinions, weighted towards informed opinions by smart people, and are therefore a trustworthy forecasting tool on any question.\n\nWoke: Prediction markets are MMOs set in a fantasy world where, if someone is Wrong On The Internet, you can take their lunch money.",
      "plaintextDescription": "Broke: Prediction markets are an aggregation of opinions, weighted towards informed opinions by smart people, and are therefore a trustworthy forecasting tool on any question.\n\nWoke: Prediction markets are MMOs set in a fantasy world where, if someone is Wrong On The Internet, you can take their lunch money."
    },
    "baseScore": 89,
    "voteCount": 58,
    "createdAt": null,
    "postedAt": "2024-08-06T03:14:44.441Z",
    "af": false
  },
  {
    "_id": "o5sfFWiFCNmrtjp88",
    "postId": "bnx4FEwxTkNm7TWCR",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Can you share any strong evidence that you're an unusually trustworthy person in regard to confidential conversations? People would in fact be risking a lot by talking to you.\n\n(This is sincere btw; I think this service should absolutely exist, but the best version of it is probably done by someone with a longstanding public reputation of circumspection.)",
      "plaintextDescription": "Can you share any strong evidence that you're an unusually trustworthy person in regard to confidential conversations? People would in fact be risking a lot by talking to you.\n\n(This is sincere btw; I think this service should absolutely exist, but the best version of it is probably done by someone with a longstanding public reputation of circumspection.)"
    },
    "baseScore": 22,
    "voteCount": 13,
    "createdAt": null,
    "postedAt": "2024-07-28T03:07:34.805Z",
    "af": false
  },
  {
    "_id": "gJ5WCdwzkBno67m8E",
    "postId": "Y4hFATbyPL9Zp3frX",
    "parentCommentId": "SGN2QWxTJsbmut3Kn",
    "topLevelCommentId": "w9BiPcmNRPJbwEe2d",
    "contents": {
      "markdown": "Good question! I picked it up from a friend at a LW meetup a decade ago, so it didn't come with all the extra baggage that vipassana meditation seems to usually carry. So this is just going to be the echo of it that works for me.\n\nStep 1 is to stare at your index finger (a very sensitive part of your body) and gently, patiently try to notice that it's still producing a background level of sensory stimulus even when it's not touching anything. That attention to the background signal, focused on a small patch of your body, is what the body scan is based on.\n\nStep 2 is learning how to \"move\" that awareness of the background signal slowly. Try to smoothly shift that awareness down your finger, knuckle by knuckle, keeping the area of awareness small by ceasing to focus on the original spot as you focus on a new spot. Then try moving that spot of awareness gradually to the base of your thumb, and noticing the muscle beneath the skin.\n\nUse Case α is harnessing that kind of awareness to relax physical tension and even pain. The next time you have a paper cut or a small burn, once you've dealt with it in the obvious objective ways and now just have to handle the pain, focus your awareness right on that spot. The sensation will still be loud, but it won't be overwhelming when you're focusing on it rather than fleeing from it. Or the next time you notice a particularly tense muscle, focus your awareness there; for me, that usually loosens it at least a little.\n\nStep 3 is the body scan itself: creating awareness for each part of your skin and muscles, gradually, bit by bit, starting from the crown of your head and slowly tracing out a path that covers everything. This is where a guided meditation could really help. I don't have one to recommend (after having the guided meditation at the meetup, I got as much of the idea as I needed), but hopefully some of the hundreds out there are as good as Random Meditating Rationalist #37 was.\n\nAnd Use Case β, when you have a migraine, is to imagine moving that awareness inside your skull, to the place where the migraine pain feels like it's concentrated. (I recommend starting from a place where the migraine seems to \"surface\"—for me, the upper orbit of my left eye—if you have such a spot.)\n\nThere's something quite odd about how this works: your brain doesn't have pain receptors, so the pain from the migraine ends up in some phantom location on your [body map](https://en.wikipedia.org/wiki/Cortical_homunculus), and it's (conveniently?) interpreted as being inside your head. By tracing your awareness inside your skull, you walk along that body map to the same phantom location as that pain, so it works out basically the same as if you were in Use Case α.\n\nHope this helps!",
      "plaintextDescription": "Good question! I picked it up from a friend at a LW meetup a decade ago, so it didn't come with all the extra baggage that vipassana meditation seems to usually carry. So this is just going to be the echo of it that works for me.\n\nStep 1 is to stare at your index finger (a very sensitive part of your body) and gently, patiently try to notice that it's still producing a background level of sensory stimulus even when it's not touching anything. That attention to the background signal, focused on a small patch of your body, is what the body scan is based on.\n\nStep 2 is learning how to \"move\" that awareness of the background signal slowly. Try to smoothly shift that awareness down your finger, knuckle by knuckle, keeping the area of awareness small by ceasing to focus on the original spot as you focus on a new spot. Then try moving that spot of awareness gradually to the base of your thumb, and noticing the muscle beneath the skin.\n\nUse Case α is harnessing that kind of awareness to relax physical tension and even pain. The next time you have a paper cut or a small burn, once you've dealt with it in the obvious objective ways and now just have to handle the pain, focus your awareness right on that spot. The sensation will still be loud, but it won't be overwhelming when you're focusing on it rather than fleeing from it. Or the next time you notice a particularly tense muscle, focus your awareness there; for me, that usually loosens it at least a little.\n\nStep 3 is the body scan itself: creating awareness for each part of your skin and muscles, gradually, bit by bit, starting from the crown of your head and slowly tracing out a path that covers everything. This is where a guided meditation could really help. I don't have one to recommend (after having the guided meditation at the meetup, I got as much of the idea as I needed), but hopefully some of the hundreds out there are as good as Random Meditating Rationalist #37 was.\n\nAnd Use Case β, when you have a migraine, is t"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-06-21T17:24:26.793Z",
    "af": false
  },
  {
    "_id": "MzLoRvoGtJoFngvgu",
    "postId": "3nDR23ksSQJ98WNDm",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I have to further compliment my past self: this section aged *extremely* well, prefiguring the Shoggoth-with-a-smiley-face analogies several years in advance.\n\n> GPT-3 is trained simply to predict continuations of text. So what would it actually optimize for, if it had a pretty good model of the world including itself and the ability to make plans in that world?\n> \n> One might hope that because it's learning to imitate humans in an unsupervised way, that it would end up fairly human, or at least act in that way. I very much doubt this, for the following reason:\n> \n> *   Two humans are fairly similar to each other, because they have very similar architectures and are learning to succeed in the same environment.\n> *   Two convergently evolved species will be similar in some ways but not others, because they have [different architectures but the same environmental pressures](https://en.wikipedia.org/wiki/Convergent_evolution).\n> *   A [mimic species](https://en.wikipedia.org/wiki/Mimicry) will be similar in some ways but not others to the species it mimics, because even if they share recent ancestry, the environmental pressures on the poisonous one are different from the environmental pressures on the mimic.\n> \n> What we have with the GPTs is the first deep learning architecture we've found that scales this well in the domain (so, probably not that much like our particular architecture), learning to mimic humans rather than growing in an environment with similar pressures. Why should we expect it to be anything but very alien under the hood, or to continue acting human once its actions take us outside of the training distribution?\n> \n> Moreover, there may be much more going on under the hood than we realize; it may take much more general cognitive power to learn and imitate the patterns of humans, than it requires us to execute those patterns.",
      "plaintextDescription": "I have to further compliment my past self: this section aged extremely well, prefiguring the Shoggoth-with-a-smiley-face analogies several years in advance.\n\n> GPT-3 is trained simply to predict continuations of text. So what would it actually optimize for, if it had a pretty good model of the world including itself and the ability to make plans in that world?\n> \n> One might hope that because it's learning to imitate humans in an unsupervised way, that it would end up fairly human, or at least act in that way. I very much doubt this, for the following reason:\n> \n>  * Two humans are fairly similar to each other, because they have very similar architectures and are learning to succeed in the same environment.\n>  * Two convergently evolved species will be similar in some ways but not others, because they have different architectures but the same environmental pressures.\n>  * A mimic species will be similar in some ways but not others to the species it mimics, because even if they share recent ancestry, the environmental pressures on the poisonous one are different from the environmental pressures on the mimic.\n> \n> What we have with the GPTs is the first deep learning architecture we've found that scales this well in the domain (so, probably not that much like our particular architecture), learning to mimic humans rather than growing in an environment with similar pressures. Why should we expect it to be anything but very alien under the hood, or to continue acting human once its actions take us outside of the training distribution?\n> \n> Moreover, there may be much more going on under the hood than we realize; it may take much more general cognitive power to learn and imitate the patterns of humans, than it requires us to execute those patterns."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-06-20T19:06:09.401Z",
    "af": true
  },
  {
    "_id": "3Hk3brKyN6HxjFajd",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": "hrpG2TsAnEuKsC3ez",
    "topLevelCommentId": "hrpG2TsAnEuKsC3ez",
    "contents": {
      "markdown": "The spun-off agent foundations team seems to have less reason than most AI safety orgs to be in the Bay Area, so moving to NZ might be worth considering for them.",
      "plaintextDescription": "The spun-off agent foundations team seems to have less reason than most AI safety orgs to be in the Bay Area, so moving to NZ might be worth considering for them."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-06-20T17:26:39.314Z",
    "af": false
  },
  {
    "_id": "6R5HJGomiTiTYSWav",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": "dgYqM5PkzDZNCmwZn",
    "topLevelCommentId": "dgYqM5PkzDZNCmwZn",
    "contents": {
      "markdown": "Note on current methodology:\n\n*   I am, for now, not doing further research when the spreadsheet lists a person whose name appears on the final leaked letter; so it's possible that some of the 23 departures among the 702 names on the final leaked letter are spurious. (I will be more thorough when I resolve the market after November.)\n*   I am counting only full-time employees and not counting contractors, as I currently believe that the 770 figure refers only to full-time employees. So far, I've seen no contractors among those who signed, but I've only checked a few; if the letter includes some categories of contractors, this gets a lot harder to resolve.\n*   I am counting nontechnical employees (e.g. recruiting, marketing) as well as technical staff, because such employees were among those who signed the letter.",
      "plaintextDescription": "Note on current methodology:\n\n * I am, for now, not doing further research when the spreadsheet lists a person whose name appears on the final leaked letter; so it's possible that some of the 23 departures among the 702 names on the final leaked letter are spurious. (I will be more thorough when I resolve the market after November.)\n * I am counting only full-time employees and not counting contractors, as I currently believe that the 770 figure refers only to full-time employees. So far, I've seen no contractors among those who signed, but I've only checked a few; if the letter includes some categories of contractors, this gets a lot harder to resolve.\n * I am counting nontechnical employees (e.g. recruiting, marketing) as well as technical staff, because such employees were among those who signed the letter."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-06-20T17:21:33.090Z",
    "af": false
  },
  {
    "_id": "b6eLkSCePbmze76sx",
    "postId": "oeZ93QTv39TeB94Wt",
    "parentCommentId": "aqc6DqStXuDjLimLh",
    "topLevelCommentId": "aqc6DqStXuDjLimLh",
    "contents": {
      "markdown": "Counterpoint: other labs might become more paranoid that SSI is ahead of them. I think your point is probably more correct than the counterpoint, but it's worth mentioning.",
      "plaintextDescription": "Counterpoint: other labs might become more paranoid that SSI is ahead of them. I think your point is probably more correct than the counterpoint, but it's worth mentioning."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-06-20T16:49:23.236Z",
    "af": false
  },
  {
    "_id": "pdeWuk5iFCFuHtwRt",
    "postId": "oeZ93QTv39TeB94Wt",
    "parentCommentId": "LGQgibsRkmNkmFDqb",
    "topLevelCommentId": "rfvYT2msJKAts3FCB",
    "contents": {
      "markdown": "Elon diversifies in the sense of \"personally micromanaging more companies\", not in the sense of \"backing companies he can't micromanage\".",
      "plaintextDescription": "Elon diversifies in the sense of \"personally micromanaging more companies\", not in the sense of \"backing companies he can't micromanage\"."
    },
    "baseScore": 7,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2024-06-20T16:46:50.423Z",
    "af": false
  },
  {
    "_id": "dgYqM5PkzDZNCmwZn",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "By my assessment, the employees who failed to sign the final leaked version of the Altman loyalty letter have now been literally [decimated](https://en.wikipedia.org/wiki/Decimation_(punishment)).\n\nI'm trying to track the relative attrition for [a Manifold market](https://manifold.markets/PatrickLV/how-many-openai-employees-who-were): of the 265 OpenAI employees who hadn't yet signed the loyalty letter by the time it was first leaked, what percent will still be at OpenAI on the one-year anniversary?\n\nI'm combining [that first leaked copy](https://www.documentcloud.org/documents/24172246-letter-to-the-openai-board-google-docs) with 505 signatures, the [final leaked copy](https://gwern.net/doc/reinforcement-learning/openai/2023-openai.pdf) with 702 signatures, the oft-repeated total headcount of 770, and [this spreadsheet](https://docs.google.com/spreadsheets/d/1B1iFZO26Dpl2iN1Jv1ZugyLktX_qxEQRaYkUHaRR8GY/) tracking OpenAI departures (albeit with many false positives—people self-reporting as OpenAI employees because they customized their GPTs—so I'm working to verify names that appear on the spreadsheet but not on the letter; I'm sure the spreadsheet has false negatives as well, alas).\n\nSo far, I've verified at least ~seven~ **\\[update: seven, with a probable eighth\\]** departures of eligible figures who hadn't signed the letter with 702 names: Leopold Aschenbrenner, Jay Joshi (not fully verified by me), Andrej Karpathy, Daniel Kokotajlo, Jan Leike, Lucas Negritto, Katarina Slama, and William Saunders. If it's true that the total headcount at the time was 770, then that's 8 out of 68, or 11.8%.\n\nCompare that to the attrition rate (as per the spreadsheet) for those who had signed the final leaked version but not the first: 10 departures out of 197, or 5.1%; and compare *that* to the attrition rate for those who signed promptly: 13 departures out of 505, or 2.6%.\n\nAny causal inferences from this correlation are left as an exercise to the reader.\n\n(A more important exercise, however: can anyone find a confirmation of the 770 number outside of unsourced media reports, or find a copy of the loyalty letter with more than 702 signatories, or ideally find a list of everyone at OpenAI at the time? I've tried a few different avenues without success.)",
      "plaintextDescription": "By my assessment, the employees who failed to sign the final leaked version of the Altman loyalty letter have now been literally decimated.\n\nI'm trying to track the relative attrition for a Manifold market: of the 265 OpenAI employees who hadn't yet signed the loyalty letter by the time it was first leaked, what percent will still be at OpenAI on the one-year anniversary?\n\nI'm combining that first leaked copy with 505 signatures, the final leaked copy with 702 signatures, the oft-repeated total headcount of 770, and this spreadsheet tracking OpenAI departures (albeit with many false positives—people self-reporting as OpenAI employees because they customized their GPTs—so I'm working to verify names that appear on the spreadsheet but not on the letter; I'm sure the spreadsheet has false negatives as well, alas).\n\nSo far, I've verified at least seven [update: seven, with a probable eighth] departures of eligible figures who hadn't signed the letter with 702 names: Leopold Aschenbrenner, Jay Joshi (not fully verified by me), Andrej Karpathy, Daniel Kokotajlo, Jan Leike, Lucas Negritto, Katarina Slama, and William Saunders. If it's true that the total headcount at the time was 770, then that's 8 out of 68, or 11.8%.\n\nCompare that to the attrition rate (as per the spreadsheet) for those who had signed the final leaked version but not the first: 10 departures out of 197, or 5.1%; and compare that to the attrition rate for those who signed promptly: 13 departures out of 505, or 2.6%.\n\nAny causal inferences from this correlation are left as an exercise to the reader.\n\n(A more important exercise, however: can anyone find a confirmation of the 770 number outside of unsourced media reports, or find a copy of the loyalty letter with more than 702 signatories, or ideally find a list of everyone at OpenAI at the time? I've tried a few different avenues without success.)"
    },
    "baseScore": 52,
    "voteCount": 28,
    "createdAt": null,
    "postedAt": "2024-06-19T22:46:41.258Z",
    "af": false
  },
  {
    "_id": "cqkfA3E7aYowTTddp",
    "postId": "oeZ93QTv39TeB94Wt",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'm not even angry, just disappointed.",
      "plaintextDescription": "I'm not even angry, just disappointed."
    },
    "baseScore": 27,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2024-06-19T17:49:31.645Z",
    "af": false
  },
  {
    "_id": "w9BiPcmNRPJbwEe2d",
    "postId": "Y4hFATbyPL9Zp3frX",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "The diagnosis is roughly correct (I would say \"most suffering is caused by an internal response of fleeing from pain but not escaping it\"), but IMO the standard proffered remedy (Buddhist-style detachment from wanting) goes too far and promises too much.\n\nRe: the diagnosis, three illustrative ways the relationship between pain, awareness, and suffering has manifested for me:\n\n*   Migraines: I get them every few weeks, and they're pretty bad. After a friend showed me how to do a vipassana body scan, on a lark I tried moving my attention to the spot inside my skull where the migraine was most intense. To my relief, this helped the suffering greatly; the pain was caused by the migraine but the suffering was caused by trying futilely to not feel the pain, and staring directly at it was a good remedy.\n*   Mental health: I'm chronically depressed and anxious. (I am *not* asking for advice about it, and there's a unique causal element so your advice is less likely than usual to help.) One thing I figured out is that I can \"wallow\" in it: optimize for feeling it as intensely as possible. For example, if I'm feeling miserable I'll intentionally lie in bed in the dark listening to my saddest music. This genuinely helps make things more bearable, and helps the worst moods pass faster, compared to the approach of trying to distract myself from the feelings or to force a contrary feeling.\n*   Psychedelics: The worst hour of my life was spent on a bad acid trip, feeling nauseous and wanting to escape that feeling, and getting stuck in a \"turning away from awareness\" motion (reflected in visual hallucination by trying to mentally push my visual field up and to the left, only for it to reset to its actual location several times per second). That was a tremendous mistake.\n\nHowever, my depression sometimes manifests as anhedonia, i.e. *true* detachment from desire, and that's *really* not all it's cracked up to be. I'm not *suffering* when I lie around all day with anhedonia, but I'm not getting any positive valence from it, and meanwhile I'm stagnating as a person. And I genuinely do not see how to wallow in anhedonia, to turn my awareness inward and find something to engage with. I've tried. It just seems like nobody's home in that state.\n\nA key, I suspect, is happiness set point. A person who takes up Buddhism or a similar practice, and starts to experience their preferences less strongly, ends up hovering more stably around their happiness set point without the highs and lows that come with excitement, anticipation, triumph, disappointment, etc. Lows hurt more than highs, so this mental motion is a net improvement. Most people have a pretty good happiness set point, so it feels like a good end result. (And in most cases people stop before ceasing to have preferences entirely; there's probably even a golden mean where they're more effective in the real world than either their natural state or a zen-wireheaded extreme.)\n\nBut I don't see much proof that detachment from desire *moves* one's happiness set point, so advertising it as a cure for unhappiness feels like the same sort of error as talking about how everyone—even poor people—should buy index funds. (Which is to say, it's good advice on the margin for the majority of people, but for some people it's irrelevant, and the correct first advice is more like \"how to get a job, or a better job\" or \"how to budget\" or \"how to cheaply refinance your credit card debt\", etc.)\n\nAnd also, I'm dubious that \\[intentionally reducing the intensity of your good moments\\] is actually helpful in the same way that \\[intentionally reducing the intensity of your bad moments\\] is? Sometimes you happen to know that a good thing is going to happen with very little risk of failure. In that case, it seems strictly better to want and desire and expect that good thing.\n\n**In short, I highly recommend turning towards experiences of pain rather than fleeing from them; but I think the Buddhist thesis is questionable.**",
      "plaintextDescription": "The diagnosis is roughly correct (I would say \"most suffering is caused by an internal response of fleeing from pain but not escaping it\"), but IMO the standard proffered remedy (Buddhist-style detachment from wanting) goes too far and promises too much.\n\nRe: the diagnosis, three illustrative ways the relationship between pain, awareness, and suffering has manifested for me:\n\n * Migraines: I get them every few weeks, and they're pretty bad. After a friend showed me how to do a vipassana body scan, on a lark I tried moving my attention to the spot inside my skull where the migraine was most intense. To my relief, this helped the suffering greatly; the pain was caused by the migraine but the suffering was caused by trying futilely to not feel the pain, and staring directly at it was a good remedy.\n * Mental health: I'm chronically depressed and anxious. (I am not asking for advice about it, and there's a unique causal element so your advice is less likely than usual to help.) One thing I figured out is that I can \"wallow\" in it: optimize for feeling it as intensely as possible. For example, if I'm feeling miserable I'll intentionally lie in bed in the dark listening to my saddest music. This genuinely helps make things more bearable, and helps the worst moods pass faster, compared to the approach of trying to distract myself from the feelings or to force a contrary feeling.\n * Psychedelics: The worst hour of my life was spent on a bad acid trip, feeling nauseous and wanting to escape that feeling, and getting stuck in a \"turning away from awareness\" motion (reflected in visual hallucination by trying to mentally push my visual field up and to the left, only for it to reset to its actual location several times per second). That was a tremendous mistake.\n\nHowever, my depression sometimes manifests as anhedonia, i.e. true detachment from desire, and that's really not all it's cracked up to be. I'm not suffering when I lie around all day with anhedonia, but I'm not gettin"
    },
    "baseScore": 11,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-06-19T17:21:49.705Z",
    "af": false
  },
  {
    "_id": "JDx29mnCoixztHCWJ",
    "postId": "ASzyQrpGQsj7Moijk",
    "parentCommentId": "monseWq9GqjTPdv6Q",
    "topLevelCommentId": "monseWq9GqjTPdv6Q",
    "contents": {
      "markdown": "Go all-in on lobbying the US and other governments to fully prohibit the training of frontier models beyond a certain level, in a way that OpenAI can't route around (so probably block Altman's foreign chip factory initiative, for instance).",
      "plaintextDescription": "Go all-in on lobbying the US and other governments to fully prohibit the training of frontier models beyond a certain level, in a way that OpenAI can't route around (so probably block Altman's foreign chip factory initiative, for instance)."
    },
    "baseScore": 17,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2024-05-21T01:46:48.938Z",
    "af": false
  },
  {
    "_id": "jgeha8XWmwCG6u4eL",
    "postId": "dgFC394qZHgj2cWAg",
    "parentCommentId": "rZ8CqJkLoFgdb8njh",
    "topLevelCommentId": "rZ8CqJkLoFgdb8njh",
    "contents": {
      "markdown": "The chess example is meant to make specific points about RL\\*F concealing a capability that remains (or is even amplified); I'm not trying to claim that the \"put up a good fight but lose\" criterion is analogous to current RL\\*F criteria. (Though it does rhyme qualitatively with \"be helpful and harmless\".)\n\nI agree that \"helpful-only\" RL*F would result in a model that scores higher on capabilities evals than the base model, possibly much higher. I'm frankly a bit worried about even training that model.",
      "plaintextDescription": "The chess example is meant to make specific points about RL*F concealing a capability that remains (or is even amplified); I'm not trying to claim that the \"put up a good fight but lose\" criterion is analogous to current RL*F criteria. (Though it does rhyme qualitatively with \"be helpful and harmless\".)\n\nI agree that \"helpful-only\" RL*F would result in a model that scores higher on capabilities evals than the base model, possibly much higher. I'm frankly a bit worried about even training that model."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-04-04T19:36:57.290Z",
    "af": true
  },
  {
    "_id": "T9AxwLTanzMZJeWXz",
    "postId": "dgFC394qZHgj2cWAg",
    "parentCommentId": "Xd5Ygp6iWrqjs2B7M",
    "topLevelCommentId": "Xd5Ygp6iWrqjs2B7M",
    "contents": {
      "markdown": "Thank you! I'd forgotten about that.",
      "plaintextDescription": "Thank you! I'd forgotten about that."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-04-04T19:23:00.234Z",
    "af": true
  },
  {
    "_id": "nomrFmudRwpkBi6ws",
    "postId": "5Dz3ZrwBzzMfaucrH",
    "parentCommentId": "q6N7rAuM82ie6Y6wR",
    "topLevelCommentId": "AaapY2KK4fDvwWaRT",
    "contents": {
      "markdown": "> Certainly RLHF can get the model to stop talking about a capability, but usually this is extremely obvious because the model gives you an explicit refusal?\n\nHow certain are you that this is always true (rather than \"we've usually noticed this even though we haven't explicitly been checking for it in general\"), and that it will continue to be so as models become stronger?\n\nIt seems to me like additionally running evals on base models is a highly reasonable precaution.",
      "plaintextDescription": "> Certainly RLHF can get the model to stop talking about a capability, but usually this is extremely obvious because the model gives you an explicit refusal?\n\nHow certain are you that this is always true (rather than \"we've usually noticed this even though we haven't explicitly been checking for it in general\"), and that it will continue to be so as models become stronger?\n\nIt seems to me like additionally running evals on base models is a highly reasonable precaution."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-04-04T18:18:52.110Z",
    "af": false
  },
  {
    "_id": "agbgDEkdacZJSDryv",
    "postId": "5Dz3ZrwBzzMfaucrH",
    "parentCommentId": "jT37k5Brmz7hPGbp7",
    "topLevelCommentId": "AaapY2KK4fDvwWaRT",
    "contents": {
      "markdown": "Oh wait, I misinterpreted you as using \"much worse\" to mean \"much scarier\", when instead you mean \"much less capable\".\n\nI'd be glad if it were the case that RL\\*F doesn't hide any meaningful capabilities existing in the base model, but I'm not sure it is the case, and I'd sure like someone to check! It sure seems like RL\\*F is likely in some cases to get the model to stop explicitly *talking about* a capability it has (unless it is jailbroken on that subject), rather than to remove the capability.\n\n(Imagine RL*Fing a base model to stop explicitly talking about arithmetic; are we sure it would un-learn the rules?)",
      "plaintextDescription": "Oh wait, I misinterpreted you as using \"much worse\" to mean \"much scarier\", when instead you mean \"much less capable\".\n\nI'd be glad if it were the case that RL*F doesn't hide any meaningful capabilities existing in the base model, but I'm not sure it is the case, and I'd sure like someone to check! It sure seems like RL*F is likely in some cases to get the model to stop explicitly talking about a capability it has (unless it is jailbroken on that subject), rather than to remove the capability.\n\n(Imagine RL*Fing a base model to stop explicitly talking about arithmetic; are we sure it would un-learn the rules?)"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-04-03T23:16:51.650Z",
    "af": false
  },
  {
    "_id": "DtpQnrpXJNapMquZF",
    "postId": "5Dz3ZrwBzzMfaucrH",
    "parentCommentId": "jT37k5Brmz7hPGbp7",
    "topLevelCommentId": "AaapY2KK4fDvwWaRT",
    "contents": {
      "markdown": "That's exactly the point: if a model has bad capabilities and deceptive alignment, then testing the post-tuned model will return a false negative for those capabilities in deployment. Until we have the kind of interpretability tools that we could deeply trust to catch deceptive alignment, we should count any capability found in the base model as if it were present in the tuned model.",
      "plaintextDescription": "That's exactly the point: if a model has bad capabilities and deceptive alignment, then testing the post-tuned model will return a false negative for those capabilities in deployment. Until we have the kind of interpretability tools that we could deeply trust to catch deceptive alignment, we should count any capability found in the base model as if it were present in the tuned model."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-04-01T16:04:13.891Z",
    "af": false
  },
  {
    "_id": "AaapY2KK4fDvwWaRT",
    "postId": "5Dz3ZrwBzzMfaucrH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'd like to see evals like DeepMind's run against the strongest pre-RL*F base models, since that actually tells you about capability.",
      "plaintextDescription": "I'd like to see evals like DeepMind's run against the strongest pre-RL*F base models, since that actually tells you about capability."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-03-29T17:49:00.629Z",
    "af": false
  },
  {
    "_id": "CZ5ZDPYqrZeixfFhK",
    "postId": "QxAFoEdtsmK783jzM",
    "parentCommentId": "yWJwDWcGPnBaEDdDA",
    "topLevelCommentId": "myBDhkmkpeZXJv2ms",
    "contents": {
      "markdown": "The WWII generation is negligible in 2024. The actual effect is partly the inverted demographic pyramid (older population means more women than men even under normal circumstances), and partly that even young Russian men die horrifically often:\n\n> At 2005 mortality rates, for example, only 7% of UK men but 37% of Russian men would die before the age of 55 years\n\nAnd for that, [a major culprit is alcohol](https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(13)62247-3/fulltext) (leading to accidents and violence, but also literally drinking oneself to death).\n\nAmong the men who don't self-destruct, I imagine a large fraction have already been taken, meaning that the gender ratio among singles has to be off the charts.",
      "plaintextDescription": "The WWII generation is negligible in 2024. The actual effect is partly the inverted demographic pyramid (older population means more women than men even under normal circumstances), and partly that even young Russian men die horrifically often:\n\n> At 2005 mortality rates, for example, only 7% of UK men but 37% of Russian men would die before the age of 55 years\n\nAnd for that, a major culprit is alcohol (leading to accidents and violence, but also literally drinking oneself to death).\n\nAmong the men who don't self-destruct, I imagine a large fraction have already been taken, meaning that the gender ratio among singles has to be off the charts."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-02-21T20:23:29.242Z",
    "af": false
  },
  {
    "_id": "myBDhkmkpeZXJv2ms",
    "postId": "QxAFoEdtsmK783jzM",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> That first statistic, that it swiped right 353 times and got to talk to 160 women, is completely insane. I mean, that’s almost a 50% match rate, whereas estimates in general are 4% to 14%.\n\nGiven Russia's fucked-up gender ratio (2.5 single women for every single man), I don't think it's that unreasonable!\n\nGenerally, the achievement of \"guy finds a woman willing to accept a proposal\" impresses me far less in Russia than it would in the USA. Let's see if this replicates in a competitive dating pool.",
      "plaintextDescription": "> That first statistic, that it swiped right 353 times and got to talk to 160 women, is completely insane. I mean, that’s almost a 50% match rate, whereas estimates in general are 4% to 14%.\n\nGiven Russia's fucked-up gender ratio (2.5 single women for every single man), I don't think it's that unreasonable!\n\nGenerally, the achievement of \"guy finds a woman willing to accept a proposal\" impresses me far less in Russia than it would in the USA. Let's see if this replicates in a competitive dating pool."
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-02-19T06:57:15.742Z",
    "af": false
  },
  {
    "_id": "oxY9tuZ5SJe3SomDH",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "In high-leverage situations, you should arguably *either* be playing tic-tac-toe (simple, legible, predictable responses) or playing 4-D chess *to win*. If you're making really nonstandard and surprising moves (especially in PR), you have no excuse for winding up with a worse outcome than you would have if you'd acted in bog-standard normal ways.\n\n(This doesn't mean suspending your ethics! Those are part of winning! But if you can't figure out how to win 4-D chess ethically, then you need to play an ethical tic-tac-toe strategy instead.)",
      "plaintextDescription": "In high-leverage situations, you should arguably either be playing tic-tac-toe (simple, legible, predictable responses) or playing 4-D chess to win. If you're making really nonstandard and surprising moves (especially in PR), you have no excuse for winding up with a worse outcome than you would have if you'd acted in bog-standard normal ways.\n\n(This doesn't mean suspending your ethics! Those are part of winning! But if you can't figure out how to win 4-D chess ethically, then you need to play an ethical tic-tac-toe strategy instead.)"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-02-02T04:25:28.930Z",
    "af": false
  },
  {
    "_id": "DaJGwH3Efz25xd2se",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": "japJKpqtgZjiRSGim",
    "topLevelCommentId": "DeQFJTE8rcBkiK5r3",
    "contents": {
      "markdown": "Ah, I'm talking about introspection in a therapy context and not about exhorting others.\n\nFor example:\n\nInternal coherence: \"I forgive myself for doing that stupid thing\".\n\nLoad-bearing but opaque: \"It makes sense to forgive myself, and I want to, but for some reason I just can't\".\n\nLoad-bearing and clear resistance: \"I want *other people* to forgive *themselves* for things like that, but when I think about forgiving myself, I get a big NOPE NOPE NOPE\".\n\nP.S. Maybe forgiving oneself isn't actually the right thing to do at the moment! But it will also be easier to learn that in the third case than in the second.",
      "plaintextDescription": "Ah, I'm talking about introspection in a therapy context and not about exhorting others.\n\nFor example:\n\nInternal coherence: \"I forgive myself for doing that stupid thing\".\n\nLoad-bearing but opaque: \"It makes sense to forgive myself, and I want to, but for some reason I just can't\".\n\nLoad-bearing and clear resistance: \"I want other people to forgive themselves for things like that, but when I think about forgiving myself, I get a big NOPE NOPE NOPE\".\n\nP.S. Maybe forgiving oneself isn't actually the right thing to do at the moment! But it will also be easier to learn that in the third case than in the second."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-01-26T17:48:17.923Z",
    "af": false
  },
  {
    "_id": "DeQFJTE8rcBkiK5r3",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "\"I endorse endorsing X\" is a sign of a really promising topic for therapy (or your preferred modality of psychological growth).\n\nIf I can simply say \"X\", then I'm internally coherent enough on that point.\n\nIf I can only say \"I endorse X\", then not-X is psychologically load-bearing for me, but often in a way that is opaque to my conscious reasoning, so working on that conflict can be slippery.\n\nBut if I can only say \"I endorse endorsing X\", then not only is not-X load-bearing for me, but there's a clear feeling of resistance to X that I can consciously hone in on, connect with, and learn about.",
      "plaintextDescription": "\"I endorse endorsing X\" is a sign of a really promising topic for therapy (or your preferred modality of psychological growth).\n\nIf I can simply say \"X\", then I'm internally coherent enough on that point.\n\nIf I can only say \"I endorse X\", then not-X is psychologically load-bearing for me, but often in a way that is opaque to my conscious reasoning, so working on that conflict can be slippery.\n\nBut if I can only say \"I endorse endorsing X\", then not only is not-X load-bearing for me, but there's a clear feeling of resistance to X that I can consciously hone in on, connect with, and learn about."
    },
    "baseScore": 19,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-01-24T23:57:11.632Z",
    "af": false
  },
  {
    "_id": "ifpPHTk7kJHFentLt",
    "postId": "kFDk4Q9QhqrDE68qp",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Re: Canadian vs American health care, the reasonable policy would be:\n\n\"Sorry, publicly funded health care won't cover this, because the expected DALYs are too expensive. We do allow private clinics to sell you the procedure, though unless you're super wealthy I think the odds of success aren't worth the cost to your family.\"\n\n(I also approve of euthanasia being offered as long as it's not a hard sell.)",
      "plaintextDescription": "Re: Canadian vs American health care, the reasonable policy would be:\n\n\"Sorry, publicly funded health care won't cover this, because the expected DALYs are too expensive. We do allow private clinics to sell you the procedure, though unless you're super wealthy I think the odds of success aren't worth the cost to your family.\"\n\n(I also approve of euthanasia being offered as long as it's not a hard sell.)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-01-17T18:05:43.970Z",
    "af": false
  },
  {
    "_id": "Fne8ZbhetgfcbBWn8",
    "postId": "q3bJYTB3dGRf5fbD9",
    "parentCommentId": "AbGHJvFA9ysEHiEQA",
    "topLevelCommentId": "AbGHJvFA9ysEHiEQA",
    "contents": {
      "markdown": "I think MIRI is correct to call it as they see it, both on general principles and because if they turn out to be wrong about genuine alignment progress being very hard, people (at large, but also including us) *should* update against MIRI's viewpoints on other topics, and in favor of the viewpoints of whichever AI safety orgs called it more correctly.",
      "plaintextDescription": "I think MIRI is correct to call it as they see it, both on general principles and because if they turn out to be wrong about genuine alignment progress being very hard, people (at large, but also including us) should update against MIRI's viewpoints on other topics, and in favor of the viewpoints of whichever AI safety orgs called it more correctly."
    },
    "baseScore": 40,
    "voteCount": 18,
    "createdAt": null,
    "postedAt": "2024-01-05T20:25:41.319Z",
    "af": false
  },
  {
    "_id": "Gf23yhpphaHFeGfxB",
    "postId": "sGpBPAPq2QttY4M2H",
    "parentCommentId": "vaeGmW5DjJTQoLzBx",
    "topLevelCommentId": "SyGhxCtBsipWnjLgp",
    "contents": {
      "markdown": "Prior to hiring Shear, [the board offered a merger to Dario Amodei, with Dario to lead the merged entity. Dario rejected the offer.](https://www.reuters.com/technology/openais-board-approached-anthropic-ceo-about-top-job-merger-sources-2023-11-21/)",
      "plaintextDescription": "Prior to hiring Shear, the board offered a merger to Dario Amodei, with Dario to lead the merged entity. Dario rejected the offer."
    },
    "baseScore": 12,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2023-11-22T20:38:35.208Z",
    "af": false
  },
  {
    "_id": "qqpKEdYs8GMHPTH36",
    "postId": "ahNcJGNtTX8JvMy93",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> I mean, I don't really care how much e.g. Facebook AI thinks they're racing right now. They're not in the game at this point.\n\nThe race dynamics are not just about who's leading. FB is 1-2 years behind (looking at LLM metrics), and it doesn't seem like they're getting further behind OpenAI/Anthropic with each generation, so I expect that the lag at the end will be at most a few years.\n\nThat means that if Facebook is unconstrained, the leading labs have only that much time to slow down for safety (or prepare a pivotal act) as they approach AGI before Facebook gets there with total recklessness.\n\nIf Microsoft!OpenAI lags the new leaders by less than FB (and I think that's likely to be the case), that shortens the safety window further.\n\nI suspect my actual crux with you is your belief (correct me if I'm misinterpreting you) that your research program will solve alignment and that it will not take much of a safety window for the leading lab to incorporate the solution, and therefore the only thing that matters is finishing the solution and getting the leading lab on board. It would be very nice if you were right, but I put a low probability on it.",
      "plaintextDescription": "> I mean, I don't really care how much e.g. Facebook AI thinks they're racing right now. They're not in the game at this point.\n\nThe race dynamics are not just about who's leading. FB is 1-2 years behind (looking at LLM metrics), and it doesn't seem like they're getting further behind OpenAI/Anthropic with each generation, so I expect that the lag at the end will be at most a few years.\n\nThat means that if Facebook is unconstrained, the leading labs have only that much time to slow down for safety (or prepare a pivotal act) as they approach AGI before Facebook gets there with total recklessness.\n\nIf Microsoft!OpenAI lags the new leaders by less than FB (and I think that's likely to be the case), that shortens the safety window further.\n\nI suspect my actual crux with you is your belief (correct me if I'm misinterpreting you) that your research program will solve alignment and that it will not take much of a safety window for the leading lab to incorporate the solution, and therefore the only thing that matters is finishing the solution and getting the leading lab on board. It would be very nice if you were right, but I put a low probability on it."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2023-11-22T00:33:02.373Z",
    "af": false
  },
  {
    "_id": "4sFoqnuADTszD3s6b",
    "postId": "KXHMCH7wCxrvKsJyn",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'm surprised that nobody has yet brought up the development that [the board offered Dario Amodei the position as a merger with Anthropic](https://www.reuters.com/technology/openais-board-approached-anthropic-ceo-about-top-job-merger-sources-2023-11-21/) (and Dario said no!).\n\n(There's no additional important content in the original article by The Information, so I linked the Reuters paywall-free version.)\n\nCrucially, this doesn't tell us in what order the board made this offer to Dario and the other known figures (GitHub CEO Nat Friedman and Scale AI CEO Alex Wang) before getting Emmett Shear, but it's plausible that merging with Anthropic was Plan A all along. Moreover, I strongly suspect that the bad blood between Sam and the Anthropic team was strong enough that Sam had to be ousted in order for a merger to be possible.\n\nSo under this hypothesis, the board decided it was important to merge with Anthropic (probably to slow the arms race), booted Sam (using the additional fig leaf of whatever lies he's been caught in), immediately asked Dario and were surprised when he rejected them, did not have an adequate backup plan, and have been scrambling ever since.\n\nP.S. Shear is known to be very much on record worrying that alignment is necessary and not likely to be easy; I'm curious what Friedman and Wang are on record as saying about AI x-risk.",
      "plaintextDescription": "I'm surprised that nobody has yet brought up the development that the board offered Dario Amodei the position as a merger with Anthropic (and Dario said no!).\n\n(There's no additional important content in the original article by The Information, so I linked the Reuters paywall-free version.)\n\nCrucially, this doesn't tell us in what order the board made this offer to Dario and the other known figures (GitHub CEO Nat Friedman and Scale AI CEO Alex Wang) before getting Emmett Shear, but it's plausible that merging with Anthropic was Plan A all along. Moreover, I strongly suspect that the bad blood between Sam and the Anthropic team was strong enough that Sam had to be ousted in order for a merger to be possible.\n\nSo under this hypothesis, the board decided it was important to merge with Anthropic (probably to slow the arms race), booted Sam (using the additional fig leaf of whatever lies he's been caught in), immediately asked Dario and were surprised when he rejected them, did not have an adequate backup plan, and have been scrambling ever since.\n\nP.S. Shear is known to be very much on record worrying that alignment is necessary and not likely to be easy; I'm curious what Friedman and Wang are on record as saying about AI x-risk."
    },
    "baseScore": 42,
    "voteCount": 22,
    "createdAt": null,
    "postedAt": "2023-11-21T20:18:31.471Z",
    "af": false
  },
  {
    "_id": "iGvELjfDKmnvCnegE",
    "postId": "KXHMCH7wCxrvKsJyn",
    "parentCommentId": "ra4EjyhoJrSGqGbpm",
    "topLevelCommentId": "SfsTfdtJFeCSGzBxi",
    "contents": {
      "markdown": "No, I don't think the board's motives were power politics; I'm saying that they failed to account for the kind of political power moves that Sam would make in response.",
      "plaintextDescription": "No, I don't think the board's motives were power politics; I'm saying that they failed to account for the kind of political power moves that Sam would make in response."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2023-11-21T20:02:39.319Z",
    "af": false
  },
  {
    "_id": "NtG4FbKiBCeDehhD5",
    "postId": "RLAQz5KgSbJiJHoAC",
    "parentCommentId": "hKzHxiBPxcSSN6ucT",
    "topLevelCommentId": "hKzHxiBPxcSSN6ucT",
    "contents": {
      "markdown": "In addition to this, Microsoft will exert greater pressure to extract mundane commercial utility from models, compared to pushing forward the frontier. Not sure how much that compensates for the second round of evaporative cooling of the safety-minded.",
      "plaintextDescription": "In addition to this, Microsoft will exert greater pressure to extract mundane commercial utility from models, compared to pushing forward the frontier. Not sure how much that compensates for the second round of evaporative cooling of the safety-minded."
    },
    "baseScore": 10,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2023-11-20T20:58:20.891Z",
    "af": false
  },
  {
    "_id": "isWuCzffsnw5AAjqa",
    "postId": "KXHMCH7wCxrvKsJyn",
    "parentCommentId": "d7rNjxbr5kpFkpn6P",
    "topLevelCommentId": "SfsTfdtJFeCSGzBxi",
    "contents": {
      "markdown": "If they thought this would be the outcome of firing Sam, they would not have done so.\n\nThe risk they took was calculated, but man, are they bad at politics.",
      "plaintextDescription": "If they thought this would be the outcome of firing Sam, they would not have done so.\n\nThe risk they took was calculated, but man, are they bad at politics."
    },
    "baseScore": 33,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2023-11-20T20:13:55.024Z",
    "af": false
  },
  {
    "_id": "uayAvsrd4PuhvuqKn",
    "postId": "KXHMCH7wCxrvKsJyn",
    "parentCommentId": "6sijZ3ezNBkFzwov8",
    "topLevelCommentId": "qpxYosQmTjRguq7TX",
    "contents": {
      "markdown": "1.  The quote is from Emmett Shear, not a board member.\n2.  The board is also following the \"don't say anything literally false\" policy by saying practically nothing publicly.\n3.  Just as I infer from Shear's qualifier that the firing did have *something* to do with safety, I infer from the board's public silence that their reason for the firing isn't one that would win back the departing OpenAI members (or would only do so at a cost that's not worth paying). \n4.  This is consistent with it being a safety concern shared by the superalignment team (who by and large didn't sign the statement at first) but not by the rest of OpenAI (who view pushing capabilities forward as a good thing, because like Sam they believe the EV of OpenAI building AGI is better than the EV of unilaterally stopping). That's my current main hypothesis.",
      "plaintextDescription": " 1. The quote is from Emmett Shear, not a board member.\n 2. The board is also following the \"don't say anything literally false\" policy by saying practically nothing publicly.\n 3. Just as I infer from Shear's qualifier that the firing did have something to do with safety, I infer from the board's public silence that their reason for the firing isn't one that would win back the departing OpenAI members (or would only do so at a cost that's not worth paying). \n 4. This is consistent with it being a safety concern shared by the superalignment team (who by and large didn't sign the statement at first) but not by the rest of OpenAI (who view pushing capabilities forward as a good thing, because like Sam they believe the EV of OpenAI building AGI is better than the EV of unilaterally stopping). That's my current main hypothesis."
    },
    "baseScore": 26,
    "voteCount": 14,
    "createdAt": null,
    "postedAt": "2023-11-20T20:10:38.338Z",
    "af": false
  },
  {
    "_id": "gvmGHMAFTNtvN5FKe",
    "postId": "KXHMCH7wCxrvKsJyn",
    "parentCommentId": "SPDh7MqNkYBopxkE5",
    "topLevelCommentId": "SfsTfdtJFeCSGzBxi",
    "contents": {
      "markdown": "It's too late for a conditional surrender now that Microsoft is a credible threat to get 100% of OpenAI's capabilities team; Ilya and Jan are communicating unconditional surrender because the alternative is even worse.",
      "plaintextDescription": "It's too late for a conditional surrender now that Microsoft is a credible threat to get 100% of OpenAI's capabilities team; Ilya and Jan are communicating unconditional surrender because the alternative is even worse."
    },
    "baseScore": 14,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2023-11-20T18:27:56.778Z",
    "af": false
  },
  {
    "_id": "ze7dtSYAtymDsei3j",
    "postId": "KXHMCH7wCxrvKsJyn",
    "parentCommentId": "gMc4rZs6A3L6ke75v",
    "topLevelCommentId": "qpxYosQmTjRguq7TX",
    "contents": {
      "markdown": "I agree, it's critical to have a very close reading of \"The board did \\*not\\* remove Sam over any specific disagreement on safety\".\n\nThis is the kind of situation where every qualifier in a statement needs to be understood as essential—if the statement were true without the word \"specific\", then I can't imagine why that word would have been inserted.",
      "plaintextDescription": "I agree, it's critical to have a very close reading of \"The board did *not* remove Sam over any specific disagreement on safety\".\n\nThis is the kind of situation where every qualifier in a statement needs to be understood as essential—if the statement were true without the word \"specific\", then I can't imagine why that word would have been inserted."
    },
    "baseScore": 15,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2023-11-20T18:09:38.579Z",
    "af": false
  },
  {
    "_id": "SfsTfdtJFeCSGzBxi",
    "postId": "KXHMCH7wCxrvKsJyn",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "The most likely explanation I can think of, for what look like about-faces by Ilya and Jan this morning, is realizing that the worst plausible outcome is exactly what we're seeing: Sam running a new OpenAI at Microsoft, free of that pesky charter. Any amount of backpedaling, and even resigning in favor of a less safety-conscious board, is preferable to that.\n\nThey came at the king and missed.",
      "plaintextDescription": "The most likely explanation I can think of, for what look like about-faces by Ilya and Jan this morning, is realizing that the worst plausible outcome is exactly what we're seeing: Sam running a new OpenAI at Microsoft, free of that pesky charter. Any amount of backpedaling, and even resigning in favor of a less safety-conscious board, is preferable to that.\n\nThey came at the king and missed."
    },
    "baseScore": 59,
    "voteCount": 46,
    "createdAt": null,
    "postedAt": "2023-11-20T17:22:18.022Z",
    "af": false
  },
  {
    "_id": "t6pehekM6iq8uAztf",
    "postId": "vFqa8DZCuhyrbSnyx",
    "parentCommentId": "pjks4TuKbqeevMzmD",
    "topLevelCommentId": "XmiGoeEZcL9M89Pyt",
    "contents": {
      "markdown": "Did anyone at OpenAI explicitly say that a factor in their release cadence was getting the public to wake up about the pace of AI research and start demanding regulation? Because this seems more like a post hoc rationalization for the release policy than like an actual intended outcome.",
      "plaintextDescription": "Did anyone at OpenAI explicitly say that a factor in their release cadence was getting the public to wake up about the pace of AI research and start demanding regulation? Because this seems more like a post hoc rationalization for the release policy than like an actual intended outcome."
    },
    "baseScore": 13,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2023-11-11T20:08:24.809Z",
    "af": false
  },
  {
    "_id": "KyaBshopqnZrn5ZtH",
    "postId": "hc9nMipTXy2sm3tJb",
    "parentCommentId": "mmuzBNqrxE8RnjQRH",
    "topLevelCommentId": "qnx95TnrhjnuJ4j7W",
    "contents": {
      "markdown": "I expect AGI to emerge as part of the frontier model training run (and thus get a godshatter of human values), rather than only emerging after fine-tuning by a troll (and get a godshatter of reversed values), so I think \"humans modified to be happy with something much cheaper than our CEV\" is a more likely endstate than \"humans suffering\" (though, again, both much less likely than \"humans dead\").",
      "plaintextDescription": "I expect AGI to emerge as part of the frontier model training run (and thus get a godshatter of human values), rather than only emerging after fine-tuning by a troll (and get a godshatter of reversed values), so I think \"humans modified to be happy with something much cheaper than our CEV\" is a more likely endstate than \"humans suffering\" (though, again, both much less likely than \"humans dead\")."
    },
    "baseScore": 9,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2023-11-10T00:35:21.003Z",
    "af": false
  },
  {
    "_id": "xvzvHPszhLZBvggew",
    "postId": "hc9nMipTXy2sm3tJb",
    "parentCommentId": "qnx95TnrhjnuJ4j7W",
    "topLevelCommentId": "qnx95TnrhjnuJ4j7W",
    "contents": {
      "markdown": "Steelmanning a position I don't quite hold: non-extinction AI x-risk scenarios aren't limited to inescapable dystopias as we imagine them.\n\n\"Kill all humans\" is certainly an instrumental subgoal of \"take control of the future lightcone\" and it certainly gains an extra epsilon of resources compared to any form of not literally killing all humans, but it's not literally required, and there are all sorts of weird things the AGI could prefer to do with humanity instead depending on what kind of [godshatter](https://www.lesswrong.com/posts/cSXZpvqpa9vbGGLtG/thou-art-godshatter) it winds up with, most of which are so far outside the realm of human reckoning that I'm not sure it's reasonable to call them dystopian. (Far outside Weirdtopia, for that matter.)\n\nIt still seems *very likely* to me that a non-aligned superhuman AGI would kill humanity in the process of taking control of the future lightcone, but I'm not as sure of that as I'm sure that it would take control.",
      "plaintextDescription": "Steelmanning a position I don't quite hold: non-extinction AI x-risk scenarios aren't limited to inescapable dystopias as we imagine them.\n\n\"Kill all humans\" is certainly an instrumental subgoal of \"take control of the future lightcone\" and it certainly gains an extra epsilon of resources compared to any form of not literally killing all humans, but it's not literally required, and there are all sorts of weird things the AGI could prefer to do with humanity instead depending on what kind of godshatter it winds up with, most of which are so far outside the realm of human reckoning that I'm not sure it's reasonable to call them dystopian. (Far outside Weirdtopia, for that matter.)\n\nIt still seems very likely to me that a non-aligned superhuman AGI would kill humanity in the process of taking control of the future lightcone, but I'm not as sure of that as I'm sure that it would take control."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2023-11-08T19:17:24.930Z",
    "af": false
  },
  {
    "_id": "PSuFDXFRq54TWCa7i",
    "postId": "WJtq4DoyT9ovPyHjH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "\\[See corrections in replies; \"think for five minutes\" was in EY posts as far back as 2007, the HPMOR chapter was in 2010, and the first CFAR retreat (though not under that name) was 2011 IIRC. Still curious to know where he got it from.\\]\n\n~Before HPMOR, \"think for five minutes by the clock\" was a CFAR exercise; I don't recall where they picked it up from.~",
      "plaintextDescription": "[See corrections in replies; \"think for five minutes\" was in EY posts as far back as 2007, the HPMOR chapter was in 2010, and the first CFAR retreat (though not under that name) was 2011 IIRC. Still curious to know where he got it from.]\n\nBefore HPMOR, \"think for five minutes by the clock\" was a CFAR exercise; I don't recall where they picked it up from."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2023-11-08T18:51:44.856Z",
    "af": false
  },
  {
    "_id": "EjLoEA7DRsnhuvRNp",
    "postId": "cY43KWjamafLbij9C",
    "parentCommentId": "GezGKs4SPAnwoA27o",
    "topLevelCommentId": "GezGKs4SPAnwoA27o",
    "contents": {
      "markdown": "I think a substantial fraction of LWers have the (usually implicit—they may not have even read about simulacra) belief that higher levels are inherently morally problematic, and that engaging on those levels about an important topic is at best excusable under [the kind of adversarial circumstances where direct lies are excusable](https://www.lesswrong.com/posts/xdwbX9pFEr7Pomaxv/meta-honesty-firming-up-honesty-around-its-edge-cases). (There's the obvious selection effect where people who feel gross about higher levels feel more comfortable on LW than almost anywhere else.)\n\nI think there need to be better public arguments against that viewpoint, not least because I'm not fully convinced it's wrong.",
      "plaintextDescription": "I think a substantial fraction of LWers have the (usually implicit—they may not have even read about simulacra) belief that higher levels are inherently morally problematic, and that engaging on those levels about an important topic is at best excusable under the kind of adversarial circumstances where direct lies are excusable. (There's the obvious selection effect where people who feel gross about higher levels feel more comfortable on LW than almost anywhere else.)\n\nI think there need to be better public arguments against that viewpoint, not least because I'm not fully convinced it's wrong."
    },
    "baseScore": 15,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2023-10-28T17:18:49.416Z",
    "af": false
  },
  {
    "_id": "WKa3AB3aMDXThvDxb",
    "postId": "aW288uWABwTruBmgF",
    "parentCommentId": "mZ2AkPSzjcQptwTqL",
    "topLevelCommentId": "AZKDRHcnGshHuvzCR",
    "contents": {
      "markdown": "Elizabeth has put at least dozens of hours into seeking good RCTs on vegan nutrition, and has come up nearly empty. At this point, if you want to say there is an expert consensus that disagrees with her, *you need to find a particular study that you are willing to stand behind*, so that we can discuss it. This is why Elizabeth wrote a post on the Adventist study—because that was the best that people were throwing at her.",
      "plaintextDescription": "Elizabeth has put at least dozens of hours into seeking good RCTs on vegan nutrition, and has come up nearly empty. At this point, if you want to say there is an expert consensus that disagrees with her, you need to find a particular study that you are willing to stand behind, so that we can discuss it. This is why Elizabeth wrote a post on the Adventist study—because that was the best that people were throwing at her."
    },
    "baseScore": 4,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2023-10-03T19:48:43.653Z",
    "af": false
  },
  {
    "_id": "wqthuXPksoDDcwj6i",
    "postId": "aW288uWABwTruBmgF",
    "parentCommentId": "NnyGjyF9jdCyuz5nQ",
    "topLevelCommentId": "AZKDRHcnGshHuvzCR",
    "contents": {
      "markdown": "This is a pretty transparent isolated demand for rigor. Can you tell me you've never uncritically cited surveys of self-reported data that make veg*n diets look good?",
      "plaintextDescription": "This is a pretty transparent isolated demand for rigor. Can you tell me you've never uncritically cited surveys of self-reported data that make veg*n diets look good?"
    },
    "baseScore": 4,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2023-10-03T19:43:53.106Z",
    "af": false
  },
  {
    "_id": "mcYA4HpdgPNpHW2iD",
    "postId": "aW288uWABwTruBmgF",
    "parentCommentId": "HDGEZSZeeASFNuPDP",
    "topLevelCommentId": "yeZzfs58LYPetqK4G",
    "contents": {
      "markdown": "Simply type the at-symbol to tag people. I don't know when LW added this, but I'm glad we have it.",
      "plaintextDescription": "Simply type the at-symbol to tag people. I don't know when LW added this, but I'm glad we have it."
    },
    "baseScore": 2,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-10-02T17:37:04.341Z",
    "af": false
  },
  {
    "_id": "NvF3apKDG7AXNDAZu",
    "postId": "aW288uWABwTruBmgF",
    "parentCommentId": "GkY6vMBAXSAieRGm6",
    "topLevelCommentId": "GkY6vMBAXSAieRGm6",
    "contents": {
      "markdown": "Your framing makes it sound like individual raising of livestock, which is silly—specialization of expertise and labor is a very good thing, and \"EA reducetarians find or start up a reasonably sized farm whose animal welfare standards seem to them to be net positive\" seems to dominate \"each EA reducetarian tries to personally raise chickens in a net positive way\" (even for those who think both are bad, the second one seems simply worse at a fixed level of consumption).",
      "plaintextDescription": "Your framing makes it sound like individual raising of livestock, which is silly—specialization of expertise and labor is a very good thing, and \"EA reducetarians find or start up a reasonably sized farm whose animal welfare standards seem to them to be net positive\" seems to dominate \"each EA reducetarian tries to personally raise chickens in a net positive way\" (even for those who think both are bad, the second one seems simply worse at a fixed level of consumption)."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-10-02T17:34:07.320Z",
    "af": false
  },
  {
    "_id": "4fJSh26uCr2F4jSYC",
    "postId": "aW288uWABwTruBmgF",
    "parentCommentId": "ZJde3sTdzuEaoaFfi",
    "topLevelCommentId": "yeZzfs58LYPetqK4G",
    "contents": {
      "markdown": "Seems fair to tag [@Liron](https://www.lesswrong.com/users/liron?mention=user) here.",
      "plaintextDescription": "Seems fair to tag @Liron here."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-10-02T17:20:35.529Z",
    "af": false
  },
  {
    "_id": "nrd2a6jS5MMitGQvq",
    "postId": "8XAxbsdtLmMaf5zta",
    "parentCommentId": "hkvkCXjeaNrXCRiop",
    "topLevelCommentId": "mrJzoHJQjEAA3FHMn",
    "contents": {
      "markdown": "I agree with \"When you say 'there's a good chance AGI is near', the general public will hear 'AGI is near'\".\n\nHowever, the general public isn't everyone, and *the people who can distinguish between the two claims* are the most important to reach (per capita, and possibly in sum).\n\nSo we'll do better by saying what we actually believe, while taking into account that some audiences will round probabilities off (and seeking ways to be rounded closer to the truth while still communicating accurately to anyone who does understand probabilistic claims). The marginal gain by rounding ourselves off at the start isn't worth the marginal loss by looking transparently overconfident to those who can tell the difference.",
      "plaintextDescription": "I agree with \"When you say 'there's a good chance AGI is near', the general public will hear 'AGI is near'\".\n\nHowever, the general public isn't everyone, and the people who can distinguish between the two claims are the most important to reach (per capita, and possibly in sum).\n\nSo we'll do better by saying what we actually believe, while taking into account that some audiences will round probabilities off (and seeking ways to be rounded closer to the truth while still communicating accurately to anyone who does understand probabilistic claims). The marginal gain by rounding ourselves off at the start isn't worth the marginal loss by looking transparently overconfident to those who can tell the difference."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-10-01T18:54:44.039Z",
    "af": true
  },
  {
    "_id": "wMZFWvvmW4mhTx8th",
    "postId": "8XAxbsdtLmMaf5zta",
    "parentCommentId": "mrJzoHJQjEAA3FHMn",
    "topLevelCommentId": "mrJzoHJQjEAA3FHMn",
    "contents": {
      "markdown": "I reached this via Joachim [pointing it out as an example](https://www.lesswrong.com/posts/aW288uWABwTruBmgF/ea-vegan-advocacy-is-not-truthseeking-and-it-s-everyone-s-1?commentId=uw4v7TbFLRkapPKtb) of someone urging epistemic defection around AI alignment, and I have to agree with him there. I think the higher difficulty posed by communicating \"we think there's a substantial probability that AGI happens in the next 10 years\" vs \"AGI is near\" is worth it even from a PR perspective, because pretending you know the day and the hour smells like bullshit to the most important people who need convincing that AI alignment is nontrivial.",
      "plaintextDescription": "I reached this via Joachim pointing it out as an example of someone urging epistemic defection around AI alignment, and I have to agree with him there. I think the higher difficulty posed by communicating \"we think there's a substantial probability that AGI happens in the next 10 years\" vs \"AGI is near\" is worth it even from a PR perspective, because pretending you know the day and the hour smells like bullshit to the most important people who need convincing that AI alignment is nontrivial."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-09-30T17:19:20.051Z",
    "af": true
  },
  {
    "_id": "ytchYiQtApnw8GL4q",
    "postId": "aW288uWABwTruBmgF",
    "parentCommentId": "NxqH7BypMGKdS5tuu",
    "topLevelCommentId": "yeZzfs58LYPetqK4G",
    "contents": {
      "markdown": "Fair enough, I've changed my wording.",
      "plaintextDescription": "Fair enough, I've changed my wording."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2023-09-30T00:49:52.852Z",
    "af": false
  },
  {
    "_id": "wvS387yx3LbRP4v6K",
    "postId": "aW288uWABwTruBmgF",
    "parentCommentId": "GqkLor9PJ3peCvP4Q",
    "topLevelCommentId": "yeZzfs58LYPetqK4G",
    "contents": {
      "markdown": "The thread is closer to this post's Counter-Examples than its examples. \n\nRichard calls out the protest for making arguments that diverge from the protesters' actual beliefs about what's worth protesting, and is highly upvoted for doing so. In the ensuing discussion, Steven changes Holly and Ben's minds on whether it's right to use the \"not really open-source\" accusation against FB (because we think true open-source would be even worse).\n\nTyler's comment that \\[for public persuasion, messages get rounded to \"yay X\" or \"boo X\" anyway, so ~it's not worth worrying about nuance~ nuance is less important\\] deserves a rebuttal, but I note that it's already got 8 disagrees vs 4 agrees, so I don't think that viewpoint is dominant.",
      "plaintextDescription": "The thread is closer to this post's Counter-Examples than its examples. \n\nRichard calls out the protest for making arguments that diverge from the protesters' actual beliefs about what's worth protesting, and is highly upvoted for doing so. In the ensuing discussion, Steven changes Holly and Ben's minds on whether it's right to use the \"not really open-source\" accusation against FB (because we think true open-source would be even worse).\n\nTyler's comment that [for public persuasion, messages get rounded to \"yay X\" or \"boo X\" anyway, so it's not worth worrying about nuance nuance is less important] deserves a rebuttal, but I note that it's already got 8 disagrees vs 4 agrees, so I don't think that viewpoint is dominant."
    },
    "baseScore": 16,
    "voteCount": 11,
    "createdAt": null,
    "postedAt": "2023-09-29T06:31:23.184Z",
    "af": false
  },
  {
    "_id": "TJ9DLrG8vQgci6ncp",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "RrGs8QZYGkaCxckGq",
    "topLevelCommentId": "RrGs8QZYGkaCxckGq",
    "contents": {
      "markdown": "I think the downvotes are coming because people don't realize you're doing the exercise at the start of the post, and rather think that you're making these claims after having read the rest of the post. I don't think you should lose karma for that, so I'm upvoting; but you may want to state at the top that's what you're doing.",
      "plaintextDescription": "I think the downvotes are coming because people don't realize you're doing the exercise at the start of the post, and rather think that you're making these claims after having read the rest of the post. I don't think you should lose karma for that, so I'm upvoting; but you may want to state at the top that's what you're doing."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2023-09-09T18:35:03.107Z",
    "af": false
  },
  {
    "_id": "znwpFEWJzb6htLXsu",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "MQ4AQm9taDYaeRd6F",
    "topLevelCommentId": "MQ4AQm9taDYaeRd6F",
    "contents": {
      "markdown": "It's a very unusual disclaimer that speaks well of the post.\n\nThe default journalistic practice at many outlets is to do an asymmetric search once the journalist or editor decides which way the wind is blowing, but of course nobody says this in the finished piece.\n\nBen is explicitly telling the reader that he did not spend another hundred hours looking for positive information about Nonlinear, so that we understand that absence of exculpatory evidence in the post should not be treated as strong evidence of absence.",
      "plaintextDescription": "It's a very unusual disclaimer that speaks well of the post.\n\nThe default journalistic practice at many outlets is to do an asymmetric search once the journalist or editor decides which way the wind is blowing, but of course nobody says this in the finished piece.\n\nBen is explicitly telling the reader that he did not spend another hundred hours looking for positive information about Nonlinear, so that we understand that absence of exculpatory evidence in the post should not be treated as strong evidence of absence."
    },
    "baseScore": 19,
    "voteCount": 14,
    "createdAt": null,
    "postedAt": "2023-09-09T18:28:30.233Z",
    "af": false
  },
  {
    "_id": "L97JQBefuju7skMzu",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "mcixezccSmvFe2xWd",
    "topLevelCommentId": "S7bLewBhhFeZudkum",
    "contents": {
      "markdown": "Which is why I said that the probabilities are similar, rather than claiming the left side exceeds the right side.",
      "plaintextDescription": "Which is why I said that the probabilities are similar, rather than claiming the left side exceeds the right side."
    },
    "baseScore": 9,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2023-09-07T20:22:38.923Z",
    "af": false
  },
  {
    "_id": "S7bLewBhhFeZudkum",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'm surprised (unless I've missed it) that nobody has explicitly pointed out the most obvious reason to take the responses of the form \"Kat/Emerson/Drew have been really good to me personally\" as very weak evidence at best.\n\nThe allegations imply that in the present situation, Kat/Emerson/Drew would immediately tell anyone in their orbit to come and post positive testimonials of them under promises of reward or threat of retaliation (precisely as the quoted Glassdoor review says).\n\nP(generic positive testimonials | accusation true) ≈ P(generic positive testimonials | accusation false).\n\nThe only thing that would be strong evidence against the claims here would be direct counterevidence to the claims in the post. Everything else so far is a smokescreen.",
      "plaintextDescription": "I'm surprised (unless I've missed it) that nobody has explicitly pointed out the most obvious reason to take the responses of the form \"Kat/Emerson/Drew have been really good to me personally\" as very weak evidence at best.\n\nThe allegations imply that in the present situation, Kat/Emerson/Drew would immediately tell anyone in their orbit to come and post positive testimonials of them under promises of reward or threat of retaliation (precisely as the quoted Glassdoor review says).\n\nP(generic positive testimonials | accusation true) ≈ P(generic positive testimonials | accusation false).\n\nThe only thing that would be strong evidence against the claims here would be direct counterevidence to the claims in the post. Everything else so far is a smokescreen."
    },
    "baseScore": 47,
    "voteCount": 28,
    "createdAt": null,
    "postedAt": "2023-09-07T20:00:04.267Z",
    "af": false
  },
  {
    "_id": "YW5b8whybq4kK253o",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "oXDkmk3zvxGtN4uxy",
    "topLevelCommentId": "XANzqTAkw85PTiWFt",
    "contents": {
      "markdown": "Plenty of \"weird and atypical\" things aren't red flags; this one, however, is a well-known predictor of abusive environments.",
      "plaintextDescription": "Plenty of \"weird and atypical\" things aren't red flags; this one, however, is a well-known predictor of abusive environments."
    },
    "baseScore": 33,
    "voteCount": 21,
    "createdAt": null,
    "postedAt": "2023-09-07T19:37:35.485Z",
    "af": false
  },
  {
    "_id": "4iZbQqKrTSzGjmv5y",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": "XANzqTAkw85PTiWFt",
    "topLevelCommentId": "XANzqTAkw85PTiWFt",
    "contents": {
      "markdown": "I believe that a commitment to transparently reward whistleblowers, in cases where you conclude they are running a risk of retaliation, is a *very* good policy when it comes to incentivizing true whistleblowing.",
      "plaintextDescription": "I believe that a commitment to transparently reward whistleblowers, in cases where you conclude they are running a risk of retaliation, is a very good policy when it comes to incentivizing true whistleblowing."
    },
    "baseScore": 23,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2023-09-07T17:44:06.388Z",
    "af": false
  },
  {
    "_id": "4n5dqHctmGD4wnWGu",
    "postId": "Lc8r4tZ2L5txxokZ8",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Ben, I want to say thank you for putting in a tremendous amount of work, and also for being willing to risk attempts at retaliation when that's a pretty clear threat. \n\nYou're in a reasonable position to take this on, having earned the social standing to make character smears unlikely to stick, and having the institutional support to fight a spurious libel claim. And you're also someone I trust to do a thorough and fair job.\n\nI wish there were someone whose opportunity cost were lower who could handle retaliation-threat reporting, but it's pretty likely that anyone with those attributes will have other important opportunities.",
      "plaintextDescription": "Ben, I want to say thank you for putting in a tremendous amount of work, and also for being willing to risk attempts at retaliation when that's a pretty clear threat. \n\nYou're in a reasonable position to take this on, having earned the social standing to make character smears unlikely to stick, and having the institutional support to fight a spurious libel claim. And you're also someone I trust to do a thorough and fair job.\n\nI wish there were someone whose opportunity cost were lower who could handle retaliation-threat reporting, but it's pretty likely that anyone with those attributes will have other important opportunities."
    },
    "baseScore": 78,
    "voteCount": 60,
    "createdAt": null,
    "postedAt": "2023-09-07T15:31:57.658Z",
    "af": false
  },
  {
    "_id": "kKwQ7YhKEhG6qDbah",
    "postId": "ZoxkWDgDueo5yoakv",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Re: QB sneaks, the obvious tradeoff that's not factored into the naive model is the risk of an injury to the QB. Running the QB can be worth it if the expected points added are high enough (goal line or pivotal 4th and 1, or if your QB can often pick up 10+ yards on a draw), but I doubt you want to roll those dice on a midfield 2nd and 4 sneak in the first quarter of a regular-season game.\n\n(God, American football is a beautiful game. I miss the days when I could enjoy it because I didn't know about the CTE.)",
      "plaintextDescription": "Re: QB sneaks, the obvious tradeoff that's not factored into the naive model is the risk of an injury to the QB. Running the QB can be worth it if the expected points added are high enough (goal line or pivotal 4th and 1, or if your QB can often pick up 10+ yards on a draw), but I doubt you want to roll those dice on a midfield 2nd and 4 sneak in the first quarter of a regular-season game.\n\n(God, American football is a beautiful game. I miss the days when I could enjoy it because I didn't know about the CTE.)"
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-09-07T00:43:45.876Z",
    "af": false
  },
  {
    "_id": "LkAjnTBauxNtxgYfX",
    "postId": "fXnpnrazqwpJmbadu",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I worked in the AI/ML org at Apple for a few recent years. They are not a live player to even the extent that Google Brain was a live player before it was cannibalized.\n\nWhen Apple says \"AI\", they really mean \"a bunch of specialized ML algorithms from warring fiefdoms, huddling together in a trenchcoat\", and I don't see Tim Cook's proclamation as anything but cheap talk.",
      "plaintextDescription": "I worked in the AI/ML org at Apple for a few recent years. They are not a live player to even the extent that Google Brain was a live player before it was cannibalized.\n\nWhen Apple says \"AI\", they really mean \"a bunch of specialized ML algorithms from warring fiefdoms, huddling together in a trenchcoat\", and I don't see Tim Cook's proclamation as anything but cheap talk."
    },
    "baseScore": 8,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2023-08-18T04:26:36.525Z",
    "af": false
  },
  {
    "_id": "ajafG7peep4Cre8DX",
    "postId": "ZvqTqSdmTQEKAmmmX",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Re: mileage, if the car is a Tesla, [it's not an accident](https://www.reuters.com/investigates/special-report/tesla-batteries-range/): they set their numbers to the maximally optimistic ones at every stage (without regard for e.g. temperature) and have a team dedicated to diverting people who call in about the reliable inaccuracy.",
      "plaintextDescription": "Re: mileage, if the car is a Tesla, it's not an accident: they set their numbers to the maximally optimistic ones at every stage (without regard for e.g. temperature) and have a team dedicated to diverting people who call in about the reliable inaccuracy."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2023-08-08T16:12:18.325Z",
    "af": false
  },
  {
    "_id": "HygD3rXhHahSFDAnT",
    "postId": "NSZhadmoYdjRKNq6X",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I'm surprised I didn't see here my biggest objection: \n\nMIRI talks about \"pivotal acts\", building an AI that's superhuman in some engineering disciplines (but not generally) and having it do a human-specified thing to halt the development of AGIs (e.g. seek and safely melt down sufficiently large compute clusters) in order to buy time for alignment work. Their main reason for this approach is that it seems less doomed to have an AI specialize in consequentialist reasoning about limited domains of physical engineering than to have it think directly about how its developers' minds work.\n\nIf you are building an alignment researcher, you are building **a powerful AI that is directly thinking about misalignment**—exploring concepts like humans' mental blind spots, deception, reward hacking, hiding thoughts from interpretability, sharp left turns, etc. It does not seem wise to build a consequentialist AI and *explicitly* train it to think about these things, even when the goal is for it to treat them as wrong. (Consider the Waluigi Effect: you may have at least latently constructed the *maximally* malicious agent!)\n\nI agree, of course, that the biggest news here is the costly commitment—my prior model of them was that their alignment team wasn't actually respected or empowered, and the current investment is very much not what I would expect them to do if that were the case going forward.",
      "plaintextDescription": "I'm surprised I didn't see here my biggest objection: \n\nMIRI talks about \"pivotal acts\", building an AI that's superhuman in some engineering disciplines (but not generally) and having it do a human-specified thing to halt the development of AGIs (e.g. seek and safely melt down sufficiently large compute clusters) in order to buy time for alignment work. Their main reason for this approach is that it seems less doomed to have an AI specialize in consequentialist reasoning about limited domains of physical engineering than to have it think directly about how its developers' minds work.\n\nIf you are building an alignment researcher, you are building a powerful AI that is directly thinking about misalignment—exploring concepts like humans' mental blind spots, deception, reward hacking, hiding thoughts from interpretability, sharp left turns, etc. It does not seem wise to build a consequentialist AI and explicitly train it to think about these things, even when the goal is for it to treat them as wrong. (Consider the Waluigi Effect: you may have at least latently constructed the maximally malicious agent!)\n\n \n\nI agree, of course, that the biggest news here is the costly commitment—my prior model of them was that their alignment team wasn't actually respected or empowered, and the current investment is very much not what I would expect them to do if that were the case going forward."
    },
    "baseScore": 36,
    "voteCount": 19,
    "createdAt": null,
    "postedAt": "2023-07-11T23:42:14.803Z",
    "af": false
  },
  {
    "_id": "8TB5HfuGtumYxn4di",
    "postId": "oASJGHGkEvjTrHPBy",
    "parentCommentId": "thP4BqRoyjHJuFgs6",
    "topLevelCommentId": "ob4zyduawdaGe7iAk",
    "contents": {
      "markdown": "I agree that, given the dynamics, it's rare to get a great journalist on a technical subject (we're lucky to have Zeynep Tufekci on public health), but my opinion is that Metz has a negative Value Over Replacement Tech Journalist, that coverage of AI in the NYT would be significantly more accurate if he quit and was replaced by whomever the Times would poach.",
      "plaintextDescription": "I agree that, given the dynamics, it's rare to get a great journalist on a technical subject (we're lucky to have Zeynep Tufekci on public health), but my opinion is that Metz has a negative Value Over Replacement Tech Journalist, that coverage of AI in the NYT would be significantly more accurate if he quit and was replaced by whomever the Times would poach."
    },
    "baseScore": 4,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2023-06-10T22:04:39.497Z",
    "af": false
  },
  {
    "_id": "3JZwe6gRWKyHWsTYD",
    "postId": "oASJGHGkEvjTrHPBy",
    "parentCommentId": "jKkxwwdevE3YdXPoM",
    "topLevelCommentId": "ob4zyduawdaGe7iAk",
    "contents": {
      "markdown": "Cade Metz already has multiple strikes against him when it comes to journalistic carelessness around the rationalist community and around AI risk. In addition to outing Scott, he [blithely mischaracterized the situation between Geoff Hinton and Google](https://twitter.com/geoffreyhinton/status/1652993570721210372). \n\nIt's harmful that the NYT still has him on this beat (though I'm sure his editors don't know/care that he's treating the topic as an anthropological curiosity rather than something worth taking seriously).",
      "plaintextDescription": "Cade Metz already has multiple strikes against him when it comes to journalistic carelessness around the rationalist community and around AI risk. In addition to outing Scott, he blithely mischaracterized the situation between Geoff Hinton and Google. \n\nIt's harmful that the NYT still has him on this beat (though I'm sure his editors don't know/care that he's treating the topic as an anthropological curiosity rather than something worth taking seriously)."
    },
    "baseScore": 19,
    "voteCount": 10,
    "createdAt": null,
    "postedAt": "2023-06-10T16:59:34.403Z",
    "af": false
  },
  {
    "_id": "pvq3uZDNdjAQyrqRr",
    "postId": "Wiz4eKi5fsomRsMbx",
    "parentCommentId": "Zo8ELctChTuCpqrmK",
    "topLevelCommentId": "Zo8ELctChTuCpqrmK",
    "contents": {
      "markdown": "Elizabeth has invested a lot of work already, and has explicitly requested that people put in some amount of work when trying to argue against her cruxes (including actually reading her cruxes, and supporting one's points with studies whose methodology one has critically checked).",
      "plaintextDescription": "Elizabeth has invested a lot of work already, and has explicitly requested that people put in some amount of work when trying to argue against her cruxes (including actually reading her cruxes, and supporting one's points with studies whose methodology one has critically checked)."
    },
    "baseScore": 1,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2023-06-02T23:12:44.265Z",
    "af": false
  },
  {
    "_id": "53iB3kkQAL8QJTguC",
    "postId": "Wiz4eKi5fsomRsMbx",
    "parentCommentId": "JTiMDEK7FkPLbd783",
    "topLevelCommentId": "JTiMDEK7FkPLbd783",
    "contents": {
      "markdown": "The citation on that sentence is the same as the first paragraph in this post about the animal-ethics.org website; Elizabeth is aware of that study and did not find it convincing.",
      "plaintextDescription": "The citation on that sentence is the same as the first paragraph in this post about the animal-ethics.org website; Elizabeth is aware of that study and did not find it convincing."
    },
    "baseScore": 1,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2023-06-02T19:33:01.692Z",
    "af": false
  },
  {
    "_id": "AJCH7P5Gbq2hML6K3",
    "postId": "Wiz4eKi5fsomRsMbx",
    "parentCommentId": "oh5Qh2qSqGHxaBFsd",
    "topLevelCommentId": "oh5Qh2qSqGHxaBFsd",
    "contents": {
      "markdown": "In this and your comments below, you recapitulate points Elizabeth made pretty exactly- so it looks like you didn't need to read it after all!",
      "plaintextDescription": "In this and your comments below, you recapitulate points Elizabeth made pretty exactly- so it looks like you didn't need to read it after all!"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2023-06-02T19:25:32.578Z",
    "af": false
  },
  {
    "_id": "hb4kTxBe9ALrBHwDR",
    "postId": "N7DxcLCjfBpEv3QwB",
    "parentCommentId": "DDt8rFDwftKGWnNkH",
    "topLevelCommentId": "DDt8rFDwftKGWnNkH",
    "contents": {
      "markdown": "My answer is \"work on applications of existing AI, not the frontier\". Advancing the frontier is the dangerous part, not using the state-of-the-art to make products.\n\nBut also, don't do frontend or infra for a company that's advancing capabilities.",
      "plaintextDescription": "My answer is \"work on applications of existing AI, not the frontier\". Advancing the frontier is the dangerous part, not using the state-of-the-art to make products.\n\nBut also, don't do frontend or infra for a company that's advancing capabilities."
    },
    "baseScore": 29,
    "voteCount": 17,
    "createdAt": null,
    "postedAt": "2023-05-28T00:42:44.509Z",
    "af": false
  },
  {
    "_id": "ekKj4E2sBBdA33hir",
    "postId": "qtEgaxSFxYanT5QbJ",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> I also had a bunch of thoughts of ‘oh, well, that’s easy, obviously you would just \\[OH MY LORD IS THIS CENSORED\\]’\n\nI applaud your Virtue of Silence, and I'm also uncomfortable with the simplicity of some of the ones I'm sitting on.",
      "plaintextDescription": "> I also had a bunch of thoughts of ‘oh, well, that’s easy, obviously you would just [OH MY LORD IS THIS CENSORED]’\n\nI applaud your Virtue of Silence, and I'm also uncomfortable with the simplicity of some of the ones I'm sitting on."
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2023-04-28T16:07:02.426Z",
    "af": false
  },
  {
    "_id": "j8sHhf7NLiAokkbxB",
    "postId": "dxRw2xPQWFLjyr7WH",
    "parentCommentId": "y7peuKgrsXCPjsfLZ",
    "topLevelCommentId": "QiCKjPv2WqbaDaehg",
    "contents": {
      "markdown": "Thanks for the info about sockpuppeting, will edit my first comment accordingly.\n\nRe: Glassdoor, the most devastating reviews were indeed after 2017, but it's still the case that *nobody* rated the CEO above average among the ~30 people who worked in the Spartz era.",
      "plaintextDescription": "Thanks for the info about sockpuppeting, will edit my first comment accordingly.\n\nRe: Glassdoor, the most devastating reviews were indeed after 2017, but it's still the case that nobody rated the CEO above average among the ~30 people who worked in the Spartz era."
    },
    "baseScore": 3,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2023-04-13T23:49:36.351Z",
    "af": false
  },
  {
    "_id": "QiCKjPv2WqbaDaehg",
    "postId": "dxRw2xPQWFLjyr7WH",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "This is a good idea; unfortunately, [based on discussions on the EA Forum](https://forum.effectivealtruism.org/posts/L4S2NCysoJxgCBuB6/announcing-nonlinear-emergency-funding?commentId=5P75dFuKLo894MQFf), Nonlinear is not an organization I would trust to handle it. (Note, as external evidence, that the Glassdoor reviews of Emerson's previous company [frequently mention a toxic upper management culture](https://forum.effectivealtruism.org/posts/L4S2NCysoJxgCBuB6/announcing-nonlinear-emergency-funding?commentId=Sj7vnBDvkQm8JoEc8) of exactly the sort that the commenter alleges at Nonlinear, and have a 0% rating of him as CEO.)\n\n\\[EDITED TO ADD: The second comment quotes reviews written after the Spartz era (although I'm sure many of them were present during it), which is misleading; moreover, the second commenter was banned for having a sockpuppet. My criticisms are thereby reduced but not eliminated.\\]",
      "plaintextDescription": "This is a good idea; unfortunately, based on discussions on the EA Forum, Nonlinear is not an organization I would trust to handle it. (Note, as external evidence, that the Glassdoor reviews of Emerson's previous company frequently mention a toxic upper management culture of exactly the sort that the commenter alleges at Nonlinear, and have a 0% rating of him as CEO.)\n\n[EDITED TO ADD: The second comment quotes reviews written after the Spartz era (although I'm sure many of them were present during it), which is misleading; moreover, the second commenter was banned for having a sockpuppet. My criticisms are thereby reduced but not eliminated.]"
    },
    "baseScore": 19,
    "voteCount": 22,
    "createdAt": null,
    "postedAt": "2023-04-13T03:38:00.537Z",
    "af": false
  },
  {
    "_id": "HDHh7zfH5cBtZf3mf",
    "postId": "JeK4ztN4nNBgjXpNS",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I bet I know your survey answer on one of Aella's questions.",
      "plaintextDescription": "I bet I know your survey answer on one of Aella's questions."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2023-04-04T00:05:58.388Z",
    "af": false
  },
  {
    "_id": "xuTQvxeLKCknJD9nW",
    "postId": "NRrbJJWnaSorrqvtZ",
    "parentCommentId": "j6i76tWkHwymf34tv",
    "topLevelCommentId": "j6i76tWkHwymf34tv",
    "contents": {
      "markdown": "Looking for \"elbows\" in a noisy time series with relatively few points is a pretty easy way to get spurious results. If the 1960-62 obesity number was overestimated by 2 points and/or the 1976-1980 number was underestimated by 2 points, it wouldn't look like 1976-1980 was a special transition at all.\n\n(And clearly errors of that magnitude happen, unless you think there's a deep reason why obesity rates were nonmonotonic from 2005-06 to 2011-12.)",
      "plaintextDescription": "Looking for \"elbows\" in a noisy time series with relatively few points is a pretty easy way to get spurious results. If the 1960-62 obesity number was overestimated by 2 points and/or the 1976-1980 number was underestimated by 2 points, it wouldn't look like 1976-1980 was a special transition at all.\n\n(And clearly errors of that magnitude happen, unless you think there's a deep reason why obesity rates were nonmonotonic from 2005-06 to 2011-12.)"
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2023-04-03T23:53:42.562Z",
    "af": false
  },
  {
    "_id": "7ENrtAbTo47fMinEn",
    "postId": "Aq5X9tapacnk2QGY4",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "\\[EDIT: fallenpegasus points out that there's a low bar to entry to this corner of TIME's website. I have to say I should have been confused that even now they let Eliezer write in his own idiom.\\]\n\nThe Eliezer of 2010 had no shot of being directly published (instead of featured in an interview that at best paints him as a curiosity) in TIME of 2010. I'm not sure about 2020.\n\nI wonder at what point the threshold of \"admitting it's at least okay to discuss Eliezer's viewpoint at face value\" was crossed for the editors of TIME. I fear the answer is \"last month\".",
      "plaintextDescription": "[EDIT: fallenpegasus points out that there's a low bar to entry to this corner of TIME's website. I have to say I should have been confused that even now they let Eliezer write in his own idiom.]\n\nThe Eliezer of 2010 had no shot of being directly published (instead of featured in an interview that at best paints him as a curiosity) in TIME of 2010. I'm not sure about 2020.\n\nI wonder at what point the threshold of \"admitting it's at least okay to discuss Eliezer's viewpoint at face value\" was crossed for the editors of TIME. I fear the answer is \"last month\"."
    },
    "baseScore": 13,
    "voteCount": 14,
    "createdAt": null,
    "postedAt": "2023-03-29T23:40:36.228Z",
    "af": false
  },
  {
    "_id": "EhRQmWC5EwX3CiAgA",
    "postId": "ygMkB9r4DuQQxrrpj",
    "parentCommentId": "wf2XLsxKwXJ64M8XE",
    "topLevelCommentId": "wf2XLsxKwXJ64M8XE",
    "contents": {
      "markdown": "I can confirm that Nate is not backdating memories—he and Eliezer were pretty clear *within MIRI* at the time that they thought Sam and Elon were making a tremendous mistake and that they were trying to figure out how to use MIRI's small influence within a worsened strategic landscape.",
      "plaintextDescription": "I can confirm that Nate is not backdating memories—he and Eliezer were pretty clear within MIRI at the time that they thought Sam and Elon were making a tremendous mistake and that they were trying to figure out how to use MIRI's small influence within a worsened strategic landscape."
    },
    "baseScore": 15,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2023-03-23T19:13:15.188Z",
    "af": false
  }
]