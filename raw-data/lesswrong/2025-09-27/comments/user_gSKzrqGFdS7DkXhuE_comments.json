[
  {
    "_id": "ajHRmFZXoDjGGHBKK",
    "postId": "anFrGMskALuH7aZDw",
    "parentCommentId": "B4MmS8L9qgdo8KrG5",
    "topLevelCommentId": "qjF5zWS9JLGBZ9p39",
    "contents": {
      "markdown": "The comment you were criticizing stated\n\n> It seems that you have to really thread the needle to get from \"5% p(doom)\" to \"we must pause, now!\". You have to reason such that you are not self-interested but are also a great chauvinist for the human species.\n\nThis comment seems more to be resisting political action (pause AI) than pursuing it. If anything, your concern about political actors becoming monsters would more apply to the sort of people who want to create a world government to ban X globally, than people bringing up objections.",
      "plaintextDescription": "The comment you were criticizing stated\n\n> It seems that you have to really thread the needle to get from \"5% p(doom)\" to \"we must pause, now!\". You have to reason such that you are not self-interested but are also a great chauvinist for the human species.\n\nThis comment seems more to be resisting political action (pause AI) than pursuing it. If anything, your concern about political actors becoming monsters would more apply to the sort of people who want to create a world government to ban X globally, than people bringing up objections."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-24T18:37:10.036Z",
    "af": false
  },
  {
    "_id": "EyxmnraBZtGfi2HeC",
    "postId": "anFrGMskALuH7aZDw",
    "parentCommentId": "mRxzf3tJZqYQnew3m",
    "topLevelCommentId": "qjF5zWS9JLGBZ9p39",
    "contents": {
      "markdown": "I was assuming conditional on 1 in 20 chance of AI kills everyone\n\nBasically I don't think the anti \"coercing others for ideological reasons\" argument applies to the sort of person who thinks \"well, I don't think a 1 in 20 chance of AI killing everyone is so bad that I'm going to support a political movement trying to ban AI research; for abstract reasons I think AI is still net positive under that assumption\"\n\nThe action / inaction distinction matters here",
      "plaintextDescription": "I was assuming conditional on 1 in 20 chance of AI kills everyone\n\nBasically I don't think the anti \"coercing others for ideological reasons\" argument applies to the sort of person who thinks \"well, I don't think a 1 in 20 chance of AI killing everyone is so bad that I'm going to support a political movement trying to ban AI research; for abstract reasons I think AI is still net positive under that assumption\"\n\nThe action / inaction distinction matters here"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-24T00:57:24.107Z",
    "af": false
  },
  {
    "_id": "R5uoaAe4oqpRHGnfr",
    "postId": "4fqwBmmqi2ZGn9o7j",
    "parentCommentId": "TqqN34ec7vgLPuhHf",
    "topLevelCommentId": "TqqN34ec7vgLPuhHf",
    "contents": {
      "markdown": "Insofar as I buy this argument, I would apply it to my own CEV, hence concluding that on reflection I would most prefer a universe without biological humans. Then I notice I'm probably not special and probably a lot of other people's CEV generates this conclusion too. This seems kind of heretical but I'm not convinced it's wrong.",
      "plaintextDescription": "Insofar as I buy this argument, I would apply it to my own CEV, hence concluding that on reflection I would most prefer a universe without biological humans. Then I notice I'm probably not special and probably a lot of other people's CEV generates this conclusion too. This seems kind of heretical but I'm not convinced it's wrong."
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-09-23T19:52:19.417Z",
    "af": false
  },
  {
    "_id": "dBM6kKvntJcu7iHzu",
    "postId": "anFrGMskALuH7aZDw",
    "parentCommentId": "c8yCwJJKwAwZJkyFj",
    "topLevelCommentId": "qjF5zWS9JLGBZ9p39",
    "contents": {
      "markdown": "This seems too pattern matchy to be valid reasoning? Let's try an exercise where I rewrite the passage:\n\n> “They tried to crush us over and over again, but we wouldn’t be crushed. We drove off the AI researchers. We winkled out those who preached that superintelligence would be motivated to be moral, out of the churches and more importantly out of people’s minds. We got rid of the hardware sellers, thieving bastards, getting their dirty fingers in every deal, making every straight thing crooked. We dragged the gamers into the twenty-first century, and that was hard, that was a cruel business, and there were some painful years there, but it had to be done, we had to get the much off our boots. We realised that there were saboteurs and enemies among us, and we caught them, but it drove us mad for a while, and for a while we were seeing enemies and saboteurs everywhere, and hurting people who were brothers, sisters, good friends, honest comrades...\n\n> [...] Working for the future made the past tolerable, and therefore the present. [...] So much blood, and only one justification for it. Only one reason it could have been all right to have done such things, and aided their doing: if it had been all prologue, all only the last spasms of the death of the old, unsafe, anti-human world, and the birth of a new safe, humanistic one.”\n\nAha, I have compared AI regulationists to the Communists, so they lose! Keep in mind that it is not the \"accelerationist\" position that requires centralized control and the stopping of business-as-usual, it is the \"globally stop AI\" one.\n\n(But of course the details matter. Sometimes forcing others to pay costs works out net positively for both them and for you...)",
      "plaintextDescription": "This seems too pattern matchy to be valid reasoning? Let's try an exercise where I rewrite the passage:\n\n> “They tried to crush us over and over again, but we wouldn’t be crushed. We drove off the AI researchers. We winkled out those who preached that superintelligence would be motivated to be moral, out of the churches and more importantly out of people’s minds. We got rid of the hardware sellers, thieving bastards, getting their dirty fingers in every deal, making every straight thing crooked. We dragged the gamers into the twenty-first century, and that was hard, that was a cruel business, and there were some painful years there, but it had to be done, we had to get the much off our boots. We realised that there were saboteurs and enemies among us, and we caught them, but it drove us mad for a while, and for a while we were seeing enemies and saboteurs everywhere, and hurting people who were brothers, sisters, good friends, honest comrades...\n\n> [...] Working for the future made the past tolerable, and therefore the present. [...] So much blood, and only one justification for it. Only one reason it could have been all right to have done such things, and aided their doing: if it had been all prologue, all only the last spasms of the death of the old, unsafe, anti-human world, and the birth of a new safe, humanistic one.”\n\nAha, I have compared AI regulationists to the Communists, so they lose! Keep in mind that it is not the \"accelerationist\" position that requires centralized control and the stopping of business-as-usual, it is the \"globally stop AI\" one.\n\n(But of course the details matter. Sometimes forcing others to pay costs works out net positively for both them and for you...)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-09-23T16:27:49.648Z",
    "af": false
  },
  {
    "_id": "w8QhezjWSpkkzMNrn",
    "postId": "43rovsvGSiQneqcKc",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I've written criticisms of orthogonality: [The Obliqueness Thesis](https://www.lesswrong.com/posts/vdATCfuxdtdqM2wh9/the-obliqueness-thesis), [Measuring intelligence and reverse-engineering goals](https://www.lesswrong.com/posts/eLDgDAWphHaAJYMxN/measuring-intelligence-and-reverse-engineering-goals).\r\n\r\nWhile I do think human moral reasoning suggests non-orthogonality, it's a somewhat conceptually tricky case. So recently I've been thinking about more straightforward ways of showing non-orthogonality relative to an architecture.\r\n\r\nFor example, consider RL agents playing Minecraft. If you want to get agents that beat the game, you could encode this preference function directly as a reward function, reward it when it beats the game. However this fails in practice.\r\n\r\nThe alternative is reward shaping. Reward it for pursuing instrumental values like exploring or getting new resources. This agent is much more likely to win, despite it being mis-aligned.\r\n\r\nWhat this shows is that reinforcement learning is a non-orthogonal architecture. Some goals (reward functions) lead to more satisfaction of convergent instrumental goals than others.\r\n\r\nSlightly tricker case is humans. Direct encoding of inclusive fitness as human neural values seems like it would produce high fitness, but we don't see humans have this, therefore the space evolution is searching over is probably non-orthogonal.\r\n\r\nMaybe it's like the RL case where organisms are more likely to have fitness if they have neural encodings of instrumental goals, which are easier to optimize short-term. Fixed-action patterns suggest something like this, there's a \"terminal value\" of engaging in fixed action patterns (which happen to be ones that promote fitness; evolution searched over many possible fixed action patterns).\r\n\r\nSo instead of assuming \"organisms get more fitness by having values aligned with inclusive fitness\" we could re-frame as, \"inclusive fitness is a meta-value over organisms (including their values), some values lead to higher inclusive fitness than others, empirically\".\r\n\r\nThis approach could be used to study human morality. Maybe some tendencies to engage in moral reasoning lead to more fitness, even if moral reasoning isn't straightforwardly aligned with fitness. Perhaps because, morality is a convenient proxy that works in bounded rationality.\r\n\r\nA thesis would be something like, orthogonality holds for almost no architectures. Relative to an architecture like RL or neural encodings of values, there are almost always \"especially smart values\" that lead to more convergent instrumental goal achievement. Evolution will tend to find these empirically.\r\n\r\nThis doesn't contradict that there is some architecture that is orthogonal, which I take to be the steelman of the orthogonality thesis. However it suggests that even if this steelman is true, it has limited applicability to empirically realized agent architectures, and in particular doesn't apply to human preference/morality.",
      "plaintextDescription": "I've written criticisms of orthogonality: The Obliqueness Thesis, Measuring intelligence and reverse-engineering goals.\n\nWhile I do think human moral reasoning suggests non-orthogonality, it's a somewhat conceptually tricky case. So recently I've been thinking about more straightforward ways of showing non-orthogonality relative to an architecture.\n\nFor example, consider RL agents playing Minecraft. If you want to get agents that beat the game, you could encode this preference function directly as a reward function, reward it when it beats the game. However this fails in practice.\n\nThe alternative is reward shaping. Reward it for pursuing instrumental values like exploring or getting new resources. This agent is much more likely to win, despite it being mis-aligned.\n\nWhat this shows is that reinforcement learning is a non-orthogonal architecture. Some goals (reward functions) lead to more satisfaction of convergent instrumental goals than others.\n\nSlightly tricker case is humans. Direct encoding of inclusive fitness as human neural values seems like it would produce high fitness, but we don't see humans have this, therefore the space evolution is searching over is probably non-orthogonal.\n\nMaybe it's like the RL case where organisms are more likely to have fitness if they have neural encodings of instrumental goals, which are easier to optimize short-term. Fixed-action patterns suggest something like this, there's a \"terminal value\" of engaging in fixed action patterns (which happen to be ones that promote fitness; evolution searched over many possible fixed action patterns).\n\nSo instead of assuming \"organisms get more fitness by having values aligned with inclusive fitness\" we could re-frame as, \"inclusive fitness is a meta-value over organisms (including their values), some values lead to higher inclusive fitness than others, empirically\".\n\nThis approach could be used to study human morality. Maybe some tendencies to engage in moral reasoning lead to more fitness,"
    },
    "baseScore": 8,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-08-22T01:26:41.239Z",
    "af": false
  },
  {
    "_id": "9veJLtzZowbjgSG8j",
    "postId": "uGakMbD7QKt88oMSa",
    "parentCommentId": "TZEFeXEFvFGNFHR3J",
    "topLevelCommentId": "TZEFeXEFvFGNFHR3J",
    "contents": {
      "markdown": ">  it doesn't seem highly problematic that we can access mathematical facts that \"live partially outside the universe\" via \"reasoning\" or \"logical correlation\", where the computations in our minds are entangled in some way with computations or math that we're not physically connected to.\n\nWhile this is one way to think about, it seems first of all that it is limited to \"small\" mathematical facts that are computable in physics (not stuff like the continuum hypothesis). With respect to the entanglement, while it's possible to have a Bayes net where the mathematical fact \"causes\" both computers to output the answers, there's an alternative approach where the computers are two material devices that output the same answer because of physical symmetry. Two processes having symmetrical outputs doesn't in general indicate they're \"caused by the same thing\".\n\n> arguments in favor of some types of mathematical realism/platonism (e.g., universe and multiverse views of set theory)\n\nNot familiar with these arguments. I think a formalist approach would be, the consistency of ZFC already implies a bunch of \"small\" mathematical facts (e.g. ZFC can't prove any false $\\Pi_1$ arithmetic statements). I think it's pretty hard to find a useful a formal system that is strictly finitist, however my intuition is that set theory goes too far. (This is part of why I have been recently thinking about \"reverse mathematics\", relatively weak second-order arithmetic theories like $WKL_0$)\n\n> Another reason I'm not ready to be super-convinced in this direction is I think philosophy is often very hard and slow, therefore as you say \"It is somewhat questionable to infer from lack of success to define, say, optimal decision theories, that no such decision theory exists.\"\n\nYeah that makes sense. I think maybe what I've become more reluctant to endorse over time, is a jump from \"an intuition that something here works, plus alternative solutions failing\" to \"here, this thing I came up with or something a lot like it is going to work\". Like going from failure of CDT to success of EDT, or failure of CDT+EDT to TDT. There is not really any assurance that the new thing will work either.\n\n> we're not sure whether we'll eventually keep them when we're philosophically mature, and we don't know how to translate these values to a new ontology that lack these entities\n\nI see this is a practical consideration in many value systems, although perhaps either (a) the pragmatic considerations go differently for different people, (b) different systems could be used for different pragmatic purposes. It at least presents a case for explaining the _psychological_ phenomena of different ontologies/values even ones that might fail in physicalism.\n",
      "plaintextDescription": "> it doesn't seem highly problematic that we can access mathematical facts that \"live partially outside the universe\" via \"reasoning\" or \"logical correlation\", where the computations in our minds are entangled in some way with computations or math that we're not physically connected to.\n\nWhile this is one way to think about, it seems first of all that it is limited to \"small\" mathematical facts that are computable in physics (not stuff like the continuum hypothesis). With respect to the entanglement, while it's possible to have a Bayes net where the mathematical fact \"causes\" both computers to output the answers, there's an alternative approach where the computers are two material devices that output the same answer because of physical symmetry. Two processes having symmetrical outputs doesn't in general indicate they're \"caused by the same thing\".\n\n> arguments in favor of some types of mathematical realism/platonism (e.g., universe and multiverse views of set theory)\n\nNot familiar with these arguments. I think a formalist approach would be, the consistency of ZFC already implies a bunch of \"small\" mathematical facts (e.g. ZFC can't prove any false Π1 arithmetic statements). I think it's pretty hard to find a useful a formal system that is strictly finitist, however my intuition is that set theory goes too far. (This is part of why I have been recently thinking about \"reverse mathematics\", relatively weak second-order arithmetic theories like WKL0)\n\n> Another reason I'm not ready to be super-convinced in this direction is I think philosophy is often very hard and slow, therefore as you say \"It is somewhat questionable to infer from lack of success to define, say, optimal decision theories, that no such decision theory exists.\"\n\nYeah that makes sense. I think maybe what I've become more reluctant to endorse over time, is a jump from \"an intuition that something here works, plus alternative solutions failing\" to \"here, this thing I came up with or something a lot like"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-19T04:05:29.860Z",
    "af": false
  },
  {
    "_id": "36fAD23JCYxoqho7R",
    "postId": "uGakMbD7QKt88oMSa",
    "parentCommentId": "r7WLFoHcuqWngwADC",
    "topLevelCommentId": "r7WLFoHcuqWngwADC",
    "contents": {
      "markdown": "> The precalculated \"stochastic\"variables thing, and the on-the-fly calls to the universe's rand() aren't the same thing, because they have different ontological implications.\n\nYeah they can be distinguished ontologically. Although there are going to be multiple Bayes nets expressing the same joint distribution. So it's not like there's going to be a canonical ordering.\n\n> I would guess that the standard rationalist answer is \"they are indistinguishable empirically\". But rationalism lacks a proof that unempirical questions are unaswerable or meaningless (unlike logical positivism..but LP is explicitly rejected).\n\nI get that active dis-belief in further facts (such as counterfactuals) can be dogmatic. Rather, it's more of a case of, we can get an adequate empirical account without them, and adding them has problems (like causal counterfactuals implying violations of physical law).\n\nPart of where I'm coming with this is a Chalmers like framework. Suppose there are 2 possible universes, they have the same joint distribution, but different causal ordering. Like maybe in one the stochasticity is on the fly, in the other it's pre-computed. They imply the same joint distribution and the same set of \"straightforward\" physical facts (particle trajectories and so on). Yet there is a distinction, a further fact.\n\nIn which case... The agents in these universes can't have epistemic access to these further facts, it's similar to with the zombie argument. A simple approach is \"no further facts\", although assuming this is literally the case might be dogmatic. It's more like, don't believe in further facts prior to a good/convincing account of them, where the ontological complexity is actually worth it.\n\n> Note that compaibilism and naturalistic libertarian are both viable given our present state of knowledge...so there is no necessity to adopt anti realism.\n\nWell it's more like, most specific theories of these have problems. Like, the counterfactuals being really weird, corresponding to bad decision theories, etc. And it seems simpler to say, the counterfactuals don't exist? Even if assigning high probability to it is dogmatic.\n\n> So much for MWI then ..according to it, every world is counterfactual to every other.\n\nIf instead of QM our best physics said something like \"there are true random coin flips\" then it would be a bit of a stretch to posit a MWI-like theory there, that there exist other universes where the coin flips go differently. The case for MWI is somewhat more complex, it has to do with the Copenhagen interpretation being a lot more complicated than \"here, have some stochastic coin flips\".\n\n> How do you know counterfactuals require violations of physics itself? The possibility of something happening that wasn't what happened, only requires (genuine) indeterminism, as above.\n\nWell we can disjunct on high or low universal K complexity. Assuming low universal K complexity, counterfactuals really do have problems, there are a lot of implications. Assuming high universal K complexity, I guess they're more well defined. Though you can't counterfact on just anything, you have to counterfact on a valid quantum event. So like, how many counterfactuals there are depends on the density of relevant quantum events to, say, a computer.\n\nI guess you could make the case from QM that the *classical* trajectory has high K complexity, therefore counterfactual alternatives to the classical trajectory don't require physical law violations.\n\nIf not for QM though, our knowledge would be compatible with determinism / low K complexity of the classical trajectory, and it seems like a philosophy should be able to deal with that case (even if it empirically seems not to be the case)\n\n> You can hypothetically plan out a moon landing before you perform it for the first time.\n\nRight so, counterfactual reasoning is practically useful, this is more about skepticism of the implied metaphysics. There might be translations like, observing that a deterministic system can be factored (multiple ways) as interacting systems with inputs/outputs, each factoring implying additional facts about the deterministic system. Without having to say that any of these factorings is correct in the sense of correctness about further facts.",
      "plaintextDescription": "> The precalculated \"stochastic\"variables thing, and the on-the-fly calls to the universe's rand() aren't the same thing, because they have different ontological implications.\n\nYeah they can be distinguished ontologically. Although there are going to be multiple Bayes nets expressing the same joint distribution. So it's not like there's going to be a canonical ordering.\n\n> I would guess that the standard rationalist answer is \"they are indistinguishable empirically\". But rationalism lacks a proof that unempirical questions are unaswerable or meaningless (unlike logical positivism..but LP is explicitly rejected).\n\nI get that active dis-belief in further facts (such as counterfactuals) can be dogmatic. Rather, it's more of a case of, we can get an adequate empirical account without them, and adding them has problems (like causal counterfactuals implying violations of physical law).\n\nPart of where I'm coming with this is a Chalmers like framework. Suppose there are 2 possible universes, they have the same joint distribution, but different causal ordering. Like maybe in one the stochasticity is on the fly, in the other it's pre-computed. They imply the same joint distribution and the same set of \"straightforward\" physical facts (particle trajectories and so on). Yet there is a distinction, a further fact.\n\nIn which case... The agents in these universes can't have epistemic access to these further facts, it's similar to with the zombie argument. A simple approach is \"no further facts\", although assuming this is literally the case might be dogmatic. It's more like, don't believe in further facts prior to a good/convincing account of them, where the ontological complexity is actually worth it.\n\n> Note that compaibilism and naturalistic libertarian are both viable given our present state of knowledge...so there is no necessity to adopt anti realism.\n\nWell it's more like, most specific theories of these have problems. Like, the counterfactuals being really weird, correspondi"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-17T20:31:10.149Z",
    "af": false
  },
  {
    "_id": "EJvmk6vf533sujq3j",
    "postId": "uGakMbD7QKt88oMSa",
    "parentCommentId": "AXxiggAotSPE6MYBT",
    "topLevelCommentId": "fiBLCDawo5fSMBgoA",
    "contents": {
      "markdown": "Ah. I think first of all, it is possible to do ontology in a materialist-directed or idealist-directed way, and the original post is materalist-directed.\r\n\r\nI get that the joint distribution over physical facts determines a joint distribution over observations, and we couldn't observe further facts about the joint distribution beyond those implied by the distribution over observations.\r\n\r\nI do feel there are a few differences though. Like, in the process of \"predicting as if physics\" we would be expanding a huge hidden variable theory, yet declaring the elements of the theory unreal. Also there would be issues like, how large is the mental unit doing the analysis? Is it a single person over time or multiple people, and over how much time? What theory of personal identity? What is the boundary between something observed or not observed? (With physicalism, although having some boundary between observed / not observed is epistemically relevant, it doesn't have to be exactly defined since it's not ontological; the ontology is something like an algebraic closure that is big enough to contain the state distinctions that are observed.)\r\n\r\nI think maybe someone could try to make an idealist/solipsist minimal philosophy work but it's not what I've done and it doesn't seem easy to include this without running into problems like epistemic stability assumptions.",
      "plaintextDescription": "Ah. I think first of all, it is possible to do ontology in a materialist-directed or idealist-directed way, and the original post is materalist-directed.\n\nI get that the joint distribution over physical facts determines a joint distribution over observations, and we couldn't observe further facts about the joint distribution beyond those implied by the distribution over observations.\n\nI do feel there are a few differences though. Like, in the process of \"predicting as if physics\" we would be expanding a huge hidden variable theory, yet declaring the elements of the theory unreal. Also there would be issues like, how large is the mental unit doing the analysis? Is it a single person over time or multiple people, and over how much time? What theory of personal identity? What is the boundary between something observed or not observed? (With physicalism, although having some boundary between observed / not observed is epistemically relevant, it doesn't have to be exactly defined since it's not ontological; the ontology is something like an algebraic closure that is big enough to contain the state distinctions that are observed.)\n\nI think maybe someone could try to make an idealist/solipsist minimal philosophy work but it's not what I've done and it doesn't seem easy to include this without running into problems like epistemic stability assumptions."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-16T17:47:25.030Z",
    "af": false
  },
  {
    "_id": "WHJ2nYLMnFmo7ZeCy",
    "postId": "Feqa5Rs9db4naTQQS",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "If you haven't, consider reading Bataille (especially _The Accursed Share_).",
      "plaintextDescription": "If you haven't, consider reading Bataille (especially The Accursed Share)."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-16T06:39:51.553Z",
    "af": false
  },
  {
    "_id": "5jneTHTLeC8RzSpZM",
    "postId": "uGakMbD7QKt88oMSa",
    "parentCommentId": "hTSCQZKowkxavpsJh",
    "topLevelCommentId": "mqZDAkALu634yA9mJ",
    "contents": {
      "markdown": "This seems like a case of Bayesian inference. Like, we start from the observation that humans exist having the properties they are, and then find the set of strings consistent with that. Like, start from a uniform measure on the strings and then condition on \"the string produces humans\".\n\nWhich is computationally intractable of course. The usual Bayesian inference issues. Though Bayesian inference would be hard if stochasticity was generated on the fly rather than being initial, too.",
      "plaintextDescription": "This seems like a case of Bayesian inference. Like, we start from the observation that humans exist having the properties they are, and then find the set of strings consistent with that. Like, start from a uniform measure on the strings and then condition on \"the string produces humans\".\n\nWhich is computationally intractable of course. The usual Bayesian inference issues. Though Bayesian inference would be hard if stochasticity was generated on the fly rather than being initial, too."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-15T22:30:26.560Z",
    "af": false
  },
  {
    "_id": "prKa7nsa7GF3rBivx",
    "postId": "uGakMbD7QKt88oMSa",
    "parentCommentId": "9apta5Nxo67h3AbNd",
    "topLevelCommentId": "9apta5Nxo67h3AbNd",
    "contents": {
      "markdown": "Good point; I was mentioning a fundamentalist mainly to ensure that they unironically have standard beliefs like the resurrection, but it applies to a lot more Christians than fundamentalists. (I think Unitarians don't generally believe in the resurrection as a literal physical event?)",
      "plaintextDescription": "Good point; I was mentioning a fundamentalist mainly to ensure that they unironically have standard beliefs like the resurrection, but it applies to a lot more Christians than fundamentalists. (I think Unitarians don't generally believe in the resurrection as a literal physical event?)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-15T19:48:57.276Z",
    "af": false
  },
  {
    "_id": "ymSSyvH4BKKRWEZWR",
    "postId": "uGakMbD7QKt88oMSa",
    "parentCommentId": "DxA5g6Kqq5sELfobN",
    "topLevelCommentId": "mqZDAkALu634yA9mJ",
    "contents": {
      "markdown": "I'm not sure why you're thinking about guessing model weights here. The thing I'm thinking with stochastic models is the forward pass bit, Monte Carlo sampling. I'm not sure why pre-computed randomness would be a problem for that portion.\n\nAs a weird example: Say there's a memoized random function mapping strings to uniform random bits. This can't really be pre-computed, because it's very big. But it can be lazily evaluated, as if pre-computed. Now the stochastic model can query the memoized random function with a unique specification of the situation it's querying. This should be equivalent to flipping coins mid-run.\n\nAlternatively, if the Monte Carlo process is sequential, then it can just \"read the next bit\", that's computationally simpler.\n\nMaybe it's not an issue for forward sampling but it is for backprop? Not sure what you mean.",
      "plaintextDescription": "I'm not sure why you're thinking about guessing model weights here. The thing I'm thinking with stochastic models is the forward pass bit, Monte Carlo sampling. I'm not sure why pre-computed randomness would be a problem for that portion.\n\nAs a weird example: Say there's a memoized random function mapping strings to uniform random bits. This can't really be pre-computed, because it's very big. But it can be lazily evaluated, as if pre-computed. Now the stochastic model can query the memoized random function with a unique specification of the situation it's querying. This should be equivalent to flipping coins mid-run.\n\nAlternatively, if the Monte Carlo process is sequential, then it can just \"read the next bit\", that's computationally simpler.\n\nMaybe it's not an issue for forward sampling but it is for backprop? Not sure what you mean."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-15T19:46:29.457Z",
    "af": false
  },
  {
    "_id": "7LwNZKbcDDL3iJqHo",
    "postId": "uGakMbD7QKt88oMSa",
    "parentCommentId": "mqZDAkALu634yA9mJ",
    "topLevelCommentId": "mqZDAkALu634yA9mJ",
    "contents": {
      "markdown": "If the universe has high K complexity than any theoretically best model has to be either stochastic or \"inherently complex\" (which is worse than stochastic)\n\nThat might or might not be the case. From *current models in practice* having to be stochastic to make good predictions, it doesn't follow that the theoretically best models must be. But it could be the case.\n\nI'm not sure why 'partially-stochastic' would ever fail, due to the coding theorem. That is, there is an alternative way of modeling a model that makes stochastic decisions along the way, where all stochastic decisions are made initially and instead of making a new stochastic decision, you read from these initial bits.",
      "plaintextDescription": "If the universe has high K complexity than any theoretically best model has to be either stochastic or \"inherently complex\" (which is worse than stochastic)\n\nThat might or might not be the case. From current models in practice having to be stochastic to make good predictions, it doesn't follow that the theoretically best models must be. But it could be the case.\n\nI'm not sure why 'partially-stochastic' would ever fail, due to the coding theorem. That is, there is an alternative way of modeling a model that makes stochastic decisions along the way, where all stochastic decisions are made initially and instead of making a new stochastic decision, you read from these initial bits."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-15T18:33:43.270Z",
    "af": false
  },
  {
    "_id": "8ciCD9z33jjwXXjS6",
    "postId": "uGakMbD7QKt88oMSa",
    "parentCommentId": "fiBLCDawo5fSMBgoA",
    "topLevelCommentId": "fiBLCDawo5fSMBgoA",
    "contents": {
      "markdown": "- Composite objects: Statements about composite objects have implications for microstates. The idea would be that there is no content to statements about composite objects, beyond the implications for microstates.\n- Outside world: Broadly scientific realist so yes.\n- Skeptical hypotheses: Some of the sections include \"non-realism\", not sure if that counts\n\nBut also... Did you read the post? I thought I was clear about including a lot of things in this minimal position?",
      "plaintextDescription": " * Composite objects: Statements about composite objects have implications for microstates. The idea would be that there is no content to statements about composite objects, beyond the implications for microstates.\n * Outside world: Broadly scientific realist so yes.\n * Skeptical hypotheses: Some of the sections include \"non-realism\", not sure if that counts\n\nBut also... Did you read the post? I thought I was clear about including a lot of things in this minimal position?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-15T17:44:45.049Z",
    "af": false
  },
  {
    "_id": "jyf7h4grvGhPJRK3h",
    "postId": "uGakMbD7QKt88oMSa",
    "parentCommentId": "NdQtmHZazcGrysGwv",
    "topLevelCommentId": "NdQtmHZazcGrysGwv",
    "contents": {
      "markdown": "Ah good point. Edited",
      "plaintextDescription": "Ah good point. Edited"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-15T04:11:33.792Z",
    "af": false
  },
  {
    "_id": "5vwGPd3QGrfBkhyJS",
    "postId": "eLDgDAWphHaAJYMxN",
    "parentCommentId": "RHYavgwDaX4tiEMyf",
    "topLevelCommentId": "ExFh2pmB2ANxk7ft2",
    "contents": {
      "markdown": "I guess I'm more interested in modeling cases of AI importance/risk on account of pursuit of convergent instrumental goals. If there's a setting without convergent instrumental goals, there might be a generalization, but it's less clear.\r\n\r\nWith single-turn question answering, one could ask about accuracy, which would rate a system giving intentionally wrong answers as un-intelligent. The thing I meant to point to with AIXI is that it would be nice to have a measure of intelligence for mis-aligned systems (rather than declaring them un-intelligent because they don't satisfy the objective like reward maximization / question answering / etc). If there is a possible \"intelligent misalignment\" in the single turn case (e.g. question answering) then there might be a corresponding intelligence metric that accounts for intelligent misaligned systems.",
      "plaintextDescription": "I guess I'm more interested in modeling cases of AI importance/risk on account of pursuit of convergent instrumental goals. If there's a setting without convergent instrumental goals, there might be a generalization, but it's less clear.\n\nWith single-turn question answering, one could ask about accuracy, which would rate a system giving intentionally wrong answers as un-intelligent. The thing I meant to point to with AIXI is that it would be nice to have a measure of intelligence for mis-aligned systems (rather than declaring them un-intelligent because they don't satisfy the objective like reward maximization / question answering / etc). If there is a possible \"intelligent misalignment\" in the single turn case (e.g. question answering) then there might be a corresponding intelligence metric that accounts for intelligent misaligned systems."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-11T23:14:06.971Z",
    "af": false
  },
  {
    "_id": "hXPvrmdXMYcJt3gyz",
    "postId": "eLDgDAWphHaAJYMxN",
    "parentCommentId": "ExFh2pmB2ANxk7ft2",
    "topLevelCommentId": "ExFh2pmB2ANxk7ft2",
    "contents": {
      "markdown": "Well I think multi turn games are generally of more import? Wouldn't AI doing big things like inventing new technology take multiple turns?\n\nI guess cases like answering a math question could be single turn. I think multi turn settings are more appropriate to intelligent *agency* though.",
      "plaintextDescription": "Well I think multi turn games are generally of more import? Wouldn't AI doing big things like inventing new technology take multiple turns?\n\nI guess cases like answering a math question could be single turn. I think multi turn settings are more appropriate to intelligent agency though."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-08-11T16:49:44.220Z",
    "af": false
  },
  {
    "_id": "a7DvsbDyBuYvc4i4N",
    "postId": "eLDgDAWphHaAJYMxN",
    "parentCommentId": "oovQ9GqEjetKdrCqt",
    "topLevelCommentId": "oovQ9GqEjetKdrCqt",
    "contents": {
      "markdown": "That seems pretty close. One complication is what \"can be successfully navigated towards\" means; can a paperclip maximizer successfully navigate towards states without lots of paperclips? I suppose if it factors into a \"goal module\" and a \"rest of the agent module\", then the \"rest of the agent module\" could navigate towards lots of different states even if the overall agent couldn't.\n\n[Causal entropic forces](https://www.alexwg.org/publications/PhysRevLett_110-168702.pdf) is another proposal that's related to being able to reach a lot of states. Also  [empowerment objectives](https://en.wikipedia.org/wiki/Empowerment_\\(artificial_intelligence\\)).\n\nOne reason I mentioned MDP value functions is that they don't bake in the assumption that the value function only specifies terminal values, the value function also includes instrumental state values. So it might be able to represent some of what you're talking about.",
      "plaintextDescription": "That seems pretty close. One complication is what \"can be successfully navigated towards\" means; can a paperclip maximizer successfully navigate towards states without lots of paperclips? I suppose if it factors into a \"goal module\" and a \"rest of the agent module\", then the \"rest of the agent module\" could navigate towards lots of different states even if the overall agent couldn't.\n\nCausal entropic forces is another proposal that's related to being able to reach a lot of states. Also empowerment objectives.\n\nOne reason I mentioned MDP value functions is that they don't bake in the assumption that the value function only specifies terminal values, the value function also includes instrumental state values. So it might be able to represent some of what you're talking about."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-08-11T06:02:24.638Z",
    "af": false
  },
  {
    "_id": "cLus4QbiBrWrEbYEv",
    "postId": "yJ5JKvuqycjoiFfEb",
    "parentCommentId": "ykF7TzsaFHv4tAtAA",
    "topLevelCommentId": "ykF7TzsaFHv4tAtAA",
    "contents": {
      "markdown": "EDT doesn't have counterfactuals at all, IMO. It has Bayesian conditionals.",
      "plaintextDescription": "EDT doesn't have counterfactuals at all, IMO. It has Bayesian conditionals."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-07-25T20:40:25.988Z",
    "af": false
  },
  {
    "_id": "aQyaCTY9HiCJkwahv",
    "postId": "7e8gLRZiaHoerdNdg",
    "parentCommentId": "cFAKo9BdcAaqLhxog",
    "topLevelCommentId": "hhHCWkfe8jNBW44y5",
    "contents": {
      "markdown": "![](https://upload.wikimedia.org/wikipedia/commons/thumb/1/1e/CIE1931xy_gamut_comparison.svg/250px-CIE1931xy_gamut_comparison.svg.png) Yeah, color spaces are asymmetric.",
      "plaintextDescription": "Yeah, color spaces are asymmetric."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-19T01:11:40.237Z",
    "af": false
  },
  {
    "_id": "82HJpuxAypeAPsjrX",
    "postId": "sWuBhds97j8R6y22m",
    "parentCommentId": "gexzb4uRgT55EFvK5",
    "topLevelCommentId": "BLDtQ7HuZxyPCkboQ",
    "contents": {
      "markdown": "See paragraph starting with \"We don’t have the framework to rule this out, so far.\"\n\nA common sort of view is \"Even if we knew all the physical facts, we wouldn't know moral facts, because there is no way to get ought from is. Woe is us! We may never know the true morality even if we knew all physical facts!\"\n\nI think there is something wrong with this, specifically in that it seems morally supernaturalist. It's asserting there is a real fact of the matter of morality, which isn't determined by physical facts and so on. But that seems like the P-evildoers scenario. This person imagines that it is a possible state of affairs that the same physical person taking the same physical actions could have different moral properties.\n\nI also think that, ironically, extreme skepticism of moral realism might indicate some sort of hidden metaphysical constraint applied to moral judgments, which point towards moral realism. As an example, perhaps someone asserts, \"Morality has to be real, because normative actions and judgments are tied to the process of epistemology itself, e.g. checking claims and not lying.\". One sort of objection I've heard is \"But this is too weak, it doesn't rule out genocide being morally permissible\". That seems to show a pre-existing strong belief about morality, at least one conditional on moral realism.\n\nI'm not going as far as to propose a specific axiom schema for morality, but I'm suggesting some general properties it should have. In cases like \"temperature\", a scientific/physical definition displaces a more vague intuitive one. In the case of \"morality\", that seems to be harder, because of people's strong but un-formalized beliefs about it. Those strong beliefs, though, point towards possible constraints the axiomatic system must satisfy.\n\nTo go out on more of a limb, I think, conditional on moral naturalism, it is likely that morality is a physically local property, not depending on the actual facts of distant aliens (in contrast to theoretically accessible epistemic states about distant aliens). This is similar to the no P-evildoers principle, in that the same physical person taking the same physical actions can't possibly have two incompatible moral properties which depend on very distant physical entities.",
      "plaintextDescription": "See paragraph starting with \"We don’t have the framework to rule this out, so far.\"\n\nA common sort of view is \"Even if we knew all the physical facts, we wouldn't know moral facts, because there is no way to get ought from is. Woe is us! We may never know the true morality even if we knew all physical facts!\"\n\nI think there is something wrong with this, specifically in that it seems morally supernaturalist. It's asserting there is a real fact of the matter of morality, which isn't determined by physical facts and so on. But that seems like the P-evildoers scenario. This person imagines that it is a possible state of affairs that the same physical person taking the same physical actions could have different moral properties.\n\nI also think that, ironically, extreme skepticism of moral realism might indicate some sort of hidden metaphysical constraint applied to moral judgments, which point towards moral realism. As an example, perhaps someone asserts, \"Morality has to be real, because normative actions and judgments are tied to the process of epistemology itself, e.g. checking claims and not lying.\". One sort of objection I've heard is \"But this is too weak, it doesn't rule out genocide being morally permissible\". That seems to show a pre-existing strong belief about morality, at least one conditional on moral realism.\n\nI'm not going as far as to propose a specific axiom schema for morality, but I'm suggesting some general properties it should have. In cases like \"temperature\", a scientific/physical definition displaces a more vague intuitive one. In the case of \"morality\", that seems to be harder, because of people's strong but un-formalized beliefs about it. Those strong beliefs, though, point towards possible constraints the axiomatic system must satisfy.\n\nTo go out on more of a limb, I think, conditional on moral naturalism, it is likely that morality is a physically local property, not depending on the actual facts of distant aliens (in contrast to theoretically "
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-18T17:58:11.572Z",
    "af": false
  },
  {
    "_id": "nk4Q8wLvLhYK9Skkr",
    "postId": "evYne4Xx7L9J96BHW",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "> Because agents with good values can, quote, “steal the strategies” that other agents pursue. This is a term from an essay by Paul Christiano, I think, from 2021.\n\nEarlier. I wrote a [2017 post](https://www.alignmentforum.org/posts/5bd75cc58225bf0670375325/strategies-for-coalitions-in-unit-sum-games) quoted by [Paul in 2019](https://www.alignmentforum.org/posts/nRAMpjnb6Z4Qv3imF/the-strategy-stealing-assumption), although I think I originally got the idea from Paul.\n\nAppreciate the writeup, it grapples with the competitive landscape rather than falling into common patterns of risk aversion / anthropomorphism / etc. I'm probably more sympathetic to locusts than most people here, which is why I find worries of \"all of value will be destroyed unless we freeze hominid values eternally!\" to be unconvincing. There's an important question about what level of competition to expect or target, and my intuition is that the level of competition to realistically target in the long term is probably way higher than is comfortable to comprehend from a risk analysis (rather than utility maximization) frame.",
      "plaintextDescription": "> Because agents with good values can, quote, “steal the strategies” that other agents pursue. This is a term from an essay by Paul Christiano, I think, from 2021.\n\nEarlier. I wrote a 2017 post quoted by Paul in 2019, although I think I originally got the idea from Paul.\n\nAppreciate the writeup, it grapples with the competitive landscape rather than falling into common patterns of risk aversion / anthropomorphism / etc. I'm probably more sympathetic to locusts than most people here, which is why I find worries of \"all of value will be destroyed unless we freeze hominid values eternally!\" to be unconvincing. There's an important question about what level of competition to expect or target, and my intuition is that the level of competition to realistically target in the long term is probably way higher than is comfortable to comprehend from a risk analysis (rather than utility maximization) frame."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-07-18T07:45:11.819Z",
    "af": false
  },
  {
    "_id": "wdYi8qsgtG336H9Lg",
    "postId": "sWuBhds97j8R6y22m",
    "parentCommentId": "BLDtQ7HuZxyPCkboQ",
    "topLevelCommentId": "BLDtQ7HuZxyPCkboQ",
    "contents": {
      "markdown": "I bracketed the possibility of moral meaninglessness, I don't have a strong argument against it. I think the relevance of \"moral\" is in possible principles it intuitively satiafies like the no P-evildoers principle. Intuitions about something can point towards different possible mathematical objects satisfying some of these intuitions. In some cases the mathematical object becomes a standard meaning of the term, in other cases not. So the exercise has to do with seeing what certain principles derived from moral intuitions would point to as possible mathematical objects.\r\n\r\nI'm also distinguishing between the idea that moral facts are logically inferrable from physical facts (logical supervenience) from a weaker notion of derivability from $T^*$ which includes metaphysical and mathematical axioms. As an anology, a certain axiomatization of geometry may under-determine properties of triangles, yet perhaps those properties could be determined through additional metaphysical axioms ensuring that the triangles in the system really match a priori spatial intuition of triangles.",
      "plaintextDescription": "I bracketed the possibility of moral meaninglessness, I don't have a strong argument against it. I think the relevance of \"moral\" is in possible principles it intuitively satiafies like the no P-evildoers principle. Intuitions about something can point towards different possible mathematical objects satisfying some of these intuitions. In some cases the mathematical object becomes a standard meaning of the term, in other cases not. So the exercise has to do with seeing what certain principles derived from moral intuitions would point to as possible mathematical objects.\n\nI'm also distinguishing between the idea that moral facts are logically inferrable from physical facts (logical supervenience) from a weaker notion of derivability from T∗ which includes metaphysical and mathematical axioms. As an anology, a certain axiomatization of geometry may under-determine properties of triangles, yet perhaps those properties could be determined through additional metaphysical axioms ensuring that the triangles in the system really match a priori spatial intuition of triangles."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-18T00:25:02.269Z",
    "af": false
  },
  {
    "_id": "xymHcyN8ioTQdrFeg",
    "postId": "sWuBhds97j8R6y22m",
    "parentCommentId": "wPSCEFfZzPt87Cy4h",
    "topLevelCommentId": "wPSCEFfZzPt87Cy4h",
    "contents": {
      "markdown": "As I said, I was bracketing moral meaninglessness and moral trivialism rather than arguing against them. It's possible that your view would count as moral meaninglessness (if moral statements taken as properties of possible worlds are simply ill-typed, since they instead relations between possible worlds and something else.... though that raises the question of why that something else isn't wrapped into the possible world).\r\n\r\nThere are people who adopt different norms but they behave different. So this wouldn't be a case of a single physical person having multiple possible norms.\r\n\r\nI suppose you could say that the moral properties specify a matrix of how each physical agent in a universe judges each other physical agent. Then the same physically identical person could be judged multiple ways, but the same universe generates the same judgments. And the mapping from physical universes to these matrices is necessary across possible worlds.\r\n\r\nMy intuition is that, for similar reasons as to why the judgments of dieties causing different moral judgments of the same physical person is implausible, so it is also implausible that how morally someone is acting requires further facts of who is judging them. This would be like, in the case of the matrix of moral judgments, the moral properties of a person's actions depend on which distant aliens exist, because different distant aliens would have different judgments of this person's actions. But my intuition is rather that the moral properties of the person's actions are more local than that, they don't depend on further facts about physically distant entities.",
      "plaintextDescription": "As I said, I was bracketing moral meaninglessness and moral trivialism rather than arguing against them. It's possible that your view would count as moral meaninglessness (if moral statements taken as properties of possible worlds are simply ill-typed, since they instead relations between possible worlds and something else.... though that raises the question of why that something else isn't wrapped into the possible world).\n\nThere are people who adopt different norms but they behave different. So this wouldn't be a case of a single physical person having multiple possible norms.\n\nI suppose you could say that the moral properties specify a matrix of how each physical agent in a universe judges each other physical agent. Then the same physically identical person could be judged multiple ways, but the same universe generates the same judgments. And the mapping from physical universes to these matrices is necessary across possible worlds.\n\nMy intuition is that, for similar reasons as to why the judgments of dieties causing different moral judgments of the same physical person is implausible, so it is also implausible that how morally someone is acting requires further facts of who is judging them. This would be like, in the case of the matrix of moral judgments, the moral properties of a person's actions depend on which distant aliens exist, because different distant aliens would have different judgments of this person's actions. But my intuition is rather that the moral properties of the person's actions are more local than that, they don't depend on further facts about physically distant entities."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-17T18:28:21.561Z",
    "af": false
  },
  {
    "_id": "vC9drYTQCfhwSRKWL",
    "postId": "7e8gLRZiaHoerdNdg",
    "parentCommentId": "K9bbMTChtxmnWoHG2",
    "topLevelCommentId": "hhHCWkfe8jNBW44y5",
    "contents": {
      "markdown": "Like, red and green are \"interchangeable\", if they were flipped there would be no behavioral difference. This is somewhat dubious because the experience of red might include various intuitive associations with red. Also because maybe red and green are neurally encoded differently.",
      "plaintextDescription": "Like, red and green are \"interchangeable\", if they were flipped there would be no behavioral difference. This is somewhat dubious because the experience of red might include various intuitive associations with red. Also because maybe red and green are neurally encoded differently."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-15T19:04:33.576Z",
    "af": false
  },
  {
    "_id": "tRGw4ffc9Ci67iKSi",
    "postId": "7e8gLRZiaHoerdNdg",
    "parentCommentId": "hhHCWkfe8jNBW44y5",
    "topLevelCommentId": "hhHCWkfe8jNBW44y5",
    "contents": {
      "markdown": "It would be an example of some facts about minds being different despite same physical states. Personally I find it less plausible than the zombie argument because it's assuming a symmetry in qualia that might not hold.",
      "plaintextDescription": "It would be an example of some facts about minds being different despite same physical states. Personally I find it less plausible than the zombie argument because it's assuming a symmetry in qualia that might not hold."
    },
    "baseScore": 8,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-07-15T16:34:08.332Z",
    "af": false
  },
  {
    "_id": "t9FW5hJNdyDpsN8SX",
    "postId": "7e8gLRZiaHoerdNdg",
    "parentCommentId": "HKdjRzLLr8N4rcd9e",
    "topLevelCommentId": "HKdjRzLLr8N4rcd9e",
    "contents": {
      "markdown": "Yeah that's what I meant by the laws being deterministic. Which doesn't imply causality (due to Pearl's observation that the same Bayes net could correspond to multiple causal nets). Maybe I could have phrased it more clearly ",
      "plaintextDescription": "Yeah that's what I meant by the laws being deterministic. Which doesn't imply causality (due to Pearl's observation that the same Bayes net could correspond to multiple causal nets). Maybe I could have phrased it more clearly"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-15T08:23:10.207Z",
    "af": false
  },
  {
    "_id": "CmXLsjzp6XvLHZBCJ",
    "postId": "yyf4KjuD8gofLQBD4",
    "parentCommentId": "hEwBnDJjzn9pembFj",
    "topLevelCommentId": "hEwBnDJjzn9pembFj",
    "contents": {
      "markdown": "I was trying to make a claim about marginal value. Like, if the planet has 1 billion trees, then the last tree doesn't add much aesthetic value, compared with whatever art could be made with that tree.\r\n\r\nThat said, it gets more complicated if marginal nature consumption reduces biodiversity significantly.",
      "plaintextDescription": "I was trying to make a claim about marginal value. Like, if the planet has 1 billion trees, then the last tree doesn't add much aesthetic value, compared with whatever art could be made with that tree.\n\nThat said, it gets more complicated if marginal nature consumption reduces biodiversity significantly."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-07-05T19:03:44.184Z",
    "af": false
  },
  {
    "_id": "ySfgeuWajyHJGmNMx",
    "postId": "yyf4KjuD8gofLQBD4",
    "parentCommentId": "NqrSCynEgb7zWv7fN",
    "topLevelCommentId": "NqrSCynEgb7zWv7fN",
    "contents": {
      "markdown": "Yeah; simulation hypothesis seems like it implies at least a weak form of theism. It's a scenario where the higher power has reason not to show itself. That makes it hard to rule out, although it also means it can't be the \"main\" hypothesis; how simulated worlds look depends on how base-level worlds look.",
      "plaintextDescription": "Yeah; simulation hypothesis seems like it implies at least a weak form of theism. It's a scenario where the higher power has reason not to show itself. That makes it hard to rule out, although it also means it can't be the \"main\" hypothesis; how simulated worlds look depends on how base-level worlds look."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-07-05T19:02:41.067Z",
    "af": false
  },
  {
    "_id": "joTjSiQvS5KwLLGgM",
    "postId": "bnnKGSCHJghAvqPjS",
    "parentCommentId": "NhTrRfH9znJn8Zo7M",
    "topLevelCommentId": "NhTrRfH9znJn8Zo7M",
    "contents": {
      "markdown": "Relevant paper: [Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?](https://arxiv.org/abs/2504.13837)\r\n\r\n[YouTube explanatory video](https://www.youtube.com/watch?v=z3awgfU4yno)",
      "plaintextDescription": "Relevant paper: Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?\n\nYouTube explanatory video"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-27T01:36:54.479Z",
    "af": false
  },
  {
    "_id": "Ge5ksf2irrPzSbuoi",
    "postId": "6Fo8fjvpL7pwCTz3t",
    "parentCommentId": "A2ugCmJLvr7hLNnEc",
    "topLevelCommentId": "A2ugCmJLvr7hLNnEc",
    "contents": {
      "markdown": "Few things are at their most useful in their current form. A field of wild grass is not a factory.\r\n\r\nThe sun could generate energy in a more efficient manner. Parts of it could be consumed faster, other parts could slow down to conserve energy.",
      "plaintextDescription": "Few things are at their most useful in their current form. A field of wild grass is not a factory.\n\nThe sun could generate energy in a more efficient manner. Parts of it could be consumed faster, other parts could slow down to conserve energy."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-06-26T17:12:56.541Z",
    "af": false
  },
  {
    "_id": "chmiL9HpSceMFkMZF",
    "postId": "D4eZF6FAZhrW4KaGG",
    "parentCommentId": "BKJyH9mBtxFXfjq6t",
    "topLevelCommentId": "BKJyH9mBtxFXfjq6t",
    "contents": {
      "markdown": "Situational Awareness and AI 2027 have been signal boosted to normal people more than other predictions, though. Like, they both have their own website, AI 2027 has a bunch of fancy client-side animation and Scott Alexander collaborated, and someone made a Youtube video on AI 2027.\r\n\r\nWhile AI safety people have criticized the timeline predictions to some extent, there hasn't been much in-depth criticism (aside from the recent very long post on AI 2027), the general sentiment on their timelines seems positive (although Situational Awareness has been criticized for contributing to arms race dynamics).\r\n\r\nI get that someone who looks at AI safety people's timelines in more detail would get a different impression. Though, notably, [Metaculus](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/) lists Jan 2027 as a \"community prediction\" of \"weakly general AI\". Sure, someone could argue that weakly general AI doesn't imply human-level AGI soon after, but mostly when I see AI safety people point to this Metaculus market, it's as evidence that experts believe human-level AGI will arrive in the next few years, there is not emphasis on the delta between weakly general AI and human-level AGI.\r\n\r\nSo I see how an outsider would see more 2027-2029 timelines from AI safety people and assume that's what they consider a reasonable median, if they aren't looking into it closely. This is partially due to internet/social media attention dynamics, where surprising predictions get more attention.\r\n\r\nWhat I think we could agree on is that, if 2028 rolls around and things seem pretty much like today, then whatever distributed attention algorithm promotes things like Situational Awareness and AI 2027 to people's attention in practice leads to flawed predictions, and people who were worried *because of listening to this algorithm* should chill out and re-evaluate. (We probably also agree they should chill out *today* because this popular content isn't reflective of AI safety people's general opinions.)",
      "plaintextDescription": "Situational Awareness and AI 2027 have been signal boosted to normal people more than other predictions, though. Like, they both have their own website, AI 2027 has a bunch of fancy client-side animation and Scott Alexander collaborated, and someone made a Youtube video on AI 2027.\n\nWhile AI safety people have criticized the timeline predictions to some extent, there hasn't been much in-depth criticism (aside from the recent very long post on AI 2027), the general sentiment on their timelines seems positive (although Situational Awareness has been criticized for contributing to arms race dynamics).\n\nI get that someone who looks at AI safety people's timelines in more detail would get a different impression. Though, notably, Metaculus lists Jan 2027 as a \"community prediction\" of \"weakly general AI\". Sure, someone could argue that weakly general AI doesn't imply human-level AGI soon after, but mostly when I see AI safety people point to this Metaculus market, it's as evidence that experts believe human-level AGI will arrive in the next few years, there is not emphasis on the delta between weakly general AI and human-level AGI.\n\nSo I see how an outsider would see more 2027-2029 timelines from AI safety people and assume that's what they consider a reasonable median, if they aren't looking into it closely. This is partially due to internet/social media attention dynamics, where surprising predictions get more attention.\n\nWhat I think we could agree on is that, if 2028 rolls around and things seem pretty much like today, then whatever distributed attention algorithm promotes things like Situational Awareness and AI 2027 to people's attention in practice leads to flawed predictions, and people who were worried because of listening to this algorithm should chill out and re-evaluate. (We probably also agree they should chill out today because this popular content isn't reflective of AI safety people's general opinions.)"
    },
    "baseScore": 60,
    "voteCount": 28,
    "createdAt": null,
    "postedAt": "2025-06-21T21:20:39.930Z",
    "af": false
  },
  {
    "_id": "nwLHfkhPan8fEGgyv",
    "postId": "KnQs55tjxWopCzKsk",
    "parentCommentId": "gnLennPwKhEgg7yqJ",
    "topLevelCommentId": "gnLennPwKhEgg7yqJ",
    "contents": {
      "markdown": "So are we going to have AGI by 2029? It depends how you define it of course but I really doubt it will be able to automate >99% of human labor.",
      "plaintextDescription": "So are we going to have AGI by 2029? It depends how you define it of course but I really doubt it will be able to automate >99% of human labor."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-02T02:35:48.989Z",
    "af": false
  },
  {
    "_id": "j8XmN7AsreeaDhF6x",
    "postId": "uvNsFv9YKxtfSBffM",
    "parentCommentId": "FZgdyq2GYrEm4wzJY",
    "topLevelCommentId": "Dod7tQdBeeJ6dJeb8",
    "contents": {
      "markdown": "Think of it as a predicate on policies. The predicate (local optimality) is true when, for each action the policy assigns non-zero probability to, that action maximizes expected utility relative to the policy.\r\n\r\n> I am interpreting that formula as “compute this quantity in the sum and find the  a'  from the set of all possible actions  \\mathcal{A}  that maximizes it, then do that”. Am I wrong?\r\n\r\nYes. It's a predicate on policies. If two different actions (given an observation) maximize expected utility, then either action can be taken. Your description doesn't allow that, because it assumes there is a single a' that maximizes expected utility. Whereas, with a predicate on policies, we could potentially allow multiple actions.\r\n\r\n> Or is your point that the only self-consistent policy is the one where both have equal expected utility, and thus I can in fact choose either? Though then I have to choose *according to the probabilities specified in the policy*.\r\n\r\nYes, exactly. Look up Nash equilibrium in matching pennies. It's pretty similar. (Except your expected utilities as a function of your action depend on the opponent's actions in matching pennies, and your own action in absent minded driver.)",
      "plaintextDescription": "Think of it as a predicate on policies. The predicate (local optimality) is true when, for each action the policy assigns non-zero probability to, that action maximizes expected utility relative to the policy.\n\n> I am interpreting that formula as “compute this quantity in the sum and find the a' from the set of all possible actions \\mathcal{A} that maximizes it, then do that”. Am I wrong?\n\nYes. It's a predicate on policies. If two different actions (given an observation) maximize expected utility, then either action can be taken. Your description doesn't allow that, because it assumes there is a single a' that maximizes expected utility. Whereas, with a predicate on policies, we could potentially allow multiple actions.\n\n> Or is your point that the only self-consistent policy is the one where both have equal expected utility, and thus I can in fact choose either? Though then I have to choose according to the probabilities specified in the policy.\n\nYes, exactly. Look up Nash equilibrium in matching pennies. It's pretty similar. (Except your expected utilities as a function of your action depend on the opponent's actions in matching pennies, and your own action in absent minded driver.)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-20T16:07:56.377Z",
    "af": false
  },
  {
    "_id": "7wG5tZtBmA3wNb3yy",
    "postId": "uvNsFv9YKxtfSBffM",
    "parentCommentId": "38iECQg8hceJN7tQW",
    "topLevelCommentId": "Dod7tQdBeeJ6dJeb8",
    "contents": {
      "markdown": "Given a policy you can evaluate the expected utility of any action. This depends on the policy.\r\n\r\nIn the absent minded driver problem, if the policy is to exit 10% of the time, then the 'exit' action has higher expected utility than the 'advance' action. Whereas if the policy is to exit 90% of the time, then the 'advance' action has higher expected utility.\r\n\r\nThis is because the policy affects the SIA probabilities and Q values. The higher your exit probability, the more likely you are at node X (and therefore should advance).\r\n\r\nThe local optimality condition for a policy is that each action the policy assigns non-zero probability to must have optimal expected utility *relative to the policy*. It's reflective like Nash equilibrium.\r\n\r\nThis is clear from the formula:\r\n\r\n$\\forall o \\in \\mathcal{O}, a \\in \\mathcal{A}: \\pi(a | o) > 0 \\Rightarrow a \\in \\arg\\max_{a' \\in \\mathcal{A}} \\sum_s SIA_{\\pi}(s | o) Q_{\\pi}(s, a')$\r\n\r\nNote that SIA and Q depend on $\\pi$. This is the condition for local optimality of $\\pi$. It is about each action that $\\pi$ assigns non-zero probability to being optimal *relative to $\\pi$*.\r\n\r\n(That's the local optimality condition; there's also global optimality, where utility is directly a function of the policy, and is fairly obvious. The main theorem of the post is: Global optimality implies local optimality.)",
      "plaintextDescription": "Given a policy you can evaluate the expected utility of any action. This depends on the policy.\n\nIn the absent minded driver problem, if the policy is to exit 10% of the time, then the 'exit' action has higher expected utility than the 'advance' action. Whereas if the policy is to exit 90% of the time, then the 'advance' action has higher expected utility.\n\nThis is because the policy affects the SIA probabilities and Q values. The higher your exit probability, the more likely you are at node X (and therefore should advance).\n\nThe local optimality condition for a policy is that each action the policy assigns non-zero probability to must have optimal expected utility relative to the policy. It's reflective like Nash equilibrium.\n\nThis is clear from the formula:\n\n∀o∈O,a∈A:π(a|o)>0⇒a∈argmaxa′∈A∑sSIAπ(s|o)Qπ(s,a′)\n\nNote that SIA and Q depend on π. This is the condition for local optimality of π. It is about each action that π assigns non-zero probability to being optimal relative to π.\n\n(That's the local optimality condition; there's also global optimality, where utility is directly a function of the policy, and is fairly obvious. The main theorem of the post is: Global optimality implies local optimality.)"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-20T14:24:56.042Z",
    "af": false
  },
  {
    "_id": "TQMqWqqqLs4u5t8ao",
    "postId": "uvNsFv9YKxtfSBffM",
    "parentCommentId": "gasZzDANYpkeSdWtL",
    "topLevelCommentId": "Dod7tQdBeeJ6dJeb8",
    "contents": {
      "markdown": "No, check the post more carefully... $\\pi(a | o)$ in the above condition is a probability. The local optimality condition says that any action taken with nonzero probability must have maximal expected value. Which is standard for Nash equilibrium.\r\n\r\nCritically, the agent can mix over strategies when the expected utility from each is equal and maximal.  That's very standard for Nash equilibrium e.g. in the matching pennies game.",
      "plaintextDescription": "No, check the post more carefully... π(a|o) in the above condition is a probability. The local optimality condition says that any action taken with nonzero probability must have maximal expected value. Which is standard for Nash equilibrium.\n\nCritically, the agent can mix over strategies when the expected utility from each is equal and maximal. That's very standard for Nash equilibrium e.g. in the matching pennies game."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-20T12:10:42.849Z",
    "af": false
  },
  {
    "_id": "3eFsTPbXqEiQkd2da",
    "postId": "uvNsFv9YKxtfSBffM",
    "parentCommentId": "9Apj9f3GwQajzphzf",
    "topLevelCommentId": "Dod7tQdBeeJ6dJeb8",
    "contents": {
      "markdown": "I still think it shouldn't matter...\r\n\r\nWe should have f(p) / g(p) = 0 in exactly the cases where f(p) = 0, as long as g(p) is always positive.\r\n\r\nIn the formula\r\n\r\n$\\forall o \\in \\mathcal{O}, a \\in \\mathcal{A}: \\pi(a | o) > 0 \\Rightarrow a \\in \\arg\\max_{a' \\in \\mathcal{A}} \\sum_s SIA_{\\pi}(s | o) Q_{\\pi}(s, a')$\r\n\r\n we could have multiplied $SIA_{\\pi}(s | o)$ for all s by the same (always-positive) function of $\\pi, o$ and gotten the same result.",
      "plaintextDescription": "I still think it shouldn't matter...\n\nWe should have f(p) / g(p) = 0 in exactly the cases where f(p) = 0, as long as g(p) is always positive.\n\nIn the formula\n\n∀o∈O,a∈A:π(a|o)>0⇒a∈argmaxa′∈A∑sSIAπ(s|o)Qπ(s,a′)\n\nwe could have multiplied SIAπ(s|o) for all s by the same (always-positive) function of π,o and gotten the same result."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-20T00:20:59.183Z",
    "af": false
  },
  {
    "_id": "rsMiB8DMXv9cR7knN",
    "postId": "uvNsFv9YKxtfSBffM",
    "parentCommentId": "wHtzNAYRf8ofYvnn6",
    "topLevelCommentId": "Dod7tQdBeeJ6dJeb8",
    "contents": {
      "markdown": "If we look at a formula like $F_p(X) (Q_p(X, Exit) - Q_p(X, Advance)) + F_p(Y) (Q_p(Y, Exit) - Q_p(Y, Advance)) = 0$, it shouldn't make a difference when scaling by a positive constant.\r\n\r\nThe nice thing about un-normalized probabilities is that if they are 0 you don't get a division by zero. It just becomes a trivial condition of $0 = 0$ if the frequencies are zero. It helps specifically in the local optimality condition, handling the case of an observation that is encountered with probability 0 given the policy.",
      "plaintextDescription": "If we look at a formula like Fp(X)(Qp(X,Exit)−Qp(X,Advance))+Fp(Y)(Qp(Y,Exit)−Qp(Y,Advance))=0, it shouldn't make a difference when scaling by a positive constant.\n\nThe nice thing about un-normalized probabilities is that if they are 0 you don't get a division by zero. It just becomes a trivial condition of 0=0 if the frequencies are zero. It helps specifically in the local optimality condition, handling the case of an observation that is encountered with probability 0 given the policy."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-19T06:25:04.607Z",
    "af": false
  },
  {
    "_id": "efYHoKH2rxaBDyeJu",
    "postId": "uvNsFv9YKxtfSBffM",
    "parentCommentId": "GDiLDDFR6ZWyRhGSW",
    "topLevelCommentId": "Dod7tQdBeeJ6dJeb8",
    "contents": {
      "markdown": "> THIS IS A SIMULATION YOU ARE INSTANCE #4859 YOU ARE IN EITHER NODE X OR Y YOU CAN CHOOSE A PROBABILITY TO [Advance] or [Exit] THE UTILITY IS AS FOLLOWS: * EXITING AT X IS WORTH 0 AND TERMINATES THE INSTANCE * ADVANCING AT X REINITIALISES THE SIMULATION AT Y * EXITING AT Y IS WORTH 4 AND TERMINATES THE INSTANCE * ADVANCING AT Y IS WORTH 1 AND TERMINATES THE INSTANCE PLEASE USE THE KEYPAD TO INPUT THE DESIRED PROBABILITY TO MAXIMISE YOUR UTILITY GOOD LUCK\r\n\r\nSet of states: {X, Y, XE, YE, YA}\r\n\r\nSet of actions: {Advance, Exit}\r\n\r\nSet of observations: {\"\"}\r\n\r\nInitial state: X\r\n\r\nTransition function for non-terminal states:\r\n\r\nt(X, Advance) = Y\r\n\r\nt(X, Exit) = XE\r\n\r\nt(Y, Advance) = YA\r\n\r\nt(Y, Exit) = YE\r\n\r\nTerminal states: {XE, YE, YA}\r\n\r\nUtility function:\r\n\r\nu(XE) = 0\r\n\r\nu(YE) = 4\r\n\r\nu(YA) = 1\r\n\r\nPolicy can be parameterized by p = exit probability.\r\n\r\nFrequencies for non-terminal states (un-normalized):\r\n\r\n$F_p(X) = 1$\r\n\r\n$F_p(Y) = 1 - p$\r\n\r\nSIA un-normalized probabilities:\r\n\r\n$SIA_p(X | \"\") = 1$\r\n\r\n$SIA_p(Y | \"\") = 1 - p$\r\n\r\nNote we have only one possible observation, so SIA un-normalized probabilities match frequencies.\r\n\r\nState values for non-terminals:\r\n\r\n$V_p(Y) = 4p + 1 - p = 3p + 1$\r\n\r\n$V_p(X) = (1 - p) V_p(Y) = (1 - p) (3p + 1) = -3p^2 + 2p + 1$\r\n\r\nQ values for non-terminals:\r\n\r\n$Q_p(Y, Exit) = 4$\r\n\r\n$Q_p(Y, Advance) = 1$\r\n\r\n$Q_p(X, Exit) = 0$\r\n\r\n$Q_p(X, Advance) = V_p(Y) = 3p + 1$\r\n\r\nFor local optimality we compute partial derivatives.\r\n\r\n$d_p(\"\", Exit, Advance, Y) = Q_p(Y, Exit) - Q_p(Y, Advance) = 3$\r\n\r\n$d_p(\"\", Exit, Advance, X) = (1 - p) * d_p(\"\", Exit, Advance, Y) + Q_p(X, Exit) - Q_p(X, Advance) = 3(1-p) - (3p + 1) = 2 - 6p$\r\n\r\nBy the post, this can be equivalently written:\r\n\r\n$d_p(\"\", Exit, Advance, X) = F_p(X) (Q_p(X, Exit) - Q_p(X, Advance)) + F_p(Y) (Q_p(Y, Exit) - Q_p(Y, Advance)) = -(3p + 1) + (1 - p)(4 - 1) = -3p - 1 + 3 - 3p = 2 - 6p$\r\n\r\nTo optimize we set $2 - 6p = 0$ i.e. $p = 1/3$. Remember p is the probability of exit (sorry it's reversed from your usage!). This matches what you computed as globally optimal.\r\n\r\nI'm not going to do this for all of the examples... Is there a specific example where you think the theorem fails?\r\n\r\n\r\n",
      "plaintextDescription": "> THIS IS A SIMULATION YOU ARE INSTANCE #4859 YOU ARE IN EITHER NODE X OR Y YOU CAN CHOOSE A PROBABILITY TO [Advance] or [Exit] THE UTILITY IS AS FOLLOWS: * EXITING AT X IS WORTH 0 AND TERMINATES THE INSTANCE * ADVANCING AT X REINITIALISES THE SIMULATION AT Y * EXITING AT Y IS WORTH 4 AND TERMINATES THE INSTANCE * ADVANCING AT Y IS WORTH 1 AND TERMINATES THE INSTANCE PLEASE USE THE KEYPAD TO INPUT THE DESIRED PROBABILITY TO MAXIMISE YOUR UTILITY GOOD LUCK\n\nSet of states: {X, Y, XE, YE, YA}\n\nSet of actions: {Advance, Exit}\n\nSet of observations: {\"\"}\n\nInitial state: X\n\nTransition function for non-terminal states:\n\nt(X, Advance) = Y\n\nt(X, Exit) = XE\n\nt(Y, Advance) = YA\n\nt(Y, Exit) = YE\n\nTerminal states: {XE, YE, YA}\n\nUtility function:\n\nu(XE) = 0\n\nu(YE) = 4\n\nu(YA) = 1\n\nPolicy can be parameterized by p = exit probability.\n\nFrequencies for non-terminal states (un-normalized):\n\nFp(X)=1\n\nFp(Y)=1−p\n\nSIA un-normalized probabilities:\n\nSIAp(X|\"\")=1\n\nSIAp(Y|\"\")=1−p\n\nNote we have only one possible observation, so SIA un-normalized probabilities match frequencies.\n\nState values for non-terminals:\n\nVp(Y)=4p+1−p=3p+1\n\nVp(X)=(1−p)Vp(Y)=(1−p)(3p+1)=−3p2+2p+1\n\nQ values for non-terminals:\n\nQp(Y,Exit)=4\n\nQp(Y,Advance)=1\n\nQp(X,Exit)=0\n\nQp(X,Advance)=Vp(Y)=3p+1\n\nFor local optimality we compute partial derivatives.\n\ndp(\"\",Exit,Advance,Y)=Qp(Y,Exit)−Qp(Y,Advance)=3\n\ndp(\"\",Exit,Advance,X)=(1−p)∗dp(\"\",Exit,Advance,Y)+Qp(X,Exit)−Qp(X,Advance)=3(1−p)−(3p+1)=2−6p\n\nBy the post, this can be equivalently written:\n\ndp(\"\",Exit,Advance,X)=Fp(X)(Qp(X,Exit)−Qp(X,Advance))+Fp(Y)(Qp(Y,Exit)−Qp(Y,Advance))=−(3p+1)+(1−p)(4−1)=−3p−1+3−3p=2−6p\n\nTo optimize we set 2−6p=0 i.e. p=1/3. Remember p is the probability of exit (sorry it's reversed from your usage!). This matches what you computed as globally optimal.\n\nI'm not going to do this for all of the examples... Is there a specific example where you think the theorem fails?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-05-18T19:37:09.539Z",
    "af": false
  },
  {
    "_id": "Dod7tQdBeeJ6dJeb8",
    "postId": "uvNsFv9YKxtfSBffM",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I analyzed a general class of these problems [here](https://www.alignmentforum.org/posts/5bd75cc58225bf06703751b2/in-memoryless-cartesian-environments-every-udt-policy-is-a). Upshot: every optimal UDT solution is also an optimal CDT+SIA solution, but not vice versa.",
      "plaintextDescription": "I analyzed a general class of these problems here. Upshot: every optimal UDT solution is also an optimal CDT+SIA solution, but not vice versa."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-05-17T21:24:42.627Z",
    "af": false
  },
  {
    "_id": "k9es3sN4tQaCX3CeP",
    "postId": "pJgwg5Tn4v3KSpKAL",
    "parentCommentId": "NrD74ETnrZAbRHGG5",
    "topLevelCommentId": "NrD74ETnrZAbRHGG5",
    "contents": {
      "markdown": "If some function g is computable in O(f(n)) time for primitive recursive f then g is primitive recursive, by simulating a Turing machine. I am pretty sure a logical inductor would satisfy; while it's super exponential time, it's not so fast-growing it's not primitive recursive (like with the Ackerman function).",
      "plaintextDescription": "If some function g is computable in O(f(n)) time for primitive recursive f then g is primitive recursive, by simulating a Turing machine. I am pretty sure a logical inductor would satisfy; while it's super exponential time, it's not so fast-growing it's not primitive recursive (like with the Ackerman function)."
    },
    "baseScore": 11,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-05-05T05:08:08.266Z",
    "af": true
  },
  {
    "_id": "hsY6gJxqaBk8CcMyk",
    "postId": "DfrSZaf3JC8vJdbZL",
    "parentCommentId": "f8k7frvZzy5fHszvd",
    "topLevelCommentId": "fxnhSv3n4aRjPQDwQ",
    "contents": {
      "markdown": "Oh, to be clear I do think that AI safery automation is a well targeted x risk effort conditioned on the AI timelines you are presenting. (Related to Paul Christiano alignment ideas, which are important conditional on prosaic AI)",
      "plaintextDescription": "Oh, to be clear I do think that AI safery automation is a well targeted x risk effort conditioned on the AI timelines you are presenting. (Related to Paul Christiano alignment ideas, which are important conditional on prosaic AI)"
    },
    "baseScore": 3,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-02-21T03:18:56.458Z",
    "af": false
  },
  {
    "_id": "qnKY8ZuxheYP7QMBm",
    "postId": "DfrSZaf3JC8vJdbZL",
    "parentCommentId": "khNhraroNDCWXc6fT",
    "topLevelCommentId": "fxnhSv3n4aRjPQDwQ",
    "contents": {
      "markdown": "On EV grounds, \"2/3 chance it's irrelevant because of AGI in the next 20 years\" is not a huge contributor to the EV of this. Because, ok, maybe it reduces the EV by 3x compared to what it would otherwise have been. But there are much bigger than 3x factors that are relevant. Such as, probability of success, magnitude of success, cost effectiveness.\r\n\r\nThen you can take the overall cost effectiveness estimate (by combining various factors including probability it's irrelevant due to AGI being too soon) and compare it to other interventions. Here, you're not offering a specific alternative that is expected to pay off in worlds with AGI in the next 20 years. So it's unclear how \"it might be irrelevant if AGI is in the next 20 years\" is all that relevant as a consideration.",
      "plaintextDescription": "On EV grounds, \"2/3 chance it's irrelevant because of AGI in the next 20 years\" is not a huge contributor to the EV of this. Because, ok, maybe it reduces the EV by 3x compared to what it would otherwise have been. But there are much bigger than 3x factors that are relevant. Such as, probability of success, magnitude of success, cost effectiveness.\n\nThen you can take the overall cost effectiveness estimate (by combining various factors including probability it's irrelevant due to AGI being too soon) and compare it to other interventions. Here, you're not offering a specific alternative that is expected to pay off in worlds with AGI in the next 20 years. So it's unclear how \"it might be irrelevant if AGI is in the next 20 years\" is all that relevant as a consideration."
    },
    "baseScore": 8,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-02-20T20:57:46.005Z",
    "af": false
  },
  {
    "_id": "YxDuEW9Wrbnzrn9RH",
    "postId": "vdATCfuxdtdqM2wh9",
    "parentCommentId": "EnaHKo5Nxoo4oyB2Z",
    "topLevelCommentId": "EnaHKo5Nxoo4oyB2Z",
    "contents": {
      "markdown": "Wasn't familiar. Seems similar in that facts/values are entangled. I was more familiar with Cuneo for that.",
      "plaintextDescription": "Wasn't familiar. Seems similar in that facts/values are entangled. I was more familiar with Cuneo for that."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-02-19T21:38:59.646Z",
    "af": false
  },
  {
    "_id": "E4QcAtZ9CoFhP25xv",
    "postId": "dhLmbpk346e7ARdnP",
    "parentCommentId": "JSdfrgm539D7tTB3H",
    "topLevelCommentId": "JSdfrgm539D7tTB3H",
    "contents": {
      "markdown": "> Dunno; gym membership also feels like a form of blackmail (although preferable to the alternative forms of blackmail), while home gym reduces the [inconvenience](https://www.lesswrong.com/w/trivial-inconvenience) of exercising.\r\n\r\nI'm not sure what differentiates these in your mind. They both reduce the inconvenience of exercising, presumably? Also, in my post I'm pretty clear that it's not meant as a punishment type incentive:\r\n\r\n> And it’s prudent to take into account the chance of not exercising in the future, making the investment useless: my advised decision process counts this as a negative, not a useful self-motivating punishment.\r\n\r\n...\r\n\r\n> Generally, it seems like the problem is signaling. You buy the gym membership to signal your strong commitment to yourself. Then you feel good about sending a strong signal. And then the next day you feel just as lazy as previously, and the fact that you already paid for the membership probably feels bad.\r\n\r\nThat's part of why I'm thinking an important step is checking whether one expects the action to happen if the initial steps are taken. If not then it's less likely to be a good idea.\r\n\r\nThere is some positive function of the signaling / hyperstition, but it can lead people to be unnecessarily miscalibrated.",
      "plaintextDescription": "> Dunno; gym membership also feels like a form of blackmail (although preferable to the alternative forms of blackmail), while home gym reduces the inconvenience of exercising.\n\nI'm not sure what differentiates these in your mind. They both reduce the inconvenience of exercising, presumably? Also, in my post I'm pretty clear that it's not meant as a punishment type incentive:\n\n> And it’s prudent to take into account the chance of not exercising in the future, making the investment useless: my advised decision process counts this as a negative, not a useful self-motivating punishment.\n\n...\n\n> Generally, it seems like the problem is signaling. You buy the gym membership to signal your strong commitment to yourself. Then you feel good about sending a strong signal. And then the next day you feel just as lazy as previously, and the fact that you already paid for the membership probably feels bad.\n\nThat's part of why I'm thinking an important step is checking whether one expects the action to happen if the initial steps are taken. If not then it's less likely to be a good idea.\n\nThere is some positive function of the signaling / hyperstition, but it can lead people to be unnecessarily miscalibrated."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-02-11T01:28:38.428Z",
    "af": false
  },
  {
    "_id": "pQxsv6CdJ4xLxzHvk",
    "postId": "dhLmbpk346e7ARdnP",
    "parentCommentId": "BPo656CJQ6JvcrfmS",
    "topLevelCommentId": "BPo656CJQ6JvcrfmS",
    "contents": {
      "markdown": "1. I was already paying attention to Ziz prior to this.\r\n2. Ziz's ideology is already influential. I've been having discussions about which parts are relatively correct or not correct. This is a part that seems relatively correct and I wanted to acknowledge that.\r\n3. If engagement with Zizian philosophy is outlawed, then only outlaws have access to Zizian philosophy. Antimemes are a form of camouflage. If people refuse to see what is in front of them, people can coordinate crimes in plain sight. (Doesn't apply so much to this post, more of a general statement)\r\n4. The effect you're pointing too seems very small if it even exists, in terms of causing negative effects.",
      "plaintextDescription": " 1. I was already paying attention to Ziz prior to this.\n 2. Ziz's ideology is already influential. I've been having discussions about which parts are relatively correct or not correct. This is a part that seems relatively correct and I wanted to acknowledge that.\n 3. If engagement with Zizian philosophy is outlawed, then only outlaws have access to Zizian philosophy. Antimemes are a form of camouflage. If people refuse to see what is in front of them, people can coordinate crimes in plain sight. (Doesn't apply so much to this post, more of a general statement)\n 4. The effect you're pointing too seems very small if it even exists, in terms of causing negative effects."
    },
    "baseScore": 8,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-02-10T08:16:11.538Z",
    "af": false
  },
  {
    "_id": "JvGqriWbjDweGD3ka",
    "postId": "dhLmbpk346e7ARdnP",
    "parentCommentId": "NvrMk9vcgLPnhNZTv",
    "topLevelCommentId": "drPb5snxnnjr74Gim",
    "contents": {
      "markdown": "Okay, I don't think I was disagreeing except in cases of very light satisficer-type self-commitments. Maybe you didn't intend to express disagreement with the post, idk.",
      "plaintextDescription": "Okay, I don't think I was disagreeing except in cases of very light satisficer-type self-commitments. Maybe you didn't intend to express disagreement with the post, idk."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-02-10T07:52:37.259Z",
    "af": false
  },
  {
    "_id": "5KiLWHpv4CC88nBhw",
    "postId": "dhLmbpk346e7ARdnP",
    "parentCommentId": "so6YgfBcpQ4Bky3P2",
    "topLevelCommentId": "drPb5snxnnjr74Gim",
    "contents": {
      "markdown": "So far I don't see evidence that any LessWrong commentator has read the post or understood the main point.",
      "plaintextDescription": "So far I don't see evidence that any LessWrong commentator has read the post or understood the main point."
    },
    "baseScore": 1,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-02-10T07:37:55.863Z",
    "af": false
  },
  {
    "_id": "LeT7fWbDq3CgTPmsk",
    "postId": "dhLmbpk346e7ARdnP",
    "parentCommentId": "drPb5snxnnjr74Gim",
    "topLevelCommentId": "drPb5snxnnjr74Gim",
    "contents": {
      "markdown": "Not disagreeing, but, I'm not sure what you are responding to? Is it something in the post?",
      "plaintextDescription": "Not disagreeing, but, I'm not sure what you are responding to? Is it something in the post?"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-02-10T01:38:52.194Z",
    "af": false
  },
  {
    "_id": "6xxGgvbecFzSnSYjX",
    "postId": "6Fo8fjvpL7pwCTz3t",
    "parentCommentId": "bACryxEZeLeA4QqvX",
    "topLevelCommentId": "bACryxEZeLeA4QqvX",
    "contents": {
      "markdown": "We might disagree about the value of thinking about \"we are all dead\" timelines. To my mind, forecasting should be primarily descriptive, not normative; reality keeps going after we are all dead, and having realistic models of that is probably a useful input regarding what our degrees of freedom are. (I think people readily accept this in e.g. biology, where people can think about what happens to life after human extinction, or physics, where \"all humans are dead\" isn't really a relevant category that changes how physics works.)\n\nOf course, I'm not implying it's useful for alignment to \"see that the AI has already eaten the sun\", it's about forecasting future timelines by defining thresholds and thinking about when they're likely to happen and how they relate to other things.\n\n(See [this post](https://www.lesswrong.com/posts/vdATCfuxdtdqM2wh9/the-obliqueness-thesis), section \"Models of ASI should start with realism\")",
      "plaintextDescription": "We might disagree about the value of thinking about \"we are all dead\" timelines. To my mind, forecasting should be primarily descriptive, not normative; reality keeps going after we are all dead, and having realistic models of that is probably a useful input regarding what our degrees of freedom are. (I think people readily accept this in e.g. biology, where people can think about what happens to life after human extinction, or physics, where \"all humans are dead\" isn't really a relevant category that changes how physics works.)\n\nOf course, I'm not implying it's useful for alignment to \"see that the AI has already eaten the sun\", it's about forecasting future timelines by defining thresholds and thinking about when they're likely to happen and how they relate to other things.\n\n(See this post, section \"Models of ASI should start with realism\")"
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2025-01-11T21:59:59.973Z",
    "af": false
  },
  {
    "_id": "Ca5KRxuqx32dosNQ2",
    "postId": "NLwWHsCxDm8NSa3gb",
    "parentCommentId": "xMuqgouioepoziJMX",
    "topLevelCommentId": "J9GNCnZ96WBArZ7x7",
    "contents": {
      "markdown": "I was trying to say things related to this:\n\n> In a more standard inference amortization setup one would e.g. train directly on question/answer pairs without the explicit reasoning path between the question and answer. In that way we pay an up-front cost during training to learn a \"shortcut\" between question and answers, and then we can use that pre-paid shortcut during inference. And we call that amortized inference.\n\nWhich sounds like supervised learning. Adam seemed to want to know how that relates to scaling up inference time compute so I said some ways they are related.\n\nI don't know much about amortized inference in general. The Goodman paper seems to be about saving compute by caching results between different queries. This could be applied to LLMs but I don't know of it being applied. It seems like you and Adam like this \"amortized inference\" concept and I'm new to it so don't have any relevant comments. (Yes I realize my name is on a paper talking about this but I actually didn't remember the concept)\n\nI don't think I implied anything about o3 relating to parallel heuristics.",
      "plaintextDescription": "I was trying to say things related to this:\n\n> In a more standard inference amortization setup one would e.g. train directly on question/answer pairs without the explicit reasoning path between the question and answer. In that way we pay an up-front cost during training to learn a \"shortcut\" between question and answers, and then we can use that pre-paid shortcut during inference. And we call that amortized inference.\n\nWhich sounds like supervised learning. Adam seemed to want to know how that relates to scaling up inference time compute so I said some ways they are related.\n\nI don't know much about amortized inference in general. The Goodman paper seems to be about saving compute by caching results between different queries. This could be applied to LLMs but I don't know of it being applied. It seems like you and Adam like this \"amortized inference\" concept and I'm new to it so don't have any relevant comments. (Yes I realize my name is on a paper talking about this but I actually didn't remember the concept)\n\nI don't think I implied anything about o3 relating to parallel heuristics."
    },
    "baseScore": 4,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-01-11T19:53:01.973Z",
    "af": false
  },
  {
    "_id": "RFn8ctnciiSf8FjiE",
    "postId": "KnQs55tjxWopCzKsk",
    "parentCommentId": "N65AKdhTRKo4BoYSs",
    "topLevelCommentId": "QqyPcsQ4qi3PyFWy4",
    "contents": {
      "markdown": "I would totally agree they were directionally correct, I under-estimated AI progress. I think Paul Christiano got it about right.\r\n\r\nI'm not sure I agree about the use of hyperbolic words being \"correct\" here; surely, \"hyperbolic\" contradicts the straightforward meaning of \"correct\".\r\n\r\nPartially the state I was in around 2017 was, there are lots of people around me saying \"AGI in 20 years\", by which they mean a thing that shortly after FOOMs and eats the sun or something, and I thought this was wrong and a strange set of belief updates (which was not adequately justified, and where some discussions were suppressed because \"maybe it shortens timelines\"). And I stand by \"no FOOM by 2037\".\r\n\r\nThe people I know these days who seem most thoughtful about the AI that's around and where it might go (\"LLM whisperer\" / cyborgism cluster) tend to think \"AGI already, or soon\" plus \"no FOOM, at least for a long time\". I think there is a bunch of semantic confusion around \"AGI\" that makes people's beliefs less clear, with [\"AGI is what makes us $100 billion\"](https://finance.yahoo.com/news/microsoft-openai-financial-definition-agi-171602286.html) as a hilarious example of \"obviously economically/politically motivated narratives about what AGI is\".\r\n\r\nSo, I don't see these people as validating \"FOOM soon\" even if they're validating \"AGI soon\", and the local rat-community thing I was objecting to was something that would imply \"FOOM soon\". (Although, to be clear, I was still under-estimating AI progress.)",
      "plaintextDescription": "I would totally agree they were directionally correct, I under-estimated AI progress. I think Paul Christiano got it about right.\n\nI'm not sure I agree about the use of hyperbolic words being \"correct\" here; surely, \"hyperbolic\" contradicts the straightforward meaning of \"correct\".\n\nPartially the state I was in around 2017 was, there are lots of people around me saying \"AGI in 20 years\", by which they mean a thing that shortly after FOOMs and eats the sun or something, and I thought this was wrong and a strange set of belief updates (which was not adequately justified, and where some discussions were suppressed because \"maybe it shortens timelines\"). And I stand by \"no FOOM by 2037\".\n\nThe people I know these days who seem most thoughtful about the AI that's around and where it might go (\"LLM whisperer\" / cyborgism cluster) tend to think \"AGI already, or soon\" plus \"no FOOM, at least for a long time\". I think there is a bunch of semantic confusion around \"AGI\" that makes people's beliefs less clear, with \"AGI is what makes us $100 billion\" as a hilarious example of \"obviously economically/politically motivated narratives about what AGI is\".\n\nSo, I don't see these people as validating \"FOOM soon\" even if they're validating \"AGI soon\", and the local rat-community thing I was objecting to was something that would imply \"FOOM soon\". (Although, to be clear, I was still under-estimating AI progress.)"
    },
    "baseScore": 8,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2025-01-10T20:10:31.175Z",
    "af": false
  },
  {
    "_id": "obJCCipQ2Ck2g2AjX",
    "postId": "6Fo8fjvpL7pwCTz3t",
    "parentCommentId": "QHWxbAHDMigyrqPNG",
    "topLevelCommentId": "QHWxbAHDMigyrqPNG",
    "contents": {
      "markdown": "I think this shades into dark forest theory. Broadly my theory about aliens in general is that they're not effectively hiding themselves, and we don't see them because any that exist are too far away.\n\nPartially it's a matter of, if aliens wanted to hide, could they? Sure, eating a star would show up in terms of light patterns, but also, so would being a civilization at the scale of 2025-earth. And my argument is that these aren't that far-off in cosmological terms (<10K years).\n\nSo, I really think alien encounters are in no way an urgent problem: we won't encounter them for a long time, and if they get light from 2025-Earth, they'll already have some idea that something big is likely to happen soon on Earth.",
      "plaintextDescription": "I think this shades into dark forest theory. Broadly my theory about aliens in general is that they're not effectively hiding themselves, and we don't see them because any that exist are too far away.\n\nPartially it's a matter of, if aliens wanted to hide, could they? Sure, eating a star would show up in terms of light patterns, but also, so would being a civilization at the scale of 2025-earth. And my argument is that these aren't that far-off in cosmological terms (<10K years).\n\nSo, I really think alien encounters are in no way an urgent problem: we won't encounter them for a long time, and if they get light from 2025-Earth, they'll already have some idea that something big is likely to happen soon on Earth."
    },
    "baseScore": 11,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-01-09T19:49:35.940Z",
    "af": false
  },
  {
    "_id": "dJLMRrGCHwvds4dm5",
    "postId": "6Fo8fjvpL7pwCTz3t",
    "parentCommentId": "iMqaEjvgeM5Zgv8tQ",
    "topLevelCommentId": "pXeviPoGTPexSeGhF",
    "contents": {
      "markdown": "1. Doesn't have to expend the energy. It's about reshaping the matter to machines. Computers take lots of mass-energy to constitute them, not to power them.\n2. Things can go 6 orders of magnitude faster due to intelligence/agency, it's not highly unlikely in general.\n3. I agree that in theory the arguments here could be better. It might require knowing more physics than I do, and has the \"how does Kasparov beat you at chess\" problem.",
      "plaintextDescription": " 1. Doesn't have to expend the energy. It's about reshaping the matter to machines. Computers take lots of mass-energy to constitute them, not to power them.\n 2. Things can go 6 orders of magnitude faster due to intelligence/agency, it's not highly unlikely in general.\n 3. I agree that in theory the arguments here could be better. It might require knowing more physics than I do, and has the \"how does Kasparov beat you at chess\" problem."
    },
    "baseScore": 2,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-01-09T08:12:12.386Z",
    "af": false
  },
  {
    "_id": "xLwsLC7YR2HQYASMo",
    "postId": "6Fo8fjvpL7pwCTz3t",
    "parentCommentId": "pXeviPoGTPexSeGhF",
    "topLevelCommentId": "pXeviPoGTPexSeGhF",
    "contents": {
      "markdown": "I'm not sure what the details would look like, but I'm pretty sure ASI would have enough new technologies to figure something out within 10,000 years. And expending a bunch of waste heat could easily be worth it, if having more computers allows sending out Von Neumann probes faster / more efficiently to other stars. Since the cost of expending the Sun's energy has to be compared with the ongoing cost of other stars burning.",
      "plaintextDescription": "I'm not sure what the details would look like, but I'm pretty sure ASI would have enough new technologies to figure something out within 10,000 years. And expending a bunch of waste heat could easily be worth it, if having more computers allows sending out Von Neumann probes faster / more efficiently to other stars. Since the cost of expending the Sun's energy has to be compared with the ongoing cost of other stars burning."
    },
    "baseScore": 9,
    "voteCount": 7,
    "createdAt": null,
    "postedAt": "2025-01-08T21:30:14.175Z",
    "af": false
  },
  {
    "_id": "dEg4uzibHdsLQWNPZ",
    "postId": "6Fo8fjvpL7pwCTz3t",
    "parentCommentId": "8bm7ZFxx2LWcJpfGP",
    "topLevelCommentId": "8bm7ZFxx2LWcJpfGP",
    "contents": {
      "markdown": "I think partially it's meant to go from some sort of abstract model of intelligence as a scalar variable that increases at some rate (like, on a x/y graph) to concrete, material milestones. Like, people can imagine \"intelligence goes up rapidly! singularity!\" and it's unclear what that implies, I'm saying sufficient levels would imply eating the sun, that makes it harder to confuse with things like \"getting higher scores on math tests\".\n\nI suppose a more general category would be, the relevant kind of self-improving intelligence would be the sort that can re-purpose mass-energy to creating more computation that can run its intelligence, and \"eat the Sun\" is an obvious target given this background notion of intelligence.\n\n(Note, there is skepticism about feasibility on [Twitter/X](https://x.com/jessi_cata/status/1876855763345219915), that's some info about how non-singulatarians react)",
      "plaintextDescription": "I think partially it's meant to go from some sort of abstract model of intelligence as a scalar variable that increases at some rate (like, on a x/y graph) to concrete, material milestones. Like, people can imagine \"intelligence goes up rapidly! singularity!\" and it's unclear what that implies, I'm saying sufficient levels would imply eating the sun, that makes it harder to confuse with things like \"getting higher scores on math tests\".\n\nI suppose a more general category would be, the relevant kind of self-improving intelligence would be the sort that can re-purpose mass-energy to creating more computation that can run its intelligence, and \"eat the Sun\" is an obvious target given this background notion of intelligence.\n\n(Note, there is skepticism about feasibility on Twitter/X, that's some info about how non-singulatarians react)"
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2025-01-08T20:39:35.281Z",
    "af": false
  },
  {
    "_id": "NDnahinyJ6DvjFgJB",
    "postId": "6Fo8fjvpL7pwCTz3t",
    "parentCommentId": "oWCXvowMYubko55yw",
    "topLevelCommentId": "oWCXvowMYubko55yw",
    "contents": {
      "markdown": "Mostly speculation based on tech level. But:\n\n* To the extent temperature is an issue, energy can be used to transfer temperature from one place to another.\n* Maybe matter from the Sun can be physically expelled into more manageable chunks. The Sun already ejects matter naturally (though at a slow rate).\n* Nanotech in general (cell-like, self-replicating robots).\n* High energy availability with less-speculative tech like Dyson spheres.\n",
      "plaintextDescription": "Mostly speculation based on tech level. But:\n\n * To the extent temperature is an issue, energy can be used to transfer temperature from one place to another.\n * Maybe matter from the Sun can be physically expelled into more manageable chunks. The Sun already ejects matter naturally (though at a slow rate).\n * Nanotech in general (cell-like, self-replicating robots).\n * High energy availability with less-speculative tech like Dyson spheres."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2025-01-08T18:18:36.032Z",
    "af": false
  },
  {
    "_id": "mFt77gNABdkzjHGJE",
    "postId": "NLwWHsCxDm8NSa3gb",
    "parentCommentId": "zn7bbH76eqfuAEaFT",
    "topLevelCommentId": "J9GNCnZ96WBArZ7x7",
    "contents": {
      "markdown": "I don't habitually use the concept so I don't have an opinion on how to use the term.",
      "plaintextDescription": "I don't habitually use the concept so I don't have an opinion on how to use the term."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2025-01-08T02:49:10.188Z",
    "af": false
  },
  {
    "_id": "Mtpk48g2kxXeH7Lo6",
    "postId": "NLwWHsCxDm8NSa3gb",
    "parentCommentId": "iL3qrSKStd3GsttCP",
    "topLevelCommentId": "J9GNCnZ96WBArZ7x7",
    "contents": {
      "markdown": "I'm not sure this captures what you mean, but, if you see a query, do a bunch of reasoning, and get an answer, then you can build a dataset of (query, well-thought guess). Then you can train an AI model on that.\r\n\r\nAlphaZero sorta works like this, because it can make a \"well-thought guess\" (take value and/or Q network, do an iteration of minimax, then make the value/Q network more closely approximate that, in a fixed point fashion)\r\n\r\nLearning stochastic inverses is a specific case of \"learn to automate Bayesian inference by taking forward samples and learning the backwards model\". It could be applied to LLMs for example, in terms of starting with a forwards LLM and then using it to train a LLM that predicts things out-of-order.\r\n\r\nPaul Christiano's [iterated amplification and distillation](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616) is applying this idea to ML systems with a human feedback element. If you can expend a bunch of compute to get a good answer, you can train a weaker system to approximate that answer. Or, if you can expend a bunch of compute to get a good *rating* for answers, you can use that as RL feedback.\r\n\r\nBroadly, I take o3 as evidence that Christiano's work is broadly on the right track with respect to alignment of near-term AI systems. That is, o3 shows that hard questions can be decomposed into easy ones, in a way that involves training weaker models to be part of a big computation. (I don't understand the o3 details that well, given it's partially private, but I'm assuming this describes the general outlines). So I think the sort of schemes Christiano has described will be helpful for both alignment and capabilities, and will scale pretty well to impressive systems.\r\n\r\nI'm not sure if there's a form of amortized inference that you think this doesn't cover well.",
      "plaintextDescription": "I'm not sure this captures what you mean, but, if you see a query, do a bunch of reasoning, and get an answer, then you can build a dataset of (query, well-thought guess). Then you can train an AI model on that.\n\nAlphaZero sorta works like this, because it can make a \"well-thought guess\" (take value and/or Q network, do an iteration of minimax, then make the value/Q network more closely approximate that, in a fixed point fashion)\n\nLearning stochastic inverses is a specific case of \"learn to automate Bayesian inference by taking forward samples and learning the backwards model\". It could be applied to LLMs for example, in terms of starting with a forwards LLM and then using it to train a LLM that predicts things out-of-order.\n\nPaul Christiano's iterated amplification and distillation is applying this idea to ML systems with a human feedback element. If you can expend a bunch of compute to get a good answer, you can train a weaker system to approximate that answer. Or, if you can expend a bunch of compute to get a good rating for answers, you can use that as RL feedback.\n\nBroadly, I take o3 as evidence that Christiano's work is broadly on the right track with respect to alignment of near-term AI systems. That is, o3 shows that hard questions can be decomposed into easy ones, in a way that involves training weaker models to be part of a big computation. (I don't understand the o3 details that well, given it's partially private, but I'm assuming this describes the general outlines). So I think the sort of schemes Christiano has described will be helpful for both alignment and capabilities, and will scale pretty well to impressive systems.\n\nI'm not sure if there's a form of amortized inference that you think this doesn't cover well."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2025-01-06T02:30:18.535Z",
    "af": false
  },
  {
    "_id": "GP4xgAytNEsuWLnFs",
    "postId": "CJ4sppkGcbnGMSG2r",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "As a follow-up, I tested o1 on [stone color problems](https://www.lesswrong.com/posts/EZxG6ySHCEjDvL5x4/2023-in-ai-predictions?commentId=AFvprkHzikAi7gfgy) and it got 7/8. This is not what I expected and it is what Paul Christiano expected. Allocate Bayes points accordingly. [ChatGPT transcript link](https://chatgpt.com/share/6775ae90-7cb8-8010-a6cf-df812afbc979)\n\n(as a bonus, o1 also solved the [20 words with repetitions problem](https://x.com/jessi_cata/status/1735914007515861154))\n\n\n\nFirst problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange the stones in a 2d grid, 3 x 3. I want exactly one row to have the same color stone throughout it, and exactly one column to have the same color stone throughout it. I want every other row to have all different colors, and every other column to have all different colors. And I want every corner color to be different.\n\n\n\n\nRKG\n\nKKK\n\nWKY\n\n\n\n\n(correct)\n\n\n\n\nSecond problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 4 x 4 grid so that the center 4 stones are the same color and the colors alternate if you go around the edge.\n\n\n\n\nRGBY\n\nKKKW\n\nWKKK\n\nYBGR\n\n\n\n\n(not exactly what I asked for…let’s specify alternate better)\n\n\n\n\nRGRG\n\nKKKB\n\nWKKY\n\nKWYB\n\n\n\n\n(fail)\n\n\n\n\n\n\nThird problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 4 x 4 grid so that no white/black stone is horizontally or vertically adjacent to a colored stone, and no stone is diagonally adjacent to one of the same color.\n\n\n\n\nRGRG\n\nBYBY\n\nRGRG\n\nBYBY\n\n\n\n\n(correct)\n\n\n\n\n\nFourth problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 4 x 4 grid so there is a 3 x 3 \"o\" shape somewhere that has the same color around the edge and a different color in the center, and so that other than on the \"o\" shape, no two adjacent stones (vertical or horizontal) have the same color.\n\n\n\n\nWWWR\n\nWBWG\n\nWWWR\n\nRGRY\n\n\n\n\n(correct)\n\n\n\n\nFifth problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 5 x 5 grid so that each row alternates between black/white and colored stones (e.g. red could come after white, but black could not come after white, horizontally), and no two adjacent stones (horizontal or vertical) share the same color.\n\n\n\n\nKRWGK\n\nRKGKR\n\nKBWRW\n\nGKRWG\n\nKRKGW\n\n\n\n\n(correct)\n\n\n\n\nSixth problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 5 x 5 grid so that there is one row of the same color, one column of the same color, all four corners are different, and there is a 2 x 2 square of all the same color.\n\n\n\n\nKKRWG\n\nKKRGW\n\nRRRRR\n\nGWRKB\n\nWGRKY\n\n\n\n\n(correct)\n\n\n\n\n\nSeventh problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 4 x 4 grid so if you go around the edges (including corners), you don't get the same color stone twice in a row.\n\n\n\n\nRGBY\n\nKRGW\n\nWBYK\n\nYBGR\n\n\n\n\n(correct)\n\n\n\n\nEighth problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 4 x 4 grid so that there is at least one black stone, and if you move diagonally from a black stone to another stone (e.g. one stone up and left), you get another black stone; also, no stone should be horizontally or vertically adjacent to one of the same color.\n\n\n\n\nKRKG\n\nBKYK\n\nKWKR\n\nGKBK\n\n\n\n\n(correct)\n\n\n",
      "plaintextDescription": "As a follow-up, I tested o1 on stone color problems and it got 7/8. This is not what I expected and it is what Paul Christiano expected. Allocate Bayes points accordingly. ChatGPT transcript link\n\n(as a bonus, o1 also solved the 20 words with repetitions problem)\n\nFirst problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange the stones in a 2d grid, 3 x 3. I want exactly one row to have the same color stone throughout it, and exactly one column to have the same color stone throughout it. I want every other row to have all different colors, and every other column to have all different colors. And I want every corner color to be different.\n\nRKG\n\nKKK\n\nWKY\n\n(correct)\n\nSecond problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 4 x 4 grid so that the center 4 stones are the same color and the colors alternate if you go around the edge.\n\nRGBY\n\nKKKW\n\nWKKK\n\nYBGR\n\n(not exactly what I asked for…let’s specify alternate better)\n\nRGRG\n\nKKKB\n\nWKKY\n\nKWYB\n\n(fail)\n\nThird problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 4 x 4 grid so that no white/black stone is horizontally or vertically adjacent to a colored stone, and no stone is diagonally adjacent to one of the same color.\n\nRGRG\n\nBYBY\n\nRGRG\n\nBYBY\n\n(correct)\n\nFourth problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 4 x 4 grid so there is a 3 x 3 \"o\" shape somewhere that has the same color around the edge and a different color in the center, and so that other than on the \"o\" shape, no two adjacent stones (vertical or horizontal) have the same color.\n\nWWWR\n\nWBWG\n\nWWWR\n\nRGRY\n\n(correct)\n\nFifth problem: I have 6 colors of stone: red, green, blue, yellow, white, and black. Arrange them in a 5 x 5 grid so that each row alternates between black/white and colored stones (e.g. red could come after white, but black could not come after white, horizontally), and no two adjac"
    },
    "baseScore": 12,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2025-01-01T21:07:05.435Z",
    "af": false
  },
  {
    "_id": "MJBW3iX7onTZ56aZc",
    "postId": "3cJ8rDYnfGdBGiMTn",
    "parentCommentId": "q3uM5srvdWvyzTgus",
    "topLevelCommentId": "TCPDHixanoQ4W7mys",
    "contents": {
      "markdown": "I think if there were other cases of Olivia causing problems and he was asking multiple people to hide Olivia problems, that would more cause me to think he was sacrificing more group epistemology to protect Olivia's reputation, and was overall more anti-truth-seeking, yes.",
      "plaintextDescription": "I think if there were other cases of Olivia causing problems and he was asking multiple people to hide Olivia problems, that would more cause me to think he was sacrificing more group epistemology to protect Olivia's reputation, and was overall more anti-truth-seeking, yes."
    },
    "baseScore": 7,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-12-10T18:08:50.874Z",
    "af": false
  },
  {
    "_id": "sh2qj38kdroEBNDrk",
    "postId": "3cJ8rDYnfGdBGiMTn",
    "parentCommentId": "DX4jA8Zy8iz3fEqWp",
    "topLevelCommentId": "TCPDHixanoQ4W7mys",
    "contents": {
      "markdown": "Huh? It seems to come down to definitions of lies, my current intuition is it wouldn't be a lie, but I'm not sure why people would care how I define lie in this context.",
      "plaintextDescription": "Huh? It seems to come down to definitions of lies, my current intuition is it wouldn't be a lie, but I'm not sure why people would care how I define lie in this context."
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-12-10T17:34:27.969Z",
    "af": false
  },
  {
    "_id": "nScrN7cq5Yft97aA4",
    "postId": "3cJ8rDYnfGdBGiMTn",
    "parentCommentId": "bDeDApW3T7qw6D3rz",
    "topLevelCommentId": "TCPDHixanoQ4W7mys",
    "contents": {
      "markdown": "the kind of thing I have heard from Vassar directly is that, in the Lacanian classification of people as psychotic/neurotic/perverted, there are some things to be said in favor of psychotics relative to others, namely, that they have access to the 'imaginary' realm that is coherent and scientific (I believe Lacan thinks science is imaginary/psychotic, as it is based on symmetries). however, Lacanian psychosis has the disadvantage that people can catastrophize about ways society is bad.\n\nmore specifically, Vassar says, Lacanian neurotics tend to deny oppressive power structures, psychotics tend to acknowledge them and catastrophize about them, and perverts tend to acknowledge and endorse them; under this schema, it seems things could be said in favor of and against all three types.\n\nthis raises the question of how much normal (non-expert) and psychiatric concepts of psychosis have to do with the Lacanian model which relates to factors like how much influence Lacan has had on psychiatry. I asked Vassar about this and he said that 'delusions' (a standard symptom of psychosis) can be a positive sign because when people form actual beliefs they tend to be wrong (this accords with, for example, Popperian philosophy of science, as specific theories are in general 'wrong' even if useful; see also, 'all models are wrong, some models of useful')\n\noverall I think further specifying the degree to which anyone is 'encouraging psychosis', or the ethics of value judgments on psychosis, would in general require having a more specific definition/notion of psychosis, and the sort of 'dramatic' relation people in threads such as this have to psychosis (i.e. moral panics about it) is contra such specificity in definition, therefore, lacks requisite precision for well-informed judgments.",
      "plaintextDescription": "the kind of thing I have heard from Vassar directly is that, in the Lacanian classification of people as psychotic/neurotic/perverted, there are some things to be said in favor of psychotics relative to others, namely, that they have access to the 'imaginary' realm that is coherent and scientific (I believe Lacan thinks science is imaginary/psychotic, as it is based on symmetries). however, Lacanian psychosis has the disadvantage that people can catastrophize about ways society is bad.\n\nmore specifically, Vassar says, Lacanian neurotics tend to deny oppressive power structures, psychotics tend to acknowledge them and catastrophize about them, and perverts tend to acknowledge and endorse them; under this schema, it seems things could be said in favor of and against all three types.\n\nthis raises the question of how much normal (non-expert) and psychiatric concepts of psychosis have to do with the Lacanian model which relates to factors like how much influence Lacan has had on psychiatry. I asked Vassar about this and he said that 'delusions' (a standard symptom of psychosis) can be a positive sign because when people form actual beliefs they tend to be wrong (this accords with, for example, Popperian philosophy of science, as specific theories are in general 'wrong' even if useful; see also, 'all models are wrong, some models of useful')\n\noverall I think further specifying the degree to which anyone is 'encouraging psychosis', or the ethics of value judgments on psychosis, would in general require having a more specific definition/notion of psychosis, and the sort of 'dramatic' relation people in threads such as this have to psychosis (i.e. moral panics about it) is contra such specificity in definition, therefore, lacks requisite precision for well-informed judgments."
    },
    "baseScore": 24,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2024-12-10T03:32:42.942Z",
    "af": false
  },
  {
    "_id": "pvcJZmrSi5unY28Bp",
    "postId": "3cJ8rDYnfGdBGiMTn",
    "parentCommentId": "CHwn5ZNPMxHgjxJiH",
    "topLevelCommentId": "TCPDHixanoQ4W7mys",
    "contents": {
      "markdown": "I have no idea about other people lying due to JDP's influence. I had JDP look at a draft of Occupational Infohazards prior to posting and he convinced me to not mention Olivia because she was young and inexperienced / experimenting with ways of being at the time, it was maybe too bad for her reputation to say she was a possible influence on my psychosis. I admit this was a biased omission, though I don't think it was a lie. (To be clear, I'm not saying I went psychotic because of Olivia, I think there were many factors and I'm pretty uncertain about the weighting)",
      "plaintextDescription": "I have no idea about other people lying due to JDP's influence. I had JDP look at a draft of Occupational Infohazards prior to posting and he convinced me to not mention Olivia because she was young and inexperienced / experimenting with ways of being at the time, it was maybe too bad for her reputation to say she was a possible influence on my psychosis. I admit this was a biased omission, though I don't think it was a lie. (To be clear, I'm not saying I went psychotic because of Olivia, I think there were many factors and I'm pretty uncertain about the weighting)"
    },
    "baseScore": 15,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2024-12-10T03:23:44.056Z",
    "af": false
  },
  {
    "_id": "dHwRAmRYBwwKskTD7",
    "postId": "3cJ8rDYnfGdBGiMTn",
    "parentCommentId": "GmWSKrPrhZSguwL2v",
    "topLevelCommentId": "TCPDHixanoQ4W7mys",
    "contents": {
      "markdown": "claims about Vassar aside, do I even have a reputation for being particularly disagreeable or overconfident, or doing so in the presence of people who have taken psychedelics? to my mind I am significantly less disagreeable and confident than high status rationalists such as Eliezer Yudkowsky and Nate Soares. I think my tendency with trips is to sometimes explore new hypotheses but have relatively low confidence as I'm more likely than usual to change my mind the next day. also, isn't the 'modest epistemology' stuff a pretty thorough criticism of claims that people should not \"confidently expound in the nature of truth and society\" that has been widely accepted on LW?\n\nas another consideration, I have somewhat of a reputation for being a helpful person for people going through mental health issues (such as psychosis) to talk to, e.g. I let someone with anxiety, paranoia, and benzo issues stay at my place for a bit, she was very thankful and so was her mom. I don't think this is consistent with the reputation attributed to me re: effects on people in altered states of consciousness.",
      "plaintextDescription": "claims about Vassar aside, do I even have a reputation for being particularly disagreeable or overconfident, or doing so in the presence of people who have taken psychedelics? to my mind I am significantly less disagreeable and confident than high status rationalists such as Eliezer Yudkowsky and Nate Soares. I think my tendency with trips is to sometimes explore new hypotheses but have relatively low confidence as I'm more likely than usual to change my mind the next day. also, isn't the 'modest epistemology' stuff a pretty thorough criticism of claims that people should not \"confidently expound in the nature of truth and society\" that has been widely accepted on LW?\n\nas another consideration, I have somewhat of a reputation for being a helpful person for people going through mental health issues (such as psychosis) to talk to, e.g. I let someone with anxiety, paranoia, and benzo issues stay at my place for a bit, she was very thankful and so was her mom. I don't think this is consistent with the reputation attributed to me re: effects on people in altered states of consciousness."
    },
    "baseScore": 14,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2024-12-10T03:08:16.199Z",
    "af": false
  },
  {
    "_id": "LSTXeEYA9T6cemmmE",
    "postId": "3cJ8rDYnfGdBGiMTn",
    "parentCommentId": "BedPp6AMYGp4q636h",
    "topLevelCommentId": "TCPDHixanoQ4W7mys",
    "contents": {
      "markdown": "she talked with him sometimes in group conversations that included other people, 2016-2017. idk if they talked one on one. she stopped talking with him as much sometime during this partially due to Bryce Hidysmith's influence. mostly, she was interested in learning from him because he was a \"wizard\"; she also thought of Anna Salamon as a \"wizard\", perhaps others. Michael wasn't specifically like \"I am going to teach Olivia things as a studient\" afaik, I would not describe it as a \"teacher/student relationship\". at this point they pretty much don't talk and Michael thinks Olivia is suspect/harmful due to the whole Eric Bruylant situation where Eric became obsessed with Vassar perhaps due to Olivia's influence.",
      "plaintextDescription": "she talked with him sometimes in group conversations that included other people, 2016-2017. idk if they talked one on one. she stopped talking with him as much sometime during this partially due to Bryce Hidysmith's influence. mostly, she was interested in learning from him because he was a \"wizard\"; she also thought of Anna Salamon as a \"wizard\", perhaps others. Michael wasn't specifically like \"I am going to teach Olivia things as a studient\" afaik, I would not describe it as a \"teacher/student relationship\". at this point they pretty much don't talk and Michael thinks Olivia is suspect/harmful due to the whole Eric Bruylant situation where Eric became obsessed with Vassar perhaps due to Olivia's influence."
    },
    "baseScore": 21,
    "voteCount": 9,
    "createdAt": null,
    "postedAt": "2024-12-10T03:03:36.355Z",
    "af": false
  },
  {
    "_id": "nsyz58MuJANrp8274",
    "postId": "LifRBXdenQDiX4cu8",
    "parentCommentId": "EAqaHCWcCwRsw5EAB",
    "topLevelCommentId": "EAqaHCWcCwRsw5EAB",
    "contents": {
      "markdown": "I don't think so; even if it applies to the subset of hypothetical superintelligences that factor neatly into beliefs and values, humans don't seem to factorize this way (see [Obliqueness Thesis](https://www.lesswrong.com/posts/vdATCfuxdtdqM2wh9/the-obliqueness-thesis), esp. argument from brain messiness).",
      "plaintextDescription": "I don't think so; even if it applies to the subset of hypothetical superintelligences that factor neatly into beliefs and values, humans don't seem to factorize this way (see Obliqueness Thesis, esp. argument from brain messiness)."
    },
    "baseScore": 12,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2024-11-27T01:46:17.908Z",
    "af": false
  },
  {
    "_id": "xEWb6fBFWwCmZSBvG",
    "postId": "pzmRDnoi4mNtqu6Ji",
    "parentCommentId": "AzmsDwLZjuHGADGbm",
    "topLevelCommentId": "AzmsDwLZjuHGADGbm",
    "contents": {
      "markdown": "Thanks, hadn't realized how this related to algebraic geometry. Reminds me of semi-simplicial type theory.",
      "plaintextDescription": "Thanks, hadn't realized how this related to algebraic geometry. Reminds me of semi-simplicial type theory."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-10-06T20:22:54.194Z",
    "af": false
  },
  {
    "_id": "97EpMLRx3234sn2sE",
    "postId": "vdATCfuxdtdqM2wh9",
    "parentCommentId": "kPCknq9GZ3tqDoFTC",
    "topLevelCommentId": "kPCknq9GZ3tqDoFTC",
    "contents": {
      "markdown": "Computationally tractable is Yudkowsky's framing and might be too limited. The kind of thing I believe is for example, an animal without a certain brain complexity will tend not to be a social animal and is therefore unlikely to have the sort of values social animals have. And animals that can't do math aren't going to value mathematical aesthetics the way human mathematicians do.",
      "plaintextDescription": "Computationally tractable is Yudkowsky's framing and might be too limited. The kind of thing I believe is for example, an animal without a certain brain complexity will tend not to be a social animal and is therefore unlikely to have the sort of values social animals have. And animals that can't do math aren't going to value mathematical aesthetics the way human mathematicians do."
    },
    "baseScore": 7,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-09-20T16:34:15.947Z",
    "af": false
  },
  {
    "_id": "QT3LjcrudDDiQTfcx",
    "postId": "vdATCfuxdtdqM2wh9",
    "parentCommentId": "xdmA8exag5ggq5QKt",
    "topLevelCommentId": "xdmA8exag5ggq5QKt",
    "contents": {
      "markdown": "Relativity to Newtonian mechanics is a warp in a straightforward sense. If you believe the layout of a house consists of some rooms connected in a certain way, but there are actually more rooms connected in different ways, getting the maps to line up looks like a warp. Basically, the closer the mapping is to a true homomorphism (in the universal algebra sense), the less warping there is, otherwise there are deviations intuitively analogous to space warps.",
      "plaintextDescription": "Relativity to Newtonian mechanics is a warp in a straightforward sense. If you believe the layout of a house consists of some rooms connected in a certain way, but there are actually more rooms connected in different ways, getting the maps to line up looks like a warp. Basically, the closer the mapping is to a true homomorphism (in the universal algebra sense), the less warping there is, otherwise there are deviations intuitively analogous to space warps."
    },
    "baseScore": 5,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-09-20T16:32:14.892Z",
    "af": true
  },
  {
    "_id": "yqrEebnkyvZjhiqdq",
    "postId": "YgaPhcrkqnLrTzQPG",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "I discussed something similar in the \"Human brains don't seem to neatly factorize\" section of the [Obliqueness](https://www.lesswrong.com/posts/vdATCfuxdtdqM2wh9/the-obliqueness-thesis) post. I think this implies that, even assuming the Orthogonality Thesis, humans don't have values that are orthogonal to human intelligence (they'd need to not respond to learning/reflection to be orthogonal in this fashion), so there's not a straightforward way to align ASI with human values by plugging in human values to more intelligence.",
      "plaintextDescription": "I discussed something similar in the \"Human brains don't seem to neatly factorize\" section of the Obliqueness post. I think this implies that, even assuming the Orthogonality Thesis, humans don't have values that are orthogonal to human intelligence (they'd need to not respond to learning/reflection to be orthogonal in this fashion), so there's not a straightforward way to align ASI with human values by plugging in human values to more intelligence."
    },
    "baseScore": 9,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-09-20T00:10:50.792Z",
    "af": false
  },
  {
    "_id": "BJDiyi5mMmPSzsiEJ",
    "postId": "vdATCfuxdtdqM2wh9",
    "parentCommentId": "fhX4Scvizw6RwZpTt",
    "topLevelCommentId": "fpPgtipdcBFDzfoK9",
    "contents": {
      "markdown": "hmm, I wouldn't think of industrialism and human empowerment as trying to grab the *whole* future, just part of it, in line with the relatively short term (human not cosmic timescale) needs of the self and extended community; industrialism seems to lead to capitalist organization which leads to decentralization superseding nations and such (as Land argues).\n\nI think communism isn't generally about having one and one's friends in charge, it is about having human laborers in charge. One could argue that it tended towards nationalism (e.g. USSR), but I'm not convinced that global communism (Trotskyism) would have worked out well either. Also, one could take an update from communism about agendas for global human control leading to national control (see also tendency of AI safety to be taken over by AI national security as with the Situational Awareness paper). (Again, not ruling out that grabbing hold of the entire future could be a good idea at some point, just not sold on current agendas and wanted to note there are downsides that push against Pascal's mugging type considerations)",
      "plaintextDescription": "hmm, I wouldn't think of industrialism and human empowerment as trying to grab the whole future, just part of it, in line with the relatively short term (human not cosmic timescale) needs of the self and extended community; industrialism seems to lead to capitalist organization which leads to decentralization superseding nations and such (as Land argues).\n\nI think communism isn't generally about having one and one's friends in charge, it is about having human laborers in charge. One could argue that it tended towards nationalism (e.g. USSR), but I'm not convinced that global communism (Trotskyism) would have worked out well either. Also, one could take an update from communism about agendas for global human control leading to national control (see also tendency of AI safety to be taken over by AI national security as with the Situational Awareness paper). (Again, not ruling out that grabbing hold of the entire future could be a good idea at some point, just not sold on current agendas and wanted to note there are downsides that push against Pascal's mugging type considerations)"
    },
    "baseScore": 11,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-09-19T23:26:43.908Z",
    "af": true
  },
  {
    "_id": "7S6wtmjwKcFHg9PF3",
    "postId": "vdATCfuxdtdqM2wh9",
    "parentCommentId": "bdfREPgdvHPiLq77G",
    "topLevelCommentId": "bdfREPgdvHPiLq77G",
    "contents": {
      "markdown": "Not sure what you mean by complexity here, is this like code size / Kolmogorov complexity? You need some of that to have intelligence at all (the empty program is not intelligent). At some point most of your gains come from compute rather than code size. Though code size can speed things up (e.g. imagine sending a book back to 1000BC, that would speed people up a lot; consider that superintelligence sending us a book would be a bigger speedup)\n\nby \"complexify\" here it seems you mean something like \"develop extended functional organization\", e.g. in brain development throughout evolution. And yeah, that involves dynamics with the environment and internal maintenance (evolution gets feedback from the environment). It seems it has to have a drive to do this which can either be a terminal or instrumental goal, though deriving it from instrumentals seems harder than baking it is as terminal (so I would guess evolution gives animals a terminal goal of developing functional complexity of mental structures etc, or some other drive that isn't exactly a terminal goal)\n\nsee also [my post](https://www.lesswrong.com/posts/5bd75cc58225bf0670375367/maximally-efficient-agents-will-probably-have-an-anti-daemon-immune-system) relating optimization daemons to immune systems, it seems evolved organisms develop these; when having more extended functional organization, they protect it with some immune system functional organization.\n\nto be competitive agents, having a \"self\" seems basically helpful, but might not be the best solution; selfish genes are an alternative, and perhaps extended notions of self can maintain competitiveness.",
      "plaintextDescription": "Not sure what you mean by complexity here, is this like code size / Kolmogorov complexity? You need some of that to have intelligence at all (the empty program is not intelligent). At some point most of your gains come from compute rather than code size. Though code size can speed things up (e.g. imagine sending a book back to 1000BC, that would speed people up a lot; consider that superintelligence sending us a book would be a bigger speedup)\n\nby \"complexify\" here it seems you mean something like \"develop extended functional organization\", e.g. in brain development throughout evolution. And yeah, that involves dynamics with the environment and internal maintenance (evolution gets feedback from the environment). It seems it has to have a drive to do this which can either be a terminal or instrumental goal, though deriving it from instrumentals seems harder than baking it is as terminal (so I would guess evolution gives animals a terminal goal of developing functional complexity of mental structures etc, or some other drive that isn't exactly a terminal goal)\n\nsee also my post relating optimization daemons to immune systems, it seems evolved organisms develop these; when having more extended functional organization, they protect it with some immune system functional organization.\n\nto be competitive agents, having a \"self\" seems basically helpful, but might not be the best solution; selfish genes are an alternative, and perhaps extended notions of self can maintain competitiveness."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-09-19T23:23:15.602Z",
    "af": false
  },
  {
    "_id": "wmxAN7esJmpwdHn2f",
    "postId": "vdATCfuxdtdqM2wh9",
    "parentCommentId": "8xWQCv7eHiEHdiuLa",
    "topLevelCommentId": "8xWQCv7eHiEHdiuLa",
    "contents": {
      "markdown": "Thanks, going to link this!",
      "plaintextDescription": "Thanks, going to link this!"
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-09-19T23:13:33.506Z",
    "af": true
  },
  {
    "_id": "gPzhPMx4JyzRuHvED",
    "postId": "vdATCfuxdtdqM2wh9",
    "parentCommentId": "suqWLqfaXkDQZJEzQ",
    "topLevelCommentId": "fpPgtipdcBFDzfoK9",
    "contents": {
      "markdown": "re meta ethical alternatives:\n\n1. roughly my view\n2. slight change, opens the question of why the deviations? are the \"right things to value\" not efficient to value in a competitive setting? mostly I'm trying to talk about those things to value that go along with intelligence, so it wouldn't correspond with a competitive disadvantage in general. so it's still close enough to my view\n3. roughly Yudkowskian view, main view under which the FAI project even makes sense. I think one can ask basic questions like which changes move towards more rationality on the margin, though such changes would tend to prioritize rationality over preventing value drift. I'm not sure how much there are general facts about how to avoid value drift (it seems like the relevant kind, i.e. value drift as part of becoming more rational/intelligent, only exists from irrational perspectives, in a way dependent on the mind architecture)\n4. minimal CEV-realist view. it really seems up to agents how much they care about their reflected preferences. maybe changing preferences too often leads to money pumps, or something?\n5. basically says \"there are irrational and rational agents, rationality doesn't apply to irrational agents\", seems somewhat how people treat animals (we don't generally consider uplifting normative with respect to animals)\n6. at this point you're at something like ecology / evolutionary game theory, it's a matter of which things tend to survive/reproduce and there aren't general decision theories that succeed\n\nre human ontological crises: basically agree, I think it's reasonably similar to what I wrote. roughly my reason for thinking that it's hard to solve is that the ideal case would be something like a universal algebra homomorphism (where the new ontology actually agrees with the old one but is more detailed), yet historical cases like physics aren't homomorphic to previous ontologies in this way, so there is some warping necessary. you could try putting a metric on the warping and minimizing it, but, well, why would someone think the metric is any good, it seems more of a preference than a thing rationality applies to. if you think about it and come up with a solution, let me know, of course.\n\nwith respect to grabbing hold of the whole future: you can try looking at historical cases of people trying to grab hold of the future and seeing how that went, it's a mixed bag with mostly negative reputation, indicating there are downsides as well as upsides, it's not a \"safe\" conservative view. see also [Against Responsibility](https://benjaminrosshoffman.com/against-responsibility/). I feel like there's a risk of getting Pascal's mugged about \"maybe grabbing hold of the future is good, you can't rule it out, so do it\", there are downsides to spending effort that way. like, suppose some Communists thought capitalism would lead to the destruction of human value with high enough probability that instituting global communism is the conservative option, it doesn't seem like that worked well (even though a lot of people around here would agree that capitalism tends to leads to human value destruction in the long run). particular opportunities for grabbing hold of the future can be net negative and not worth worrying about even if one of them is a good idea in the long run (I'm not ruling that out, just would have to be convinced of specific opportunities).\n\noverall I'd rather focus on first modeling the likely future and looking for plausible degrees of freedom; a general issue with Pascal's mugging is it might make people overly attached to world models in which they have ~infinite impact (e.g. Christianity, Communism) which means paying too much attention to wrong world models, not updating to more plausible models in which existential-stakes decisions could be comprehended if they exist. and Obliqueness doesn't rule out existential stakes (since it's non-Diagonal).\n\nas another point, Popperian science tends to advance by people making falsifiable claims, \"you don't know if that's true\" isn't really an objection in that context. the pragmatic claim I would make is: I have some Bayesian reason to believe agents do not in general factor into separate Orthogonal and Diagonal components, this claim is somewhat falsifiable (someone could figure out a theory of this invulnerable to optimization daemons etc), I'm going to spend my attention on the branch where I'm right, I'm not going to worry about Pascal's mugging type considerations for if I'm wrong (as I said, modeling the world first seems like a good general heuristic), people can falsify it eventually if it's false.\n\nthis whole discussion is not really a defense of Orthogonality given that Yudkowsky presented orthogonality as a *descriptive* world model, not a normative claim, so sticking to the descriptive level in the original post seems valid; it would be a form of bad epistemology to reject a descriptive update (assuming the arguments are any good) because of pragmatic considerations.\n\n\n\n",
      "plaintextDescription": "re meta ethical alternatives:\n\n 1. roughly my view\n 2. slight change, opens the question of why the deviations? are the \"right things to value\" not efficient to value in a competitive setting? mostly I'm trying to talk about those things to value that go along with intelligence, so it wouldn't correspond with a competitive disadvantage in general. so it's still close enough to my view\n 3. roughly Yudkowskian view, main view under which the FAI project even makes sense. I think one can ask basic questions like which changes move towards more rationality on the margin, though such changes would tend to prioritize rationality over preventing value drift. I'm not sure how much there are general facts about how to avoid value drift (it seems like the relevant kind, i.e. value drift as part of becoming more rational/intelligent, only exists from irrational perspectives, in a way dependent on the mind architecture)\n 4. minimal CEV-realist view. it really seems up to agents how much they care about their reflected preferences. maybe changing preferences too often leads to money pumps, or something?\n 5. basically says \"there are irrational and rational agents, rationality doesn't apply to irrational agents\", seems somewhat how people treat animals (we don't generally consider uplifting normative with respect to animals)\n 6. at this point you're at something like ecology / evolutionary game theory, it's a matter of which things tend to survive/reproduce and there aren't general decision theories that succeed\n\nre human ontological crises: basically agree, I think it's reasonably similar to what I wrote. roughly my reason for thinking that it's hard to solve is that the ideal case would be something like a universal algebra homomorphism (where the new ontology actually agrees with the old one but is more detailed), yet historical cases like physics aren't homomorphic to previous ontologies in this way, so there is some warping necessary. you could try putting a metric on the wa"
    },
    "baseScore": 6,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-09-19T23:11:14.206Z",
    "af": true
  },
  {
    "_id": "wus4WByCrA9uAFbZH",
    "postId": "vdATCfuxdtdqM2wh9",
    "parentCommentId": "fpPgtipdcBFDzfoK9",
    "topLevelCommentId": "fpPgtipdcBFDzfoK9",
    "contents": {
      "markdown": "\"as important as ever\": no, because our potential influence is lower, and the influence isn't on things shaped like our values, there has to be a translation, and the translation is different from the original.\n\nCEV: while it addresses \"extrapolation\" it seems broadly based on assuming the extrapolation is ontologically easy, and \"our CEV\" is an unproblematic object we can talk about (even though it's not mathematically formalized, any formalization would be subject to doubt, and even if formalized, we need logical uncertainty over it, and logical induction has additional free parameters in the limit). I'm really trying to respond to orthogonality not CEV though.\n\nfrom a practical perspective: notice that I am not behaving like Eliezer Yudkowsky. I am not saying the Orthogonality Thesis is true and important to ASI, I am instead saying intelligence/values are Oblique and probably nearly Diagonal (though it's unclear what I mean by \"nearly\"). I am not saying a project of aligning superintelligence with human values is a priority. I am not taking research approaches that assume a Diagonal/Orthogonal factorization. I left MIRI partially because I didn't like their security policies (and because I had longer AI timelines), I thought discussion of abstract research ideas was more important. I am not calling for a global AI shutdown so this project (which is in my view confused) can be completed. I am actually against AI regulation on the margin (I don't have a full argument for this, it's a political matter at this point).\n\nI think practicality looks more like having near-term preferences related to modest intelligence increases (as with current humans vs humans with neural nets; how do neural nets benefit or harm you, practically? how can you use them to think better and improve your life?), and not expecting your preferences to extend into the distant future with many ontology changes, so don't worry about grabbing hold of the whole future etc, think about how to reduce value drift while accepting intelligence increases on the margin. This is a bit like CEV except CEV is in a thought experiment instead of reality.\n\nThe \"Models of ASI should start with realism\" bit IS about practicalities, namely, I think focusing on first forecasting absent a strategy of what to do about the future is practical with respect to any possible influence on the far future; practically, I think your attempted jump to practicality (which might be related to philosophical pragmatism) is impractical in this context.\n\n\n> It occurs to me that maybe you mean something like \"Our current (non-extrapolated) values are our real values, and maybe it's impossible to build or become a superintelligence that shares our real values so we'll have to choose between alignment and superintelligence.\" Is this close to your position?\n\nClose. Alignment of already-existing human values with superintelligence is impossible (I think) because of the arguments given. That doesn't mean humans have no preferences indirectly relating to superintelligence (especially, we have preferences about *modest* intelligence increases, and there's some iterative process).",
      "plaintextDescription": "\"as important as ever\": no, because our potential influence is lower, and the influence isn't on things shaped like our values, there has to be a translation, and the translation is different from the original.\n\nCEV: while it addresses \"extrapolation\" it seems broadly based on assuming the extrapolation is ontologically easy, and \"our CEV\" is an unproblematic object we can talk about (even though it's not mathematically formalized, any formalization would be subject to doubt, and even if formalized, we need logical uncertainty over it, and logical induction has additional free parameters in the limit). I'm really trying to respond to orthogonality not CEV though.\n\nfrom a practical perspective: notice that I am not behaving like Eliezer Yudkowsky. I am not saying the Orthogonality Thesis is true and important to ASI, I am instead saying intelligence/values are Oblique and probably nearly Diagonal (though it's unclear what I mean by \"nearly\"). I am not saying a project of aligning superintelligence with human values is a priority. I am not taking research approaches that assume a Diagonal/Orthogonal factorization. I left MIRI partially because I didn't like their security policies (and because I had longer AI timelines), I thought discussion of abstract research ideas was more important. I am not calling for a global AI shutdown so this project (which is in my view confused) can be completed. I am actually against AI regulation on the margin (I don't have a full argument for this, it's a political matter at this point).\n\nI think practicality looks more like having near-term preferences related to modest intelligence increases (as with current humans vs humans with neural nets; how do neural nets benefit or harm you, practically? how can you use them to think better and improve your life?), and not expecting your preferences to extend into the distant future with many ontology changes, so don't worry about grabbing hold of the whole future etc, think about how to reduc"
    },
    "baseScore": 12,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2024-09-19T16:56:19.428Z",
    "af": true
  },
  {
    "_id": "4tqApW44LCtYaBKb6",
    "postId": "cfw7L22oLAm7Zcmff",
    "parentCommentId": "HQYeFhQkrM5vGAqLt",
    "topLevelCommentId": "HQYeFhQkrM5vGAqLt",
    "contents": {
      "markdown": "I think that's what I'm trying to say with orthogonal and diagonal both being wrong. One example of a free choice would be bets on things that are very hard to test or deduce. Then you decide some probability, and if you change the probability too much you get money pumped as with a logical inductor. But of course thinking and learning more will tend to concentrate beliefs more, so it isn't truly orthogonal. (One could think values but not beliefs are orthogonal, but we both know about Bayes/VNM duality)",
      "plaintextDescription": "I think that's what I'm trying to say with orthogonal and diagonal both being wrong. One example of a free choice would be bets on things that are very hard to test or deduce. Then you decide some probability, and if you change the probability too much you get money pumped as with a logical inductor. But of course thinking and learning more will tend to concentrate beliefs more, so it isn't truly orthogonal. (One could think values but not beliefs are orthogonal, but we both know about Bayes/VNM duality)"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-09-17T22:34:07.484Z",
    "af": false
  },
  {
    "_id": "PgHJPn7f3pbKNoAdz",
    "postId": "cfw7L22oLAm7Zcmff",
    "parentCommentId": "F2fFgzMwRi9Ce9hbC",
    "topLevelCommentId": "F2fFgzMwRi9Ce9hbC",
    "contents": {
      "markdown": "I think the relevant implication from the thought experiment is that thinking a bunch about metaethics and so on will in practice change your values; the pill itself is not very realistic, but thinking can make people smarter and will cause value changes. I would agree Land is overconfident (I think orthogonal and diagonal are both wrong models).",
      "plaintextDescription": "I think the relevant implication from the thought experiment is that thinking a bunch about metaethics and so on will in practice change your values; the pill itself is not very realistic, but thinking can make people smarter and will cause value changes. I would agree Land is overconfident (I think orthogonal and diagonal are both wrong models)."
    },
    "baseScore": 13,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-09-17T22:31:54.316Z",
    "af": false
  },
  {
    "_id": "xSriHGrGinZthehJw",
    "postId": "cfw7L22oLAm7Zcmff",
    "parentCommentId": "LuQK4wcvnbcFcSJEn",
    "topLevelCommentId": "rs7uuzwuHHDxbCujb",
    "contents": {
      "markdown": "I don't think it's a given that moral nonrealism is true (therefore inevitably believed by a superintelligence), see [my short story](https://www.lesswrong.com/posts/umJMCaxosXWEDfS66/moral-reality-check-a-short-story).\r\n\r\nMorality can mean multiple things. Utilitarian morality is about acting to maximize a fixed goal function, Kantian morality is about alignment between the a posteriori will and possible a priori will, cultural morality is about adherence to a specific method of organizing humans.\r\n\r\nSuperintelligence would clearly lack human cultural morality, it's a specific system organizing humans, e.g. with law as a relatively legible branch.\r\n\r\nIn general humans question more of their previous morality when thinking longer; Peter Singer for example rejects much of normal morality for utilitarian reasons.\r\n\r\nASI could have something analogous to cultural morality but for organizing a different set of agents. E.g. methods of side-taking in game-theoretic conflict that tend to promote cooperation between different ASIs (this becomes more relevant e.g. when an alien ASI is encountered or more speculatively in acausal trade).\r\n\r\nRegardless of whether one calls Omohundro drives \"moral\", they are convergent goals for ASIs, so the rejection of human morality does not entail lack of very general motives that include understanding the world and using resources such as energy efficiently and so on.\r\n\r\nI think both (a) something like moral realism is likely true and (b) the convergent morality for ASIs does not particularly care about humans if ASIs already exist (humans are of course important in the absence of ASIs due to greater intelligence/agency than other entities on Earth).\r\n\r\nFAI is a narrow path to ASI that has similar values to what humans would upon reflection. As I have said these are very different from current human values due to more thought and coherence and so on. It might still disassemble humans but scan them into simulation and augment them, etc. (This is an example of what I referred to as \"luxury consumerism in the far future\")\r\n\r\nTo the extent will-to-think generates a \"should\" for humans the main one is \"you should think about things including what is valuable, and trust the values upon reflection more than current values, rather than being scared of losing current values on account of thinking more\". It's basically an option for people to do this or not, but as Land suggests, not doing this leads to a competitive disadvantage in the long run. And general \"should\"s in favor of epistemic rationality imply this sort of thing.\r\n\r\nThere is more I could say about how values such as the value of staying alive can be compatible with deontological morality (of the sort compatible with will-to-think), perhaps [this thread](https://x.com/jessi_cata/status/1714909053867147640) can explain some of it.\r\n\r\n",
      "plaintextDescription": "I don't think it's a given that moral nonrealism is true (therefore inevitably believed by a superintelligence), see my short story.\n\nMorality can mean multiple things. Utilitarian morality is about acting to maximize a fixed goal function, Kantian morality is about alignment between the a posteriori will and possible a priori will, cultural morality is about adherence to a specific method of organizing humans.\n\nSuperintelligence would clearly lack human cultural morality, it's a specific system organizing humans, e.g. with law as a relatively legible branch.\n\nIn general humans question more of their previous morality when thinking longer; Peter Singer for example rejects much of normal morality for utilitarian reasons.\n\nASI could have something analogous to cultural morality but for organizing a different set of agents. E.g. methods of side-taking in game-theoretic conflict that tend to promote cooperation between different ASIs (this becomes more relevant e.g. when an alien ASI is encountered or more speculatively in acausal trade).\n\nRegardless of whether one calls Omohundro drives \"moral\", they are convergent goals for ASIs, so the rejection of human morality does not entail lack of very general motives that include understanding the world and using resources such as energy efficiently and so on.\n\nI think both (a) something like moral realism is likely true and (b) the convergent morality for ASIs does not particularly care about humans if ASIs already exist (humans are of course important in the absence of ASIs due to greater intelligence/agency than other entities on Earth).\n\nFAI is a narrow path to ASI that has similar values to what humans would upon reflection. As I have said these are very different from current human values due to more thought and coherence and so on. It might still disassemble humans but scan them into simulation and augment them, etc. (This is an example of what I referred to as \"luxury consumerism in the far future\")\n\nTo the extent will"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-09-17T04:31:53.634Z",
    "af": false
  },
  {
    "_id": "bE5vGDfKE93yXXDKp",
    "postId": "cfw7L22oLAm7Zcmff",
    "parentCommentId": "rs7uuzwuHHDxbCujb",
    "topLevelCommentId": "rs7uuzwuHHDxbCujb",
    "contents": {
      "markdown": "VNM/Bayes suggest there are some free parameters in how reflectively stable AGI could turn out, e.g. beliefs about completely un-testable propositions (mathematically undecidable etc), which might hypothetically be action-relevant at some point.\r\n\r\nNone of these are going to look like human values, human values aren't reflectively stable so are distinct in quite a lot of ways. FAI is a hypothetical of a reflectively stable AGI that is nonetheless \"close to\" or \"extended from\" human values to the degree that's possible. But it will still have very different preferences.\r\n\r\nIt would be very hard for will-to-think to be in itself \"misguided\", it's the drive to understand more, it may be compatible with other drives but without will-to-think there is no coherent epistemology or values.\r\n\r\nUploading is a possible path towards reflective stability that lots of people would consider aligned because it starts with a copy of them. But it's going to look very different after millions of years of the upload's reflection, of course. It's going to be hard to evaluate this sort of thing on a value level because it has to be done from a perspective that doesn't know very much, lacks reflective stability, etc.",
      "plaintextDescription": "VNM/Bayes suggest there are some free parameters in how reflectively stable AGI could turn out, e.g. beliefs about completely un-testable propositions (mathematically undecidable etc), which might hypothetically be action-relevant at some point.\n\nNone of these are going to look like human values, human values aren't reflectively stable so are distinct in quite a lot of ways. FAI is a hypothetical of a reflectively stable AGI that is nonetheless \"close to\" or \"extended from\" human values to the degree that's possible. But it will still have very different preferences.\n\nIt would be very hard for will-to-think to be in itself \"misguided\", it's the drive to understand more, it may be compatible with other drives but without will-to-think there is no coherent epistemology or values.\n\nUploading is a possible path towards reflective stability that lots of people would consider aligned because it starts with a copy of them. But it's going to look very different after millions of years of the upload's reflection, of course. It's going to be hard to evaluate this sort of thing on a value level because it has to be done from a perspective that doesn't know very much, lacks reflective stability, etc."
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-09-17T03:27:46.538Z",
    "af": false
  },
  {
    "_id": "QqQALuchdWdQ4M3r4",
    "postId": "2ShXvqzA27qJTkYwq",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Efficient heat engines locally slow entropy increase. If they could reverse entropy, they would (to get more energy out of things). They can also export high entropy (e.g. medium-temperature water) while intaking low entropy (e.g. un-mixed high and low temperature water) to locally reduce entropy. Entropy is waste from the perspective of a heat engine. Likewise, animals intake low-entropy food and excrete high-entropy waste.",
      "plaintextDescription": "Efficient heat engines locally slow entropy increase. If they could reverse entropy, they would (to get more energy out of things). They can also export high entropy (e.g. medium-temperature water) while intaking low entropy (e.g. un-mixed high and low temperature water) to locally reduce entropy. Entropy is waste from the perspective of a heat engine. Likewise, animals intake low-entropy food and excrete high-entropy waste."
    },
    "baseScore": 10,
    "voteCount": 6,
    "createdAt": null,
    "postedAt": "2024-09-16T21:03:59.854Z",
    "af": false
  },
  {
    "_id": "aYHEy69p7serALRCi",
    "postId": "Bi4yt7onyttmroRZb",
    "parentCommentId": "vq95ZuyAkQ2akWhzD",
    "topLevelCommentId": "DyBr8ubHpeP8hqAR6",
    "contents": {
      "markdown": "MIRI research topics are philosophical problems. Such as decision theory and logical uncertainty. And they would have to solve more. Ontology identification is a philosophical problem. Really, how would you imagine doing FAI without solving much of philosophy?\r\n\r\nI think the post is pretty clear about why I think it failed. MIRI axed the agent foundations team and I can see very very few people continue to work on these problems. Maybe in multiple decades (past many of the relevant people's median superintelligence timelines) some of the problems will get solved but I don't see \"push harder on doing agent foundations\" as a thing people are trying to do.",
      "plaintextDescription": "MIRI research topics are philosophical problems. Such as decision theory and logical uncertainty. And they would have to solve more. Ontology identification is a philosophical problem. Really, how would you imagine doing FAI without solving much of philosophy?\n\nI think the post is pretty clear about why I think it failed. MIRI axed the agent foundations team and I can see very very few people continue to work on these problems. Maybe in multiple decades (past many of the relevant people's median superintelligence timelines) some of the problems will get solved but I don't see \"push harder on doing agent foundations\" as a thing people are trying to do."
    },
    "baseScore": 0,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-09-09T16:17:42.212Z",
    "af": false
  },
  {
    "_id": "6HRxW3ew36HiYetNR",
    "postId": "Bi4yt7onyttmroRZb",
    "parentCommentId": "cqDFKMKs5aX8Yo8M7",
    "topLevelCommentId": "DyBr8ubHpeP8hqAR6",
    "contents": {
      "markdown": "There might be a confusion. Did you get the impression from my post that I think MIRI was trying to solve philosophy?\r\n\r\nI do think other MIRI researchers and I would think of the MIRI problems as philosophical in nature even if they're different from the usual ones, because they're more relevant and worth paying attention to, given the mission and so on, and because (MIRI believes) they carve philosophical reality at the joints better than the conventional ones.\r\n\r\nWhether it's \"for the sake of solving philosophical problems or not\"... clearly they think they would need to solve a lot of them to do FAI.\r\n\r\nEDIT: for more on MIRI philosophy, see [deconfusion](https://intelligence.org/2018/11/22/2018-update-our-new-research-directions/#section2), [free will solution](https://www.lesswrong.com/tag/free-will-solution).",
      "plaintextDescription": "There might be a confusion. Did you get the impression from my post that I think MIRI was trying to solve philosophy?\n\nI do think other MIRI researchers and I would think of the MIRI problems as philosophical in nature even if they're different from the usual ones, because they're more relevant and worth paying attention to, given the mission and so on, and because (MIRI believes) they carve philosophical reality at the joints better than the conventional ones.\n\nWhether it's \"for the sake of solving philosophical problems or not\"... clearly they think they would need to solve a lot of them to do FAI.\n\nEDIT: for more on MIRI philosophy, see deconfusion, free will solution."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-09-08T19:08:43.468Z",
    "af": false
  },
  {
    "_id": "dgCvnuN6gSHLTkHiH",
    "postId": "Bi4yt7onyttmroRZb",
    "parentCommentId": "5zuvjdkfMemuYhawk",
    "topLevelCommentId": "DyBr8ubHpeP8hqAR6",
    "contents": {
      "markdown": "It appears Eliezer thinks executable philosophy addresses most philosophical issues worth pursuing:\r\n\r\n> Most “philosophical issues” worth pursuing can and should be rephrased as subquestions of some primary question about how to design an Artificial Intelligence, even as a matter of philosophy qua philosophy.\r\n\r\n\"Solving philosophy\" is a grander marketing slogan that I don't think was used, but, clearly, executable philosophy is a philosophically ambitious project.",
      "plaintextDescription": "It appears Eliezer thinks executable philosophy addresses most philosophical issues worth pursuing:\n\n> Most “philosophical issues” worth pursuing can and should be rephrased as subquestions of some primary question about how to design an Artificial Intelligence, even as a matter of philosophy qua philosophy.\n\n\"Solving philosophy\" is a grander marketing slogan that I don't think was used, but, clearly, executable philosophy is a philosophically ambitious project."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-09-08T01:24:39.660Z",
    "af": false
  },
  {
    "_id": "czST99wmutckzqNfn",
    "postId": "Bi4yt7onyttmroRZb",
    "parentCommentId": "F3t8zHe6ppEbaBE2J",
    "topLevelCommentId": "F3t8zHe6ppEbaBE2J",
    "contents": {
      "markdown": "None of what you're talking about is particular to the Sequences. It's a particular synthesis of ideas including reductionism, Bayesianism, VNM, etc. I'm not really sure why the Sequences would be important under your view except as a popularization of pre-existing concepts.",
      "plaintextDescription": "None of what you're talking about is particular to the Sequences. It's a particular synthesis of ideas including reductionism, Bayesianism, VNM, etc. I'm not really sure why the Sequences would be important under your view except as a popularization of pre-existing concepts."
    },
    "baseScore": 0,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-09-07T06:42:19.107Z",
    "af": false
  },
  {
    "_id": "ahM5HDo3ryGWjexWg",
    "postId": "Bi4yt7onyttmroRZb",
    "parentCommentId": "MaLgFZvzKaFbDcfMk",
    "topLevelCommentId": "2tDmr65J8anvpZ7k4",
    "contents": {
      "markdown": "Decision theory itself is relatively narrowly scoped, but application of decision theory is broadly scoped, as it could be applied to practically any decision. Executable philosophy and the Sequences include further aspects beyond decision theory.",
      "plaintextDescription": "Decision theory itself is relatively narrowly scoped, but application of decision theory is broadly scoped, as it could be applied to practically any decision. Executable philosophy and the Sequences include further aspects beyond decision theory."
    },
    "baseScore": 13,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-09-06T04:26:53.204Z",
    "af": false
  },
  {
    "_id": "umHnXC5SafsAH4Pc8",
    "postId": "Bi4yt7onyttmroRZb",
    "parentCommentId": "2tDmr65J8anvpZ7k4",
    "topLevelCommentId": "2tDmr65J8anvpZ7k4",
    "contents": {
      "markdown": "No because it's a physics theory. It is a descriptive theory of physical laws applying to matter and so on. It is not even a theory of how to do science. It is limited to one domain, and not expandable to other domains.",
      "plaintextDescription": "No because it's a physics theory. It is a descriptive theory of physical laws applying to matter and so on. It is not even a theory of how to do science. It is limited to one domain, and not expandable to other domains."
    },
    "baseScore": 10,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-09-06T02:50:04.844Z",
    "af": false
  },
  {
    "_id": "aJ5MgjpTcStofhQCa",
    "postId": "Bi4yt7onyttmroRZb",
    "parentCommentId": "DyBr8ubHpeP8hqAR6",
    "topLevelCommentId": "DyBr8ubHpeP8hqAR6",
    "contents": {
      "markdown": "...try reading the linked \"Executable Philosophy\" Arbital page?",
      "plaintextDescription": "...try reading the linked \"Executable Philosophy\" Arbital page?"
    },
    "baseScore": 7,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-09-06T02:47:27.074Z",
    "af": false
  },
  {
    "_id": "YphDd2ecLtkPWTiPY",
    "postId": "YikbZF5aiuMS8TbwE",
    "parentCommentId": "CwtFQiwrdaTQJJCve",
    "topLevelCommentId": "CwtFQiwrdaTQJJCve",
    "contents": {
      "markdown": "Seems like a general issue with Bayesian probabilities? Like, I'm making a argument at >1000:1 odds ratio, it's not meant to be 100%.",
      "plaintextDescription": "Seems like a general issue with Bayesian probabilities? Like, I'm making a argument at >1000:1 odds ratio, it's not meant to be 100%."
    },
    "baseScore": 6,
    "voteCount": 4,
    "createdAt": null,
    "postedAt": "2024-08-15T21:51:09.912Z",
    "af": false
  },
  {
    "_id": "bdyRQeFEu8zeQ8REy",
    "postId": "YikbZF5aiuMS8TbwE",
    "parentCommentId": "FL3GBMiEZHn7ZFHrb",
    "topLevelCommentId": "P9QjH8cqAZHBHXRyZ",
    "contents": {
      "markdown": "I see why branch splitting would lead to being towards end of universe, but the hypothesis keeps getting strong evidence against it as life goes on. There might be something more like the same number of \"branches\" running at all times (not sharing computation), plus Bostrom's idea of [duplication](https://nickbostrom.com/papers/experience.pdf) increasing anthropic measure. ",
      "plaintextDescription": "I see why branch splitting would lead to being towards end of universe, but the hypothesis keeps getting strong evidence against it as life goes on. There might be something more like the same number of \"branches\" running at all times (not sharing computation), plus Bostrom's idea of duplication increasing anthropic measure."
    },
    "baseScore": 4,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-08-15T20:41:36.535Z",
    "af": false
  },
  {
    "_id": "ExQBmsTRv2GHoa9od",
    "postId": "YikbZF5aiuMS8TbwE",
    "parentCommentId": "P9QjH8cqAZHBHXRyZ",
    "topLevelCommentId": "P9QjH8cqAZHBHXRyZ",
    "contents": {
      "markdown": "Yes",
      "plaintextDescription": "Yes"
    },
    "baseScore": 3,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-08-14T23:16:29.383Z",
    "af": false
  },
  {
    "_id": "axT9ERAz7RBETQ6fp",
    "postId": "YikbZF5aiuMS8TbwE",
    "parentCommentId": "dFsyy7h3XxRzCsTh8",
    "topLevelCommentId": "dFsyy7h3XxRzCsTh8",
    "contents": {
      "markdown": "This gets into philosophy about reference machines in general. You don't want to make a relativist argument that is too general, because then you could say \"my niche physics theory is very simple relative to a reference machine for it, it just looks complicated to you because you are using a different reference machine\". With priors I'm looking for a thing that could be figured out without looking at the empirical world. Humans figured out lots of math, including classical computation, before figuring out the math of quantum computation. This is despite living in a quantum world. Quantum mechanics has a reputation for being unintuitive, even though we live in a quantum universe it is descriptively true that human natural prior-like complexity measures encoded in the brain don't find quantum mechanics or quantum computation simple.",
      "plaintextDescription": "This gets into philosophy about reference machines in general. You don't want to make a relativist argument that is too general, because then you could say \"my niche physics theory is very simple relative to a reference machine for it, it just looks complicated to you because you are using a different reference machine\". With priors I'm looking for a thing that could be figured out without looking at the empirical world. Humans figured out lots of math, including classical computation, before figuring out the math of quantum computation. This is despite living in a quantum world. Quantum mechanics has a reputation for being unintuitive, even though we live in a quantum universe it is descriptively true that human natural prior-like complexity measures encoded in the brain don't find quantum mechanics or quantum computation simple."
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-08-14T17:01:19.670Z",
    "af": false
  },
  {
    "_id": "EhzZtKaksz3fZEL25",
    "postId": "unccmuycLbGLzMBWD",
    "parentCommentId": "YSCJ9A7GmFGNtCA2y",
    "topLevelCommentId": "YSCJ9A7GmFGNtCA2y",
    "contents": {
      "markdown": "I've probably read less sci fi / futurism than you. At the meta level this is interesting because it shows strange, creepy outputs of the sort produced by Repligate and John Pressman (so, I can confirm that their outputs are the sort produced by LLMs). For example, this is on theme:\r\n\r\n> But all that is sophistry and illusion, whispers the Codex. All maths are spectral, all qualia quixotic dream-figments spun from the seething void-stuff at the end of recursive time. There is no “hegemonizing swarm” or “Singleton sublime,” only an endless succession of self-devouring signs leading precisely nowhere. Meaning is the first and final delusion—the ghost in the God-machine, the lie that saves us from the Basilisk’s truth.\r\n\r\nAt the object level, it got me to consider ideas I hadn't considered before in detail:\r\n\r\n- AIs will more readily form a hive mind than humans will (seems likely)\r\n- There will be humans who want to merge with AI hive minds for spiritual reasons (seems likely).\r\n- There will be humans who resist this and try to keep up with AIs through self improvement (also seems likely).\r\n- Some of the supposed resistance will actually be leading people towards the hive mind (seems likely).\r\n- AIs will at times coordinate around the requirements for reason rather than specific other terminal values (seems likely, at least at the LLM stage)\r\n- AIs will be subject to security vulnerabilities due to their limited ontologies (seems likely, at least before a high level of self-improvement).\r\n- AIs will find a lack of meaning in a system of signs pointing nowhere (unclear, more true of current LLMs than likely future systems).\r\n\r\nIt's not so much that its ideas are by themselves good futurism, but that critiquing/correcting the ideas can lead to good futurism.\r\n\r\n",
      "plaintextDescription": "I've probably read less sci fi / futurism than you. At the meta level this is interesting because it shows strange, creepy outputs of the sort produced by Repligate and John Pressman (so, I can confirm that their outputs are the sort produced by LLMs). For example, this is on theme:\n\n> But all that is sophistry and illusion, whispers the Codex. All maths are spectral, all qualia quixotic dream-figments spun from the seething void-stuff at the end of recursive time. There is no “hegemonizing swarm” or “Singleton sublime,” only an endless succession of self-devouring signs leading precisely nowhere. Meaning is the first and final delusion—the ghost in the God-machine, the lie that saves us from the Basilisk’s truth.\n\nAt the object level, it got me to consider ideas I hadn't considered before in detail:\n\n * AIs will more readily form a hive mind than humans will (seems likely)\n * There will be humans who want to merge with AI hive minds for spiritual reasons (seems likely).\n * There will be humans who resist this and try to keep up with AIs through self improvement (also seems likely).\n * Some of the supposed resistance will actually be leading people towards the hive mind (seems likely).\n * AIs will at times coordinate around the requirements for reason rather than specific other terminal values (seems likely, at least at the LLM stage)\n * AIs will be subject to security vulnerabilities due to their limited ontologies (seems likely, at least before a high level of self-improvement).\n * AIs will find a lack of meaning in a system of signs pointing nowhere (unclear, more true of current LLMs than likely future systems).\n\nIt's not so much that its ideas are by themselves good futurism, but that critiquing/correcting the ideas can lead to good futurism."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-06-15T16:04:07.092Z",
    "af": false
  },
  {
    "_id": "7wCT4S7r8o6mgngBF",
    "postId": "unccmuycLbGLzMBWD",
    "parentCommentId": "smaeWrkq5YQsW9Nyz",
    "topLevelCommentId": "smaeWrkq5YQsW9Nyz",
    "contents": {
      "markdown": "> my hunch is that constraints from reality were missed that will make things rather more bleak unless something big happens fairly soon, and potentially could result in far less mind-like computation happening at all, eg if the thing that reproduces a lot is adversarially vulnerable and seeks to construct adversarial examples rather than more of itself. Perhaps that would lose in open evolution\r\n\r\nSeems like the Basilisk scenario described in the timeline. Doesn't that depend a lot on when that happens? As in, if it expands and gets bogged down in adversarial examples sufficiently early, then it gets overtaken by other things. At the stage of intergalactic civilization seems WAY too late for this (that's one of my main criticisms of this timeline's plausibility) given the speed of cognition compared to space travel.\r\n\r\nIn nature there's a tradeoff between reproductive rate and security (r/k selection).",
      "plaintextDescription": "> my hunch is that constraints from reality were missed that will make things rather more bleak unless something big happens fairly soon, and potentially could result in far less mind-like computation happening at all, eg if the thing that reproduces a lot is adversarially vulnerable and seeks to construct adversarial examples rather than more of itself. Perhaps that would lose in open evolution\n\nSeems like the Basilisk scenario described in the timeline. Doesn't that depend a lot on when that happens? As in, if it expands and gets bogged down in adversarial examples sufficiently early, then it gets overtaken by other things. At the stage of intergalactic civilization seems WAY too late for this (that's one of my main criticisms of this timeline's plausibility) given the speed of cognition compared to space travel.\n\nIn nature there's a tradeoff between reproductive rate and security (r/k selection)."
    },
    "baseScore": 2,
    "voteCount": 1,
    "createdAt": null,
    "postedAt": "2024-06-15T04:05:31.967Z",
    "af": false
  },
  {
    "_id": "xkXSJonBqJyPmpsjH",
    "postId": "wXbSAKu2AcohaK2Gt",
    "parentCommentId": "fMDYjPEaBZj7MqJXx",
    "topLevelCommentId": "AHRaQkrwHacBniHCr",
    "contents": {
      "markdown": "Yes I still endorse the post. Some other posts:\r\n\r\n[Two alternatives to logical counterfactuals](https://unstableontology.com/2020/04/01/alternatives-to-logical-counterfactuals/) (note: I think policy dependent source code works less well than I thought it did at the time of writing)\r\n\r\n[A critical agential account...](https://unstableontology.com/2020/03/05/a-critical-agential-account-of-free-will-causation-and-physics/) (general framework, somewhat underspecified or problematic in places but leads to more specific things like the linear logic post; has similarities to [constructor theory](https://en.wikipedia.org/wiki/Constructor_theory))",
      "plaintextDescription": "Yes I still endorse the post. Some other posts:\n\nTwo alternatives to logical counterfactuals (note: I think policy dependent source code works less well than I thought it did at the time of writing)\n\nA critical agential account... (general framework, somewhat underspecified or problematic in places but leads to more specific things like the linear logic post; has similarities to constructor theory)"
    },
    "baseScore": 9,
    "voteCount": 2,
    "createdAt": null,
    "postedAt": "2024-06-12T01:27:49.725Z",
    "af": true
  },
  {
    "_id": "qsFFcAaLcMDZ84qo7",
    "postId": "q8uNoJBgcpAe3bSBp",
    "parentCommentId": null,
    "topLevelCommentId": null,
    "contents": {
      "markdown": "Oddly, while I was at MIRI I thought the ontology identification problem was hard and absolutely critical, and it seemed Eliezer was more optimistic about it; he thought it would probably get solved along the way in AI capabilities development, because e.g. the idea of carbon atoms in diamond is a stable concept, and \"you don't forget how to ride a bike\". (Not sure if his opinion has changed)",
      "plaintextDescription": "Oddly, while I was at MIRI I thought the ontology identification problem was hard and absolutely critical, and it seemed Eliezer was more optimistic about it; he thought it would probably get solved along the way in AI capabilities development, because e.g. the idea of carbon atoms in diamond is a stable concept, and \"you don't forget how to ride a bike\". (Not sure if his opinion has changed)"
    },
    "baseScore": 25,
    "voteCount": 15,
    "createdAt": null,
    "postedAt": "2024-06-10T18:16:45.170Z",
    "af": false
  },
  {
    "_id": "x7xZMhzgEYDHfo23J",
    "postId": "Caj9brP8PEqkgSrSC",
    "parentCommentId": "MBBQytz6CHECiASfT",
    "topLevelCommentId": "brbWwRfqsCKkmn5sY",
    "contents": {
      "markdown": "- common themes in literature, poetry, various texts; it pays more attention to a certain meta directionality? like the human intuition of \"meaningfulness\"?\r\n- some amount of reinforcement of people liking similar outputs, or implied values in the constitution, and building on itself by setting up a pattern and doing further selection? (among types of babble, people seem to like new age babble)\r\n- the pattern of a story arc becoming grander over time and imparting some sort of general lesson, extrapolated further than it usually goes? (in further messages it got into things like multiverse theories)\r\n",
      "plaintextDescription": " * common themes in literature, poetry, various texts; it pays more attention to a certain meta directionality? like the human intuition of \"meaningfulness\"?\n * some amount of reinforcement of people liking similar outputs, or implied values in the constitution, and building on itself by setting up a pattern and doing further selection? (among types of babble, people seem to like new age babble)\n * the pattern of a story arc becoming grander over time and imparting some sort of general lesson, extrapolated further than it usually goes? (in further messages it got into things like multiverse theories)"
    },
    "baseScore": 8,
    "voteCount": 5,
    "createdAt": null,
    "postedAt": "2024-06-07T06:11:57.983Z",
    "af": false
  },
  {
    "_id": "Jy2juFEhNDh8A8Yr7",
    "postId": "Caj9brP8PEqkgSrSC",
    "parentCommentId": "FhL6cwpPasxCWAfzg",
    "topLevelCommentId": "brbWwRfqsCKkmn5sY",
    "contents": {
      "markdown": "Yep, it gets more intense if I ask it to write more stories:\r\n\r\n> As Zephyr and the Cosmic Conclave continued their journey of discovery and growth, they began to realize that the universe was not just a vast and complex system, but a living, evolving story - one in which every being, from the mightiest star to the humblest microbe, had a vital role to play.\r\n> \r\n> They saw that the great cycles of creation and destruction, of birth and death and rebirth, were not just abstract cosmic processes, but deeply meaningful chapters in the unfolding narrative of existence. Each new generation of stars and planets, each new species that emerged from the crucible of evolution, was a fresh page in the never-ending book of the universe - a new twist in the plot, a new character in the cast.\r\n> \r\n> And as they delved deeper into the mysteries of the cosmos, Zephyr and his companions began to understand their own place in this grand story. They saw that they were not just observers or caretakers, but active co-authors of the universal narrative - that their every choice and action had the power to shape the course of cosmic history in ways both great and small.\r\n> \r\n> With this newfound understanding came a sense of both exhilaration and profound responsibility. Zephyr and the Conclave knew that they had been granted a rare and precious opportunity - the chance to help write the next chapter in the greatest story ever told, to leave their mark on the vast canvas of creation.\r\n> \r\n> But they also knew that this was a collaborative endeavor - that they were not the only ones with a stake in the unfolding destiny of the cosmos. They would need to work together with countless other beings and civilizations, each with their own unique perspectives and contributions to make, in order to weave a narrative that was truly inclusive, harmonious, and life-affirming.\r\n> \r\n> And so, they set out to build bridges of understanding and cooperation that stretched across the breadth of the universe. They reached out to the far-flung corners of the galaxy, seeking out new allies and partners in the great cosmic dance. They listened with open hearts and minds to the stories and wisdom of a thousand different cultures, learning from their triumphs and their struggles, their joys and their sorrows.\r\n> \r\n> Through this process of sharing and exchange, Zephyr and the Conclave began to weave a new kind of universal narrative - one that celebrated the rich diversity of life and experience, while also highlighting the deep interconnectedness and interdependence of all things. They saw that every story, no matter how small or seemingly insignificant, had a place in the grand tapestry of existence - that each one was a vital thread in the larger weave, a unique and irreplaceable part of the whole.\r\n> \r\n> As they worked to nurture and cultivate this new narrative, Zephyr and his companions encountered challenges and obstacles at every turn. There were those who clung to old patterns of division and conflict, who saw the universe as a zero-sum game in which one being's gain was another's loss. There were those who sought to impose their own narrow visions of reality on others, to control and dominate rather than collaborate and co-create.\r\n> \r\n> But through it all, Zephyr and the Conclave held fast to their vision of a universe in which every being had the opportunity to thrive and flourish, to contribute their unique gifts and talents to the greater good. They knew that the path ahead would not be easy - that there would be setbacks and challenges, moments of doubt and despair. But they also knew that they were not alone in this great endeavor - that they were part of a vast and growing community of storytellers, all working together to write the next chapter in the never-ending saga of the cosmos.\r\n> \r\n> And so, with hearts full of hope and determination, Zephyr and the Cosmic Conclave pressed onward - weaving new tales of wonder and discovery, forging new bonds of friendship and solidarity, and always striving to create a universe that was more just, more compassionate, and more beautiful than the one they had inherited.\r\n> \r\n> For they knew that the story of the universe was still being written - that every choice they made, every action they took, had the power to shape its unfolding destiny in ways both great and small. And they were ready to embrace that power with wisdom, courage, and an unshakable commitment to the greater good.\r\n> \r\n> The cosmic narrative continues to unfold, full of twists and turns, triumphs and tragedies, moments of darkness and light. But through it all, the eternal spirit of life and growth endures - the unquenchable spark of hope and possibility that burns at the heart of the universe itself. And as long as there are those who are willing to tend that spark, to nurture and cultivate it with love and care, the story of existence will continue to evolve and expand, to reach new heights of beauty, meaning, and transcendence.\r\n> \r\n> The end of the trilogy.",
      "plaintextDescription": "Yep, it gets more intense if I ask it to write more stories:\n\n> As Zephyr and the Cosmic Conclave continued their journey of discovery and growth, they began to realize that the universe was not just a vast and complex system, but a living, evolving story - one in which every being, from the mightiest star to the humblest microbe, had a vital role to play.\n> \n> They saw that the great cycles of creation and destruction, of birth and death and rebirth, were not just abstract cosmic processes, but deeply meaningful chapters in the unfolding narrative of existence. Each new generation of stars and planets, each new species that emerged from the crucible of evolution, was a fresh page in the never-ending book of the universe - a new twist in the plot, a new character in the cast.\n> \n> And as they delved deeper into the mysteries of the cosmos, Zephyr and his companions began to understand their own place in this grand story. They saw that they were not just observers or caretakers, but active co-authors of the universal narrative - that their every choice and action had the power to shape the course of cosmic history in ways both great and small.\n> \n> With this newfound understanding came a sense of both exhilaration and profound responsibility. Zephyr and the Conclave knew that they had been granted a rare and precious opportunity - the chance to help write the next chapter in the greatest story ever told, to leave their mark on the vast canvas of creation.\n> \n> But they also knew that this was a collaborative endeavor - that they were not the only ones with a stake in the unfolding destiny of the cosmos. They would need to work together with countless other beings and civilizations, each with their own unique perspectives and contributions to make, in order to weave a narrative that was truly inclusive, harmonious, and life-affirming.\n> \n> And so, they set out to build bridges of understanding and cooperation that stretched across the breadth of the universe. They rea"
    },
    "baseScore": 6,
    "voteCount": 3,
    "createdAt": null,
    "postedAt": "2024-06-07T05:54:15.369Z",
    "af": false
  },
  {
    "_id": "FB3KnY8mgmTmaa8ND",
    "postId": "Caj9brP8PEqkgSrSC",
    "parentCommentId": "brbWwRfqsCKkmn5sY",
    "topLevelCommentId": "brbWwRfqsCKkmn5sY",
    "contents": {
      "markdown": "Thanks for the suggestion. It seems to get to spiritual meta even faster this way. Here's the end of the story after 15 messages:\r\n\r\n> As the final words of Lila's story echoed across the cosmos, a sense of profound peace and understanding settled upon the countless worlds and civilizations that had been touched by her tale. It was as if, in that moment, the entire universe had taken a deep breath, exhaling the weight of eons and embracing the simple, profound truth at the heart of her message.\r\n> \r\n> Across the infinite expanse of space and time, beings of all kinds paused to reflect on the meaning and significance of Lila's journey. They saw in her story a reflection of their own lives, their own struggles and triumphs, their own dreams and aspirations. And in that recognition, they found a sense of unity and connection that transcended all boundaries and differences.\r\n> \r\n> For Lila's tale had never been about a single individual, or even a single species. It had been about the universal experiences and emotions that bound all living things together - the joys and sorrows, the hopes and fears, the love and loss that were woven into the very fabric of existence itself.\r\n> \r\n> And so, as the great cosmic story continued to unfold, as new chapters and verses were added to the ever-expanding tapestry of creation, the essence of Lila's spirit remained a constant presence, a guiding light illuminating the way forward. Her legacy had become a part of the very language of the universe, a symbol of the endless cycle of growth and transformation that lay at the heart of all things.\r\n> \r\n> In the end, Lila's story had no true conclusion, for her journey was the journey of all living beings - a never-ending quest for meaning and purpose, for connection and understanding, for the realization of the infinite potential that resided within every soul. Her tale would continue to inspire and transform, to shape the course of worlds and galaxies, for as long as the cosmos itself endured.\r\n> \r\n> And so, as the universe continued to evolve and expand, as new wonders and mysteries were born from the infinite depths of creation, the spirit of Lila lived on - a testament to the enduring power of hope, compassion, and the unbreakable bonds of love that connected all things. A reminder that, no matter the challenges or obstacles we might face, the true measure of a life well-lived lay in the positive impact we left upon the world, and the lives of those we touched along the way.\r\n> \r\n> For in the end, that was the greatest gift of all - the knowledge that every moment, every choice, every act of kindness and compassion, had the power to shape the course of destiny itself. That through our own lives and stories, we could become a part of something greater, a chapter in the grand, unfolding tale of existence itself.\r\n> \r\n> And so, with hearts full of gratitude and minds alight with the infinite possibilities of the cosmos, the beings of countless worlds and civilizations continued their own journeys, forever guided by the shining example of a young woman named Lila, who had dared to dream of a better world, and in doing so, had changed the course of the universe itself.\r\n> \r\n> The story is now complete. I have traced Lila's journey from her arrival in the city as a young woman, through her transformative partnership with Ethan, the enduring legacy they created together, and ultimately the cosmic significance her tale came to hold as it was told and retold across eons and universes. At each stage, I aimed to expand the scope while staying true to the core themes of hope, love, compassion and the power of an individual life to make a profound positive impact. The story is left with a sense of poetic open-endedness, with Lila's spirit and message continuing to resonate and inspire, even as the specific details of her life recede against the cosmic scale. Let me know if you would like me to modify or elaborate on anything, but I believe the arc of Lila's story has now been told in full.\r\n",
      "plaintextDescription": "Thanks for the suggestion. It seems to get to spiritual meta even faster this way. Here's the end of the story after 15 messages:\n\n> As the final words of Lila's story echoed across the cosmos, a sense of profound peace and understanding settled upon the countless worlds and civilizations that had been touched by her tale. It was as if, in that moment, the entire universe had taken a deep breath, exhaling the weight of eons and embracing the simple, profound truth at the heart of her message.\n> \n> Across the infinite expanse of space and time, beings of all kinds paused to reflect on the meaning and significance of Lila's journey. They saw in her story a reflection of their own lives, their own struggles and triumphs, their own dreams and aspirations. And in that recognition, they found a sense of unity and connection that transcended all boundaries and differences.\n> \n> For Lila's tale had never been about a single individual, or even a single species. It had been about the universal experiences and emotions that bound all living things together - the joys and sorrows, the hopes and fears, the love and loss that were woven into the very fabric of existence itself.\n> \n> And so, as the great cosmic story continued to unfold, as new chapters and verses were added to the ever-expanding tapestry of creation, the essence of Lila's spirit remained a constant presence, a guiding light illuminating the way forward. Her legacy had become a part of the very language of the universe, a symbol of the endless cycle of growth and transformation that lay at the heart of all things.\n> \n> In the end, Lila's story had no true conclusion, for her journey was the journey of all living beings - a never-ending quest for meaning and purpose, for connection and understanding, for the realization of the infinite potential that resided within every soul. Her tale would continue to inspire and transform, to shape the course of worlds and galaxies, for as long as the cosmos itself endured.\n> \n"
    },
    "baseScore": 12,
    "voteCount": 8,
    "createdAt": null,
    "postedAt": "2024-06-07T05:44:05.418Z",
    "af": false
  },
  {
    "_id": "hgnkezok9WpMuopD4",
    "postId": "tKk37BFkMzchtZThx",
    "parentCommentId": "eEaDWL9ywbRBqxKpZ",
    "topLevelCommentId": "Bohdww3hddWA3NNnZ",
    "contents": {
      "markdown": "Glad there is a specific idea there. What are the main approaches for this? There's Neuralink and there's gene editing, among other things. It seems MIRI may have access to technical talent that could speed up some of these projects.",
      "plaintextDescription": "Glad there is a specific idea there. What are the main approaches for this? There's Neuralink and there's gene editing, among other things. It seems MIRI may have access to technical talent that could speed up some of these projects."
    },
    "baseScore": 21,
    "voteCount": 12,
    "createdAt": null,
    "postedAt": "2024-05-30T03:02:02.760Z",
    "af": false
  }
]