[
  {
    "userId": "nmk3nLpQE89dMRzzN",
    "username": "Eliezer_Yudkowsky",
    "displayName": "Eliezer Yudkowsky",
    "karma": 153177,
    "afKarma": 1907,
    "ai_safety_tags": [
      "MIRI",
      "MIRI"
    ],
    "post_count_in_ai_safety": 2,
    "_id": "nmk3nLpQE89dMRzzN",
    "slug": "eliezer_yudkowsky",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 952,
    "commentCount": 7699,
    "createdAt": "2009-02-23T21:58:56.739Z",
    "profileTagIds": [],
    "post_count": 50,
    "comment_count": 100
  },
  {
    "userId": "r38pkCm7wF4M44MDQ",
    "username": "Raemon",
    "displayName": "Raemon",
    "karma": 59085,
    "afKarma": 731,
    "ai_safety_tags": [
      "MIRI",
      "AI Governance"
    ],
    "post_count_in_ai_safety": 2,
    "_id": "r38pkCm7wF4M44MDQ",
    "slug": "raemon",
    "bio": "LessWrong team member / moderator. I've been a LessWrong organizer since 2011, with roughly equal focus on the cultural, practical and intellectual aspects of the community. My first project was creating the Secular Solstice and helping groups across the world run their own version of it. More recently I've been interested in improving my own epistemic standards and helping others to do so as well.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 491,
    "commentCount": 8582,
    "createdAt": "2010-09-09T02:09:20.629Z",
    "profileTagIds": [],
    "post_count": 50,
    "comment_count": 100
  },
  {
    "userId": "N9zj5qpTfqmbn9dro",
    "username": "Zvi",
    "displayName": "Zvi",
    "karma": 53718,
    "afKarma": 146,
    "ai_safety_tags": [
      "AI Governance",
      "AI Governance",
      "Chain-of-Thought Alignment"
    ],
    "post_count_in_ai_safety": 3,
    "_id": "N9zj5qpTfqmbn9dro",
    "slug": "zvi",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": "thezvi.wordpress.com",
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 1000,
    "commentCount": 1476,
    "createdAt": "2009-03-31T20:54:54.077Z",
    "profileTagIds": [],
    "post_count": 50,
    "comment_count": 100
  },
  {
    "userId": "2aoRX3ookcCozcb3m",
    "username": "RobbBB",
    "displayName": "Rob Bensinger",
    "karma": 22702,
    "afKarma": 1384,
    "ai_safety_tags": [
      "MIRI",
      "MIRI",
      "MIRI",
      "MIRI",
      "MIRI"
    ],
    "post_count_in_ai_safety": 5,
    "_id": "2aoRX3ookcCozcb3m",
    "slug": "robbbb",
    "bio": "Communications @ MIRI. Unless otherwise indicated, my posts and comments here reflect my own views, and not necessarily my employer's. (Though we agree about an awful lot.)",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 125,
    "commentCount": 2232,
    "createdAt": "2012-08-10T00:50:11.669Z",
    "profileTagIds": [],
    "post_count": 50,
    "comment_count": 100
  },
  {
    "userId": "dfZAq9eZxs4BB4Ji5",
    "username": "ryan_greenblatt",
    "displayName": "ryan_greenblatt",
    "karma": 19977,
    "afKarma": 4823,
    "ai_safety_tags": [
      "Outer Alignment",
      "AI Governance",
      "AI Governance"
    ],
    "post_count_in_ai_safety": 3,
    "_id": "dfZAq9eZxs4BB4Ji5",
    "slug": "ryan_greenblatt",
    "bio": "I'm the chief scientist at Redwood Research.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 50,
    "commentCount": 1893,
    "createdAt": "2021-06-08T20:21:15.520Z",
    "profileTagIds": [],
    "post_count": 50,
    "comment_count": 100
  },
  {
    "userId": "BCmzFRdQhqLPREvat",
    "username": "ricraz",
    "displayName": "Richard_Ngo",
    "karma": 19906,
    "afKarma": 2854,
    "ai_safety_tags": [
      "Inner Alignment",
      "Inner Alignment",
      "AI Governance"
    ],
    "post_count_in_ai_safety": 3,
    "_id": "BCmzFRdQhqLPREvat",
    "slug": "ricraz",
    "bio": "Formerly alignment and governance researcher at DeepMind and OpenAI. Now independent.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 169,
    "commentCount": 1114,
    "createdAt": "2013-07-14T15:42:06.397Z",
    "profileTagIds": [],
    "post_count": 50,
    "comment_count": 100
  },
  {
    "userId": "n6LYNw2uGfYnD4pX2",
    "username": "lsusr",
    "displayName": "lsusr",
    "karma": 18664,
    "afKarma": 25,
    "ai_safety_tags": [
      "Mesa-Optimization"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "n6LYNw2uGfYnD4pX2",
    "slug": "lsusr",
    "bio": "Here is a [list of all my public writings and videos (from before February 2025).](https://www.lsusr.com/)",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 290,
    "commentCount": 1735,
    "createdAt": "2019-08-03T22:27:09.960Z",
    "profileTagIds": [],
    "post_count": 50,
    "comment_count": 100
  },
  {
    "userId": "4fh2AAe3n7oBviyxx",
    "username": "orthonormal",
    "displayName": "orthonormal",
    "karma": 17953,
    "afKarma": 288,
    "ai_safety_tags": [
      "Mesa-Optimization"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "4fh2AAe3n7oBviyxx",
    "slug": "orthonormal",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 70,
    "commentCount": 2824,
    "createdAt": "2009-03-22T16:06:51.665Z",
    "profileTagIds": [],
    "post_count": 50,
    "comment_count": 100
  },
  {
    "userId": "AThTtkDufXp3rmMDa",
    "username": "evhub",
    "displayName": "evhub",
    "karma": 14730,
    "afKarma": 4677,
    "ai_safety_tags": [
      "MIRI"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "AThTtkDufXp3rmMDa",
    "slug": "evhub",
    "bio": "Evan Hubinger (he/him/his) ([evanjhub@gmail.com](mailto:evanjhub@gmail.com))\n\nHead of [Alignment Stress-Testing](https://www.alignmentforum.org/posts/EPDSdXr8YbsDkgsDG/introducing-alignment-stress-testing-at-anthropic) at [Anthropic](https://www.anthropic.com/). My posts and comments are my own and do not represent Anthropic's positions, policies, strategies, or opinions.\n\nPreviously: MIRI, OpenAI\n\nSee: “[Why I'm joining Anthropic](https://www.lesswrong.com/posts/7jn5aDadcMH6sFeJe/why-i-m-joining-anthropic)”\n\nSelected work:\n\n*   “[Auditing language models for hidden objectives](https://www.alignmentforum.org/posts/wSKPuBfgkkqfTpmWJ/auditing-language-models-for-hidden-objectives)”\n*   “[Alignment faking in large language models](https://www.alignmentforum.org/posts/njAZwT8nkHnjipJku/alignment-faking-in-large-language-models)”\n*   “[Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://www.alignmentforum.org/posts/ZAsJv7xijKTfZkMtr/sleeper-agents-training-deceptive-llms-that-persist-through)”\n*   “[Conditioning Predictive Models](https://www.lesswrong.com/s/n3utvGrgC2SGi9xQX)”\n*   “[An overview of 11 proposals for building safe advanced AI](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai)”\n*   “[Risks from Learned Optimization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB)”",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 72,
    "commentCount": 805,
    "createdAt": "2017-01-17T06:05:22.405Z",
    "profileTagIds": [],
    "post_count": 50,
    "comment_count": 100
  },
  {
    "userId": "rx7xLaHCh3m7Po385",
    "username": "Buck",
    "displayName": "Buck",
    "karma": 14669,
    "afKarma": 3100,
    "ai_safety_tags": [
      "AI Governance"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "rx7xLaHCh3m7Po385",
    "slug": "buck",
    "bio": "CEO at Redwood Research.\n\nAI safety is a highly collaborative field--almost all the points I make were either explained to me by someone else, or developed in conversation with other people. I'm saying this here because it would feel repetitive to say \"these ideas were developed in collaboration with various people\" in all my comments, but I want to have it on the record that the ideas I present were almost entirely not developed by me in isolation.\n\nPlease contact me via email (bshlegeris@gmail.com) instead of messaging me on LessWrong.\n\nIf we are ever arguing on LessWrong and you feel like it's kind of heated and would go better if we just talked about it verbally, please feel free to contact me and I'll probably be willing to call to discuss briefly.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 47,
    "commentCount": 569,
    "createdAt": "2018-04-20T18:36:03.024Z",
    "profileTagIds": [],
    "post_count": 50,
    "comment_count": 100
  }
]