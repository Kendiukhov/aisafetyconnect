[
  {
    "userId": "nmk3nLpQE89dMRzzN",
    "username": "Eliezer_Yudkowsky",
    "displayName": "Eliezer Yudkowsky",
    "karma": 153177,
    "afKarma": 1907,
    "ai_safety_tags": [
      "MIRI",
      "MIRI"
    ],
    "post_count_in_ai_safety": 2,
    "_id": "nmk3nLpQE89dMRzzN",
    "slug": "eliezer_yudkowsky",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 952,
    "commentCount": 7699,
    "createdAt": "2009-02-23T21:58:56.739Z",
    "profileTagIds": []
  },
  {
    "userId": "r38pkCm7wF4M44MDQ",
    "username": "Raemon",
    "displayName": "Raemon",
    "karma": 59085,
    "afKarma": 731,
    "ai_safety_tags": [
      "MIRI",
      "AI Governance"
    ],
    "post_count_in_ai_safety": 2,
    "_id": "r38pkCm7wF4M44MDQ",
    "slug": "raemon",
    "bio": "LessWrong team member / moderator. I've been a LessWrong organizer since 2011, with roughly equal focus on the cultural, practical and intellectual aspects of the community. My first project was creating the Secular Solstice and helping groups across the world run their own version of it. More recently I've been interested in improving my own epistemic standards and helping others to do so as well.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 491,
    "commentCount": 8582,
    "createdAt": "2010-09-09T02:09:20.629Z",
    "profileTagIds": []
  },
  {
    "userId": "N9zj5qpTfqmbn9dro",
    "username": "Zvi",
    "displayName": "Zvi",
    "karma": 53718,
    "afKarma": 146,
    "ai_safety_tags": [
      "AI Governance",
      "AI Governance",
      "Chain-of-Thought Alignment"
    ],
    "post_count_in_ai_safety": 3,
    "_id": "N9zj5qpTfqmbn9dro",
    "slug": "zvi",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": "thezvi.wordpress.com",
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 1000,
    "commentCount": 1476,
    "createdAt": "2009-03-31T20:54:54.077Z",
    "profileTagIds": []
  },
  {
    "userId": "2aoRX3ookcCozcb3m",
    "username": "RobbBB",
    "displayName": "Rob Bensinger",
    "karma": 22702,
    "afKarma": 1384,
    "ai_safety_tags": [
      "MIRI",
      "MIRI",
      "MIRI",
      "MIRI",
      "MIRI"
    ],
    "post_count_in_ai_safety": 5,
    "_id": "2aoRX3ookcCozcb3m",
    "slug": "robbbb",
    "bio": "Communications @ MIRI. Unless otherwise indicated, my posts and comments here reflect my own views, and not necessarily my employer's. (Though we agree about an awful lot.)",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 125,
    "commentCount": 2232,
    "createdAt": "2012-08-10T00:50:11.669Z",
    "profileTagIds": []
  },
  {
    "userId": "dfZAq9eZxs4BB4Ji5",
    "username": "ryan_greenblatt",
    "displayName": "ryan_greenblatt",
    "karma": 19977,
    "afKarma": 4823,
    "ai_safety_tags": [
      "Outer Alignment",
      "AI Governance",
      "AI Governance"
    ],
    "post_count_in_ai_safety": 3,
    "_id": "dfZAq9eZxs4BB4Ji5",
    "slug": "ryan_greenblatt",
    "bio": "I'm the chief scientist at Redwood Research.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 50,
    "commentCount": 1893,
    "createdAt": "2021-06-08T20:21:15.520Z",
    "profileTagIds": []
  },
  {
    "userId": "BCmzFRdQhqLPREvat",
    "username": "ricraz",
    "displayName": "Richard_Ngo",
    "karma": 19906,
    "afKarma": 2854,
    "ai_safety_tags": [
      "Inner Alignment",
      "Inner Alignment",
      "AI Governance"
    ],
    "post_count_in_ai_safety": 3,
    "_id": "BCmzFRdQhqLPREvat",
    "slug": "ricraz",
    "bio": "Formerly alignment and governance researcher at DeepMind and OpenAI. Now independent.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 169,
    "commentCount": 1114,
    "createdAt": "2013-07-14T15:42:06.397Z",
    "profileTagIds": []
  },
  {
    "userId": "n6LYNw2uGfYnD4pX2",
    "username": "lsusr",
    "displayName": "lsusr",
    "karma": 18664,
    "afKarma": 25,
    "ai_safety_tags": [
      "Mesa-Optimization"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "n6LYNw2uGfYnD4pX2",
    "slug": "lsusr",
    "bio": "Here is a [list of all my public writings and videos (from before February 2025).](https://www.lsusr.com/)",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 290,
    "commentCount": 1735,
    "createdAt": "2019-08-03T22:27:09.960Z",
    "profileTagIds": []
  },
  {
    "userId": "4fh2AAe3n7oBviyxx",
    "username": "orthonormal",
    "displayName": "orthonormal",
    "karma": 17953,
    "afKarma": 288,
    "ai_safety_tags": [
      "Mesa-Optimization"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "4fh2AAe3n7oBviyxx",
    "slug": "orthonormal",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 70,
    "commentCount": 2824,
    "createdAt": "2009-03-22T16:06:51.665Z",
    "profileTagIds": []
  },
  {
    "userId": "AThTtkDufXp3rmMDa",
    "username": "evhub",
    "displayName": "evhub",
    "karma": 14730,
    "afKarma": 4677,
    "ai_safety_tags": [
      "MIRI"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "AThTtkDufXp3rmMDa",
    "slug": "evhub",
    "bio": "Evan Hubinger (he/him/his) ([evanjhub@gmail.com](mailto:evanjhub@gmail.com))\n\nHead of [Alignment Stress-Testing](https://www.alignmentforum.org/posts/EPDSdXr8YbsDkgsDG/introducing-alignment-stress-testing-at-anthropic) at [Anthropic](https://www.anthropic.com/). My posts and comments are my own and do not represent Anthropic's positions, policies, strategies, or opinions.\n\nPreviously: MIRI, OpenAI\n\nSee: “[Why I'm joining Anthropic](https://www.lesswrong.com/posts/7jn5aDadcMH6sFeJe/why-i-m-joining-anthropic)”\n\nSelected work:\n\n*   “[Auditing language models for hidden objectives](https://www.alignmentforum.org/posts/wSKPuBfgkkqfTpmWJ/auditing-language-models-for-hidden-objectives)”\n*   “[Alignment faking in large language models](https://www.alignmentforum.org/posts/njAZwT8nkHnjipJku/alignment-faking-in-large-language-models)”\n*   “[Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training](https://www.alignmentforum.org/posts/ZAsJv7xijKTfZkMtr/sleeper-agents-training-deceptive-llms-that-persist-through)”\n*   “[Conditioning Predictive Models](https://www.lesswrong.com/s/n3utvGrgC2SGi9xQX)”\n*   “[An overview of 11 proposals for building safe advanced AI](https://www.alignmentforum.org/posts/fRsjBseRuvRhMPPE5/an-overview-of-11-proposals-for-building-safe-advanced-ai)”\n*   “[Risks from Learned Optimization](https://www.alignmentforum.org/s/r9tYkB2a8Fp4DN8yB)”",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 72,
    "commentCount": 805,
    "createdAt": "2017-01-17T06:05:22.405Z",
    "profileTagIds": []
  },
  {
    "userId": "rx7xLaHCh3m7Po385",
    "username": "Buck",
    "displayName": "Buck",
    "karma": 14669,
    "afKarma": 3100,
    "ai_safety_tags": [
      "AI Governance"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "rx7xLaHCh3m7Po385",
    "slug": "buck",
    "bio": "CEO at Redwood Research.\n\nAI safety is a highly collaborative field--almost all the points I make were either explained to me by someone else, or developed in conversation with other people. I'm saying this here because it would feel repetitive to say \"these ideas were developed in collaboration with various people\" in all my comments, but I want to have it on the record that the ideas I present were almost entirely not developed by me in isolation.\n\nPlease contact me via email (bshlegeris@gmail.com) instead of messaging me on LessWrong.\n\nIf we are ever arguing on LessWrong and you feel like it's kind of heated and would go better if we just talked about it verbally, please feel free to contact me and I'll probably be willing to call to discuss briefly.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 47,
    "commentCount": 569,
    "createdAt": "2018-04-20T18:36:03.024Z",
    "profileTagIds": []
  },
  {
    "userId": "gSKzrqGFdS7DkXhuE",
    "username": "jessica.liu.taylor",
    "displayName": "jessicata",
    "karma": 10263,
    "afKarma": 824,
    "ai_safety_tags": [
      "MIRI",
      "MIRI"
    ],
    "post_count_in_ai_safety": 2,
    "_id": "gSKzrqGFdS7DkXhuE",
    "slug": "jessica-liu-taylor",
    "bio": "Jessica Taylor. CS undergrad and Master's at Stanford; former research fellow at MIRI.\n\nI work on decision theory, social epistemology, strategy, naturalized agency, mathematical foundations, decentralized networking systems and applications, theory of mind, and functional programming languages.\n\nBlog: [unstableontology.com](http://unstableontology.com)\n\nTwitter: [https://twitter.com/jessi_cata](https://twitter.com/jessi_cata)",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 70,
    "commentCount": 966,
    "createdAt": "2017-06-18T22:55:04.584Z",
    "profileTagIds": []
  },
  {
    "userId": "fjERoRhgjipqw3z2b",
    "username": "Mitchell_Porter",
    "displayName": "Mitchell_Porter",
    "karma": 9273,
    "afKarma": 6,
    "ai_safety_tags": [
      "AI Governance"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "fjERoRhgjipqw3z2b",
    "slug": "mitchell_porter",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 48,
    "commentCount": 2412,
    "createdAt": "2009-05-28T02:36:19.394Z",
    "profileTagIds": []
  },
  {
    "userId": "g8JkZfL8PTqAefpvx",
    "username": "JenniferRM",
    "displayName": "JenniferRM",
    "karma": 8978,
    "afKarma": 18,
    "ai_safety_tags": [
      "Mesa-Optimization"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "g8JkZfL8PTqAefpvx",
    "slug": "jenniferrm",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 34,
    "commentCount": 1377,
    "createdAt": "2009-03-06T17:16:50.600Z",
    "profileTagIds": []
  },
  {
    "userId": "DgsGzjyBXN8XSK22q",
    "username": "DanielFilan",
    "displayName": "DanielFilan",
    "karma": 8942,
    "afKarma": 1868,
    "ai_safety_tags": [
      "Inner Alignment",
      "AI Governance",
      "Mesa-Optimization"
    ],
    "post_count_in_ai_safety": 3,
    "_id": "DgsGzjyBXN8XSK22q",
    "slug": "danielfilan",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": "http://danielfilan.com",
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 152,
    "commentCount": 1396,
    "createdAt": "2014-01-30T11:04:39.341Z",
    "profileTagIds": []
  },
  {
    "userId": "nDpieb7g8huozpx9j",
    "username": "Thane Ruthenis",
    "displayName": "Thane Ruthenis",
    "karma": 8911,
    "afKarma": 852,
    "ai_safety_tags": [
      "Mesa-Optimization",
      "Mesa-Optimization",
      "Mesa-Optimization",
      "Mesa-Optimization",
      "Mesa-Optimization"
    ],
    "post_count_in_ai_safety": 5,
    "_id": "nDpieb7g8huozpx9j",
    "slug": "thane-ruthenis",
    "bio": "Agent-foundations researcher. Working on [Synthesizing Standalone World-Models](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/synthesizing-standalone-world-models-bounties-seeking), aiming at a technical solution to the AGI risk fit for worlds where alignment is punishingly hard and we only get one try.\n\nCurrently looking for additional funders ($1k+, [details](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/synthesizing-standalone-world-models-bounties-seeking#Funding)). Consider reaching out if you're interested, or [donating](https://manifund.org/projects/synthesizing-standalone-world-models) directly.\n\nOr [get me to pay *you* money](https://www.lesswrong.com/posts/LngR93YwiEpJ3kiWh/synthesizing-standalone-world-models-bounties-seeking#Bounties) ($5-$100) by spotting holes in my agenda or providing other useful information.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 46,
    "commentCount": 1000,
    "createdAt": "2022-03-20T15:21:33.973Z",
    "profileTagIds": []
  },
  {
    "userId": "mfgrYb4LMk7NWXsSB",
    "username": "tailcalled",
    "displayName": "tailcalled",
    "karma": 7887,
    "afKarma": 77,
    "ai_safety_tags": [
      "MIRI"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "mfgrYb4LMk7NWXsSB",
    "slug": "tailcalled",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 107,
    "commentCount": 2411,
    "createdAt": "2015-01-27T20:50:11.327Z",
    "profileTagIds": []
  },
  {
    "userId": "XLwKyCK7JmC292ZCC",
    "username": "Chris_Leong",
    "displayName": "Chris_Leong",
    "karma": 7708,
    "afKarma": 458,
    "ai_safety_tags": [
      "MIRI",
      "Inner Alignment"
    ],
    "post_count_in_ai_safety": 2,
    "_id": "XLwKyCK7JmC292ZCC",
    "slug": "chris_leong",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 229,
    "commentCount": 2181,
    "createdAt": "2009-05-28T03:08:43.251Z",
    "profileTagIds": []
  },
  {
    "userId": "TCjNiBLBPyhZq5BuM",
    "username": "Seth Herd",
    "displayName": "Seth Herd",
    "karma": 7411,
    "afKarma": 239,
    "ai_safety_tags": [
      "Inner Alignment",
      "Chain-of-Thought Alignment",
      "Chain-of-Thought Alignment",
      "Chain-of-Thought Alignment",
      "Chain-of-Thought Alignment"
    ],
    "post_count_in_ai_safety": 5,
    "_id": "TCjNiBLBPyhZq5BuM",
    "slug": "seth-herd",
    "bio": "Message me here or at seth dot herd at gmail dot com.\n\nI was a researcher in cognitive psychology and cognitive neuroscience for two decades and change. I studied complex human thought using neural network models of brain function. I'm applying that knowledge to figuring out how we can align AI as developers make it to \"think for itself\" in all the ways that make humans capable and dangerous.\n\nIf you're new to alignment, see the Research Overview section below. Field veterans who are curious about my particular take and approach should see the More on My Approach section at the end of the profile.\n\nImportant posts:\n----------------\n\n*   On LLM-based agents as a route to takeover-capable AGI\n    *   [LLM AGI will have memory, and memory changes alignment](https://www.lesswrong.com/posts/aKncW36ZdEnzxLo8A/llm-agi-will-have-memory-and-memory-changes-alignment)\n    *   [Brief argument for short timelines being quite possible](https://www.lesswrong.com/posts/oC4wv4nTrs2yrP5hz/what-are-the-strongest-arguments-for-very-short-timelines?commentId=3vSTG4gZgvz9ki5LP)\n    *   [Capabilities and alignment of LLM cognitive architectures](https://www.lesswrong.com/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures)\n        *   Cognitive psychology perspective on routes to LLM-based AGI with no breakthroughs needed\n*   AGI risk interactions with societal power structures and incentives:\n    *   [Whether governments will control AGI is important and neglected](https://www.lesswrong.com/posts/fFqABwAHMvhHSFmce/whether-governments-will-control-agi-is-important-and)\n    *   [If we solve alignment, do we die anyway?](https://www.lesswrong.com/posts/kLpFvEBisPagBLTtM/if-we-solve-alignment-do-we-die-anyway-1)\n        *   Risks of proliferating human-controlled AGI\n    *   [Fear of centralized power vs. fear of misaligned AGI: Vitalik Buterin on 80,000 Hours](https://www.lesswrong.com/posts/6iJrd8c9jxRstxJyE/fear-of-centralized-power-vs-fear-of-misaligned-agi-vitalik)\n*   On the psychology of alignment as a field:\n    *   [Cruxes of disagreement on alignment difficulty](https://www.lesswrong.com/posts/ye78Dip8YNgLBKGcy/seth-herd-s-shortform?commentId=FpdvoZsmmrNLekkz9)\n    *   [Motivated reasoning/confirmation bias as the most important cognitive bias](https://www.lesswrong.com/posts/j789HDCKLoiKGjBik/which-biases-are-most-important-to-overcome#LW8zAxTguKj8ibDfX)\n*   On technical alignment of LLM-based AGI agents:\n    *   [System 2 Alignment](https://www.lesswrong.com/posts/cus5CGmLrjBRgcPSF/system-2-alignment) on how developers will try to align LLM agent AGI\n    *   [Seven sources of goals in LLM agents](https://www.lesswrong.com/posts/nHDhst47yzDCpGstx/seven-sources-of-goals-in-llm-agents) brief problem statement\n    *   [Internal independent review for language model agent alignment](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent)\n*   On AGI alignment targets assuming technical alignment\n    *   [Problems with instruction-following as an alignment target](https://www.lesswrong.com/posts/CSFa9rvGNGAfCzBk6/problems-with-instruction-following-as-an-alignment-target)\n    *   [Instruction-following AGI is easier and more likely than value aligned AGI](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than)\n    *   [Goals selected from learned knowledge: an alternative to RL alignment](https://www.alignmentforum.org/posts/DfJCTp4MxmTFnYvgF/goals-selected-from-learned-knowledge-an-alternative-to-rl)\n*   On communicating AGI risks:\n    *   [Anthropomorphizing AI might be good, actually](https://www.lesswrong.com/posts/JfgME2Kdo5tuWkP59/anthropomorphizing-ai-might-be-good-actually)\n    *   [Humanity isn’t remotely longtermist, so arguments for AGI x-risk should focus on the near term](https://www.lesswrong.com/posts/fdracpKGbH4xqprQK/humanity-isn-t-remotely-longtermist-so-arguments-for-agi-x)\n    *   [AI scares and changing public beliefs](https://www.lesswrong.com/posts/ou5raNNjamAaahtWG/ai-scares-and-changing-public-beliefs)\n\nResearch Overview:\n------------------\n\n*Alignment* is the study of how to give AIs goals or values aligned with ours, so we're not in competition with our own creations. Recent breakthroughs in AI like ChatGPT make it possible we'll have smarter-than-human AIs soon. So we'd better get ready. If their goals don't align well enough with ours, they'll probably outsmart us and get their way — and treat us as we do ants or monkeys. See this [excellent intro video](https://www.youtube.com/watch?v=-H7e4XlMgg0) for more. \n\nThere are [good and deep reasons](https://www.lesswrong.com/posts/LLRtjkvh9AackwuNB/on-a-list-of-lethalities) to think that aligning AI will be very hard. But I think we have [promising solutions](https://alignmentforum.org/posts/xqqhwbH2mq6i4iLmK/we-have-promising-alignment-plans-with-low-taxes) that bypass most of those difficulties, and could be relatively easy to use for the types of AGI we're most likely to develop first. \n\nThat doesn't mean I think building AGI is safe. Humans often screw up complex projects, particularly on the first try, and we won't get many tries. If it were up to me I'd Shut It All Down, but I don't see how we could get all of humanity to stop building AGI. So I focus on finding alignment solutions for the types of AGI people are building.\n\nIn brief I think we can probably [build and align language model agents](https://alignmentforum.org/posts/ogHr8SvGqg9pW5wsT/capabilities-and-alignment-of-llm-cognitive-architectures) (or language model cognitive architectures) even when they're more autonomous and competent than humans. We'd use a [stacking suite of alignment methods](https://www.alignmentforum.org/posts/Q7XWGqL4HjjRmhEyG/internal-independent-review-for-language-model-agent) that can mostly or entirely [avoid using RL for alignment](https://alignmentforum.org/posts/DfJCTp4MxmTFnYvgF/goals-selected-from-learned-knowledge-an-alternative-to-rl), and achieve corrigibility (human-in-the-loop error correction) by having a [central goal of following instructions](https://alignmentforum.org/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than). This scenario leaves multiple humans in charge of ASIs, creating some dangerous dynamics, but those problems might be navigated, too. \n\nBio\n---\n\nI did computational cognitive neuroscience research from getting my PhD in 2006 until the end of 2022. I've worked on computational theories of vision, executive function, episodic memory, and decision-making, using neural network models of brain function to integrate data across levels of analysis from psychological down to molecular mechanisms of learning in neurons, and everything in between. I've focused on the interactions between different brain neural networks that are needed to explain complex thought. [Here's a list of my publications.](https://sethaherd.com/neuroscience-publications/) \n\nI was increasingly concerned with AGI applications of the research, and reluctant to publish my full theories lest they be used to accelerate AI progress. I'm incredibly excited to now be working full-time on alignment, currently as a research fellow at the [Astera Institute](https://astera.org).  \n\nMore on My Approach\n-------------------\n\nThe field of AGI alignment is \"pre-paradigmatic.\" So I spend a lot of my time thinking about what problems need to be solved, and how we should go about solving them. Solving the wrong problems seems like a waste of time we can't afford.\n\nWhen LLMs suddenly started looking intelligent and useful, I noted that applying cognitive neuroscience ideas to them might well enable them to reach AGI and soon ASI levels. Current LLMs are like humans with no episodic memory for their experiences, and very little executive function for planning and goal-directed self-control. Adding those cognitive systems to LLMs can make them into cognitive architectures with all of humans' cognitive capacities - a [\"real\" artificial general intelligence](https://www.lesswrong.com/posts/YW249knFccwATGxki/real-agi) that will soon be able to outsmart humans. \n\nMy work since then has convinced me that we could probably also align such an AGI so that it stays aligned even if it grows much smarter than we are.  Instead of trying to give it a definition of ethics it can't misunderstand or re-interpret (value alignment mis-specification), we'll continue doing with the alignment target developers currently use: [Instruction-following](https://www.lesswrong.com/posts/7NvKrqoQgJkZJmcuD/instruction-following-agi-is-easier-and-more-likely-than). It's counter-intuitive to imagine an intelligent entity that wants nothing more than to follow instructions, but there's no logical reason this can't be done.  An instruction-following proto-AGI can be instructed to act as a helpful collaborator in keeping it aligned as it grows smarter. \n\nThere are significant problems to be solved in prioritizing instructions; we would need an agent to prioritize more recent instructions over previous ones, including hypothetical future instructions. \n\nI increasingly suspect we should be actively working to build such intelligences. It seems like our our best hope of survival, since I don't see how we can convince the whole world to pause AGI efforts, and other routes to AGI seem much harder to align since they won't \"think\" in English. Thus far, I haven't been able to engage enough careful critique of my ideas to know if this is wishful thinking, so I haven't embarked on actually helping develop language model cognitive architectures.\n\nEven though these approaches are pretty straightforward, they'd have to be implemented carefully. Humans often get things wrong on their first try at a complex project. So my p(doom) estimate of our long-term survival as a species is in the 50% range, too complex to call. That's despite having a pretty good mix of relevant knowledge and having spent a lot of time working through various scenarios. So I think anyone with a very high or very low estimate is overestimating their certainty.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 43,
    "commentCount": 1817,
    "createdAt": "2022-09-05T00:45:45.715Z",
    "profileTagIds": []
  },
  {
    "userId": "QpvwBD5AtmmFDTC3T",
    "username": "leogao",
    "displayName": "leogao",
    "karma": 7176,
    "afKarma": 890,
    "ai_safety_tags": [
      "Mesa-Optimization"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "QpvwBD5AtmmFDTC3T",
    "slug": "leogao",
    "bio": "",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 32,
    "commentCount": 499,
    "createdAt": "2020-04-19T18:50:05.381Z",
    "profileTagIds": []
  },
  {
    "userId": "x5S2Kuj6TfQTGuo63",
    "username": "thomas-kwa",
    "displayName": "Thomas Kwa",
    "karma": 7018,
    "afKarma": 566,
    "ai_safety_tags": [
      "MIRI"
    ],
    "post_count_in_ai_safety": 1,
    "_id": "x5S2Kuj6TfQTGuo63",
    "slug": "thomas-kwa",
    "bio": "Member of technical staff at [METR](https://metr.org/).\n\nPreviously: [Vivek Hebbar's team at MIRI](https://www.lesswrong.com/posts/qbcuk8WwFnTZcXTd6/thomas-kwa-s-miri-research-experience) **→** [Adrià Garriga-Alonso](https://agarri.ga/) on [](https://agarri.ga/) [various empirical alignment projects](https://www.lesswrong.com/posts/bf3vciB36dnd75ZKJ/thomas-kwa-s-research-journal) → METR.\n\nI have signed no contracts or agreements whose existence I cannot mention.",
    "jobTitle": null,
    "organization": null,
    "careerStage": null,
    "website": null,
    "linkedinProfileURL": null,
    "githubProfileURL": null,
    "twitterProfileURL": null,
    "postCount": 26,
    "commentCount": 800,
    "createdAt": "2019-12-09T06:28:45.933Z",
    "profileTagIds": []
  }
]