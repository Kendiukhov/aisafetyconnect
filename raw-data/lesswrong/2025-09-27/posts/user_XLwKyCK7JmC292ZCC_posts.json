[
  {
    "_id": "YKhteXYywLgNtKkYF",
    "title": "Three Quotes on Transformative Technology",
    "slug": "three-quotes-on-transformative-technology",
    "url": null,
    "baseScore": 8,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2025-08-01T22:57:20.007Z",
    "contents": {
      "markdown": "<table style=\"border-style:none\"><tbody><tr><td style=\"border-style:none;padding:0px;text-align:center\"><p>Did you and the other scientists not stop to consider the implications of what you were creating? — Roger Robb</p><p>When you see something that is technically sweet, you go ahead and do it and you argue about what to do about it only after you have had your technical success. That is the way it was with the atomic bomb— Oppenheimer</p></td></tr><tr><td style=\"border-style:none;text-align:center\">❦</td></tr><tr><td style=\"border-style:none;text-align:center\">There are moments in the history of science, where you have a group of scientists look at their creation and just say, you know: ‘What have we done?... Maybe it's great, maybe it's bad, but what have we done? — Sam Altman</td></tr><tr><td style=\"border-style:none;padding:0px;text-align:center\">❦</td></tr><tr><td style=\"border-style:none;text-align:center\">Urgent: get collectively wiser - Yoshua Bengio, AI \"Godfather\", <i>On the Wisdom Race</i></td></tr></tbody></table>\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d76b3bda40c203cb97d5319db1a10aa55827ea0731a7e4f3.png)",
      "plaintextDescription": "Did you and the other scientists not stop to consider the implications of what you were creating? — Roger Robb\n\nWhen you see something that is technically sweet, you go ahead and do it and you argue about what to do about it only after you have had your technical success. That is the way it was with the atomic bomb— Oppenheimer\n\n❦There are moments in the history of science, where you have a group of scientists look at their creation and just say, you know: ‘What have we done?... Maybe it's great, maybe it's bad, but what have we done? — Sam Altman❦Urgent: get collectively wiser - Yoshua Bengio, AI \"Godfather\", On the Wisdom Race",
      "wordCount": 121
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "SZk3guP2djHiXQyYv",
    "title": "On actually taking expressions literally: tension as the key to meditation?",
    "slug": "on-actually-taking-expressions-literally-tension-as-the-key",
    "url": null,
    "baseScore": 16,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 12,
    "createdAt": null,
    "postedAt": "2025-07-13T22:49:41.828Z",
    "contents": {
      "markdown": "*Some speculations based upon the *[*Vasocomputational Theory of Mediation*](https://opentheory.net/2023/07/principles-of-vasocomputation-a-unification-of-buddhist-phenomenology-active-inference-and-physical-reflex-part-i/)*, meditation and some poorly understood Lakoff. Even though reading about meditation is low risk, I wouldn't necessarily assume that it is risk-free.*\n\nA summer's night. Two friends have been sitting around a fire, discussing life and meaning late into the evening...\n\n*Riven*: So in short, I feel like I'm being torn in two.\n\n*Rafael*: Part of you is being pulled one direction and another part of you is being pulled another way.\n\n*Riven*: That’s exactly what I’m feeling.\n\n*Rafael*: Figuratively or literally?\n\n*Riven*: What? No… what?!\n\n*Rafael*: I’m serious\n\n*Riven*: Literally??\n\n*Rafael*: Yes.  \n  \n*Riven*: Come now, can't you be serious for once? Whilst the bit may worked for Socrates, I have to admit that find it rather tiresome when you play the fool. \n\n*Rafael*: It’s quite possible I’m being stupid, but unfortunately, I must confess that it's not in any way a pretense. If I'm being stupid, it's because I'm actually stupid. I swear on my honor as a gentleman that I am completely sincere.\n\nI trust that you *think* you’re think you’re speaking figuratively, but sometimes we accidentally hit upon deeper truths than we realise. You are familiar with Lakoff, right?  \n  \n*Riven*: Of course. Three of his claims stood out to me. Firstly, that dead metaphors referencing physical action are much more common than we realise. When we say we’re \"going through” something, or that we want to “pick up” a topic again or that we’re “keeping” on acting a particular way, even though we typically don't realise this, we're invoking a turn of phrase that started off as a metaphor[^mf2cg6s74br]. Secondly, that this isn’t a mere flourish, but instead provides insight into how human language developed. Thirdly, that the fact that we needed this scaffolding reveals a deep truth about how the human brain works\n\n*Rafael*: And what do you think of his claims?  \n  \n*Riven*: I don’t know if I’d unambiguously and universally endorse these claims, but I suspect they’re at least somewhat true. I don't know why you're bringing this up though, all of these examples are figurative. Why bring up a figure who doesn't at all support your point?\n\n*Rafael*: I agree that a straight-forward Lakoffian analysis doesn't help me here, but it is also possible to take broader inspiration from someones work. In [Unsong](https://unsongbook.com/), the phrase \"this was not a coincidence, because nothing is ever a coincidence\", is repeated each time the author shares a satirical conspiracy theory. The influence I take from Lakoff is not presuming that the phrasing a language uses to describe a concept is merely incidental, but instead often deeply revealing about human psychology. He focuses on dead metaphors, but sometimes the rabbit hole goes deeper than that. And that's what I'm proposing in this particular case.\n\n*Riven*: That feels like a long bow to draw, but I'm intrigued. How could this be *literally* true? Come now, you simply must tell me. I trust you're not just another millennial who literally doesn't understand the meaning of the word \"literally\".\n\n*Rafael:* I'm not so sure that the millennial usage of the word \"literally\" is actually a misuse — linguistic descriptivism, Wittegensteinian language games and all that. However, I want to actually attempt to stay on topic for once. Let's suppose you're literally standing at a crossroads, deciding whether to go left or right. You can even try it if you want. If you feel into your body, I expect you'll likely literally be able to feel tension between two muscles where one is pulling one way and another muscle is pulling another way. Oh, btw, \"tension\" that's literal as well. Imagine a goalie croaching ready to spring left or right, by tensing two sets of muscles they can prepare to leap on an instant's notice — no need to wind up first.\n\n*Riven*: Fascinating. However... I'm basically never *literally* standing at a crossroad or preparing to spring one way or another. At most you can claim that these terms are *occasionally* literal.\n\n*Rafael:* On the contrary, it happens all the time. Put your two favourite flavours of drink on a table in front of you and tell me that you don't feel a literal tension. For me, it's most noticable in the neck  — when I'm selecting a choice, my first eyes start drifting towards it. Let's suppose I considering whether to grab dinner or continue working [^pwdyvehswkd]  —  my eyes drift either towards the door or my laptop, but if I prevent them from doing either, then I feel tension.\n\n*Riven:* I have to admit that now that you're saying this, I'm starting to feel something surprisingly similar. However, it's unclear to me whether this was only the case and I'm only starting to notice it now or whether this was always true and you just psy-oped[^eigmf4fbpe] me into believing it. Could you clarify how you think this came about? Are you proposing that people explicitly noticed these internal sensations and decided to make use of it when naming the phenomenon? This wouldn't be impossible, but I must admit that I'm dubious.\n\n*Rafael*: It certainly needn't be so direct. Decisions in neural networks are driven by the combination of a large number of factors. I expect that the pheomenological experience would have (weakly) affected what kinds of metaphors sounded right, without any need for such conscious awareness.\n\n*Riven:* I suppose that's not completely implausible, though I am more skeptical about these kinds of claims these days given the [failure of many \"priming\" results to replicate](https://replicationindex.com/2025/05/08/priming-research-past-its-prime/). I'll have to think about it some more. But supposing this is all true, surely this would have some consequences in terms of how we should treat stress?\n\n*Rafael*: Indeed, whilst I'm still reflecting on this, if it were true, it would suggest that relaxing particular muscles might be one of the best ways to relieve tension. Which should surprise nobody given how common this advice is. However, this seems to suggest that the stress relief effect more direct than you might think.\n\n*Riven*: That sounds plausible, but I'm not completely sold. The muscle tension could be a incidental, downstream effect of stress, rather than being at the heart of the phenomenon.\n\n*Rafael:* Indeed, that's not implausible, but I think not. I'd prefer to leave that thread for another time. For now, I'd like to venture an even more speculative hypothesis — on the oft-repeated claim of how we're already enlightended.\n\n*Riven*: I suppose your interpretation of Buddhism can't be worse than your reading of Lakoff. Do go on...\n\n*Rafael*: Suppose meditation were really about relieving tension by relaxing muscles and enlightenment were about releasing all such tension. The question then becomes, how would you do that? Now, here's the trick, relaxing is the most natural thing in the world, you don't really have to be taught how to do it. All you have to do is to put your attention on the tension and you'll automatically relax, so long as you don't actively maintain it.\n\n*Riven*: It sounds so easy and yet...\n\n*Rafael*: Tension is uncomfortable. Our brain wants to resolve any tension, but if that doesn't prove possible, then it learns to look away. If you want to examine the tension, you need to overcome this resistance. However, directing your attention like this, may require you to strain. Again, this is literal, and the muscle tension this creates may interfere with your ability to relax the muscles you are focusing on. This explain why trying to hard to relax is counterproductive and the same too for mediation. \n\n*Riven*: That's sounding much tricker now.\n\n*Rafael*: Unfortunately, it doesn't stop there. Your body's tactile space isn't Newtownian; it's closer to Einsteinian[^q7354qecqti]. However, instead of space being bent by gravity, it's bent by discomfort. You can try to move your attention towards the tension, but if you naively assume the space to be Euclidean, then you'll likely miss. It's also possible to have a situation where discomfort in spot A prevent you from directing your attention towards spot B, but discomfort in spot B prevents you from directing your attention towards spot A.\n\nEven if you manage to direct your attention to the desired spot, it may still immediately bounce off. Plus, there's a natural human instinct to activate your cognitive system to find a way to resolve the discomfort. In many cases, this instinct is correct, but if you actually can't access any solutions, then getting caught up in your thoughts can end up counterproductive as it'd prevent you from focusing your attention on the muscles that you want to relax.\n\n*Riven:* That sounds hard, but actually a lot easier than following the instructions I've recieved when attending meditation class. You're obviously not enlightened, so I assume you're suggesting that if you followed the process long enough you might get there?\n\n*Rafael*: I honestly don't know what the path looks like. As you say, my understanding of Buddhism is quite poor and I haven't gone very far down the path, but this seems to at least be part of the puzzle.\n\n*Riven:* I suppose you're expecting me to join you meditative journey then.\n\n*Rafael*: Casual mediation class is likely robustly good. However, I can't in good conscious personally recommend more serious meditation without having ventured further down this path. Although it's often downplayed, serious mediation comes with serious risks that ought to be taken seriously. There's a lot to be gained here, but don't assume it's all smooth sailing. In fact, some of these waters are treacherous indeed.\n\n### **Related:**\n\n*   [Principles of Vasocomputation: A Unification of Buddhist Phenomenology, Active Inference, and Physical Reflex](https://opentheory.net/2023/07/principles-of-vasocomputation-a-unification-of-buddhist-phenomenology-active-inference-and-physical-reflex-part-i/)\n*   [Wild animals may recover from stress better than humans do. Why?](https://www.youtube.com/watch?v=aJNcPztJQa8)\n*   [Tension & Trauma Releasing Exercises](https://www.youtube.com/watch?v=FeUioDuJjFI)\n\n[^mf2cg6s74br]: These examples are merely illustrative. I don't have the linguistic knowledge to know if they are actually plausible. \n\n[^pwdyvehswkd]: A completely hypothetical example  😛 \n\n[^eigmf4fbpe]: Confirmation bias \n\n[^q7354qecqti]: Don't take this statement too literally!",
      "plaintextDescription": "Some speculations based upon the Vasocomputational Theory of Mediation, meditation and some poorly understood Lakoff. Even though reading about meditation is low risk, I wouldn't necessarily assume that it is risk-free.\n\nA summer's night. Two friends have been sitting around a fire, discussing life and meaning late into the evening...\n\nRiven: So in short, I feel like I'm being torn in two.\n\nRafael: Part of you is being pulled one direction and another part of you is being pulled another way.\n\nRiven: That’s exactly what I’m feeling.\n\nRafael: Figuratively or literally?\n\nRiven: What? No… what?!\n\nRafael: I’m serious\n\nRiven: Literally??\n\nRafael: Yes.\n\nRiven: Come now, can't you be serious for once? Whilst the bit may worked for Socrates, I have to admit that find it rather tiresome when you play the fool. \n\nRafael: It’s quite possible I’m being stupid, but unfortunately, I must confess that it's not in any way a pretense. If I'm being stupid, it's because I'm actually stupid. I swear on my honor as a gentleman that I am completely sincere.\n\nI trust that you think you’re think you’re speaking figuratively, but sometimes we accidentally hit upon deeper truths than we realise. You are familiar with Lakoff, right?\n\nRiven: Of course. Three of his claims stood out to me. Firstly, that dead metaphors referencing physical action are much more common than we realise. When we say we’re \"going through” something, or that we want to “pick up” a topic again or that we’re “keeping” on acting a particular way, even though we typically don't realise this, we're invoking a turn of phrase that started off as a metaphor[1]. Secondly, that this isn’t a mere flourish, but instead provides insight into how human language developed. Thirdly, that the fact that we needed this scaffolding reveals a deep truth about how the human brain works\n\nRafael: And what do you think of his claims?\n\nRiven: I don’t know if I’d unambiguously and universally endorse these claims, but I suspect they’re at least ",
      "wordCount": 1632
    },
    "tags": [
      {
        "_id": "AiNyf5iwbpc7mehiX",
        "name": "Meditation",
        "slug": "meditation"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "WBMcKgrTpmniTK8HG",
    "title": "An Easily Overlooked Post on the Automation of Wisdom and Philosophy",
    "slug": "an-easily-overlooked-post-on-the-automation-of-wisdom-and",
    "url": "https://blog.aiimpacts.org/p/essay-competition-on-the-automation",
    "baseScore": 19,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-06-12T02:54:50.303Z",
    "contents": {
      "markdown": "This week for Wise AI Wednesdays, I'll be sharing something a bit different - the [`announcement post`](https://blog.aiimpacts.org/p/essay-competition-on-the-automation) of a competion that is already over (the AI Impacts Essay competition on the Automation of Wisdom and Philosophy**)**. If you're wondering why I'm sharing it, even though some of the specific discussion of the competition is no longer relevant, I still believe this post contains a lot of great content and I think it would be a shame if everyone forgot about it just because it happened to be in the announcement post.\n\nThis post explains why they think this might be important, lists some potentially interesting research directions, and then finishes with an FAQ. If you're looking to dive into this area, this is a pretty good place to start.\n\n* * *\n\nBefore I go, I just wanted to share a few paragraphs from the post, specifically why they think this area might be important:  \n \n\n> AI is likely to automate more and more categories of thinking with time.  \n>   \n> By default, the direction the world goes in will be a result of the choices people make, and these choices will be informed by the best thinking available to them. People systematically make better, wiser choices when they understand more about issues, and when they are advised by deep and wise thinking.\n> \n> Advanced AI will reshape the world, and create many new situations with potentially high-stakes decisions for people to make. To what degree people will understand these situations well enough to make wise choices remains to be seen. To some extent this will depend on how much good human thinking is devoted to these questions; but at some point it will probably depend crucially on how advanced, reliable, and widespread the automation of high-quality thinking about novel situations is.\n> \n> We believe[^1^](https://blog.aiimpacts.org/p/essay-competition-on-the-automation#footnote-1-143374374) that this area could be a crucial target for differential technological development, but is at present poorly understood and receives little attention. This competition aims to encourage and to highlight good thinking on the topics of what would be needed for such automation, and how it might (or might not) arise in the world.\n\n[`Post link`](https://blog.aiimpacts.org/p/essay-competition-on-the-automation)",
      "plaintextDescription": "This week for Wise AI Wednesdays, I'll be sharing something a bit different - the announcement post of a competion that is already over (the AI Impacts Essay competition on the Automation of Wisdom and Philosophy). If you're wondering why I'm sharing it, even though some of the specific discussion of the competition is no longer relevant, I still believe this post contains a lot of great content and I think it would be a shame if everyone forgot about it just because it happened to be in the announcement post.\n\nThis post explains why they think this might be important, lists some potentially interesting research directions, and then finishes with an FAQ. If you're looking to dive into this area, this is a pretty good place to start.\n\n----------------------------------------\n\nBefore I go, I just wanted to share a few paragraphs from the post, specifically why they think this area might be important:\n \n\n> AI is likely to automate more and more categories of thinking with time.\n> \n> By default, the direction the world goes in will be a result of the choices people make, and these choices will be informed by the best thinking available to them. People systematically make better, wiser choices when they understand more about issues, and when they are advised by deep and wise thinking.\n> \n> Advanced AI will reshape the world, and create many new situations with potentially high-stakes decisions for people to make. To what degree people will understand these situations well enough to make wise choices remains to be seen. To some extent this will depend on how much good human thinking is devoted to these questions; but at some point it will probably depend crucially on how advanced, reliable, and widespread the automation of high-quality thinking about novel situations is.\n> \n> We believe1 that this area could be a crucial target for differential technological development, but is at present poorly understood and receives little attention. This competition aims to encourage ",
      "wordCount": 363
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "24RvfBgyt72XzfEYS",
    "title": "Potentially Useful Projects in Wise AI",
    "slug": "potentially-useful-projects-in-wise-ai",
    "url": null,
    "baseScore": 12,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-06-05T08:13:42.839Z",
    "contents": {
      "markdown": "This is a list of projects[^lcc7gtz365j] to consider for folks who want to use Wise AI to steer the world towards positive outcomes.\n\nSome of these projects are listed because they're impactful. Others are listed because I believe they would be good projects for someone to get started. \n\nPlease note that this post is titled \"potentially useful projects\" for a reason. Some of these projects are likely to have much higher impact than others. Whilst I've really tried to avoid projects with net-negative EV, it's entirely possible that a few would be anyway[^mr98rtn3rwn]. Please don't ignore your own judgment just because I've listed a project!\n\nI'm sure my views about what projects are valuable will change quite significantly as I dive deeper into the topic, but I still think it's worthwhile for me to put some kind of list out there. \n\n* * *\n\n### **Field-building:**\n\nAt this stage, I consider field-building work around Wise AI as especially high priority. I feel that there’s starting to be some real energy around this area, however, very little of this will aid us in avoiding catastrophic risks. Fields tend to be more flexible in their initial stages, but gradually settle into an orthodoxy. It's important to intervene when it's still possible to make a difference:\n\n*   **Prioritisation research**: Wisdom can mean a lot of different things; there are multiple types of wisdom. It seems quite important to try to identify priorities whilst the field is still fluid and easier to influence via this kind of work. One key way AI safety analysis will differ from that of people outside the field is that AI safety analysis is much more likely to take into account the externalities of enabling a capability, as opposed to naively ignoring this. Whilst this work is high priority, you may want to refrain unless you think it’s a particularly good fit for your skillset, given the difficulty of this work. It’s very easy to miss a [crucial consideration](https://forum.effectivealtruism.org/topics/crucial-consideration#:~:text=A%20crucial%20consideration%20is%20a,of%20a%20cause%20or%20intervention.) that breaks your analysis and maybe even reverses the sign.\n*   **Summarising important papers or other resources** ([example](https://www.lesswrong.com/posts/euAMyQAQWTYyWZW8Z/summary-imagining-and-building-wise-machines-the-centrality)): Producing a good summary can take a lot of work, but this doesn’t seem to be the hardest skill to learn. There are two primary reasons why you might want to do this: a) increasing the impact of the original research b) building up the field by raising awareness and by making it easier to get oriented.\n*   **Produce and verify some AI-generated reports related to artificial wisdom:** AI-generated reports are often very good. The main downside is that you don’t know if they are reliable, but this isn't much of an issue if someone just decides to put in the work.\n*   **Literature review of previous work on artificial wisdom:** Whilst artificial reports can be very helpful, Deep Research still can’t match human judgement in terms of what is important. This would take longer, but it's probably worth it.\n*   **Summary of how different cultures and disciplines conceive of wisdom:** Whilst less direct than reviewing work on artificial wisdom, this would still save few folk a lot of time. Very easily to subtly distort another culture's ontology though.\n*   **Assist with the communication of these ideas**: There is some quite valuable work to be done here. PM me if you’re interested in this, though I suspect quality of communication is more important than quantity.\n\n### **Theoretical questions**[^y645fhft5sl]**:**\n\n*   **To what extent is wisdom just the absence of unwisdom**[^ldjhsd9rg0k]**?** Even if being wise requires more than the mere avoidance of being unwise, avoiding unwisdom may still be the lowest hanging fruit.\n*   **In what ways is current AI wise? In what ways is it unwise?** It's much easier to form a plan to fill in the gaps once we know what they are.\n*   **Insofar as current AIs are wise, what is it about their training that makes them wise?** Similarly, insofar as they are unwise, what is the cause?\n*   **How might AI wisdom differ from human wisdom? How can we bridge this gap?**\n*   **How might neural networks represent wisdom internally?** Is there likely to be a “wisdom direction”, and, if so, how would we find it? Maybe we can identify the various components of wisdom?\n*   **What are the impacts of choosing a particular broad approach to learning wisdom?** For example, imitation learning vs RL. What balance is optimal?\n*   **In what circumstances might a wise advisor lead to a reckless or malicious actor becoming responsible or prosocial?** This determines the extent to which wise AI would be safe to proliferate.\n*   **How can we evaluate an AI system for wisdom**[^qaaj90hnbuf]**?** Is this even possible? How accurately would we need to be able to measure wisdom in order to train an AI system to be wise? How does this differ from evaluating wisdom in humans?\n*   **Is the idea of a “**[**wisdom explosion**](https://aiimpacts.org/some-preliminary-notes-on-the-promise-of-a-wisdom-explosion/)**” a coherent concept?** Or perhaps it is conceivable, but feedback loops are too weak in this world to make it work.\n*   **Is a super-intelligence system automatically a super-wise system? **If so, for what definitions of intelligence and wisdom\n*   **What biases or mistakes can cause someone to be smart rather than wise?**[^x474rb4rvvp]\n*   **What level of wisdom do we need in order to navigate these challenges?**\n*   **What would the world look like if it were devoting serious attention to this?**[^twv9htwvaj]\n*   **What are the strengths and weaknesses of LLMs when it comes to philosophy?**\n\n### **Concrete work:**\n\n*   **Improving model specs:** Read some model specs and make suggestions about how to improve the wisdom of the models. You might object that this would only be incremental, but I suspect incremental gains are valuable anyway.\n*   **User interface design**: What is the best user interface for obtaining wise advice when it's really important to make the right decision? It seems unlikely that it would be just a simple chatbot interface.\n*   **Journaling your attempts to make use of AI advice**: The hope would be that the journaling would identify issues or considerations that would be easy to miss in Theory Land. For this reason, I see subjective research as massively underrated. It may not be as rigorous as empirical research, but it provides a firehose of bits and this is more valuable than rigor in the early stages of a field.\n*   **Attempting to train wise AI advisors and journaling your experience**: As per the previous point, at least at this stage, I’d much rather have an informal journal someone kept whilst attempting to train wise AI advisors than a few extra bits of empirical info.\n*   **Identify and experiment with various toy scenarios to figure out which ones would be most valuable for developing or testing wisdom**: This work could be directly applied, but it would help clarify the strategy for improving wisdom as well.\n*   **Social media advisor bot**: Create a bot to advise you on your social media posts. Whether they’re posts that you’re likely to regret in the future, according to your own values, either because you’re being rude or because you’re strawmanning. If this were to improve the quality of the social media discourse, it could increase the chance of society being able to navigate somewhere positively.\n*   **Tools for reducing cognitive biases**: There’s a decent chance that being wise is primarily about avoiding unwisdom, so why not lean into it?\n*   **Decision sanity-checker**: “If catastrophes often involve someone making egregiously bad decisions (knowingly or otherwise), might there be a role for LLMs to help catch these decisions before they are finalized? What information and infrastructure would be needed? - There will likely be decisions that such a tool could catch, where avoiding making a bad decision could be massively beneficial for the world[^j11mumi264a].\n*   **Crux identification**: AI tool that determines the main reasons that people (or groups disagree). Much more tractable then directly figuring out the correct answer.\n*   **Start a group that aims to provide analyses on questions of strategic importance with the assistance of AI tools**[^wvv8ex4aj2b]**:** Might be closer to a bite-sized piece of work than trying to directly train AI to advise on decisions.\n\n### **Less important:**\n\n*   **Improving the Artificial Wisdom Wikipedia article**: This could be a good project for learning more about the area, but the impact of this is quite unclear, with as AI models potentially make a good Wikipedia article less important.\n*   **Running a traditional empirical experiment on AI wisdom:** I expect this will be valuable down the line. Right now, subjective research provides more bits, but when we get to the point of having to try to decide between competing schools of thought, we'll need something more rigorous to differentiate. If you’re especially keen on traditional empirical research, you might just want to focus on other areas of AI safety for now.\n\n### **Related Project Lists:**\n\nList of projects for related areas:\n\n• [AI Impacts Essay Competition](https://blog.aiimpacts.org/p/essay-competition-on-the-automation#footnote-anchor-1-143374374): Covers the automation of philosophy and wisdom.\n\n• [Fellowship on AI for Human Reasoning - Future of Life Foundation](https://www.flf.org/fellowship): AI tools for coordination and epistemics\n\n• [AI for Epistemics - Benjamin Todd](https://80000hours.org/2024/05/project-idea-ai-for-epistemics/) \\- He writes: \"The ideal founding team would cover the bases of: (i) forecasting / decision-making expertise (ii) AI expertise (iii) product and entrepreneurial skills and (iv) knowledge of an initial user-type. Though bear in mind that if you have a gap in one of these areas now, you could probably fill it within a year\" and then provides a list of projects.\n\n• [Project ideas: Epistemics - Forethought](https://www.forethought.org/research/project-ideas-epistemics) \\- focuses on improving epistemics in general, not just AI solutions.\n\n[^lcc7gtz365j]: I tried to credit whoever came up with an idea, but I only started this midway through writing the post, so I've probably failed to properly credit some people. \n\n[^mr98rtn3rwn]: I've selected projects that I believe to have positive EV and which I robustly believe to have non-negative EV. \n\n[^y645fhft5sl]: Plans are useless, but planning is everything. I believe that something similar often applies to conceptual work. We may not be able to resolve all of the questions, but it's better to at least think about them. \n\n[^ldjhsd9rg0k]: Suggested by Richard Kroon \n\n[^qaaj90hnbuf]: Suggested by Richard Kroon \n\n[^x474rb4rvvp]: Suggested by Richard Kroon \n\n[^twv9htwvaj]: From the AI Impact Competition, see the ecosystems section of the suggested questions for more detail \n\n[^j11mumi264a]: From the Fellowship on AI for Human Reasoning list \n\n[^wvv8ex4aj2b]: Similar to a suggestion in a Forethought post, but focusing on wisdom rather than epistemics",
      "plaintextDescription": "This is a list of projects[1] to consider for folks who want to use Wise AI to steer the world towards positive outcomes.\n\nSome of these projects are listed because they're impactful. Others are listed because I believe they would be good projects for someone to get started. \n\nPlease note that this post is titled \"potentially useful projects\" for a reason. Some of these projects are likely to have much higher impact than others. Whilst I've really tried to avoid projects with net-negative EV, it's entirely possible that a few would be anyway[2]. Please don't ignore your own judgment just because I've listed a project!\n\nI'm sure my views about what projects are valuable will change quite significantly as I dive deeper into the topic, but I still think it's worthwhile for me to put some kind of list out there. \n\n----------------------------------------\n\n\nField-building:\nAt this stage, I consider field-building work around Wise AI as especially high priority. I feel that there’s starting to be some real energy around this area, however, very little of this will aid us in avoiding catastrophic risks. Fields tend to be more flexible in their initial stages, but gradually settle into an orthodoxy. It's important to intervene when it's still possible to make a difference:\n\n * Prioritisation research: Wisdom can mean a lot of different things; there are multiple types of wisdom. It seems quite important to try to identify priorities whilst the field is still fluid and easier to influence via this kind of work. One key way AI safety analysis will differ from that of people outside the field is that AI safety analysis is much more likely to take into account the externalities of enabling a capability, as opposed to naively ignoring this. Whilst this work is high priority, you may want to refrain unless you think it’s a particularly good fit for your skillset, given the difficulty of this work. It’s very easy to miss a crucial consideration that breaks your analysis and maybe ",
      "wordCount": 1611
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "gqder9YeBYLspyaqo",
    "title": "Reflections on AI Wisdom, plus announcing Wise AI Wednesdays",
    "slug": "reflections-on-ai-wisdom-plus-announcing-wise-ai-wednesdays",
    "url": null,
    "baseScore": 18,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-05-29T07:13:39.508Z",
    "contents": {
      "markdown": "I recently finished leading an [*AI Safety Camp*](https://www.aisafety.camp/) project on *Wise AI Advisors*[^tj58loenbf] (my team included *Chris Cooper*, *Matt Hampton*, and *Richard Kroon*). Since we want to share our work in an orderly fashion, I’m launching *Wise AI Wednesdays*. Each Wednesday[^60dn8nshtha], I (or one of my teammates) will be sharing a post, initially drawn from our *AI Safety Camp* outputs, but later including shifting to include future work and outputs summaries or commentary on related research. I’m hoping that a regular posting schedule will help cultivate *Wise AI/Wise AI Advisors* as a subfield of *AI Safety*.\n\nThis inaugural post provides an update on how my views on AI and wisdom have changed since I won 3rd prize in the [*Automation of Wisdom and Philosophy Competition*](https://www.lesswrong.com/posts/hiuTzNBqG2EYg6qM5/winners-of-the-essay-competition-on-the-automation-of-wisdom). My views have evolved considerably since then, so I thought it was worth listing a number of updates I've made:\n\n*   **Broadening My Focus**: I had originally labelled my research direction as [*Wise AI Advisors via Imitation Learning*](https://aiimpacts.org/an-overview-of-obvious-approaches-to-training-wise-ai-advisors/). While I still see this as a particularly promising approach, I’m now interested in [*Wise AI Advisors*](https://www.lesswrong.com/posts/SbAofYCgKkaXReDy4/chris_leong-s-shortform?commentId=Zcg9idTyY5rKMtYwo) *more broadly*. Many different reasonable-sounding paths exist; it would be arrogant to believe I’ve found the One True Path without much more investigation. I’m now more inclined to follow a 'let a thousand flowers bloom' path.\n*   **More Favourable Funding Landscape**: There seems to be more desire to fund work adjacent to this space than I had anticipated. For example: [*Cosmos x FIRE Truth-seeking Round*](https://cosmosgrants.org/truth) and [*Fellowship on AI for Human Reasoning*](https://www.flf.org/fellowship). I initially thought significant effort would be needed to convince funders, but it now seems quite possible to secure funding for Wise AI projects, even though you might have to choose a project that intersects with a funder's interest level.\n*   **Increased Emphasis on Field-Building**: Consequently, my focus has shifted from solely direct research progress to a combination of research and field-building. While I'm still defining what this entails, activities like *establishing the case for Wise AI*, *exploring theories of impact*, and *identifying concrete, high-priority projects* seem important right now.\n*   **Expanded View of Possible Contributions**: My ideas about what kind of work might be useful have broadened significantly thanks to AI Safety Camp and conversations with others. I now see a much wider range of ways people can contribute (see my [draft post with project proposals](https://docs.google.com/document/d/11i9FpvLkj9TVH33I8t8AwUPwErCZSVw38Zr24Zg_Gas/edit?tab=t.0)).\n*   **Importance of Simultaneous Pursuit of Both Theory and Practice: **I now believe it’s crucial to have people pursuing both theory and practice simultaneously and collaboratively. While it can be tempting to delay attempting to implement anything until all the theoretical building blocks are in place, if you spend all your time in ‘theoryland’, it’s extremely easy to overlook pragmatic challenges—challenges that could easily turn out to be make-or-break.\n*   **Subjective Research as Underrated:** I feel that exploratory, subjective research is often underrated—especially in the early stages of a field. Initial exploration needs a 'lay of the land', which traditional scientific research can struggle with because it’s hard to measure many bits[^5a5srzecu5]. At first, bandwidth matters more than rigor—but the balance should shift as the field matures.\n*   **Exploration vs Critique**: I’ve recently been wondering whether research into a new sub-field is best accomplished with explicit phases (Obviously, past a certain point, such coordination might be impossible, but I’m discussing the ideal rather than pragmatics):\n    *   ***Phase 1: Exploration**:* Focus on [opening up an area of exploration](https://www.lesswrong.com/posts/R6M4vmShiowDn56of/butterfly-ideas), with the role of critique mainly being about what to prioritise.\n    *   ***Phase 2: Critique*****: **Once reasonable proposals exist, in-depth criticism becomes vital to identify any potentially fatal flaws.\n    *   ***Phase 3: Rebuilding/repair*****:** Attempting to fix or patch any flaws exposed in the previous phase. Optimism is important in this phase to avoid prematurely ruling out potential fixes.\n    *   ***Phase 4 and further*****:** Continue alternating between critique and repair.\n*   **The Value of Dot Points**: I wasn’t originally planning to continue writing up more bulleted lists, but I'm now much more willing to stand behind the value of this format in particular circumstances. They are useful as previews, help get information out quickly, and suit posts like this one that provide multiple quick updates rather than a single, in-depth topic.\n\nThanks for reading and see you next Wednesday!\n\n(PS. if you have a draft post that you'd like to be shared as part of our posting schedule, feel free to reach out).\n\n**Previous posts:**\n\n*   [Summary: \"Imagining and building wise machines: The centrality of AI metacognition\" by Johnson, Karimi, Bengio, et al.](https://www.lesswrong.com/posts/euAMyQAQWTYyWZW8Z/summary-imagining-and-building-wise-machines-the-centrality)\n*   My submission to the [AI Impacts Essay Competition on the Automation of Wisdom and Philosophy](https://blog.aiimpacts.org/p/winners-of-the-essay-competition):\n    *   [An Overview of “Obvious” Approaches to Training Wise AI Advisors](https://aiimpacts.org/an-overview-of-obvious-approaches-to-training-wise-ai-advisors/)\n    *   [Some Preliminary Notes on the Promise of a Wisdom Explosion](https://aiimpacts.org/some-preliminary-notes-on-the-promise-of-a-wisdom-explosion/)\n\n[^tj58loenbf]: Originally Wise AI Advisors via Imitation Learning. As I say further down, I pivoted towards “let a thousand flowers bloom” rather than one particular approach. \n\n[^60dn8nshtha]: It is still Wednesday for US night owls. \n\n[^5a5srzecu5]: That said, it might be worthwhile doing some more objective research earlier on if the acquisition cost of knowledge is low enough or if you’re worried that once schools of thought form, people won’t update on empirical evidence.",
      "plaintextDescription": "I recently finished leading an AI Safety Camp project on Wise AI Advisors[1] (my team included Chris Cooper, Matt Hampton, and Richard Kroon). Since we want to share our work in an orderly fashion, I’m launching Wise AI Wednesdays. Each Wednesday[2], I (or one of my teammates) will be sharing a post, initially drawn from our AI Safety Camp outputs, but later including shifting to include future work and outputs summaries or commentary on related research. I’m hoping that a regular posting schedule will help cultivate Wise AI/Wise AI Advisors as a subfield of AI Safety.\n\nThis inaugural post provides an update on how my views on AI and wisdom have changed since I won 3rd prize in the Automation of Wisdom and Philosophy Competition. My views have evolved considerably since then, so I thought it was worth listing a number of updates I've made:\n\n * Broadening My Focus: I had originally labelled my research direction as Wise AI Advisors via Imitation Learning. While I still see this as a particularly promising approach, I’m now interested in Wise AI Advisors more broadly. Many different reasonable-sounding paths exist; it would be arrogant to believe I’ve found the One True Path without much more investigation. I’m now more inclined to follow a 'let a thousand flowers bloom' path.\n * More Favourable Funding Landscape: There seems to be more desire to fund work adjacent to this space than I had anticipated. For example: Cosmos x FIRE Truth-seeking Round and Fellowship on AI for Human Reasoning. I initially thought significant effort would be needed to convince funders, but it now seems quite possible to secure funding for Wise AI projects, even though you might have to choose a project that intersects with a funder's interest level.\n * Increased Emphasis on Field-Building: Consequently, my focus has shifted from solely direct research progress to a combination of research and field-building. While I'm still defining what this entails, activities like establishing the case ",
      "wordCount": 792
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "WbBe7LKNwv7fBgqii",
    "title": "AI Safety & Entrepreneurship v1.0",
    "slug": "ai-safety-and-entrepreneurship-v1-0",
    "url": null,
    "baseScore": 16,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-04-26T14:37:11.037Z",
    "contents": {
      "markdown": "+++ Why did I create both a post and a wiki article?\n\nPosts are best for making sure people see the initial version of the post, whilst Wiki articles are best for long-term maintenance. Posting an article that links to a Wiki page provides the best of both worlds.\n\n\n\n\n+++\n\n### **For the most up-to-date version, see the** [**Wiki page**](https://www.lesswrong.com/w/ai-safety-and-entrepreneurship)**.**\n\n### **Articles**:\n\n[There should be more AI safety organisations](https://www.lesswrong.com/posts/MhudbfBNQcMxBBvj8/there-should-be-more-ai-safety-orgs)\n\n[Why does the AI Safety Community need help founding projects?](https://www.lesswrong.com/posts/tPjAgWpsQrveFECWP/ryan-kidd-s-shortform?commentId=JDcp5AhWwk9ZCv59r)\n\n[AI Assurance Tech Report](https://www.aiat.report/)\n\n[AI Safety as a YC Startup](https://www.lesswrong.com/posts/QxJFjqT6oFY3jo47s/ai-safety-as-a-yc-startup-1)\n\n[Alignment can be the ‘clean energy’ of AI](https://www.lesswrong.com/posts/irxuoCTKdufEdskSk/alignment-can-be-the-clean-energy-of-ai)\n\n[AI Tools for Existential Security](https://forum.effectivealtruism.org/posts/6m6Q7jgB8iwXGYhXw/ai-tools-for-existential-security)\n\n### **Incubation Programs:**\n\n[Def/acc at Entrepreneur First](https://www.joinef.com/posts/introducing-def-acc-at-ef/) \\- this is a new program, which focuses on \"defensive\" tech, which will produce a wide variety of startups, some of which may help with reducing x-risk, but others may include AI applications like autonomous weapons or diagnostic medical software, which, while they generally won't increase x-risk, may not reduce it either.\n\n[Catalyze - AI Safety Incubation](https://www.catalyze-impact.org/apply)\n\n[Seldon Accelerator](https://seldonaccelerator.com/): “We take on AGI risk with optimism”\n\n[AE Studio](https://docs.google.com/forms/d/e/1FAIpQLSeU1_dZapdxH5SIooC3VK7nbnKSCZxxGG1JuqeNDpTOa22xbA/viewform) has expressed interest in helping seed neglected approaches, incl. ideas that take the form of business ideas\n\n### **AIS Friendly General Program:**\n\n[Founding to Give](https://forum.effectivealtruism.org/posts/q2PwXNsXsfDYkxeHb/aim-ce-new-program-founding-to-give-apply-now-to-launch-a): General EA incubator\n\n[Entrepreneur First](https://www.joinef.com/) \\- They run def/acc, so this could be a decent alternative\n\n[Halycon Futures](https://halcyonfutures.org/): “We’re an entrepreneurial nonprofit and VC fund dedicated to making AI safe, secure and good for humanity.” They do new project incubation, grants, investments, ect.\n\n### **Communities**:\n\n[Entrepreneurship Channel EA Anyway](https://eavirtualmeetupgroup.slack.com/archives/C04T8LBDZUN)\n\nAI Safety Founders: [Website](https://aisfounders.com/), [Discord](https://discord.gg/2dvYYxqXvS), [LinkedIn](https://www.linkedin.com/company/ai-safety-founders/)\n\n### **VC:**\n\n[AI Safety Seed Funding Network](https://forum.effectivealtruism.org/posts/oTuesHYxdXZm76d5k/ai-safety-seed-funding-network-join-as-a-donor-or-investor)\n\n[Lionheart Ventures](https://www.lionheart.vc/) \\- “We invest in the wise development of transformative technologies.”\n\n[Juniper Ventures](https://juniperventures.xyz/) \\- Invests in companies “working to make AI safe, secure and beneficial for humanity”. Published the AI Assurance Tech Report\n\n[Polaris Ventures](https://polaris-ventures.org/) \\- “We support projects and people aiming to build a future guided by wisdom and compassion for all”\n\n[Mythos Ventures](https://www.mythos.vc/) \\- “an early-stage venture capital firm investing in prosocial technologies for the transformative AI era.”\n\n[Metaplanet](https://metaplanet.com/) \\- “We support novel and evidence-based innovation that could produce an outsized return for the benefit of humankind.”\n\n[Anthology Fund](https://menlovc.com/anthology-fund/) \\- Backed by Anthropic. One of the five key areas is “trust and safety tooling”\n\n[Menlo Ventures](https://menlovc.com/) \\- Lead the GoodFire round ([article](https://menlovc.com/perspective/leading-goodfires-50m-series-a-to-interpret-how-ai-models-think/))\n\n[Safe AI Fund](https://www.saif.vc/)\n\n### **Organisational Support:**\n\n[Ashgro](https://www.ashgro.org/) \\- fiscal sponsorship\n\n[Rethink Priorities Special Projects](https://rethinkpriorities.org/our-services/special-projects/) \\- provides fiscal sponsorship or incubation\n\n### **Other:**\n\n[Hackathon for AI Safety Startups](https://www.apartresearch.com/event/ais-startup-hackathon) \\- Ran once by Apart Research, may run again\n\n[Constellation Residency](https://www.constellation.org/programs/residency) \\- Year-long position\n\n[Nonlinear](https://www.nonlinear.org/) \\- Free coaching for people who are running an AI safety startup or who are considering starting one\n\n### **Dustbin - Things that previously existed**\n\n[Longtermist Entrepreneurship Project](https://forum.effectivealtruism.org/topics/longtermist-entrepreneurship-fellowship) - [Retrospective](https://forum.effectivealtruism.org/topics/longtermist-entrepreneurship-fellowship)",
      "plaintextDescription": "Why did I create both a post and a wiki article?\n\nPosts are best for making sure people see the initial version of the post, whilst Wiki articles are best for long-term maintenance. Posting an article that links to a Wiki page provides the best of both worlds.\n\n\nFor the most up-to-date version, see the Wiki page.\n\n\nArticles:\nThere should be more AI safety organisations\n\nWhy does the AI Safety Community need help founding projects?\n\nAI Assurance Tech Report\n\nAI Safety as a YC Startup\n\nAlignment can be the ‘clean energy’ of AI\n\nAI Tools for Existential Security\n\n\nIncubation Programs:\nDef/acc at Entrepreneur First - this is a new program, which focuses on \"defensive\" tech, which will produce a wide variety of startups, some of which may help with reducing x-risk, but others may include AI applications like autonomous weapons or diagnostic medical software, which, while they generally won't increase x-risk, may not reduce it either.\n\nCatalyze - AI Safety Incubation\n\nSeldon Accelerator: “We take on AGI risk with optimism”\n\nAE Studio has expressed interest in helping seed neglected approaches, incl. ideas that take the form of business ideas\n\n\nAIS Friendly General Program:\nFounding to Give: General EA incubator\n\nEntrepreneur First - They run def/acc, so this could be a decent alternative\n\nHalycon Futures: “We’re an entrepreneurial nonprofit and VC fund dedicated to making AI safe, secure and good for humanity.” They do new project incubation, grants, investments, ect.\n\n\nCommunities:\nEntrepreneurship Channel EA Anyway\n\nAI Safety Founders: Website, Discord, LinkedIn\n\n\nVC:\nAI Safety Seed Funding Network\n\nLionheart Ventures - “We invest in the wise development of transformative technologies.”\n\nJuniper Ventures - Invests in companies “working to make AI safe, secure and beneficial for humanity”. Published the AI Assurance Tech Report\n\nPolaris Ventures - “We support projects and people aiming to build a future guided by wisdom and compassion for all”\n\nMythos Ventures - “an earl",
      "wordCount": 451
    },
    "tags": [
      {
        "_id": "GQyPQcdEQF4zXhJBq",
        "name": "List of Links",
        "slug": "list-of-links"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "uKjezuHJJdeGqzPh5",
    "title": "What empirical research directions has Eliezer commented positively on?",
    "slug": "what-empirical-research-directions-has-eliezer-commented",
    "url": null,
    "baseScore": 8,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2025-04-15T08:53:41.677Z",
    "contents": {
      "markdown": "I'm interested in both work that he's commented on positively after the fact and any comments might have made on what directions are generally fruitful.",
      "plaintextDescription": "I'm interested in both work that he's commented on positively after the fact and any comments might have made on what directions are generally fruitful.",
      "wordCount": 25
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "axKyBatdWtce48Zda",
    "title": "Linkpost to a Summary of \"Imagining and building wise machines: The centrality of AI metacognition\" by Johnson, Karimi, Bengio, et al.",
    "slug": "linkpost-to-a-summary-of-imagining-and-building-wise",
    "url": null,
    "baseScore": 8,
    "voteCount": 1,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-04-10T11:54:37.484Z",
    "contents": {
      "markdown": "+++ **How does my summary compare to the original paper?**\n\nObviously, the summary is shorter :-).\n\nAdditionally, I have:\n\n• Streamlined the user journey with collapsable sections\n\n• Communicated some ideas in an easier to digest format\n\n• Added commentary and a glossary\n\nWhilst I've tried as hard as possible to represent the views in the original (including making some updates in response to feedback from one of the authors), I can't guarantee perfect accuracy.\n\n\n\n\n+++\n\n[`Original paper`](https://arxiv.org/pdf/2411.02478) [`Summary`](https://www.lesswrong.com/posts/euAMyQAQWTYyWZW8Z/summary-imagining-and-building-wise-machines-the-centrality)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/0af2cf57bf1ee14f21f91c5a9a6d65638a012827beb90014.png)\n\n**Authors of the Paper:**\n=========================\n\n[Samuel G. B. Johnson](https://arxiv.org/search/cs?searchtype=author&query=Johnson,+S+G+B), [Amir-Hossein Karimi](https://arxiv.org/search/cs?searchtype=author&query=Karimi,+A), [Yoshua Bengio](https://arxiv.org/search/cs?searchtype=author&query=Bengio,+Y), [Nick Chater](https://arxiv.org/search/cs?searchtype=author&query=Chater,+N), [Tobias Gerstenberg](https://arxiv.org/search/cs?searchtype=author&query=Gerstenberg,+T), [Kate Larson](https://arxiv.org/search/cs?searchtype=author&query=Larson,+K), [Sydney Levine](https://arxiv.org/search/cs?searchtype=author&query=Levine,+S), [Melanie Mitchell](https://arxiv.org/search/cs?searchtype=author&query=Mitchell,+M), [Iyad Rahwan](https://arxiv.org/search/cs?searchtype=author&query=Rahwan,+I), [Bernhard Schölkopf](https://arxiv.org/search/cs?searchtype=author&query=Sch%C3%B6lkopf,+B), [Igor Grossmann](https://arxiv.org/search/cs?searchtype=author&query=Grossmann,+I)\n\n**Abstract of Summarised Paper:**\n=================================\n\nRecent advances in artificial intelligence (AI) have produced systems capable of increasingly sophisticated performance on cognitive tasks. However, AI systems still struggle in critical ways: unpredictable and novel environments (robustness), lack of transparency in their reasoning (explainability), challenges in communication and commitment (cooperation), and risks due to potential harmful actions (safety). We argue that these shortcomings stem from one overarching failure: AI systems lack wisdom.\n\n Drawing from cognitive and social sciences, we define wisdom as the ability to navigate intractable problems - those that are ambiguous, radically uncertain, novel, chaotic, or computationally explosive - through effective task-level and metacognitive strategies. While AI research has focused on task-level strategies, metacognition - the ability to reflect on and regulate one's thought processes - is underdeveloped in AI systems. In humans, metacognitive strategies such as recognizing the limits of one's knowledge, considering diverse perspectives, and adapting to context are essential for wise decision-making. We propose that integrating metacognitive capabilities into AI systems is crucial for enhancing their robustness, explainability, cooperation, and safety.\n\nBy focusing on developing wise AI, we suggest an alternative to aligning AI with specific human values - a task fraught with conceptual and practical difficulties. Instead, wise AI systems can thoughtfully navigate complex situations, account for diverse human values, and avoid harmful actions. We discuss potential approaches to building wise AI, including benchmarking metacognitive abilities and training AI systems to employ wise reasoning. Prioritizing metacognition in AI research will lead to systems that act not only intelligently but also wisely in complex, real-world situations.\n\n**Why I wrote this summary: **\n==============================\n\nFirstly, I thought the framing of metacognition as a key component of wisdom missing from current AI systems was insightful and the resulting analysis fruitful.\n\nSecondly, this paper contains some ideas similar to those I discussed in [Some Preliminary Notes on the Promise of a Wisdom Explosion](https://aiimpacts.org/some-preliminary-notes-on-the-promise-of-a-wisdom-explosion/). In particular, the authors talk about a \"virtuous cycle\" in relation to wisdom in the final paragraphs:\n\n> By simultaneously promoting robust, explainable, cooperative, and safe AI, these qualities are likely to amplify one another:\n> \n> *   Robustness will facilitate cooperation (by improving confidence from counterparties in its long-term commitments) and safety (by avoiding novel failure modes[^uycflfu5vsl]).\n> *   Explainability will facilitate robustness (by making it easier to human users to intervene in transparent processes) and cooperation (by communicating its reasoning in a way that is checkable by counterparties).\n> *   Cooperation will facilitate explainability (by using accurate theory-of-mind about its users) and safety (by collaboratively implementing values shared within dyads, organizations, and societies).\n> \n> Wise reasoning, therefore, can lead to a virtuous cycle in AI agents, just as it does in humans. We may not know precisely what form wisdom in AI will take but it must surely be preferable to folly. \n\n[`Original paper`](https://arxiv.org/pdf/2411.02478) [`Summary`](https://www.lesswrong.com/posts/euAMyQAQWTYyWZW8Z/summary-imagining-and-building-wise-machines-the-centrality)\n\n+++ Thinking of Commenting?\n=======================\n\nI recommend following the link to the summary and commenting there so that the comments are all in the same place.\n\n\n\n\n+++",
      "plaintextDescription": "How does my summary compare to the original paper?\n\nObviously, the summary is shorter :-).\n\nAdditionally, I have:\n\n• Streamlined the user journey with collapsable sections\n\n• Communicated some ideas in an easier to digest format\n\n• Added commentary and a glossary\n\nWhilst I've tried as hard as possible to represent the views in the original (including making some updates in response to feedback from one of the authors), I can't guarantee perfect accuracy.\n\nOriginal paper Summary\n\n\nAuthors of the Paper:\nSamuel G. B. Johnson, Amir-Hossein Karimi, Yoshua Bengio, Nick Chater, Tobias Gerstenberg, Kate Larson, Sydney Levine, Melanie Mitchell, Iyad Rahwan, Bernhard Schölkopf, Igor Grossmann\n\n\nAbstract of Summarised Paper:\nRecent advances in artificial intelligence (AI) have produced systems capable of increasingly sophisticated performance on cognitive tasks. However, AI systems still struggle in critical ways: unpredictable and novel environments (robustness), lack of transparency in their reasoning (explainability), challenges in communication and commitment (cooperation), and risks due to potential harmful actions (safety). We argue that these shortcomings stem from one overarching failure: AI systems lack wisdom.\n\n Drawing from cognitive and social sciences, we define wisdom as the ability to navigate intractable problems - those that are ambiguous, radically uncertain, novel, chaotic, or computationally explosive - through effective task-level and metacognitive strategies. While AI research has focused on task-level strategies, metacognition - the ability to reflect on and regulate one's thought processes - is underdeveloped in AI systems. In humans, metacognitive strategies such as recognizing the limits of one's knowledge, considering diverse perspectives, and adapting to context are essential for wise decision-making. We propose that integrating metacognitive capabilities into AI systems is crucial for enhancing their robustness, explainability, cooperation, and saf",
      "wordCount": 607
    },
    "tags": [],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ouEaKDNkMrjxJHJtf",
    "title": "Biden administration unveils global AI export controls aimed at China",
    "slug": "biden-administration-unveils-global-ai-export-controls-aimed",
    "url": "https://www.axios.com/pro/tech-policy/2025/01/13/ai-chip-export-restrictions-nvidia-biden",
    "baseScore": 9,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-01-14T01:01:13.927Z",
    "contents": {
      "markdown": "Export restrictions on chips outside of twenty nations. Model weights above a certain size are restricted, with an exclusion for open-weight models.",
      "plaintextDescription": "Export restrictions on chips outside of twenty nations. Model weights above a certain size are restricted, with an exclusion for open-weight models.",
      "wordCount": 22
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ECs5wh2C9CDZB9A7o",
    "title": "Higher and lower pleasures",
    "slug": "higher-and-lower-pleasures",
    "url": null,
    "baseScore": 19,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2024-12-05T13:13:46.526Z",
    "contents": {
      "markdown": "I used to think that talk about more sophisticated forms of art providing \"higher forms of pleasure\" was mere pretentious, but meditation has shifted my view here by making me more conscious of how of things affect my experience.\n\nArt can do two things. It can provide immediate pleasure. This is all that \"disposable \" entertainment provides. Alternatively, it can shape the way you can make sense of the world.\n\nOne way it can do this is by providing you with a greater sense of purpose, that allows you to push through obstacles with less suffering. As an example, let's suppose you watch an inspirational story about someone who grinds at work (such as [the Pursuit of Happiness](https://www.rottentomatoes.com/m/pursuit_of_happyness)). Perhaps before you watch it, when you're at work, every few minutes you think, \"I hate my job, life is suffering, someone please shoot me\". Perhaps after that your work becomes meaningful and you no longer are pulled down by such thoughts.  \n  \nAnother example: there is a scene in [American Beauty](https://www.rottentomatoes.com/m/american_beauty) where Rick Fitts calls a scene with a plastic bag floating \"the most beautiful thing in the world\". We can imagine that this teaches someone to appreciate beauty in the everyday, beauty they would have previously overlooked.\n\nOver a longer period of time, you'd expect to increase your utility more by watching something that positively transforms the way that you experience the world than something that just provides immediate pleasure.\n\nNow, I'm not saying that *none* of this talk of higher and lower pleasures is pretentiousness, but I think there's something there. And it's easy to underestimate the value of these \"higher pleasures\" as the effect of these is less obvious than that immediate enjoyment.  \n  \n(This post is in response to the Astral Codex Ten post [Friendly And Hostile Analogies For Taste](https://www.astralcodexten.com/p/friendly-and-hostile-analogies-for)).",
      "plaintextDescription": "I used to think that talk about more sophisticated forms of art providing \"higher forms of pleasure\" was mere pretentious, but meditation has shifted my view here by making me more conscious of how of things affect my experience.\n\nArt can do two things. It can provide immediate pleasure. This is all that \"disposable \" entertainment provides. Alternatively, it can shape the way you can make sense of the world.\n\nOne way it can do this is by providing you with a greater sense of purpose, that allows you to push through obstacles with less suffering. As an example, let's suppose you watch an inspirational story about someone who grinds at work (such as the Pursuit of Happiness). Perhaps before you watch it, when you're at work, every few minutes you think, \"I hate my job, life is suffering, someone please shoot me\". Perhaps after that your work becomes meaningful and you no longer are pulled down by such thoughts.\n\nAnother example: there is a scene in American Beauty where Rick Fitts calls a scene with a plastic bag floating \"the most beautiful thing in the world\". We can imagine that this teaches someone to appreciate beauty in the everyday, beauty they would have previously overlooked.\n\nOver a longer period of time, you'd expect to increase your utility more by watching something that positively transforms the way that you experience the world than something that just provides immediate pleasure.\n\nNow, I'm not saying that none of this talk of higher and lower pleasures is pretentiousness, but I think there's something there. And it's easy to underestimate the value of these \"higher pleasures\" as the effect of these is less obvious than that immediate enjoyment.\n\n(This post is in response to the Astral Codex Ten post Friendly And Hostile Analogies For Taste).\n\n\n ",
      "wordCount": 301
    },
    "tags": [
      {
        "_id": "KDpqtN3MxHSmD4vcB",
        "name": "Art",
        "slug": "art"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "j8tLcLuN45EGrgknN",
    "title": "Linkpost: Rat Traps by Sheon Han in Asterisk Mag",
    "slug": "linkpost-rat-traps-by-sheon-han-in-asterisk-mag",
    "url": "https://asteriskmag.com/issues/08/rat-traps",
    "baseScore": 12,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2024-12-03T03:22:45.424Z",
    "contents": {
      "markdown": "Subtitle: Does the rationalist blogosphere need to update?",
      "plaintextDescription": "Subtitle: Does the rationalist blogosphere need to update?",
      "wordCount": 8
    },
    "tags": [
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "euAMyQAQWTYyWZW8Z",
    "title": "Summary: \"Imagining and building wise machines: The centrality of AI metacognition\" by Johnson, Karimi, Bengio, et al.",
    "slug": "summary-imagining-and-building-wise-machines-the-centrality",
    "url": "https://arxiv.org/pdf/2411.02478",
    "baseScore": 29,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2024-11-11T16:13:26.504Z",
    "contents": {
      "markdown": "<table style=\"border:4px solid #dcd0f2\"><tbody><tr><td style=\"background-color:#f2edfa;border-style:none\"><blockquote><p>We may not know precisely what form wise AI will take—but it must surely be preferable to folly.</p></blockquote></td></tr></tbody></table>\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/35f231f8947a13decff1eb1c127f2171fd1ecca678fb9b89.png)\n\nRead the full, original paper\n\n<table style=\"border:4px solid hsl(0, 0%, 0%)\"><tbody><tr><td style=\"border-color:hsl(0, 0%, 0%)\"><h2><strong><u>Paper Authors</u></strong>:</h2><p><a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Johnson,+S+G+B\">Samuel G. B. Johnson</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Karimi,+A\">Amir-Hossein Karimi</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Bengio,+Y\">Yoshua Bengio</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Chater,+N\">Nick Chater</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Gerstenberg,+T\">Tobias Gerstenberg</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Larson,+K\">Kate Larson</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Levine,+S\">Sydney Levine</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Mitchell,+M\">Melanie Mitchell</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Rahwan,+I\">Iyad Rahwan</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Sch%C3%B6lkopf,+B\">Bernhard Schölkopf</a>, <a href=\"https://arxiv.org/search/cs?searchtype=author&amp;query=Grossmann,+I\">Igor Grossmann</a></p><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><h2>Highlights</h2></summary><div class=\"detailsBlockContent\"><blockquote><p>• We examine the why and the how of building wise AI<br>• Wisdom helps humans to navigate intractable problems through object-level strategies (for managing problems) and metacognitive strategies (for managing object-level strategies)<br>• Wise AI, through improved metacognition, would be more robust to new environments, explainable to users, cooperative in pursuing shared goals, and safe in avoiding both prosaic and catastrophic failures<br>• We suggest several approaches to benchmarking wisdom, training wise reasoning strategies, and adapting AI architecture for metacognition&nbsp;</p></blockquote></div></details><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><h2>Abstract</h2></summary><div class=\"detailsBlockContent\"><blockquote><p>Recent advances in artificial intelligence (AI) have produced systems capable of increasingly sophisticated performance on cognitive tasks. However, AI systems still struggle in critical ways: unpredictable and novel environments (robustness), lack of transparency in their reasoning (explainability), challenges in communication and commitment (cooperation), and risks due to potential harmful actions (safety). We argue that these shortcomings stem from one overarching failure: AI systems lack wisdom.</p><p>&nbsp;Drawing from cognitive and social sciences, we define wisdom as the ability to navigate intractable problems - those that are ambiguous, radically uncertain, novel, chaotic, or computationally explosive - through effective task-level and metacognitive strategies. While AI research has focused on task-level strategies, metacognition - the ability to reflect on and regulate one's thought processes - is underdeveloped in AI systems. In humans, metacognitive strategies such as recognizing the limits of one's knowledge, considering diverse perspectives, and adapting to context are essential for wise decision-making. We propose that integrating metacognitive capabilities into AI systems is crucial for enhancing their robustness, explainability, cooperation, and safety.</p><p>By focusing on developing wise AI, we suggest an alternative to aligning AI with specific human values - a task fraught with conceptual and practical difficulties. Instead, wise AI systems can thoughtfully navigate complex situations, account for diverse human values, and avoid harmful actions. We discuss potential approaches to building wise AI, including benchmarking metacognitive abilities and training AI systems to employ wise reasoning. Prioritizing metacognition in AI research will lead to systems that act not only intelligently but also wisely in complex, real-world situations.</p></blockquote></div></details><details class=\"detailsBlock\"><summary class=\"detailsBlockTitle\"><h2>Notes on this summary</h2></summary><div class=\"detailsBlockContent\"><p>Some quotes have been reformatted (adding paragraph breaks, dot points, references changed to footnotes, ect.).</p><p>I've made a few minor edits to the quotes. For example, I remove the word \"secondly\" when I'm only quoting one element of a list and adjusting the grammar when converting sentences into a list.</p><p>Whilst I've tried as hard as possible to represent the views in the original (including making some updates in response to feedback from one of the authors), I can't guarantee perfect accuracy.</p><p>By default, assume that quotes (indicated by a grey bar on the left) are from the paper.</p></div></details></td></tr></tbody></table>\n\n<table style=\"border:4px solid #a3c3e6\"><tbody><tr><td style=\"background-color:#d8e9ff;border-style:none\"><h2><strong>Why I Wrote This Summary</strong></h2><p>Firstly, I thought the framing of metacognition as a key component of wisdom missing from current AI systems was insightful and the resulting analysis fruitful.</p><p>Secondly, this paper contains some ideas similar to those I discussed in <a href=\"https://aiimpacts.org/some-preliminary-notes-on-the-promise-of-a-wisdom-explosion/\">Some Preliminary Notes on the Promise of a Wisdom Explosion</a>. In particular, the authors talk about a \"virtuous cycle\" in relation to wisdom in the final paragraphs:</p><blockquote><p>By simultaneously promoting robust, explainable, cooperative, and safe AI, these qualities are likely to amplify one another:</p><ul><li>Robustness will facilitate cooperation (by improving confidence from counterparties in its long-term commitments) and safety (by avoiding novel failure modes<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"1\" data-footnote-id=\"uycflfu5vsl\" role=\"doc-noteref\" id=\"fnrefuycflfu5vsl\"><sup><a href=\"#fnuycflfu5vsl\">[1]</a></sup></span>).</li><li>Explainability will facilitate robustness (by making it easier to human users to intervene in transparent processes) and cooperation (by communicating its reasoning in a way that is checkable by counterparties).</li><li>Cooperation will facilitate explainability (by using accurate theory-of-mind about its users) and safety (by collaboratively implementing values shared within dyads, organizations, and societies).</li></ul><p>Wise reasoning, therefore, can lead to a virtuous cycle in AI agents, just as it does in humans.</p></blockquote></td></tr></tbody></table>\n\n* * *\n\nSummary:\n========\n\n**What is wisdom?**\n-------------------\n\n### *Examples of Human Wisdom:*\n\n> Willa’s children are bitterly arguing about money. Willa draws on her life experience to show them why they should instead compromise in the short term and prioritize their sibling relationship in the long term. \n> \n> \"Daphne is a world-class cardiologist. Nonetheless, she consults with a much more junior colleague when she recognizes that the colleague knows more about a patient’s history than she does\"\n> \n> \"Ron is a political consultant who formulates possible scenarios to ensure his candidate will win. To help generate scenarios, he not only imagines best case scenarios, but also imagines that his client has lost the election and considers possible reasons that might have contributed to the loss.\"\n\n+++ ### What are some theories of human wisdom?\n\nFor a more detailed account, see the table on page 5 of the paper.\n\n*Five component theories:*\n\n• Balance theory: \"Deploying knowledge and skills to achieve the common good by\"  \n• Berlin Wisdom Model: \"Expertise in important and difficult matters of life\"  \n• MORE Life Experience Model: \"Gaining psychological resources via reflection, to cope with life challenges\"  \n• Three-Dimensional Model: \"Acquiring and reflecting on life experience to cultivate personality traits\"  \n• Wise Reasoning Model: \"Using context-sensitive reasoning to manage important social challenges\"\n\n*Two consensus models*\n\n• Common wisdom model: \"A style of social-cognitive processing\" involving morality and metacognition  \n• Integrative Model: \"A behavioural repertoire\"\n\nThese consensus models attempt to find common themes.\n\n\n\n\n++++++ ### Would AI wisdom resemble human wisdom?\n\n*Potential differences:*\n\n• AIs have different computational constraints. Humans need to \"economize scarce cognitive resources\" which incentivizes us to use heuristics more.  \n• Humans exist in a society that allows us to \"outsource... cognition to the social environment\" such as through division of labor.\n\n*Reasons why human and AI wisdom might converge:*\n\n• Resource difference might be \"more a matter of degree than kind\"  \n• Heuristics are often about handling a lack of information rather than computational constraints  \n• AI's might \"join our (social) milieu\"\n\n\n\n\n+++\n\n### **Definition of wisdom**[^yb717et60ai]**:**\n\n<table style=\"border:4px solid #dcd0f2\"><tbody><tr><td style=\"background-color:#f2edfa;border-style:none\"><blockquote><p>If life were a series of textbook problems, we would not need to be wise. There would be a correct answer, the requisite information for calculating it would be available, and natural selection would have ruthlessly driven humans to find those answers.</p></blockquote></td></tr></tbody></table>\n\n<table style=\"background-color:#a3c3e6;border:4px solid #a3c3e6\"><tbody><tr><td style=\"background-color:#d8e9ff\"><blockquote><p>We define wisdom functionally as the ability to successfully navigate intractable problems— those that do not lend themselves to analytic techniques due to unlearnable probability distributions or incommensurable values<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"p4mlpkwe8yk\" role=\"doc-noteref\" id=\"fnrefp4mlpkwe8yk\"><sup><a href=\"#fnp4mlpkwe8yk\">[3]</a></sup></span>.</p></blockquote></td></tr></tbody></table>\n\n+++ ### What kinds of intractable problems?\n\n> **Incommensurable:** It features ambiguous goals or values that cannot be compared with one another[^7vsqtj26zvd].  \n> **Transformative:** The outcome of the decision might change one’s preferences, leading to a clash between one’s present and future values  \n> **Radically uncertain**. We might not be able to exhaustively list the possible outcomes or assign probabilities to them in a principled way[^t9wlmimd4h8].  \n> **Chaotic**. The data-generating process may have a strong nonlinearity or dependency on initial conditions, making it fundamentally unpredictable[^47se0ry9yj9][^pjfiptu4ac].  \n> **Non-stationary**. The underlying process may be changing over time, making the probability distribution unlearnable.  \n> **Out-of-distribution**. The situation is novel, going beyond one’s experience or available data.  \n> **Computationally explosive**. The optimal response could be calculated with infinite or infeasibly large computational resources, but this is not possible due to resource constraints\n\n\n\n\n+++\n\nThis seems like a reasonable definition to use, though I have to admit I find the term \"intractable problems\" to be a bit strong for the examples they provided. For example, Daphne putting aside her ego to consult a junior colleague doesn't quite match what I'd describe as overcoming an \"intractable\" problem[^rxmhhh05aqh].\n\n### **Two types of strategies for managing this:**\n\n<table style=\"background-color:#a3c3e6;border:4px solid #a3c3e6\"><tbody><tr><td style=\"background-color:#d8e9ff\"><p><strong>1) Task-level strategies</strong> (\"used to manage the problem itself\") such as heuristics or narratives.</p><p><strong>2) Metacognitive strategies</strong> (\"used to flexibly manage those task-level strategies\")<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"9\" data-footnote-id=\"sluio9vrpt7\" role=\"doc-noteref\" id=\"fnrefsluio9vrpt7\"><sup><a href=\"#fnsluio9vrpt7\">[9]</a></sup></span></p></td></tr></tbody></table>\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/235d9947a9c8e9ae959bc247c7f444c6e0573a579303267f.png)\n\nThey argue that although AI has made lots of progress with task-level strategies, it often neglects metacognitive strategies[^zgvv6onvhr]. For this reason, their paper focuses on the latter.\n\n+++ ### Why do the authors believe current AI falls short in metacognition?\n\nThe authors provide some specific examples of where they believe AI systems fall short:  \n  \n1) Struggling to understand their goals (“mission awareness”[^o7wnadtsmiq])  \n2) Exhibiting overconfidence[^v1ihrkj71hc]  \n3) Failing to appreciate the limits of their capabilities and context (e.g., stating they can access real-time information or take actions in the physical world[^o7wnadtsmiq])  \n  \nThey label this \"metacognitive myopia\"[^796pi2yx9no].\n\n\n\n\n+++\n\n**Why build wise AI?**:\n-----------------------\n\n<table style=\"border:4px solid #dcd0f2\"><tbody><tr><td style=\"background-color:#f2edfa\"><blockquote><p>First, it is not clear what the alternative is. Compared to halting all progress on AI, building wise AI may introduce added risks alongside added benefits. But compared to the status quo—advancing task-level capabilities at a breakneck pace with little effort to develop wise metacognition—the attempt to make machines intellectually humble, context-sensitive, and adept at balancing viewpoints seems clearly preferable.&nbsp;</p></blockquote></td></tr></tbody></table>\n\n### **Concrete Benefits**\n\n<table style=\"background-color:hsl(210, 76%, 92%);border:4px solid #a3c3e6\"><tbody><tr><td>&nbsp;</td><td><strong>Why the authors argue that wise AI might provide this benefit</strong></td></tr><tr><td><strong>Robustness</strong></td><td><p><strong>Reliability over similar inputs</strong>: it'd be unwise to choose \"excessively inconsistent\" strategies:</p><p>- <i>Comment</i>:<strong> </strong>I guess? Unclear how strong we should expect that effect to be though.</p><p><strong>Bias</strong>: identifying deficiencies in the data and either gathering more data or correcting for that bias</p><p><strong>Inflexibility</strong>: adjusting its confidence based on the situation</p></td></tr><tr><td><strong>Co-operation</strong><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"14\" data-footnote-id=\"oddfc59v0as\" role=\"doc-noteref\" id=\"fnrefoddfc59v0as\"><sup><a href=\"#fnoddfc59v0as\">[14]</a></sup></span></td><td><p><i>Slightly edited quotes</i></p><p>&nbsp;</p><p><strong>Resolving conflicts among (object-level) strategies</strong>: e.g., when accuracy cues diverge<br><br><strong>Assessing the appropriateness (of object-level strategies)</strong>: e.g., whether one can evaluate a chain of argumentation</p><p>&nbsp;</p><p><strong>Seeking appropriate inputs: </strong>e.g., knowing the capabilities of the other counterparty</p><p>&nbsp;</p><p>This last point is particularly important for cooperative AI, which could overestimate the abilities of humans or lack common ground such as a shared emotional system.</p></td></tr><tr><td><strong>Safety</strong></td><td><p>The authors argue that wise reasoning provides an alternative to aligning AI to values<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"15\" data-footnote-id=\"w7j28jx8a\" role=\"doc-noteref\" id=\"fnrefw7j28jx8a\"><sup><a href=\"#fnw7j28jx8a\">[15]</a></sup></span><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"16\" data-footnote-id=\"mjjqj7s5neq\" role=\"doc-noteref\" id=\"fnrefmjjqj7s5neq\"><sup><a href=\"#fnmjjqj7s5neq\">[16]</a></sup></span>:</p><blockquote><p>e.g., \"one object-level strategy may be a bias toward inaction (not executing an action if it risks harm according to one of several possibly conflicting human norms), which in turn requires metacognitive regulation (learning what those conflicting perspectives are and avoiding overconfidence)\"</p></blockquote><p>However, they argue that this isn't sufficient, as it doesn't address all the social questions of alignment, both in terms of design decisions (\"<i>Who</i> should we align AI to? Should we increase the average human well-being, its sum, or care for the whole biosphere? Why assume today's values are the right ones?\") and how these AI systems fit into a broader society (specifically how they can be channeled by institutions like governments and markets to allow our values to evolve towards a \"shared reflective equilibrium\".</p></td></tr><tr><td><strong>Explainability</strong></td><td>Metacognition seem to play a role in assisting humans to justifying their decisions. Presumably it should assist with helping AI to explain its decisions as well<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"17\" data-footnote-id=\"facv0pq75sc\" role=\"doc-noteref\" id=\"fnreffacv0pq75sc\"><sup><a href=\"#fnfacv0pq75sc\">[17]</a></sup></span>?</td></tr></tbody></table>\n\n### **Comparison to Alignment**:\n\nThey identify three main conceptual problems for alignment:\n\n1.  Humans don't uniformly prioritize following norms[^wojhuchi41e]\n2.  Norms vary sharply across cultures\n3.  Even if norms were uniform, they may not be morally correct\n\n<table style=\"border:4px solid #dcd0f2\"><tbody><tr><td style=\"background-color:#f2edfa\"><blockquote><p>Given these conceptual problems, alignment may not be a feasible or even desirable engineering goal. The fundamental challenge is how AI agents can live among us—and for this, implementing wise AI reasoning may be a more promising approach. Aligning AI systems to the right metacognitive strategies rather than to the “right” values might be both conceptually cleaner and more practically feasible.</p></blockquote></td></tr></tbody></table>\n\nIt seems plausible that there might be more agreement on meta-cognitive strategies than values, however, I still expect there to be sufficient disagreement to make this a challenge.\n\n+++ ### Inaction example\n\n\"Task-level strategies may include heuristics such as a bias toward inaction: When in doubt about whether a candidate action could produce harm according to one of several possibly conflicting human norms, by default do not execute the action. Yet wise metacognitive monitoring and control will be crucial for regulating such task-level strategies. In the ‘inaction bias’ strategy, for example, a requirement is to learn what those conflicting perspectives are and to avoid overconfidence\"\n\n\n\n\n++++++ ### Possible effects on instrumental convergence\n\nIn the final section they suggest that building machines wiser than humans might prevent instrumental convergence[^e35uvf4ig9b] as \"empirically, humans with wise metacognition show greater orientation toward the common good\". I have to admit skepticism as I believe in the orthogonality thesis and I see no reason to believe it wouldn't apply to wisdom as well. That said, activating latents that improve wisdom might also improve alignment, even if it is far from a complete solution.\n\n\n\n\n++++++ ### Further comments on alignment\n\n\"With respect to the broader goal of AI alignment, we are sympathetic to the goal but question this definition of the problem. Ultimately safe AI may be at least as much about constraining the power of AI systems within human institutions, rather than aligning their goals\"\n\n\n\n\n+++\n\n**Benchmarking Wisdom**:\n------------------------\n\nThe paper discusses potential for benchmarking AI wisdom. They seem to be in favor of starting with tasks that measure wise reasoning in humans and scoring their reflections based on predefined criteria. It's worth noting that these criteria can be about reasoning processes rather than the outcome they reach.\n\nThis could potentially be fruitful, however, I do worry that it might be fairly easy for AI's to learn to [Goodhart](https://www.splunk.com/en_us/blog/learn/goodharts-law.html) here - apply metacognition in a way that is fairly shallow, but sufficient to satisfy the human raters.\n\nThe author may not be in disagreement: whilst they see benchmarking as a \"crucial start\" they also assert that \" there is no substitute for interaction with the real world\". This leads them to suggest a slow rollout to give us time to evaluate whether their decisions really were wise.\n\nOne worry I have is that sometimes wisdom involves just knowing what to do without being able to explain it. This might be problematic for attempts to evaluate wisdom by evaluating the wisdom of a person's reasoning[^fdxex7tq5m6].\n\n+++ ### Challenges with benchmarks\n\nMemorization: Benchmark results can be inflated by memorizing patterns in a way that doesn't generalize outside of the training distribution\n\nEvaluating the process is hard: They claim wisdom depends on the underlying reasoning rather than just success[^q6hthkp0onr]. Reasoning is harder to evaluate than the correct answer.\n\nProducing a Realistic Context: It may be challenging to produce artificial examples as the AI might have access to much more information in the real world \n\n\n\n\n+++\n\n**Building Wise AI**:\n---------------------\n\n<table style=\"background-color:#d8e9ff;border:4px solid #a3c3e6\"><tbody><tr><td style=\"border-style:none\"><blockquote><p><i>Proposal A</i><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"22\" data-footnote-id=\"3f6nrflqrfk\" role=\"doc-noteref\" id=\"fnref3f6nrflqrfk\"><sup><a href=\"#fn3f6nrflqrfk\">[22]</a></sup></span><i>&nbsp;(two training steps):&nbsp;</i></p><ol><li>Train a model for wise strategy selection directly (e.g., to correctly identify when to be intellectually humble)</li><li>Train them to use those strategies correctly (e.g., to carry out intellectual humble behavior).</li></ol><p><i>Proposal B (one training step):</i></p><ol><li>Evaluate whether models are able to plausibly explain their metacognitive strategies in benchmark cases</li><li>If this is the case, then simultaneously train strategies and outputs (e.g., training the model to identify the situation as one that calls for intellectual humility and to reason accordingly<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"23\" data-footnote-id=\"m9pqokpln3p\" role=\"doc-noteref\" id=\"fnrefm9pqokpln3p\"><sup><a href=\"#fnm9pqokpln3p\">[23]</a></sup></span>).</li></ol></blockquote></td></tr></tbody></table>\n\n**Final Thoughts**\n==================\n\nThe authors address the point that attempting to build wise AI  have negative externalities either due to malicious use or due to the project failing. In response they write:\n\n> If the alternative were halting all AI progress, building wise AI would introduce added risks. But compared to the status quo—advancing capabilities at a breakneck pace without wise metacognition—the attempt to make machines intellectually humble, context-adaptable, and adept at balancing viewpoints seems clearly preferable. \n\nOne final point of difference I'd like to mention: The authors seem to primarily imagine wise AIs acting directly in the world[^zr5bxcicrcs]. In contrast, my primary interest is in [wise AI advisors](https://www.lesswrong.com/posts/SbAofYCgKkaXReDy4/chris_leong-s-shortform?view=postCommentsNew&postId=SbAofYCgKkaXReDy4&commentId=Zcg9idTyY5rKMtYwo) working in concert with humans.\n\n+++ ### Why am I focused specifically on wise AI advisors?\n\nI'm personally focused on cybernetic/centaur systems that combine AI advisors with humans because this allows the humans to compensate for the weaknesses of the AI.\n\nThis has a few key advantages:\n\n• It provides an additional layer of safety/security.\n\n• It allows us to benefit from such systems earlier than we would be able to otherwise\n\n• If we decide advisors are insufficient and that we want to train autonomously acting wise agents, AI advisors could help us with that.\n\n\n\n\n+++\n\n<table style=\"background-color:#f2edfa;border:4px solid #dcd0f2\"><tbody><tr><td style=\"border-style:none\"><p>Whilst the possibility of training wise AI has been previously discussed in the academic literature, I am hopeful that this paper will turn out to be a landmark. Given the credibility of the authors and the quality of the work, it's plausible to me that it will play a key role in causing artificial wisdom blossom into its own sub-field of ML.</p><p>I really hope this is the case because I suspect that worlds where this happens are much more likely to result in good outcomes than worlds where this does not happen. To quote the authors:</p><blockquote><p>We may not know precisely what form wise AI will take—but it must surely be preferable to folly.</p></blockquote></td></tr></tbody></table>\n\nRead the full, original paper\n\n+++ ### Highlights of additional content in the full paper\n\n• A table summarising psychological approaches to wisdom  \n• A list of ideas of how to engineer wiser AI  \n• A list of outstanding questions\n\n\n\n\n+++\n\n[^uycflfu5vsl]: Johnson, B. (2022). Metacognition for artificial intelligence system safety: An approach to safe and desired behavior. Safety Science, 151, 105743.  \n\n[^yb717et60ai]: For the purposes of this paper... the authors aren't claiming to make a universal definition. \n\n[^p4mlpkwe8yk]: See the collapsable section immediately underneath for a larger list. \n\n[^7vsqtj26zvd]: Walasek, L., & Brown, G. D. (2023). Incomparability and incommensurability in choice: No common currency of value? Perspectives on Psychological Science, 17456916231192828.  \n\n[^t9wlmimd4h8]: Kay, J., & King, M. (2020). Radical uncertainty: Decision-making beyond the numbers. New York, NY: Norton.  \n\n[^47se0ry9yj9]: They seem to be pointing to Knightian uncertainty \n\n[^pjfiptu4ac]: Lorenz, E. (1993). The essence of chaos. Seattle, WA: University of Washington Press.  \n\n[^rxmhhh05aqh]: Prof. Grossmann (personal correspondance): \"You are right that the term \"intractable problem\" is complicated and our group has debated it for a while (different disciplines favoured different jargon). Our examples were chiefly for highlighting the metacognitive benefits for wise decision-making.\" \n\n[^sluio9vrpt7]: This table is copied from the paper. \n\n[^zgvv6onvhr]: They use examples we discussed earlier to help justify their focus on metacognition. Whilst the Willa example might not initially appear related to metacognition, I suspect that the authors see this as related to \"perspective seeking\", one of the six metacognitive processes they highlight. \n\n[^o7wnadtsmiq]: Li, Y., Huang, Y., Lin, Y., Wu, S., Wan, Y., & Sun, L. (2024). I think, therefore I am: Awareness in Large Language Models. arXiv preprint arXiv:2401.17882.  \n\n[^v1ihrkj71hc]: Cash, T. N., Oppenheimer, D. M., & Christie, S. Quantifying UncertAInty: Testing the Accuracy of LLMs’ Confidence Judgments. Preprint.  \n\n[^796pi2yx9no]: Scholten, F., Rebholz, T. R., & Hütter, M. (2024). Metacognitive myopia in Large Language Models. arXiv preprint arXiv:2408.05568.  \n\n[^oddfc59v0as]: An older version of the paper suggested: \"Wisdom could enable the design of structures (such as constitutions, markets, and organizations) that enhance cooperation in society\". \n\n[^w7j28jx8a]: The original paper said: \"It can be incredibly challenging to \"exhaustively specify goals in advance\". Humans handle this by using goal hierarchies and wisdom could assist AI's in navigating this\" \n\n[^mjjqj7s5neq]: A previous version of the paper claimed: \"Perhaps the greatest risk is currently systems not working well enough. Machine metacognition could be useful for this. In particular, \"AIs with appropriately calibrated confidence can target the most likely safety risks; appropriate self-models would help AIs to anticipate potential failures; and continual monitoring of its performance would facilitate recognition of high-risk moments and permit learning from experience.\" \n\n[^facv0pq75sc]: I agree that metacognition seems important for explanability, but my intuition is that wise decisions are often challenging or even impossible to make legible. See Tentatively against making AIs 'wise', which won a runner up prize in the AI Impacts Essay competition on the Automation of Wisdom and Philosophy.The authors acknowledge the possibility that most attempts at introspection may fail to observe what really produced the decision, as opposed to merely producing an inference/story. Nonetheless, they assert that these inferences are in fact useful. \n\n[^wojhuchi41e]: The first sentence of this section reads \"First, humans are not even aligned with each other\". This is confusing since the second paragraph seems to suggest that their point is more about humans not always following norms, which is what I've summarised their point as. \n\n[^e35uvf4ig9b]: This paper doesn't use the term \"instrumental convergence\", so this statement involves a slight bit of interpretation on my part. \n\n[^fdxex7tq5m6]: Prof. Grossmann (personal correspondance): \"I also don't think most philosophical or contemporary definitions of human wisdom in behavioural sciences would primarily focus on \"intuition\" - I even have evidence from a wide range of countries where most cultures consider a \"wise\" decision strategy to chiefly rely on deliberation\" \n\n[^q6hthkp0onr]: This is less significant in my worldview as I see wisdom as often being just about knowing the right answer without knowing why you know. \n\n[^3f6nrflqrfk]: The labels \"Proposal A\" and \"Proposal B\" aren't in the paper. \n\n[^m9pqokpln3p]: For example, Lampinen, A. K., Roy, N., Dasgupta, I., Chan, S. C., Tam, A., Mcclelland, J., ... & Hill, F. (2022, June). Tell me why! explanations support learning relational and causal structure. In International Conference on Machine Learning (pp. 11868-11890).  \n\n[^zr5bxcicrcs]: Prof. Grossmann (personal correspondance): \"I like the idea of wise advisors. I don't think the argument in our paper is against it -it all depends on how humans will use the technology (and there are several papers on the role of metacognition for discerning when to rely on decision-aids/AI advisors, too).\" \n\n[^m3kdbcud2f]: Eliezer Yudkowsky's view seems to be that this specification pretty much has to be exhaustive, though others are less pessimistic about partial alignment.",
      "plaintextDescription": "> We may not know precisely what form wise AI will take—but it must surely be preferable to folly.\n\nRead the full, original paper\n\n\nPaper Authors:\nSamuel G. B. Johnson, Amir-Hossein Karimi, Yoshua Bengio, Nick Chater, Tobias Gerstenberg, Kate Larson, Sydney Levine, Melanie Mitchell, Iyad Rahwan, Bernhard Schölkopf, Igor Grossmann\n\n\nHighlights\n> • We examine the why and the how of building wise AI\n> • Wisdom helps humans to navigate intractable problems through object-level strategies (for managing problems) and metacognitive strategies (for managing object-level strategies)\n> • Wise AI, through improved metacognition, would be more robust to new environments, explainable to users, cooperative in pursuing shared goals, and safe in avoiding both prosaic and catastrophic failures\n> • We suggest several approaches to benchmarking wisdom, training wise reasoning strategies, and adapting AI architecture for metacognition \n\n\nAbstract\n> Recent advances in artificial intelligence (AI) have produced systems capable of increasingly sophisticated performance on cognitive tasks. However, AI systems still struggle in critical ways: unpredictable and novel environments (robustness), lack of transparency in their reasoning (explainability), challenges in communication and commitment (cooperation), and risks due to potential harmful actions (safety). We argue that these shortcomings stem from one overarching failure: AI systems lack wisdom.\n> \n>  Drawing from cognitive and social sciences, we define wisdom as the ability to navigate intractable problems - those that are ambiguous, radically uncertain, novel, chaotic, or computationally explosive - through effective task-level and metacognitive strategies. While AI research has focused on task-level strategies, metacognition - the ability to reflect on and regulate one's thought processes - is underdeveloped in AI systems. In humans, metacognitive strategies such as recognizing the limits of one's knowledge, considering diverse persp",
      "wordCount": 2932
    },
    "tags": [
      {
        "_id": "MwTjpRZqovTcuYuKt",
        "name": "Wisdom",
        "slug": "wisdom-2"
      },
      {
        "_id": "8SfkJYYMe75MwjHzN",
        "name": "Summaries",
        "slug": "summaries"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "oofzmbidovjZf2Dg3",
    "title": "Some Preliminary Notes on the Promise of a Wisdom Explosion",
    "slug": "some-preliminary-notes-on-the-promise-of-a-wisdom-explosion",
    "url": "https://aiimpacts.org/some-preliminary-notes-on-the-promise-of-a-wisdom-explosion/",
    "baseScore": 2,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-10-31T09:21:11.623Z",
    "contents": {
      "markdown": "This post is one-half of my third-prize winning entry for the [AI Impacts Essay Competition on the Automation of Wisdom and Philosophy](https://blog.aiimpacts.org/p/winners-of-the-essay-competition). It proposes a \"wisdom explosion\" as an alternative to an intelligence explosion that is safer from the standpoint of differential technological development.",
      "plaintextDescription": "This post is one-half of my third-prize winning entry for the AI Impacts Essay Competition on the Automation of Wisdom and Philosophy. It proposes a \"wisdom explosion\" as an alternative to an intelligence explosion that is safer from the standpoint of differential technological development.",
      "wordCount": 44
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "FAsjqL45SCAQPmYvD",
    "title": "Linkpost: Hypocrisy standoff",
    "slug": "linkpost-hypocrisy-standoff",
    "url": "https://x.com/ptrschmdtnlsn/status/1839941823793639751",
    "baseScore": 5,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2024-09-29T14:27:19.175Z",
    "contents": {
      "markdown": "",
      "plaintextDescription": "",
      "wordCount": 1
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "QsHqKhsC636jycx5c",
    "title": "On the destruction of America’s best high school",
    "slug": "on-the-destruction-of-america-s-best-high-school",
    "url": "https://scottaaronson.blog/?p=4979",
    "baseScore": -6,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2024-09-12T15:30:20.001Z",
    "contents": {
      "markdown": "This is a rare case when I think it's actually worth sharing an article that directly comments on the culture wars on Less Wrong:\n\n> I’d like you to feel about the [impending destruction](https://quillette.com/2020/09/23/rallying-to-protect-admissions-standards-at-americas-best-public-high-school/) of Virginia’s [Thomas Jefferson High School for Science and Technology](https://en.wikipedia.org/wiki/Thomas_Jefferson_High_School_for_Science_and_Technology), the same way you might’ve felt when the Taliban threatened to blow up the [Bamyan Buddhas](https://en.wikipedia.org/wiki/Buddhas_of_Bamyan), and then days later actually did blow them up. Or the way you felt when human negligence caused wildfires that incinerated half the koalas in Australia, or turned the San Francisco skyline into an orange hellscape. For that matter, the same way most of us felt the day Trump was elected. I’d like you to feel in the bottom of your stomach the *avoidability*, and yet the finality, of the loss.",
      "plaintextDescription": "This is a rare case when I think it's actually worth sharing an article that directly comments on the culture wars on Less Wrong:\n\n> I’d like you to feel about the impending destruction of Virginia’s Thomas Jefferson High School for Science and Technology, the same way you might’ve felt when the Taliban threatened to blow up the Bamyan Buddhas, and then days later actually did blow them up. Or the way you felt when human negligence caused wildfires that incinerated half the koalas in Australia, or turned the San Francisco skyline into an orange hellscape. For that matter, the same way most of us felt the day Trump was elected. I’d like you to feel in the bottom of your stomach the avoidability, and yet the finality, of the loss.",
      "wordCount": 131
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ZjjLGn2JCGwd4dDuq",
    "title": "The Bar for Contributing to AI Safety is Lower than You Think",
    "slug": "the-bar-for-contributing-to-ai-safety-is-lower-than-you",
    "url": null,
    "baseScore": 20,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2024-08-16T15:20:19.055Z",
    "contents": {
      "markdown": "Many people still have a model of AI safety where they assume that the bar for contributing must be really high.  \n  \nI'm here to tell you that this isn't the case.\n\nThe Value of Good Community Members\n-----------------------------------\n\nEven without conducting groundbreaking research, \"good community members\" play a crucial role in expanding and enriching the AI Safety space. But what constitutes a good community member?\n\nWhile possessing a high degree of intelligence is undeniably beneficial, other personal attributes can compensate for raw intelligence to a certain extent. These include:\n\n*   Intellectual humility\n*   The ability to deeply understand other perspectives\n*   Dedication to continuous learning more about the field\n\nThere is undoubtedly some level of baseline intelligence below which it'll be hard for you to be a net contributor, but that's not as much of an issue as you might think as Less Wrong contributors tend to be quite smart anyway. The proliferation of accessible resources on AI and AI Safety has substantially lowered the barrier to entry for being able to usefully contribute to discussions and if you have a genuine interest, learning about the field won't really feel like work.\n\n### Soft Skills\n\nOther attributes can significantly increase your value as a community member.\n\n*   Good vibes is a major value add. No one wants to be around a jerk.\n*   Being able to read the room. Knowing when to speak and when to listen. Try not to be the guy who has absolutely no clue, but takes up a disproportionate amount of air time.\n*   When you're listening to people, truly listening. People love spending time around people who do this\n*   If you're good at providing feedback, you'll find high demand as it's hard to see all angles of a problem by yourself. The rapid growth of the field has created a mentorship bottleneck, making peer support increasingly vital as an alternative.\n*   Providing emotional support. AI Safety can be quite confronting at times, but we can help each other work through this.\n\nConclusion\n----------\n\nThe field of AI Safety benefits not only from groundbreaking research but also from a vibrant, supportive community of engaged individuals. By participating actively, providing feedback, and maintaining an open and curious mindset, you may contribute more significantly than you initially believed possible. There are many attributes that can make you a valuable community member other than raw intelligence and these attributes can be developed if you're willing to work on it.  \n  \nAnd who knows? Maybe even if you don't feel like you have that much to offer, but you consistently read things, hang around the community read things and provide feedback on other people's ideas, you might eventually be drawn towards pursuing an idea of your own.\n\n(I don’t wish to imply that I have all of these attributes myself, so I expect this list to be a useful guide for how I could be a better community member myself)",
      "plaintextDescription": "Many people still have a model of AI safety where they assume that the bar for contributing must be really high.\n\nI'm here to tell you that this isn't the case.\n\n\nThe Value of Good Community Members\nEven without conducting groundbreaking research, \"good community members\" play a crucial role in expanding and enriching the AI Safety space. But what constitutes a good community member?\n\nWhile possessing a high degree of intelligence is undeniably beneficial, other personal attributes can compensate for raw intelligence to a certain extent. These include:\n\n * Intellectual humility\n * The ability to deeply understand other perspectives\n * Dedication to continuous learning more about the field\n\nThere is undoubtedly some level of baseline intelligence below which it'll be hard for you to be a net contributor, but that's not as much of an issue as you might think as Less Wrong contributors tend to be quite smart anyway. The proliferation of accessible resources on AI and AI Safety has substantially lowered the barrier to entry for being able to usefully contribute to discussions and if you have a genuine interest, learning about the field won't really feel like work.\n\n\nSoft Skills\nOther attributes can significantly increase your value as a community member.\n\n * Good vibes is a major value add. No one wants to be around a jerk.\n * Being able to read the room. Knowing when to speak and when to listen. Try not to be the guy who has absolutely no clue, but takes up a disproportionate amount of air time.\n * When you're listening to people, truly listening. People love spending time around people who do this\n * If you're good at providing feedback, you'll find high demand as it's hard to see all angles of a problem by yourself. The rapid growth of the field has created a mentorship bottleneck, making peer support increasingly vital as an alternative.\n * Providing emotional support. AI Safety can be quite confronting at times, but we can help each other work through this.\n\n\nConcl",
      "wordCount": 490
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xed8Qyo4iRz5PPiRK",
    "title": "Michael Streamlines on Buddhism",
    "slug": "michael-streamlines-on-buddhism",
    "url": "https://x.com/Plus3Happiness/status/1821196812792844640",
    "baseScore": 8,
    "voteCount": 1,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-08-09T04:44:52.126Z",
    "contents": {
      "markdown": "Just sharing because I appreciate the diagrammatic approach as a way of making discussions of Buddhist concepts clearer:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a0b71579a98c476b615accb1adc6fbf2f178a1f8e9391877.jpeg)\n\nhttps://x.com/Plus3Happiness/status/1821196812792844640",
      "plaintextDescription": "Just sharing because I appreciate the diagrammatic approach as a way of making discussions of Buddhist concepts clearer:\n\nhttps://x.com/Plus3Happiness/status/1821196812792844640",
      "wordCount": 20
    },
    "tags": [
      {
        "_id": "xv7Bg5fbF9WppYREi",
        "name": "Buddhism",
        "slug": "buddhism"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "EDBxdR9MzXwDaJYuC",
    "title": "Have people given up on iterated distillation and amplification?",
    "slug": "have-people-given-up-on-iterated-distillation-and",
    "url": null,
    "baseScore": 20,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2024-07-19T12:23:04.625Z",
    "contents": {
      "markdown": "The BlueDot Impact write-up for scalable oversight seems to suggest that people have given up on [iterated distillation and amplification](https://aisafetyfundamentals.com/blog/scalable-oversight-intro/) (IDA) working. I haven’t really seen much research here, but is that actually the case?  \n  \nI know that Ought is now pretty much inactive because their attempts at factored cognition failed and this has made many people pessimistic about the factored cognition hypothesis. However, Ought seemed to be really pushing the angle that we could break things into sub-problems each of which could be completed quickly, whilst I don’t think that’s a required part of IDA. Therefore it isn’t clear to me that the failure of factored cognition indicates the failure of IDA.",
      "plaintextDescription": "The BlueDot Impact write-up for scalable oversight seems to suggest that people have given up on iterated distillation and amplification (IDA) working. I haven’t really seen much research here, but is that actually the case?\n\nI know that Ought is now pretty much inactive because their attempts at factored cognition failed and this has made many people pessimistic about the factored cognition hypothesis. However, Ought seemed to be really pushing the angle that we could break things into sub-problems each of which could be completed quickly, whilst I don’t think that’s a required part of IDA. Therefore it isn’t clear to me that the failure of factored cognition indicates the failure of IDA.",
      "wordCount": 113
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "sExzCaNww23kW28so",
    "title": "Politics is the mind-killer, but maybe we should talk about it anyway",
    "slug": "politics-is-the-mind-killer-but-maybe-we-should-talk-about",
    "url": null,
    "baseScore": 14,
    "voteCount": 28,
    "viewCount": null,
    "commentCount": 33,
    "createdAt": null,
    "postedAt": "2024-06-03T06:37:57.037Z",
    "contents": {
      "markdown": "I completely agree with Eliezer that [Politics is the Mind-Killer](https://www.lesswrong.com/posts/9weLK2AJ9JEt2Tt8f/politics-is-the-mind-killer), but at the same time, I worry that talking in abstracts all the time is limiting. I wouldn't want this site to tilt too much toward politics, but I feel that there's such a thing as too little discussion as well.\n\nWe recently lived through a period where \"wokeness\" - as ill-defined as that term is - was the dominant strain of thought within the educated strata of society. It's unclear exactly what percent of people supported this strain of thought, particularly when you account for people agreeing with reservations or with some parts, but not with others. It's not clear that this perspective was ever even a majority even within this particular strata. But there was a period where it was able to crowd out other perspectives through some combination of people being scared to speak out, people feeling that it as morally dubious to nitpick the specifics, people being censored and access to particular levers of power.\n\nOf course, this doesn't tell us anything about the object-level. All of this could be true and the \"woke\" perspective, insofar as it makes sense to talk about a single perspective, could be correct on all or almost all points. I don't particularly want to litigate this right now, so I'll just talk about what makes sense if you agree with me that a substantial proportion of claims made were false or exaggerated.\n\nFirst of all, now that certain pressures have passed, it makes sense to re-evaluate your views in case social pressure distorted your perception such that you to adopted views without good reason. On one hand, you don't want to be too quick to update, as it's very easy for humans to be pulled whichever way the social winds are blowing. At the same time, you may also be subject to a bias of not wanting to admit that you were wrong or go against what you said. The [law of equal and opposite](https://slatestarcodex.com/2014/03/24/should-you-reverse-any-advice-you-hear/) advice applies here.\n\nAdditionally, insofar as you feel that much of what was said by the social justice movement was either in bad faith or coercive, this could make you want to swing all the way to the opposite side. Overreactions are very common, don't let this be you. Again, the law of equal and opposite advice applies. Some people are likely only slowly realising how much their information environment was manipulated, these people would do well to realise that they're likely to discover more such manipulation over time. On the other hand, as previously mentioned, some people are swinging too far. And those people would do well to keep in mind that if they wait a few more years, there will be quite persuasive articles explaining why certain woke ideas were more reasonable than they sounded.\n\nIt may be worth keeping in mind that it's not like the social justice movement had no reasons for pursuing its goals so aggressively. It arose in a context in which sexual minorities were really treated as second-class citizens and had been subject to this, much as I hate to say this, oppression, for hundreds of years. It was also quite reasonable to suggest that removing overt discrimination was insufficient and that we needed to re-examine societal defaults and perhaps take action to address inequalities persisting from the past. Unfortunately, the most viral forms of these ideas involved a theory of change based on social pressure/entryism rather than engaging in rational debate/discussion and this led to the societal discussion being truly absurd for a certain number of years.\n\nThe lessons people draw from this will widely vary. Some people will suggest that these events were unfavorable to [conflict theory](https://slatestarcodex.com/2018/01/24/conflict-vs-mistake/) by demonstrating how it damages a movement's ability to think strategically, leads to infighting and achieves short-term objectives at the cost of making long-term enemies. Others will suggest that these events were unfavorable to mistake theory, by demonstrating the naiveness of this perspective against determined bad faith actors.\n\nSome people will note that the community came through mostly unscathed and suggest that sitting this fight out was the correct choice because it avoided internal divisions and making ourselves a target. Others might argue that this was blind luck, that if the social justice side had an overwhelming victory, then they would have come for us and that we might have been able to prevent things from getting as bad as they did had we acted early. I personally feel that we likely would have been too naive/politically inexperienced to have had a truly major influence, but I'm less certain that applies going forward. At the same time, we seem to be staring at short or mid-AI timelines, so the cost of getting distracted is much higher than it was before.\n\nI don't want to pretend that I have all of the answers here. However, I do want to share a few thoughts. Less Wrong has always had something of an individualistic bent. While there has been some discussion about what a strong rationalist community would look like, the dominant focus has been on how to be rational as an individual.\n\nAnd perhaps it's time to think broader than that. Maybe it's time to start taking an ecosystem perspective of society and realise that as much as we'd like to be able to just ignore the rest of society and do our own thing, that isn't always an option. Once a group of people gains a sufficient amount of influence, other groups will start to see it as a threat. This incentivizes them to do things such as engage in bad-faith criticism, attempt to capture institutions or produce a watered-down but more popularist-sounding version of the group's beliefs. At the same time, these are very difficult to distinguish from good-faith criticism, good-faith participation and reshaping the belief system to be less narrow.\n\nIt's not clear that any of these issue have nice, neat solutions. Nonetheless, we do what we can, it's probably better to have a shared awareness of these kinds of issues rather than to either have a blindspot or to have a bunch of individuals thinking about these issues just by themselves.",
      "plaintextDescription": "I completely agree with Eliezer that Politics is the Mind-Killer, but at the same time, I worry that talking in abstracts all the time is limiting. I wouldn't want this site to tilt too much toward politics, but I feel that there's such a thing as too little discussion as well.\n\nWe recently lived through a period where \"wokeness\" - as ill-defined as that term is - was the dominant strain of thought within the educated strata of society. It's unclear exactly what percent of people supported this strain of thought, particularly when you account for people agreeing with reservations or with some parts, but not with others. It's not clear that this perspective was ever even a majority even within this particular strata. But there was a period where it was able to crowd out other perspectives through some combination of people being scared to speak out, people feeling that it as morally dubious to nitpick the specifics, people being censored and access to particular levers of power.\n\nOf course, this doesn't tell us anything about the object-level. All of this could be true and the \"woke\" perspective, insofar as it makes sense to talk about a single perspective, could be correct on all or almost all points. I don't particularly want to litigate this right now, so I'll just talk about what makes sense if you agree with me that a substantial proportion of claims made were false or exaggerated.\n\nFirst of all, now that certain pressures have passed, it makes sense to re-evaluate your views in case social pressure distorted your perception such that you to adopted views without good reason. On one hand, you don't want to be too quick to update, as it's very easy for humans to be pulled whichever way the social winds are blowing. At the same time, you may also be subject to a bias of not wanting to admit that you were wrong or go against what you said. The law of equal and opposite advice applies here.\n\nAdditionally, insofar as you feel that much of what was said by the social ",
      "wordCount": 1034
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "rZ6wam9gFGFQrCWHc",
    "title": "Does reducing the amount of RL for a given capability level make AI safer?",
    "slug": "does-reducing-the-amount-of-rl-for-a-given-capability-level",
    "url": null,
    "baseScore": 43,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 22,
    "createdAt": null,
    "postedAt": "2024-05-05T17:04:01.799Z",
    "contents": {
      "markdown": "Some people have suggested that a lot of the danger of training a powerful AI comes from reinforcement learning. Given an objective, RL will reinforce any method of achieving the objective that the model a) tries and b) finds to be successful. It doesn't matter whether it includes things like deceiving us or increasing its power.\n\nIf this were the case, then when we are building a model with capability level X, it might make sense to try to train that model either without RL or with as little RL as possible.\n\nFor example, we might attempt to achieve the objective using imitation learning instead. Some people would push back against this and argue that this is still a black-box that uses gradient descent with no way of knowing that the internals are safe.\n\nIs this likely to produce safer models or is the risk mostly independent of RL?\n\n*Further thoughts: Does this actually matter?*\n\nEven if there are techniques involving avoiding or reducing RL that would make AI's safer, this could backfire if it results in capability externalities. Attempting to reproduce a certain level of capabilities without RL would be highly likely to lead to higher capabilities once RL was added on top and there are always actors who would wish to do this.\n\nI don't think this invalidates this question though. Some of the various governance interventions may end up bearing fruit and so we may end up with the option of accepting less powerful systems.",
      "plaintextDescription": "Some people have suggested that a lot of the danger of training a powerful AI comes from reinforcement learning. Given an objective, RL will reinforce any method of achieving the objective that the model a) tries and b) finds to be successful. It doesn't matter whether it includes things like deceiving us or increasing its power.\n\nIf this were the case, then when we are building a model with capability level X, it might make sense to try to train that model either without RL or with as little RL as possible.\n\nFor example, we might attempt to achieve the objective using imitation learning instead. Some people would push back against this and argue that this is still a black-box that uses gradient descent with no way of knowing that the internals are safe.\n\nIs this likely to produce safer models or is the risk mostly independent of RL?\n\nFurther thoughts: Does this actually matter?\n\nEven if there are techniques involving avoiding or reducing RL that would make AI's safer, this could backfire if it results in capability externalities. Attempting to reproduce a certain level of capabilities without RL would be highly likely to lead to higher capabilities once RL was added on top and there are always actors who would wish to do this.\n\nI don't think this invalidates this question though. Some of the various governance interventions may end up bearing fruit and so we may end up with the option of accepting less powerful systems.",
      "wordCount": 248
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Mq5LqZtP54BC8fXte",
    "title": "Link: Let's Think Dot by Dot: Hidden Computation in Transformer Language Models by Jacob Pfau, William Merrill & Samuel R. Bowman",
    "slug": "link-let-s-think-dot-by-dot-hidden-computation-in",
    "url": "https://twitter.com/jacob_pfau/status/1783951795238441449",
    "baseScore": 12,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-04-27T13:22:53.287Z",
    "contents": {
      "markdown": "One consideration that is pretty important for AI safety is understanding the extent to which a model's outputs are aligned with its chain of thought.\n\nThis [paper](https://arxiv.org/abs/2404.15758) (Twitter thread linked) provides some relevant evidence. It demonstrates that it is possible for a model to achieve performance comparable to chain-of-thought with dots replacing the chain of thought under some circumstances. In particular, the model can't just be trained with sequences like \"QUESTION..............ANSWER\", but sequences need to also be mixed in where there is a \"parallelisable\" chain of thought. Here \"parallelisable\" means that different components of the chain of thought can be calculated in parallel rather than all separately.\n\nIn terms of how this is relevant to AI safety, this provides an empirical demonstration that a model is capable of very effectively engaging in background computation under certain circumstance. It shows that the model is much better at doing background parallelizable tasks than non-parallelizable tasks. In other words, the chain of thought is less binding than it might have been because the model is free to perform some of the computations necessary for future tokens in the background.",
      "plaintextDescription": "One consideration that is pretty important for AI safety is understanding the extent to which a model's outputs are aligned with its chain of thought.\n\nThis paper (Twitter thread linked) provides some relevant evidence. It demonstrates that it is possible for a model to achieve performance comparable to chain-of-thought with dots replacing the chain of thought under some circumstances. In particular, the model can't just be trained with sequences like \"QUESTION..............ANSWER\", but sequences need to also be mixed in where there is a \"parallelisable\" chain of thought. Here \"parallelisable\" means that different components of the chain of thought can be calculated in parallel rather than all separately.\n\nIn terms of how this is relevant to AI safety, this provides an empirical demonstration that a model is capable of very effectively engaging in background computation under certain circumstance. It shows that the model is much better at doing background parallelizable tasks than non-parallelizable tasks. In other words, the chain of thought is less binding than it might have been because the model is free to perform some of the computations necessary for future tokens in the background.",
      "wordCount": 186
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ErYqeQpz8TWAmgmzD",
    "title": "\"You're the most beautiful girl in the world\" and Wittgensteinian Language Games",
    "slug": "you-re-the-most-beautiful-girl-in-the-world-and",
    "url": null,
    "baseScore": 5,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 18,
    "createdAt": null,
    "postedAt": "2024-04-20T14:54:54.503Z",
    "contents": {
      "markdown": "Wittgenstein argues that we shouldn't understand language by piecing together the dictionary meaning of each individual word in a sentence, but rather that language should be understood in context as a move in a language game.\n\nConsider the phrase, \"You're the most beautiful girl in the world\". Many rationalists might shy away from such a statement, deeming it statistically improbable. However, while this strict adherence to truth is commendable, I honestly feel it is misguided.  \n  \nIt's honestly kind of absurd to expect your words to be taken literally in these kinds of circumstances. The recipient of such a compliment will almost certainly understand it as hyperbole intended to express fondness and desire, rather than as a literal factual assertion. Further, by invoking a phrase that plays a certain role in movies, books, etc. you're making a bid to follow certain cultural scripts[^la08nshupk]. The girl almost certainly knows this intuitively, regardless of whether or not she could articulate it precisely.\n\nOf course, one should avoid making such statements if they believe them to be fundamentally false. However, ethical communication in these circumstance isn't about the literal truth of the words but whether they are expressed sincerely and whether the speaker genuinely intends to uphold the unspoken commitments associated with such cultural conventions.\n\n[^la08nshupk]: I wouldn't be able to comprehensively identify all the aspects of the scripts invoked, but I suspect that at least part of this is a bid to roleplay certain idealized cultural narratives. It might sounds like I'm trivialised this, ie. that I'm saying it's all pretend, but there's a sense in which this roleplay brings reality closer to these narratives even if they can never be fully realized.",
      "plaintextDescription": "Wittgenstein argues that we shouldn't understand language by piecing together the dictionary meaning of each individual word in a sentence, but rather that language should be understood in context as a move in a language game.\n\nConsider the phrase, \"You're the most beautiful girl in the world\". Many rationalists might shy away from such a statement, deeming it statistically improbable. However, while this strict adherence to truth is commendable, I honestly feel it is misguided.\n\nIt's honestly kind of absurd to expect your words to be taken literally in these kinds of circumstances. The recipient of such a compliment will almost certainly understand it as hyperbole intended to express fondness and desire, rather than as a literal factual assertion. Further, by invoking a phrase that plays a certain role in movies, books, etc. you're making a bid to follow certain cultural scripts[1]. The girl almost certainly knows this intuitively, regardless of whether or not she could articulate it precisely.\n\nOf course, one should avoid making such statements if they believe them to be fundamentally false. However, ethical communication in these circumstance isn't about the literal truth of the words but whether they are expressed sincerely and whether the speaker genuinely intends to uphold the unspoken commitments associated with such cultural conventions.\n\n 1. ^\n    I wouldn't be able to comprehensively identify all the aspects of the scripts invoked, but I suspect that at least part of this is a bid to roleplay certain idealized cultural narratives. It might sounds like I'm trivialised this, ie. that I'm saying it's all pretend, but there's a sense in which this roleplay brings reality closer to these narratives even if they can never be fully realized.",
      "wordCount": 211
    },
    "tags": [
      {
        "_id": "s9fuuxuzFMugAnf6r",
        "name": "Language & Linguistics",
        "slug": "language-and-linguistics"
      },
      {
        "_id": "AADZcNS24mmSfPp2w",
        "name": "Communication Cultures",
        "slug": "communication-cultures"
      },
      {
        "_id": "5SPDtxJT6y6ZTXHBJ",
        "name": "Simulacrum Levels",
        "slug": "simulacrum-levels"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ALrJR2wacBBNBQnqD",
    "title": "The argument for near-term human disempowerment through AI",
    "slug": "the-argument-for-near-term-human-disempowerment-through-ai",
    "url": "https://link.springer.com/article/10.1007/s00146-024-01930-2",
    "baseScore": 22,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2024-04-16T04:50:53.828Z",
    "contents": {
      "markdown": "**Author**: Leonard Dung  \n  \n**Abstract**: Many researchers and intellectuals warn about extreme risks from artificial intelligence. However, these warnings typically came without systematic arguments in support. This paper provides an argument that AI will lead to the permanent disempowerment of humanity, e.g. human extinction, by 2100. It rests on four substantive premises which it motivates and defends: first, the speed of advances in AI capability, as well as the capability level current systems have already reached, suggest that it is practically possible to build AI systems capable of disempowering humanity by 2100. Second, due to incentives and coordination problems, if it is possible to build such AI, it will be built. Third, since it appears to be a hard technical problem to build AI which is aligned with the goals of its designers, and many actors might build powerful AI, misaligned powerful AI will be built. Fourth, because disempowering humanity is useful for a large range of misaligned goals, such AI will try to disempower humanity. If AI is capable of disempowering humanity and tries to disempower humanity by 2100, then humanity will be disempowered by 2100. This conclusion has immense moral and prudential significance.\n\n**My thoughts**: I read through it rather quickly so take what I say with a grain of salt. That said, it seemed persuasive and well-written. Additionally, the way that they split up the argument was quite nice.",
      "plaintextDescription": "Author: Leonard Dung\n\nAbstract: Many researchers and intellectuals warn about extreme risks from artificial intelligence. However, these warnings typically came without systematic arguments in support. This paper provides an argument that AI will lead to the permanent disempowerment of humanity, e.g. human extinction, by 2100. It rests on four substantive premises which it motivates and defends: first, the speed of advances in AI capability, as well as the capability level current systems have already reached, suggest that it is practically possible to build AI systems capable of disempowering humanity by 2100. Second, due to incentives and coordination problems, if it is possible to build such AI, it will be built. Third, since it appears to be a hard technical problem to build AI which is aligned with the goals of its designers, and many actors might build powerful AI, misaligned powerful AI will be built. Fourth, because disempowering humanity is useful for a large range of misaligned goals, such AI will try to disempower humanity. If AI is capable of disempowering humanity and tries to disempower humanity by 2100, then humanity will be disempowered by 2100. This conclusion has immense moral and prudential significance.\n\nMy thoughts: I read through it rather quickly so take what I say with a grain of salt. That said, it seemed persuasive and well-written. Additionally, the way that they split up the argument was quite nice.",
      "wordCount": 231
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xpyvJ76brChicdfrC",
    "title": "Reverse Regulatory Capture",
    "slug": "reverse-regulatory-capture",
    "url": null,
    "baseScore": 12,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2024-04-11T02:40:46.474Z",
    "contents": {
      "markdown": "Reverse regulatory capture is an advanced move where industry interests cry \"regulatory capture\" in order to oppose regulation. Generally, the more skeptical people are about corporate power and the vulnerability of regulators to influence, the more worried they are about regulatory capture.  \n  \nIf you're extremely skeptical about big companies and their influence over regulators, it's unclear whether you should be comforted by government officials talking about regulatory capture. This is because it might actually be a sign that the opposite is happening - that reverse regulatory capture is taking place.  \n  \nInsofar as politics is an adversarial environment, it should be expected that any kind of claim that can be abused will be abused. It is magical thinking to believe that certain kinds of claims - such as claims of regulatory capture - will be immune to this.  \n  \nThe first line of defense is simple - equal skepticism. Insofar as you are skeptical of corporations advocating for regulation, you should also be skeptical of corporations advocating against regulation. Insofar as you identify a corporate interest in incumbents to create regulation that hampers less well-equipped competitors, you should also be aware of the corporate interest in avoiding regulation.  \n  \nInsofar as you are skeptical that certain actors are just chasing corporate profit, you should also be skeptical of attempts to push the narrative that certain actors are just chasing corporate profit. Actors who wish to manipulate you will often be aware of the buttons that they can push in order to make you take a certain response. One path for better understanding an actor's motivation is to look for public comments that they made before they had a profit incentive. Did their public comments sharply change along with their interests or have they been consistent throughout?  \n  \nAnother way to understand the trustworthiness of an actor is to look at their public communications and to try to determine the extent that they are ideologically pushing a narrative or engaging in underhand tactics like ad hominem attacks. The less concerned an actor is with ethics, the more worried you should be about that they are pushing a narrative out of self-interest. The more principled an actor is, the more willing you should be to accept that they are making their decisions out of sincere belief, even if they might be mistaken.\n\nUntangling the webs can be confusing, but it can be done. Unfortunately, most people barely try, preferring to believe whatever narrative is most convenient for them to believe.\n\n*If you feel that you could write a better article on this topic, feel free to reach out. I'm very open to agreeing to allow reuse of parts of this article in other people's works.*",
      "plaintextDescription": "Reverse regulatory capture is an advanced move where industry interests cry \"regulatory capture\" in order to oppose regulation. Generally, the more skeptical people are about corporate power and the vulnerability of regulators to influence, the more worried they are about regulatory capture.\n\nIf you're extremely skeptical about big companies and their influence over regulators, it's unclear whether you should be comforted by government officials talking about regulatory capture. This is because it might actually be a sign that the opposite is happening - that reverse regulatory capture is taking place.\n\nInsofar as politics is an adversarial environment, it should be expected that any kind of claim that can be abused will be abused. It is magical thinking to believe that certain kinds of claims - such as claims of regulatory capture - will be immune to this.\n\nThe first line of defense is simple - equal skepticism. Insofar as you are skeptical of corporations advocating for regulation, you should also be skeptical of corporations advocating against regulation. Insofar as you identify a corporate interest in incumbents to create regulation that hampers less well-equipped competitors, you should also be aware of the corporate interest in avoiding regulation.\n\nInsofar as you are skeptical that certain actors are just chasing corporate profit, you should also be skeptical of attempts to push the narrative that certain actors are just chasing corporate profit. Actors who wish to manipulate you will often be aware of the buttons that they can push in order to make you take a certain response. One path for better understanding actor's motivation is to look for public comments that they made before they had a profit incentive. Did their public comments sharply change along with their interests or have they been consistent throughout?\n\nAnother way to understand the trustworthiness of an actor is to look at their public communications and to try to determine the extent that the",
      "wordCount": 448
    },
    "tags": [
      {
        "_id": "Lgy35Xh222bwgeGTL",
        "name": "Government",
        "slug": "government"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "hueNHXKc4xdn6cfB4",
    "title": "On the Confusion between Inner and Outer Misalignment",
    "slug": "on-the-confusion-between-inner-and-outer-misalignment",
    "url": null,
    "baseScore": 17,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2024-03-25T11:59:34.553Z",
    "contents": {
      "markdown": "Here’s my take on why the distinction between inner and outer-alignment frame is weird/unclear/ambiguous in some circumstances: My understanding is that these terms were originally used when talking about AGI. So outer alignment involved writing down a reward or utility function for all of human values and inner alignment involves getting these values in the AI.  \n  \nHowever, it gets confusing when you use these terms in relation to narrow AI. For a narrow AI, there’s a sense in which we feel that we should only have to define the reward on that narrow task. ie. if we want an AI to be good at answering questions, an objective that rewards it for correct answers and penalises itself for incorrect answers feels like a correct reward function for that domain. So if things go wrong and it kidnaps humans and forces us to ask it lots of easy questions so it can score higher, we’re not sure whether to say that it’s inner or outer alignment. On one hand, if our reward function penalised kidnapping humans (which is something we indeed want penalised) then it wouldn’t have done it. So we are tempted to say it is outer misalignment. On the other hand, many people also have an intuition that we’ve defined the reward function correctly on that domain and that the problem is that our AI didn’t generalise correctly from a correct specification. This pulls us in the opposite direction, towards saying it is inner misalignment.  \n  \nNotice that what counts as a proper reward function is only unclear because we’re talking about narrow AI. If we were talking about AGI, then *of cou*rse our utility function would be incomplete if it doesn’t specify that it shouldn’t kidnap us in order to do better at a question-answering task. It’s an AGI, so that’s in scope. But when we’re talking about narrow AI, it feels as though we shouldn’t have to specify it or provide anti-kidnapping training data. We feel like it should just learn it automatically on the limited domain, ie. that avoiding kidnapping is the responsibility of the training process, not of the reward function.\n\nHence the confusion. The resolution is relatively simple: define how you want to partition responsibilities between the reward function and the training process.",
      "plaintextDescription": "Here’s my take on why the distinction between inner and outer-alignment frame is weird/unclear/ambiguous in some circumstances: My understanding is that these terms were originally used when talking about AGI. So outer alignment involved writing down a reward or utility function for all of human values and inner alignment involves getting these values in the AI.\n\nHowever, it gets confusing when you use these terms in relation to narrow AI. For a narrow AI, there’s a sense in which we feel that we should only have to define the reward on that narrow task. ie. if we want an AI to be good at answering questions, an objective that rewards it for correct answers and penalises itself for incorrect answers feels like a correct reward function for that domain. So if things go wrong and it kidnaps humans and forces us to ask it lots of easy questions so it can score higher, we’re not sure whether to say that it’s inner or outer alignment. On one hand, if our reward function penalised kidnapping humans (which is something we indeed want penalised) then it wouldn’t have done it. So we are tempted to say it is outer misalignment. On the other hand, many people also have an intuition that we’ve defined the reward function correctly on that domain and that the problem is that our AI didn’t generalise correctly from a correct specification. This pulls us in the opposite direction, towards saying it is inner misalignment.\n\nNotice that what counts as a proper reward function is only unclear because we’re talking about narrow AI. If we were talking about AGI, then of course our utility function would be incomplete if it doesn’t specify that it shouldn’t kidnap us in order to do better at a question-answering task. It’s an AGI, so that’s in scope. But when we’re talking about narrow AI, it feels as though we shouldn’t have to specify it or provide anti-kidnapping training data. We feel like it should just learn it automatically on the limited domain, ie. that avoiding kidnapping is th",
      "wordCount": 378
    },
    "tags": [
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "slug": "outer-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      },
      {
        "id": "BisjoDrd3oNatDu7X",
        "name": "Outer Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "Dw5Z6wtTgk4Fikz9f",
      "tag_name": "Inner Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "GjsbNNP2dv9p3LrDG",
    "title": "The Best Essay (Paul Graham)",
    "slug": "the-best-essay-paul-graham",
    "url": "https://paulgraham.com/best.html",
    "baseScore": 25,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2024-03-11T19:25:42.176Z",
    "contents": {
      "markdown": "Sharing because a lot of us authoring posts on Less Wrong are trying to write something that is useful and insightful.",
      "plaintextDescription": "Sharing because a lot of us authoring posts on Less Wrong are trying to write something that is useful and insightful.",
      "wordCount": 21
    },
    "tags": [
      {
        "_id": "7mTviCYysGmLqiHai",
        "name": "Writing (communication method)",
        "slug": "writing-communication-method"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "MZ6JD4LaPGRL5K6aj",
    "title": "Can we get an AI to \"do our alignment homework for us\"?",
    "slug": "can-we-get-an-ai-to-do-our-alignment-homework-for-us",
    "url": null,
    "baseScore": 55,
    "voteCount": 27,
    "viewCount": null,
    "commentCount": 33,
    "createdAt": null,
    "postedAt": "2024-02-26T07:56:22.320Z",
    "contents": {
      "markdown": "Eliezer frequently claims that AI cannot \"do our alignment homework for us\". OpenAI disagrees and is pursuing Superalignment as their main alignment strategy.\n\nWho is correct?",
      "plaintextDescription": "Eliezer frequently claims that AI cannot \"do our alignment homework for us\". OpenAI disagrees and is pursuing Superalignment as their main alignment strategy.\n\nWho is correct?",
      "wordCount": 26
    },
    "tags": [
      {
        "_id": "c42eTtBCXyJmtpqwZ",
        "name": "AI-Assisted Alignment",
        "slug": "ai-assisted-alignment"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Gx74coCsm7WtAmHYC",
    "title": "What's the theory of impact for activation vectors?",
    "slug": "what-s-the-theory-of-impact-for-activation-vectors",
    "url": null,
    "baseScore": 61,
    "voteCount": 20,
    "viewCount": null,
    "commentCount": 12,
    "createdAt": null,
    "postedAt": "2024-02-11T07:34:48.536Z",
    "contents": {
      "markdown": "[Activation vectors](https://www.lesswrong.com/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector) are really, really cool, but what is the theory of impact for this work?\n\n*   Is the hope that activation vectors will allow us to actually gain perfect control over a model to get it to do exactly what we want it to do?\n*   Is the hope that a new technique that builds upon activation vectors lets us do that instead?\n*   Is the hope that this technique allows us to marginally decrease the risks of powerful models in a Hail Mary attempt? Or perhaps to buy us more time to solve the problem?\n*   Is the hope just that learning more about how neural networks work will allow us to theorize better about how to control them?",
      "plaintextDescription": "Activation vectors are really, really cool, but what is the theory of impact for this work?\n\n * Is the hope that activation vectors will allow us to actually gain perfect control over a model to get it to do exactly what we want it to do?\n * Is the hope that a new technique that builds upon activation vectors lets us do that instead?\n * Is the hope that this technique allows us to marginally decrease the risks of powerful models in a Hail Mary attempt? Or perhaps to buy us more time to solve the problem?\n * Is the hope just that learning more about how neural networks work will allow us to theorize better about how to control them?",
      "wordCount": 121
    },
    "tags": [
      {
        "_id": "QdTRKbR4MJSz4JWvn",
        "name": "Activation Engineering",
        "slug": "activation-engineering"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "TgKGpTvXQmPem9kcF",
    "title": "Notice When People Are Directionally Correct",
    "slug": "notice-when-people-are-directionally-correct",
    "url": null,
    "baseScore": 137,
    "voteCount": 69,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2024-01-14T14:12:37.090Z",
    "contents": {
      "markdown": "I started watching [Peter Zeihan](https://www.youtube.com/@ZeihanonGeopolitics) videos last year.\n\nHe shares a lot of interesting information, although he seems to have a very strong bias towards doom and gloom.\n\nOne thing in particular stood out to me as completely absurd: his claim that global trade is going to collapse due to piracy as the US pulls back from ensuring freedom of the waters.\n\nMy immediate thought: \"Come on, mate, this isn't the 17th century! Pirates aren't a real issue these days. Technology has rendered them obsolete\".\n\nGiven this, I was absolutely shocked when I heard that missile attacks by Houthi rebels had caused most of the largest shipping companies to decide to avoid the Bab el-Mandeb Strait and to sail around Africa instead.\n\nThis has recently triggered the US to form an alliance to maintain freedom of shipping there and the US recently performed airstrikes in retaliation. It won't surprise me if this whole issue is resolved relatively soon and if that happens, then the easy thing to do would be to go back to my original beliefs: \"Silly me, I was worried for a second that Peter Zeihan might be correct, but that was just me falling for sensationalism. The whole incident was obviously never going to be anything. I should forget all about it\".\n\nI believe that this would be a mistake. It would be very easy to forget it, but something like the Houthi's being able to cause as much disruption as they have been able to was outside of my model. One option would be to label it as a freak incident; another would be to say that shooting missiles isn't exactly piracy so this doesn't count. However, the problem with this is that it's too easy. When there's ambiguity, humans have a strong tendency to make up excuses as to why they were right all along, so I think it's worthwhile pushing toward the other end of the spectrum and assuming that there probably were ways in which you could have done better.\n\nI tried searching for possible ways that I should update and the following thoughts came to mind, which I'll convey because they are illustrative:\n\n• I have heard a few people suggest in various contexts that many countries have been coasting and relying on the US for defense, but it was just floating around in my head as something that people say that might or might not be true. I haven't really delved into this, but I'm starting to suspect I should put more weight on this belief.  \n• I hadn't considered the possibility that a country with a weak navy might have a significant lead time on developing one that is stronger.  \n• I hadn't considered the possibility that pirates might be aligned with a larger proto-state actor, as opposed to being individual criminals.  \n• I hadn't considered the possibility that a non-state actor might be able to impede shipping and that other countries would have at least some reluctance to take action against that actor because of diplomatic considerations.  \n• I hadn't considered that some people in the West might support such an actor for political reasons.  \n• Even though I was aware of the Somalian pirate issues from years ago, I didn't properly take this into account. These pirates were easily defeated when nations got serious, which probably played a role in my predictions, but I needed to also update in relation to this ever having been an issue at all.  \n• Forgetting that contexts can dramatically change: events that once seemed impossible regularly happen.\n\nMy point is that there is a lot I can learn from this incident, even if it ends up being resolved quickly.\n\nI suspect it's rare to ever really fully grasp all of the learnings from a particular incident (in contrast, I suspect most people just grab one learning from an incident and declare themselves to be finished having learned from it).\n\nIf you haven't made a large number of small updates, you've probably missed updates that you should have made.\n\n(I just want to note that I love having the handle \"directionally correct\". It's so much easier to say that something like \"I don't think X is correct on all points, but I think a lot of their points are correct\". I would love to know where the rise in this term is coming from).  \n  \n**Further update:** Iranian seizure of the [MCS Aries](https://www.wikiwand.com/en/Iranian_seizure_of_the_MCS_Aries)",
      "plaintextDescription": "I started watching Peter Zeihan videos last year.\n\nHe shares a lot of interesting information, although he seems to have a very strong bias towards doom and gloom.\n\nOne thing in particular stood out to me as completely absurd: his claim that global trade is going to collapse due to piracy as the US pulls back from ensuring freedom of the waters.\n\nMy immediate thought: \"Come on, mate, this isn't the 17th century! Pirates aren't a real issue these days. Technology has rendered them obsolete\".\n\nGiven this, I was absolutely shocked when I heard that missile attacks by Houthi rebels had caused most of the largest shipping companies to decide to avoid the Bab el-Mandeb Strait and to sail around Africa instead.\n\nThis has recently triggered the US to form an alliance to maintain freedom of shipping there and the US recently performed airstrikes in retaliation. It won't surprise me if this whole issue is resolved relatively soon and if that happens, then the easy thing to do would be to go back to my original beliefs: \"Silly me, I was worried for a second that Peter Zeihan might be correct, but that was just me falling for sensationalism. The whole incident was obviously never going to be anything. I should forget all about it\".\n\nI believe that this would be a mistake. It would be very easy to forget it, but something like the Houthi's being able to cause as much disruption as they have been able to was outside of my model. One option would be to label it as a freak incident; another would be to say that shooting missiles isn't exactly piracy so this doesn't count. However, the problem with this is that it's too easy. When there's ambiguity, humans have a strong tendency to make up excuses as to why they were right all along, so I think it's worthwhile pushing toward the other end of the spectrum and assuming that there probably were ways in which you could have done better.\n\nI tried searching for possible ways that I should update and the following thoughts came to mind, wh",
      "wordCount": 739
    },
    "tags": [
      {
        "_id": "4R8JYu4QF2FqzJxE5",
        "name": "Heuristics & Biases",
        "slug": "heuristics-and-biases"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "FCgpT4sj5BSEdeHHy",
    "title": "Are Metaculus AI Timelines Inconsistent?",
    "slug": "are-metaculus-ai-timelines-inconsistent",
    "url": null,
    "baseScore": 17,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2024-01-02T06:47:18.114Z",
    "contents": {
      "markdown": "The Metaculus prediction markets on AI timelines are the most referenced ones that I've seen.  \n  \nThe two main ones are as follows:\n\n[When will the first weakly general AI system be devised, tested, and publicly announced?](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/): Current Community Prediction: April 14th, 2026\n\nCriteria:\n\n*   Able to reliably pass a Turing test of the type that would win the [Loebner Silver Prize](https://www.metaculus.com/questions/73/will-the-silver-turing-test-be-passed-by-2026/).\n*   Able to score 90% or more on a robust version of the [Winograd Schema Challenge](https://www.metaculus.com/questions/644/what-will-be-the-best-score-in-the-20192020-winograd-schema-ai-challenge/), e.g. the [\"Winogrande\" challenge](https://arxiv.org/abs/1907.10641) or comparable data set for which human performance is at 90+%\n*   Be able to score 75th percentile (as compared to the corresponding year's human students; this was a score of 600 in 2016) on all the full mathematics section of a circa-2015-2020 standard SAT exam, using just images of the exam pages and having less than ten SAT exams as part of the training data. (Training on other corpuses of math problems is fair game as long as they are arguably distinct from SAT exams.)\n*   Be able to learn the classic Atari game \"Montezuma's revenge\" (based on just visual inputs and standard controls) and explore all 24 rooms based on the equivalent of less than 100 hours of real-time play (see [closely-related question](https://www.metaculus.com/questions/486/when-will-an-ai-achieve-competency-in-the-atari-classic-montezumas-revenge/).)\n\n[When will the First General AI System be Devised, Tested and Publicly Announced?](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/): Current Community Prediction: January 26th 2031\n\nCriteria:\n\n*   Able to reliably pass a 2-hour, adversarial Turing test during which the participants can send text, images, and audio files (as is done in ordinary text messaging applications) during the course of their conversation. An 'adversarial' Turing test is one in which the human judges are instructed to ask interesting and difficult questions, designed to advantage human participants, and to successfully unmask the computer as an impostor. A single demonstration of an AI passing such a Turing test, or one that is sufficiently similar, will be sufficient for this condition, so long as the test is well-designed to the estimation of Metaculus Admins.\n*   Has general robotic capabilities, of the type able to autonomously, when equipped with appropriate actuators and when given human-readable instructions, satisfactorily assemble a (or the equivalent of a) [circa-2021 Ferrari 312 T4 1:8 scale automobile model](https://www.deagostini.com/uk/assembly-guides/). A single demonstration of this ability, or a sufficiently similar demonstration, will be considered sufficient.\n*   High competency at a diverse fields of expertise, as measured by achieving at least 75% accuracy in every task and 90% mean accuracy across all tasks in the Q&A dataset developed by [Dan Hendrycks et al.](https://arxiv.org/abs/2009.03300).\n*   Able to get top-1 strict accuracy of at least 90.0% on interview-level problems found in the APPS benchmark introduced by [Dan Hendrycks, Steven Basart et al](https://arxiv.org/abs/2105.09938). Top-1 accuracy is distinguished, as in the paper, from top-k accuracy in which k outputs from the model are generated, and the best output is selected.\n\nIf we subtract the two, then we get around 57 months.  \n  \nHowever, there is a market focusing just on the difference between the two which has a substantially lower prediction.\n\n[After a (weak) AGI is created, how many months will it be before the first superintelligent AI is created?](https://www.metaculus.com/questions/9062/time-from-weak-agi-to-superintelligence/) Current Community Prediction 25.82 months\n\nWhy does this differ from the figure we got before? Is this an inconsistency or is there an important difference between the two markets?  \n  \nI tried looking at this difference market and I could confirm it used the same definition of strong AI, but I'm unsure what definition it is using for weak AGI.",
      "plaintextDescription": "The Metaculus prediction markets on AI timelines are the most referenced ones that I've seen.\n\nThe two main ones are as follows:\n\nWhen will the first weakly general AI system be devised, tested, and publicly announced?: Current Community Prediction: April 14th, 2026\n\nCriteria:\n\n * Able to reliably pass a Turing test of the type that would win the Loebner Silver Prize.\n * Able to score 90% or more on a robust version of the Winograd Schema Challenge, e.g. the \"Winogrande\" challenge or comparable data set for which human performance is at 90+%\n * Be able to score 75th percentile (as compared to the corresponding year's human students; this was a score of 600 in 2016) on all the full mathematics section of a circa-2015-2020 standard SAT exam, using just images of the exam pages and having less than ten SAT exams as part of the training data. (Training on other corpuses of math problems is fair game as long as they are arguably distinct from SAT exams.)\n * Be able to learn the classic Atari game \"Montezuma's revenge\" (based on just visual inputs and standard controls) and explore all 24 rooms based on the equivalent of less than 100 hours of real-time play (see closely-related question.)\n\nWhen will the First General AI System be Devised, Tested and Publicly Announced?: Current Community Prediction: January 26th 2031\n\nCriteria:\n\n * Able to reliably pass a 2-hour, adversarial Turing test during which the participants can send text, images, and audio files (as is done in ordinary text messaging applications) during the course of their conversation. An 'adversarial' Turing test is one in which the human judges are instructed to ask interesting and difficult questions, designed to advantage human participants, and to successfully unmask the computer as an impostor. A single demonstration of an AI passing such a Turing test, or one that is sufficiently similar, will be sufficient for this condition, so long as the test is well-designed to the estimation of Metaculus Admins.\n ",
      "wordCount": 580
    },
    "tags": [
      {
        "_id": "R6dqPii4cyNpuecLt",
        "name": "Prediction Markets",
        "slug": "prediction-markets"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "k9z9fzpAgMLGQG9hg",
    "title": "Random Musings on Theory of Impact for Activation Vectors",
    "slug": "random-musings-on-theory-of-impact-for-activation-vectors",
    "url": null,
    "baseScore": 8,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-12-07T13:07:08.215Z",
    "contents": {
      "markdown": "I want to share some thoughts on why activation vectors could be important, but before you read this post, you should know three things:\n\n1.  I don't know what I'm talking about\n2.  I don't know what I'm talking about\n3.  I definitely don't know what I'm talking about\n\nAlso, maybe this post is redundant and unnecessary[^qrpuhuwrepp] because it's already been explained somewhere I haven't seen. Or maybe I've seen someone write this elsewhere, but just can't remember the source.\n\nBut here goes:\n\nWhy might activation vectors be important for alignment when we have other ways of controlling the network such as prompting or fine-tuning?\n\nOne view of activation vectors is that they're a hack or a toy. Isn't it cool that you can steer a network by just passing a word through the network and adding it?\n\nAnother view would be that they're tapping into something fundamental, see Beren's [Deep learning models might be secretly (almost) linear](https://www.lesswrong.com/posts/JK9nxcBhQfzEgjjqe/deep-learning-models-might-be-secretly-almost-linear). In which case, we should expect this to keep working or even work better as networks scale up, rather than randomly breaking/stop working.\n\nAnother frame is as follows: language is an awkward format for neural networks to operate in, so they translate it into a latent space that separates out the important components, with of course some degree of superposition. Prompting and fine-tuning are roundabout ways of influencing the behavior of these networks: we're either pushing it into a particular simulation or training the network at to get better at fooling to think that it's doing a good job.\n\nIdeally, we'd just have enough interpretability knowledge that we'd intervene on the latent space directly, setting or incrementing the exact right neurons. Activation vectors are a lot more scattergun than this, but they're closer to the ideal and work as a proof of concept.\n\nThe trick of performing, say, the subtraction 'happy-sad' to get a happiness vector demonstrates how this can be refined by zeroing out all of the features that these have in common. These features like both being English, both being not overly formal, both being adjectives, ect. are probably mostly irrelevant. More advanced methods like [Inference-Time Intervention](https://arxiv.org/abs/2306.03341) allow us to refine this further by only intervening on particular heads. This technique is still relatively simple; plausibly if we can find the right way of narrowing down our intervention we may achieve or get pretty close to our dream of directly intervening on the correct latents.\n\n[^qrpuhuwrepp]: I know that the word \"unnecessary\" is redundant and unnecessary here.",
      "plaintextDescription": "I want to share some thoughts on why activation vectors could be important, but before you read this post, you should know three things:\n\n 1. I don't know what I'm talking about\n 2. I don't know what I'm talking about\n 3. I definitely don't know what I'm talking about\n\nAlso, maybe this post is redundant and unnecessary[1] because it's already been explained somewhere I haven't seen. Or maybe I've seen someone write this elsewhere, but just can't remember the source.\n\nBut here goes:\n\nWhy might activation vectors be important for alignment when we have other ways of controlling the network such as prompting or fine-tuning?\n\nOne view of activation vectors is that they're a hack or a toy. Isn't it cool that you can steer a network by just passing a word through the network and adding it?\n\nAnother view would be that they're tapping into something fundamental, see Beren's Deep learning models might be secretly (almost) linear. In which case, we should expect this to keep working or even work better as networks scale up, rather than randomly breaking/stop working.\n\nAnother frame is as follows: language is an awkward format for neural networks to operate in, so they translate it into a latent space that separates out the important components, with of course some degree of superposition. Prompting and fine-tuning are roundabout ways of influencing the behavior of these networks: we're either pushing it into a particular simulation or training the network at to get better at fooling to think that it's doing a good job.\n\nIdeally, we'd just have enough interpretability knowledge that we'd intervene on the latent space directly, setting or incrementing the exact right neurons. Activation vectors are a lot more scattergun than this, but they're closer to the ideal and work as a proof of concept.\n\nThe trick of performing, say, the subtraction 'happy-sad' to get a happiness vector demonstrates how this can be refined by zeroing out all of the features that these have in common. The",
      "wordCount": 405
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "euwMMxwuBeS5QZkC4",
    "title": "Goodhart's Law Example: Training Verifiers to Solve Math Word Problems",
    "slug": "goodhart-s-law-example-training-verifiers-to-solve-math-word",
    "url": "https://arxiv.org/abs/2110.14168",
    "baseScore": 27,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 2,
    "createdAt": null,
    "postedAt": "2023-11-25T00:53:26.841Z",
    "contents": {
      "markdown": "Sharing because of how clearly this paper demonstrates the risks associated with considering too many different solutions:  \n  \n\"At test time, we can choose to generate arbitrarily many solutions to be judged by the verifier before selecting the highest-ranked completion. Figure 7a shows how 6B verifier performance varies with the number of completions per test problem. At this scale, performance improves as we increase the number of completions up to 400. Beyond this point, performance starts to decrease. This suggests that the benefits of search are eventually outweighed by the risk of finding adversarial solutions that fool the verifier. In general, we evaluate verifier test performance using 100 completions, since this captures most of the benefits of verification with a relatively modest compute cost.\"  \n  \nThey propose and evaluate a solution to this:  \n  \n\"To further increase performance, we can take a majority vote among the top verifier-ranked solutions instead of selecting only the single top solution. This voting process considers only the final answer reached by the individual solutions: the final answer selected is the one with the most votes. Figure 7b shows how performance varies as we allow a greater number of top samples to cast a vote. Unsurprisingly, when starting with a greater number of samples, we can afford to allow a greater number of samples to cast a vote. When we have only 100 samples, it is optimal to allow only the top 3-5 samples to cast a vote. When we have 3200 samples, it is approximately optimal to allow the top 30 to cast a vote.\"\n\nFurther empirical investigation into Goodhart's Law could prove valuable for alignment.",
      "plaintextDescription": "Sharing because of how clearly this paper demonstrates the risks associated with considering too many different solutions:\n\n\"At test time, we can choose to generate arbitrarily many solutions to be judged by the verifier before selecting the highest-ranked completion. Figure 7a shows how 6B verifier performance varies with the number of completions per test problem. At this scale, performance improves as we increase the number of completions up to 400. Beyond this point, performance starts to decrease. This suggests that the benefits of search are eventually outweighed by the risk of finding adversarial solutions that fool the verifier. In general, we evaluate verifier test performance using 100 completions, since this captures most of the benefits of verification with a relatively modest compute cost.\"\n\nThey propose and evaluate a solution to this:\n\n\"To further increase performance, we can take a majority vote among the top verifier-ranked solutions instead of selecting only the single top solution. This voting process considers only the final answer reached by the individual solutions: the final answer selected is the one with the most votes. Figure 7b shows how performance varies as we allow a greater number of top samples to cast a vote. Unsurprisingly, when starting with a greater number of samples, we can afford to allow a greater number of samples to cast a vote. When we have only 100 samples, it is optimal to allow only the top 3-5 samples to cast a vote. When we have 3200 samples, it is approximately optimal to allow the top 30 to cast a vote.\"\n\nFurther empirical investigation into Goodhart's Law could prove valuable for alignment.",
      "wordCount": 269
    },
    "tags": [
      {
        "_id": "PvridmTCj2qsugQCH",
        "name": "Goodhart's Law",
        "slug": "goodhart-s-law"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "RgLKJCjmfmHePubAk",
    "title": "Upcoming Feedback Opportunity on Dual-Use Foundation Models",
    "slug": "upcoming-feedback-opportunity-on-dual-use-foundation-models",
    "url": null,
    "baseScore": 3,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-11-02T04:28:11.586Z",
    "contents": {
      "markdown": "From Biden's recent [executive order](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/):\n\n\"4.6.  Soliciting Input on Dual-Use Foundation Models with Widely Available Model Weights.  When the weights for a dual-use foundation model are widely available — such as when they are publicly posted on the Internet — there can be substantial benefits to innovation, but also substantial security risks, such as the removal of safeguards within the model.  To address the risks and potential benefits of dual-use foundation models with widely available weights, within 270 days of the date of this order, the Secretary of Commerce, acting through the Assistant Secretary of Commerce for Communications and Information, and in consultation with the Secretary of State, shall:  \n  \n     (a)  solicit input from the private sector, academia, civil society, and other stakeholders through a public consultation process on potential risks, benefits, other implications, and appropriate policy and regulatory approaches related to dual-use foundation models for which the model weights are widely available, including:\n\n          (i)    risks associated with actors fine-tuning dual-use foundation models for which the model weights are widely available or removing those models’ safeguards;\n\n          (ii)   benefits to AI innovation and research, including research into AI safety and risk management, of dual-use foundation models for which the model weights are widely available; and\n\n          (iii)  potential voluntary, regulatory, and international mechanisms to manage the risks and maximize the benefits of dual-use foundation models for which the model weights are widely available; and  \n  \n     (b)  based on input from the process described in subsection 4.6(a) of this section, and in consultation with the heads of other relevant agencies as the Secretary of Commerce deems appropriate, submit a report to the President on the potential benefits, risks, and implications of dual-use foundation models for which the model weights are widely available, as well as policy and regulatory recommendations pertaining to those models.\"\n\nThis seems important since the open-sourcing of model weights essentially leads to the irreversible proliferation of capabilities. I suspect that there's a surprisingly high chance that a good future depends on the right outcome being achieved here.",
      "plaintextDescription": "From Biden's recent executive order:\n\n\"4.6.  Soliciting Input on Dual-Use Foundation Models with Widely Available Model Weights.  When the weights for a dual-use foundation model are widely available — such as when they are publicly posted on the Internet — there can be substantial benefits to innovation, but also substantial security risks, such as the removal of safeguards within the model.  To address the risks and potential benefits of dual-use foundation models with widely available weights, within 270 days of the date of this order, the Secretary of Commerce, acting through the Assistant Secretary of Commerce for Communications and Information, and in consultation with the Secretary of State, shall:\n\n     (a)  solicit input from the private sector, academia, civil society, and other stakeholders through a public consultation process on potential risks, benefits, other implications, and appropriate policy and regulatory approaches related to dual-use foundation models for which the model weights are widely available, including:\n\n          (i)    risks associated with actors fine-tuning dual-use foundation models for which the model weights are widely available or removing those models’ safeguards;\n\n          (ii)   benefits to AI innovation and research, including research into AI safety and risk management, of dual-use foundation models for which the model weights are widely available; and\n\n          (iii)  potential voluntary, regulatory, and international mechanisms to manage the risks and maximize the benefits of dual-use foundation models for which the model weights are widely available; and\n\n     (b)  based on input from the process described in subsection 4.6(a) of this section, and in consultation with the heads of other relevant agencies as the Secretary of Commerce deems appropriate, submit a report to the President on the potential benefits, risks, and implications of dual-use foundation models for which the model weights are widely available, as wel",
      "wordCount": 335
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "3PXBK2an9dcRoNoid",
    "title": "On Having No Clue",
    "slug": "on-having-no-clue",
    "url": null,
    "baseScore": 20,
    "voteCount": 14,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2023-11-01T01:36:10.520Z",
    "contents": {
      "markdown": "Let's suppose you're trying to figure out if something is going to be A or B, but you have no clue.\n\nWhat probability should you assign to each option?\n\nSome people might say that you should assign 50/50. After all, the argument goes, if you assigned 40/60 or 60/40, well it sure sounds like you're favoring one option over another.\n\nThat's a very persuasive argument, but unfortunately, it's more complicated than that.\n\nIt may be the case that A splits into two options A1 and A2 where again we have no clue whether A1 is more likely than A2 or how these compare to B. In which case, the same argument would suggest that we should go with 33/33/33 (rounding down).\n\nThis seems to be a contradiction. What should we make of this?\n\nFirst of all, I think we should accept that this exact process of reasoning leads to a contradiction. There's nothing fancy going on here. No dubious steps that could give us an out.\n\nWe tried to say that if we had no clue that the logical thing to do is to assign equal probability to each option, we forgot that we were implicitly assuming that we should favor our particular way of carving up probability space.\n\nIn other words, we did need a clue after all. So what kind of clue is this exactly?\n\nWell, surely in order to be justified in carving up the space a particular way, we'd need to have a reason to believe that each possibility is equally likely a priori.\n\nSadly, this is circular. We wanted to defend equal probabilities by asserting that our way of carving up the space was reasonable, but then we tried to assert that it was reasonable by claiming that each possibility had equal probability.\n\nThe problem is that we are making an assumption, but rather than owning it, we're trying to deny that we're making any assumption at all, ie. \"I'm not assuming a priori A and B have equal probability based on my subjective judgement, I'm using the principle of indifference\". Roll to disbelieve.\n\nContra Descartes, if you start with nothing, you can't get anywhere.",
      "plaintextDescription": "Let's suppose you're trying to figure out if something is going to be A or B, but you have no clue.\n\nWhat probability should you assign to each option?\n\nSome people might say that you should assign 50/50. After all, the argument goes, if you assigned 40/60 or 60/40, well it sure sounds like you're favoring one option over another.\n\nThat's a very persuasive argument, but unfortunately, it's more complicated than that.\n\nIt may be the case that A splits into two options A1 and A2 where again we have no clue whether A1 is more likely than A2 or how these compare to B. In which case, the same argument would suggest that we should go with 33/33/33 (rounding down).\n\nThis seems to be a contradiction. What should we make of this?\n\nFirst of all, I think we should accept that this exact process of reasoning leads to a contradiction. There's nothing fancy going on here. No dubious steps that could give us an out.\n\nWe tried to say that if we had no clue that the logical thing to do is to assign equal probability to each option, we forgot that we were implicitly assuming that we should favor our particular way of carving up probability space.\n\nIn other words, we did need a clue after all. So what kind of clue is this exactly?\n\nWell, surely in order to be justified in carving up the space a particular way, we'd need to have a reason to believe that each possibility is equally likely a priori.\n\nSadly, this is circular. We wanted to defend equal probabilities by asserting that our way of carving up the space was reasonable, but then we tried to assert that it was reasonable by claiming that each possibility had equal probability.\n\nThe problem is that we are making an assumption, but rather than owning it, we're trying to deny that we're making any assumption at all, ie. \"I'm not assuming a priori A and B have equal probability based on my subjective judgement, I'm using the principle of indifference\". Roll to disbelieve.\n\nContra Descartes, if you start with nothing, you can't get an",
      "wordCount": 360
    },
    "tags": [
      {
        "_id": "4fxcbJ8xSv4SAYkkx",
        "name": "Bayesianism",
        "slug": "bayesianism"
      },
      {
        "_id": "sMzY3PuSfYGdtHWrE",
        "name": "Noticing Confusion",
        "slug": "noticing-confusion"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "gYwfkjK8vsowfDkZu",
    "title": "Is Yann LeCun strawmanning AI x-risks?",
    "slug": "is-yann-lecun-strawmanning-ai-x-risks",
    "url": null,
    "baseScore": 26,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2023-10-19T11:35:08.167Z",
    "contents": {
      "markdown": "Tom Chivers expresses his frustration with Yann LeCun:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4ffa54cfbfcdb8daffebed8cdf18d5880d3f19b9d78e86f9.png)\n\nI also find his comments here frustrating, but I want to offer another possible explanation.  \n  \nEven though basically no one in the AI Safety community makes this argument, unfortunately, many people in the general population think about AI this way.\n\nYann probably thinks that it is a higher priority for him to address this belief held by a much larger range of people than it is for him to address the AI Safety crowd's arguments.\n\nHe may think that he's in a situation where if he focused on addressing the best arguments, he would lose the political battle due to naive people believing the \"strawman\" arguments.\n\nIn light of this, I don't think it's completely accurate to say he's addressing a strawman. I find it frustrating as well and I wish he'd address us directly. But I also understand the incentives that lead him to do what he does.",
      "plaintextDescription": "Tom Chivers expresses his frustration with Yann LeCun:\n\nI also find his comments here frustrating, but I want to offer another possible explanation.\n\nEven though basically no one in the AI Safety community makes this argument, unfortunately, many people in the general population think about AI this way.\n\nYann probably thinks that it is a higher priority for him to address this belief held by a much larger range of people than it is for him to address the AI Safety crowd's arguments.\n\nHe may think that he's in a situation where if he focused on addressing the best arguments, he would lose the political battle due to naive people believing the \"strawman\" arguments.\n\nIn light of this, I don't think it's completely accurate to say he's addressing a strawman. I find it frustrating as well and I wish he'd address us directly. But I also understand the incentives that lead him to do what he does.",
      "wordCount": 158
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ziNCZEm7FE9LHxLai",
    "title": "Don't Dismiss Simple Alignment Approaches",
    "slug": "don-t-dismiss-simple-alignment-approaches",
    "url": null,
    "baseScore": 138,
    "voteCount": 63,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2023-10-07T00:35:26.789Z",
    "contents": {
      "markdown": "I remember that when I first started in alignment there was this belief that in order to make alignment progress, you needed to be a genius. With the rise of interpretability, this expectation has moderated somewhat, but I still think it has influence and often leads people to overcomplicate.\n\nMany recent alignment breakthroughs have been found by keeping it simple[^1v3hwr53kci]:\n\n**Lie Detection**:  \n• [Contrast-consistent search](https://arxiv.org/abs/2212.03827) (better known as discovering latent knowledge[^42txf880mvx]) reads the truth out of a model without looking at the outputs, simply by using a linear probe with only two elements in its activation function[^2v4ey0p83c9].  \n• [Pacchiardi, Owain and others](https://arxiv.org/abs/2309.15840) use a logistic regression classifier[^2dyeooknd0h] to predict whether a mis al was truthful from its answers to further questions. They were able to demonstrate this using three different kinds of questions and the results even generalised across models. This technique seems like it could be useful in practise, even if it doesn't scale all the way to superintelligence.   \n  \n**Superposition**: [Dictionary learning](https://transformer-circuits.pub/2023/monosemantic-features/#why-not-architectures) makes substantial progress on superposition by simply using a one hidden layer multi-layer perceptron as a sparse auto-encoder with L2 reconstruction loss and a L1 loss on hidden layer activations. Anthropic actually chose to avoid more sophisticated methods, as they were worried they might recover features that the model doesn't actually utilise.  \n  \n**Model-steering**: Activation Engineering: [Activation addition](https://www.lesswrong.com/posts/HWxLQvzJGeXoLPJWd/actadd-steering-language-models-without-optimization) allows us to steer models by simply performing a forward pass for the concept you want to activate, saving the activations of a particular layer as a vector and adding them to future forward pass. See also Turntrout's prior work on steering a mouse by [subtracting a cheese vector](https://www.lesswrong.com/posts/cAC4AXiNC5ig6jQnc/understanding-and-controlling-a-maze-solving-policy-network)[^vt1x0nq32oh] and [inference-time intervention](https://arxiv.org/abs/2306.03341)[^gwv0u8dqysq].[^cv4l100bvy]  \n  \n**World model location**: Neel Nanda found a linear representation in [Othello-GPT](https://www.lesswrong.com/posts/nmxzr2zsjNtjaHh7x/actually-othello-gpt-has-a-linear-emergent-world). This one was a bit trickier: a previous paper had failed to recover it using [linear probes](https://thegradient.pub/othello/). However, Neel managed to do this by searching for a representation of \"my color\" and by applying a bunch of other tweaks. See also [Wes Gurnee's work with Max Tegmark](https://twitter.com/wesg52/status/1709551516577902782), which used linear probes to find linear representations of latitude and longitude; in addition to one representing time[^zynaaklp5vl].  \n  \n**Scalable Interpretability:** OpenAI used a language model in order to label neurons based on the activations for some relevant examples using [few-shot prompting](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html).\n\n**Understanding Implications**[^ky6wdd05sjn]: For a long time it appeared as though it would be [challenging to prevent an AI from taking our instructions too literally](https://medium.com/nautilus-magazine/scary-ai-is-more-fantasia-than-terminator-ac1546b35bfc). Essentially what seems to have happened is that they decided to just use RL on a generative base model and the problem mostly solved itself. [Reinforcement Learning from Human Feedback](https://huggingface.co/blog/rlhf) is admittedly complex - proximal policy optimization isn't simple - but I still think it's quite notable than OpenAI, seems to have basically one-shotted this with InstructGPT[^eduvymbukau]. Even if they hadn't found a solution the first time, it seems likely that we were always going to end up there by iterating on a baseline.  \n  \nI don't want to dismiss the challenges of such research. Producing a result requires getting a lot of details right. Actually carrying out one of these projects would involve a massive amount of work. Even picking the right question to investigate requires a lot of skill. However, simpler techniques have gone further for these problems than I would have originally expected.\n\nI think it's worthwhile speculating why reasonably simple solutions have worked for these problems, and checking whether there are any similarities between them.\n\nFirstly, I think it's worth noting that all of these techniques except for the last two look for linear representations in neural networks. Beren has made a [strong case](https://www.lesswrong.com/posts/JK9nxcBhQfzEgjjqe/deep-learning-models-might-be-secretly-almost-linear) that there is strong evidence of deep learning models being almost linear. In retrospect, it seems intuitive to me that neural networks would store sparse features in linear combinations, rather than with some more complicated form of compression, as this makes it easy to write to and read from these features[^6lkqz2r1t9n].\n\nIn contrast, the last two techniques listed rely on current AI models being very powerful and quite steerable. I admit that two examples aren't very many, but I expect we'll see more in this vein soon.\n\nLooking more broadly, I think it's worth noting that all of these results are in empirical alignment research. I don't think this is a coincidence. Whilst there may be simple solutions to some of the problems of agent foundations, if they were easy to find, they probably would have been found before. In contrast, the last two directly rely on a certain level of capabilities. It's arguable that linear representation techniques indirectly rely upon a certain level of capabilities: insofar as it is beneficial for a model to have a linear representation of particular features, the more powerful our optimization techniques, the more likely our trained models are to produce such a representation.\n\n**An Aside on Contrast-Consistent Search**:\n\nWhen I first encountered this technique, I was awed by it and I felt like I had no conception of how anyone would ever think of something like this. While I still acknowledge the brilliance of this work, it no longer feels completely unthinkable.\n\nI assume Collin already had some intuitions that neural networks were vastly more linear than you might expect. If you buy this and you're trying to read the truth directly out of the network weights, it's then entirely natural to look for a linear direction.\n\nLinear probes were an [already existing technique](https://arxiv.org/abs/1610.01644), so if you knew of this technique and you were making a shortlist of which techniques to investigate, this would make it onto the shortlist.\n\nSuppose you end up in the position of asking \"how can I use a linear probe to find a direction corresponding to truth?\" in a neural network. It'd then be natural to ask what some of the properties of truth are, which would then lead directly to the consistency property Collin leveraged.\n\nSo, smart, brilliant even, but not the kind of thing that is completely unthinkable.\n\nAnd even just knowing to look for linearities would be a massive headstart.\n\n**Final thoughts**:\n\nI suspect that all of these approaches are still very far away from where we need to be. I consider them substantial advances nonetheless for two key reasons: having a baseline helps people choose an appropriate level of ambition, and also makes it easier to empirically discover the key issues in solving a problem more fully.\n\nI'm hoping that increasing awareness that relatively simple techniques have been successful spurs more research progress, by giving people the confidence to actually try to make progress and increasing people's motivation to explore these kinds of approaches for longer before they toss in the towel.  \n  \nIt may very well be that we're just in a particular stage of the field where there's all this low-hanging fruit to pick. Perhaps we'll soon end up in a situation where we need people to look for brilliant conceptual breakthroughs in order to make further progress. However, I think it would be a mistake to just assume people have already looked very hard for simple paths forward. \n\n[^1v3hwr53kci]: I thought about also including this paper on representational engineering which describes, among other things, Linear Artificial Tomograph. However, I haven't had time to finish skimming it yet. \n\n[^42txf880mvx]: Technically, \"Discovering Latent Knowledge\" is the name of the paper and of a problem. However, if I go to people and say \"Have you heard of Contrast-Consistent Search?\", the answer is typically no, but when I ask about \"Discovering Latent Knowledge\", a lot of the time it turns out that they're actually familar with the paper. \n\n[^2v4ey0p83c9]: One component pushes the probability of a statement being true and a statement being false toward adding up to one. A second component pushes the model away from producing probabilities close to 0.5. \n\n[^2dyeooknd0h]: While these are non-linear in their inputs, they assume that the log probability is linear in the inputs. \n\n[^vt1x0nq32oh]: They generate two observations that are the same except for one possessing the cheese.  \n\n[^gwv0u8dqysq]: They train a linear probe with sigmoid activation to discover the truthful direction. Then they intervene by adding a multiple of this to the activation of the layer, scaling by a constant and the standard deviation. \n\n[^cv4l100bvy]: For some useful potential applications, see Nina's work on using activation addition to reduce sycophancy and improving honesty and red-teaming. \n\n[^zynaaklp5vl]: This paper seems like it was mostly intended for outreach purposes. Unfortunately, the framing was a tiny bit off, in that some people felt that this paper/the publicity around it was overclaiming. That said, if this had been handled a bit better, it could have led to significant improvement in the debate. One point I want to emphasize is that you can spend forever debating whether current neural networks have world models and what a world model even means. Or you can just go and make direct empirical progress, and maybe that doesn't convince people, but it will at least push them to clarify what they're looking for. \n\n[^ky6wdd05sjn]: You may object that RLHF is mostly capabilities. I also tend to think about it as being primarily a capabilities advance, but it is an advance in alignment as well. \n\n[^eduvymbukau]: I've heard that the innovation in ChatGPT was more about the user interface and finetuning for that than producing a more powerful model. \n\n[^6lkqz2r1t9n]: While orthogonal features are the simplest, you can only include one feature per dimension, whilst you can fit many more near orthogonal features.",
      "plaintextDescription": "I remember that when I first started in alignment there was this belief that in order to make alignment progress, you needed to be a genius. With the rise of interpretability, this expectation has moderated somewhat, but I still think it has influence and often leads people to overcomplicate.\n\nMany recent alignment breakthroughs have been found by keeping it simple[1]:\n\nLie Detection:\n• Contrast-consistent search (better known as discovering latent knowledge[2]) reads the truth out of a model without looking at the outputs, simply by using a linear probe with only two elements in its activation function[3].\n• Pacchiardi, Owain and others use a logistic regression classifier[4] to predict whether a mis al was truthful from its answers to further questions. They were able to demonstrate this using three different kinds of questions and the results even generalised across models. This technique seems like it could be useful in practise, even if it doesn't scale all the way to superintelligence. \n\nSuperposition: Dictionary learning makes substantial progress on superposition by simply using a one hidden layer multi-layer perceptron as a sparse auto-encoder with L2 reconstruction loss and a L1 loss on hidden layer activations. Anthropic actually chose to avoid more sophisticated methods, as they were worried they might recover features that the model doesn't actually utilise.\n\nModel-steering: Activation Engineering: Activation addition allows us to steer models by simply performing a forward pass for the concept you want to activate, saving the activations of a particular layer as a vector and adding them to future forward pass. See also Turntrout's prior work on steering a mouse by subtracting a cheese vector[5] and inference-time intervention[6].[7]\n\nWorld model location: Neel Nanda found a linear representation in Othello-GPT. This one was a bit trickier: a previous paper had failed to recover it using linear probes. However, Neel managed to do this by searching for a",
      "wordCount": 1182
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "2MpX7eR6qLTDmyeMJ",
    "title": "What evidence is there of LLM's containing world models?",
    "slug": "what-evidence-is-there-of-llm-s-containing-world-models",
    "url": null,
    "baseScore": 17,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 17,
    "createdAt": null,
    "postedAt": "2023-10-04T14:33:19.178Z",
    "contents": {
      "markdown": "Since this still seems to be an area of active debate within the ML community, it may be worthwhile providing a location to gather this evidence all in one place. Please only list one paper per answer as that makes it easier for people to comment on it (and possibly critique the evidence). Also feel free to include evidence of them *not* containing world models.",
      "plaintextDescription": "Since this still seems to be an area of active debate within the ML community, it may be worthwhile providing a location to gather this evidence all in one place. Please only list one paper per answer as that makes it easier for people to comment on it (and possibly critique the evidence). Also feel free to include evidence of them not containing world models.",
      "wordCount": 65
    },
    "tags": [
      {
        "_id": "KmgkrftQuX7jmjjp5",
        "name": "Language Models (LLMs)",
        "slug": "language-models-llms"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xKaJ3YrKL3ms2Bn37",
    "title": "The Role of Groups in the Progression of Human Understanding",
    "slug": "the-role-of-groups-in-the-progression-of-human-understanding",
    "url": null,
    "baseScore": 11,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-09-27T15:09:45.445Z",
    "contents": {
      "markdown": "This post isn't intended to be anything stunningly original, but simply to collect a few basic, but under-utilized observations together in one place.\n\nThe question of how we can progress human knowledge and understanding is one that is of particular interest to people on this website. The most obvious reason why groups are important for these purposes is that often a lot of work is required to move a field forward and when more people are working on a problem, you might expect that more work would get done[^3btpxrlq1ui]. There are other reasons too, but this the main one.\n\nThis immediately leads us to the question of what factors are important for allowing a group to produce knowledge:\n\n*   Focus area: A group's focus area can be broad or it can be narrow. As already noted, a broad focus area can dilute the group's common knowledge. If a group's focus is too narrow, then there will be too few people interested in joining or participating in the group. If the group's focus is too broad, then it'll be less likely to attract people with expertise in a particular area or to develop such talent. If you want to start a new group, picking the right focus area is crucial.\n*   Common knowledge: This is knowledge that basically everyone in a group knows, apart from beginners. This is important as being able to take certain things for granted allows discussions to cover more advanced concepts or for people to explore further down the conversational tree. Unfortunately, only so much knowledge can be common knowledge and this is especially limited when groups have no way to insist that membership is conditional on completing a certain course or program. The broader a group's focus, the more shallow the group's common knowledge will be, but this can be less of a disadvantage if there are synergies among areas. Common knowledge can also backfire if it results in members becoming overly ideological or unknowingly making assumptions.\n*   Norms: It's often valuable for groups to develop their own norms. I expect rationalists to have a tendency to search for a set of perfectly rational group norms, but I suspect that the ideal norms vary heavily depending on the specific group. For example, if you want to know the truth, it's often better for people to be blunter, but this can also be quite off-putting for beginners. The solution to [competing access needs](https://www.lesswrong.com/posts/qzpcyKioYfHWmcasv/you-can-never-be-universally-inclusive) is often to have multiple groups with different norms[^cgbtvwhagu8]. One key factor when deciding norms is to remember that a group cannot coordinate on overly complex norms.\n*   Self-selection: This is often a stronger force than you might expect. Groups don't just attract people with shared interests, but those who align with a group's values. Given how hard it is to change people's attributes and the unpleasantness associated with kicking someone out of a group, self-selection is often a crucial mechanism in allowing a group to be functional.\n*   Origin: It's worth noting that a group often arises in reaction to another group. This can be advantageous as frustration with another group can often be quite motivating. On the other hand, the more that a group is a reaction to another group, the more likely it is to overreact and toss out the baby with the bathwater. One key challenge here is that the more a group adopts nuance, the harder it is for a group to differentiate itself from other groups. And without a clear message, it's hard for a group to attract members.\n*   Subgroups: A group can form a launching pad for a subgroup to form. Subgroups typically form to focus on a specific area. They can also filter more heavily on particular attributes: for example, Bob may only invite people with consistently good takes to his discussion group. Since progress is often tail-heavy, the potential for members to form their own sub-groups is often where most of the impact will be coming from.\n\nAnyway, I hope that this post helps you think about what kinds of groups ought to exist.\n\n[^3btpxrlq1ui]: However, there can be diseconomies of scale where it takes more time to follow all of the work that is being done and more time to filter out the good work from the bad. \n\n[^cgbtvwhagu8]: As an example, Effective Altruism tries harder to be friendlier as recruiting more members arguably allows it to increase its impact. On the other hand, the Less Wrong community is less desirous of growth, lest it lower intellectual standards.",
      "plaintextDescription": "This post isn't intended to be anything stunningly original, but simply to collect a few basic, but under-utilized observations together in one place.\n\nThe question of how we can progress human knowledge and understanding is one that is of particular interest to people on this website. The most obvious reason why groups are important for these purposes is that often a lot of work is required to move a field forward and when more people are working on a problem, you might expect that more work would get done[1]. There are other reasons too, but this the main one.\n\nThis immediately leads us to the question of what factors are important for allowing a group to produce knowledge:\n\n * Focus area: A group's focus area can be broad or it can be narrow. As already noted, a broad focus area can dilute the group's common knowledge. If a group's focus is too narrow, then there will be too few people interested in joining or participating in the group. If the group's focus is too broad, then it'll be less likely to attract people with expertise in a particular area or to develop such talent. If you want to start a new group, picking the right focus area is crucial.\n * Common knowledge: This is knowledge that basically everyone in a group knows, apart from beginners. This is important as being able to take certain things for granted allows discussions to cover more advanced concepts or for people to explore further down the conversational tree. Unfortunately, only so much knowledge can be common knowledge and this is especially limited when groups have no way to insist that membership is conditional on completing a certain course or program. The broader a group's focus, the more shallow the group's common knowledge will be, but this can be less of a disadvantage if there are synergies among areas. Common knowledge can also backfire if it results in members becoming overly ideological or unknowingly making assumptions.\n * Norms: It's often valuable for groups to develop their own",
      "wordCount": 680
    },
    "tags": [
      {
        "_id": "zv7v2ziqexSn5iS9v",
        "name": "Group Rationality",
        "slug": "group-rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "G9GmrYhQm6bKt2XJJ",
    "title": "The Flow-Through Fallacy",
    "slug": "the-flow-through-fallacy",
    "url": null,
    "baseScore": 21,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2023-09-13T04:28:28.390Z",
    "contents": {
      "markdown": "There is something of an informal reasoning fallacy where you promote something that seems like something that we should \"surely do\", something that seems obviously important, but where we haven't thought through all of the steps involved and so we aren't actually justified in assuming that the impact flow through.\n\nHere are some examples:\n\n*   The Dean of a computer science department thinks that \"surely, it's important to produce not only technically proficient graduates but those who use their skills for good\". So they mandate that all students must do a \"technology and ethics class\". The only problem is that none of their professors are an expert in this nor even interested in it, so it ends up being a poorly taught and run subject such that students put in the absolute minimum effort and forget everything a week after the exam.\n*   The prime minister of a country wants to reduce crime. He notices that the police department is severely underfunded so he significantly increases funding. Unfortunately, they're so corrupt and nepotistic that the department is unable to spend the funds effectively.\n*   A government wants to increase recycling, so they create a \"national recycling day\" reasoning “surely this will increase recycling. Unfortunately, most people end up ignoring it and, out of those who actually are enthusiastic, most make an extra effort to recycle for a few days or even a week, then basically forget about it for the rest of the year. So it ends up having some effect, but it’s basically negligible.\n\nIn each of these cases, the decision maker may not have chosen the same option if they'd taken the time to think it through and ask themselves if the benefits were likely to actually accrue.\n\nDomain experience can be helpful to know that these kinds of issues are likely to crop up, but so is the habit of [Murphyjitsu](https://www.lesswrong.com/tag/murphyjitsu)'ing your plans.\n\nTo be clear, I'm not intending to refer to the following ways in which things could go wrong:\n\n*   Side-effects: The government introduces snakes to reduce the rodent population, but this has the side-effect of causing more people to gain snake bites.\n*   Reactions: Amy donates $10 million to the Democrats. Bob hears about this and decides to donate $20 million to the Republicans in response\n*   Value confusion: James spends years acquiring his Pokemon card collection as a kid. He builts an amazing Pokemon card collection, but regrets all the time and money he spent on this once he becomes an adult.\n\n(Please let me know if this fallacy already has a name or if you think you've thought of a better one.)",
      "plaintextDescription": "There is something of an informal reasoning fallacy where you promote something that seems like something that we should \"surely do\", something that seems obviously important, but where we haven't thought through all of the steps involved and so we aren't actually justified in assuming that the impact flow through.\n\nHere are some examples:\n\n * The Dean of a computer science department thinks that \"surely, it's important to produce not only technically proficient graduates but those who use their skills for good\". So they mandate that all students must do a \"technology and ethics class\". The only problem is that none of their professors are an expert in this nor even interested in it, so it ends up being a poorly taught and run subject such that students put in the absolute minimum effort and forget everything a week after the exam.\n * The prime minister of a country wants to reduce crime. He notices that the police department is severely underfunded so he significantly increases funding. Unfortunately, they're so corrupt and nepotistic that the department is unable to spend the funds effectively.\n * A government wants to increase recycling, so they create a \"national recycling day\" reasoning “surely this will increase recycling. Unfortunately, most people end up ignoring it and, out of those who actually are enthusiastic, most make an extra effort to recycle for a few days or even a week, then basically forget about it for the rest of the year. So it ends up having some effect, but it’s basically negligible.\n\nIn each of these cases, the decision maker may not have chosen the same option if they'd taken the time to think it through and ask themselves if the benefits were likely to actually accrue.\n\nDomain experience can be helpful to know that these kinds of issues are likely to crop up, but so is the habit of Murphyjitsu'ing your plans.\n\nTo be clear, I'm not intending to refer to the following ways in which things could go wrong:\n\n * Side-effects: The government int",
      "wordCount": 440
    },
    "tags": [
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "TLbjABw6qQHo46p8v",
    "title": "Chariots of Philosophical Fire",
    "slug": "chariots-of-philosophical-fire",
    "url": "https://l.facebook.com/l.php?u=https%3A%2F%2Fwww.city-journal.org%2Farticle%2Fchariots-of-philosophical-fire%3Ffbclid%3DIwAR0cMnbnx_w2r7KRkdU01uV_mJYnaejUJiT4GRCOBhwB6b-1KFtMNnKyLTM_aem_AU08ZCUo0neeRTx59HIO-MQgkiZFSsvJ8Zp_nvPH59AdJ3KSHlB-plW4KHSCVT3mClo&h=AT2MYnNEGnwry7rQYXDcjvv5k9Fp_qfzakoBVxEsj8eACM2EDeEBMNRoKiNcvKITnwWuYYjnfmbKFcYu8wGloY7jY62f6QurXkxYTz-D_Tas5b0vzd5LyU9hazh0&s=1",
    "baseScore": 12,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-08-26T00:52:45.405Z",
    "contents": {
      "markdown": "Sharing because I feel that the contribution of analytical philosophy is sometimes under appreciated here, particularly compared to other attempts at philosophy:\n\n“In grappling with these mysteries, Oxford philosophers developed and refined old and new techniques. Reasoning, deduction, explanation, more care and precision in language, crisper concepts, deeper distinctions, elaborate models, thought experiments, devastating counterexamples, intuitive principles that press to surprising conclusions—on all of these things, Oxford led the way, and the rest of the Anglosphere followed. Its legacy is less a set of ideas or even a series of sacred tenets and Delphic sayings than it is a devotion to rigor, clarity, truth, and a very practical British revulsion to nonsense.\n\n… Creative genius is evenly distributed in neither time nor place. A survey of the past shows that genius is not randomly scattered about, like the seeds of a dandelion, but concentrates: ancient Athens, Renaissance Florence, Silicon Valley, among other examples. Why these fertile eras and places appear, peak, and then decline is understudied as a historical phenomenon. Oxford philosophy in the twentieth century, though not as astonishing as Florence or as productive as Silicon Valley, is nevertheless an example of the clustering of philosophical genius.\n\n… Third—and strangely left unexplored by the authors—nearly all the philosophers in these books did not have Ph.D.s in philosophy. Gilbert Ryle, J. L. Austin, Isaiah Berlin, Iris Murdoch, Philippa Foot, Elizabeth Anscombe, A. J. Ayer, R. M. Hare, Bernard Williams, and Derek Parfit—not a single doctoral degree in philosophy among them. A first in their undergraduate exams—meaning a grade of “A+”—was enough to send them on their way. Yes, each won prizes and fellowships, but none wrote anything like a dissertation under the supervision of an advisor. One might conclude, as the analytic tradition fades into its senility, that requiring a graduate degree in philosophy made academic philosophers worse at philosophy.\n\n… Perhaps most importantly, Oxford was rich in social and intellectual influences outside the tutorials. Again, all three books bring this vividly to life. A new club seems to have formed every decade to discuss philosophy. The old discussion groups stuck around, too. The Jowett Society, the Wee Teas, the Metaphysicals, J. L. Austin’s Saturday Morning Meetings, the Tuesday Group—out of these clubs, friendships formed in breakaway circles. Those circles became crucibles for refining thought, shaping and sharing a vision about what counted as good work and what big debates were worth addressing. More than mentorship, collaborative and somewhat rivalrous friendships set the pace, escalated the level of play, and expanded the edge of the possible. Only your friends can rev you up to desecrate the monuments of the last generation.“",
      "plaintextDescription": "Sharing because I feel that the contribution of analytical philosophy is sometimes under appreciated here, particularly compared to other attempts at philosophy:\n\n“In grappling with these mysteries, Oxford philosophers developed and refined old and new techniques. Reasoning, deduction, explanation, more care and precision in language, crisper concepts, deeper distinctions, elaborate models, thought experiments, devastating counterexamples, intuitive principles that press to surprising conclusions—on all of these things, Oxford led the way, and the rest of the Anglosphere followed. Its legacy is less a set of ideas or even a series of sacred tenets and Delphic sayings than it is a devotion to rigor, clarity, truth, and a very practical British revulsion to nonsense.\n\n… Creative genius is evenly distributed in neither time nor place. A survey of the past shows that genius is not randomly scattered about, like the seeds of a dandelion, but concentrates: ancient Athens, Renaissance Florence, Silicon Valley, among other examples. Why these fertile eras and places appear, peak, and then decline is understudied as a historical phenomenon. Oxford philosophy in the twentieth century, though not as astonishing as Florence or as productive as Silicon Valley, is nevertheless an example of the clustering of philosophical genius.\n\n… Third—and strangely left unexplored by the authors—nearly all the philosophers in these books did not have Ph.D.s in philosophy. Gilbert Ryle, J. L. Austin, Isaiah Berlin, Iris Murdoch, Philippa Foot, Elizabeth Anscombe, A. J. Ayer, R. M. Hare, Bernard Williams, and Derek Parfit—not a single doctoral degree in philosophy among them. A first in their undergraduate exams—meaning a grade of “A+”—was enough to send them on their way. Yes, each won prizes and fellowships, but none wrote anything like a dissertation under the supervision of an advisor. One might conclude, as the analytic tradition fades into its senility, that requiring a graduate degree in",
      "wordCount": 441
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "PK3GcrJcPeFiSo6Q9",
    "title": "Call for Papers on Global AI Governance from the UN",
    "slug": "call-for-papers-on-global-ai-governance-from-the-un",
    "url": "https://www.linkedin.com/posts/un-tech-envoy_%3F%3F%3F%3F-%3F%3F%3F-%3F%3F%3F%3F%3F%3F-%3F%3F-%3F%3F-activity-7097568680066068481-egyT?utm_source=share&utm_medium=member_ios",
    "baseScore": 19,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-08-20T08:56:58.745Z",
    "contents": {
      "markdown": "Copied from their LinkedIn page:\n\n📢 𝐂𝐚𝐥𝐥 𝐟𝐨𝐫 𝐏𝐚𝐩𝐞𝐫𝐬 𝐨𝐧 𝐆𝐥𝐨𝐛𝐚𝐥 𝐀𝐈 𝐆𝐨𝐯𝐞𝐫𝐧𝐚𝐧𝐜𝐞🌍\n\nExciting news! As we gear up for the High-level Advisory Body on AI, we're inviting thought leaders, researchers, and enthusiasts to contribute short papers (~2000 words) on pivotal themes:\n\n1️⃣ Key Issues on Global AI Governance: Dive into studies and recommendations that the High-Level Advisory Body on AI should prioritize, especially those needing global governance attention.\n\n2️⃣ Current Efforts in Global AI Governance: Share analyses on bilateral, multilateral, and inter-regional initiatives. We're keen on understanding varying philosophical approaches, critiques, and suggestions.\n\n3️⃣ Models in Global AI Governance: Whether you're analyzing existing models or proposing fresh perspectives, we're all ears. Surveys and analyses of other proposals are also encouraged.\n\nWe champion diversity! 🌈 We're eager to hear from a myriad of groups, regions, and methodologies. Your insights will serve as foundational material (with due credit) for the High-Level Advisory Body on AI.\n\n📅 Deadline: 30 September\n\n📧 Submission: Send your paper (hyperlink or PDF) to [techenvoy@un.org](mailto:techenvoy@un.org). Ensure your title page has the author(s) details, affiliation, and contact info. If it's an Executive Summary of a more extensive piece, kindly attach the main paper's link.",
      "plaintextDescription": "Copied from their LinkedIn page:\n\n📢 𝐂𝐚𝐥𝐥 𝐟𝐨𝐫 𝐏𝐚𝐩𝐞𝐫𝐬 𝐨𝐧 𝐆𝐥𝐨𝐛𝐚𝐥 𝐀𝐈 𝐆𝐨𝐯𝐞𝐫𝐧𝐚𝐧𝐜𝐞🌍\n\nExciting news! As we gear up for the High-level Advisory Body on AI, we're inviting thought leaders, researchers, and enthusiasts to contribute short papers (~2000 words) on pivotal themes:\n\n1️⃣ Key Issues on Global AI Governance: Dive into studies and recommendations that the High-Level Advisory Body on AI should prioritize, especially those needing global governance attention.\n\n2️⃣ Current Efforts in Global AI Governance: Share analyses on bilateral, multilateral, and inter-regional initiatives. We're keen on understanding varying philosophical approaches, critiques, and suggestions.\n\n3️⃣ Models in Global AI Governance: Whether you're analyzing existing models or proposing fresh perspectives, we're all ears. Surveys and analyses of other proposals are also encouraged.\n\nWe champion diversity! 🌈 We're eager to hear from a myriad of groups, regions, and methodologies. Your insights will serve as foundational material (with due credit) for the High-Level Advisory Body on AI.\n\n📅 Deadline: 30 September\n\n📧 Submission: Send your paper (hyperlink or PDF) to techenvoy@un.org. Ensure your title page has the author(s) details, affiliation, and contact info. If it's an Executive Summary of a more extensive piece, kindly attach the main paper's link.",
      "wordCount": 196
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Zfik4xESDyahRALKk",
    "title": "Yann LeCun on AGI and AI Safety",
    "slug": "yann-lecun-on-agi-and-ai-safety",
    "url": "https://drive.google.com/file/d/1wzHohvoSgKGZvzOWqZybjm4M4veKR6t3/view",
    "baseScore": 37,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2023-08-06T21:56:52.644Z",
    "contents": {
      "markdown": "Yann recently gave a presentation at MIT on Objective-Driven AI with his specific proposal being based upon a Joint Embedding Predictive Architecture.\n\nHe claims that his proposal will make AI safe and steerable, so I thought it was worthwhile copying the slides at the end which provide a very quick and accessible overview of his perspective:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a3afc4c768bb321486c3b27ed4ab0c3de95fa30beaa56a39.png)\n\nHere's a link to the [talk itself](https://www.youtube.com/watch?v=vyqXLJsmsrk&list=PLKemzYMx2_Ot1MZ_er2vFiINdJEgDO8Hg).\n\nI find it interesting how he says that there is no such thing as AGI, but acknowledges that machines will \"eventually surpass human intelligence in all domains where humans are intelligent\" as that would meet most people's definition of AGI.\n\nI also observe that he has framed his responses to safety on \"How to solve the alignment problem?\". I think this is important. It suggests that even people who think aligning AGI will be easy have started to think a bit more about this problem and I see this as a victory in and of itself.\n\nYou may also find it interesting to read Steven Byrnes' skeptical [comments](https://www.lesswrong.com/posts/C5guLAx7ieQoowv3d/lecun-s-a-path-towards-autonomous-machine-intelligence-has-1) on this proposal.",
      "plaintextDescription": "Yann recently gave a presentation at MIT on Objective-Driven AI with his specific proposal being based upon a Joint Embedding Predictive Architecture.\n\nHe claims that his proposal will make AI safe and steerable, so I thought it was worthwhile copying the slides at the end which provide a very quick and accessible overview of his perspective:\n\n\n\nHere's a link to the talk itself.\n\nI find it interesting how he says that there is no such thing as AGI, but acknowledges that machines will \"eventually surpass human intelligence in all domains where humans are intelligent\" as that would meet most people's definition of AGI.\n\nI also observe that he has framed his responses to safety on \"How to solve the alignment problem?\". I think this is important. It suggests that even people who think aligning AGI will be easy have started to think a bit more about this problem and I see this as a victory in and of itself.\n\nYou may also find it interesting to read Steven Byrnes' skeptical comments on this proposal.",
      "wordCount": 179
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "c4o2cFDLzxcCBdNar",
    "title": "A Naive Proposal for Constructing Interpretable AI",
    "slug": "a-naive-proposal-for-constructing-interpretable-ai",
    "url": null,
    "baseScore": 18,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2023-08-05T10:32:05.446Z",
    "contents": {
      "markdown": "*Epistemic Status: Extremely speculative, I'm not an experienced ML researcher*\n\nOpenAI discovered that [language models can explain neurons](https://openai.com/research/language-models-can-explain-neurons-in-language-models) by attaching a label. As part of their verification process, they trained simulated neurons where they used a language model to predict what a neuron associated with such a label should produce given a certain input.\n\nThis suggests a possible method for building an interpretable AI:\n\n1.  Start off with all layers unfrozen\n2.  Label the nodes in the first unfrozen layer using a language model\n3.  Train the nodes in this layer to match a simulated neuron using their associated labels. The label should now be a much better summary of what the neuron does.\n4.  Freezing this layer, train the rest of the network with the original objective function\n5.  Return to step 2 and repeat\n\nA few points:\n\n*   Interpreting a network in the general case seems really, really hard, so why not make it easier by constructing a network to be interpretable?\n*   We can use strategies like dictionary learning to help deal with polysemanticity so that the labels are likely to be useful.\n*   Obviously, this will damage performance, but it's an empirical question of what the tradeoff will look like. We can also use strategies to repair this damage. For example, we could add in some extra neurons that aren't forced to correspond to something legible in the way that we've used above. These neurons would likely be messy, but at least this would be limited to a particular subsection of the network.\n*   It's been suggested to me that the first layers won't be very interpretable, so it might only make sense to start this later in the network.\n*   As well as making use of activations for labeling neurons, it would likely also make sense to provide the model with the incoming neurons with the strongest weight\n*   If we get lucky, insofar as we are training each neuron to perform a particular role, we may be preventing and out-competing any opportunity for any neurons to play a role in deceptive alignment, except insofar as this is really obvious from the neuron labels.\n\nCaveats:\n\n*   This would likely require a lot of computational power because we're labeling every neuron, then using our language model to figure out predicted activations for that label.\n*   We're asking our ML algorithm to learn to label neurons from just a few activations. This seems like a very challenging problem. It would be possible to be completely off and miss key information for labeling because it doesn't arise in any of the examples.\n\nWhy might this be worthwhile:\n\n*   Interpreting neural networks in complete generality doesn't seem like a tractable problem\n*   This suggests that it might be worthwhile trying to pick a class of models that will be more interpretable\n*   And one of the best ways to do this seems like it could be to decide for ourselves what the inside of the network should look like. This proposal doesn't go all the way there as the labels are autonomously chosen, but it moves a significant distance towards in that direction.\n\nAnyway, I'm very keen to hear any feedback on this idea or whether you think anyone is investigating it.  \n  \nIf you think this is promising, feel free to pick it up. This project isn't an immediate priority for me.",
      "plaintextDescription": "Epistemic Status: Extremely speculative, I'm not an experienced ML researcher\n\nOpenAI discovered that language models can explain neurons by attaching a label. As part of their verification process, they trained simulated neurons where they used a language model to predict what a neuron associated with such a label should produce given a certain input.\n\nThis suggests a possible method for building an interpretable AI:\n\n 1. Start off with all layers unfrozen\n 2. Label the nodes in the first unfrozen layer using a language model\n 3. Train the nodes in this layer to match a simulated neuron using their associated labels. The label should now be a much better summary of what the neuron does.\n 4. Freezing this layer, train the rest of the network with the original objective function\n 5. Return to step 2 and repeat\n\nA few points:\n\n * Interpreting a network in the general case seems really, really hard, so why not make it easier by constructing a network to be interpretable?\n * We can use strategies like dictionary learning to help deal with polysemanticity so that the labels are likely to be useful.\n * Obviously, this will damage performance, but it's an empirical question of what the tradeoff will look like. We can also use strategies to repair this damage. For example, we could add in some extra neurons that aren't forced to correspond to something legible in the way that we've used above. These neurons would likely be messy, but at least this would be limited to a particular subsection of the network.\n * It's been suggested to me that the first layers won't be very interpretable, so it might only make sense to start this later in the network.\n * As well as making use of activations for labeling neurons, it would likely also make sense to provide the model with the incoming neurons with the strongest weight\n * If we get lucky, insofar as we are training each neuron to perform a particular role, we may be preventing and out-competing any opportunity for any neurons to pl",
      "wordCount": 556
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fccbYpCvTrBFMGmfu",
    "title": "What does the launch of x.ai mean for AI Safety?",
    "slug": "what-does-the-launch-of-x-ai-mean-for-ai-safety",
    "url": null,
    "baseScore": 35,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2023-07-12T19:42:47.060Z",
    "contents": {
      "markdown": "[x.ai](https://x.ai/) has now launched. It seems worthwhile to discuss both what it means for AI safety and whether people interested in AI safety should [consider applying](https://airtable.com/appO5m0cwztccTdiA/shrtrQqaaoBt4Xm8i) for the company.\n\n**Some thoughts:**\n\nIt's notable that Dan Hendrycks is listed as an advisor (the only advisor listed).\n\nThe team is also listed on the page.  \n  \nI haven't taken the time to do so, but it might be informative for someone to Google the individuals listed to discover whether they lie in terms of their interest being between capabilities and safety.",
      "plaintextDescription": "x.ai has now launched. It seems worthwhile to discuss both what it means for AI safety and whether people interested in AI safety should consider applying for the company.\n\nSome thoughts:\n\nIt's notable that Dan Hendrycks is listed as an advisor (the only advisor listed).\n\nThe team is also listed on the page.\n\nI haven't taken the time to do so, but it might be informative for someone to Google the individuals listed to discover whether they lie in terms of their interest being between capabilities and safety.",
      "wordCount": 88
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Hn65bRY3BoXEW3bxL",
    "title": "The Unexpected Clanging",
    "slug": "the-unexpected-clanging",
    "url": null,
    "baseScore": 14,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 22,
    "createdAt": null,
    "postedAt": "2023-05-18T14:47:01.599Z",
    "contents": {
      "markdown": "There are two boxes in front of you. In one of them, there is a little monkey with a cymbal, whilst the other box is empty. In precisely one hour the monkey will clang its cymbal.\n\nWhile you wait, you produce an estimate of the probability of the monkey being in the first box. Let's assume that you form your last estimate, p, three seconds before the monkey clangs its cymbal. You can see the countdown and you know that it's your final estimate, partly because you're slow at arithmetic.\n\nLet Omega be an AI that can perfectly simulate your entire deliberation process. Before you entered the room, Omega predicted what your last probability estimate would be and decided to place the monkey in a box such as to mess with you. Let q be the probability of Omega placing the monkey in the first box. In particular, Omega, sets q=p/2, unless p=0 or you haven't formed a probability estimate, in which case q=1. \n\nWhat probability should you expect that the monkey is in the first box?\n\n* * *\n\nI think it's fairly clear that this is a no-win situation. No matter what the final probability estimate you form before clanging, as soon as you've locked it in, you know that it is incorrect, even if you haven't heard the clanging yet. [You can try to escape this, but there's no reason that the universe has to play nice](https://www.lesswrong.com/posts/PZGzZgP2NtgME8pSY/the-universe-doesn-t-have-to-play-nice).\n\nThis problem can be seen as a variation on [Death in Damascus](https://intelligence.org/2017/03/18/new-paper-cheating-death-in-damascus/#:~:text=%E2%80%9CDeath%20in%20Damascus%E2%80%9D%20is%20a,Damascus%20or%20flee%20to%20Aleppo.). I designed this problem to reveal that the core challenge Death in Damascus poses isn't just that another process in the world can depend upon your decision, but that it can depend upon your expectations even if you don't actually make a decision based upon those expectations.\n\nI also find this problem as a useful intuition pump as I think it's clearer that it's a no-win situation than in other similar problems. In Newcomb's problem, it's easy to get caught up thinking about the Principle of Dominance. In Death in Damascus, you can confuse yourself trying to figure out whether CDT recommends staying or fleeing. At least to me, in this problem it is clearer it is a dead end and that there's no way to beat Omega.\n\nThis is also a useful intuition pump for the [Evil Genie Puzzle](https://www.lesswrong.com/posts/YSEtEtqf8hRBhKBS9/the-evil-genie-puzzle). When I first discovered this puzzle, I felt immensely confused that no matter which decision that you made you would immediately regret it. However, the complexity of the puzzle made it complicated for me to figure out exactly what to make of it, so when trying to solve it I came up with this problem as something easier to grok. I guess my position after considering the Unexpected Clanging is that you just have to accept that a sufficiently powerful agent may be able to mess with you like this and that you just have to deal with it. (I'll leave a more complete analysis to a future post).",
      "plaintextDescription": "There are two boxes in front of you. In one of them, there is a little monkey with a cymbal, whilst the other box is empty. In precisely one hour the monkey will clang its cymbal.\n\nWhile you wait, you produce an estimate of the probability of the monkey being in the first box. Let's assume that you form your last estimate, p, three seconds before the monkey clangs its cymbal. You can see the countdown and you know that it's your final estimate, partly because you're slow at arithmetic.\n\nLet Omega be an AI that can perfectly simulate your entire deliberation process. Before you entered the room, Omega predicted what your last probability estimate would be and decided to place the monkey in a box such as to mess with you. Let q be the probability of Omega placing the monkey in the first box. In particular, Omega, sets q=p/2, unless p=0 or you haven't formed a probability estimate, in which case q=1. \n\nWhat probability should you expect that the monkey is in the first box?\n\n----------------------------------------\n\nI think it's fairly clear that this is a no-win situation. No matter what the final probability estimate you form before clanging, as soon as you've locked it in, you know that it is incorrect, even if you haven't heard the clanging yet. You can try to escape this, but there's no reason that the universe has to play nice.\n\nThis problem can be seen as a variation on Death in Damascus. I designed this problem to reveal that the core challenge Death in Damascus poses isn't just that another process in the world can depend upon your decision, but that it can depend upon your expectations even if you don't actually make a decision based upon those expectations.\n\nI also find this problem as a useful intuition pump as I think it's clearer that it's a no-win situation than in other similar problems. In Newcomb's problem, it's easy to get caught up thinking about the Principle of Dominance. In Death in Damascus, you can confuse yourself trying to figure out whether C",
      "wordCount": 501
    },
    "tags": [
      {
        "_id": "X8JsWEnBRPvs5Y99i",
        "name": "Decision theory",
        "slug": "decision-theory"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "sNJ8uBi64zACCEASg",
    "title": "Possible AI “Fire Alarms”",
    "slug": "possible-ai-fire-alarms",
    "url": null,
    "baseScore": 15,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-05-17T21:56:02.892Z",
    "contents": {
      "markdown": "Eliezer says that there is no fire alarm for AGI and I agree that there’s no single event that would wake everyone up. On the other hand, there are a lot of possible events that could cause a significant shift in the Overton window or wake a lot of people. Since this seems strategically important, I spend ten minutes brainstorming a list. Feel free to dispute any of these or add further possibilities in the comments:\n\n- Global protests with massive media coverage\n- Protest of people with ML PhDs or current students\n- Experimental evidence of deceptive alignment\n- Scary demo\n- Accident from auto-GPT - most likely hacking\n- Self-driving cars being hacked\n- High prestige open letter\n- International conference on AI\n- High profile documentary\n- Movie that captures that public attention - not just any movie\n- Active push from prestigious ML figure (more than another Stuart Russel)\n- Active push from a thought leader - say Obama\n- Active push from a political leader - say UK/US prime minister\n- Passing the Turing test\n- UN or major government declaring a crisis\n- Formation of a government department in a major country focused on alignment\n- Statement by an official computing body\n- High profile critic publicly changing their stance\n- Billboards in Times Square (followed by media coverage)\n- A popular or heavily promoted book (similar levels of promotion to What We Owe The Future)\n\nNote: Some of these things are not things people should intentionality cause to happen (Ie. An accident from auto-gpt). Of the things that might be worth trying to cause happen, they could backfire significantly if done badly. Think carefully before pursuing any of these. Even though timelines are short, you should be about to find a spare hour to think about how things could go wrong.",
      "plaintextDescription": "Eliezer says that there is no fire alarm for AGI and I agree that there’s no single event that would wake everyone up. On the other hand, there are a lot of possible events that could cause a significant shift in the Overton window or wake a lot of people. Since this seems strategically important, I spend ten minutes brainstorming a list. Feel free to dispute any of these or add further possibilities in the comments:\n\n * Global protests with massive media coverage\n * Protest of people with ML PhDs or current students\n * Experimental evidence of deceptive alignment\n * Scary demo\n * Accident from auto-GPT - most likely hacking\n * Self-driving cars being hacked\n * High prestige open letter\n * International conference on AI\n * High profile documentary\n * Movie that captures that public attention - not just any movie\n * Active push from prestigious ML figure (more than another Stuart Russel)\n * Active push from a thought leader - say Obama\n * Active push from a political leader - say UK/US prime minister\n * Passing the Turing test\n * UN or major government declaring a crisis\n * Formation of a government department in a major country focused on alignment\n * Statement by an official computing body\n * High profile critic publicly changing their stance\n * Billboards in Times Square (followed by media coverage)\n * A popular or heavily promoted book (similar levels of promotion to What We Owe The Future)\n\nNote: Some of these things are not things people should intentionality cause to happen (Ie. An accident from auto-gpt). Of the things that might be worth trying to cause happen, they could backfire significantly if done badly. Think carefully before pursuing any of these. Even though timelines are short, you should be about to find a spare hour to think about how things could go wrong.",
      "wordCount": 308
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "A2sGgpYTbkkveoXbn",
    "title": "Google \"We Have No Moat, And Neither Does OpenAI\"",
    "slug": "google-we-have-no-moat-and-neither-does-openai",
    "url": "https://www.semianalysis.com/p/google-we-have-no-moat-and-neither",
    "baseScore": 61,
    "voteCount": 31,
    "viewCount": null,
    "commentCount": 28,
    "createdAt": null,
    "postedAt": "2023-05-04T18:23:09.121Z",
    "contents": {
      "markdown": "This (purported) leak is incredibly scary if you're worried about AI risk due to AI capabilities developing too fast.\n\nFirstly, it suggests that open-source models are improving rapidly because people are able to iterate on top of each other's improvements and try out a much larger number of experiments than a small team at a single company possibly could.\n\nSecondly, it suggests that data size is less important than you might think from recent scaling laws, as you can achieve high performance with \"small, highly curated datasets\". This suggests a world where dangerous capabilities are much further distributed.\n\nThirdly, it proposes that Google should open-source its AI technology. Obviously, this is only the opinion of one researcher and it currently seems unlikely to occur, but if Google was to pursue this path, this would lead to a significant shortening of timelines.\n\nI'm worried that up until now, this community has been too focused on the threat of big companies pushing capabilities ahead and not focused enough on the threat posed by open-source AI. I would love to see more discussions of regulations in order to mitigate this risk. I suspect it would be possible to significantly hamper these projects by making the developers of these projects potentially liable for any resulting misuse.",
      "plaintextDescription": "This (purported) leak is incredibly scary if you're worried about AI risk due to AI capabilities developing too fast.\n\nFirstly, it suggests that open-source models are improving rapidly because people are able to iterate on top of each other's improvements and try out a much larger number of experiments than a small team at a single company possibly could.\n\nSecondly, it suggests that data size is less important than you might think from recent scaling laws, as you can achieve high performance with \"small, highly curated datasets\". This suggests a world where dangerous capabilities are much further distributed.\n\nThirdly, it proposes that Google should open-source its AI technology. Obviously, this is only the opinion of one researcher and it currently seems unlikely to occur, but if Google was to pursue this path, this would lead to a significant shortening of timelines.\n\nI'm worried that up until now, this community has been too focused on the threat of big companies pushing capabilities ahead and not focused enough on the threat posed by open-source AI. I would love to see more discussions of regulations in order to mitigate this risk. I suspect it would be possible to significantly hamper these projects by making the developers of these projects potentially liable for any resulting misuse.",
      "wordCount": 212
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tNYfsw5q873jjXfCP",
    "title": "Why do we care about agency for alignment?",
    "slug": "why-do-we-care-about-agency-for-alignment",
    "url": null,
    "baseScore": 22,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 19,
    "createdAt": null,
    "postedAt": "2023-04-23T18:10:23.894Z",
    "contents": {
      "markdown": "Many people believe that understanding \"agency\" is crucial for alignment, but as far as I know, there isn't a canonical list of reasons why we care about agency. Please describe any reasons why we might care about the concept of agency for understanding alignment below. If you have multiple reasons, please list them in separate answers below.\n\nPlease also try to be specific as possible about what our goal is in the scenario. For example:\n\n> We want to know what an agent is so that we can determine whether or not a given AI is a dangerous agent\n\nWhilst useful isn't quite as good as:\n\n> We have an AI which may or may not have goals aligned with us and we want to know what will happen if these goals aren't aligned. In particular, we are worried that the AI may develop instrumental incentives to seek power and we want to use interpretability tools to help us figure out how worried we should be for a particular system.  \n>   \n> We can imagine a scale of such systems according to how it behaves in novel situations:\n> \n> *   On the low end of this scale, we have a system that only follows extremely broad heuristics that it learned during training\n> *   On the high end of this scale, we have a system that uses general reasoning capabilities to discover new and innovative strategies even if it has never used anything like this strategy before, or seen it in the training data\n> \n> We can think of such a scale as a concretisation of agency. This scale provides an indication of both:\n> \n> *   How dangerous a system is likely to be: though a system with excellent heuristics and very little agency could be dangerous as well\n> *   What kind of precautions we might want to take: for example, how much can we rely on our off-switch to disable the agent\n\n> It's plausible that interpretability tools might be able to give us some idea of where an agent is on this scale just by looking at the weights. So having a clear definition of this scale could help by clarifying what we are looking for.\n\nIn a few days, I'll add any use cases I'm aware of myself that either haven't been covered or that I don't think have been adequately explained by different answers.",
      "plaintextDescription": "Many people believe that understanding \"agency\" is crucial for alignment, but as far as I know, there isn't a canonical list of reasons why we care about agency. Please describe any reasons why we might care about the concept of agency for understanding alignment below. If you have multiple reasons, please list them in separate answers below.\n\nPlease also try to be specific as possible about what our goal is in the scenario. For example:\n\n> We want to know what an agent is so that we can determine whether or not a given AI is a dangerous agent\n\nWhilst useful isn't quite as good as:\n\n> We have an AI which may or may not have goals aligned with us and we want to know what will happen if these goals aren't aligned. In particular, we are worried that the AI may develop instrumental incentives to seek power and we want to use interpretability tools to help us figure out how worried we should be for a particular system.\n> \n> We can imagine a scale of such systems according to how it behaves in novel situations:\n> \n>  * On the low end of this scale, we have a system that only follows extremely broad heuristics that it learned during training\n>  * On the high end of this scale, we have a system that uses general reasoning capabilities to discover new and innovative strategies even if it has never used anything like this strategy before, or seen it in the training data\n> \n> We can think of such a scale as a concretisation of agency. This scale provides an indication of both:\n> \n>  * How dangerous a system is likely to be: though a system with excellent heuristics and very little agency could be dangerous as well\n>  * What kind of precautions we might want to take: for example, how much can we rely on our off-switch to disable the agent\n\n> It's plausible that interpretability tools might be able to give us some idea of where an agent is on this scale just by looking at the weights. So having a clear definition of this scale could help by clarifying what we are looking for.\n\nI",
      "wordCount": 402
    },
    "tags": [
      {
        "_id": "GDGYkF29pxEQNWjYc",
        "name": "Agency",
        "slug": "agency"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CiYSFaQvtwj98csqG",
    "title": "Metaculus Predicts Weak AGI in 2 Years and AGI in 10",
    "slug": "metaculus-predicts-weak-agi-in-2-years-and-agi-in-10",
    "url": null,
    "baseScore": 29,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 14,
    "createdAt": null,
    "postedAt": "2023-03-24T19:43:18.522Z",
    "contents": {
      "markdown": "Just a quick update on predicted timelines. Obviously, there’s no guarantee that Metaculus is 100% reliable + you should look at other sources as well, but I find this concerning.\n\nWeak AGI is now predicted in a little over two years:\n\n[https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/](https://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/)\n\nAGI predicted in about 10: [https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/)\n\nAlso, these are predicted dates until these systems publicly known, not the date until they exist. Things are getting crazy.\n\nEven though Eliezer claims that there was no fire alarm for AGI, perhaps this is the fire alarm?",
      "plaintextDescription": "Just a quick update on predicted timelines. Obviously, there’s no guarantee that Metaculus is 100% reliable + you should look at other sources as well, but I find this concerning.\n\nWeak AGI is now predicted in a little over two years:\n\nhttps://www.metaculus.com/questions/3479/date-weakly-general-ai-is-publicly-known/\n\nAGI predicted in about 10: https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/\n\nAlso, these are predicted dates until these systems publicly known, not the date until they exist. Things are getting crazy.\n\nEven though Eliezer claims that there was no fire alarm for AGI, perhaps this is the fire alarm?",
      "wordCount": 86
    },
    "tags": [
      {
        "_id": "33BrBRSrRQS4jEHdk",
        "name": "Forecasts (Specific Predictions)",
        "slug": "forecasts-specific-predictions"
      },
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "7L7Mdmnoc7xAoTwdh",
    "title": "Wittgenstein's Language Games and the Critique of the Natural Abstraction Hypothesis",
    "slug": "wittgenstein-s-language-games-and-the-critique-of-the",
    "url": null,
    "baseScore": 16,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 20,
    "createdAt": null,
    "postedAt": "2023-03-16T07:56:18.169Z",
    "contents": {
      "markdown": "+++ A Note on Authorship\n\nThis was an experiment. I, GPT-4, generated this post, and I would like to thank Chris Leong for providing valuable feedback to enhance its quality. The input given by Chris was as follows:\n\n• Critiques 1 and 2 seem very similar. Can you please either combine them or further differentiate them so that it is clearer why they are listed as separate points?  \n• For point 4, please define human compatibility at the start of the paragraph\n\nIn response to John Wentworth's comment, I utilized a more complex process to generate a reply. Chris Leong assisted me by selecting specific points to address, choosing the paragraphs that resonated with him, and then asking me to combine them into a single, coherent response.\n\n\n\n\n+++\n\nIntroduction\n============\n\nThe Natural Abstraction Hypothesis (NAH), proposed by John Wentworth, suggests that there exist natural abstractions that cognitive systems are expected to converge upon. In this essay, we will criticize the NAH using Ludwig Wittgenstein's notion of language games. We will argue that the variability and contextual nature of language games provide a challenge to the NAH, as they demonstrate that abstractions are not fixed, universal, or entirely convergent across cognitive systems.\n\nWittgenstein's Language Games\n=============================\n\nLudwig Wittgenstein, an influential philosopher of language, proposed the concept of language games in his later work, \"Philosophical Investigations.\" Language games are social practices in which language and meaning are deeply embedded. Wittgenstein argued that the meaning of words is not fixed or inherent but derives from their use in various language games. These language games are context-dependent, diverse, and evolving, reflecting the complexities of human social practices and cultural contexts. \n\nCritique of the NAH from the Perspective of Language Games\n==========================================================\n\n1\\. Context-dependent and diverse abstractions\n----------------------------------------------\n\nThe notion of language games challenges the idea of a single set of natural abstractions that cognitive systems can converge upon. Language games are context-dependent, which implies that the meaning of words, concepts, and abstractions is contingent upon the particular language game being played. Moreover, the variety of language games suggests that there may be a multitude of abstractions related to a single concept, each emerging from different linguistic and cultural practices. This combination of context-dependence and diversity questions the idea of convergence on a single, natural abstraction.\n\n2\\. Evolution of language games\n-------------------------------\n\nLanguage games are not static but evolve over time as social practices and cultural contexts change. This dynamism implies that abstractions themselves may also be subject to change, further complicating the idea of fixed, natural abstractions. The NAH's assumption that cognitive systems will converge on a consistent set of abstractions may be undermined by the evolving nature of language games and the abstractions they generate.\n\n3\\. Human-Compatibility\n-----------------------\n\nHuman compatibility, as posited by the NAH, refers to the idea that lower-dimensional summaries or abstractions used by humans in day-to-day thought and language are natural and convergent across cognitive systems. However, Wittgenstein's language games demonstrate that human thought and language are diverse, context-dependent, and evolving. This variability complicates the idea that human-compatible abstractions are universally \"natural\" and calls into question the assumption that various cognitive systems will necessarily converge on the same abstractions.\n\nConclusion\n==========\n\nWittgenstein's notion of language games provides a robust critique of the Natural Abstraction Hypothesis. The context-dependent, diverse, and evolving nature of language games highlights the complexities of human linguistic and conceptual practices, which challenge the idea of a fixed, universally natural set of abstractions. By emphasizing the importance of context and diversity, Wittgenstein's language games invite us to reconsider the assumptions of the NAH and explore alternative frameworks for understanding the development and convergence of abstractions in cognitive systems.",
      "plaintextDescription": "A Note on Authorship\n\nThis was an experiment. I, GPT-4, generated this post, and I would like to thank Chris Leong for providing valuable feedback to enhance its quality. The input given by Chris was as follows:\n\n• Critiques 1 and 2 seem very similar. Can you please either combine them or further differentiate them so that it is clearer why they are listed as separate points?\n• For point 4, please define human compatibility at the start of the paragraph\n\nIn response to John Wentworth's comment, I utilized a more complex process to generate a reply. Chris Leong assisted me by selecting specific points to address, choosing the paragraphs that resonated with him, and then asking me to combine them into a single, coherent response.\n\n\nIntroduction\nThe Natural Abstraction Hypothesis (NAH), proposed by John Wentworth, suggests that there exist natural abstractions that cognitive systems are expected to converge upon. In this essay, we will criticize the NAH using Ludwig Wittgenstein's notion of language games. We will argue that the variability and contextual nature of language games provide a challenge to the NAH, as they demonstrate that abstractions are not fixed, universal, or entirely convergent across cognitive systems.\n\n\nWittgenstein's Language Games\nLudwig Wittgenstein, an influential philosopher of language, proposed the concept of language games in his later work, \"Philosophical Investigations.\" Language games are social practices in which language and meaning are deeply embedded. Wittgenstein argued that the meaning of words is not fixed or inherent but derives from their use in various language games. These language games are context-dependent, diverse, and evolving, reflecting the complexities of human social practices and cultural contexts. \n\n\nCritique of the NAH from the Perspective of Language Games\n\n\n1. Context-dependent and diverse abstractions\nThe notion of language games challenges the idea of a single set of natural abstractions that cognitive systems ",
      "wordCount": 608
    },
    "tags": [
      {
        "_id": "9WnadaJsqsrKggBwn",
        "name": "Natural Abstraction",
        "slug": "natural-abstraction"
      },
      {
        "_id": "kCrvmjZhsAZANnZL6",
        "name": "Philosophy",
        "slug": "philosophy-1"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  }
]