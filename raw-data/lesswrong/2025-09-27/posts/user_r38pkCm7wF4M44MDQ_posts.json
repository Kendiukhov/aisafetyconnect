[
  {
    "_id": "oxv3jSviEdpBFAz9w",
    "title": "The Illustrated Petrov Day Ceremony",
    "slug": "the-illustrated-petrov-day-ceremony",
    "url": null,
    "baseScore": 80,
    "voteCount": 24,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2025-09-26T21:01:04.287Z",
    "contents": {
      "markdown": "Since 2014, some people have celebrated Petrov Day with a small in-person ceremony, with readings by candlelight that tell the story of Petrov within the context of the long arc of history, created by Jim Babcock.\n\nI've found this pretty meaningful, an it somehow feels \"authentic\" to me, like a real holiday. Which, as the creator of [Secular Solstice 2020](https://www.lesswrong.com/posts/zdKCifZXQTKNazFyk/secular-solstice-2020) it is humbling to say, feels more true than Solstice did (for the first few years, Solstice felt a little obviously \"made up\", and now it has more storied history, it instead often feels a bit too much like \"a show\" as opposed to \"a holiday\", at least when you go to the large productions in the Bay or NYC).\n\nI don't know how much my experience generalizes, but having been familiar with Seders (classic Jewish, or [rationalist](https://www.lesswrong.com/posts/FxrqQbZKff9BoGhtc/the-rationalist-haggadot-collection)), it just felt to me like a real historical holiday built from a parallel dimension that tumbled into my world.\n\nSolstice was originally intended to be a small affair that people held in people's living room. I ended up holding a large version as an attempt to draw more attention to it. Then it turned out the large version ended up being where most people anthropically end up experiencing Solstice.\n\nJim designed the Petrov Day ceremony to be deliberately awkward to scale – it's designed to find around a dining room table, with everyone within reach of a set of candles. If you end up with more than 20 people, Jim recommends splitting up into multiple groups in multiple rooms. There was a hope for it to fill a niche that encouraged small groups, copying the format of a Seder.\n\nThis is nice, but, it does mean it's harder to actually get a lot of people to know about it. \n\n* * *\n\nFor the last several years, LessWrong has done some kind of interactive (some might say \"gimmicky\") Petrov Day event each year. Some people complain that it's unclear if they are more like \"playing a game\" or more like \"participating in a sacred ritual where people will judge you horribly if you defect\", which has been disorienting. \n\nWe've worked on various ways to signal more clearly what type of activity it is in recent years.\n\nNormally, we conceive-of-and-implement Petrov Day basically the week-of in a matter of days. This means whatever symbolically-world-destroy-ing activity we implement has a higher-than-usual propensity for bugs that we ship at 1 AM. Some Lightcone team members dislike that and think it's horribly embarrassing. I agree it is horribly embarrassing but also think it is a) hilarious and b) surprisingly on-theme and giving the event a weird sort of authentic uncertain about \"did the site go down On Purpose For Reasons, or was it technical mistake in our Missile Detection Code?\".\n\nLast year's Petrov Day we did a particularly elaborate LARP/Social-Deception Game (see [announcement](https://www.lesswrong.com/posts/6LJ6xcHEjKF9zWKzs/completed-the-2024-petrov-day-scenario) and [retrospective](https://www.lesswrong.com/posts/NsxFcB2EAxcY6cdGh/2024-petrov-day-retrospective)). It was cool in some ways. But it made it feel kinda... hard to top? Anything game-like we did felt like it'd either need to be *even more elaborate*, or feel a bit weaksauce in comparison.\n\nSo, this year, despite successfully beginning to plan Petrov Day weeks in advance, I pitched:\n\n\"What if instead a button that destroys the site, we just... encourage people to celebrate Petrov Day, and make a nice illustrated Petrov Day story?\"\n\nAnd then we did that.\n\nI ended up deciding to just implement the existing ceremony as an animated ritual, so people who didn't have an existing community of Petrov Day celebration participants could get a feel for what it was like.\n\nThe \"encourage people to celebrate Petrov Day meetups\" didn't obviously work super well, although in our defense it turned out it was overlapping with both ACX Everywhere and IABI reading groups and we had recently tapped out our usual resources for hustling up a lot of meetups. \n\nThe Illustrated Ceremony is available from the frontpage today. Tomorrow, it'll live on at [http://lesswrong.com/petrov/ceremony](http://lesswrong.com/petrov/ceremony).\n\nI had fun making it. I hope you like it. \n\nThank you Jim for creating it originally, and thank you Stanislav Petrov for not destroying[^v8mtt6agyv] the world that one time.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ba62881fa636c00e20b9c5674a87b8c2ecaad7a27f94c3f7.png)\n\nJim designed the ceremony to work with a regular menorah, to make it more accessible,  \nbut I like doing the ceremony with dope-ass candles.\n\n[^v8mtt6agyv]: According to Nuclear war is unlikely to cause human extinction, \"destroy the world\" might be two levels of false instead of the usual one (i.e. \"the world's still there guys, it's just us who are fucked.\") But, you know what I mean.",
      "plaintextDescription": "Since 2014, some people have celebrated Petrov Day with a small in-person ceremony, with readings by candlelight that tell the story of Petrov within the context of the long arc of history, created by Jim Babcock.\n\nI've found this pretty meaningful, an it somehow feels \"authentic\" to me, like a real holiday. Which, as the creator of Secular Solstice 2020 it is humbling to say, feels more true than Solstice did (for the first few years, Solstice felt a little obviously \"made up\", and now it has more storied history, it instead often feels a bit too much like \"a show\" as opposed to \"a holiday\", at least when you go to the large productions in the Bay or NYC).\n\nI don't know how much my experience generalizes, but having been familiar with Seders (classic Jewish, or rationalist), it just felt to me like a real historical holiday built from a parallel dimension that tumbled into my world.\n\nSolstice was originally intended to be a small affair that people held in people's living room. I ended up holding a large version as an attempt to draw more attention to it. Then it turned out the large version ended up being where most people anthropically end up experiencing Solstice.\n\nJim designed the Petrov Day ceremony to be deliberately awkward to scale – it's designed to find around a dining room table, with everyone within reach of a set of candles. If you end up with more than 20 people, Jim recommends splitting up into multiple groups in multiple rooms. There was a hope for it to fill a niche that encouraged small groups, copying the format of a Seder.\n\nThis is nice, but, it does mean it's harder to actually get a lot of people to know about it. \n\n----------------------------------------\n\nFor the last several years, LessWrong has done some kind of interactive (some might say \"gimmicky\") Petrov Day event each year. Some people complain that it's unclear if they are more like \"playing a game\" or more like \"participating in a sacred ritual where people will judge you horribly i",
      "wordCount": 718
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "kkWmybhaig4oSWkgX",
    "title": "\"Shut It Down\" is simpler than  \"Controlled Takeoff\"",
    "slug": "shut-it-down-is-simpler-than-controlled-takeoff",
    "url": null,
    "baseScore": 89,
    "voteCount": 35,
    "viewCount": null,
    "commentCount": 29,
    "createdAt": null,
    "postedAt": "2025-09-24T17:21:20.917Z",
    "contents": {
      "markdown": "Two somewhat different plans for buying time and improving AI outcomes are: \"Global Shutdown\" and \"Global Controlled Takeoff.\"\n\n(Some other plans some people believe in include *\"ad hoc semi-controlled semi-slowed takeoff\"* and *\"race, then burn the lead on either superalignment or scary demos\"* and *\"decentralized differential defensive tech world.\"*. I mostly don't expect those to work, but am mostly not talking about them in this post.)\n\n\"Global Shutdown\" and \"Global Controlled Takeoff\" both include an early step of \"consolidate all GPUs and similar chips into locations that can be easily monitored.\" \n\nThe **Shut Down** plan then says things like \"you cannot do any frontier development with the consolidated GPUs\" (maybe you can use GPUs to run existing models that seem pretty safe, depends on implimentation details). Also, maybe, any research into new algorithms needs to be approved by an international org, and frontier algorithm development is illegal. *(This is maybe hard to enforce, but, it is might dramatically reduce the amount of R&D that goes into it, since you can't be a billion dollar company who straightforwardly pours tons of resources into it without going to jail)*\n\n**Controlled Takeoff** says instead (as I currently understand advocates to advocate) something like \"Frontier research continues, slowly, carefully, leveraging frontier controlled AI to do a ton of alignment research.\"\n\nI'm generally pro \"Shut It Down\", but I also think Global Controlled Takeoff is much better than the status quo (both because it seems better in isolation, and because achieving it makes Shut Down easier), and I see some of the appeal depending on your exact beliefs.\n\nBut, some notes on strategy here.\n\n* * *\n\n\"What's more impossible?\"\n=========================\n\nA lot of AI safety arguments boil down to \"what seems least impossible?\". Is it more impossible to get a Global Shutdown, or to solve safe superintelligence with anything remotely like our current understanding or the understanding we're likely to get over the next 5-10 years?\n\nI've heard a number of people say flatly \"you're *not* going to get a global shut down\", with a tone of finality that sounds like they think this is basically impossible. \n\nI'm not *entirely* sure  I've correctly tracked which people are saying which things and whether I'm accidentally conflating statements from different people. But I think I've heard at least some people say \"you're not getting a shut down\" with that tone, who nonetheless advocate for controlled takeoff.\n\nI certainly agree getting a global shutdown is very hard. But it's not obvious to me that getting a global controlled takeoff is much easier.\n\nTwo gears I want to make sure people are tracking:\n\n**Gear 1: \"Consolidate and monitor the GPUs\" is a huge political lift, regardless. **\n\nBy the time you've gotten various world powers and corporations to do this extremely major, expensive action, I think something has significantly changed about the political landscape. I don't see how you'd get it without world leaders taking AI more \"fundamentally seriously\", in a way that would make other expensive plans a lot more tractable.\n\n**Gear 2: \"You need to compare the tractability of Global Shut Down vs Global Controlled Takeoff** ***That Actually Works*****, as opposed to Something That Looks Close To But Not Actually A Controlled Takeoff.\"**\n\nAlong with **Gear 3: \"Shut it down\" is much** ***simpler*** **than \"Controlled Takeoff.\"**\n\nA Global Controlled Takeoff That Works has a lot of moving parts. \n\nYou need the international agreement to be capable of making any kind of sensible distinctions between safe and unsafe training runs, or even \"marginally safer\" vs \"marginally less safe\" training runs. \n\nYou need the international agreement to not turn into molochian regulatory-captured horror that perversely reverses the intent of the agreement and creates a class of bureaucrats who don't know anything about AI and use the agreement to dole out favors.\n\nThese problems still exist in some versions of Shut It Down too, to be clear (if you're trying to also ban algorithmic research – a lot of versions of that seem like they leave room to argue about whether agent foundations or interpretability count). But, they at least get coupled with \"no *large* training runs, *period.\"*\n\nI think \"guys, everyone *just stop*\" is a way easier schelling point to coordinate around, than \"everyone, we're going to slow down and try to figure out alignment as best we can using current techniques.\"\n\nSo, I am not currently convinced that Global Controlled Takeoff That Actually Works is any more politically tractable than Global Shut Down.\n\n(Caveat: Insofar as your plan is \"well, we *will* totally get a molochian moral maze horror, but, it'll generally move slower and that buys time\", eh, okay, seems reasonable. But, at least be clear to yourself about what you're aiming for)\n\n* * *\n\n**Gear 4: Removing** ***pressure to accelerate*** **is valuable for the epistemics of the people doing the AI-assisted alignment (if you're trying that).**\n\nOne reason I think the Anthropic plan is actively bad, instead of \"at-least-okay-ish,\" is that (given how hard they seem to *actively oppose* any kind of serious regulation that would slow them down), they seem intent on remaining in a world where, while they are supposedly working on aligning the next generation of AI, they have constant economic pressure to ship the next thing soon.\n\nI believe, maybe, you can leverage AI to help you align AI.\n\nI am pretty confident that at least *some* of the tools you need to navigate aligning unbounded superintelligence (or confidently *avoiding creating* unbounded superintelligence,) involves \"precise conceptual reasoning\" of a kind Anthropic-et-all seem actively allergic to. (see also [behaviorism vs cognitivism](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research). Anthropic culture seems to actively pride itself on empirics and be actively suspicious of attempts to reason ahead without empirics)\n\nI'm not confident that you need that *much* precise conceptual reasoning / reasoning ahead. (MIRI has an inside view that says this is... not like impossible hard, but, is hard in a fairly deep way that nobody is showing respect for. I don't have a clear inside view about \"how hard is it\", but I have an inside view that it's harder than Anthropic's revealed actions thinks it is)\n\nI think thinking through this and figuring out *whether* you need conceptual tools that you aren't currently good at in order to succeed, is very hard, and people are extremely biased about it. \n\nI think the difficult is exacerbated further if your competitor is shipping the next generation of product, and know-in-your-heart that you're reaching ASL danger levels that at least should give you some pause to think about it, but, the evidence isn't clear, and it would be extremely convenient for you and your org if your current level of control/alignment was sufficient to run the next training run.\n\nSo a lot of what I care most about with Shutdown/Controlled-Takeoff is making it no longer true that there is an economic incentive to rush ahead. (I think either Shutdown-y or Controlled Takeoff-y can both potentially work for this, if there's actually a trusted third party who is the one that makes calls about whether the next training run is allowed, who has the guns and compute).\n\n**Gear 5: Political tractability will change as demos get scarier.** \n\nI'm not super thrilled with the \"race to the edge, then burn the lead on scary demos\" plan (specifically the \"racing\" part). But, I do think we will get much scarier demos as we approach AGI. \n\nPoliticians maybe don't understand abstract arguments (although I think responses to If Anyone Builds It suggests they at least sometimes do). But I think there are various flavors of Sufficiently Scary Demos that will make the threat much more salient without needing to route through abstract arguments.\n\nI think one of the most important things to be preparing for is leveraging Sufficiently Scary Demos when they arrive. I think this includes beginning to argue seriously now for global treaty shaped things and have them on hand so people go \"oh, okay, I guess we do need that thing Those Guys Were Talking About After All\" instead of just being bewildered.\n\nGears rather than Bottom Lines\n==============================\n\nI won't make a decisive claim that any of the above should be decisive in anyone's decisionmaking. I'm still processing the update that [I can't really simulate the entire political disagreement yet](https://www.lesswrong.com/posts/vJrP7nbnJTwk4fcbk/simulating-the-rest-of-the-political-disagreement) and I'm not sure what other gears I'm missing from other people's perspective.\n\nBut, these are all individual gears that seem pretty important to me, which I think should be part of other people's overall strategizing.\n\nI have a similar point comparing the feasibility of \"Global Shut Down\" vs \"Decentralized Differentially Defensive Tech world that Actually Works\", but, that's a fairly complex and different argument.\n\n[^sue50uomwck]: to be clear this bias also applies in the MIRI-esque direction. But, they're not the one rushing ahead inventing AGI.",
      "plaintextDescription": "Two somewhat different plans for buying time and improving AI outcomes are: \"Global Shutdown\" and \"Global Controlled Takeoff.\"\n\n(Some other plans some people believe in include \"ad hoc semi-controlled semi-slowed takeoff\" and \"race, then burn the lead on either superalignment or scary demos\" and \"decentralized differential defensive tech world.\". I mostly don't expect those to work, but am mostly not talking about them in this post.)\n\n\"Global Shutdown\" and \"Global Controlled Takeoff\" both include an early step of \"consolidate all GPUs and similar chips into locations that can be easily monitored.\" \n\nThe Shut Down plan then says things like \"you cannot do any frontier development with the consolidated GPUs\" (maybe you can use GPUs to run existing models that seem pretty safe, depends on implimentation details). Also, maybe, any research into new algorithms needs to be approved by an international org, and frontier algorithm development is illegal. (This is maybe hard to enforce, but, it is might dramatically reduce the amount of R&D that goes into it, since you can't be a billion dollar company who straightforwardly pours tons of resources into it without going to jail)\n\nControlled Takeoff says instead (as I currently understand advocates to advocate) something like \"Frontier research continues, slowly, carefully, leveraging frontier controlled AI to do a ton of alignment research.\"\n\nI'm generally pro \"Shut It Down\", but I also think Global Controlled Takeoff is much better than the status quo (both because it seems better in isolation, and because achieving it makes Shut Down easier), and I see some of the appeal depending on your exact beliefs.\n\nBut, some notes on strategy here.\n\n----------------------------------------\n\n\n\"What's more impossible?\"\nA lot of AI safety arguments boil down to \"what seems least impossible?\". Is it more impossible to get a Global Shutdown, or to solve safe superintelligence with anything remotely like our current understanding or the und",
      "wordCount": 1445
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "Xp9ie6pEWFT8Nnhka",
    "title": "Accelerando as a \"Slow, Reasonably Nice Takeoff\" Story",
    "slug": "accelerando-as-a-slow-reasonably-nice-takeoff-story",
    "url": null,
    "baseScore": 68,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2025-09-23T02:15:51.854Z",
    "contents": {
      "markdown": "When I hear a lot of people talk about Slow Takeoff, many of them seem like they are mostly imagining the early part of that takeoff – the part that feels human comprehensible. They're still not imagining superintelligence in the limit.\n\nThere are some genres of Slow Takeoff that culminate in somebody \"leveraging controlled AI to help fully solve the alignment problem, eventually get fully aligned superintelligence, and then end the acute risk period.\" \n\nBut the sort of person I'm thinking of, for this blogpost, usually doesn't seem to have a concrete visualization of something that could plausibly end the period where anyone could choose to deploy uncontrolled superintelligence. They tend to not like Coherent Extrapolated Volition or similar things.\n\nThey seem to be imagining a multipolar d/acc world, where defensive technologies and balance of power is such that you keep getting something like a regular economy running. And even if shit gets quite weird, in some sense it's still the same sort of things happening as today.\n\nI think this world is unlikely. *But*, I do think it'd be good for someone to write a really fleshed out takeoff story and/or forecast that runs with those assumptions.\n\nUnfortunately, slow takeoff stories take longer so there's a lot more moving parts, you have to invent future politics and economics and how they play out together. \n\nBut, fortunately, someone... kinda already did this? \n\nIt's a novel called Accelerando. It was written between 2001 and 2005. And the broad strokes of it still feel kinda reasonable, if I'm starting with multipolar d/acc-ish  optimistic assumptions. \n\nA thing that is nice about Accelerando is that it wasn't written by someone particularly trying to achieve a political outcome, which reduces an important source of potential bias. On the flipside, it *was* written by someone trying to tell a good human-comprehensible story, so, it has that bias instead. (It contains some random elements that don't automatically follow from what we currently know to be true).\n\nIt has lots of details that are too specific for a random sci-fi author in 2001 to have gotten right. But, I think reading through it is helpful for getting some intuitions about what an AI-accelerated world might look and feel like.\n\nIt's probably worth reading the book if you haven't (you can buy it here). But, it contains some vignettes in each chapter that make for a decent summary of the broad strokes. I've compiled some excerpts here that I think make for an okay standalone experience, and I've tried to strip out most bits that spoil the human-centric plot. \n\n(It was hard to strip out *all* spoilers, but, I think I leave enough gaps you'll still have a good time reading the novel afterwards)\n\nThe story is more optimistic than seems realistic to me. But, it's about as optimistic a world as feels plausibly coherent to me that takes place in a centrally multipolar d/acc-ish world that doesn't route through \"someone actually builds very powerful friendly AI that is able to set very strong, permanent safeguards in place.\"\n\nPart 1: \"Slow Takeoff\"\n======================\n\nIn Accelerando, a decade passes between each chapter. It starts approximately 2020. \n\n(The forecasted timing is somewhat off but I bet not too far behind. Most of the tech that exists in chapter 1 could *probably* be built today, but just barely, and it hasn't reached the level of saturation implied in the novel. \n\nChapter 1 \n----------\n\nThe first chapter's vignette is the most character focused (later ones read more like a news bulletin). But, I think it's kind of useful to have the anchor of a specific guy who lives on the cutting edge of the future. \n\nI think this is supposed to take place in the 2010s, which is... early. I think most of the tech here *just barely* exists today, but without quite as much market saturation as the book implies, but would probably exist in 1-8 years.\n\nRemember this is written in 2001.\n\n> Manfred has a suite at the Hotel Jan Luyken paid for by a grateful multinational consumer protection group, and an unlimited public transport pass paid for by a Scottish sambapunk band in return for services rendered. \n> \n> He has airline employee’s travel rights with six flag carriers despite never having worked for an airline. His bush jacket has sixty-four compact supercomputing clusters sewn into it, four per pocket, courtesy of an invisible college that wants to grow up to be the next Media Lab. \n> \n> His dumb clothing comes made to measure from an e-tailor in the Philippines he’s never met. Law firms handle his patent applications on a pro bono basis, and, boy, does he patent a lot—although he always signs the rights over to the Free Intellect Foundation, as contributions to their obligation-free infrastructure project. \n> \n> In IP geek circles, Manfred is legendary; he’s the guy who patented the business practice of moving your e-business somewhere with a slack intellectual property regime in order to evade licensing encumbrances. He’s the guy who patented using genetic algorithms to patent everything they can permutate from an initial description of a problem domain—not just a better mousetrap, but the set of all possible better mousetraps. Roughly a third of his inventions are legal, a third are illegal, and the remainder are legal but will become illegal as soon as the legislatosaurus wakes up, smells the coffee, and panics. \n> \n> \\[...\\]\n> \n> Manfred is at the peak of his profession, which is essentially coming up with whacky but workable ideas and giving them to people who will make fortunes with them. He does this for free, gratis. In return, he has virtual immunity from the tyranny of cash; money is a symptom of poverty, after all, and Manfred never has to pay for anything. \n> \n> There are drawbacks, however. Being a pronoiac meme-broker is a constant burn of future shock—he has to assimilate more than a megabyte of text and several gigs of AV content every day just to stay current. The Internal Revenue Service is investigating him continuously because it doesn’t believe his lifestyle can exist without racketeering. And then there are the items that no money can’t buy: like the respect of his parents. He hasn’t spoken to them for three years, his father thinks he’s a hippy scrounger, and his mother still hasn’t forgiven him for dropping out of his down-market Harvard emulation course. (They’re still locked in the boringly bourgeois twen-cen paradigm of college-career-kids.) \n> \n> \\[...\\]\n> \n> Manfred drops in at his hotel suite, unpacks his Aineko, plugs in a fresh set of cells to charge, and sticks most of his private keys in the safe. Then he heads straight for the party, which is currently happening at De Wildemann’s; it’s a twenty-minute walk, and the only real hazard is dodging the trams that sneak up on him behind the cover of his moving map display. \n> \n> Along the way, his glasses bring him up to date on the news. Europe has achieved peaceful political union for the first time ever: They’re using this unprecedented state of affairs to harmonize the curvature of bananas. The Middle East is, well, it’s just as bad as ever, but the war on fundamentalism doesn’t hold much interest for Manfred. In San Diego, researchers are uploading lobsters into cyberspace, starting with the stomatogastric ganglion, one neuron at a time. They’re burning GM cocoa in Belize and books in Georgia. NASA still can’t put a man on the moon. Russia has reelected the communist government with an increased majority in the Duma; meanwhile, in China, fevered rumors circulate about an imminent rehabilitation, the second coming of Mao, who will save them from the consequences of the Three Gorges disaster. \n> \n> In business news, the US Justice Department is—ironically—outraged at the Baby Bills. The divested Microsoft divisions have automated their legal processes and are spawning subsidiaries, IPOing them, and exchanging title in a bizarre parody of bacterial plasmid exchange, so fast that, by the time the windfall tax demands are served, the targets don’t exist anymore, even though the same staff are working on the same software in the same Mumbai cubicle farms. \n> \n> Welcome to the twenty-first century.\n\nChapter 2\n---------\n\n> Welcome to the \\[second decade of the\\] early twenty-first century, human. \n> \n> It’s night in Milton Keynes, sunrise in Hong Kong. Moore’s Law rolls inexorably on, dragging humanity toward the uncertain future. The planets of the solar system have a combined mass of approximately 2×1027 kilograms. \n> \n> Around the world, laboring women produce forty-five thousand babies a day, representing 1023 MIPS of processing power. Also around the world, fab lines casually churn out thirty million microprocessors a day, representing 1023 MIPS. \n> \n> In another ten months, most of the MIPS being added to the solar system will be machine-hosted for the first time. About ten years after that, the solar system’s installed processing power will nudge the critical 1 MIPS per gram threshold—one million instructions per second per gram of matter. After that, singularity—a vanishing point beyond which extrapolating progress becomes meaningless. The time remaining before the intelligence spike is down to single-digit years . . .\n\nChapter 3\n---------\n\n> Welcome to the eve of the third decade: a time of chaos characterized by an all-out depression in the space industries. \n> \n> Most of the thinking power on the planet is now manufactured rather than born; there are ten microprocessors for every human being, and the number is doubling every fourteen months. Population growth in the developing world has stalled, the birth rate dropping below replacement level. In the wired nations, more forward-looking politicians are looking for ways to enfranchise their nascent AI base. \n> \n> Space exploration is still stalled on the cusp of the second recession of the century. The Malaysian government has announced the goal of placing an imam on Mars within ten years, but nobody else cares enough to try. \n> \n> The Space Settlers Society is still trying to interest Disney Corp. in the media rights to their latest L5 colony plan, unaware that there’s already a colony out there and it isn’t human: First-generation uploads, Californian spiny lobsters in wobbly symbiosis with elderly expert systems, thrive aboard an asteroid mining project established by the Franklin Trust. Meanwhile, Chinese space agency cutbacks are threatening the continued existence of Moonbase Mao. Nobody, it seems, has figured out how to turn a profit out beyond geosynchronous orbit. \n\nPart II: \"Point of Inflection\"\n==============================\n\nChapter 4\n---------\n\n> Welcome to the fourth decade. The thinking mass of the solar system now exceeds one MIPS per gram; it’s still pretty dumb, but it’s not dumb all over. The human population is near maximum overshoot, pushing nine billion, but its growth rate is tipping toward negative numbers, and bits of what used to be the first world are now facing a middle-aged average. Human cogitation provides about 1028 MIPS of the solar system’s brainpower. \n> \n> The real thinking is mostly done by the halo of a thousand trillion processors that surround the meat machines with a haze of computation—individually a tenth as powerful as a human brain, collectively they’re ten thousand times more powerful, and their numbers are doubling every twenty million seconds. They’re up to 1033 MIPS and rising, although there’s a long way to go before the solar system is fully awake. \n> \n> Technologies come, technologies go, but nobody even five years ago predicted that there’d be tinned primates in orbit around Jupiter by now: A synergy of emergent industries and strange business models have kick-started the space age again, aided and abetted by the discovery of (so far undecrypted) signals from ETs. Unexpected fringe riders are developing new ecological niches on the edge of the human information space, light-minutes and light-hours from the core, as an expansion that has hung fire since the 1970s gets under way. \n> \n> Amber, like most of the postindustrialists aboard the orphanage ship Ernst Sanger, is in her early teens. While their natural abilities are in many cases enhanced by germ-line genetic recombination, thanks to her mother’s early ideals she has to rely on brute computational enhancements. She doesn’t have a posterior parietal cortex hacked for extra short-term memory, or an anterior superior temporal gyrus tweaked for superior verbal insight, but she’s grown up with neural implants that feel as natural to her as lungs or fingers. Half her wetware is running outside her skull on an array of processor nodes hooked into her brain by quantum-entangled communication channels—her own personal metacortex. \n> \n> These kids are mutant youth, burning bright: Not quite incomprehensible to their parents, but profoundly alien—the generation gap is as wide as the 1960s and as deep as the solar system. Their parents, born in the gutter years of the twenty-first century, grew up with white elephant shuttles and a space station that just went round and round, and computers that went beep when you pushed their buttons. The idea that Jupiter orbit was somewhere you could go was as profoundly counterintuitive as the Internet to a baby boomer. \n> \n> Most of the passengers on the can have run away from parents who think that teenagers belong in school, unable to come to terms with a generation so heavily augmented that they are fundamentally brighter than the adults around them. Amber was fluent in nine languages by the age of six, only two of them human and six of them serializable; when she was seven, her mother took her to the school psychiatrist for speaking in synthetic tongues. \n> \n> That was the final straw for Amber: Using an illicit anonymous phone, she called her father. \n\nIn this chapter, Amber ends up initiating an automated factory-expansion process on the moons of Jupiter, that ends up making her a powerful cyborg (with the crust-of-multiple moons worth of computronium augmenting her).\n\nChapter 5\n---------\n\nThis chapter (the middle of the \"point of inflection\" act) has three exposition sections.\n\nThe first:\n\n> Greetings from the fifth decade of the century of wonders. \n> \n> The solar system that lies roughly twenty-eight trillion kilometers—just short of three light years—behind the speeding starwhisp *Field Circus* is seething with change. There have been more technological advances in the past ten years than in the entire previous expanse of human history—and more unforeseen accidents. \n> \n> Lots of hard problems have proven to be tractable. The planetary genome and proteome have been mapped so exhaustively that the biosciences are now focusing on the challenge of the phenome—plotting the phase-space defined by the intersection of genes and biochemical structures, understanding how extended phenotypic traits are generated and contribute to evolutionary fitness. \n> \n> The biosphere has become surreal: Small dragons have been sighted nesting in the Scottish highlands, and in the American Midwest, raccoons have been caught programming microwave ovens. \n> \n> The computing power of the solar system is now around one thousand MIPS per gram, and is unlikely to increase in the near term—all but a fraction of one percent of the dumb matter is still locked up below the accessible planetary crusts, and the sapience/mass ratio has hit a glass ceiling that will only be broken when people, corporations, or other posthumans get around to dismantling the larger planets. A start has already been made in Jupiter orbit and the asteroid belt. Greenpeace has sent squatters to occupy Eros and Juno, but the average asteroid is now surrounded by a reef of specialized nanomachinery and debris, victims of a cosmic land grab unmatched since the days of the wild west. \n> \n> The best brains flourish in free fall, minds surrounded by a sapient aether of extensions that out-think their meaty cortices by many orders of magnitude—minds like Amber, Queen of the Inner Ring Imperium, the first self-extending power center in Jupiter orbit. \n> \n> Down at the bottom of the terrestrial gravity well, there has been a major economic catastrophe. Cheap immortagens, out-of-control personality adjuvants, and a new formal theory of uncertainty have knocked the bottom out of the insurance and underwriting industries. Gambling on a continuation of the worst aspects of the human condition—disease, senescence, and death—looks like a good way to lose money, and a deflationary spiral lasting almost fifty hours has taken down huge swaths of the global stock market. Genius, good looks, and long life are now considered basic human rights in the developed world: Even the poorest backwaters are feeling extended effects from the commoditization of intelligence. \n> \n> Not everything is sweetness and light in the era of mature nanotechnology. Widespread intelligence amplification doesn’t lead to widespread rational behavior. New religions and mystery cults explode across the planet; much of the Net is unusable, flattened by successive semiotic jihads. India and Pakistan have held their long-awaited nuclear war: External intervention by US and EU nanosats prevented most of the IRBMs from getting through, but the subsequent spate of network raids and Basilisk attacks cause havoc. Luckily, infowar turns out to be more survivable than nuclear war—especially once it is discovered that a simple anti-aliasing filter stops nine out of ten neural-wetware-crashing Langford fractals from causing anything worse than a mild headache. \n> \n> New discoveries this decade include the origins of the weakly repulsive force responsible for changes in the rate of expansion of the universe after the big bang, and on a less abstract level, experimental implementations of a Turing Oracle using quantum entanglement circuits: a device that can determine whether a given functional expression can be evaluated in finite time. It’s boom time in the field of Extreme Cosmology, where some of the more recherché researchers are bickering over the possibility that the entire universe was created as a computing device, with a program encoded in the small print of the Planck constant. And theorists are talking again about the possibility of using artificial wormholes to provide instantaneous connections between distant corners of space-time. \n> \n> Most people have forgotten about the well-known extraterrestrial transmission received fifteen years earlier. Very few people know anything about the second, more complex transmission received a little later. Many of those are now passengers or spectators of the *Field Circus*: a light-sail craft that is speeding out of Sol system on a laser beam generated by Amber’s installations in low-Jupiter orbit. (Superconducting tethers anchored to Amalthea drag through Jupiter’s magnetosphere, providing gigawatts of electricity for the hungry lasers: energy that comes in turn from the small moon’s orbital momentum.) \n> \n> Manufactured by Airbus-Cisco years earlier, the *Field Circus* is a hick backwater, isolated from the mainstream of human culture, its systems complexity limited by mass. The destination lies nearly three light years from Earth, and even with high acceleration and relativistic cruise speeds the one-kilogram starwhisp and its hundred-kilogram light sail will take the best part of seven years to get there. Sending a human-sized probe is beyond even the vast energy budget of the new orbital states in Jupiter system—near-lightspeed travel is horrifically expensive. \n> \n> Rather than a big, self-propelled ship with canned primates for passengers, as previous generations had envisaged, the starship is a Coke-can-sized slab of nanocomputers, running a neural simulation of the uploaded brain states of some tens of humans at merely normal speed. \n> \n> By the time its occupants beam themselves home again for download into freshly cloned bodies, a linear extrapolation shows that as much change will have overtaken human civilization as in the preceding fifty millennia—the sum total of H. sapiens sapiens’ time on Earth. But that’s okay by Amber, because what she expects to find in orbit around the brown dwarf Hyundai +4904/-56 will be worth the wait.\n\nThe second:\n\n> Outside the light cone of the Field Circus, on the other side of the spacelike separation between Amber’s little kingdom in motion and the depths of empire time that grip the solar system’s entangled quantum networks, a singular new reality is taking shape. \n> \n> Welcome to the moment of maximum change. \n> \n> About ten billion humans are alive in the solar system, each mind surrounded by an exocortex of distributed agents, threads of personality spun right out of their heads to run on the clouds of utility fog—infinitely flexible computing resources as thin as aerogel—in which they live. The foggy depths are alive with high-bandwidth sparkles; most of Earth’s biosphere has been wrapped in cotton wool and preserved for future examination. For every living human, a thousand million software agents carry information into the farthest corners of the consciousness address space. \n> \n> The sun, for so long an unremarkable mildly variable G2 dwarf, has vanished within a gray cloud that englobes it except for a narrow belt around the plane of the ecliptic. Sunlight falls, unchanged, on the inner planets: except for Mercury, which is no longer present, having been dismantled completely and turned into solar-powered high-temperature nanocomputers. A much fiercer light falls on Venus, now surrounded by glittering ferns of carbon crystals that pump angular momentum into the barely spinning planet via huge superconducting loops wound around its equator. This planet, too, is due to be dismantled. Jupiter, Neptune, Uranus—all sprout rings as impressive as Saturn’s. But the task of cannibalizing the gas giants will take many times longer than the small rocky bodies of the inner system. \n> \n> The ten billion inhabitants of this radically changed star system remember being human; almost half of them predate the millennium. Some of them still are human, untouched by the drive of metaevolution that has replaced blind Darwinian change with a goal-directed teleological progress. They cower in gated communities and hill forts, mumbling prayers and cursing the ungodly meddlers with the natural order of things. But eight out of every ten living humans are included in the phase-change. It’s the most inclusive revolution in the human condition since the discovery of speech. \n> \n> A million outbreaks of gray goo—runaway nanoreplicator excursions—threaten to raise the temperature of the biosphere dramatically. They’re all contained by the planetary-scale immune system fashioned from what was once the World Health Organization. Weirder catastrophes threaten the boson factories in the Oort cloud. Antimatter factories hover over the solar poles. Sol system shows all the symptoms of a runaway intelligence excursion, exuberant blemishes as normal for a technological civilization as skin problems on a human adolescent. \n> \n> The economic map of the planet has changed beyond recognition. Both capitalism and communism, bickering ideological children of a protoindustrial outlook, are as obsolete as the divine right of kings. Companies are alive, and dead people may live again, too. Globalism and tribalism have run to completion, diverging respectively into homogeneous interoperability and the Schwarzschild radius of insularity. Beings that remember being human plan the deconstruction of Jupiter, the creation of a great simulation space that will expand the habitat available within the solar system. By converting all the nonstellar mass of the solar system into processors, they can accommodate as many human-equivalent minds as a civilization with a planet hosting ten billion humans in orbit around every star in the galaxy. \n> \n> A more mature version of Amber lives down in the surging chaos of near-Jupiter space; there’s an instance of Pierre, too, although he has relocated light-hours away, near Neptune. Whether she still sometimes thinks of her relativistic twin, nobody can tell. In a way, it doesn’t matter, because by the time the Field Circus returns to Jupiter orbit, as much subjective time will have elapsed for the fast-thinkers back home as will flash by in the real universe between this moment and the end of the era of star formation, many billions of years hence.\n\nAnd finally:\n\n> Welcome to the downslope on the far side of the curve of accelerating progress. \n> \n> Back in the solar system, Earth orbits through a dusty tunnel in space. Sunlight still reaches the birth world, but much of the rest of the star’s output has been trapped by the growing concentric shells of computronium built from the wreckage of the innermost planets. \n> \n> Two billion or so mostly unmodified humans scramble in the wreckage of the phase transition, not understanding why the vasty superculture they so resented has fallen quiet. Little information leaks through their fundamentalist firewalls, but what there is shows a disquieting picture of a society where there are no bodies anymore. Utility foglets blown on the wind form aerogel towers larger than cyclones, removing the last traces of physical human civilization from most of Europe and the North American coastlines. Enclaves huddle behind their walls and wonder at the monsters and portents roaming the desert of postindustrial civilization, mistaking acceleration for collapse. \n> \n> The hazy shells of computronium that ring the sun—concentric clouds of nanocomputers the size of rice grains, powered by sunlight, orbiting in shells like the packed layers of a Matrioshka doll—are still immature, holding barely a thousandth of the physical planetary mass of the system, but they already support a classical computational density of 1042 MIPS; enough to support a billion civilizations as complex as the one that existed immediately before the great disassembly. The conversion hasn’t yet reached the gas giants, and some scant outer-system enclaves remain independent—Amber’s Ring Imperium still exists as a separate entity, and will do so for some years to come—but the inner solar system planets, with the exception of Earth, have been colonized more thoroughly than any dusty NASA proposal from the dawn of the space age could have envisaged. \n> \n> From outside the Accelerated civilization, it isn’t really possible to know what’s going on inside. The problem is bandwidth: While it’s possible to send data in and get data out, the sheer amount of computation going on in the virtual spaces of the Acceleration dwarfs any external observer. Inside that swarm, minds a trillion or more times as complex as humanity think thoughts as far beyond human imagination as a microprocessor is beyond a nematode worm. A million random human civilizations flourish in worldscapes tucked in the corner of this world-mind. Death is abolished, life is triumphant. A thousand ideologies flower, human nature adapted where necessary to make this possible. Ecologies of thought are forming in a Cambrian explosion of ideas, for the solar system is finally rising to consciousness, and mind is no longer restricted to the mere kilotons of gray fatty meat harbored in fragile human skulls. \n> \n> Somewhere in the Acceleration, colorless green ideas adrift in furious sleep remember a tiny starship launched years ago, and pay attention. Soon, they realize, the starship will be in position to act as their proxy in an ages-long conversation. Negotiations for access to Amber’s extrasolar asset commence; the Ring Imperium prospers, at least for a while. But first, the operating software on the human side of the network link will require an upgrade.\n\nChapter 6\n---------\n\n> Welcome to decade the sixth, millennium three. These old datelines don’t mean so much anymore, for while some billions of fleshbody humans are still infected with viral memes, the significance of theocentric dating has been dealt a body blow. This may be the fifties, but what that means to you depends on how fast your reality rate runs. The various upload clades exploding across the reaches of the solar system vary by several orders of magnitude—some are barely out of 2049, while others are exploring the subjective thousandth millennium. \n> \n> While the *Field Circus* floats in orbit \\[... around\\] the brown dwarf Hyundai +4904/-56), while Amber and her crew are trapped \\[...\\] —while all this is going on, the damnfool human species has finally succeeded in making itself obsolete. \n> \n> The proximate cause of its displacement from the pinnacle of creation (or the pinnacle of teleological self-congratulation, depending on your stance on evolutionary biology) is an attack of self-aware corporations. The phrase “smart money” has taken on a whole new meaning, for the collision between international business law and neurocomputing technology has given rise to a whole new family of species—fast-moving corporate carnivores in the net. The planet Mercury has been broken up by a consortium of energy brokers, and Venus is an expanding debris cloud, energized to a violent glare by the trapped and channeled solar output. A million billion fist-sized computing caltrops, backsides glowing dull red with the efflux from their thinking, orbit the sun at various inclinations no farther out than Mercury used to be. \n> \n> Billions of fleshbody humans refuse to have anything to do with the blasphemous new realities. Many of their leaders denounce the uploads and AIs as soulless machines. Many more are timid, harboring self-preservation memes that amplify a previously healthy aversion to having one’s brain peeled like an onion by mind-mapping robots into an all-pervading neurosis. Sales of electrified tinfoil-lined hats are at an all-time high. \n> \n> Still, hundreds of millions have already traded their meat puppets for mind machines, and they breed fast. In another few years, the fleshbody populace will be an absolute minority of the posthuman clade. Sometime later, there will probably be a war. The dwellers in the thoughtcloud are hungry for dumb matter to convert, and the fleshbodies make notoriously poor use of the collection of silicon and rare elements that pool at the bottom of the gravity well that is Earth. \n> \n> Energy and thought are driving a phase-change in the condensed matter substance of the solar system. The MIPS per kilogram metric is on the steep upward leg of a sigmoid curve—dumb matter is coming to life as the mind children restructure everything with voracious nanomechanical servants. The thoughtcloud forming in orbit around the sun will ultimately be the graveyard of a biological ecology, another marker in space visible to the telescopes of any new iron-age species with the insight to understand what they’re seeing: the death throes of dumb matter, the birth of a habitable reality vaster than a galaxy and far speedier. \n> \n> Death throes that ‘within a few centuries’ will mean the extinction of biological life within a light year or so of that star—for the majestic Matrioshka brains, though they are the pinnacles of sentient civilization, are intrinsically hostile environments for fleshy life.\n\nPart III: \"Singularity\"\n=======================\n\nChapter 7:\n----------\n\n> Welcome to decade eight, third millennium, when the effects of the phase-change in the structure of the solar system are finally becoming visible on a cosmological scale. \n> \n> There are about eleven billion future-shocked primates in various states of life and undeath throughout the solar system. Most of them cluster where the interpersonal bandwidth is hottest, down in the water zone around old Earth. Earth’s biosphere has been in the intensive care ward for decades, weird rashes of hot-burning replicators erupting across it before the World Health Organization can fix them—gray goo, thylacines, dragons. \n> \n> The last great transglobal trade empire, run from the arcologies of Hong Kong, has collapsed along with capitalism, rendered obsolete by a bunch of superior deterministic resource allocation algorithms collectively known as Economics 2.0. Mercury, Venus, Mars, and Luna are all well on the way to disintegration, mass pumped into orbit with energy stolen from the haze of free-flying thermoelectrics that cluster so thickly around the solar poles that the sun resembles a fuzzy red ball of wool the size of a young red giant. \n> \n> Humans are just barely intelligent tool users; Darwinian evolutionary selection stopped when language and tool use converged, leaving the average hairy meme carrier sadly deficient in smarts. Now the brightly burning beacon of sapience isn’t held by humans anymore—their cross-infectious enthusiasms have spread to a myriad of other hosts, several types of which are qualitatively better at thinking. \n> \n> At last count, there were about a thousand nonhuman intelligent species in Sol space, split evenly between posthumans on one side, naturally self-organizing AIs in the middle, and mammalian nonhumans on the other. The common mammal neural chassis is easily upgraded to human-style intelligence in most species that can carry, feed and cool a half kilogram of gray matter, and the descendants of a hundred ethics-challenged doctoral theses are now demanding equal rights. So are the unquiet dead: the panopticon-logged net ghosts of people who lived recently enough to imprint their identities on the information age, and the ambitious theological engineering schemes of the Reformed Tiplerite Church of Latter-Day Saints (who want to emulate all possible human beings in real time, so that they can have the opportunity to be saved). \n> \n> The human memesphere is coming alive, although how long it remains recognizably human is open to question. The informational density of the inner planets is visibly converging on Avogadro’s number of bits per mole, one bit per atom, as the deconstructed dumb matter of the inner planets (apart from Earth, preserved for now like a picturesque historic building stranded in an industrial park) is converted into computronium. \n> \n> And it’s not just the inner system. The same forces are at work on Jupiter’s moons, and those of Saturn, although it’ll take thousands of years rather than mere decades to dismantle the gas giants themselves. Even the entire solar energy budget isn’t enough to pump Jupiter’s enormous mass to orbital velocity in less than centuries. The fast-burning primitive thinkers descended from the African plains apes may have vanished completely or transcended their fleshy architecture before the solar Matrioshka brain is finished. It won’t be long now . . .\n\nChapter 8\n---------\n\nBefore it gets to the usual News Bulletin, Chapter 8 introduces this FAQ:\n\n> Welcome to Saturn, your new home world. This FAQ (Frequently Asked Questions) memeplex is designed to orient you and explain the following: \n> \n> *   How you got here\n> *   Where “here” is\n> *   Things you should avoid doing\n> *   Things you might want to do as soon as possible\n> *   Where to go for more information. \n> \n> If you are remembering this presentation, you are probably resimulated. This is not the same as being resurrected. You may remember dying. Do not worry: Like all your other memories, it is a fabrication. In fact, this is the first time you have ever been alive. (Exception: If you died after the singularity, you may be a genuine resurrectee. In which case, why are you reading this FAQ?) \n> \n> **HOW YOU GOT HERE: **\n> \n> The center of the solar system—Mercury, Venus, Earth’s Moon, Mars, the asteroid belt, and Jupiter—have been dismantled, or are being dismantled, by weakly godlike intelligences. *\\[NB: Monotheistic clergy and Europeans who remember living prior to 1600, see alternative memeplex “in the beginning.”\\] *\n> \n> A weakly godlike intelligence is not a supernatural agency but the product of a highly advanced society that learned how to artificially create souls \\[*late twentieth century: software\\]* and translate human minds into souls and vice versa. *\\[Core concepts: Human beings all have souls. Souls are software objects. Software is not immortal.\\]*\n> \n> Some of the weakly godlike intelligences appear to cultivate an interest in their human antecedents—for whatever reason is not known. (Possibilities include the study of history through horticulture, entertainment through live-action role-playing, revenge, and economic forgery.) While no definitive analysis is possible, all the resimulated persons to date exhibit certain common characteristics: They are all based on well-documented historical persons, their memories show suspicious gaps \\[see: smoke and mirrors\\], and they are ignorant of or predate the singularity *\\[see: Turing Oracle, Vinge catastrophe\\]. *\n> \n> It is believed that the weakly godlike agencies have created you as a vehicle for the introspective study of your historical antecedent by backward-chaining from your corpus of documented works, and the back-projected genome derived from your collateral descendants, to generate an abstract description of your computational state vector. This technique is extremely intensive *\\[see: expTime-complete algorithms, Turing Oracle, time travel, industrial magic\\]* but marginally plausible in the absence of supernatural explanations. \n> \n> After experiencing your life, the weakly godlike agencies have expelled you. For reasons unknown, they chose to do this by transmitting your upload state and genome/proteome complex to receivers owned and operated by a consortium of charities based on Saturn. These charities have provided for your basic needs, including the body you now occupy. \n> \n> In summary: You are a reconstruction of someone who lived and died a long time ago, not a reincarnation. You have no intrinsic moral right to the identity you believe to be your own, and an extensive body of case law states that you do not inherit your antecedent’s possessions. Other than that, you are a free individual. \n> \n> Note that fictional resimulation is strictly forbidden. If you have reason to believe that you may be a fictional character, you must contact the city immediately*. \\[ See: James Bond, Spider Jerusalem.\\]* Failure to comply is a felony. \n> \n> **WHERE YOU ARE: **\n> \n> You are on Saturn. Saturn is a gas giant planet 120,500 kilometers in diameter, located 1.5 billion kilometers from Earth’s sun. *\\[NB: Europeans who remember living prior to 1580, see alternative memeplex “the flat Earth—not”.\\]* \n> \n> Saturn has been partially terraformed by posthuman emigrants from Earth and Jupiter orbit: The ground beneath your feet is, in reality, the floor of a hydrogen balloon the size of a continent, floating in Saturn’s upper atmosphere. *\\[NB: Europeans who remember living prior to 1790, internalize the supplementary memeplex: “the Brothers Montgolfier.”\\]* \n> \n> The balloon is very safe, but mining activities and the use of ballistic weapons are strongly deprecated because the air outside is unbreathable and extremely cold. \n> \n> The society you have been instantiated in is extremely wealthy within the scope of Economics 1.0, the value transfer system developed by human beings during and after your own time. Money exists, and is used for the usual range of goods and services, but the basics—food, water, air, power, off-the-shelf clothing, housing, historical entertainment, and monster trucks—are free. An implicit social contract dictates that, in return for access to these facilities, you obey certain laws. \n> \n> If you wish to opt out of this social contract, be advised that other worlds may run Economics 2.0 or subsequent releases. These value-transfer systems are more efficient—hence wealthier—than Economics 1.0, but true participation in Economics 2.0 is not possible without dehumanizing cognitive surgery. \n> \n> Thus, in absolute terms, although this society is richer than any you have ever heard of, it is also a poverty-stricken backwater compared to its neighbors. \n> \n> **THINGS YOU SHOULD AVOID DOING: **\n> \n> Many activities that have been classified as crimes in other societies are legal here.\n> \n> These include but are not limited to: acts of worship, art, sex, violence, communication, or commerce between consenting competent sapients of any species, except where such acts transgress the list of prohibitions below. *\\[See additional memeplex: competence defined.\\] *\n> \n> Some activities are prohibited here and may have been legal in your previous experience. These include willful deprivation of ability to consent *\\[see: slavery\\],* interference in the absence of consent *\\[see: minors, legal status of\\]*, formation of limited liability companies *\\[see: singularity\\]*, and invasion of defended privacy *\\[see: the Slug, Cognitive Pyramid Schemes, Brain Hacking, Thompson Trust Exploit\\]*. \n> \n> Some activities unfamiliar to you are highly illegal and should be scrupulously avoided. These include: possession of nuclear weapons, possession of unlimited autonomous replicators *\\[see: gray goo\\],* coercive assimilationism *\\[see: borganism, aggressive\\]*, coercive halting of Turing-equivalent personalities *\\[see: Basilisks\\],* and applied theological engineering *\\[see: God bothering\\]*. \n> \n> Some activities superficially familiar to you are merely stupid and should be avoided for your safety, although they are not illegal as such. These include: giving your bank account details to the son of the Nigerian Minister of Finance; buying title to bridges, skyscrapers, spacecraft, planets, or other real assets; murder; selling your identity; and entering into financial contracts with entities running Economics 2.0 or higher. \n> \n> **THINGS YOU SHOULD DO AS SOON AS POSSIBLE: **\n> \n> Many material artifacts you may consider essential to life are freely available—just ask the city, and it will grow you clothes, a house, food, or other basic essentials. Note, however, that the library of public domain structure templates is of necessity restrictive and does not contain items that are highly fashionable or that remain in copyright. Nor will the city provide you with replicators, weapons, sexual favors, slaves, or zombies. \n> \n> You are advised to register as a citizen as soon as possible. If the individual you are a resimulation of can be confirmed dead, you may adopt their name but not—in law—any lien or claim on their property, contracts, or descendants. You register as a citizen by asking the city to register you; the process is painless and typically complete within four hours. Unless you are registered, your legal status as a sapient organism may be challenged. The ability to request citizenship rights is one of the legal tests for sapience, and failure to comply may place you in legal jeopardy. \n> \n> You can renounce your citizenship whenever you wish: This may be desirable if you emigrate to another polity. While many things are free, it is highly likely that you possess no employable skills, and therefore, no way of earning money with which to purchase unfree items. The pace of change in the past century has rendered almost all skills you may have learned obsolete *\\[see: singularity\\].* \n> \n> However, owing to the rapid pace of change, many cooperatives, trusts, and guilds offer on-the-job training or educational loans. Your ability to learn depends on your ability to take information in the format in which it is offered. Implants are frequently used to provide a direct link between your brain and the intelligent machines that surround it. A basic core implant set is available on request from the city. \\[See: implant security, firewall, wetware.\\] \n> \n> Your health is probably good if you have just been reinstantiated, and is likely to remain good for some time. Most diseases are curable, and in event of an incurable ailment or injury, a new body may be provided—for a fee. (In event of your murder, you will be furnished with a new body at the expense of your killer.) If you have any preexisting medical conditions or handicaps, consult the city.\n> \n> The city is an agoric-annealing participatory democracy with a limited liability constitution. Its current executive agency is a weakly godlike intelligence that chooses to associate with human-equivalent intelligences: This agency is colloquially known as \\[spoilers\\] and may manifest itself in a variety of physical avatars if corporeal interaction is desired. (Prior to the arrival of \\[spoilers\\] the city used a variety of human-designed expert systems that provided suboptimal performance.) \n> \n> The city’s mission statement is to provide a mediatory environment for human-equivalent intelligences and to preserve same in the face of external aggression. Citizens are encouraged to participate in the ongoing political processes of determining such responses. Citizens also have a duty to serve on a jury if called (including senatorial service), and to defend the city. \n> \n> **WHERE TO GO FOR FURTHER INFORMATION: **\n> \n> Until you have registered as a citizen and obtained basic implants, all further questions should be directed to the city. Once you have learned to use your implants, you will not need to ask this question. \n\n**Followed later by:**\n\n> Welcome to decade the ninth, singularity plus one gigasecond (or maybe more—nobody’s quite sure when, or indeed if, a singularity has been created). \n> \n> The human population of the solar system is either six billion, or sixty billion, depending on whether you class-forked state vectors of posthumans and the simulations of dead phenotypes running in the Vile Offspring’s Schrödinger boxes as people. Most of the physically incarnate still live on Earth, but the lily pads floating beneath continent-sized hot-hydrogen balloons in Saturn’s upper atmosphere already house a few million, and the writing is on the wall for the rocky inner planets. \n> \n> All the remaining human-equivalent intelligences with half a clue to rub together are trying to emigrate before the Vile Offspring decide to recycle Earth to fill in a gap in the concentric shells of nanocomputers they’re running on. The half-constructed Matrioshka brain already darkens the skies of Earth and has caused a massive crash in the planet’s photosynthetic biomass, as plants starve for short-wavelength light. \n> \n> Since decade the seventh, the computational density of the solar system has soared. Within the asteroid belt, more than half the available planetary mass has been turned into nanopro-cessors, tied together by quantum entanglement into a web so dense that each gram of matter can simulate all the possible life experiences of an individual human being in a scant handful of minutes.\n> \n> Economics 2.0 is itself obsolescent, forced to mutate in a furious survivalist arms race by \\[spoilers\\]. Only the name remains as a vague shorthand for merely human-equivalent intelligences to use when describing interactions they don’t understand. \n> \n> The latest generation of posthuman entities is less overtly hostile to humans, but much more alien than the generations of the fifties and seventies. Among their less comprehensible activities, the Vile Offspring are engaged in exploring the phase-space of all possible human experiences from the inside out. Perhaps they caught a dose of the Tiplerite heresy along the way, for now a steady stream of resimulant uploads is pouring through the downsystem relays in Titan orbit. \n\n**Even later in chapter 8:**\n\n> Welcome to the afterglow of the intelligence supernova, little tapeworm. \n> \n> Tapeworms have on the order of a thousand neurons, pulsing furiously to keep their little bodies twitching. Human beings have on the order of a hundred billion neurons. What is happening in the inner solar system as the Vile Offspring churn and reconfigure the fast-thinking structured dust clouds that were once planets is as far beyond the ken of merely human consciousness as the thoughts of a Gödel are beyond the twitching tropisms of a worm. Personality modules bounded by the speed of light, sucking down billions of times the processing power of a human brain, form and re-form in the halo of glowing nanopro-cessors that shrouds the sun in a ruddy, glowing cloud. \n> \n> Mercury, Venus, Mars, Ceres, and the asteroids—all gone. Luna is a silvery iridescent sphere, planed smooth down to micrometer heights, luminous with diffraction patterns. Only Earth, the cradle of human civilization, remains untransformed; and Earth, too, will be dismantled soon enough, for already a trellis of space elevators webs the planet around its equator, lifting refugee dumb matter into orbit and flinging it at the wildlife preserves of the outer system. \n> \n> The intelligence bloom that gnaws at Jupiter’s moons with claws of molecular machinery won’t stop until it runs out of dumb matter to convert into computronium. By the time it does, it will have as much brainpower as you’d get if you placed a planet with a population of six billion future-shocked primates in orbit around every star in the Milky Way galaxy. But right now, it’s still stupid, having converted barely a percentage point of the mass of the solar system—it’s a mere Magellanic Cloud civilization, infantile and unsubtle and still perilously close to its carbon-chemistry roots. \n> \n> It’s hard for tapeworms living in warm intestinal mulch to wrap their thousand-neuron brains around whatever it is that the vastly more complex entities who host them are discussing, but one thing’s sure—the owners have a lot of things going on, not all of them under conscious control. The churning of gastric secretions and the steady ventilation of lungs are incomprehensible to the simple brains of tapeworms, but they serve the purpose of keeping the humans alive and provide the environment the worms live in. And other more esoteric functions that contribute to survival—the intricate dance of specialized cloned lymphocytes in their bone marrow and lymph nodes, the random permutations of antibodies constantly churning for possible matches to intruder molecules warning of the presence of pollution—are all going on beneath the level of conscious control. \n> \n> Autonomic defenses. Antibodies. Intelligence blooms gnawing at the edges of the outer system. And humans are not as unsophisticated as mulch wrigglers, they can see the writing on the wall. Is it any surprise that among the ones who look outward, the real debate is not over whether to run but over how far and how fast?\n\nChapter 9\n---------\n\n> \\[A nearby\\] brown dwarf system has succumbed to an anthropic infestation.\n> \n> An unoptimized instance of H. sapiens maintains state coherency for only two to three gigaseconds before it succumbs to necrosis. But in only about ten gigaseconds, the infestation has turned the dead brown dwarf system upside down. \n> \n> They strip-mined the chilly planets to make environments suitable for their own variety of carbon life. They rearranged moons, building massive structures the size of asteroids. They ripped wormhole endpoints free of the routers and turned them into their own crude point-to-point network, learned how to generate new wormholes, then ran their own packet-switched polities over them. \n> \n> Wormhole traffic now supports an ever-expanding mesh of interstellar human commerce, but always in the darkness between the lit stars and the strange, metal-depleted dwarfs with the suspiciously low-entropy radiation. The sheer temerity of the project is mind-boggling. Notwithstanding that canned apes are simply not suited to life in the interstellar void, especially in orbit around a brown dwarf whose planets make Pluto seem like a tropical paradise, they’ve taken over the whole damn system. \n> \n> New Japan is one of the newer human polities in this system, a bunch of nodes physically collocated in the humaniformed spaces of the colony cylinders. Its designers evidently only knew about old Nippon from recordings made back before Earth was dismantled, and worked from a combination of nostalgia-trip videos, Miyazaki movies, and anime culture. Nevertheless, it’s the home of numerous human beings—even if they are about as similar to their historical antecedents as New Japan is to its long-gone namesake. \n> \n> Humanity? \n> \n> Their grandparents would recognize them, mostly. The ones who are truly beyond the ken of twentieth-century survivors stayed back home in the red-hot clouds of nanocomputers that have replaced the planets that once orbited Earth’s sun in stately Copernican harmony. The fast-thinking Matrioshka brains are as incomprehensible to their merely posthuman ancestors as an ICBM to an amoeba—and about as inhabitable. \n> \n> Space is dusted with the corpses of Matrioshka brains that have long since burned out, informational collapse taking down entire civilizations that stayed in close orbit around their home stars. Farther away, galaxy-sized intelligences beat incomprehensible rhythms against the darkness of the vacuum, trying to hack the Planck substrate into doing their bidding. \n> \n> Posthumans, and the few other semitranscended species \\[...\\] live furtively in the darkness between these islands of brilliance. There are, it would seem, advantages to not being too intelligent. \n> \n> Humanity. Monadic intelligences, mostly trapped within their own skulls, living in small family groups within larger tribal networks, adaptable to territorial or migratory lifestyles. \n> \n> Those were the options on offer before the great acceleration. Now that dumb matter thinks, with every kilogram of wallpaper potentially hosting hundreds of uploaded ancestors, now that every door is potentially a wormhole to a hab half a parsec away, the humans can stay in the same place while the landscape migrates and mutates past them, streaming into the luxurious void of their personal history. Life is rich here, endlessly varied and sometimes confusing. \n> \n> So it is that tribal groups remain, their associations mediated across teraklicks and gigaseconds by exotic agencies. And sometimes the agencies will vanish for a while, reappearing later like an unexpected jape upon the infinite.\n\nPostscript\n==========\n\nI don't really buy, given the scenario, that humans-qua-humans actually survive as much as they are depicted here. The Accelerando world doesn't seem to exactly have \"grabby\" posthumans or aliens, which seems unrealistic to me, because it only takes one to render all available matter under assault by vastly powerful forces that traditional humans couldn't defend against, even weak brown dwarf stars. \n\n(It's been awhile since I read it, I vaguely recall some in-universe reasons that it worked out with less grabbiness, but they were not reasons I expect to generalize to our world).\n\nAccelerando is deliberately unclear about what's going on inside the Vile Offspring posthumans. It's not known whether they are conscious or otherwise have properties that I'd consider morally valuable. \n\nThe story doesn't really grapple with Hansonian arguments, about what evolutionary forces start applying once all matter has been claimed, and we leave [the dreamtime](https://www.overcomingbias.com/p/this-is-the-dream-timehtml). (That is: there are no longer growing piles of resources that allow populations to grow while still having a high-surplus standard of living. And, there is no mechanism enforcing limits on reproduction. This implies a reversion to subsistence living, albeit in a very different form than our primitive ancestors)",
      "plaintextDescription": "When I hear a lot of people talk about Slow Takeoff, many of them seem like they are mostly imagining the early part of that takeoff – the part that feels human comprehensible. They're still not imagining superintelligence in the limit.\n\nThere are some genres of Slow Takeoff that culminate in somebody \"leveraging controlled AI to help fully solve the alignment problem, eventually get fully aligned superintelligence, and then end the acute risk period.\" \n\nBut the sort of person I'm thinking of, for this blogpost, usually doesn't seem to have a concrete visualization of something that could plausibly end the period where anyone could choose to deploy uncontrolled superintelligence. They tend to not like Coherent Extrapolated Volition or similar things.\n\nThey seem to be imagining a multipolar d/acc world, where defensive technologies and balance of power is such that you keep getting something like a regular economy running. And even if shit gets quite weird, in some sense it's still the same sort of things happening as today.\n\nI think this world is unlikely. But, I do think it'd be good for someone to write a really fleshed out takeoff story and/or forecast that runs with those assumptions.\n\nUnfortunately, slow takeoff stories take longer so there's a lot more moving parts, you have to invent future politics and economics and how they play out together. \n\nBut, fortunately, someone... kinda already did this? \n\nIt's a novel called Accelerando. It was written between 2001 and 2005. And the broad strokes of it still feel kinda reasonable, if I'm starting with multipolar d/acc-ish  optimistic assumptions. \n\nA thing that is nice about Accelerando is that it wasn't written by someone particularly trying to achieve a political outcome, which reduces an important source of potential bias. On the flipside, it was written by someone trying to tell a good human-comprehensible story, so, it has that bias instead. (It contains some random elements that don't automatically follow fr",
      "wordCount": 8894
    },
    "tags": [
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "voEAJ9nFBAqau8pNN",
    "title": "The title is reasonable",
    "slug": "the-title-is-reasonable",
    "url": null,
    "baseScore": 190,
    "voteCount": 82,
    "viewCount": null,
    "commentCount": 124,
    "createdAt": null,
    "postedAt": "2025-09-20T08:59:10.085Z",
    "contents": {
      "markdown": "Alt title: \"I don't believe you that you actually disagree particularly with the core thesis of the book, if you pay attention to what it actually says.\"\n---------------------------------------------------------------------------------------------------------------------------------------------------------\n\nI'm annoyed by various people who seem to be complaining about the book title being \"unreasonable\". i.e. who don't merely disagree with the title of \"[If Anyone Builds It, Everyone Dies](https://www.amazon.com/gp/product/0316595640)\", but, think something like: \"Eliezer/Nate violated a Group-Epistemic-Norm.\" \n\nI think the title is reasonable. \n\nI think the title is probably true. I'm less confident than Eliezer/Nate, but I don't think it's unreasonable for them to be confident in it given their epistemic state. So I want to defend several decisions about the book I think were: \n\n1.  Actually pretty reasonable from a meta-group-epistemics/comms perspective\n2.  Very important to do.\n\nI've heard different things from different people and maybe am drawing a cluster where there is none, but, some things I've heard:\n\n**Complaint #1**: *\"They really shouldn't have exaggerated the situation like this.\"*\n\n**Complaint #2:** *\"Eliezer and Nate are crazy overconfident, and it's going to cost them/us credibility.\"*\n\n**Complaint #3:** \"*It sucks that the people with the visible views are going to be more extreme, eye-catching and simplistic. There's a nearby title/thesis I might have agreed with, but it matters a lot not to mislead people about the details.\"*\n\n\"Group epistemic norms\" includes both how individuals reason, and how they present ideas to a larger group for deliberation. \n\nComplaint #1 emphasizes culpability about dishonesty (by exaggeration). I agree that'd be a big deal. But, this is just really clearly false. Whatever else you think, its pretty clear from loads of consistent writing that Eliezer and Nate do just literally believe the title, and earnestly think it's important.\n\nComplaint #2  emphasizes culpability in terms of \"knowingly bad reasoning mistakes.\" i.e, \"Eliezer/Nate made reasoning mistakes that led them to this position, it's pretty obvious that those are reasoning mistakes, and people should be held accountable for major media campaigns based on obvious mistakes like that.\" \n\nI do think it's sometimes important to criticize people for something like that. But, not this time, because I don't think they made obvious reasoning mistakes.\n\nI have the most sympathy for Complaint #3. I agree there's a memetic bias towards sensationalism in outreach. (Although there are also major biases towards \"normalcy\" / \"we're gonna be okay\" / \"we don't need to change anything major\". One could argue about which bias is stronger, but mostly I think they're both important to model separately).\n\nIt does suck if you think something false is propagating. If you think that, seems good to write up what you think is true and argue about it.[^wv7soe0rsb]\n\nIf people-more-optimistic-than-me turn out to be right about some things, I'd agree the book and title may have been a mistake. \n\nAlso, I totally agree that Eliezer/Nate do have some patterns that are worth complaining about on group epistemic grounds, that *aren't* the contents of the book. But, that's not a problem with the book.\n\nI think it'd be great for someone who earnestly believes \"If anyone builds it, everyone probably dies but it's hard to know\" to publicly argue for that instead.\n\nI. Reasons the \"Everyone Dies\" thesis is reasonable\n===================================================\n\nWhat the book does and doesn't say\n----------------------------------\n\nThe book says, confidently, that:\n\n> **If any company or group, anywhere on the planet, builds an artificial superintelligence using anything remotely like current techniques, based on anything remotely like the present understanding of AI, then everyone, everywhere on Earth, will die.**\n\nThe book does *not* claim confidently that AI will come soon, or shaped any particular way. (It does make some guesses about what is likely, but, those are guesses and the book is pretty clear about the difference in epistemic status).\n\nThe book doesn't say you *can't* build something that's *not* \"It\", that is useful in some ways. (It specifically expresses some hope in using narrow biomedical-AI to solve various problems).\n\nThe book says *if* you build it, everyone dies.\n\n\"It\" means AI that is actually smart enough to confidently defeat humanity. This can include, \"somewhat powerful, but with enough strategic awareness to maneuver into more power without getting caught.\" (Which is particularly easy if people just straightforwardly keep deploying AIs as they scale them up).\n\nThe book is slightly unclear about what \"based on current techniques\" means (which feels like a fair complaint). But, I think it's fairly obvious that they mean the class of AI training that is \"grown\" more than \"crafted\" – i.e. any techniques that involve a lot of opaque training, where you can't make at least a decently confident guess about how powerful the next training run will turn out, and how it'll handle various edge cases.\n\nDo you think interpretability could advance to where we can make reasonably confident predictions about what the next generation would do? Cool. (I'm more skeptical it'll happen fast enough, but, it's not a disagreement with the core thesis of the book, since it'd change the *\"based on anything like **today's understanding** of AI\"* clause)[^klxoaztaa2r]\n\nDo you think it's possible to control somewhat-strong-AI with a variety of techniques that make it *less* likely that it *would* be able to take over all humanity? I think there is *some* kind of potential major disagreement somewhere around here (see below), but it's not *automatically* a disagreement. \n\nDo you think there will be at least one company that's actually sufficiently careful as we approach more dangerous levels of AI, with enough organizational awareness to (probably) stop when they get to a run more dangerous than they know how to handle? Cool. I'm skeptical about that too. And this one might lead to disagreement with the book's secondary thesis of \"And therefore, Shut It Down,\" but, it's not (necessarily) a disagreement with *\"\\*If\\* someone built AI powerful enough to destroy humanity based on AI that is grown in unpredictable ways with similar-to-current understanding of AI, then everyone will die.\"*\n\nThe book is making a (relatively) narrow claim. \n\nYou might *still* disagree with that claim. I think there are valid reasons to disagree, or at least assign significantly less confidence to the claim. \n\nBut none of the reasons listed so far are disagreements with the thesis. And, remember, if the reason you disagree is because you think our understanding of AI will improve dramatically, or there will be a paradigm shift specifically away from \"unpredictably grown\" AI, this also isn't actually a disagreement with the sentence.\n\nI think a pretty reasonable variation on the above is \"Look, I agree we need more understanding of AI to safely align a superintelligence, and better paradigms. But, I don't expect to agree with Eliezer on the *specifics* of how much more understanding we need, when we get into the nuts and bolts. And I expect a lot of progress on those fronts by default, which changes my relationship to the secondary thesis of 'and therefore, shut it all down.\" But, I think it makes more sense to characterize this as \"disagree with the main thesis by degree, but not in overall thrust).\n\nI also think a lot of people just don't really believe in AI that is smart enough to outmaneuver all humanity. I think they're wrong. But, if you don't really believe in this, and think the book title is false, I... roll to disbelieve on you actually really simulating the world where there's an AI powerful enough to outmaneuver humanity?\n\nThe claims are presented reasonably\n-----------------------------------\n\nA complaint I have about Realtime Conversation Eliezer, or Comment-Thread Eliezer, is that he often talks forcefully, unwilling to change frames, with a tone of \"I'm talking to idiots\", and visibly not particularly listening to any nuanced arguments anyone is trying to make. \n\nBut, I don't have that sort of complaint about this book. \n\nSomething I like about the book is it lays out disjunctive arguments, like “we think ultimately, a naively developed superintelligence would want to kill everyone, for reasons A, B, C and D. Maybe you don’t buy reasons B, C and D. But that still leaves you with A, and here’s are argument that although Reason A might not lead literally everyone dying, the expected outcome is still something horrifying.”\n\n(An example of that was: For “might the AI keep us as pets?”, the book answers (paraphrased) “We don’t think so. But, even if they did… note that, while humans keep dogs as pets, we don’t keep *wolves* as pets. Look at the transform from wolf to dog. An AI might keep us as pets, but, if that’s your hope, imagine the transform from Wolves-to-Dogs and equivalent transforms on humans.”) \n\nSimilarly, I like that in the AI Takeoff scenario, there are several instances where it walks through \"Here are several different things the AI could try to do next. You might imagine that some of them aren't possible, because the humans are doing X/Y/Z. Okay, let's assume X/Y/Z rule out options 1/2/3. But, that leaves options 4/5/6. Which of them does the AI do? Probably all of them, and then sees which one works best.\"\n\n**Reminder: All possible views of the future are wild.**\n\n[@Scott Alexander](https://www.lesswrong.com/users/scottalexander?mention=user) described the AI Takeoff story thus:\n\n> It doesn’t just sound like sci-fi \\[specifically compared to \"hard sci fi\"\\]; it sounds like unnecessarily dramatic sci-fi. I’m not sure how much of this is a literary failure vs. different assumptions on the part of the authors.\" \n\nI... really don't know what Scott expected a story that featured actual superintelligence to look like. I think the authors bent over backwards giving us one of the least-sci-fi stories you could possibly tell that includes superintelligence doing anything at all, without resorting to \"superintelligence just won't ever exist.\" \n\nEliezer and Nate make sure the takeover scenario doesn't depend on technologies that we don't have some existing examples of. The amount of \"fast takeoff\" seems like the amount of scaleup you'd expect if the graphs just kept going up the way they're currently going up, by approximately the same mechanisms they currently go up (i.e. some algorithmic improvements, some scaling). \n\nSure, Galvanic would first run Sable on smaller amounts of compute. And... then they will run it on larger amounts of compute (and as I understand it, it'd be a new, surprising fact if they limited themselves to scaling up slowly/linearly rather than by a noticeable multiplier or order-of-magnitude. If I am wrong about current lab practices here, please link me some evidence).\n\nIf this story feels crazy to you, I want to remind you that [all possible views of the future are wild](https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/). Either some exponential graphs suddenly stop for unclear reasons, or some exponential graphs keep going and batshit crazy stuff can happen that your intuitions are not prepared for. You can believe option A if you want, but, it's not like \"the exponential graphs that have been consistent over hundreds of years suddenly stop\" is a viewpoint that you can safely point to as a \"moderate\" and claim to give the other guy burden of proof.\n\nYou don't have the luxury of being the sort of moderate who doesn't have to believe something pretty crazy sounding here, one way or another. \n\n(If you haven't yet read the [Holden post on Wildness](https://www.cold-takes.com/all-possible-views-about-humanitys-future-are-wild/), I ask you do so before arguing with this. It's pretty short and also fun to read fwiw)\n\n**The Online Resources spell out the epistemic status more clearly.**\n\nIn the FAQ question, \"[So there's at least a chance of the AI keeping us alive](https://ifanyonebuildsit.com/5/so-theres-at-least-a-chance-of-ai-keeping-us-alive)?\", they state more explicitly:\n\n> It’s overwhelmingly more likely that \\[superintelligent\\] AI kills everyone.\n> \n> In these online resources, we’re willing to engage with a pretty wide variety of weird and unlikely scenarios, for the sake of spelling out why we think they’re unlikely and why (in most cases) they would still be catastrophically bad outcomes for humanity.\n> \n> We don’t think that these niche scenarios should distract from the headline, however. The most likely outcome, if we rush into creating smarter-than-human AI, is that the AI consumes the Earth for resources in pursuit of some end, wiping out humanity in the process.\n> \n> The book title isn’t intended to communicate complete certitude. We mean the book title in the manner of someone who sees a friend lifting a vial of poison to their lips and shouts, “Don’t drink that! You’ll die!”\n> \n> Yes, it’s technicallypossible that you’ll get rushed to the hospital and that a genius doctor mightconcoct an unprecedented miracle cure that merely leaves you paralyzed from the neck down. We’re not saying there’s no possibility of miracles. But if even the miracles don’t lead to especially good outcomes, then it seems even clearer that we *shouldn’t drink the poison*.\n\n**The book doesn't actually overextend the arguments and common discourse norms.**\n\nThis adds up to seeming to me that:\n\n*   The book makes a reasonable case for why Eliezer and Nate are personally pretty confident in the title.\n*   The book, I think, does a decent job giving you some space to think “well, I don’t buy that particular argument.\"\n*   The book acknowledges “if you don’t buy some of these arguments, yeah, maybe everyone might not literally die and maybe the AI might care about humans in some way, but we still think it's very unlikely to care about humans in a way that should be comforting.\"\n\nIf a book in the 50s was called \"Nuclear War would kill us all\", I think that book would have been incorrect (based of my most recent read of [Nuclear war is unlikely to cause human extinction](https://www.lesswrong.com/posts/sT6NxFxso6Z9xjS7o/nuclear-war-is-unlikely-to-cause-human-extinction)), but I wouldn't think the authors were unreasonable for arguing it, especially if they pointed out things like \"and yeah, if our models of nuclear winter are wrong, everyone wouldn't literally die, but civilization would still be pretty fucked\", and I would think the people giving the authors a hard time about it were being obnoxious pedants, not heroes of epistemic virtue.\n\n(I would think people arguing \"but, the nuclear winter models *are* wrong, so, yeah, we're more in the 'civilization would be fucked' world than the 'everyone literally dies world.\" would be doing a good valuable service. But I wouldn't think it'd really change the takeaways very much).\n\nII. Specific points to maybe disagree on\n========================================\n\nThere are some opinions that seem like plausible opinions to hold, given humanity's current level of knowledge, that lead to actual disagreement with \"If anyone builds \\[an AI smart enough to outmanuever humanity\\] \\[that is grown in unpredictable ways\\] \\[based on approximately our current understanding of AI\\]\".\n\nAnd the book does have a secondary thesis of \"And therefore, Shut It Down\", and you can disagree with that separately from \"If anyone builds it, everyone dies.\"\n\nRight now, the arguments that I've heard sophisticated enough versions of to seem worth acknowledging include:\n\n1.  Very slightly nice AIs would find being nice cheap.\n    *   (argument against \"everyone literally dies.\")\n2.  AI-assisted alignment is reasonably likely to work. Misuse or dumber-AI-run-amuck is likely enough to be comparably bad to superintelligence. And it's meanwhile easier to coordinate now with smaller actors. So, we should roll the dice now rather than try for a pause.\n    *   (argument against \"Shut It (completely) Down\")\n3.  We can get a lot of very useful narrow-ish work out of somewhat-more-advanced-models that'll help us learn enough to make significant progress on alignment.\n    1.  (argument against \"Shut It Down (now)\")\n4.  We can keep finding ways to increase the cost of taking over humanity. There's no boolean between \"superintelligent enough to outthinking humanity\" and \"not\", and this is a broken frame that is preventing you from noticing alternative strategies.\n    *   (argument against \"It\" being the right concept to use)\n\nI disagree with the first two being very meaningful (as counterarguments to the book). More on that in a sec.\n\nArgument #3 is somewhat interesting, but, given that it'd take years to get a successful Global Moratorium, I don't see any reason not to *start pushing for* a long global pause now.\n\nI think the fourth one is fairly interesting. While I strongly disagree with some major assumptions in the Redwood Plan as I understand it, various flavors of \"leverage narrow / medium-strength controlled AIs to buy time\" feel like they might be an important piece of the gameboard. Insofar as Argument #3 helped Buck step outside the MIRI frame and invent Control, and insofar as that helps buy time, yep, seems important.\n\nThis is complicated by \"there is a giant Cope Memeplex that *really* doesn't want to have to slow down or worry too much\", so while I agree it's good to be able to step outside the Yudkowsky frame, I think most people doing it are way more likely to end up [slipping out of reality](https://www.lesswrong.com/posts/m6dLwGbAGtAYMHsda/epistemic-slipperiness-1) and believing nonsense than getting anywhere helpful.\n\nI won't get into *that* much detail about either topic, since that'd pretty much be a post to itself. But, I'll link to some of the [IABED Online Resources](https://ifanyonebuildsit.com/resources), and share some quick notes about why I disagree that even the sophisticated versions of these so far don't seem very useful arguments to me.\n\nOn the meta-level: It currently feels plausible to me to have some interesting disagreements with the book here, but I don't see any interesting disagreements that add up to \"Eliezer/Nate particularly fucked up epistemically or communicatively\" or \"you shouldn't basically hope the book succeeds at its goal.\"\n\nNotes on Niceness\n-----------------\n\nThere are some flavors of \"AI might be slightly nice\" that are interesting. But, they don't seem like it changes any of our decisions. It just makes us a bit more hopeful about the end result.\n\nGiven the [counteraguments](https://ifanyonebuildsit.com/5/wont-ais-care-at-least-a-little-about-humans), I don't see a reason to think this more than single-digit-percent likely to be especially relevant. (I can see >9% likelihood the AIs are \"nice enough that something interesting-ish happens\" but not >9% likelihood that we shouldn't think the outcome is still extremely bad. The people who think otherwise seem extremely motivatedly-cope-y to me).\n\nNote also that it's *very* expensive for the AI to not boil the oceans / etc as fast as possible, since that means losing a many galaxies worth of resources, so it seems like it's not enough to be \"very slightly\" nice – it has to be, like, pretty actively nice.\n\nWhich plan is Least Impossible?\n-------------------------------\n\nA lot of x-risk disagreements boil down to \"which pretty impossible-seeming thing is only actually Very Hard instead of Impossibly Hard.\"\n\nThere's an argument I *haven't* heard a sophisticated version of, which is \"there's no way you're getting a Global Pause.\"\n\nI certainly believe that this is an extremely difficult goal, and a lot of major things would need to change in order for it to happen. I haven't heard any real argument we should think it's more impossible than, say, Trump winning the presidency and going on to do various Trumpy things. \n\n*(Please don't get into arguing about Trump in the comments. I'm hoping that whatever you think of Trump, you agree he's doing a bunch of stuff most people would previously have probably expected to be outside the overton window. If this turns out to be an important substantive disagreement I'll make a separate container post for it)*\n\nMeanwhile, the counter-impossible-thing I've heard several people putting hope on is \"We can run a *lot* of controlled AIs, where (first) we have them do fairly straightforward automation of not-that-complex empirical work, which helps us get to a point where we trust them enough to give them more openended research tasks.\"\n\nThen, we run a *lot* of those real fast, such that they substantial increase the total amount of alignment-research-months happening during a not-very-long-slowdown.\n\nThe arguments for why this is extremely dangerous, from the book and online resources and maybe some past writing, are, recapped:\n\n**There's no good training data. **\n\nWe don't even know how to verify alignment work is particular useful *among humans,* let alone in an automatedly gradable way. \n\n**Goal Directedness is pernicious. Corrigibility is anti-natural.**\n\nThe way an AI would develop the ability to think extended, useful creative research thoughts that you might fully outsource to, is via becoming perniciously goal directed. You can't do months or years of openended research without fractally noticing subproblems, figuring out new goals, and relentless finding new approaches to tackle them.  \n  \nOnce you do that, it's a fact of the universe, that the programmers can't change, that \"you'd do better at these goals if you didn't have to be fully obedient\", and while programmers can install various safeguards, those safeguards are pumping upstream and will have to pump harder and harder as the AI gets more intelligent. And if you want it to make at least as much progress as a decent AI researcher, it needs to be quite smart.\n\n**Security is very difficult**\n\nThe surface area of ways an AI can escape and maneuver are enormous. (I think it's *plausible* to have a smallish number of carefully controlled, semi-powerful AIs if you are paying a lot of attention. The place I completely get off the train is where you then try to get *lots* of subjective hours of research time out of thousands of models).\n\n**Alignment is among the most dangerous tasks**\n\n\"Thinking about how to align AIs\" requires both for the AI to think how \"how would I make smarter version of myself\" and \"how would I make it aligned to humans?\". The former skillset  directly helps them recursively self-improve. The latter skillset helps them manipulate humans.\n\n**MIRI did make a pretty substantive try.**\n\nOne of the more useful lines for me, in the [Online Resources](https://ifanyonebuildsit.com/resources), in their [extended discussions about corrigibility](https://ifanyonebuildsit.com/11/shutdown-buttons-and-corrigibility).\n\n> We ran some workshops, and the workshops had various mathematicians of various stripes (including an International Mathematical Olympiad gold medalist), but nobody came up with a really good idea.\n> \n> This does not mean that the territory has been exhausted. Earth has not come remotely near to going as hard on this problem as it has gone on, say, string theory, nor offered anything like the seven-digit salaries on offer for advancing AI capabilities.\n> \n> But we learned something from the exercise. We learned not just about the problem itself, but also about how hard it was to get outside grantmakers or journal editors to be able to *understand what the problem was*. A surprising number of people saw simple mathematical puzzles and said, “They expect AI to be simple and mathematical,” and failed to see the underlying point that it is [hard to injure an AI’s steering abilities](https://ifanyonebuildsit.com/3/smart-ais-spot-lies-and-opportunities#deep-machinery-of-steering)*,* just like how it’s [hard to injure its probabilities](https://ifanyonebuildsit.com/3/smart-ais-spot-lies-and-opportunities#deep-machinery-of-prediction).\n> \n> If there were a natural shape for AIs that let you fix mistakes you made along the way, you might hope to find a simple mathematical reflection of that shape in toy models. All the difficulties that crop up in every corner when working with toy models are suggestive of difficulties that will crop up in real life; all the extra complications in the real world don’t make the problem *easier.*\n\nThere was a related quote I can't find now, that maybe was just in an earlier draft of the Online Resources, to the effect of \"this \\[our process of attempting to solve corrigibility\\] is the real reason we have this much confidence about this being quite hard and our current understanding not being anywhere near adequate.\" \n\n(Fwiw I think it is a mistake that this isn't at least briefly mentioned in the book. The actual details would go over most people's heads, but, having any kind of pointer to \"why are these guys so damn confident?\" seems like it'd be quite useful)\n\nIII. Overton Smashing, and Hope\n===============================\n\nOr: \"Why is this book really important, not just 'reasonable?'\"\n---------------------------------------------------------------\n\nI, personally, [believe in](https://www.lesswrong.com/posts/duvzdffTzL3dWJcxn/believing-in-1) this book. [^kkx1zmo7gdo]\n\nIf you don't already believe in it, you're probably not going to because of my intuitions here. But, I want to say why it's deeply important to me that the book is reasonable, not just arguing on the internet because I'm triggered and annoyed about some stuff.\n\n**I believe in the book partly because it looks like it might work.** \n\nThe number (and hit-rate) of NatSec endorsements surprised me. More recently some senators seem to have been bringing up existential risk of their own initiative. When I showed the website to a (non-rationalist) friend who lives near DC and has previously worked for think-tank-ish org, I expected them to have a knee-jerk reaction of ‘man that’s weird and a bit cringe’, or ‘I’d be somewhat embarrassed to share this website with colleagues’, and instead they just looked worried and said “okay, I’m worried”, and we had a fairly matter-of-fact conversation about it.\n\nIt feels like the world is waking up to AI, and is aware that it is some kind of big deal that they don’t understand, and that there’s something unsettling about it. \n\nI think the world is ready for this book.\n\n**I also believe in the book because, honestly, the entire rest of the AI safety community’s output just does not feel adequate to me to the task of ensuring AI goes well. **\n\nI’m personally only like 60% on “if anyone built It, everyone would die.” But I’m like 80% on “if anyone built It, the results would be unrecoverably catastrophic,” and the remaining 20% is a mix of model uncertainty and luck. Nobody has produced counterarguments that feel compelling, just \"maybe something else will happen?\", and the way people choose their words almost always suggests some kind of confusion or cope.\n\nThe plans that people propose mostly do not seem to be counter-arguing the actual difficult parts of the problem. \n\nThe book gives me more hope than anything else has in the past few years. \n\n**Overton Smashing is a thing. I really want at least** ***some*** **people trying.**\n\nIt’s easy to have the idea “try to change the Overton window.” Unfortunately, changing the Overton window is very difficult. It would be hard for most people to pull it off. I think it helps to have a mix of conviction backed by deep models, and some existing notoriety. There are only a few other people who seem to me like they might be able to pull it off. (It'd be cool if at least one of Bengio, Hinton, Hassabis or Amodei end up trying. I think Buck actually might do a good job if he tried.)\n\nSmashing an overton window does not look like \"say the careful measured thing, but, a bit louder/stronger.\" Trying to do it halfway won't work. But going all in with conviction and style, seems like it does work. It looks like Bengio, Hinton, Hassabis and Amodei are each trying to do some kind of measured/careful strategy, and it's salient that if they shifted a bit, things would get worse instead of better. \n\n(Sigh... I think I might need to talk about Trump again. This time it seems more centrally relevant to talk about in the comments. But, like, dude, look at how bulletproof the guy seems to be. He also, like, says falsehoods a lot and I'm not suggesting emulating him-in-particular, but I point to him as an existence proof of what can work)\n\nPeople keep asking \"why can't Eliezer tone it down.\" I don't think Eliezer is the best possible spokesperson. I acknowledge some downside risk to him going on a major media campaign. But I think people are very confused about how loadbearing the things some people find irritating are. How many fields and subcultures have you founded, man? Fields and subcultures and major new political directions are not founded (generally) by people without some significant fraction of haters.\n\nYou can't file off *all* the edges, and still have anything left that works. You can only reroll on which combination of inspiring and irritating things you're working with.\n\nI want there to be more people who competently execute on \"overton smash.\" The next successful person would probably look pretty different from Eliezer, because part of overton smashing is having a unique style backed by deep models and taste and each person's taste/style/models are pretty unique. It'd be great to have people with more diversity of \"ways they are inspiring and also grating.\"\n\nMeanwhile, we have this book. It's the Yudkowsky version of the book. If you don't like that, find someone who actually could write a better one. (Or, rather, find someone who could execute on a successful overton smashing strategy, which would probably look pretty different than a book since there already is a book, but would still look and feel pretty extreme in *some* way).\n\n**Would it have been better to use a title that fewer people would feel the need to disclaim?**\n\nI think Eliezer and Nate are basically correct to believe the overwhelming likelihood *if* someone built \"It\" would be everyone dying. \n\nStill, maybe they should have written a book with a title that more people around these parts wouldn't feel the need to disclaim, and that the entire x-risk community could have enthusiastically gotten behind. I think they should have at least considered that. Something more like \"If anyone builds it, everyone loses.\" (that title doesn't quite work, but, you know, something like that)\n\nMy own answer is \"maybe\" - I see the upside. I want to note some of the downsides or counter-considerations. \n\n*(Note: I'm specifically considering this from within the epistemic state of \"if you did pretty confidently believe everyone would literally die, and that if they didn't literally die, the thing that happened instead would be catastrophically bad for most people's values and astronomically bad from Eliezer/Nate's values)*\n\nCounter-considerations include:\n\nAFAICT, Eliezer and Nate spent like ~8 years deliberately backing off and toning tone, out of a vague deferral to people saying \"guys you suck at PR and being the public faces of this movement.\" The result of this was (from their perspective) \"EA gets co-opted by OpenAI, which launches a race that dramatically increases the danger the world faces.\"\n\nSo, the background context here is that they have tried more epistemic-prisoner's-dilemma-cooperative-ish strategies, and they haven't worked well. \n\nAlso, it seems like there's a large industrial complex of people arguing for various flavors of \"things are pretty safe\", and there's barely anyone at all stating plainly \"IABED\". MIRI's overall strategy right now is to speak plainly about what they believe, both because they think it needs to be said and no one else is saying it, and because they hope just straightforwardly saying what they believe will net a reputation for candor that you don't get if people get a whiff of you trying to modulate your beliefs based on public perception.\n\nNone of that is an argument that they should exaggerate or lean-extra-into beliefs that they don't endorse. But, given that they *are* confident about it, it's an argument not to go out of their way to try to say something else.\n\n**I don't currently buy that it** ***costs*** **much to have this book asking for total shutdown.**\n\nMy sense is it's pretty common for political groups to have an extreme wing and a less extreme wing, and for them to be synergistic. Good cop, bad cop. Martin Luther King and Malcolm X. \n\nIf what you want is some kind of global coordination that *isn't* a total shutdown, I think it's still probably better to have Yudkowsky over there saying \"shut it all down\" and say \"Well, I dunno about that guy. I don't think we need to shut it all down, but I do think we want some serious coordination.\"\n\n**I believe in the book.**\n\nPlease buy a copy if you haven't yet. \n\nPlease tell your friends about it. \n\nAnd, disagree where appropriate, but, please don't give it a hard time for lame pedantic reasons, or jump to assuming you disagree because you don't like something about the vibe. ~Please don't awkwardly distance yourself because it didn't end up saying exactly the things you would have said, unless it's actually fucking important.~ *(I endorse something close to this but the nuances matter a lot and I wrote this at 5am and don't stand by it enough for it to be the closing sentence of this post)*  \n  \n[You can buy the book here.](https://www.amazon.com/gp/product/0316595640)\n\n[^wv7soe0rsb]: (edit in response to Rohin's comment: It additionally sucks that writing up what's true and arguing for it is penalized in the game against sensationalism. I don't think it's so penalized it's not worth doing, though) \n\n[^klxoaztaa2r]: Paul Christiano and Buck both complain about (paraphrased) \"Eliezer equivocates between 'we have to get it right on the first critical try' and 'we can't learn anything important before the first critical try.'\" I agree something-in-this-space feels like a fair complaint, especially in combination with Eliezer not engaging that much with the more thoughtful critics, and tending to talk down to them in a way that doesn't seem to be really listening to the nuances they're trying to point to and round them to nearest strawman of themselves. II think this is a super valid thing to complain about Eliezer. But, it's not the title or thesis of the book. (because, if we survive because we learned useful things, I'd say that doesn't count as \"anywhere near our current understanding\"). \n\n[^kkx1zmo7gdo]: \"Believing in\" doesn't mean \"assign >50% chance to working\", it means \"assign enough chance (~20%?) that it feels worth investing substantially in and coordinating around.\" See Believing In by Anna Salamon.",
      "plaintextDescription": "Alt title: \"I don't believe you that you actually disagree particularly with the core thesis of the book, if you pay attention to what it actually says.\"\n \n\nI'm annoyed by various people who seem to be complaining about the book title being \"unreasonable\". i.e. who don't merely disagree with the title of \"If Anyone Builds It, Everyone Dies\", but, think something like: \"Eliezer/Nate violated a Group-Epistemic-Norm.\" \n\nI think the title is reasonable. \n\nI think the title is probably true. I'm less confident than Eliezer/Nate, but I don't think it's unreasonable for them to be confident in it given their epistemic state. So I want to defend several decisions about the book I think were: \n\n 1. Actually pretty reasonable from a meta-group-epistemics/comms perspective\n 2. Very important to do.\n\nI've heard different things from different people and maybe am drawing a cluster where there is none, but, some things I've heard:\n\nComplaint #1: \"They really shouldn't have exaggerated the situation like this.\"\n\nComplaint #2: \"Eliezer and Nate are crazy overconfident, and it's going to cost them/us credibility.\"\n\nComplaint #3: \"It sucks that the people with the visible views are going to be more extreme, eye-catching and simplistic. There's a nearby title/thesis I might have agreed with, but it matters a lot not to mislead people about the details.\"\n\n\"Group epistemic norms\" includes both how individuals reason, and how they present ideas to a larger group for deliberation. \n\nComplaint #1 emphasizes culpability about dishonesty (by exaggeration). I agree that'd be a big deal. But, this is just really clearly false. Whatever else you think, its pretty clear from loads of consistent writing that Eliezer and Nate do just literally believe the title, and earnestly think it's important.\n\nComplaint #2 emphasizes culpability in terms of \"knowingly bad reasoning mistakes.\" i.e, \"Eliezer/Nate made reasoning mistakes that led them to this position, it's pretty obvious that those are reasonin",
      "wordCount": 5395
    },
    "tags": [
      {
        "_id": "sxC9YqS2t9TGhc4oD",
        "name": "IABIED",
        "slug": "iabied-1"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "mve2bunf6YfTeiAvd",
    "title": "Meetup Month",
    "slug": "meetup-month-1",
    "url": null,
    "baseScore": 45,
    "voteCount": 18,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2025-09-17T21:10:52.318Z",
    "contents": {
      "markdown": "It’s meetup month! If you’ve been vaguely thinking of getting involved with a some kind of rationalsphere in-person community stuff, now is a great time to do that, because lots of other people are doing that!\n\nIt’s the usual time of the year for [Astral Codex Everywhere](https://www.lesswrong.com/posts/6umEbXvotXicRPvGs/meetups-everywhere-2025-times-and-places) – if you’re the sorta folk who likes to read Scott Alexander, and likes other people who like Scott Alexander, but only really can summon the wherewithal to go out to a meetup once a year, this is the Official Coordinated Schelling Time to do that. There are meetups scheduled in 180 cities. Probably one of those is near you!\n\nThis year, we have two other specific types of meetups it seemed good to coordinate around: Celebrating [Petrov Day](https://www.lesswrong.com/posts/QtyKq4BDyuJ3tysoK/9-26-is-petrov-day), and reading groups for the recently released [If Anyone Builds It, Everyone Dies](https://www.lesswrong.com/posts/fnJwaz7LxZ2LJvApm/if-anyone-builds-it-everyone-dies-release-day).\n\nPetrov Day\n==========\n\n[September 26th is Petrov Day](https://www.lesswrong.com/posts/QtyKq4BDyuJ3tysoK/9-26-is-petrov-day) – a day when humanity came close to the brink of nuclear war. Stanislav Petrov worked in the Soviet military, and received an alert indicating a nuclear attack. But the information was suspicious – the alert reported only five warheads incoming. His orders were forward the report of a nuclear attack to his superiors, who may likely have retaliated and initiated a large scale nuclear war.\n\nInstead, he reported it as a false alarm.\n\n(It was, indeed, a false alarm, possibly triggered by a flock of birds)\n\nOn LessWrong, Petrov Day has come to be celebrated as a holiday about existential risk more generally. There have been [many ways of celebrating Petrov Day](https://www.lesswrong.com/posts/XJxwFMSL5TPN2usC6/modes-of-petrov-day). But the version I’ve found pretty meaningful as a holiday ritual is Jim Babcock’s hourlong ceremony, designed for 6-12 people. (If you have more people than that, he recommends splitting up into multiple subgroups who each fit around a table).\n\nIt involves printing out some booklets and some candles, and taking turns reading through the story of Petrov Day (along with context from throughout human history). \n\nIn previous years, Jim and/or I have one a mad last-minute scramble to remind people and try to buy candles. (Note: there’s a [bunch of ways you can screw up ritual candles](https://www.lesswrong.com/posts/tdLzqMueEp8zkqxfF/on-not-screwing-up-ritual-candles)). \n\nThis year, we thought “let’s try to help people think about this more than 2 days in advance.”\n\nIf you want to host a Petrov Day ceremony, you’ll want some supplies. There ceremony involves 8 candles. Two of them don’t actually get lit, and represent futures where humanity flourishes or extinguishes itself. \n\nFor a mix of aesthetics/practicality, I recommend getting [these candles](https://www.amazon.com/dp/B01AVEODA2?th=1), with [these candle holders](http://amazon.com/Hosley-Candle-Holders-Weddings-Meditation/dp/B085GLYYCB). *(I personally like having two fancier candles representing the flourishing and extinction futures, such as this *[*one for flourishing*](http://amazon.com/Mister-Candle-Pillar-Unscented-Poured/dp/B01N7OWYT1) *and*[*this one for extinction*](https://www.amazon.com/Skeleton-Head-Sculpture-Halloween-Manifestation/dp/B0D3JDHPTY?th=1)*)*\n\nYou’ll also need to print out a 8-12 copies of [this booklet](http://petrovday.com/downloads/PetrovDay-DoubleSidedBooklet.pdf) for readings[^qhqps65ei4m]. *(I recommend finding a local Fedex or similar, to print out several copies, to simplify the process.)*\n\n[Click here to create a Petrov Day event](https://www.lesswrong.com/newPost?eventForm=true&PETROV=true) for the frontpage map.\n\n*If Anyone Builds It* reading groups\n====================================\n\nMIRI’s new book launches this week. It’s particularly valuable if people buy copies of it by Sep 20th (to make it more likely to appear on bestseller lists, which in turn make it more likely to get press and get into the mainstream consciousness).\n\nThe book is a (relatively) succinct articulation of the core arguments for AI being likely to destroy humanity. It’s a pretty big topic to think through, and it seemed valuable to encourage public (or semi-public) reading groups where people can read and discuss it together.\n\nSome considerations:\n\n*   The book is short enough it might make sense to have a single meetup for discussing it, but, you might also consider holding multiple meetups that discuss a given chapter, or, group of chapters.\n*   If you’re discussing one chapter per session, you might consider explicitly scheduling an optional opening hour for reading the chapter, for people who’d have trouble making time but would enjoy reading it quietly with friends.\n\n[Click here to create a lesswrong event for an If Anyone Builds It reading group](https://www.lesswrong.com/newPost?eventForm=true&IFANYONE=true) for our frontpage meetup map.\n\nYou can [buy copies for your group here](https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640).\n\n*Note, if you would like some support getting your reading group running (i.e. suggested discussion questions, and potentially financial help buying the books) you can *[*fill out this form*](https://airtable.com/appgM36VHCg9MDEU3/shr4mK6ihTss27kzI)*.*\n\n* * *\n\nMeetups will show up on our home page map, for people viewing the site on desktop:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d8e4f89858b32c6d1a9eb93d381a55652d5e6c74563589be.png)\n\nAnd, of course, if you aren’t particularly interested in any of those things but just want to (re)connect with your local LessWrong meetup for whatever events they’re currently hosting, you can [view our usual community map filter for the “LW” events](https://www.lesswrong.com/community?filters=LW).\n\nHappy meeting up, happy book launch, and happy Petrov Day. :)\n\n[^qhqps65ei4m]: Note: This version of it is optimized for reading, or printing single-sided, rather than printing a doublesided booklet, if you want to look it over.",
      "plaintextDescription": "It’s meetup month! If you’ve been vaguely thinking of getting involved with a some kind of rationalsphere in-person community stuff, now is a great time to do that, because lots of other people are doing that!\n\nIt’s the usual time of the year for Astral Codex Everywhere – if you’re the sorta folk who likes to read Scott Alexander, and likes other people who like Scott Alexander, but only really can summon the wherewithal to go out to a meetup once a year, this is the Official Coordinated Schelling Time to do that. There are meetups scheduled in 180 cities. Probably one of those is near you!\n\nThis year, we have two other specific types of meetups it seemed good to coordinate around: Celebrating Petrov Day, and reading groups for the recently released If Anyone Builds It, Everyone Dies.\n\n\nPetrov Day\nSeptember 26th is Petrov Day – a day when humanity came close to the brink of nuclear war. Stanislav Petrov worked in the Soviet military, and received an alert indicating a nuclear attack. But the information was suspicious – the alert reported only five warheads incoming. His orders were forward the report of a nuclear attack to his superiors, who may likely have retaliated and initiated a large scale nuclear war.\n\nInstead, he reported it as a false alarm.\n\n(It was, indeed, a false alarm, possibly triggered by a flock of birds)\n\nOn LessWrong, Petrov Day has come to be celebrated as a holiday about existential risk more generally. There have been many ways of celebrating Petrov Day. But the version I’ve found pretty meaningful as a holiday ritual is Jim Babcock’s hourlong ceremony, designed for 6-12 people. (If you have more people than that, he recommends splitting up into multiple subgroups who each fit around a table).\n\nIt involves printing out some booklets and some candles, and taking turns reading through the story of Petrov Day (along with context from throughout human history). \n\nIn previous years, Jim and/or I have one a mad last-minute scramble to remind people ",
      "wordCount": 792
    },
    "tags": [
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "4HEzArQNEkjKRaJZf",
    "title": "Salt Lake City reading group for If Anyone Builds It, Everyone Dies",
    "slug": "salt-lake-city-reading-group-for-if-anyone-builds-it",
    "url": null,
    "baseScore": 13,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-09-16T23:13:48.481Z",
    "contents": {
      "markdown": "This is the Salt Lake City community's reading group for [If Anyone Builds It, Everyone Dies](https://www.amazon.com/Anyone-Builds-Everyone-Dies-Superhuman/dp/0316595640).\n\n\\[posted by Raemon with permission from the group\\]",
      "plaintextDescription": "This is the Salt Lake City community's reading group for If Anyone Builds It, Everyone Dies.\n\n[posted by Raemon with permission from the group]",
      "wordCount": 24
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "vJrP7nbnJTwk4fcbk",
    "title": "Simulating the *rest* of the political disagreement",
    "slug": "simulating-the-rest-of-the-political-disagreement",
    "url": null,
    "baseScore": 121,
    "voteCount": 50,
    "viewCount": null,
    "commentCount": 16,
    "createdAt": null,
    "postedAt": "2025-09-02T22:06:55.304Z",
    "contents": {
      "markdown": "There's a mistake I made a couple times and didn't really internalize the lesson as fast as I'd like. Moreover, it wasn't even a failure to generalize, it was basically a failure to even have a single update stick about a single situation.\n\nThe particular example was me saying, roughly:\n\n> Look, I'm 60%+ on \"Alignment is quite hard, in a way that's unlikely to be solved without a 6+ year pause.\" I can imagine believing it was lower, but it feels crazy to me to think it's lower than like 15%. And at 15%, it's still horrendously irresponsible to solve AI takeoff via rushing forward and winging-it than \"everybody *stop*, and actually give yourselves time think.\" \n\nThe error mode here is something like \"I was imagining what I'd think if you slid this one belief slider from ~60%+ to 15%, without imagining all the other beliefs that would probably be different if I earnestly believed the 15%.\" \n\nThat error feels like a \"reasonable honest mistake.\"\n\nBut, the part where I was like \"C'mon guys, even if you only, like, sorta-kinda agreed with me on this point, you'd *still* obviously be part of my political coalition for a global halt that is able to last 10+ years, right?\"\n\n...that feels like a more pernicious, political error. A desire to live in the world where my political coalition has more power, and a bit of an attempt to incept others into thinking it's true.\n\n(This is an epistemic error, not *necessarily* a strategic error. Political coalitions are often won by people believing in them harder than it made sense to. But, given that I've *also* staked my macrostrategy on *\"LessWrong is a place for shared mapmaking, and putting a lot of effort to hold onto that even as the incentives push towards political manuevering,\"* I'd have to count it as a strategic error for me in this context)\n\nThe specific counterarguments I heard were:\n\n*   If \"Superalignment is real hard risk\" is only like 15%, you might have primary threat models that are pretty differently shaped, and be focusing your efforts on reducing risk in the other 85% of worlds.\n*   Relatedly, my phrasing made more sense if the goal was to cut risk down to something \"acceptable\" (like, <5%). You might think it's more useful to focus on strategies that are more likely to work, and which cut risk down from, say, 70% to 35%. (which does seem more plausible to me if I believed alignment wouldn't likely require 6+ year pauses to get right).\n\nNow I'm not arguing that those rejoinders are slam dunks. But, I hadn't thought of them when I was making the argument, and I don't currently have a strong counter-counterargument at the moment. Upon reflection, I can see a little slippery-graspy move I was doing where I was hoping to skip over the hard work of fully simulating another perspective and addressing all their points.\n\n(to spell out: the above arguments are specifically against \"if AI alignment is only 15% likely to difficult enough to require a substantial pause, you should \\[be angling a bit to either pause or at least preserve option-value to pause\". It's not an argument against alignment likely requiring a pause)\n\n...\n\nI do still overall think we need a long pause to have a decent chance of non-horrible things happening. And I still feel like something [epistemically slippery](https://lesswrong.com/posts/m6dLwGbAGtAYMHsda/epistemic-slipperiness-1) is going on in the worldviews of most people who are hopeful about survival in a world where companies continue mostly rushing towards superintelligence. \n\nBut, seems good for me to acknowledge when I did something epistemically slippery myself. In particular given that I think that epistemic-slipperiness is a fairly central problem in the public conversation about AI, and it'd probablyhelp to get better at public convos about it.\n\nThe Takeaways\n-------------\n\nNotice thoughts like \"anyone who even believes a weak version of My Thing should end up agreeing with my ultimate conclusion\", and hold them with at least a bit of skepticism. (The exact TAP probably depends a bit on the situation)\n\nMore generally, remember if that variation in belief often doesn't just turn on a single knob, if someone disagrees with one piece they probably disagreeabout a bunch of other pieces. Disagreements are more frustratingly fractal than you might hope.\n\n(See also: [\"You can't possibly succeed without \\[My Pet Issue\\]\"](https://www.lesswrong.com/posts/gxbJfvb5eejrGkA9w/you-can-t-possibly-succeed-without-my-pet-issue))\n\nAppendix: The prior arguments\n-----------------------------\n\nI first made this-sort-of-claim in a conversation with Zac Hatfield-Dodds that I'd later recount on [Anthropic, and taking \"technical philosophy\" more seriously](https://lesswrong.com/posts/7uTPrqZ3xQntwQgYz/anthropic-and-taking-technical-philosophy-more-seriously#Tying_together_in_). I don't think I actually made the error here exactly). But in the comments, Ryan Greenblatt replied with some counterarguments and I said \"oh, yeah that makes sense\", and later in [The Problem](https://lesswrong.com/posts/kgb58RL88YChkkBNf/the-problem?commentId=JNghTZpcQcG6fQr6D) I ended up running through the same loop with Buck.",
      "plaintextDescription": "There's a mistake I made a couple times and didn't really internalize the lesson as fast as I'd like. Moreover, it wasn't even a failure to generalize, it was basically a failure to even have a single update stick about a single situation.\n\nThe particular example was me saying, roughly:\n\n> Look, I'm 60%+ on \"Alignment is quite hard, in a way that's unlikely to be solved without a 6+ year pause.\" I can imagine believing it was lower, but it feels crazy to me to think it's lower than like 15%. And at 15%, it's still horrendously irresponsible to solve AI takeoff via rushing forward and winging-it than \"everybody stop, and actually give yourselves time think.\" \n\nThe error mode here is something like \"I was imagining what I'd think if you slid this one belief slider from ~60%+ to 15%, without imagining all the other beliefs that would probably be different if I earnestly believed the 15%.\" \n\nThat error feels like a \"reasonable honest mistake.\"\n\nBut, the part where I was like \"C'mon guys, even if you only, like, sorta-kinda agreed with me on this point, you'd still obviously be part of my political coalition for a global halt that is able to last 10+ years, right?\"\n\n...that feels like a more pernicious, political error. A desire to live in the world where my political coalition has more power, and a bit of an attempt to incept others into thinking it's true.\n\n(This is an epistemic error, not necessarily a strategic error. Political coalitions are often won by people believing in them harder than it made sense to. But, given that I've also staked my macrostrategy on \"LessWrong is a place for shared mapmaking, and putting a lot of effort to hold onto that even as the incentives push towards political manuevering,\" I'd have to count it as a strategic error for me in this context)\n\nThe specific counterarguments I heard were:\n\n * If \"Superalignment is real hard risk\" is only like 15%, you might have primary threat models that are pretty differently shaped, and be focusing you",
      "wordCount": 729
    },
    "tags": [
      {
        "_id": "wzgcQCrwKfETcBpR9",
        "name": "Disagreement",
        "slug": "disagreement"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "4mBaixwf4k8jk7fG4",
    "title": "Yudkowsky on \"Don't use p(doom)\"",
    "slug": "yudkowsky-on-don-t-use-p-doom",
    "url": null,
    "baseScore": 98,
    "voteCount": 52,
    "viewCount": null,
    "commentCount": 39,
    "createdAt": null,
    "postedAt": "2025-08-22T23:44:04.541Z",
    "contents": {
      "markdown": "For awhile, I kinda assumed Eliezer had basically coined the concept of p(Doom). Then I was surprised one day to hear him complaining about it being an antipattern he specifically thought was unhelpful and wished people would stop.\n\nHe noted: \"If you want to trade statements that will actually be informative about how you think things work, I'd suggest, \"What is the minimum necessary and sufficient policy that you think would prevent extinction?**\"**\n\nComplete text of the [corresponding X Thread](https://x.com/ESYudkowsky/status/1823530314087718982):\n\n> I spent two decades yelling at nearby people to stop trading their insane made-up \"AI timelines\" at parties.  Just as it seemed like I'd finally gotten them to listen, people invented \"p(doom)\" to trade around instead.  I think it fills the same psychological role.\n> \n> If you want to trade statements that will actually be informative about how you think things work, I'd suggest, \"What is the minimum necessary and sufficient policy that you think would prevent extinction?\"\n> \n> The idea of a \"p(doom)\" isn't quite as facially insane as \"AGI timelines\" as marker of personal identity, but \n> \n> (1) you want action-conditional doom, \n> \n> (2) people with the same numbers may have wildly different models, \n> \n> (3) these are pretty rough log-odds and it may do violence to your own mind to force itself to express its internal intuitions in those terms *which is why I don't go around forcing my mind to think in those terms myself,* \n> \n> (4) most people haven't had the elementary training in calibration and prediction markets that would be required for them to express this number meaningfully and you're demanding them to do it anyways, \n> \n> (5) the actual social role being played by this number is as some sort of weird astrological sign and *that's not going to help people think in an unpressured way* about the various underlying factual questions that ought finally and at the very end to sum to a guess about how reality goes.\n\nOrthonormal responds:\n\n> IMO \"p(doom)\" was a predictable outgrowth of the discussion kicked off by Death with Dignity, specifically saying you thought of success in remote log-odds. (I did find it a fair price to pay for the way the post made the discourse more serious, ironically for April Fools.)\n\nEliezer responds:\n\n> the post was about **fighting for changes in log-odds** and **at no point tried to give an absolute number**\n\n(There is some further back-and-forth about this)\n\nI kinda agree with Orthonormal (on this being a fairly natural outgrowth of Death with Dignity's ), although I think it's more generally downstream of \"be a culture that puts probabilities on things.\"\n\nI'm posting this partly because, I had vaguely been attributing \"p(Doom)\" discourse to Eliezer, and it seemed like maybe other people were too, and it seemed good to correct the record for anyone else who also thought that.\n\nI know at least a few other prominent x-risky thinkers who also think p(Doom) is a kind of bad way to compress worldviews. I'm posting this today because Neel Nanda recently [tweeted about](https://x.com/NeelNanda5/status/1949096131398652313) generally hating being asked for his p(Doom), noting a time an interviewer recently asked him about it and he replied:\n\n> I decline to answer that question because I am not particularly confident in my answer, and I think that the social norm of asking for off-the-cuff numbers falsely implies that they matter more than they do.\n\nQuick Takes\n-----------\n\nOriginally, I wanted to flesh out this post with some thinking about what people are trying to get out of \"saying your p(Doom)\", and how to best achieve that. Spelling that out nicely turned out to be a lot of work, but, it still seemed nice to crosspost this mini-essay. \n\nI guess I will include some takes without justifying them yet, and hash things out in the comments:\n\n**0\\. It's not nonzero useful to discuss p(Doom), the thing that is weird/bad is how much people fixate on it relative to other things.**\n\n**1\\. It's not useful to articulate** ***more precise*** **p(Doom) than you have a credibly calibrated belief about.**\n\n i.e. most people probably should be saying things more like \"idk somewhere between X and Y%?\" than \"Z%.\"\n\nI think it's better to give ranges than to just use words like \"probably?\" \"probably not\" \"very unlikely\", because not everyone means the same thing by those words, and having common units is helpful for avoiding misunderstandings.\n\n**2.** You should at least factor out **\"p(Doom) if we build superintelligence soon under present conditions using present techniques\"** vs \"**p(Doom) all-things-considered.\" **\n\n\"How hard is navigating superintelligence\" and \"how competently will humanity navigate superintelligence\" seem like fairly different questions.\n\nIn particular, this helps prevent p(Doom) being more like a measure of mood-affiliation of vibes than an actual prediction.\n\nI like Eliezer's alternate question of \"What is the minimum necessary and sufficient policy that you think would prevent extinction?\" (with \"nothing\" being an acceptable answer), but, it does seem noticeably harder to answer and at least one of the nice things about p(Doom) is you probably have an implicit thing you already believe. (It's also a much more opinionated framing)\n\n**3\\. It's useful to track some kind of consensus about p(Doom)-ish questions, but not for the reasons most people think.**\n\nIt's *not* good for leaning into tribal camps about how doomy to be.\n\nIt's also not good for figuring out what sort of ideas or questions are acceptable to talk about.\n\nI think it's *kinda reasonable* for people who are mostly not going to think about x-risk anyways, and who don't trust themselves or want to put in the time to evaluate the arguments themselves, to differ to some vague consensus of people you trust.\n\n(I do think it's pretty silly to do that re: mainstream AI experts, because so many of them are clearly not paying much attention to the arguments at all. But, if you don't trust anyone in the x-risk community, idk, I don't have a better suggestion. But you are hear reading LessWrong so probably you aren't doing that)\n\nIt does seem kinda useful to track what mainstream experts believe, for purposes of modeling mainstream society so you can then make predictions about interventions on mainstream society. But, a) it still seems better to separate \"doom-if-prosaic-AI-under-present-conditions\" from \"overall doom\" b) I think it's easy to fall into some tribal dynamics here. Please don't.\n\nIt also seems kinda useful to track what the x-risk-thinkers consensus is, for purposes of modeling the x-risk conversation and how to make intellectual progress on it, but, again, don't fall into the attractor of overdoing it or doing it in a tribal way, and don't overindex on p(Doom) as opposed to all the other questions that are worth asking. \n\nAppendix: The Rob Bensinger Files\n=================================\n\nSome alternate ways of breaking down this question:\n\nRob Bensinger has made some previous attempts at more nuanced things. In [2021 he sent this 2-question survey to ~117 people](https://www.lesswrong.com/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results) working on long-term AI risk\n\n> 1\\. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of humanity not doing enough technical AI safety research?\n> \n> 2\\. How likely do you think it is that the overall value of the future will be drastically less than it could have been, as a result of AI systems not doing/optimizing what the people deploying them wanted/intended?\n\nAnd in 2023 made this much more [multifacted view-shapshot chart](https://www.lesswrong.com/posts/ZDQvYKuyH5aTrZKoY/ai-views-snapshots).\n\n![Image](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/ZDQvYKuyH5aTrZKoY/yf4ldqoaioxq9zwr2aze)\n\n### **...but you only get** [~**five words**~](https://www.lesswrong.com/posts/4ZvJab25tDebB8FGE/you-get-about-five-words) **a couple questions.**\n\nSo insofar as you wanted something like \"a barometer of what some people think\", I think this last one is too complex to be useful except as a one-time highish effort survey.",
      "plaintextDescription": "For awhile, I kinda assumed Eliezer had basically coined the concept of p(Doom). Then I was surprised one day to hear him complaining about it being an antipattern he specifically thought was unhelpful and wished people would stop.\n\nHe noted: \"If you want to trade statements that will actually be informative about how you think things work, I'd suggest, \"What is the minimum necessary and sufficient policy that you think would prevent extinction?\"\n\nComplete text of the corresponding X Thread:\n\n> I spent two decades yelling at nearby people to stop trading their insane made-up \"AI timelines\" at parties.  Just as it seemed like I'd finally gotten them to listen, people invented \"p(doom)\" to trade around instead.  I think it fills the same psychological role.\n> \n> If you want to trade statements that will actually be informative about how you think things work, I'd suggest, \"What is the minimum necessary and sufficient policy that you think would prevent extinction?\"\n> \n> The idea of a \"p(doom)\" isn't quite as facially insane as \"AGI timelines\" as marker of personal identity, but \n> \n> (1) you want action-conditional doom, \n> \n> (2) people with the same numbers may have wildly different models, \n> \n> (3) these are pretty rough log-odds and it may do violence to your own mind to force itself to express its internal intuitions in those terms which is why I don't go around forcing my mind to think in those terms myself, \n> \n> (4) most people haven't had the elementary training in calibration and prediction markets that would be required for them to express this number meaningfully and you're demanding them to do it anyways, \n> \n> (5) the actual social role being played by this number is as some sort of weird astrological sign and that's not going to help people think in an unpressured way about the various underlying factual questions that ought finally and at the very end to sum to a guess about how reality goes.\n\nOrthonormal responds:\n\n> IMO \"p(doom)\" was a predictable o",
      "wordCount": 1128
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "zvisSDFPLWofyFxEQ",
    "title": "Debugging for Mid Coders",
    "slug": "debugging-for-mid-coders",
    "url": null,
    "baseScore": 81,
    "voteCount": 34,
    "viewCount": null,
    "commentCount": 41,
    "createdAt": null,
    "postedAt": "2025-08-16T22:32:27.471Z",
    "contents": {
      "markdown": "I struggled with learning to debug code for a long time. Exercises for learning debugging tended focus on small, toy examples that didn't grapple with the complexity of real codebases. I would read advice on the internet like:\n\n*   Try to create a reliable replication of the debug\n*   Create a minimal demonstration of the bug\n*   Read error messages carefully\n*   Use a debugger\n\nI'd often be starting from a situation where the bug only happens sometimes and I have no idea when, the error log is about some unrelated library I was using and had nothing to do with what the bug would turn out to be. Getting to a point where it was even clear **what exactly the symptoms were** was a giant opaque question-mark for me.\n\nI didn't improve much at debugging until I got generally serious about rationality training. Debugging is a nice in-between difficulty between \"toy puzzle\" and \"solve a complex openended real world problem.\" (Code debugging is \"real world problem-solving\", but, it's a part of the world where you reliably know there's a solution, working in an environment you have near-complete control over and visibility into).\n\nI attribute a lot of my improvement to fairly general problem-solving tools like:\n\n*   Be Patient\n*   [Mentally prepare for a lot of complicated steps in a row](https://www.lesswrong.com/posts/XNm5rc2MN83hsi4kh/buckle-up-bucko-this-ain-t-over-till-it-s-over)\n*   Try to be explicitly \"forming specific hypotheses, and then refuting them\" rather than just kinda randomly flailing/fiddling.\n*   Expand your working memory somehow. Take notes about the scope of the problem as you understand it. Get a larger monitor so you can more easily glance at more facets of the codebase or your notes.\n*   Ask \"what questions would be helpful to ask myself?\"\n*   When you are stuck, make explicit note of what feels difficult about the situation, and brainstorm ways of dealing with those difficulties.\n*   Asking \"This is impossible. Why *exactly* is it impossible?\"\n\nI've written other essays on general rationality/problem-solving. But, here I wanted to write the essay I wish past me had gotten, about some tacit knowledge of debugging. (Partly because it seemed useful, and partly because I'm interested in the general art of \"articulating tacit knowledge\").\n\nNote that I'm *still* a kinda mid-level programmer and senior people may have better advice than me, or think some of this is wrong. This is partly me just writing to help myself understand, and see how well I can currently explain things. \n\nBe willing to patiently walk up the stack trace (but, watch out for red-herrings and gotchas)\n---------------------------------------------------------------------------------------------\n\nOne core skill of debugging is the ability to patiently, thoroughly start from your first clue, and work your way through the codebase, developing a model of what exactly is happening. (Instead of reaching the edge of the current file and giving up)\n\nUnfortunately, *another* core skill of debugging is knowing when you're about to follow the code into a pointless direction that won't really help you. \n\nGotcha #1: \"X does not exist\", \"can't read X\", \"X is undefined\", etc, are often \"downstream symptoms\", rather than the bug itself.\n----------------------------------------------------------------------------------------------------------------------------------\n\nIf you get an error like \"X doesn't exist\", there's a codepath that expects X to exist, but it doesn't. Whatever code caused it to not-exist **isn't particularly likely to be located anywhere near the code that's trying to read X.**\n\nSo, unless you look at the part of the codebase flagging \"can't find X\" and you can clearly see that X was supposed to be created in the same file or have a good reason to think it was created nearby, probably what you should instead be looking for are places that are supposed to create or retrieve X. \n\nThis goes double if the error is in some random library somewhere deep in your dependencies.\n\n*(I originally called this pattern a \"red-herring.\" A senior-dev colleague told me they draw a sharp distinction between \"red-herrings\" and \"downstream symptoms.\")*\n\nGotcha #2: Notice abstraction boundaries, and irrelevant abstractions.\n----------------------------------------------------------------------\n\nThe motivating incident that prompted this post was when I was pairing with a junior dev on debugging the [JargonBot Beta Test](http://lesswrong.com/posts/sZvMLWrWomN28uWA9/jargonbot-beta-test). We had recently built an API endpoint[^vh8q38t290h] for retrieving the jargon definitions for a post. It had been working recently. Suddenly it was not working.\n\nWe started with where an error was getting thrown, worked our way backwards in the code path... and then followed the trail to another file... and then more backwards... \n\n...and then at some point both I and the junior dev said \"it... probably isn't useful to look further backwards.\" (The junior dev said this hesitatingly, and I agreed)\n\nWe were right. How did we know that?\n\nThe answer (I think) is that if we stepped backwards any further, we were leaving the part of the codebase that was particularly about Jargon. Beyond that lay the underlying infrastructure for the LessWrong codebase, i.e. the code responsible for making API endpoints work at all. Last we checked, the underlying infrastructure of LessWrong worked fine, and we hadn't done anything to mess with that. \n\nSo, even though we were still thoroughly confused about what the problem was, it seemed at least like it should be contained to within these few files.\n\nThere was a chunk of the codebase that you'd naturally describe as \"about the JargonBot stuff.\" And there was a point you might naturally describe as it's edge, before passing into another part of the codebase.\n\nNow, that was sort of playing on easy mode – we knew we had just started building JargonBot, it would be pretty crazy for the bug to not live somewhere in the files we had just written. But, a few weeks later we were debugging some other part of the codebase that *other* people had written, awhile ago. (I believe this was integrating the jargon into the post sidebar, where side-comments, side-notes, and reacts live). \n\nIt turned out the sidebar was more complicated than I'd have guessed. The jargon wasn't rendering correctly. We had to pass backwards through a few different abstraction layers – first checking the new code where we were trying to fetch and render the jargon. Then back into the code for the sidebar itself. Then backwards into where the sidebar was integrated into the post page. And then I was confused about how the sidebar even *was* integrated into the post page, it wasn't nearly as straightforward as I thought.\n\nThen I sort of roughly got how the sidebar/post interaction worked, but was still confused about my bug. I could recurse further up the code path...\n\n...but it would be pretty unlikely for whatever was going wrong to be happening outside of the post page itself.\n\nThis all amounts to: \n\n*   keep track of \"abstraction boundaries\" (that is, parts of the codebase that naturally group together for a purpose)\n*   think in terms of \"what sort of hypotheses remotely make sense\", to limit the search space of things you need to build an understanding of.\n\nBinary Search\n-------------\n\nThere is some intricate art to looking at a confusing output, and generating hypotheses that might possibly make sense. A good developer has both some object-level knowledge about their codebase, or codebases in general, that can help them narrow in quickly on where the problem is located. \n\nI don't know how to articulate the details of that skill. BUT, the good new is there is a dumb, stupid algorithm you can follow to help you narrow it down even if you're tired, frustrated and braindead:\n\n1: Follow the code backwards towards the earliest codepath that could possibly be relevant, and the earliest point where it is working as expected.\n\n2\\. Find the spot about halfway between the earliest place where things work as expected, and the final place where you observe the bug. Log all relevant variables (or add a breakpoint and use a debugger).\n\n3\\. If things look normal there, then find a new spot about halfway between that midpoint, and the final-spot-where-the-bug-was-observed. If it *doesn't* look normal, find a new spot about halfway between the *earliest* working spot, and the midpoint.\n\n4\\. Repeat steps 2-3 until you've found the moment where things break.\n\nNow you have a much smaller surface area you need to look at and understand. And instead of stepping through 100 lines of code, you had to find ~6 midpoints.\n\n**Binary searching through** ~**time**~ **spacetime**\n\nA variation on this is if you know the code *used to work* and now it doesn't. You can binary search through your history. If it worked a *long* time ago, on an older commit, you can binary search through your commit history. Start with the oldest commit where it worked, check a commit about halfway between that and the latest commit. Repeat. Git has a tool to help streamline process for this called `git bisect`. \n\nIf it worked *earlier today* (and you didn't make any commits in the meanwhile), you may need to do something more like \"binary search through your undo history.\" This is complicated by:\n\n*   you might have undo history in multiple files\n*   even within a given undo history, you also have to do the more classic \"binary search from the earliest point in the codepath that things-look-correct to the latest point in the codepath where they don't.\" \n\nSometimes this neatly divides into \"binary search through undo-space\" followed by \"binary search within one history-state of the codebase\". But sometimes you need to kind of maintain a multidimensional mental map that includes both changes-to-the-codebase and places-within-the-codebase, and figure out what it even means to binary search that in a somewhat ad-hoc way I'm not sure how to articulate.\n\nNotice confusion, and check assumptions.\n----------------------------------------\n\nAlso, look at your data, not just your code.\n--------------------------------------------\n\nJust yesterday, I was trying build a text editor that took in markdown files, and rendered them as html that I could edit in a wysiwyg fashion. At some point, even though it was supposedly translating the markdown into html, it was still showing up with markdown formatting.\n\nThis happened after ~I~ an llm made some completely unrelated changes that really shouldn't have had anything to do with that.\n\nI was pulling my hair out trying to figure out what was going wrong, stepping back and forth through undo history, looking at anything that changed.\n\nEventually, I stopped looking at the codepath, and looked at the file I was trying to load into the editor. \n\nAt some point, I'd accidentally overwritten the file with corrupt markdown that was sort of... markdown nested inside html nested inside markdown. Or something.\n\nOh.\n\nIf I'd been a better rationalists, earlier in the hair-pulling-out-process, I could have noticed \"this is confusing and doesn't make any goddamn sense.\" [A rationalist should be more confused by fiction than reality](https://baserates-prod-test.vercel.app/posts/5JDkW4MYXit2CquLs/your-strength-as-a-rationalist). If I'm confused, one of my assumptions is fiction. And made a list of everything that could possibly be relevant, and see if there was anything I hadn't looked at yet, rather than looping back and forth in the undo history, vaguely flailing and hoping to notice something new.\n\nBut, it's kind of cognitively expensive to be a rationalist all the time. \n\nA simpler thing I could have done is be better debugger with some object-level debugging knowledge, and note that sometimes, it's not the code that's wrong, it's the data the code is trying to operate on that is wrong. \"Check the data\" probably should have been part of my initial pass at mapping out the problem.\n\n(This ties back to Gotcha #1: if you're getting \"X is not defined\", where X was created awhile ago and not by the obvious nearby parts of the codebase, sometimes try looking up X in your database and see if there's anything weird about it)\n\nThings I *don't* know\n=====================\n\nSo, that was a bunch of stuff I've painstakingly figured out. Some of it I got pieces of by pairing with other more senior developers. The senior developers I've worked with often are thinking so quickly/intuitively it's fairly hard for them to slow down and explain what's going on. \n\nI'm hoping to live in a world where people around me get better at tacit knowledge explication. \n\nHere's some random stuff I *still* don't have a good handle on, that I'd like it if somebody explained:\n\n**How** ***do*** **you learn to replicate bugs**, when they happen inconsistently in no discernable pattern? especially when the bug comes up, like, once every couple days or weeks, instead of once every 5 minutes.\n\n**How does one \"read the docs?\".** Sometimes I ask how a senior dev figured something out, and they say \"I read the documentation and it explained it.\" And I'm like \"okay, duh. but... there's so much fucking documentation. I can't possibly be expected to read it all?\"\n\nDo people just read really fast? I think they have some heuristics for figuring out what parts to read and how to skim, which maybe involves something like binary search and tracking-abstraction-borders. But something about this still feels opaque to me.\n\n**Something something \"how to build up a model of the entire stack.\"** Sometimes, the source of a problem doesn't live in the codebase, it lives in the dev-ops of how the codebase is deployed. It could have to do with the database setup, or the deployment server, or our caching later. I recently got a *little* better at sniffing but this still feels like a muddy mire to me.\n\n[^vh8q38t290h]: This isn't exactly the right description but is accurate enough to be an example.",
      "plaintextDescription": "I struggled with learning to debug code for a long time. Exercises for learning debugging tended focus on small, toy examples that didn't grapple with the complexity of real codebases. I would read advice on the internet like:\n\n * Try to create a reliable replication of the debug\n * Create a minimal demonstration of the bug\n * Read error messages carefully\n * Use a debugger\n\nI'd often be starting from a situation where the bug only happens sometimes and I have no idea when, the error log is about some unrelated library I was using and had nothing to do with what the bug would turn out to be. Getting to a point where it was even clear what exactly the symptoms were was a giant opaque question-mark for me.\n\nI didn't improve much at debugging until I got generally serious about rationality training. Debugging is a nice in-between difficulty between \"toy puzzle\" and \"solve a complex openended real world problem.\" (Code debugging is \"real world problem-solving\", but, it's a part of the world where you reliably know there's a solution, working in an environment you have near-complete control over and visibility into).\n\nI attribute a lot of my improvement to fairly general problem-solving tools like:\n\n * Be Patient\n * Mentally prepare for a lot of complicated steps in a row\n * Try to be explicitly \"forming specific hypotheses, and then refuting them\" rather than just kinda randomly flailing/fiddling.\n * Expand your working memory somehow. Take notes about the scope of the problem as you understand it. Get a larger monitor so you can more easily glance at more facets of the codebase or your notes.\n * Ask \"what questions would be helpful to ask myself?\"\n * When you are stuck, make explicit note of what feels difficult about the situation, and brainstorm ways of dealing with those difficulties.\n * Asking \"This is impossible. Why exactly is it impossible?\"\n\nI've written other essays on general rationality/problem-solving. But, here I wanted to write the essay I wish past me ha",
      "wordCount": 2243
    },
    "tags": [
      {
        "_id": "HFou6RHqFagkyrKkW",
        "name": "Programming",
        "slug": "programming"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "vWqtvLDYyJDHq7roe",
    "title": "How many species has humanity driven extinct?",
    "slug": "how-many-species-has-humanity-driven-extinct",
    "url": null,
    "baseScore": 42,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2025-08-02T02:50:27.174Z",
    "contents": {
      "markdown": "Does anyone know, like, a reasonable *lower* bound on number of species humanity has driven extinct? \n\nI've seen crazy high numbers that (last I checked) seemed to be an extrapolation by people with an ideological ax to grind. (Based on how much habitat we've destroyed, and assuming some number of species per unit-area). ChatGPT suggested \"900\" as the lower bound based on some snippet they found from \"International Union for Conservation of Nature (IUCN)\", but looking into it was a enough work I thought I might toss it to the LessWrong folk and see if anyone could get a more confident answer.",
      "plaintextDescription": "Does anyone know, like, a reasonable lower bound on number of species humanity has driven extinct? \n\nI've seen crazy high numbers that (last I checked) seemed to be an extrapolation by people with an ideological ax to grind. (Based on how much habitat we've destroyed, and assuming some number of species per unit-area). ChatGPT suggested \"900\" as the lower bound based on some snippet they found from \"International Union for Conservation of Nature (IUCN)\", but looking into it was a enough work I thought I might toss it to the LessWrong folk and see if anyone could get a more confident answer.",
      "wordCount": 102
    },
    "tags": [
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "FtxrC2xtTF7wu5k6E",
    "title": "Bodydouble / Thinking Assistant matchmaking",
    "slug": "bodydouble-thinking-assistant-matchmaking",
    "url": null,
    "baseScore": 51,
    "voteCount": 22,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2025-07-16T19:54:48.642Z",
    "contents": {
      "markdown": "I keep meaning to write up a more substantive followup to the [Hire (or Become) a Thinking Assistant](https://www.lesswrong.com/posts/ajT6q5aMokagfthvG/hire-or-become-a-thinking-assistant) post. But, I think this still basically the biggest productivity effect size I know of, and more people should be doing it, and seemed worth writing the simple version of the post.\n\nI tried out 4 assistants who messaged me after the previous post, and ultimately settled into a rhythm of using one who had availability that matched my needs best. I think all of them were net helpful. \n\nSo far they've all been remote contractors that I share my screen with on zoom. Previously I had worked with one in person which also went well although was a bit harder to schedule with.\n\nMost of what I do is just have them check in on me every 10 minutes and make sure I haven't gotten off track. Sometimes, I talk out loud about a problem with them. \n\nThey varied in how much chemistry I felt I had with them, and in how good they were at more active metacognitive assistance. \n\nThe way I basically related to it in most cases was \"I'm in charge of figuring out my metacognitive trigger-actions, and I direct them to ping me if it seems like I'm in a situation where my explicitly-written TAPs suggest I should do X.\" \n\n(It's a delicate operation to actually let people be more actively involved with helping my thought process. I think roughly 1.5 of the 5 people I've worked with had the right chemistry/skills to help more proactively than that right out of the gate)\n\nSome people had foreign accents that made it harder to talk-through-things with. They were still pretty good for the basic \"check in every 10 minutes\" process, and a strategy that worked decently was communicating with them more in writing (i.e. have them observe failure modes I seemed to be falling into and writing them down).\n\n**Negotiating Upwards**\n\nI do think chemistry here matters a lot, and I'd recommend starting off with 1-2 sessions that are largely for getting a feel for whether a given person is working out, and to try out a few of them before settling into a longer-term working relationship.\n\nI found it somewhat awkward when I sometimes needed to say \"okay, this isn't working out enough for it to be worth the amount I was previously paying\". There is a skill of being able to push through that awkwardness. Meanwhile I expect it to work best for most people to start off with lower expectations and payment-rate and then raise it if it's going well.\n\nOne system I found good was paying different rates for \"just checking in every 10 minutes\" style work and \"active thinking assistance.\" Where by default someone is mostly working on their own stuff, but periodically can upgrade to \"okay for this problem it's worth paying more for more active attention.\"\n\n**Someone should still build the Uber-for-FocusMate app**\n\nIdeally, I'd still like someone to build an app that gets enough critical mass to easily be able to find a thinking-buddy on-a-whim. I thought about building that myself but I just have too much else going on. I think it does require someone's full-time entrepreneurial spirit to really get going.\n\nApps normally have a bit of an awkward thing with rating-systems, where in practice people feel bad about leaving less than a 5-star review, and it's hard to leave nuanced feedback. I expect it to be even more awkward here where the problems will tend to be minor personality frictions. If I were building the app I think figuring out how to extract good information would probably one of the main difficulties. \n\nI think displaying publicly some info about how many different people had hired a person more than 3 times or something to maybe be the main way to convey a contractor's quality.\n\n**Meanwhile, matchingmaking thread**\n\nIf you are interested in either finding or being a body-double / thinking-assistant, I recommend posting about it here, with what sort of availability you're looking for or can provide.",
      "plaintextDescription": "I keep meaning to write up a more substantive followup to the Hire (or Become) a Thinking Assistant post. But, I think this still basically the biggest productivity effect size I know of, and more people should be doing it, and seemed worth writing the simple version of the post.\n\nI tried out 4 assistants who messaged me after the previous post, and ultimately settled into a rhythm of using one who had availability that matched my needs best. I think all of them were net helpful. \n\nSo far they've all been remote contractors that I share my screen with on zoom. Previously I had worked with one in person which also went well although was a bit harder to schedule with.\n\nMost of what I do is just have them check in on me every 10 minutes and make sure I haven't gotten off track. Sometimes, I talk out loud about a problem with them. \n\nThey varied in how much chemistry I felt I had with them, and in how good they were at more active metacognitive assistance. \n\nThe way I basically related to it in most cases was \"I'm in charge of figuring out my metacognitive trigger-actions, and I direct them to ping me if it seems like I'm in a situation where my explicitly-written TAPs suggest I should do X.\" \n\n(It's a delicate operation to actually let people be more actively involved with helping my thought process. I think roughly 1.5 of the 5 people I've worked with had the right chemistry/skills to help more proactively than that right out of the gate)\n\nSome people had foreign accents that made it harder to talk-through-things with. They were still pretty good for the basic \"check in every 10 minutes\" process, and a strategy that worked decently was communicating with them more in writing (i.e. have them observe failure modes I seemed to be falling into and writing them down).\n\nNegotiating Upwards\n\nI do think chemistry here matters a lot, and I'd recommend starting off with 1-2 sessions that are largely for getting a feel for whether a given person is working out, and to try out a ",
      "wordCount": 680
    },
    "tags": [
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "XNm5rc2MN83hsi4kh",
    "title": "\"Buckle up bucko, and get ready for multiple hard cognitive steps.\"",
    "slug": "buckle-up-bucko-and-get-ready-for-multiple-hard-cognitive",
    "url": null,
    "baseScore": 148,
    "voteCount": 66,
    "viewCount": null,
    "commentCount": 26,
    "createdAt": null,
    "postedAt": "2025-07-05T01:47:30.342Z",
    "contents": {
      "markdown": "*The second in a series of bite-sized rationality prompts*[^2dmksz04hwh]*.*\n\nOften, if I'm bouncing off a problem, one issue is that I *intuitively expect the problem to be easy.* My brain loops through my available action space, looking for an action that'll solve the problem. Each action that I can easily see, won't work. I circle around and around the same set of thoughts, not making any progress.\n\nI eventually say to myself \"Okay, I seem to be in a hard problem. Time to do some rationality?\"\n\nAnd then, I realize, there's not going to be a single action that solves the problem. It is time to:\n\na) make a plan, with multiple steps\n\nb) deal with the fact that *many* of those steps will be annoying\n\nand c) notice that I'm not even sure the plan will work, so after completing the next 2-3 steps I will probably have to generate *more* steps (itself an annoying step), and do *those ones too*, over and over, until it is done.\n\nThere's a lot of messy details inside the process of \"figure out that plan, and continue to adjust it on the fly.\" But, I find the specific move of \"accept that I am in a prolonged, multi-step annoying process\" to be very valuable.\n\nBefore I do that, every time I run into another annoying subproblem, not only is it object-level annoying, but it feels surprising, jarring. My brain reports \"unfair\", even though that doesn't really make any sense.  (\"What!? Again!!! ARRR! It's not supposed to be like this!\")\n\nIt's time buckle up, and get ready for a long, bumpy ride.\n\nFor me, \"Buckling up\" is basically a particular specialized kind of [grieving](https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1). I had hoped I lived in the world where this problem was easy and I could get away with a short burst of effort. Alas, I do not live in that world. I need to let that hope go, and re-orient. I use the word \"grieving\" to acknowledge a certain weight to it. I think there is shared structure between letting go a loved one who has died, and letting go of a belief that a particular problem is easy. \n\nIt's often kind of psychologically loadbearing to think your problems will be easy. Alas, sometimes, you have to find a different way to be psychologically load-beared.\n\nSo I shift my mental (and sometimes physical) posture to one that is ready for a prolonged marathon. I get ready to take breaks and pace myself.\n\nI take a deep breath.\n\nAnd then I get to work, thinking.\n\nSome examples of the starting points:\n\n*   **Debugging a mysterious problem in a codebase.** I'm flailing around making console.logs, hoping the problem will reveal itself, without making a plan.\n*   **Dealing with bureaucracy.** Surely, it shouldn't be that many steps to get a doctor's appointment. Alas, not only is it more than one step to get the doctor's appointment, I maybe need to get *another* one after that.\n*   **Navigating a social conflict.** I keep trying to say the one phrase that'll convince them, or smooth everything over. Alas, the social conflict is deep and gnarly and isn't going away. Dealing with it requires modeling multiple steps out.\n*   **Building a complex new product.** Sometimes this on a longer scale: surely my initial weekend hackathon prototype is basically working and I can just build the obvious things and ship it and sell it. Alas, my customer's problems are more complex than I realized and I'm going to need to keep asking more questions and modeling new aspects of the situation and iterate significantly on my original idea.\n\nWhat comes after the \"buckle up\" move depends on the situation, but often includes taking more breaks, thinking more strategically, and spending more time on \"meta.\"\n\nTriggers\n--------\n\nWhen is it a good time to buckle up? Sometimes structured-procrastination, or circling around a problem obliquely, is a more efficient approach to solving a problem (i.e. mull it over in the background, eventually realize the answer in the shower).\n\nMoments I find particularly good to consider buckling up include:\n\n*   When you've tried the same thing multiple times, especially with a \"flailing\" sort of feel.\n*   When you feel surprised about something being effortful, multiple times in a row.\n*   You just kinda of know in your heart that you're not going to get this thing done without buckling.\n*   You've tried mulling it over / taking a way / etc, and are still stuck.\n*   It's particularly important for it to get done *quickly* (even if background mulling would be more efficient)\n\nExercises for the Reader\n------------------------\n\nThe skill here is \"notice when it's time to make a difficult, prolonged effort\", and then shift into \"okay, I'm ready\" with as little feet-dragging as you can. \n\nI don't actually have experience purposefully practicing this skill. My guess is that to seriously practice it would involve remembering a situation where you dragged your feet for awhile before accepting \"this is actually hard\", and then noticing all the little moments in between the first clue you could have noticed, and the final shift towards acceptance. (Essentially, the [\"Think It Faster\"](https://www.lesswrong.com/posts/F9WyMPK4J3JFrxrSA/the-think-it-faster-exercise) exercise for a situation that involved the \"buckle up\" move, which can take like 30-60 minutes)\n\nRight now you're probably, like, reading this blogpost on your lunch break or in the evening when you're tired. If you want to get like 60 seconds of progress on becoming the sort of people who buckle up more smoothly/painlessly, I'd maybe try spending those 60 seconds:\n\n1.  Trying to remember some moment where you bounced around for awhile, before eventually switching to buckled up \"okay I'm going to persist on this task\" mode\n2.  Try to remember as many details about how the transition went as you can\n3.  ...idk, let your brain mull it over a bit and see if anything bubbles up.\n\n**Writing it up**\n\nIf you're comfortable with it, I'd appreciate people who share their experience of attempting this exercise. (Both so I can see how many people actually attempted it, and what range of things came up when they did?). \n\nAlso, for bonus points: make a public [Fatebook Prediction](https://fatebook.io/) for whether this will turn out to have been a helpful blogpost for you in a year (maybe [using the browser extension](https://fatebook.io/extension) so you can paste the result directly into the comments here. See [Fluent, Cruxy Predictions](https://www.lesswrong.com/posts/wDpXshpakpYDcTtug/fluent-cruxy-predictions-1) for some background conceptgs here)\n\n[^2dmksz04hwh]: This wasn't my original \"step 2\", but it kept coming up and seemed like it was worth making it's own post.",
      "plaintextDescription": "The second in a series of bite-sized rationality prompts[1].\n\n \n\nOften, if I'm bouncing off a problem, one issue is that I intuitively expect the problem to be easy. My brain loops through my available action space, looking for an action that'll solve the problem. Each action that I can easily see, won't work. I circle around and around the same set of thoughts, not making any progress.\n\nI eventually say to myself \"Okay, I seem to be in a hard problem. Time to do some rationality?\"\n\nAnd then, I realize, there's not going to be a single action that solves the problem. It is time to:\n\na) make a plan, with multiple steps\n\nb) deal with the fact that many of those steps will be annoying\n\nand c) notice that I'm not even sure the plan will work, so after completing the next 2-3 steps I will probably have to generate more steps (itself an annoying step), and do those ones too, over and over, until it is done.\n\nThere's a lot of messy details inside the process of \"figure out that plan, and continue to adjust it on the fly.\" But, I find the specific move of \"accept that I am in a prolonged, multi-step annoying process\" to be very valuable.\n\nBefore I do that, every time I run into another annoying subproblem, not only is it object-level annoying, but it feels surprising, jarring. My brain reports \"unfair\", even though that doesn't really make any sense.  (\"What!? Again!!! ARRR! It's not supposed to be like this!\")\n\nIt's time buckle up, and get ready for a long, bumpy ride.\n\nFor me, \"Buckling up\" is basically a particular specialized kind of grieving. I had hoped I lived in the world where this problem was easy and I could get away with a short burst of effort. Alas, I do not live in that world. I need to let that hope go, and re-orient. I use the word \"grieving\" to acknowledge a certain weight to it. I think there is shared structure between letting go a loved one who has died, and letting go of a belief that a particular problem is easy. \n\nIt's often kind of psychologically l",
      "wordCount": 1076
    },
    "tags": [
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ry4nLykB2piWJpK7M",
    "title": "\"What's my goal?\"",
    "slug": "what-s-my-goal",
    "url": null,
    "baseScore": 122,
    "voteCount": 50,
    "viewCount": null,
    "commentCount": 9,
    "createdAt": null,
    "postedAt": "2025-07-02T02:58:39.008Z",
    "contents": {
      "markdown": "*The first in a series of bite-sized rationality prompts*[^0024k0ktmsgac]*.*\n\nThis is my most common opening-move for Instrumental Rationality. There are many, many other pieces of instrumental rationality. But asking this question is usually a helpful way to get started. Often, simply asking myself \"what's my goal?\" is enough to direct my brain to a noticeably better solution, with no further work.\n\nExamples\n========\n\nPuzzle Games\n------------\n\nI'm playing Portal 2, or [Baba is You](https://www.lesswrong.com/posts/jqb3prwGQjLriq7Lu/exercise-planmaking-surprise-anticipation-and-baba-is-you). I'm fiddling around with the level randomly, sometimes going in circles. I notice I've been doing that awhile. \n\nI ask \"what's my goal?\"\n\nAnd then my eyes automatically glance at the exit for the level and realize I can't possibly make progress unless I solve a particular obstacle, which none of my fiddling-around was going to help with.\n\nArguing\n-------\n\nI'm arguing with a person, poking holes in their position. I easily notice considerations that feel really compelling to me that reinforce that my side is correct. I notice it's getting heated. \n\nI ask \"what's my goal?\"\n\nAnd then realize I had basically been trying to look smart and \"win\" some kind of imaginary points. But what I actually needed to do was change their mind. Which requires modeling their current beliefs and figure out what would actually persuade *them,* not what cached beliefs are compelling to me. \n\n(Or, which requires de-escalating the tension and making it feel safe for them to consider changing their mind, whereas I'd previously been actively been implying that if I was right, they were dumb and should feel dumb. Nobody wants to feel that way)\n\n((Or, maybe... *I* actually want to learn and potentially change *my* mind about something, and repeating reasons that I already think I'm right isn't really helping anyone, I need to be asking myself \"okay, but what *would* change my mind?))\n\nProduct Discovery\n-----------------\n\nI'm writing up a proposal for what my team should do next. I'm fleshing out a lot of details, getting excited about possibilities. I notice I've spent a couple days on this.\n\nI ask \"what's my goal?\"\n\nAnd the answer is \"help my boss figure out what we should do next.\" My boss has given me a couple weeks to come up with some pitches for what to do. And I realize:\n\na) for the first week, I should probably doing a lot more exploring of radically different ideas rather than latching onto the first one I came up with.\n\nb) I should be anticipating what my boss's objections will be, since it's ultimately his call.\n\nI begin strategically directing my attention to areas that will help me find more novel ideas, and asking \"what would be the fastest way to figure out if this idea is any good?\"\n\nTriggers\n========\n\nWhen is it a good time to ask \"what's my goal?\"?\n\nOne good time is \"when you are first starting a project, or planning your day/week\", so you just skip ahead past a lot of meandering unstrategic false starts on the wrong problem.\n\n(Sometimes, at the beginning of a project, you will decide \"my goal is to figure out what-the-hell-I'm-even-doing, and the best way to do that is via meandering play without stressing about goals.\" But, like, now you've doublechecked)\n\nAnother good time is \"when you get a nagging feeling that you're doing the wrong thing\" or \"you notice you sure have been spending a lot of time on this particular step, and are feeling bored or frustrated.\"\n\nExercises for the Reader\n------------------------\n\n*Note: if you don't feel like doing all of these, maybe try just one of whichever is easiest.*\n\nCome up with 1-2 recent times you would have benefited from asking yourself \"what's my goal?\" earlier.\n\nCome up with 1-2 projects/actions you're *currently in the middle of,* where you have a nagging suspicion you're not really tackling the right goal. \n\nCome up with 1-2 moments you expect to run into in the next week where it seems likely you're going to go tackle something without really understanding your goals and predictably waste some time.\n\n* * *\n\n*See also* [*summoning-sapience*](https://www.lesswrong.com/w/summoning-sapience) *and* [*Humans are not automatically strategic*](https://www.lesswrong.com/posts/PBRWb2Em5SNeWYwwB/humans-are-not-automatically-strategic)\n\n[^0024k0ktmsgac]: I am experimenting with the mantra \"your rationality paradigm is not complete until it is decomposed into simple sentences that people can immediately understand.\"",
      "plaintextDescription": "The first in a series of bite-sized rationality prompts[1].\n\n \n\nThis is my most common opening-move for Instrumental Rationality. There are many, many other pieces of instrumental rationality. But asking this question is usually a helpful way to get started. Often, simply asking myself \"what's my goal?\" is enough to direct my brain to a noticeably better solution, with no further work.\n\n\nExamples\n\n\nPuzzle Games\nI'm playing Portal 2, or Baba is You. I'm fiddling around with the level randomly, sometimes going in circles. I notice I've been doing that awhile. \n\nI ask \"what's my goal?\"\n\nAnd then my eyes automatically glance at the exit for the level and realize I can't possibly make progress unless I solve a particular obstacle, which none of my fiddling-around was going to help with.\n\n\nArguing\nI'm arguing with a person, poking holes in their position. I easily notice considerations that feel really compelling to me that reinforce that my side is correct. I notice it's getting heated. \n\nI ask \"what's my goal?\"\n\nAnd then realize I had basically been trying to look smart and \"win\" some kind of imaginary points. But what I actually needed to do was change their mind. Which requires modeling their current beliefs and figure out what would actually persuade them, not what cached beliefs are compelling to me. \n\n(Or, which requires de-escalating the tension and making it feel safe for them to consider changing their mind, whereas I'd previously been actively been implying that if I was right, they were dumb and should feel dumb. Nobody wants to feel that way)\n\n((Or, maybe... I actually want to learn and potentially change my mind about something, and repeating reasons that I already think I'm right isn't really helping anyone, I need to be asking myself \"okay, but what would change my mind?))\n\n\nProduct Discovery\nI'm writing up a proposal for what my team should do next. I'm fleshing out a lot of details, getting excited about possibilities. I notice I've spent a couple days o",
      "wordCount": 691
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "KrFPLtBxvvdySkE83",
    "title": "Hiring* an AI** Artist for LessWrong/Lightcone",
    "slug": "hiring-an-ai-artist-for-lesswrong-lightcone",
    "url": null,
    "baseScore": 30,
    "voteCount": 16,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2025-06-30T19:01:57.229Z",
    "contents": {
      "markdown": "Over the past few years, Lightcone has started using AI art in more of our products. This is a fairly easy and fun part of our job, but I've noticed often there's just a lotta art that needs to get made which we don't quite have the bandwidth to do ourselves. \n\nSo I'm looking into hiring* an AI** artist for periodic contract gigs (this wouldn't be a fulltime thing, I have one initial job in mind, if it goes well we may periodically have other jobs to offer).\n\n*\\* we aren't sure we want to hire a person for this, this isn't a top organizational priority, it's more like I'm checking if someone exists who would integrate pretty quickly/easily into our workflow.*\n\n*\\*\\* in theory, we could hire, like, a traditional artist. I think realistically it just wouldn't actually be worth the money. The part of me that originally got a degree in Art is sad about this, but, like, notably we didn't have much art at all on LessWrong until AI made it much lower cost and achievable for it to be a lightweight part of product iteration.*\n\nOnboarding into the Lightcone Aesthetic / House Style\n-----------------------------------------------------\n\nI know a lot of people are making lots of AI art for fun these days and are pretty good at it. I think it *is* a noticeably different task to make a bunch of art in a particular style matching particular aesthetic guidelines. \n\nIn order for this to make sense, we need someone who has good artistic taste, who is able to make high level creative decisions while tracking a lot of subtleties about what the Lightcone Aesthetic (and various sub-aesthetics) are going for.\n\nI honestly have no idea how much makes sense to pay for this sort of role, I could imagine anywhere from \"pretty cheap for someone who finds it very fun/easy and hits our minimum requirements\" to \"pay more serious professional artist salary for someone we can trust to make high level decisions autonomously.\"\n\nIf you are interested, send me a DM with a link to a portfolio.",
      "plaintextDescription": "Over the past few years, Lightcone has started using AI art in more of our products. This is a fairly easy and fun part of our job, but I've noticed often there's just a lotta art that needs to get made which we don't quite have the bandwidth to do ourselves. \n\nSo I'm looking into hiring* an AI** artist for periodic contract gigs (this wouldn't be a fulltime thing, I have one initial job in mind, if it goes well we may periodically have other jobs to offer).\n\n* we aren't sure we want to hire a person for this, this isn't a top organizational priority, it's more like I'm checking if someone exists who would integrate pretty quickly/easily into our workflow.\n\n** in theory, we could hire, like, a traditional artist. I think realistically it just wouldn't actually be worth the money. The part of me that originally got a degree in Art is sad about this, but, like, notably we didn't have much art at all on LessWrong until AI made it much lower cost and achievable for it to be a lightweight part of product iteration.\n\n\nOnboarding into the Lightcone Aesthetic / House Style\nI know a lot of people are making lots of AI art for fun these days and are pretty good at it. I think it is a noticeably different task to make a bunch of art in a particular style matching particular aesthetic guidelines. \n\nIn order for this to make sense, we need someone who has good artistic taste, who is able to make high level creative decisions while tracking a lot of subtleties about what the Lightcone Aesthetic (and various sub-aesthetics) are going for.\n\nI honestly have no idea how much makes sense to pay for this sort of role, I could imagine anywhere from \"pretty cheap for someone who finds it very fun/easy and hits our minimum requirements\" to \"pay more serious professional artist salary for someone we can trust to make high level decisions autonomously.\"\n\nIf you are interested, send me a DM with a link to a portfolio.",
      "wordCount": 350
    },
    "tags": [
      {
        "_id": "R4txjAfF9d9wJBRA6",
        "name": "Hiring",
        "slug": "hiring"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "RPfLDwnLTdxvfBHwH",
    "title": "Social status games might have \"compute weight class\" in the future",
    "slug": "social-status-games-might-have-compute-weight-class-in-the",
    "url": null,
    "baseScore": 34,
    "voteCount": 14,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2025-05-07T18:56:46.333Z",
    "contents": {
      "markdown": "*(Note: I don't think \"social status games\" are bad, although I think it's usually not healthy/helpful to focus on them as \"social status games qua games.\" i.e. I have some motivation to be good at telling fun stories at parties, or write music that people will like, or throw interesting events. There's also things like \"being a good listening\" / \"helpful advice-giver.\" Some of this is motivated intrinsic joy of the activity, but some is motivated by wanting to feel respected/cool. This seems fine in healthy doses)*\n\nA thought I had while reading [We probably won't just play status games with each other after AGI](https://www.lesswrong.com/posts/hQTBLGsQjucm3hjQ6/we-probably-won-t-just-play-status-games-with-each-other) (which I expected to say \"we'll do other things than play status games\" but AFAICT said *\"we'll play status games with AIs too\"*).\n\nA response I first had was \"but, the AI will be so massively better than us at everything, it'd just be lame to be competing with them.\"\n\nBut, thinking about it a bit more: probably eventually, many/most people are uploads, and they are *also* running on optimized artificial brains. Bio humans may have access to various augmentations, either biological enhancement or tools like in [The Gentle Romance](https://www.lesswrong.com/posts/Rz4ijbeKgPAaedg3n/the-gentle-romance).\n\nI'm not sure about bio humans, but, probably there will eventually be less distinction between uploads and various types of AIs. There will be superintelligences with jupiter brains. There may be some uploads with jupiter brains too. Uploads will probably compete socially with AIs.\n\nIn martial arts, we have a concept of \"weight class\", to make fights \"fair\" and and interesting. I guess we actually sort of already have this for social status games – people tend to compete in arenas with similar socioeconomic status and background. (There's also a \"locality\" aspect, partially eroded by the internet, where you tend to be competing with people in your same geographic area. On the internet, there's still something like \"scenes\", like you might be trying to make cool Filk Songs in the filk community)\n\nSomewhere out there are Old Money dynasties and tech billionaires jockeying for position. But, it's not super relevant to me. There's a sort of automatic self-sorting (although sometimes jarring, when like a young programmer suddenly is a billionaire CEO and then finds themselves in a different social arena they don't understand but affects their career so they kinda have to engage even if they don't want to)\n\nIn some futures, there could be humans and AI competing for who can tell the best joke or run the most interesting events or make cool art. There could be epic-level \"designed to scale\" AI systems that are competing with the equivalent of Jupiter Brain'd Taylor Swift. There might be superintelligences in charge of the solar system or galaxy, and might be augmented posthumans who either have a direct role there, or who monitor/sanity check the AI)\n\nBut, then there might be local groups, with a mix of humans and AIs that weren't designed to scale-and-takeover things, just interact. Each with access to the same plugins and \n\n\"Will the AIs be conscious or appreciate the game\". I dunno. Right now I particularly hope AIs aren't conscious because we have no way of really communicating with them or interacting with them nicely as moral patients. Later on, I may specifically hope that they're sentient so that if they end up winning the universe, and least there's somebody home to appreciate it.\"\n\nDoes this cinematic universe \"hang together\" worldbuilding wise?\". I dunno, haven't thought about it too hard. My mainline scenario is \"FOOM Doom\" and my secondary mainline scenario is \"dystopian moloch leading to disneyland with no children.\" \n\nBut, \"compute weight class for social games\" feels like an interesting idea.",
      "plaintextDescription": "(Note: I don't think \"social status games\" are bad, although I think it's usually not healthy/helpful to focus on them as \"social status games qua games.\" i.e. I have some motivation to be good at telling fun stories at parties, or write music that people will like, or throw interesting events. There's also things like \"being a good listening\" / \"helpful advice-giver.\" Some of this is motivated intrinsic joy of the activity, but some is motivated by wanting to feel respected/cool. This seems fine in healthy doses)\n\nA thought I had while reading We probably won't just play status games with each other after AGI (which I expected to say \"we'll do other things than play status games\" but AFAICT said \"we'll play status games with AIs too\").\n\nA response I first had was \"but, the AI will be so massively better than us at everything, it'd just be lame to be competing with them.\"\n\nBut, thinking about it a bit more: probably eventually, many/most people are uploads, and they are also running on optimized artificial brains. Bio humans may have access to various augmentations, either biological enhancement or tools like in The Gentle Romance.\n\nI'm not sure about bio humans, but, probably there will eventually be less distinction between uploads and various types of AIs. There will be superintelligences with jupiter brains. There may be some uploads with jupiter brains too. Uploads will probably compete socially with AIs.\n\nIn martial arts, we have a concept of \"weight class\", to make fights \"fair\" and and interesting. I guess we actually sort of already have this for social status games – people tend to compete in arenas with similar socioeconomic status and background. (There's also a \"locality\" aspect, partially eroded by the internet, where you tend to be competing with people in your same geographic area. On the internet, there's still something like \"scenes\", like you might be trying to make cool Filk Songs in the filk community)\n\nSomewhere out there are Old Money dynastie",
      "wordCount": 612
    },
    "tags": [
      {
        "_id": "pGqRLe9bFDX2G2kXY",
        "name": "Futurism",
        "slug": "futurism"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "t46PYSvHHtJLxmrxn",
    "title": "What are important UI-shaped problems that Lightcone could tackle?",
    "slug": "what-are-important-ui-shaped-problems-that-lightcone-could",
    "url": null,
    "baseScore": 59,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 22,
    "createdAt": null,
    "postedAt": "2025-04-27T00:02:36.311Z",
    "contents": {
      "markdown": "As I think about \"what to do about AI x-risk?\", some principles that seem useful to me:\n\n1.  Short timelines seem plausible enough that, **for the next year or so, I'd like to focus on plans that are relevant if takeoff begins in the next few years**. In a year, if it looks more like there are some fundamental bottlenecks on true creative thinking, I may consider more projects that only payoff in longer-timeline stories.\n2.  Given \"short timelines\", I feel most optimistic on plans that capitalize on skills that I'm already good at (but maybe multiclassing at things that I can learn quickly with LLM assistance).\n3.  I think \"UI design\" is a skill that I (and Lightcone more broadly) am pretty good at. And, I believe the [Interfaces as a Scarce Resource](https://www.lesswrong.com/posts/hyShz2ABiKX56j5tJ/interfaces-as-a-scarce-resource) hypothesis – the world is often bottlenecked on ability to process and make-use-of information in complicated, messy domains.\n\n(In addition to LessWrong, the Lightcone team has worked on the S-Process, which makes it much easier for grantmakers to argue and negotiate pretty complex positions about how much they value various orgs).\n\nIf I've got a UI-shaped hammer, what are some nails that seem like they need doing? (In particular, that are somehow relevant to x-risk)\n\nSome thoughts so far\n====================\n\n**Cyborgism**\n\nLast fall, I was oriented around building good UI for LLM-assisted-thinking. \n\nIn addition to it generally seeming like a fruitful green field, I had a specific hypothesis that, once LLMs Get Real Gud, there will be an important distinction between \"being able to get useful work out of them given a minute's work, and, being able to get useful work out of them of them in <5 seconds.\" The latter is something that can become a true \"part of your exobrain.\" The former is still more like a tool you're using.\n\nI'm less bullish on that now because, while I think most people aren't quite tackling this with the particular taste I'd apply, it does sure seem like everyone is working on \"do stuff with LLMs\" and it's not where the underpicked fruit is.\n\n**\"Schleppy work\" in narrow, technical domains**\n\nIt seems like there may be narrow, technical domains specific narrow domains, where there's some kinds of tasks that rarely get done because they're too hard to think about –you need tons of context, the context is littered around various places. Or, maybe it's all technically in one place but you have to sift through a lot of extraneous details.\n\nA past example of this would be \"hoverovers in coding IDEs for types and docs\", linters, etc. The ability to right-click on a function call to go to the original implementation of the function. \n\nFor a less technical example: spellchecking and grammarchecking in word processor docs.\n\nA possible (current, important) example might be \"something something Mech Interp a la Chris Olah's earlier work on distill.pub\". I don't actually know how Mech Interp works these days, I vaguely believe there are visualizers/heat maps for neurons, but I'm not sure how useful those actually are for the kinds of analysis that are most important.\n\nAn example John Wentworth has mentioned a couple times is \"automatically generating examples of your abstractions and making sure they type-check.\" (This involves both UI, and some deep technical work to actually verify the type checking)\n\n**What kinds of markets need to exist, that are difficult because of evaluation or reputation or (cognitive) transaction costs?**\n\nIt's sort of surprising that Amazon, Uber or Lugg work. Why aren't people receiving [bobcats](https://xkcd.com/325/) when they order a stapler all the time? A large part of the answer are rating systems, and an ecosystem where it's not trivial to build up a reputation. \n\nWhat are some places where you can't easily buy or find a thing because of adversarial optimization? \n\n...\n===\n\nWith those illustrative examples: do you work on x-risk or x-risk adjacent things? What are some places in your work where it's confusing or annoying to find things, or figure things out?",
      "plaintextDescription": "As I think about \"what to do about AI x-risk?\", some principles that seem useful to me:\n\n 1. Short timelines seem plausible enough that, for the next year or so, I'd like to focus on plans that are relevant if takeoff begins in the next few years. In a year, if it looks more like there are some fundamental bottlenecks on true creative thinking, I may consider more projects that only payoff in longer-timeline stories.\n 2. Given \"short timelines\", I feel most optimistic on plans that capitalize on skills that I'm already good at (but maybe multiclassing at things that I can learn quickly with LLM assistance).\n 3. I think \"UI design\" is a skill that I (and Lightcone more broadly) am pretty good at. And, I believe the Interfaces as a Scarce Resource hypothesis – the world is often bottlenecked on ability to process and make-use-of information in complicated, messy domains.\n\n(In addition to LessWrong, the Lightcone team has worked on the S-Process, which makes it much easier for grantmakers to argue and negotiate pretty complex positions about how much they value various orgs).\n\nIf I've got a UI-shaped hammer, what are some nails that seem like they need doing? (In particular, that are somehow relevant to x-risk)\n\n\nSome thoughts so far\nCyborgism\n\nLast fall, I was oriented around building good UI for LLM-assisted-thinking. \n\nIn addition to it generally seeming like a fruitful green field, I had a specific hypothesis that, once LLMs Get Real Gud, there will be an important distinction between \"being able to get useful work out of them given a minute's work, and, being able to get useful work out of them of them in <5 seconds.\" The latter is something that can become a true \"part of your exobrain.\" The former is still more like a tool you're using.\n\nI'm less bullish on that now because, while I think most people aren't quite tackling this with the particular taste I'd apply, it does sure seem like everyone is working on \"do stuff with LLMs\" and it's not where the underpicke",
      "wordCount": 667
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "7uTPrqZ3xQntwQgYz",
    "title": "Anthropic, and taking \"technical philosophy\" more seriously",
    "slug": "anthropic-and-taking-technical-philosophy-more-seriously",
    "url": null,
    "baseScore": 139,
    "voteCount": 56,
    "viewCount": null,
    "commentCount": 29,
    "createdAt": null,
    "postedAt": "2025-03-13T01:48:54.184Z",
    "contents": {
      "markdown": "So, I have a lot of complaints about Anthropic, and about how  EA / AI safety people often relate to Anthropic (i.e. treating the company as more trustworthy/good than makes sense). \n\nAt some point I may write up a post that is focused on those complaints.\n\nBut after years of arguing with Anthropic employees, and reading into the few public writing they've done, my sense is Dario/Anthropic-leadership are at least reasonably earnestly trying to do good things within their worldview.\n\nSo I want to just argue with the object-level parts of that worldview that I disagree with.\n\nI think the Anthropic worldview is something like:\n\n1.  Superalignment is probably not *that* hard to navigate.[^o5kal3jhwhg]\n2.  Misuse by totalitarian regimes or rogue actors is reasonably likely by default, and very bad.\n3.  AGI founded in The West would not be bad in the ways that totalitarian regimes would be.\n4.  Quick empiricism is a way better approach to solving research problems than trying to \"think ahead\" too cleverly.\n5.  It's better to only make strong claims when you have strong evidence to back them up, and only make strong commitments when you're confident about those commitments being the right ones. (This is partly about \"well, there are some political games you just do need to play\", but at least partly motivated by epistemic principles).[^7g8zrnivifi] \n\nMy current understanding of the corresponding Anthropic strategy is:\n\n1.  Stay on the frontier of AI, so that they reliably have \"a seat at the table\" during the realpolitik discussions of how AI policy plays out.\n2.  Race to get ~human-level AGI that helps with scalable superalignment research.[^wagsk91lc2]\n3.  Iterate on various empirical frameworks for scalable oversight, e.g. various ways you can use weaker models to check that larger untrusted models are safe. Use a mix of interpretability and control techniques to help.\n4.  Encourage the West to race, banking on it being easier to get companies / politicians to pivot to safer practices when there's clearer smoking guns, and clearer asks to make of people.\n5.  Eventually they do need to spend a bunch of political chips, which they [seem](https://www.anthropic.com/news/anthropic-s-recommendations-ostp-u-s-ai-action-plan) [to](https://www.youtube.com/watch?v=4poqjZlM8Lo) be starting to spend this year. But, those chips are focused on something like: \"race to ensure lead over China, then maybe briefly slow down when the lead is more secure and we're close to the dangerous ASL levels, then invent superintelligence, and use it to solve a bunch of problems\"[^2ymqp0cjhgm] (as opposed to \"global moratorium with international buy-in\")\n6.  ????\n\nI haven't seen a clear plan for what comes exactly after step #5. I've seen Dario say in interviews \"we will need to Have A Some Kind of Conversation About AI, Together.\" This is pretty vague. It might be vague because Dario doesn't have a plan there, or might be that the asks are too overton-shattering to say right now and he is waiting till there's clearer evidence to throw at people.\n\nI think it's plausible Dario's take is like \"look it's not that useful to have a plan in advance here because it'll depend a ton on what the gameboard looks like at the time.\"\n\nI: Arguments for \"Technical Philosophy\"\n=======================================\n\nThe crucial questions in my mind here are:\n\n**Technical**\n\n*   Do we need [10-30 years of serial research](https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development), which can't be parallelized?\n*   Does superalignment require \"extreme philosophical competence?\"\n*   Is the Anthropic culture/playbook prepared for the potential necessity of 10+ year pauses?\n*   What does the curve of \"alignment difficulty vs capabilities\" look like, right around the point where AI becomes capable enough to meaningfully help with \"ending the acute risk period?\"\n\n**Geopolitical**\n\n*   How inevitable is racing?\n*   How willing would China / others be to go along with a serious pause proposal?\n*   How much Overton-window-smashing do we need \"right now\" to get to an adequate world and how practical is that?\n\n**Tying together in:**\n\n*   Should Anthropic comms and strategy be way more focused on getting everyone to slow down, and way less focused on racing against totalitarian states?\n\nI don't disagree that totalitarian AI would be real bad. It's quite plausible to me that the \"global pause\" crowd are underweighting how bad it would be.\n\nBut I'm personally at like \"it's at least ~60% that super alignment is Real Hard\", and that the Anthropic playbook and research culture is not well suited for solving it. And this makes the \"how bad is totalitarian AI?\" question kind of moot.\n\nIt feels vaguely reasonable to me to have a belief as low as 15% on \"Superalignment is Real Hard in a way that requires like a 10-30 year pause.\" And, at 15%, it still feels pretty crazy to be oriented around racing the way Anthropic is. \n\nI once talked to Zac Hatfield-Dodds who agreed with me about \"15% on 'superalignment requires 10+ year pause\" being a crux,\" but their number was like 5%[^kld4y8tympj]. And, yeah – if I earnestly believed 5%, I would honestly  be unsure how to compare risk of x-risk from risk of catastrophic misuse. I was pretty happy to find such a crisp disagreement – a crux! a *double* crux even!\n\nBut, my guess is that Anthropic employees have biased reasons for thinking the risk is so low, or can be treated as low in practice. See [Optimistic Assumptions, Longterm Planning, and \"Cope\"](https://www.lesswrong.com/posts/8ZR3xsWb6TdvmL8kx/optimistic-assumptions-longterm-planning-and-cope). But, over my years of arguing with them, I've found it harder to present a slam-dunk case than I initially expected, and I think it'd be good to actually argue some of the object level details. \n\nI will articulate some arguments here, and mostly I am hoping for more argument and back-and-forth in the comments. I think it'd be cool if this ended with someone representative of Anthropic having more of an extended debate with someone with stronger takes on the technical side of things than me.\n\n10-30 years of serial research, or \"extreme philosophical competence.\"\n======================================================================\n\nAnthropic seems like they *are* making costly enough signals of \"we may need to slow down for 6-12 months or so, and we want other people to be ready for that too\", that I believe they take that earnestly.\n\nBut they aren't saying anything like \"hey everyone, we maybe will have to stop for like *over a decade* in over to figure things out.\"  And this is the thing I most wish they would do.\n\nThe notion of \"we need a lot of serial research time\" was written up by Nate Soares in a post on [differential technological development](https://www.lesswrong.com/posts/vQNJrJqebXEWjJfnz/a-note-about-differential-technological-development). The argument is: even though some types of research will get easier to do when we have more \"realistic/representative\" AI, or more researchers available to focus on the problem, there are some kinds of research that really require one person to do a lot of deep thinking, integrating concepts, synthesizing, which historically has seemed to take blocks of time that are measured in decades. \n\nRight now, we're in an \"iterative empiricism\" regime with AI, where you can see some problems, try some obvious solutions, see how those solutions work, and iterate on the next generation. The problem is that at some point, AI will probably hit a critical threshold where it rapidly recursively self-improves.\n\nDoes your alignment process safely scale to infinity?\n-----------------------------------------------------\n\nA lot of safeguards that hold up when the next generation of AI is only slightly more powerful, break down when the next generation has scaled to \"unboundedly powerful.\"\n\nI think Dario agrees with this part. My sense from scattered conversations and public comms and reading between the lines is \"Anthropic will be able to notice and slow down before we get to this point, and make sure they have a good scalable oversight system in place before you begin the recursive self-improvement process.\"\n\nWhere I think we disagree, is how badly the Anthropic playbook is suited for handling this shift. \n\nI think for the shift from \"near human\" to \"massively superhuman\" to go safely, your alignment team needs to be very competent at \"technical philosophy.\" \n\nBy \"technical philosophical competence,\" I think I mostly mean: \"Ability to define and reason about abstract concepts with extreme precision. In some cases, about concepts which humanity has struggled to agree on for thousands of years. And, do that defining in a way that is robust to extreme optimization, and is robust to ontological updates by entities that know a lot more about us and the universe than we know.\"\n\nPosts like [AGI Ruin: A List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities) (and, basically the entire LessWrong sequences) have tried to argue this point. Here is my attempt at a quick recap of why this is important:\n\n*   **Pointing:**\n    *   You need to successfully point the AI at anything at all. (This may superficially seem like it's working with current LLMs, but it isn't actually anywhere close to robust enough to hold up)\n    *   You need to point the AI at some kind of *nuanced abstract target*, in particular, that remains stable as the AI updates its ontology.\n    *   (You also eventually need to point the AI at a cluster of messy human-value-concepts in particular. Though from what I gather, MIRI-ish people think if you get the first two things, this last part isn't actually that hard)  \n*   **Unbounded optimization is dangerous.** You need to do all of this with almost perfect mathematical robustness, because extreme levels of optimization are incredibly dangerous (in a way that is counterintuitive to most people)\n*   **Interpersonal Resolution. **It may not be required initially (depending on your plan), but ultimately, at some point your AI, or civilization of AIs, needs to successfully navigate human values, in a way that doesn't just work for one person, but somehow deals with interpersonal differences in values.\n*   **Intrinsic Goal-directedness. **You have to do this even if you build an “oracle AI.\" If your oracle is sufficiently powerful to be useful (i.e. can quickly make new scientific breakthroughs), it must think in terms similar to \"having goals\" and \"achieving particular consequences\", because that is necessary for efficiently solving complex problems.[^abjpuuusbcj]\n*   **Someone will totally build reckless AI. **You do *have* to deal with all these problems, because even if you can get controlled human-level (or slightly moderately superhuman) AI… sooner or later someone is going to build a reckless, goal directed AI and not even bother isolating it from the internet.\n*   **Pivotal necessity.** To deal with reckless AI, you need a lot of optimization power, to robustly prevent major companies and nationstates and eventually small hard-to-identify actors building reckless AI. Maybe you need a [pivotal act](https://www.lesswrong.com/w/pivotal-act). Maybe it's better to frame it as a [pivotal process](https://www.lesswrong.com/posts/etNJcXCsKC6izQQZj/pivotal-outcomes-and-pivotal-processes). It seems good if it's more of a collaborative thing that world nations are deciding on together, but you need a pivotal *something,* and even if you have a lot of buy-in, it needs to be able to contend with people/companies/nations that aren't bought in. \n\nWe need AI (or, at least some extremely powerful technology) that can help us end the acute risk period. \n\n\"Okay, but what does the alignment difficulty curve look like at the point where AI is powerful enough to *start* being useful for Acute Risk Period reduction?\"\n----------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nA counterpoint I got from Anthropic employee Drake Thomas (note: not speaking for Anthropic or anything, just discussing his own models), is something like:\n\n> Sure, I buy that unbounded optimization is hella dangerous, and that you can't naively scale our current processes to handle it. This seems very obvious.\n> \n> But, the question here is not \"can we handle unbounded optimization right away?\". The question is \"can we use ~medium-power AI to incrementally help us handle optimization needed to execute a pivotal act?\" (It's notably not even \"get us all the way to safe unbounded superintelligence\" – if we have a pivotal act, we can then take more time to get things right before we try to handle that case).\n> \n> Can an IQ 150 AI help align an IQ 155 AI, in a way that is clear and legibly good to humans? Can a 155 IQ AI do that for a 160 AI? Where do we expect this to break down?\n> \n> What \"IQ\" of an AI would we need to do a pivotal act? Is it before or after the MIRI folk expect a sharp left turn? Why do they think that? \n> \n> ![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/fcd78e507b89f82114912d38d03d712f6b30431c2e079c07.png)\n\nI think this is a kind of reasonable point and I'm interested in takes from more MIRI-ish crowd people on it. But, I still feel quite worried.\n\nAre there any pivotal acts that aren't philosophically loaded?\n--------------------------------------------------------------\n\nA lot of hope (I think both for me, and my current understanding of Drake's take), is \"can you build an AI that helps invent powerful (non-AI) technology, where it's sort of straightforward to use the technology to stop people from building reckless AI.\n\nThe only technology that I've imagined thus far that feels plausible is \"invent uploading or bio enhancements that make humans a lot smarter\" (essentially approaching \"aligned superintelligence\" by starting from humanity, rather than ML). \n\nThis does feel intuitively plausible to me, but:\n\n1\\. I think, to build AI powerful enough to invent such technology *quickly*[^nerh18x8w3d], you need to be able to point the AI at abstract targets that are robust under ontological change. I don't actually expect these are any easier than the more stereotypical \"\\[moral\\] philosophy questions people have argued about for thousands of years.\"\n\n2\\. Even if powerful enough AI to help invent such technology is (currently) safe, it's probably at least pretty close to \"would be actively dangerous with slightly different training or scaffolding.\" \n\n3\\. You still end up needing to solve the problem of \"prevent humans or AIs from building reckless AI, indefinitely\", even if you kick the can to uplifted human successors. And even if you've gotten a lot of buy-in from major governments [^bxs2h21cji]and such, you do need to be capable of robustly stopping many smart actors, who work persistently over a long time. This requires a lot of optimization power, enough that even if it's not getting into \"unboundedly huge\" territory. I dunno, if your plan is routing through uplifted human successors figuring it out I still think you need to start contending with the details of this now.\n\n4\\. If we need a decades-long-pause, then even the world will need to successfully notice and orient to that fact. By default I expect tons of economic and political pressure towards various actors trying to to get more AI power even if there's broad agreement that it's dangerous. If AI is \"technically philosophically challenging\", then it's important for Anthropic to understand that and to use it's \"seat at the table\" strategy to try to (ideally) help convey this (or, maybe \"argue from authority.\")\n\n5\\. Even if you, you probably want to get actual fully superhuman AI solving tons of problems.\n\nYour org culture needs to handle the philosophy\n-----------------------------------------------\n\nAnthropic is banking on \"Medium-strong AI can help us handle Strong AI.\" I'm not sure whether the leadership is thinking more about using medium-AI to \"figure out how to align strong-AI\" or more like using it to \"do narrower things that buy us more time.\"\n\nTo be clear: I am *also* banking on leveraging \"medium-strong AI\" to help, at this point. But, I think leveraging it to help align superhuman AI requires directly tackling the philosophically hard parts. \n\nI get a sense that Anthropic research culture is sort of allergic to philosophy. I think this allergy is there for a reason – there's a lot of people making longwinded arguments that are, in fact, BS. The bitter lesson taught the ML community that simple, scalable methods that leverage computation tend to ultimately outperform hand-crafted, theoretically sophisticated approaches. \n\nBut, when you get to the point where you're within one \"accidental leave-the-AI-training too long\" away from superintelligence, you think you really do need world-class technically-grounded philosophy of a kind that I'm not sure humanity has even seen yet. \n\nPhilosophers-as-a-field have not reached consensus on a lot of important issues. This includes both moral \"what is The Good\" kind of questions, but also more basic seeming things like \"what even is an object?\". IMO, this doesn't mean \"you can't trust philosophy\", it means \"you should expect it to be hard, and you need to deal with it anyway.\"\n\nI'm actually worried about the default result of trying to leverage LLM-agents to help solve these sorts of problems to make people stupider, instead of smarter. My experience is that LLMs have nudged me towards trying to solve problems of the shape that LLMs are good at solving. There's already a massive set of biases nudging people to try substitute easier versions of the alignment problem that don't help much, and LLMs will exacerbate that.\n\nI can buy \"when you have human-level AI, you get to run tons of empirical experiments that tell you a lot about an AI's psychology and internals\", that you don't get to run on humans. **But, those experiments need to pointed towards having a deep, robust understanding in order for it to be safe to scale AI past the human levels.** When I hear vague descriptions of the sort of experiments Anthropic seems to be running, they do not seem pointed in that direction. \n\n(what *would* count as the right sort of experiments? I don't really know. I am hoping for some followup debate between prosaic and agent-foundations-esque researchers on the details of this)\n\nAlso, like, you should be way more pessimistic about how this is organizationally hard\n--------------------------------------------------------------------------------------\n\nI wrote [\"Carefully Bootstrapped Alignment\" is organizationally hard](https://www.lesswrong.com/posts/thkAtqoQwN6DtaiGT/carefully-bootstrapped-alignment-is-organizationally-hard) and [Recursive Middle Manager Hell](https://www.lesswrong.com/posts/pHfPvb4JMhGDr4B7n/recursive-middle-manager-hell) kind of with the explicit goal of slowing down how rapidly Anthropic scaled, because I think it's very hard for large orgs to handle this kind of nuance well.\n\nThat didn't work, at least in the sense of \"not hiring a lot of people.\" Two years ago, when I last talked to a bunch of Anthropic peeps about this, my sense was that there were indeed some of the pathologies I was worried about. \n\nSince then, I've been at least somewhat pleasantly surprised about vague vibes about how Dario controls Anthropic (that is to say: seems like he tries to actually run it with his models). \n\nI still think it is extremely hard to steer a large organization, and my sense is Dario's attention is still naturally occupied by tons of things about \"how to run a generally successful company\" (which is already quite hard), which I doubt leaves nearly enough room for either the theoretic questions of \"what is really needed to align superintelligence\" and \"what implications does that have for a company that needs to hit a very narrow research target in a short timeline?\"\n\nListing Cruxes & Followup Debate\n================================\n\nRecapping the overall point here: \n\nIf alignment were sufficiently hard in particular ways, I don't think Anthropic's cluster of strategies makes sense. If alignment is hard, it doesn't matter if China wins the race. China still loses and so does everyone else. The most important thing would be somehow slowing down across board. \n\nIt doesn't matter if we get a few years of beneficial technical progress if some US lab or government project eventually runs an AI with fewer safeguards, or if humanity cedes control of it's major institutions to AI processes.\n\nThe thing I am hoping to come out of the comments here is getting more surface area on people's cruxes, while engaging with the entirety of the problem. I'd like it if people got more specific about where they disagree.\n\nIf you disagree with my framing here, that's fine, but if so I'd like to see your own framing that engages with the entirety of the problem (and framed around \"what would be sufficient to think Anthropic should significantly change it's research or policy comms?\"\n\nDrake's arguments about \"what capability levels do we actually need to execute useful pivotal acts? What does the alignment-difficult curve look like right around that area?\" were somewhat new-to-me, and I don't think I've seen a thorough writeup from the \"AI is very hard\" crowd that really engaged with that. \n\nI'll be writing a top-level comment that list out many of my own cruxes.\n\n[^o5kal3jhwhg]: Anthropic people self report as thinking \"alignment may be hard\", but, I'm comparing this to the MIRI cluster who are like \"it is so hard your plan is fundamentally bad, please stop.\" \n\n[^7g8zrnivifi]: This is a guess after talking with Drake Thomas about his sense of the Anthropic view on comms strategy during the drafting of this post, not a deeply-integrated piece of Ray's model of Anthropic. \n\n[^wagsk91lc2]: This is based on comments like in Dario's post on export controls: \"If China can't get millions of chips, we'll (at least temporarily) live in a unipolar world, where only the US and its allies have these models.It's unclear whether the unipolar world will last, but there's at least the possibility that, because AI systems can eventually help make even smarter AI systems, a temporary lead could be parlayed into a durable advantage.\" \n\n[^2ymqp0cjhgm]: This is noticeably different than what I'd be spending chips on. \n\n[^kld4y8tympj]: He added recently:On a minute's reflection, I'd still endorse 2%-5% on this exact claim, though I'd go maybe-much higher on \"superalignment in practice takes at least $years of serial research (with models stronger than current SOTA) to solve.\" \n\n[^abjpuuusbcj]: The Anthropic beta-readers to objected to this one. One said:Disagree, at least with the implication for non-myopic goals.  I see no serious barrier to systems which competently perform faster-than-human, maybe-smarter-than-human research, as an essentially local tree of tasks with a clear \"stop if unsafe\" priority on all of them.(tbc not claiming this solves all problems!) \n\n[^nerh18x8w3d]: I can imagine AI that merely correctly finds existing relevant facts and arguments and being logically coherent, without being deeply good at original thinking, which doesn't require extreme philosophical competence. But, I don't think this will be enough to invent novel tech faster than someone else will build reckless AI. \n\n[^bxs2h21cji]: I certainly want this, but I don't know that we'll actually get it.",
      "plaintextDescription": "So, I have a lot of complaints about Anthropic, and about how  EA / AI safety people often relate to Anthropic (i.e. treating the company as more trustworthy/good than makes sense). \n\nAt some point I may write up a post that is focused on those complaints.\n\nBut after years of arguing with Anthropic employees, and reading into the few public writing they've done, my sense is Dario/Anthropic-leadership are at least reasonably earnestly trying to do good things within their worldview.\n\nSo I want to just argue with the object-level parts of that worldview that I disagree with.\n\nI think the Anthropic worldview is something like:\n\n 1. Superalignment is probably not that hard to navigate.[1]\n 2. Misuse by totalitarian regimes or rogue actors is reasonably likely by default, and very bad.\n 3. AGI founded in The West would not be bad in the ways that totalitarian regimes would be.\n 4. Quick empiricism is a way better approach to solving research problems than trying to \"think ahead\" too cleverly.\n 5. It's better to only make strong claims when you have strong evidence to back them up, and only make strong commitments when you're confident about those commitments being the right ones. (This is partly about \"well, there are some political games you just do need to play\", but at least partly motivated by epistemic principles).[2] \n\nMy current understanding of the corresponding Anthropic strategy is:\n\n 1. Stay on the frontier of AI, so that they reliably have \"a seat at the table\" during the realpolitik discussions of how AI policy plays out.\n 2. Race to get ~human-level AGI that helps with scalable superalignment research.[3]\n 3. Iterate on various empirical frameworks for scalable oversight, e.g. various ways you can use weaker models to check that larger untrusted models are safe. Use a mix of interpretability and control techniques to help.\n 4. Encourage the West to race, banking on it being easier to get companies / politicians to pivot to safer practices when there's clear",
      "wordCount": 3383
    },
    "tags": [
      {
        "_id": "aHay2tebonHAYKtac",
        "name": "Anthropic (org)",
        "slug": "anthropic-org"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "skEhdgiK6T5HzMhis",
    "title": "\"Think it Faster\" worksheet",
    "slug": "think-it-faster-worksheet",
    "url": null,
    "baseScore": 66,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2025-02-08T22:02:27.697Z",
    "contents": null,
    "tags": [
      {
        "_id": "DWWZwkxTJs4d5WrcX",
        "name": "Exercises / Problem-Sets",
        "slug": "exercises-problem-sets"
      },
      {
        "_id": "2wjPMY34by2gXEXA2",
        "name": "Techniques",
        "slug": "techniques"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "sHvByGZRCsFuxtTKr",
    "title": "Voting Results for the 2023 Review",
    "slug": "voting-results-for-the-2023-review",
    "url": null,
    "baseScore": 86,
    "voteCount": 34,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2025-02-06T08:00:37.461Z",
    "contents": {
      "markdown": "The votes are in for the 2023 Review!\n\n6,264 posts were written in 2023\n\n662 of them were nominated.\n\n209 of them got at least one review, and a positive review-vote total.\n\n50 of them shall be displayed in the Best of LessWrong, Year 2023.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/96a0e2747a3f11aac7be85f99794b9b422e157f487ad2b88.png)\n\nReviews\n=======\n\nExactly 100 people wrote reviews, and many of them I found particularly valuable. I want to give some particular shout outs to Ryan Greenblatt, John Wentworth, and Steve Byrnes.\n\nI found Ryan Greenblatt's reviews to be sort of relentlessly reasonable. He reviewed many posts thoughtfully, clearly state flaws, value props, and concrete potential improvements. I particularly liked his review of [Anthropic's Responsible Scaling Policy & Long-Term Benefit Trust](https://www.lesswrong.com/posts/6tjHf5ykvFqaNCErH/anthropic-s-responsible-scaling-policy-and-long-term-benefit?commentId=NyqcvZifqznNGKxdT)\n\nI personally had the most direct \"worldview update\" from John Wentworth's [The Case Against AI Control Research](http://localhost:3000/posts/8wBN8cdNAv3c7vt6p/the-case-against-ai-control-research). I don't fully agree with John's take, but I found distinguishing how bad scheming is at different AI stages, and what else needs to be going right at that stage, a quite useful frame.\n\nI was also grateful for Steve Byrnes extensive [opinionated review of the  “Sharp Left Turn” discourse](https://www.lesswrong.com/posts/2yLyT6kB7BQvTfEuZ/sharp-left-turn-discourse-an-opinionated-review).\n\nHere's a general screenshot of top of the [Review Leaderboard.](https://www.lesswrong.com/reviews/2023) Thanks to everyone who participated.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a24211c1ef658edefa512344d05b98680a9eca43db9edb35.png)\n\nAnd, reminder, reviews now appear on the Spotlight Items, at the top of the home page, and on the [/bestoflesswrong](https://www.lesswrong.com/bestoflesswrong) page.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/bee8410dcc95ffc629ff0705a0dbd038eea769f9d3e63480.png)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/27d4ba57e5210829dbb601d73ad7594e11d1787e8013c3c8.png)\n\nWeighted Vote Totals\n====================\n\nIn past years, we calculated the votes based on two groups of users: Established users with 1000+ karma (who got 3x the vote weight), and users with <1000 karma. \n\nThis time, we're replacing that arbitrary cliff with a more granular \"your review vote gets multiplied by your Strong Vote power.\" I mentioned we'd be doing something like that in the [announcement post](https://www.lesswrong.com/posts/pudQtkre7f9GLmb2b/the-2023-lesswrong-review-the-basic-ask#Sliding_Scale_of_Voting_Power), noting:\n\n> I haven't been happy with the arbitrary cliff here. It gives *more* power than I really wanted to allocate to people with 1000 karma, and not enough weight to people who have been around much longer and have demonstrated good judgment. But, karma is still a pretty messy indicator, so I don't want to give too *much* power to high karma users either.\n\nUltimately it seemed that \"[Strong Vote power](https://www.lesswrong.com/posts/7Sx3CJXA7JHxY2yDG/strong-votes-update-deployed)\" basically did what we wanted. Very high karma users get more voting power, but each additional point takes roughly twice as much karma as the previous point, which limits how extreme the difference can get. Users pretty quickly ramp up to around ~6x voting power. Most longterm users will have somewhere around 8-9x. A few users get as high as 14x.\n\nThat said, to avoid de-anonymization of votes, on this post I'm displaying the \"raw\" vote strength of each vote, before being multiplied.\n\nThe Results\n===========\n\nOkay. You probably kinda scrolled past all that to get to what you're all here for: what were the best posts of LessWrong 2023, according to you, the people?\n\n408 people voted. 161 people cast the six votes we asked to get the Good Citizen Stamp. Here's what they thought:\n\n<table><tbody><tr><td class=\"item-count\">0</td><td><a class=\"post-title\" href=\"/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion\">AI Control: Improving Safety Despite Intentional Subversion</a> <span class=\"post-author\">Buck</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.4\" class=\"dot\" style=\"width:12.600000000000001px; height:12.600000000000001px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"6.2\" class=\"dot\" style=\"width:9.3px; height:9.3px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">1</td><td><a class=\"post-title\" href=\"/posts/Zp6wG5eQFLGWwcG6j/focus-on-the-places-where-you-feel-shocked-everyone-s\">Focus on the places where you feel shocked everyone's dropping the ball</a> <span class=\"post-author\">So8res</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.9\" class=\"dot\" style=\"width:13.350000000000001px; height:13.350000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">2</td><td><a class=\"post-title\" href=\"/posts/JEhW3HDMKzekDShva/significantly-enhancing-adult-intelligence-with-gene-editing\">Significantly Enhancing Adult Intelligence With Gene Editing May Be Possible</a> <span class=\"post-author\">GeneSmith</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.9\" class=\"dot\" style=\"width:13.350000000000001px; height:13.350000000000001px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"8.1\" class=\"dot\" style=\"width:12.149999999999999px; height:12.149999999999999px; background:#5f9b65\"></span><span title=\"6.2\" class=\"dot\" style=\"width:9.3px; height:9.3px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">3</td><td><a class=\"post-title\" href=\"/posts/KpMNqA5BiCRozCwM3/social-dark-matter\">Social Dark Matter</a> <span class=\"post-author\">Duncan Sabien (Deactivated)</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">4</td><td><a class=\"post-title\" href=\"/posts/HcJPJxkyCsrpSdCii/statement-on-ai-extinction-signed-by-agi-labs-top-academics\">Statement on AI Extinction - Signed by AGI Labs, Top Academics, and Many Other Notable Figures</a> <span class=\"post-author\">Dan H</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">5</td><td><a class=\"post-title\" href=\"/posts/oM9pEezyCb4dCsuKq/pausing-ai-developments-isn-t-enough-we-need-to-shut-it-all-1\">Pausing AI Developments Isn't Enough. We Need to Shut it All Down</a> <span class=\"post-author\">Eliezer Yudkowsky</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"8.6\" class=\"dot\" style=\"width:12.899999999999999px; height:12.899999999999999px; background:#5f9b65\"></span><span title=\"8.1\" class=\"dot\" style=\"width:12.149999999999999px; height:12.149999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">6</td><td><a class=\"post-title\" href=\"/posts/RryyWNmJNnLowbhfC/please-don-t-throw-your-mind-away\">Please don't throw your mind away</a> <span class=\"post-author\">TsviBT</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"8.1\" class=\"dot\" style=\"width:12.149999999999999px; height:12.149999999999999px; background:#5f9b65\"></span><span title=\"6.2\" class=\"dot\" style=\"width:9.3px; height:9.3px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">7</td><td><a class=\"post-title\" href=\"/posts/nnDTgmzRrzDMiPF9B/how-much-do-you-believe-your-results\">How much do you believe your results?</a> <span class=\"post-author\">Eric Neyman</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">8</td><td><a class=\"post-title\" href=\"/posts/K2D45BNxnZjdpSX2j/ai-timelines\">AI Timelines</a> <span class=\"post-author\">habryka</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">9</td><td><a class=\"post-title\" href=\"/posts/XPv4sYrKnPzeJASuk/basics-of-rationalist-discourse-1\">Basics of Rationalist Discourse</a> <span class=\"post-author\">Duncan Sabien (Deactivated)</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.9\" class=\"dot\" style=\"width:13.350000000000001px; height:13.350000000000001px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"8.6\" class=\"dot\" style=\"width:12.899999999999999px; height:12.899999999999999px; background:#5f9b65\"></span><span title=\"8.4\" class=\"dot\" style=\"width:12.600000000000001px; height:12.600000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">10</td><td><a class=\"post-title\" href=\"/posts/ejxwraMP5ye7Bgmpm/things-i-learned-by-spending-five-thousand-hours-in-non-ea\">Things I Learned by Spending Five Thousand Hours In Non-EA Charities</a> <span class=\"post-author\">jenn</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">11</td><td><a class=\"post-title\" href=\"/posts/Gc9FGtdXhK9sCSEYu/what-a-compute-centric-framework-says-about-ai-takeoff\">What a compute-centric framework says about AI takeoff speeds</a> <span class=\"post-author\">Tom Davidson</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">12</td><td><a class=\"post-title\" href=\"/posts/yT22RcWrxZcXyGjsA/how-to-have-polygenically-screened-children\">How to have Polygenically Screened Children</a> <span class=\"post-author\">GeneSmith</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.1\" class=\"dot\" style=\"width:12.149999999999999px; height:12.149999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">13</td><td><a class=\"post-title\" href=\"/posts/gvzW46Z3BsaZsLc25/natural-abstractions-key-claims-theorems-and-critiques-1\">Natural Abstractions: Key claims, Theorems, and Critiques</a> <span class=\"post-author\">LawrenceC</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"7.9\" class=\"dot\" style=\"width:11.850000000000001px; height:11.850000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">14</td><td><a class=\"post-title\" href=\"/posts/pYWA7hYJmXnuyby33/alignment-implications-of-llm-successes-a-debate-in-one-act\">Alignment Implications of LLM Successes: a Debate in One Act</a> <span class=\"post-author\">Zack_M_Davis</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.8\" class=\"dot\" style=\"width:13.200000000000001px; height:13.200000000000001px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"6.2\" class=\"dot\" style=\"width:9.3px; height:9.3px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">15</td><td><a class=\"post-title\" href=\"/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1\">Model Organisms of Misalignment: The Case for a New Pillar of Alignment Research</a> <span class=\"post-author\">evhub</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"6.2\" class=\"dot\" style=\"width:9.3px; height:9.3px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">16</td><td><a class=\"post-title\" href=\"/posts/dWQWzGCSFj6GTZHz7/natural-latents-the-math\">Natural Latents: The Math</a> <span class=\"post-author\">johnswentworth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"7.9\" class=\"dot\" style=\"width:11.850000000000001px; height:11.850000000000001px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">17</td><td><a class=\"post-title\" href=\"/posts/5spBue2z2tw4JuDCx/steering-gpt-2-xl-by-adding-an-activation-vector\">Steering GPT-2-XL by adding an activation vector</a> <span class=\"post-author\">TurnTrout</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">18</td><td><a class=\"post-title\" href=\"/posts/aPeJE8bSo6rAFoLqg/solidgoldmagikarp-plus-prompt-generation\">SolidGoldMagikarp (plus, prompt generation)</a> <span class=\"post-author\">Jessica Rumbelow</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">19</td><td><a class=\"post-title\" href=\"/posts/GJgudfEvNx8oeyffH/the-ants-and-the-grasshopper\">The ants and the grasshopper</a> <span class=\"post-author\">Richard_Ngo</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">20</td><td><a class=\"post-title\" href=\"/posts/pZrvkZzL2JnbRgEBC/feedbackloop-first-rationality\">Feedbackloop-first Rationality</a> <span class=\"post-author\">Raemon</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"8.6\" class=\"dot\" style=\"width:12.899999999999999px; height:12.899999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">21</td><td><a class=\"post-title\" href=\"/posts/XWwvwytieLtEWaFJX/deep-deceptiveness\">Deep Deceptiveness</a> <span class=\"post-author\">So8res</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.9\" class=\"dot\" style=\"width:13.350000000000001px; height:13.350000000000001px; background:#5f9b65\"></span><span title=\"8.1\" class=\"dot\" style=\"width:12.149999999999999px; height:12.149999999999999px; background:#5f9b65\"></span><span title=\"6.2\" class=\"dot\" style=\"width:9.3px; height:9.3px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">22</td><td><a class=\"post-title\" href=\"/posts/aW288uWABwTruBmgF/ea-vegan-advocacy-is-not-truthseeking-and-it-s-everyone-s-1\">EA Vegan Advocacy is not truthseeking, and it’s everyone’s problem</a> <span class=\"post-author\">Elizabeth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"7.9\" class=\"dot\" style=\"width:11.850000000000001px; height:11.850000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">23</td><td><a class=\"post-title\" href=\"/posts/jRf4WENQnhssCb6mJ/davidad-s-bold-plan-for-alignment-an-in-depth-explanation\">Davidad's Bold Plan for Alignment: An In-Depth Explanation</a> <span class=\"post-author\">Charbel-Raphaël</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">24</td><td><a class=\"post-title\" href=\"/posts/gPPdYTwPkdvAEYyEK/fucking-goddamn-basics-of-rationalist-discourse\">Fucking Goddamn Basics of Rationalist Discourse</a> <span class=\"post-author\">LoganStrohl</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.6\" class=\"dot\" style=\"width:12.899999999999999px; height:12.899999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">25</td><td><a class=\"post-title\" href=\"/posts/LNA8mubrByG7SFacm/against-almost-every-theory-of-impact-of-interpretability-1\">Against Almost Every Theory of Impact of Interpretability</a> <span class=\"post-author\">Charbel-Raphaël</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">26</td><td><a class=\"post-title\" href=\"/posts/HJNtrNHf688FoHsHM/guide-to-rationalist-interior-decorating\">Guide to rationalist interior decorating</a> <span class=\"post-author\">mingyuan</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">27</td><td><a class=\"post-title\" href=\"/posts/yFofRxg7RRQYCcwFA/new-report-scheming-ais-will-ais-fake-alignment-during\">New report: \"Scheming AIs: Will AIs fake alignment during training in order to get power?\"</a> <span class=\"post-author\">Joe Carlsmith</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">28</td><td><a class=\"post-title\" href=\"/posts/bHozHrQD4qxvKdfqq/predictable-updating-about-ai-risk\">Predictable updating about AI risk</a> <span class=\"post-author\">Joe Carlsmith</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"6.2\" class=\"dot\" style=\"width:9.3px; height:9.3px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">29</td><td><a class=\"post-title\" href=\"/posts/pk9mofif2jWbc6Tv3/fiction-a-disneyland-without-children\">[Fiction] A Disneyland Without Children</a> <span class=\"post-author\">L Rudolf L</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.8\" class=\"dot\" style=\"width:13.200000000000001px; height:13.200000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">30</td><td><a class=\"post-title\" href=\"/posts/yA8DWsHJeFZhDcQuo/the-talk-a-brief-explanation-of-sexual-dimorphism\">The Talk: a brief explanation of sexual dimorphism</a> <span class=\"post-author\">Malmesbury</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">31</td><td><a class=\"post-title\" href=\"/posts/thkAtqoQwN6DtaiGT/carefully-bootstrapped-alignment-is-organizationally-hard\">\"Carefully Bootstrapped Alignment\" is organizationally hard</a> <span class=\"post-author\">Raemon</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">32</td><td><a class=\"post-title\" href=\"/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies\">Tuning your Cognitive Strategies</a> <span class=\"post-author\">Raemon</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"8.4\" class=\"dot\" style=\"width:12.600000000000001px; height:12.600000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">33</td><td><a class=\"post-title\" href=\"/posts/zidQmfFhMgwFzcHhs/enemies-vs-malefactors\">Enemies vs Malefactors</a> <span class=\"post-author\">So8res</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">34</td><td><a class=\"post-title\" href=\"/posts/LzQtrHSYDafXynofq/the-parable-of-the-king-and-the-random-process\">The Parable of the King and the Random Process</a> <span class=\"post-author\">moridinamael</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">35</td><td><a class=\"post-title\" href=\"/posts/nH4c3Q9t9F3nJ7y8W/gpts-are-predictors-not-imitators\">GPTs are Predictors, not Imitators</a> <span class=\"post-author\">Eliezer Yudkowsky</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.6\" class=\"dot\" style=\"width:12.899999999999999px; height:12.899999999999999px; background:#5f9b65\"></span><span title=\"8.1\" class=\"dot\" style=\"width:12.149999999999999px; height:12.149999999999999px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"6.2\" class=\"dot\" style=\"width:9.3px; height:9.3px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">36</td><td><a class=\"post-title\" href=\"/posts/6HEYbsqk35butCYTe/labs-should-be-explicit-about-why-they-are-building-agi\">Labs should be explicit about why they are building AGI</a> <span class=\"post-author\">peterbarnett</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">37</td><td><a class=\"post-title\" href=\"/posts/R5yL6oZxqJfmqnuje/cultivating-a-state-of-mind-where-new-ideas-are-born\">Cultivating a state of mind where new ideas are born</a> <span class=\"post-author\">Henrik Karlsson</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">38</td><td><a class=\"post-title\" href=\"/posts/iy2o4nQj9DnQD7Yhj/discussion-with-nate-soares-on-a-key-alignment-difficulty\">Discussion with Nate Soares on a key alignment difficulty</a> <span class=\"post-author\">HoldenKarnofsky</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">39</td><td><a class=\"post-title\" href=\"/posts/bkfgTSHhm3mqxgTmw/loudly-give-up-don-t-quietly-fade\">Loudly Give Up, Don't Quietly Fade</a> <span class=\"post-author\">Screwtape</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.6\" class=\"dot\" style=\"width:12.899999999999999px; height:12.899999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">40</td><td><a class=\"post-title\" href=\"/posts/wB7hdo4LDdhZ7kwJw/we-don-t-trade-with-ants\">We don’t trade with ants</a> <span class=\"post-author\">KatjaGrace</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">41</td><td><a class=\"post-title\" href=\"/posts/fovfuFdpuEwQzJu2w/neural-networks-generalize-because-of-this-one-weird-trick\">Neural networks generalize because of this one weird trick</a> <span class=\"post-author\">Jesse Hoogland</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">42</td><td><a class=\"post-title\" href=\"/posts/xWMqsvHapP3nwdSW8/my-views-on-doom\">My views on “doom”</a> <span class=\"post-author\">paulfchristiano</span></td><td><div class=\"dots-row\"><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">43</td><td><a class=\"post-title\" href=\"/posts/Wiz4eKi5fsomRsMbx/change-my-mind-veganism-entails-trade-offs-and-health-is-one\">Change my mind: Veganism entails trade-offs, and health is one of the axes</a> <span class=\"post-author\">Elizabeth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"8.1\" class=\"dot\" style=\"width:12.149999999999999px; height:12.149999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">44</td><td><a class=\"post-title\" href=\"/posts/f3kM7NM5eGMTp3KtZ/lessons-on-how-to-get-things-right-on-the-first-try\">Lessons On How To Get Things Right On The First Try</a> <span class=\"post-author\">johnswentworth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.6\" class=\"dot\" style=\"width:12.899999999999999px; height:12.899999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">45</td><td><a class=\"post-title\" href=\"/posts/zaaGsFBeDTpCsYHef/shallow-review-of-live-agendas-in-alignment-and-safety\">Shallow review of live agendas in alignment &amp; safety</a> <span class=\"post-author\">technicalities</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"6.2\" class=\"dot\" style=\"width:9.3px; height:9.3px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">46</td><td><a class=\"post-title\" href=\"/posts/ZwshvqiqCvXPsZEct/the-learning-theoretic-agenda-status-2023\">The Learning-Theoretic Agenda: Status 2023</a> <span class=\"post-author\">Vanessa Kosoy</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">47</td><td><a class=\"post-title\" href=\"/posts/F6HSHzKezkh6aoTr2/improving-the-welfare-of-ais-a-nearcasted-proposal\">Improving the Welfare of AIs: A Nearcasted Proposal</a> <span class=\"post-author\">ryan_greenblatt</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">48</td><td><a class=\"post-title\" href=\"/posts/AocXh6gJ9tJC2WyCL/book-review-going-infinite\">Book Review: Going Infinite</a> <span class=\"post-author\">Zvi</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.9\" class=\"dot\" style=\"width:13.350000000000001px; height:13.350000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">49</td><td><a class=\"post-title\" href=\"/posts/2sLwt2cSAag74nsdN/speaking-to-congressional-staffers-about-ai-risk\">Speaking to Congressional staffers about AI risk</a> <span class=\"post-author\">Zuko's Shame</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">50</td><td><a class=\"post-title\" href=\"/posts/NyiFLzSrkfkDW4S7o/why-it-s-so-hard-to-talk-about-consciousness\">Why it's so hard to talk about Consciousness</a> <span class=\"post-author\">Rafael Harth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.4\" class=\"dot\" style=\"width:12.600000000000001px; height:12.600000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">51</td><td><a class=\"post-title\" href=\"/posts/3RSq3bfnzuL3sp46J/acausal-normalcy\">Acausal normalcy</a> <span class=\"post-author\">Andrew_Critch</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">52</td><td><a class=\"post-title\" href=\"/posts/TjaeCWvLZtEDAS5Ex/towards-developmental-interpretability\">Towards Developmental Interpretability</a> <span class=\"post-author\">Jesse Hoogland</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">53</td><td><a class=\"post-title\" href=\"/posts/uGDtroD26aLvHSoK2/dear-self-we-need-to-talk-about-ambition-1\">Dear Self; we need to talk about ambition</a> <span class=\"post-author\">Elizabeth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">54</td><td><a class=\"post-title\" href=\"/posts/YyosBAutg4bzScaLu/thoughts-on-ai-is-easy-to-control-by-pope-and-belrose\">Thoughts on “AI is easy to control” by Pope &amp; Belrose</a> <span class=\"post-author\">Steven Byrnes</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">55</td><td><a class=\"post-title\" href=\"/posts/fRSj2W4Fjje8rQWm9/thoughts-on-sharing-information-about-language-model\">Thoughts on sharing information about language model capabilities</a> <span class=\"post-author\">paulfchristiano</span></td><td><div class=\"dots-row\"><span title=\"7.9\" class=\"dot\" style=\"width:11.850000000000001px; height:11.850000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">56</td><td><a class=\"post-title\" href=\"/posts/8dbimB7EJXuYxmteW/fixdt\">FixDT</a> <span class=\"post-author\">abramdemski</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">57</td><td><a class=\"post-title\" href=\"/posts/pDzdb4smpzT3Lwbym/my-model-of-ea-burnout\">My Model Of EA Burnout</a> <span class=\"post-author\">LoganStrohl</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">58</td><td><a class=\"post-title\" href=\"/posts/QBeN49SoKpDMX3kKk/accidentally-load-bearing\">Accidentally Load Bearing</a> <span class=\"post-author\">jefftk</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">59</td><td><a class=\"post-title\" href=\"/posts/gQyphPbaLHBMJoghD/comp-sci-in-2027-short-story-by-eliezer-yudkowsky\">Comp Sci in 2027 (Short story by Eliezer Yudkowsky)</a> <span class=\"post-author\">sudo</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">60</td><td><a class=\"post-title\" href=\"/posts/HfqbjwpAEGep9mHhc/the-plan-2023-version\">The Plan - 2023 Version</a> <span class=\"post-author\">johnswentworth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"7.9\" class=\"dot\" style=\"width:11.850000000000001px; height:11.850000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">61</td><td><a class=\"post-title\" href=\"/posts/fuSaKr6t6Zuh6GKaQ/when-is-goodhart-catastrophic\">When is Goodhart catastrophic?</a> <span class=\"post-author\">Drake Thomas</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"7.9\" class=\"dot\" style=\"width:11.850000000000001px; height:11.850000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">62</td><td><a class=\"post-title\" href=\"/posts/i5kijcjFJD6bn7dwq/evaluating-the-historical-value-misspecification-argument\">Evaluating the historical value misspecification argument</a> <span class=\"post-author\">Matthew Barnett</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.8\" class=\"dot\" style=\"width:13.200000000000001px; height:13.200000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">63</td><td><a class=\"post-title\" href=\"/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast\">Contrast Pairs Drive the Empirical Performance of Contrast Consistent Search (CCS)</a> <span class=\"post-author\">Scott Emmons</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">64</td><td><a class=\"post-title\" href=\"/posts/jtoPawEhLNXNxvgTT/bing-chat-is-blatantly-aggressively-misaligned\">Bing Chat is blatantly, aggressively misaligned</a> <span class=\"post-author\">evhub</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">65</td><td><a class=\"post-title\" href=\"/posts/bxt7uCiHam4QXrQAA/cyborgism\">Cyborgism</a> <span class=\"post-author\">NicholasKees</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">66</td><td><a class=\"post-title\" href=\"/posts/zqmAMst8hmsdJqrpR/shell-games\">Shell games</a> <span class=\"post-author\">TsviBT</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">67</td><td><a class=\"post-title\" href=\"/posts/EjgfreeibTXRx9Ham/ten-levels-of-ai-alignment-difficulty\">Ten Levels of AI Alignment Difficulty</a> <span class=\"post-author\">Sammy Martin</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">68</td><td><a class=\"post-title\" href=\"/posts/yS3d46m23wRKDQobt/introducing-fatebook-the-fastest-way-to-make-and-track\">Introducing Fatebook: the fastest way to make and track predictions</a> <span class=\"post-author\">Adam B</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">69</td><td><a class=\"post-title\" href=\"/posts/2WpPRrqrFQa6n2x3W/modal-fixpoint-cooperation-without-loeb-s-theorem\">Modal Fixpoint Cooperation without Löb's Theorem</a> <span class=\"post-author\">Andrew_Critch</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">70</td><td><a class=\"post-title\" href=\"/posts/CTBta9i8sav7tjC2r/how-to-hopefully-ethically-make-money-off-of-agi\">How to (hopefully ethically) make money off of AGI</a> <span class=\"post-author\">habryka</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">71</td><td><a class=\"post-title\" href=\"/posts/CvfZrrEokjCu3XHXp/ai-practical-advice-for-the-worried\">AI: Practical Advice for the Worried</a> <span class=\"post-author\">Zvi</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">72</td><td><a class=\"post-title\" href=\"/posts/As7bjEAbNpidKx6LR/valence-series-1-introduction\">[Valence series] 1. Introduction</a> <span class=\"post-author\">Steven Byrnes</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">73</td><td><a class=\"post-title\" href=\"/posts/TDqvQFks6TWutJEKu/towards-monosemanticity-decomposing-language-models-with\">Towards Monosemanticity: Decomposing Language Models With Dictionary Learning</a> <span class=\"post-author\">Zac Hatfield-Dodds</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">74</td><td><a class=\"post-title\" href=\"/posts/895Qmhyud2PjDhte6/responses-to-apparent-rationalist-confusions-about-game\">Responses to apparent rationalist confusions about game / decision theory</a> <span class=\"post-author\">Anthony DiGiovanni</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">75</td><td><a class=\"post-title\" href=\"/posts/dkchEFnM9EYpW3bjK/why-and-how-to-graduate-early-u-s\">Why and How to Graduate Early [U.S.]</a> <span class=\"post-author\">Tego</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"7.9\" class=\"dot\" style=\"width:11.850000000000001px; height:11.850000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">76</td><td><a class=\"post-title\" href=\"/posts/RydETq379eoWqBFvj/updates-and-reflections-on-optimal-exercise-after-nearly-a\">Updates and Reflections on Optimal Exercise after Nearly a Decade</a> <span class=\"post-author\">romeostevensit</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">77</td><td><a class=\"post-title\" href=\"/posts/MveBB3aCz3k2jzgJs/getting-started-with-naturalism\">Getting Started With Naturalism</a> <span class=\"post-author\">LoganStrohl</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.9\" class=\"dot\" style=\"width:13.350000000000001px; height:13.350000000000001px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">78</td><td><a class=\"post-title\" href=\"/posts/bzmLC3J8PsknwRZbr/why-not-subagents\">Why Not Subagents?</a> <span class=\"post-author\">johnswentworth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">79</td><td><a class=\"post-title\" href=\"/posts/MbWWKbyD5gLhJgfwn/meta-level-adversarial-evaluation-of-oversight-techniques-1\">Meta-level adversarial evaluation of oversight techniques might allow robust measurement of their adequacy</a> <span class=\"post-author\">Buck</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">80</td><td><a class=\"post-title\" href=\"/posts/wnkGXcAq4DCgY8HqA/a-case-for-ai-alignment-being-difficult\">A case for AI alignment being difficult</a> <span class=\"post-author\">jessicata</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">81</td><td><a class=\"post-title\" href=\"/posts/KpD2fJa6zo8o2MBxg/consciousness-as-a-conflationary-alliance-term-for\">Consciousness as a conflationary alliance term for intrinsically valued internal experiences</a> <span class=\"post-author\">Andrew_Critch</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">82</td><td><a class=\"post-title\" href=\"/posts/ydmctK2qLyrR9Xztd/the-101-space-you-will-always-have-with-you\">The 101 Space You Will Always Have With You</a> <span class=\"post-author\">Screwtape</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">83</td><td><a class=\"post-title\" href=\"/posts/WCj7WgFSLmyKaMwPR/coup-probes-catching-catastrophes-with-probes-trained-off\">Coup probes: Catching catastrophes with probes trained off-policy</a> <span class=\"post-author\">Fabien Roger</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">84</td><td><a class=\"post-title\" href=\"/posts/9Fdd9N7Escg3tcymb/preventing-language-models-from-hiding-their-reasoning\">Preventing Language Models from hiding their reasoning</a> <span class=\"post-author\">Fabien Roger</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">85</td><td><a class=\"post-title\" href=\"/posts/WGEPBmErv8ufrq8Fc/teleosemantics\">Teleosemantics!</a> <span class=\"post-author\">abramdemski</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">86</td><td><a class=\"post-title\" href=\"/posts/wAczufCpMdaamF9fy/my-objections-to-we-re-all-gonna-die-with-eliezer-yudkowsky\">My Objections to \"We’re All Gonna Die with Eliezer Yudkowsky\"</a> <span class=\"post-author\">Quintin Pope</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#bf360c\"></span><span title=\"-3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">87</td><td><a class=\"post-title\" href=\"/posts/3gAccKDW6nRKFumpP/why-not-just-outsource-alignment-research-to-an-ai\">Why Not Just Outsource Alignment Research To An AI?</a> <span class=\"post-author\">johnswentworth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">88</td><td><a class=\"post-title\" href=\"/posts/CYN7swrefEss4e3Qe/childhoods-of-exceptional-people\">Childhoods of exceptional people</a> <span class=\"post-author\">Henrik Karlsson</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">89</td><td><a class=\"post-title\" href=\"/posts/khFC2a4pLPvGtXAGG/how-to-catch-an-ai-liar-lie-detection-in-black-box-llms-by\">How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions</a> <span class=\"post-author\">JanB</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.8\" class=\"dot\" style=\"width:13.200000000000001px; height:13.200000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">90</td><td><a class=\"post-title\" href=\"/posts/tHvFtfFKjhfy3sQpC/the-soul-key\">The Soul Key</a> <span class=\"post-author\">Richard_Ngo</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">91</td><td><a class=\"post-title\" href=\"/posts/jruPqRkyFjbariAKL/never-drop-a-ball\">Never Drop A Ball</a> <span class=\"post-author\">Screwtape</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">92</td><td><a class=\"post-title\" href=\"/posts/bF353RHmuzFQcsokF/cohabitive-games-so-far\">Cohabitive Games so Far</a> <span class=\"post-author\">mako yass</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.8\" class=\"dot\" style=\"width:13.200000000000001px; height:13.200000000000001px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">93</td><td><a class=\"post-title\" href=\"/posts/LHAJuYy453YwiKFt5/the-salt-in-pasta-water-fallacy\">The salt in pasta water fallacy</a> <span class=\"post-author\">Thomas Sepulchre</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">94</td><td><a class=\"post-title\" href=\"/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models\">Untrusted smart models and trusted dumb models</a> <span class=\"post-author\">Buck</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">95</td><td><a class=\"post-title\" href=\"/posts/XHtygebvHoJSSeNPP/some-rules-for-an-algebra-of-bayes-nets\">Some Rules for an Algebra of Bayes Nets</a> <span class=\"post-author\">johnswentworth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">96</td><td><a class=\"post-title\" href=\"/posts/rvpEF2mBLeZE9j53n/how-to-bounded-distrust\">How to Bounded Distrust</a> <span class=\"post-author\">Zvi</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">97</td><td><a class=\"post-title\" href=\"/posts/gGSvwd62TJAxxhcGh/yudkowsky-vs-hanson-on-foom-whose-predictions-were-better\">Yudkowsky vs Hanson on FOOM: Whose Predictions Were Better?</a> <span class=\"post-author\">1a3orn</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">98</td><td><a class=\"post-title\" href=\"/posts/5sWNnbHRkExfLaS49/before-smart-ai-there-will-be-many-mediocre-or-specialized\">Before smart AI, there will be many mediocre or specialized AIs</a> <span class=\"post-author\">Lukas Finnveden</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">99</td><td><a class=\"post-title\" href=\"/posts/BpTDJj6TrqGYTjFcZ/a-golden-age-of-building-excerpts-and-lessons-from-empire\">A Golden Age of Building? Excerpts and lessons from Empire State, Pentagon, Skunk Works and SpaceX</a> <span class=\"post-author\">Bird Concept</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">100</td><td><a class=\"post-title\" href=\"/posts/mSeesg7i4d9scWAet/apocalypse-insurance-and-the-hardline-libertarian-take-on-ai\">Apocalypse insurance, and the hardline libertarian take on AI risk</a> <span class=\"post-author\">So8res</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">101</td><td><a class=\"post-title\" href=\"/posts/mfFn46AEiBL5EzaXr/a-freshman-year-during-the-ai-midgame-my-approach-to-the\">A freshman year during the AI midgame: my approach to the next year</a> <span class=\"post-author\">Buck</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">102</td><td><a class=\"post-title\" href=\"/posts/4S6zunFNFY3f5JYxt/aumann-agreement-is-common\">Aumann-agreement is common</a> <span class=\"post-author\">tailcalled</span></td><td><div class=\"dots-row\"><span title=\"8.8\" class=\"dot\" style=\"width:13.200000000000001px; height:13.200000000000001px; background:#5f9b65\"></span><span title=\"7.9\" class=\"dot\" style=\"width:11.850000000000001px; height:11.850000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">103</td><td><a class=\"post-title\" href=\"/posts/FijbeqdovkgAusGgz/grey-goo-is-unlikely\">grey goo is unlikely</a> <span class=\"post-author\">bhauth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">104</td><td><a class=\"post-title\" href=\"/posts/6frs5xTkeLc9vZSRN/iron-deficiencies-are-very-bad-and-you-should-treat-them\">Iron deficiencies are very bad and you should treat them</a> <span class=\"post-author\">Elizabeth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">105</td><td><a class=\"post-title\" href=\"/posts/xDNyXGCDephBuNF8c/dark-forest-theories\">Dark Forest Theories</a> <span class=\"post-author\">Raemon</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">106</td><td><a class=\"post-title\" href=\"/posts/ybmDkJAj3rdrrauuu/connectomics-seems-great-from-an-ai-x-risk-perspective\">Connectomics seems great from an AI x-risk perspective</a> <span class=\"post-author\">Steven Byrnes</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">107</td><td><a class=\"post-title\" href=\"/posts/sTDfraZab47KiRMmT/views-on-when-agi-comes-and-on-strategy-to-reduce\">Views on when AGI comes and on strategy to reduce existential risk </a><span class=\"post-author\">TsviBT</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">108</td><td><a class=\"post-title\" href=\"/posts/qAdDzcBuDBLexb4fC/the-neglected-approaches-approach-ae-studio-s-alignment\">The 'Neglected Approaches' Approach: AE Studio's Alignment Agenda</a> <span class=\"post-author\">Cameron Berg</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.7\" class=\"dot\" style=\"width:13.049999999999999px; height:13.049999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">109</td><td><a class=\"post-title\" href=\"/posts/WJtq4DoyT9ovPyHjH/thinking-by-the-clock\">Thinking By The Clock</a> <span class=\"post-author\">Screwtape</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">110</td><td><a class=\"post-title\" href=\"/posts/JcgtKunqmELefxksx/killing-socrates\">Killing Socrates</a> <span class=\"post-author\">Duncan Sabien (Deactivated)</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.6\" class=\"dot\" style=\"width:12.899999999999999px; height:12.899999999999999px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-7.9\" class=\"dot\" style=\"width:11.850000000000001px; height:11.850000000000001px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">111</td><td><a class=\"post-title\" href=\"/posts/niPHLviGAWzzZTzit/you-don-t-get-to-have-cool-flaws\">You don't get to have cool flaws</a> <span class=\"post-author\">Neil</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">112</td><td><a class=\"post-title\" href=\"/posts/woCPxs8GxE7H35zzK/noting-an-error-in-inadequate-equilibria\">Noting an error in Inadequate Equilibria</a> <span class=\"post-author\">Matthew Barnett</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">113</td><td><a class=\"post-title\" href=\"/posts/e4GBj6jxRZcsHFSvP/assume-bad-faith\">Assume Bad Faith</a> <span class=\"post-author\">Zack_M_Davis</span></td><td><div class=\"dots-row\"><span title=\"8.8\" class=\"dot\" style=\"width:13.200000000000001px; height:13.200000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">114</td><td><a class=\"post-title\" href=\"/posts/BKvJNzALpxS3LafEs/measuring-and-improving-the-faithfulness-of-model-generated\">Measuring and Improving the Faithfulness of Model-Generated Reasoning </a><span class=\"post-author\">Ansh Radhakrishnan</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">115</td><td><a class=\"post-title\" href=\"/posts/dBmfb76zx6wjPsBC7/when-can-we-trust-model-evaluations\">When can we trust model evaluations?</a> <span class=\"post-author\">evhub</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">116</td><td><a class=\"post-title\" href=\"/posts/inALbAqdx63KTaGgs/benchmarks-for-detecting-measurement-tampering-redwood\">Benchmarks for Detecting Measurement Tampering [Redwood Research]</a> <span class=\"post-author\">ryan_greenblatt</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">117</td><td><a class=\"post-title\" href=\"/posts/fsGEyCYhqs7AWwdCe/learning-theoretic-agenda-reading-list\">Learning-theoretic agenda reading list</a> <span class=\"post-author\">Vanessa Kosoy</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">118</td><td><a class=\"post-title\" href=\"/posts/PiPH4gkcMuvLALymK/exercise-solve-thinking-physics\">Exercise: Solve \"Thinking Physics\"</a> <span class=\"post-author\">Raemon</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">119</td><td><a class=\"post-title\" href=\"/posts/YEioD8YLgxih3ydxP/why-simulator-ais-want-to-be-active-inference-ais\">Why Simulator AIs want to be Active Inference AIs</a> <span class=\"post-author\">Jan_Kulveit</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">120</td><td><a class=\"post-title\" href=\"/posts/6FkWnktH3mjMAxdRT/what-i-would-do-if-i-wasn-t-at-arc-evals\">What I would do if I wasn’t at ARC Evals</a> <span class=\"post-author\">LawrenceC</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">121</td><td><a class=\"post-title\" href=\"/posts/HACcn8roty9KBAWzZ/evaluations-of-new-ai-safety-researchers-can-be-noisy\">Evaluations (of new AI Safety researchers) can be noisy</a> <span class=\"post-author\">LawrenceC</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">122</td><td><a class=\"post-title\" href=\"/posts/bteq4hMW2hqtKE49d/the-king-and-the-golem\">The King and the Golem</a> <span class=\"post-author\">Richard_Ngo</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">123</td><td><a class=\"post-title\" href=\"/posts/BsAt7bhGJqgsi7vpo/love-reverence-and-life\">Love, Reverence, and Life</a> <span class=\"post-author\">Elizabeth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">124</td><td><a class=\"post-title\" href=\"/posts/hirhSqvEAq7pdnyPG/auditing-failures-vs-concentrated-failures\">Auditing failures vs concentrated failures</a> <span class=\"post-author\">ryan_greenblatt</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">125</td><td><a class=\"post-title\" href=\"/posts/svuBpoSduzhYjFPrA/elements-of-rationalist-discourse\">Elements of Rationalist Discourse</a> <span class=\"post-author\">Rob Bensinger</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">126</td><td><a class=\"post-title\" href=\"/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn\">Evolution provides no evidence for the sharp left turn</a> <span class=\"post-author\">Quintin Pope</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-8.9\" class=\"dot\" style=\"width:13.350000000000001px; height:13.350000000000001px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">127</td><td><a class=\"post-title\" href=\"/posts/ufW5LvcwDuL6qjdBT/latent-variables-for-prediction-markets-motivation-technical\">Latent variables for prediction markets: motivation, technical guide, and design considerations</a> <span class=\"post-author\">tailcalled</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">128</td><td><a class=\"post-title\" href=\"/posts/t5W87hQF5gKyTofQB/ufo-betting-put-up-or-shut-up\">UFO Betting: Put Up or Shut Up</a> <span class=\"post-author\">RatsWrongAboutUAP</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">129</td><td><a class=\"post-title\" href=\"/posts/zswuToWK6zpYSwmCn/some-background-for-reasoning-about-dual-use-alignment\">Some background for reasoning about dual-use alignment research</a> <span class=\"post-author\">Charlie Steiner</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"8.8\" class=\"dot\" style=\"width:13.200000000000001px; height:13.200000000000001px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">130</td><td><a class=\"post-title\" href=\"/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization\">Scalable Oversight and Weak-to-Strong Generalization: Compatible approaches to the same problem</a> <span class=\"post-author\">Ansh Radhakrishnan</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">131</td><td><a class=\"post-title\" href=\"/posts/DfQ3Ls45WnYuguqfc/the-base-rate-times-news-through-prediction-markets\">The Base Rate Times, news through prediction markets</a> <span class=\"post-author\">vandemonian</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">132</td><td><a class=\"post-title\" href=\"/posts/myJ5RbHwtxLzvuwXb/mob-and-bailey\">Mob and Bailey</a> <span class=\"post-author\">Screwtape</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">133</td><td><a class=\"post-title\" href=\"/posts/8F4dXYriqbsom46x5/pretraining-language-models-with-human-preferences\">Pretraining Language Models with Human Preferences</a> <span class=\"post-author\">Tomek Korbak</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">134</td><td><a class=\"post-title\" href=\"/posts/fqryrxnvpSr5w2dDJ/touch-reality-as-soon-as-possible-when-doing-machine\">Touch reality as soon as possible (when doing machine learning research)</a> <span class=\"post-author\">LawrenceC</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">135</td><td><a class=\"post-title\" href=\"/posts/xsB3dDg5ubqnT7nsn/poc-or-or-gtfo-culture-as-partial-antidote-to-alignment\">POC || GTFO culture as partial antidote to alignment wordcelism</a> <span class=\"post-author\">lc</span></td><td><div class=\"dots-row\"><span title=\"7.9\" class=\"dot\" style=\"width:11.850000000000001px; height:11.850000000000001px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">136</td><td><a class=\"post-title\" href=\"/posts/JByMmDjnBLnHSqhkS/a-to-z-of-things\">A to Z of things</a> <span class=\"post-author\">KatjaGrace</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">137</td><td><a class=\"post-title\" href=\"/posts/8SjnKxjLniCAmcjnG/openai-deepmind-anthropic-etc-should-shut-down\">OpenAI, DeepMind, Anthropic, etc. should shut down.</a> <span class=\"post-author\">Tamsin Leake</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">138</td><td><a class=\"post-title\" href=\"/posts/mkDs86ciwwjnpyeZW/model-care-execution\">Model, Care, Execution</a> <span class=\"post-author\">Ricki Heicklen</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">139</td><td><a class=\"post-title\" href=\"/posts/rMZvtrrcfwTMy685K/symbol-referent-confusions-in-language-model-alignment\">Symbol/Referent Confusions in Language Model Alignment Experiments</a> <span class=\"post-author\">johnswentworth</span></td><td><div class=\"dots-row\"><span title=\"8.1\" class=\"dot\" style=\"width:12.149999999999999px; height:12.149999999999999px; background:#5f9b65\"></span><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">140</td><td><a class=\"post-title\" href=\"/posts/uRyKkyYstxZkCNcoP/careless-talk-on-us-china-ai-competition-and-criticism-of\">Careless talk on US-China AI competition? (and criticism of CAIS coverage)</a> <span class=\"post-author\">Oliver Sourbut</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">141</td><td><a class=\"post-title\" href=\"/posts/rwkkcgSpnAyE8oNo3/alexander-and-yudkowsky-on-agi-goals\">Alexander and Yudkowsky on AGI goals</a> <span class=\"post-author\">Scott Alexander</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">142</td><td><a class=\"post-title\" href=\"/posts/6dn6hnFRgqqWJbwk9/deception-chess-game-1\">Deception Chess: Game #1</a> <span class=\"post-author\">Zane</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">143</td><td><a class=\"post-title\" href=\"/posts/ikoi5PcyZBw5v8pbt/ethodynamics-of-omelas\">Ethodynamics of Omelas</a> <span class=\"post-author\">dr_s</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">144</td><td><a class=\"post-title\" href=\"/posts/JpM9Gi9frSENjexEr/here-s-why-i-m-hesitant-to-respond-in-more-depth\">Here's Why I'm Hesitant To Respond In More Depth</a> <span class=\"post-author\">DirectedEvolution</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">145</td><td><a class=\"post-title\" href=\"/posts/psYNRb3JCncQBjd4v/shutting-down-the-lightcone-offices\">Shutting Down the Lightcone Offices</a> <span class=\"post-author\">habryka</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">146</td><td><a class=\"post-title\" href=\"/posts/LwvfYGZTSud7Rsuyw/shoes-with-springs\">shoes with springs</a> <span class=\"post-author\">bhauth</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">147</td><td><a class=\"post-title\" href=\"/posts/uauTcRiLseDyh49YL/would-you-work-harder-in-the-least-convenient-possible-world\">Would You Work Harder In The Least Convenient Possible World? </a><span class=\"post-author\">Firinn</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">148</td><td><a class=\"post-title\" href=\"/posts/qPrPNakJBq23muf4n/bayesian-networks-aren-t-necessarily-causal\">Bayesian Networks Aren't Necessarily Causal</a> <span class=\"post-author\">Zack_M_Davis</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">149</td><td><a class=\"post-title\" href=\"/posts/EsxowsJsRopTGALCX/one-day-sooner\">One Day Sooner</a> <span class=\"post-author\">Screwtape</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">150</td><td><a class=\"post-title\" href=\"/posts/LKAogXdruuZXdx6ZH/publish-or-perish-a-quick-note-on-why-you-should-try-to-make\">\"Publish or Perish\" (a quick note on why you should try to make your work legible to existing academic communities)</a> <span class=\"post-author\">David Scott Krueger (formerly: capybaralet)</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-6.2\" class=\"dot\" style=\"width:9.3px; height:9.3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">151</td><td><a class=\"post-title\" href=\"/posts/6tjHf5ykvFqaNCErH/anthropic-s-responsible-scaling-policy-and-long-term-benefit\">Anthropic's Responsible Scaling Policy &amp; Long-Term Benefit Trust</a> <span class=\"post-author\">Zac Hatfield-Dodds</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">152</td><td><a class=\"post-title\" href=\"/posts/rmfjo4Wmtgq8qa2B7/think-carefully-before-calling-rl-policies-agents\">Think carefully before calling RL policies \"agents\"</a> <span class=\"post-author\">TurnTrout</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">153</td><td><a class=\"post-title\" href=\"/posts/bNKcKb8LYaQe49kWF/going-crazy-and-getting-better-again\">Going Crazy and Getting Better Again</a> <span class=\"post-author\">Evenstar</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">154</td><td><a class=\"post-title\" href=\"/posts/2ujT9renJwdrcBqcE/the-benevolence-of-the-butcher\">The benevolence of the butcher</a> <span class=\"post-author\">dr_s</span></td><td><div class=\"dots-row\"><span title=\"8.8\" class=\"dot\" style=\"width:13.200000000000001px; height:13.200000000000001px; background:#5f9b65\"></span><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">155</td><td><a class=\"post-title\" href=\"/posts/Ypo9kNpp45EELgEdK/nietzsche-s-morality-in-plain-english\">Nietzsche's Morality in Plain English</a> <span class=\"post-author\">Arjun Panickssery</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">156</td><td><a class=\"post-title\" href=\"/posts/inARBH5DwQTrvvRj8/cryonics-and-regret\">Cryonics and Regret</a> <span class=\"post-author\">MvB</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">157</td><td><a class=\"post-title\" href=\"/posts/mcnWZBnbeDz7KKtjJ/rsps-are-pauses-done-right\">RSPs are pauses done right</a> <span class=\"post-author\">evhub</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">158</td><td><a class=\"post-title\" href=\"/posts/GkC6YTu4DWp2zwf9k/giant-in-scrutable-matrices-maybe-the-best-of-all-possible\">Giant (In)scrutable Matrices: (Maybe) the Best of All Possible Worlds</a> <span class=\"post-author\">1a3orn</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-8.1\" class=\"dot\" style=\"width:12.149999999999999px; height:12.149999999999999px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">159</td><td><a class=\"post-title\" href=\"/posts/MArdnet7pwgALaeKs/why-i-m-not-into-the-free-energy-principle\">Why I’m not into the Free Energy Principle</a> <span class=\"post-author\">Steven Byrnes</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">160</td><td><a class=\"post-title\" href=\"/posts/jkY6QdCfAXHJk3kea/the-petertodd-phenomenon\">The ‘ petertodd’ phenomenon</a> <span class=\"post-author\">mwatkins</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">161</td><td><a class=\"post-title\" href=\"/posts/ftPQugx2726zAL2Ff/which-personality-traits-are-real-stress-testing-the-lexical\">Which personality traits are real? Stress-testing the lexical hypothesis</a> <span class=\"post-author\">tailcalled</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">162</td><td><a class=\"post-title\" href=\"/posts/ibaCBwfnehYestpi5/green-goo-is-plausible\">Green goo is plausible</a> <span class=\"post-author\">anithite</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">163</td><td><a class=\"post-title\" href=\"/posts/TkwJupKzDpGHLxoG6/rational-unilateralists-aren-t-so-cursed\">Rational Unilateralists Aren't So Cursed</a> <span class=\"post-author\">SCP</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">164</td><td><a class=\"post-title\" href=\"/posts/9qJYdFkhNQimvmjBF/competitive-cooperative-and-cohabitive\">Competitive, Cooperative, and Cohabitive</a> <span class=\"post-author\">Screwtape</span></td><td><div class=\"dots-row\"><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">165</td><td><a class=\"post-title\" href=\"/posts/Pweg9xpKknkNwN8Fx/have-attention-spans-been-declining\">Have Attention Spans Been Declining?</a> <span class=\"post-author\">niplav</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">166</td><td><a class=\"post-title\" href=\"/posts/d2HvpKWQ2XGNsHr8s/hell-is-game-theory-folk-theorems\">Hell is Game Theory Folk Theorems</a> <span class=\"post-author\">jessicata</span></td><td><div class=\"dots-row\"><span title=\"7.8\" class=\"dot\" style=\"width:11.7px; height:11.7px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">167</td><td><a class=\"post-title\" href=\"/posts/zt6hRsDE84HeBKh7E/reducing-sycophancy-and-improving-honesty-via-activation\">Reducing sycophancy and improving honesty via activation steering</a> <span class=\"post-author\">Nina Panickssery</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">168</td><td><a class=\"post-title\" href=\"/posts/NvwjExA7FcPDoo3L7/are-there-cognitive-realms\">Are there cognitive realms?</a> <span class=\"post-author\">TsviBT</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.6\" class=\"dot\" style=\"width:5.4px; height:5.4px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">169</td><td><a class=\"post-title\" href=\"/posts/SX6wQEdGfzz7GKYvp/rationalist-discourse-is-like-physicist-motors\">\"Rationalist Discourse\" Is Like \"Physicist Motors\"</a> <span class=\"post-author\">Zack_M_Davis</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">170</td><td><a class=\"post-title\" href=\"/posts/iJnBLanZLephL5cao/being-at-peace-with-doom\">Being at peace with Doom</a> <span class=\"post-author\">Johannes C. Mayer</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">171</td><td><a class=\"post-title\" href=\"/posts/JjqZexMgvarBFMKPs/recreating-the-caring-drive\">Recreating the caring drive</a> <span class=\"post-author\">Catnee</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">172</td><td><a class=\"post-title\" href=\"/posts/SfZRWxktiFFJ5FNk8/the-god-of-humanity-and-the-god-of-the-robot-utilitarians\">The God of Humanity, and the God of the Robot Utilitarians</a> <span class=\"post-author\">Raemon</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">173</td><td><a class=\"post-title\" href=\"/posts/dcoxvEhAfYcov2LA6/agentized-llms-will-change-the-alignment-landscape\">Agentized LLMs will change the alignment landscape</a> <span class=\"post-author\">Seth Herd</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">174</td><td><a class=\"post-title\" href=\"/posts/AWoZBzxdm4DoGgiSj/ability-to-solve-long-horizon-tasks-correlates-with-wanting\">Ability to solve long-horizon tasks correlates with wanting things in the behaviorist sense</a> <span class=\"post-author\">So8res</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.8\" class=\"dot\" style=\"width:5.699999999999999px; height:5.699999999999999px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">175</td><td><a class=\"post-title\" href=\"/posts/jyM7MSTvy8Qs6aZcz/what-s-up-with-responsible-scaling-policies\">What's up with \"Responsible Scaling Policies\"?</a> <span class=\"post-author\">habryka</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">176</td><td><a class=\"post-title\" href=\"/posts/SibcE6H9i6wifzqAF/the-real-fanfic-is-the-friends-we-made-along-the-way\">The Real Fanfic Is The Friends We Made Along The Way</a> <span class=\"post-author\">Eneasz</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">177</td><td><a class=\"post-title\" href=\"/posts/LbbrnRvc9QwjJeics/new-user-s-guide-to-lesswrong\">New User's Guide to LessWrong</a> <span class=\"post-author\">Ruby</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">178</td><td><a class=\"post-title\" href=\"/posts/wCtegGaWxttfKZsfx/we-don-t-understand-what-happened-with-culture-enough\">We don't understand what happened with culture enough</a> <span class=\"post-author\">Jan_Kulveit</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">179</td><td><a class=\"post-title\" href=\"/posts/FQhtpHFiPacG3KrvD/seth-explains-consciousness\">Seth Explains Consciousness</a> <span class=\"post-author\">Jacob Falkovich</span></td><td><div class=\"dots-row\"><span title=\"8.4\" class=\"dot\" style=\"width:12.600000000000001px; height:12.600000000000001px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">180</td><td><a class=\"post-title\" href=\"/posts/SqgRtCwueovvwxpDQ/valence-series-2-valence-and-normativity\">[Valence series] 2. Valence &amp; Normativity</a> <span class=\"post-author\">Steven Byrnes</span></td><td><div class=\"dots-row\"><span title=\"8.4\" class=\"dot\" style=\"width:12.600000000000001px; height:12.600000000000001px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">181</td><td><a class=\"post-title\" href=\"/posts/hsA8hnvuyaXZdx3am/fifty-flips\">Fifty Flips</a> <span class=\"post-author\">abstractapplic</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">182</td><td><a class=\"post-title\" href=\"/posts/Qw6qsgR2Qm6rp574A/so-you-want-to-save-the-world-an-account-in-paladinhood\">So you want to save the world? An account in paladinhood</a> <span class=\"post-author\">Tamsin Leake</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">183</td><td><a class=\"post-title\" href=\"/posts/Fbk9H6ipfybHyqjrp/a-playbook-for-ai-risk-reduction-focused-on-misaligned-ai\">A Playbook for AI Risk Reduction (focused on misaligned AI)</a> <span class=\"post-author\">HoldenKarnofsky</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">184</td><td><a class=\"post-title\" href=\"/posts/umJMCaxosXWEDfS66/moral-reality-check-a-short-story\">Moral Reality Check (a short story)</a> <span class=\"post-author\">jessicata</span></td><td><div class=\"dots-row\"><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">185</td><td><a class=\"post-title\" href=\"/posts/oRTzrjJDss6twSnBD/takeaways-from-calibration-training\">Takeaways from calibration training</a> <span class=\"post-author\">Olli Järviniemi</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">186</td><td><a class=\"post-title\" href=\"/posts/Q3XaZTExzDpCLr4wu/efficiency-and-resource-use-scaling-parity\">Efficiency and resource use scaling parity</a> <span class=\"post-author\">Ege Erdil</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">187</td><td><a class=\"post-title\" href=\"/posts/cgAaShQeuL53zzGsT/spaciousness-in-partner-dance-a-naturalism-demo\">Spaciousness In Partner Dance: A Naturalism Demo</a> <span class=\"post-author\">LoganStrohl</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">188</td><td><a class=\"post-title\" href=\"/posts/rM8DwFKZM4eB7i2p8/valence-series-3-valence-and-beliefs\">[Valence series] 3. Valence &amp; Beliefs</a> <span class=\"post-author\">Steven Byrnes</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">189</td><td><a class=\"post-title\" href=\"/posts/w2gYsWF3CLBXZkHfg/fighting-without-hope\">Fighting without hope</a> <span class=\"post-author\">Zuko's Shame</span></td><td><div class=\"dots-row\"><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">190</td><td><a class=\"post-title\" href=\"/posts/Mqvn5asmXuBoCSGZy/on-tapping-out\">On Tapping Out</a> <span class=\"post-author\">Screwtape</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">191</td><td><a class=\"post-title\" href=\"/posts/emsHFEnyxXR8nEj6p/beware-of-fake-alternatives\">Beware of Fake Alternatives</a> <span class=\"post-author\">silentbob</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">192</td><td><a class=\"post-title\" href=\"/posts/SpDHvbcJsiE5mxBzj/truth-and-advantage-response-to-a-draft-of-ai-safety-seems\">Truth and Advantage: Response to a draft of \"AI safety seems hard to measure\"</a> <span class=\"post-author\">So8res</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">193</td><td><a class=\"post-title\" href=\"/posts/WkchhorbLsSMbLacZ/ai-1-sydney-and-bing\">AI #1: Sydney and Bing</a> <span class=\"post-author\">Zvi</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">194</td><td><a class=\"post-title\" href=\"/posts/bku9odAYPyQwHqzCo/the-shape-of-agi-cartoons-and-back-of-envelope\">The shape of AGI: Cartoons and back of envelope</a> <span class=\"post-author\">boazbarak</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">195</td><td><a class=\"post-title\" href=\"/posts/Nnis3dFYZRRAzWuEA/underwater-torture-chambers-the-horror-of-fish-farming\">Underwater Torture Chambers: The Horror Of Fish Farming</a> <span class=\"post-author\">omnizoid</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">196</td><td><a class=\"post-title\" href=\"/posts/yXiu6DBxKKWXC8Ygx/finding-neurons-in-a-haystack-case-studies-with-sparse\">Finding Neurons in a Haystack: Case Studies with Sparse Probing</a> <span class=\"post-author\">wesg</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">197</td><td><a class=\"post-title\" href=\"/posts/Be3ertyJfwDdQucdd/how-should-turntrout-handle-his-deepmind-equity-situation\">How should TurnTrout handle his DeepMind equity situation?</a> <span class=\"post-author\">habryka</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">198</td><td><a class=\"post-title\" href=\"/posts/odtMt7zbMuuyavaZB/when-do-brains-beat-brawn-in-chess-an-experiment\">When do \"brains beat brawn\" in Chess? An experiment</a> <span class=\"post-author\">titotal</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.5\" class=\"dot\" style=\"width:5.25px; height:5.25px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">199</td><td><a class=\"post-title\" href=\"/posts/yCuzmCsE86BTu9PfA/there-are-no-coherence-theorems\">There are no coherence theorems</a> <span class=\"post-author\">Dan H</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-2.7\" class=\"dot\" style=\"width:4.050000000000001px; height:4.050000000000001px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">200</td><td><a class=\"post-title\" href=\"/posts/5qyytGqZWv5bt6723/the-power-of-high-speed-stupidity\">The Power of High Speed Stupidity</a> <span class=\"post-author\">robotelvis</span></td><td><div class=\"dots-row\"><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">201</td><td><a class=\"post-title\" href=\"/posts/naEDcbicBBykiTFwi/book-review-consciousness-explained-as-the-great-catalyst\">Book Review: Consciousness Explained (as the Great Catalyst)</a> <span class=\"post-author\">Rafael Harth</span></td><td><div class=\"dots-row\"><span title=\"3.7\" class=\"dot\" style=\"width:5.550000000000001px; height:5.550000000000001px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">202</td><td><a class=\"post-title\" href=\"/posts/Neh76ueECviJ6p75o/large-language-models-learn-to-represent-the-world\">Large language models learn to represent the world</a> <span class=\"post-author\">gjm</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">203</td><td><a class=\"post-title\" href=\"/posts/JAmyTWoukk8xzhE9n/ruining-an-expected-log-money-maximizer\">Ruining an expected-log-money maximizer</a> <span class=\"post-author\">philh</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">204</td><td><a class=\"post-title\" href=\"/posts/LRuaCRpTc2zMhDDbb/a-problem-with-the-most-recently-published-version-of-cev\">A problem with the most recently published version of CEV</a> <span class=\"post-author\">ThomasCederborg</span></td><td><div class=\"dots-row\"><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">205</td><td><a class=\"post-title\" href=\"/posts/R28YGeAzDHehrnc7f/in-defense-of-parselmouths\">In Defense of Parselmouths</a> <span class=\"post-author\">Screwtape</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">206</td><td><a class=\"post-title\" href=\"/posts/Kfr7PgXRQow9kbGx7/five-worlds-of-ai-by-scott-aaronson-and-boaz-barak\">Five Worlds of AI (by Scott Aaronson and Boaz Barak)</a> <span class=\"post-author\">mishka</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">207</td><td><a class=\"post-title\" href=\"/posts/r8w2SxmYqgssdjWwd/when-omnipotence-is-not-enough\">When Omnipotence is Not Enough</a> <span class=\"post-author\">lsusr</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">208</td><td><a class=\"post-title\" href=\"/posts/LCduhA4m3RhMjZJPA/why-you-should-never-update-your-beliefs\">Why You Should Never Update Your Beliefs</a> <span class=\"post-author\">Arjun Panickssery</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">209</td><td><a class=\"post-title\" href=\"/posts/9nEBWxjAHSu3ncr6v/responsible-scaling-policies-are-risk-management-done-wrong\">Responsible Scaling Policies Are Risk Management Done Wrong</a> <span class=\"post-author\">simeon_c</span></td><td><div class=\"dots-row\"><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.6\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">210</td><td><a class=\"post-title\" href=\"/posts/EvKa7EakoXreCkhC6/a-way-to-be-okay\">A Way To Be Okay</a> <span class=\"post-author\">Duncan Sabien (Deactivated)</span></td><td><div class=\"dots-row\"><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span></div></td></tr><tr><td class=\"item-count\">211</td><td><a class=\"post-title\" href=\"/posts/vfjptEJ2oahLqRyZz/justice-cherryl\">\"Justice, Cherryl.\"</a> <span class=\"post-author\">Zack_M_Davis</span></td><td><div class=\"dots-row\"><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">212</td><td><a class=\"post-title\" href=\"/posts/4mM8RYsm4okrqGSqx/large-language-models-can-strategically-deceive-their-users\">Large Language Models can Strategically Deceive their Users when Put Under Pressure.</a> <span class=\"post-author\">ReaderM</span></td><td><div class=\"dots-row\"><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span></div></td></tr><tr><td class=\"item-count\">213</td><td><a class=\"post-title\" href=\"/posts/T5RzkFcNpRdckGauu/link-a-community-alert-about-ziz\">[Link] A community alert about Ziz</a> <span class=\"post-author\">DanielFilan</span></td><td><div class=\"dots-row\"><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"4\" class=\"dot\" style=\"width:6px; height:6px; background:#5f9b65\"></span><span title=\"3.9\" class=\"dot\" style=\"width:5.85px; height:5.85px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"1\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#5f9b65\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"0\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-0.9\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-1\" class=\"dot\" style=\"width:3px; height:3px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-4\" class=\"dot\" style=\"width:6px; height:6px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span><span title=\"-9\" class=\"dot\" style=\"width:13.5px; height:13.5px; background:#bf360c\"></span></div></td></tr></tbody></table>\n\nUpdates to the Best of LessWrong:   \nComing Soon\n================================================\n\nIt'll take a little while to get the results polished with nice art and organization on the /bestoflesswrong page. Stay tuned for a final announcement. (This year this might still take a week or two. Hopefully next year we'll have most of the final polishing fully automated)",
      "plaintextDescription": "The votes are in for the 2023 Review!\n\n6,264 posts were written in 2023\n\n662 of them were nominated.\n\n209 of them got at least one review, and a positive review-vote total.\n\n50 of them shall be displayed in the Best of LessWrong, Year 2023.\n\n\nReviews\nExactly 100 people wrote reviews, and many of them I found particularly valuable. I want to give some particular shout outs to Ryan Greenblatt, John Wentworth, and Steve Byrnes.\n\nI found Ryan Greenblatt's reviews to be sort of relentlessly reasonable. He reviewed many posts thoughtfully, clearly state flaws, value props, and concrete potential improvements. I particularly liked his review of Anthropic's Responsible Scaling Policy & Long-Term Benefit Trust\n\nI personally had the most direct \"worldview update\" from John Wentworth's The Case Against AI Control Research. I don't fully agree with John's take, but I found distinguishing how bad scheming is at different AI stages, and what else needs to be going right at that stage, a quite useful frame.\n\nI was also grateful for Steve Byrnes extensive opinionated review of the  “Sharp Left Turn” discourse.\n\nHere's a general screenshot of top of the Review Leaderboard. Thanks to everyone who participated.\n\n \n\nAnd, reminder, reviews now appear on the Spotlight Items, at the top of the home page, and on the /bestoflesswrong page.\n\n\nWeighted Vote Totals\nIn past years, we calculated the votes based on two groups of users: Established users with 1000+ karma (who got 3x the vote weight), and users with <1000 karma. \n\nThis time, we're replacing that arbitrary cliff with a more granular \"your review vote gets multiplied by your Strong Vote power.\" I mentioned we'd be doing something like that in the announcement post, noting:\n\n> I haven't been happy with the arbitrary cliff here. It gives more power than I really wanted to allocate to people with 1000 karma, and not enough weight to people who have been around much longer and have demonstrated good judgment. But, karma is still a pretty",
      "wordCount": 20651
    },
    "tags": [
      {
        "_id": "2dz4FEXCdbk2BwDpc",
        "name": "LessWrong Review",
        "slug": "lesswrong-review"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Y4bKhhZyZ7ru7zqsh",
    "title": "C'mon guys, Deliberate Practice is Real",
    "slug": "c-mon-guys-deliberate-practice-is-real",
    "url": null,
    "baseScore": 99,
    "voteCount": 44,
    "viewCount": null,
    "commentCount": 25,
    "createdAt": null,
    "postedAt": "2025-02-05T22:33:59.069Z",
    "contents": {
      "markdown": "I'm writing a more in-depth review of the State of Feedbackloop Rationality. (I've written a short review on the [original post](https://www.lesswrong.com/posts/pZrvkZzL2JnbRgEBC/feedbackloop-first-rationality?commentId=gm5ssqTvhfrT8zBt6))\n\nBut I feel like a lot of people have some kind of skepticism that isn't really addressed by \"well, after 6 months of work spread out over 1.5 years, I've made... some progress, which feels promising enough to me to keep going but is not overwhelming evidence if you're skeptical.\"\n\nSo, this post is sort of a rant about that. \n\nA lot of people have default skepticism about \"the feedbackloop rationality paradigm\" because, well, every rationality paradigm so far has come with big dreams and underdelivered. I, too, have big dreams, and I am probably not going to achieve them as much as I want. That's not a crux[^jpg5tugat1j] for me.\n\nA lot of people also have some notion that \"building multiple feedback loops\" is a kind of elaborate meta that puts your head in the clouds. Dudes, the whole point of multiple feedback loops is to put you in direct contact with reality in as many ways as practical.\n\nThe thing that drives me nuts is, is that the basic premise here is:\n\nIf you put in a lot of effortful focused, practice...  \n...on the edge of your ability  \n...with careful attention to immediate feedback  \n...and periodic careful attention to \"is this process helping my longterm goals?\"\n\n...you will get better at thinking.\n\n(And, if a lot of people systematically do that and we track what works for them, we can get from the domain of Purposeful Practice to Deliberate Practice[^re39o7ehvb])  \n\nThis seems so reasonable. This really is not promising a free lunch.\n\n(I should clarify: I am not saying that there is robust scientific evidence for deliberate practice working for thinking. There is not – the evidence for meta-learning is generally confused and sucks and there particularly is not evidence that ~deliberate~ purposeful practice works on openended conceptual questions. I'm saying \"c'mon, what the fuck do you think will work better at improving your ability to think? a) careful, focused practice on the edge of your ability with careful attention to feedback b) just generally trying to do object level work in domains where you ~never get much feedback on whether you're really accomplishing your ultimate goals?\")\n\nThere are a lot of followup questions that aren't as obvious, but it seems crazy to not believe that. And if you took it [seriously](https://www.lesswrong.com/posts/Gh2qQHrCg3teQen3c/rationalists-are-less-credulous-but-better-at-taking-ideas), I don't think the right response is to shrug, and wait for Ray to come back with some visibly impressive results. \n\nI am, in fact, trying to get some visibly impressive object-level shit done so that skeptical people say \"okay I guess the evidence is enough to warrant a more serious look.\" If maintain a lot of momentum, I might be at that point in ~a year. Figuring out how to do that without burning out is kinda hard (which is what I'm currently working on).\n\nBut, I feel like I really shouldn't have to do this, to argue the claim: \"effortful, focused practice at thinking with careful attention to feedback will pay off\" and \"improved thinking is important for making plans about x-risk and AI safety.\"\n\nSome people followup with \"I believe Ray found something that worked for Ray, but don't believe it'll generalize.\" Which is also a sort of fair response to generically finding out some guy is out there inventing a rationality paradigm without knowing any details. But, c'mon guys, the *whole point* of the Feedbackloop Rationality Paradigm is to improve your own feedbackloops with your own judgment, with a mixture of legible arguments as well as feelings in your heart. \n\nI'm annoyed about it because, there's a lot of people around who seem pretty smart in many ways, who I think/hope can help humanity thread the needle of fate, but seem at least somewhat metacognitively dumb sometimes in ways that seem fixable. They bounce off for various reasons. (I get this more from established professionals in the x-risk space, than from median LessWrong folk who generally seem excited for some kind of rationality training content.)\n\nIf I ask them why, they mention some arguments that seems like, sure, a reasonable thing to think about  (i.e. \"if you focus on feedback you might Goodhart yourself\", or \"I don't have hours and hours to spend doing toy exercises.\"). But they don't spend 5 minutes thinking about how to address those problems, which IMO seem totally possible to address. \n\nI know a small number of people who seem to have actually constructed some kind of practice regimen for themselves (which might look very different from mine). This rant is not directed at them. (I do think those people often could use more deliberate attention on what sort of practice they do, and how often they do it. But, I'm not mad about it)\n\nIs the evidence for this ironclad? No. But neither is the evidence that just focusing on your object level work is going to be enough to steer towards the most important questions and approaches in time. And... I just roll to disbelieve that your true objection is that you don't believe in purposeful practice.\n\nFAQ on some things that seem reasonable to be skeptical about\n=============================================================\n\nI don't think practicing \"better thinking\" will help me. I think I should focus on domain-specific skills.\n----------------------------------------------------------------------------------------------------------\n\nFor many people – yep, that sounds correct. Literally the first post in the Feedbackloop rationality sequence is [Rationality !== Winning](https://www.lesswrong.com/posts/3GSRhtrs2adzpXcbY/rationality-winning), where I note that even if rationality *can* help you with your problems, if you have pretty ordinary problems, it's not obviously the right tool for the job.\n\nI'm focused on \"think that helps you solve confusing problems, for which there is no established playbook.\"\n\nWith AI safety in particular, I think you should be worried that the job is confusing, such that it's not obvious what the right tool for the job is. We've never demonstrably stopped an unfriendly AI FOOM or a slow-rolling loss of power before. I think you should have a lot of uncertainty about what the right approaches are.\n\nWe know how to mentor people into specific research paradigms, but AFAICT humanity doesn't have much systematized training on how to figure out which research field (if any) is coming at things from the right angle.\n\nI'm particularly worried that a lot of people are just following a gradient of learning ML, assuming the ML research paradigm is basically the right lens, without cultivating the ability to form good taste on whether that's true. (And, for that matter, that other people are following a gradient of \"learn whatever math happened to be involved with existing agent foundations research and try to get better at that.\")\n\nBut, you can't practice the most *important* parts of thinking, for preparadigmatic research. Focusing on feedback loops will lead you to goodhart.\n---------------------------------------------------------------------------------------------------------------------------------------------------\n\nI think if you sit and think about it, with an attitude of \"how can I solve this problem?\", rather than \"this seems obviously hopeless\", it is really not that hard to z\n\nThere’s a cluster of skills necessary for tackling a lot of “real x-risk work”, or really, any kind of pursuing a difficult, confusing goal. Some examples:\n\n*   Noticing you are confused\n*   Noticing you are *still* confused\n*   Sitting with the discomfort of being confused and also not sure if your project is on the right track for a long time.\n*   Generating situation-appropriate-strategies for dealing with that confusion and tractionlessness.\n*   Cultivating open curiosity\n*   Finding questions worth asking.\n*   Noticing that your strategy was subtly wrong, and figuring out a way to make it work.\n*   Grieving for the fact that your current project was fundamentally flawed and you wasted a lot of time, and moving on\n*   Noticing you are missing a skill, or knowledge, and figuring out how to quickly gain that skill/knowledge.\n*   Noticing when your plans are resting on shaky assumptions\n*   Validating those assumptions early, if you can.\n*   Developing a calibrated sense of when you’re meandering thought process is going somewhere valuable, vs when you’re off track.\n\nI think all of those are trainable skills. Yes, it's possible to misgeneralize from toy problems to real problems, but I think the general engine of \"practice on a variety of Toy Confusing Problems, and develop a sense of the failure modes you run into there, and how to account for them\" just makes a pretty solid foundation for tracking the subtler or deeper versions of those problems that you run into in the wild.\n\nIs it possible to overfit on your training? Yes, that's why I recommend a variety of types of confusing challenges, so you can sus out which skills actually generalize. \n\nMaybe, but this will just take so much time. Why is this going to pay off enough to be better than object level research?\n-------------------------------------------------------------------------------------------------------------------------\n\nThis is the question that feels most reasonable to me. Purposeful Practice requires peak cognitive hours, and you don't have that many of those.\n\nMy main answer is: Identifying skills that you need day-to-day for your most important projects, and finding an angle for practicing with focused attention for ~15 minutes a day (applied to your day job). You keep your peak hours focused on your primary goals, with a bit of time directed at \"how can I push the frontier of my thinking capacity?\"\n\nIt seems reasonable to spend those 15 minutes on a mix of domain-specific skills and higher level decisionmaking. I do my personal practice of a mix of \"code debugging\" (a naturally occurring rationality challenge) and \"deciding what to do today.\"\n\nSome details from my self-review of the feedbackloop rationality post\n\n> **Fluent enough for your day job. **\n> \n> The primary aim of my workshops and practice is to get new skills fluent enough that you can appy them to your day job, because that's where it's practical to deliberate practice in a way that pays for itself, rather than being an exhausting extra thing you do.\n> \n> \"Fluency at new skill that seem demonstrably useful\" is also a large enough effect size that there's at least *something* you can measure near term, to get a sense of whether the workshop is working.\n> \n> **Five minute versions of skills**. \n> \n> Relatedly: many skills have elaborate, comprehensive versions that take ~an hour to get the full value of, but you're realisitically not going to do those most of the time. So it's important to boil them down into something you can do in 5 minutes (or, 30 seconds).\n> \n> **Morning Orient Prompts.**\n> \n> A thing I've find useful for myself, and now think of as one of the primary goal of the workshop to get people to try out, is a \"morning orient prompt list\" that you do every day.\n> \n> It's important that it be every day, even when you don't need it too much, so that you still have a habit of metacognition for the times you need it (but, when you don't need it too much, it's fine/good to do a very quick version of it)\n> \n> It's useful to have a list of explicit prompts, because that gives you an artifact that's easier to iterate on.\n\n**Also, consider at least one extended deliberate practice push**\n\nI think ~20 hours is often enough to get \"n00b gains\" in a new specific skill. My personal experience is that ~40 hours has been enough to take a place where I had plateau'd, and push through to a demonstrably higher level.\n\nI think it's worth doing this at least once and maybe twice, in a continuous marathon focus, to get a visceral sense of how much work is involved, and which sorts of things actually help you-in-particular. But, this is a lot more expensive, and I don't begrudge people who say \"well, that's more than I can commit.\"\n\nRealistically, I know I won't stick to this sort of thing, and even if it'd work if I stick to it, in practice it'll just sort of sputter and fail. Which doesn't seem worth it.\n--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\nSigh, nod. This is indeed the [reason that deliberate practice kinda sucks which is why it's underinvested in which is why I think there's alpha in it.](https://www.lesswrong.com/posts/FEMWS79AFTmKq2iJK/rationality-research-report-towards-10x-ooda-looping#Feedback_loops_and__deliberate_practice___vs__Just_Clicking_) \n\nGenerally my answer to this is to a) try to build structures that generally help people with this, b) also be investing in a [Thinking Assistant](https://www.lesswrong.com/posts/ajT6q5aMokagfthvG/hire-or-become-a-thinking-assistant) ecosystem that can help people with general focus, of which \"help do your serious practice.\"\n\nI have limited time so I don't have generally open offers to help with this, but my current process is \"after you've come to a workshop, you're invited to Deliberate Practice club which meets ~once a month, and charges money to keep it sustainable for me + filter for seriousness\", where we both orient on \"How did the last month ago? Are we on track \n\nOkay but I bet Ray is doing some oddly specific stuff that works for Ray and don't really think it'll help me.\n--------------------------------------------------------------------------------------------------------------\n\nMaybe! I think people's cognitive styles do vary.\n\nI feel pretty happy if people read this, and go off on their own to construct their own practice regimens that work for them.\n\nI do think I've got a bunch of domain expertise by now in \"how to construct exercises on-the-fly that help people train skills that are relevant to them.\" At my workshops, I  explicitly tell people \"you can do a different exercise, if a given session doesn't feel useful to you.\" A problem is that designing exercises for yourself is itself a skill, and most people end up doing something not-too-productive the first couple times they try. But I generally try to be building containers that help people put in the practice, wherever they're currently at.\n\nI do have a specific vibe though. I'm working on being more flexible about it. (Someone said \"I kinda wish there was a Logan-y version of this workshop\" and I think that's a pretty legit thing to want in the world)\n\nThe skill ceiling here doesn't seem that high, I'm already pretty competent, it doesn't seem worth it.\n------------------------------------------------------------------------------------------------------\n\nMaybe. I don't know you. Maybe you've consumed all the low-and-medium hanging fruit here.\n\nBut, idk. 10x programmers seem to exist. [10x UI designers seem to exist](https://www.lesswrong.com/posts/F9WyMPK4J3JFrxrSA/the-think-it-faster-exercise). When I look at the gears of how people seem to struggle with it, it seems to me pretty likely that 10x confusing-problem-navigation should be possible. There are tons of subtly wrong ways to make plans, and tons of subtly-or-not-so-subtly dumb ways of sticking with them too long.\n\nI realize that is not that good an argument if you don't already share my intuitions here. But I am interested in hearing from specific people about what-they-think-they-know-and-why-they-think-they-know it about how much room for improvement they think they have.\n\nWhy (exactly) won't it work (for you, specifically)\n===================================================\n\nOne of my favorite frames/prompts I've found in the past year is \"Very specifically, why is this impossible?\". And then, for each of those reasons\n\nI've seen a few people respond to this with some kind of broad dismissal.\n\nI am interested in hearing critiques from people who've set, like, at least a 15 minute timer to sit and ask themselves, \"Okay, suppose I did want to improve at these sorts of skills, or related ones that feel more relevant to me, in a way I believed in. What concretely is hard about that? Where do I expect it to go wrong?\", and then come back with something more specific than \"idk it just seems like this sort of thing won't work.\"\n\nAnd then, for each of those reasons, ask \"okay, why is *that* impossible?\"\n\nAnd sometimes you're left with pieces that are actually impossible. Sometimes you are left with pieces that are merely \"very hard.\" And sometimes, as soon as you sit and think about it for 2 seconds, you go \"oh, well, okay obviously I could do that, it'd just be slightly inconvenient.\"\n\nI would like to hear critiques from people who have spent at least some time inhabiting the \"okay, if I tried to roll up my sleeves and just do this, either for myself, or for contributing somehow to an ecosystem of confusing-problem-solving-training, what actually goes wrong?\"\n\n[^jpg5tugat1j]: I am holding the big dreams as my polaris because it's helpful for cutting as quickly as possible to whatever the best outcomes turn out to be. \"Shoot for the moon and... you'll end up in low-earth orbit, which is pretty cool actually!\".\"Shoot for olympic-level training for x-risk researchers and you'll end up with pretty solid training that gets a bunch of people more traction on the hardest parts of the problem(s).\" \n\n[^re39o7ehvb]: Contrasted with Naive Practice, Purposeful Practice involves sustained focus, on skills at the edge of your ability, with explicit goals, paying careful attention to feedback.Deliberate practice is \"Purposeful Practice that we know works.\" (I claim that the current state of my paradigm is \"minimum-viable-deliberate-practice\", in that it's been through at least a couple rounds of weeding out stuff that didn't work, and the stuff that remains has worked at least decently)",
      "plaintextDescription": "I'm writing a more in-depth review of the State of Feedbackloop Rationality. (I've written a short review on the original post)\n\nBut I feel like a lot of people have some kind of skepticism that isn't really addressed by \"well, after 6 months of work spread out over 1.5 years, I've made... some progress, which feels promising enough to me to keep going but is not overwhelming evidence if you're skeptical.\"\n\nSo, this post is sort of a rant about that. \n\nA lot of people have default skepticism about \"the feedbackloop rationality paradigm\" because, well, every rationality paradigm so far has come with big dreams and underdelivered. I, too, have big dreams, and I am probably not going to achieve them as much as I want. That's not a crux[1] for me.\n\nA lot of people also have some notion that \"building multiple feedback loops\" is a kind of elaborate meta that puts your head in the clouds. Dudes, the whole point of multiple feedback loops is to put you in direct contact with reality in as many ways as practical.\n\nThe thing that drives me nuts is, is that the basic premise here is:\n\n \n\nIf you put in a lot of effortful focused, practice...\n...on the edge of your ability\n...with careful attention to immediate feedback\n...and periodic careful attention to \"is this process helping my longterm goals?\"\n\n...you will get better at thinking.\n\n(And, if a lot of people systematically do that and we track what works for them, we can get from the domain of Purposeful Practice to Deliberate Practice[2])  \n\n \n\nThis seems so reasonable. This really is not promising a free lunch.\n\n(I should clarify: I am not saying that there is robust scientific evidence for deliberate practice working for thinking. There is not – the evidence for meta-learning is generally confused and sucks and there particularly is not evidence that deliberate purposeful practice works on openended conceptual questions. I'm saying \"c'mon, what the fuck do you think will work better at improving your ability to think? a)",
      "wordCount": 2730
    },
    "tags": [
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "DnWkYz3w6xxsekok9",
    "title": "Wired on: \"DOGE personnel with admin access to Federal Payment System\"",
    "slug": "wired-on-doge-personnel-with-admin-access-to-federal-payment",
    "url": "https://web.archive.org/web/20250205123049/https://www.wired.com/story/elon-musk-associate-bfs-federal-payment-system/",
    "baseScore": 88,
    "voteCount": 36,
    "viewCount": null,
    "commentCount": 45,
    "createdAt": null,
    "postedAt": "2025-02-05T21:32:11.205Z",
    "contents": {
      "markdown": "I haven't looked into this in detail, and I'm not actually sure how unique a situation this is. But, it updated me on \"institutional changes to the US that might be quite bad[^fur3b5s2hdo]\", and it seemed good if LessWrong folk did some sort of Orient Step on it.\n\n(Please generally be cautious on LessWrong talking about politics. I am interested in people commenting here who have read the [LessWrong Political Prerequisites](https://www.lesswrong.com/sequences/ZnSMHcWjRx6yT4H92) sequence. I'll be deleting or at least unhesitatingly strong downvoting comments that seem to be doing unreflective partisan dunking)\n\n((But, that's not meant to mean \"don't talk about political actions.\" If this is as big a deal as it sounds, I want to be able to talk about \"what to do do?\". But I want that talking-about-it to feel more like practically thinking through an action space, than blindly getting sucked into a political egregore))\n\n> A 25-year-old engineer named Marko Elez, who previously worked for two [Elon Musk companies](https://web.archive.org/web/20250205123049/https://www.wired.com/story/elon-musk-government-young-engineers/), has direct access to Treasury Department systems responsible for nearly all payments made by the [US government](https://web.archive.org/web/20250205123049/https://www.wired.com/story/elon-musk-lieutenant-gsa-ai-agency/), three sources tell WIRED.\n> \n> Two of those sources say that Elez’s privileges include the ability not just to read but to write code on two of the most sensitive systems in the US government: the Payment Automation Manager and Secure Payment System at the Bureau of the Fiscal Service (BFS). Housed on a secure mainframe, these systems control, on a granular level, government payments that in their totality amount to more than a fifth of the US economy.\n> \n> Despite [reporting that suggests](https://web.archive.org/web/20250205123049/https://www.politico.com/news/2025/02/01/musk-claims-doge-lax-treasury-00201946) that Musk’s so-called [Department of Government Efficiency](https://web.archive.org/web/20250205123049/https://www.wired.com/story/doge-elon-musk/) (DOGE) task force has access to these Treasury systems on a “read-only” level, sources say Elez, who has visited a Kansas City office housing BFS systems, has many administrator-level privileges. Typically, those admin privileges could give someone the power to log in to servers through secure shell access, navigate the entire file system, change user permissions, and delete or modify critical files. That could allow someone to bypass the security measures of, and potentially cause irreversible changes to, the very systems they have access to.\n> \n> “You could do anything with these privileges,” says one source with knowledge of the system, who adds that they cannot conceive of a reason that anyone would need them for purposes of simply hunting down fraudulent payments or analyzing disbursement flow.\n> \n> \"Technically I don't see why this couldn't happen,\" a federal IT worker tells WIRED in a phone call late on Monday night, referring to the possibility of a [DOGE](https://web.archive.org/web/20250205123049/https://www.wired.com/story/usaid-researchers-email-access/) employee being granted elevated access to a government server. \"If you would have asked me a week ago, I'd have told you that this kind of thing would never in a million years happen. But now, who the fuck knows.\"\n\n[^fur3b5s2hdo]: I currently am more anticipating things like \"institutional decay / general corruption / loss-of-trust\" than \"dictatorial takeover.\" But mostly I'm like \"seems like weird and alarming things are happening and it's worth paying attention to with some scout mindset.\"",
      "plaintextDescription": "I haven't looked into this in detail, and I'm not actually sure how unique a situation this is. But, it updated me on \"institutional changes to the US that might be quite bad[1]\", and it seemed good if LessWrong folk did some sort of Orient Step on it.\n\n(Please generally be cautious on LessWrong talking about politics. I am interested in people commenting here who have read the LessWrong Political Prerequisites sequence. I'll be deleting or at least unhesitatingly strong downvoting comments that seem to be doing unreflective partisan dunking)\n\n((But, that's not meant to mean \"don't talk about political actions.\" If this is as big a deal as it sounds, I want to be able to talk about \"what to do do?\". But I want that talking-about-it to feel more like practically thinking through an action space, than blindly getting sucked into a political egregore))\n\n> A 25-year-old engineer named Marko Elez, who previously worked for two Elon Musk companies, has direct access to Treasury Department systems responsible for nearly all payments made by the US government, three sources tell WIRED.\n> \n> Two of those sources say that Elez’s privileges include the ability not just to read but to write code on two of the most sensitive systems in the US government: the Payment Automation Manager and Secure Payment System at the Bureau of the Fiscal Service (BFS). Housed on a secure mainframe, these systems control, on a granular level, government payments that in their totality amount to more than a fifth of the US economy.\n> \n> Despite reporting that suggests that Musk’s so-called Department of Government Efficiency (DOGE) task force has access to these Treasury systems on a “read-only” level, sources say Elez, who has visited a Kansas City office housing BFS systems, has many administrator-level privileges. Typically, those admin privileges could give someone the power to log in to servers through secure shell access, navigate the entire file system, change user permissions, and delete o",
      "wordCount": 474
    },
    "tags": [
      {
        "_id": "kdbs6xBndPkmrYAxM",
        "name": "Politics",
        "slug": "politics"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "bPycPKLkdsuZ9hCKa",
    "title": "Last week of the Discussion Phase",
    "slug": "last-week-of-the-discussion-phase",
    "url": null,
    "baseScore": 35,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-01-09T19:26:59.136Z",
    "contents": {
      "markdown": "*Previously:* [*The 2023 LessWrong Review: The Basic Ask*](https://www.lesswrong.com/posts/pudQtkre7f9GLmb2b/the-2023-lesswrong-review-the-basic-ask)\n\nThere's one week left in the Discussion Phase of the 2023 Review. Here's a few things you might want to spend that time doing, in escalating order of effort:\n\n**Review three posts,** without stressing much about the details. Go the [Quick Review](https://www.lesswrong.com/quickReview/2023)[^4qlfczsocrv] page, pick some posts you remember and had opinions about. Reread them, and write a review.\n\n**Think about which posts should be in the Best of LessWrong.** Go to the [Advanced Review](https://www.lesswrong.com/reviewVoting/2023)[^d0tt0ku1m2] page, look at which posts are currently in the top 50, and at the posts that *aren't* in the top 50 which you think should be. Write reviews arguing for posts you think are underappreciated.\n\n**Think big picture, longform thoughts about the conversations from 2023.** After reviewing several posts, write a top level post with the [2023 Longform Review](https://www.lesswrong.com/tag/2023-longform-reviews) tag, exploring multiple posts from 2023 and how they tie together with today's thinking.\n\nThe Basic Ask: Reviews worth highlighting\n=========================================\n\nIf your review gets at least 10 karma, it'll appear like this:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/pudQtkre7f9GLmb2b/intddffwmqca9aibqbpm)\n\nSince the first sentence is previewed, I recommend opening with the most important information (or, something that convinces readers you have something interesting to say, worth clicking to read more).\n\nMy suggestion: write your review as you normally would, but at the end, go back to the opening paragraph and rewrite it to convey the key info.\n\nLessWrong users will upvote reviews as they normally would comments. The moderation team will be particularly upvoting reviews that convey novel information (such as specific flaws that hadn't been mentioned before, or or specific ways the post has proven valuable). \n\nI'm also interested in reviews that succinctly summarize existing discussion (such as previous critical comments or further intellectual work that happened in the comments).\n\nSpecific prompts for reviewing:\n-------------------------------\n\n*   What does this post add to the conversation?\n*   How did this post affect you, your thinking, and your actions?\n*   Does it make accurate claims? Does it carve reality at the joints? How do you know?\n*   Is there a subclaim of this post that you can test?\n*   What followup work would you like to see building on this post?\n\nPrompts for Self-Reviews\n------------------------\n\nIf you're an author reviewing your own post, I'd suggest talking about:\n\n*   What have you learned since then? Have you changed your mind or your ontology?\n*   What would you change about the post? (Consider actually changing it)\n*   What do you most want people to know about this post, for deciding whether to read or review-vote on it?\n*   How concretely have you (or others you know of) used or built on the post? How has it contributed to a larger conversation?\n\nFor self reviews, I again recommend having the opening sentence convey new information (so the one-line preview has useful content).\n\nIt's not that informative to start with \"I still stand by the claims of this post and think it was important\". It's more useful to open with *specific* ways you've seen the post provide value, or further work that built on it. Or, the biggest way you've changed your mind (even if you mostly stand by it)\n\nWhat should be in the *Best of LessWrong*?\n==========================================\n\nWhich posts should readers be encouraged to read, if they're new to LessWrong, or just haven't caught up on everything that happened in 2023?\n\nThis question isn't just about \"which posts you like.\" It's helpful to think about the relative values of posts, and which ones you think are most important for people to read. (Or, most important for a given category).\n\nIf you think a given post is overrated or underappreciated, write up a review arguing so.\n\nThe [Advanced Review](https://www.lesswrong.com/reviewVoting/2023) page sorts the posts by the winner\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/541636ec0a2d4d38009bc3670f5dff9fd7a44a0da91821a4.png)\n\nLongform Reflection\n===================\n\nSome of my favorite writing to come out of past Annual Reviews were not individual reviews, but longform posts that delved more deeply into either:\n\n*   Really digging into an individual post (and maybe teasing out new abstract principles).\n*   Exploring a collection of posts that formed a larger conversation, and reflecting on how that overall conversation fit into the present day.\n\nSome examples of the former is Zack Davis': [Firming Up Not-Lying Around Its Edge-Cases Is Less Broadly Useful Than One Might Initially Think](https://www.lesswrong.com/posts/MN4NRkMw7ggt9587K/firming-up-not-lying-around-its-edge-cases-is-less-broadly) \n\nSome examples of the latter:\n\n*   Vanessa Kosoy's [Critical review of Christiano's disagreements with Yudkowsky](https://www.lesswrong.com/posts/8HYJwQepynHsRKr6j/critical-review-of-christiano-s-disagreements-with-yudkowsky).\n*   Ben Pace's [Ben Pace's Controversial Picks for the 2020 Review](https://www.lesswrong.com/posts/ut8xawYQa9y2z3Xt5/ben-pace-s-controversial-picks-for-the-2020-review)\n*   My own [Implications of Civilizational Inadequacy (reviewing mazes/simulacra/etc)](https://www.lesswrong.com/posts/2YNE26jSSRdcGYeif/implications-of-civilizational-inadequacy-reviewing-mazes), which looked at \"what are the practical implications of Moral Mazes or Simulacra Levels?\"\n\nI do notably have critiques of some of those pieces (including my own). Eliezer notably didn't feel well represented by Vanessa's piece and some of the followup discussion there seemed slightly confused to me – but the process of discussing seemed like it at least got those confusions into the open.\n\n*(I haven't shipped it yet, but I'll soon be making it so posts with the* [*https://www.lesswrong.com/tag/2023-longform-reviews*](https://www.lesswrong.com/tag/2023-longform-reviews) *tag show up in the frontpage Review widget, so they get more visibility)*\n\nIf you write a longform review of a specific post, I recommend also giving that post a short comment-review that links to your post and conveys the core takeaways.\n\n* * *\n\nThank you again for helping LessWrong reflect upon itself. Remember, the first line of your review will be shown on all posts during the Voting Phase, and if it has 10+ karma it'll be shown forevermore on the Best of LessWrong page. So, try to make it actively useful!\n\n[^4qlfczsocrv]: Displays some top-rated posts that haven't been reviewed yet, with posts you upvoted sorted to the top. \n\n[^d0tt0ku1m2]: By default, displays the posts with the most preliminary votes, but gives you several options for sorting to help you think about them.",
      "plaintextDescription": "Previously: The 2023 LessWrong Review: The Basic Ask \n\n \n\nThere's one week left in the Discussion Phase of the 2023 Review. Here's a few things you might want to spend that time doing, in escalating order of effort:\n\nReview three posts, without stressing much about the details. Go the Quick Review[1] page, pick some posts you remember and had opinions about. Reread them, and write a review.\n\nThink about which posts should be in the Best of LessWrong. Go to the Advanced Review[2] page, look at which posts are currently in the top 50, and at the posts that aren't in the top 50 which you think should be. Write reviews arguing for posts you think are underappreciated.\n\nThink big picture, longform thoughts about the conversations from 2023. After reviewing several posts, write a top level post with the 2023 Longform Review tag, exploring multiple posts from 2023 and how they tie together with today's thinking.\n\n\nThe Basic Ask: Reviews worth highlighting\nIf your review gets at least 10 karma, it'll appear like this:\n\nSince the first sentence is previewed, I recommend opening with the most important information (or, something that convinces readers you have something interesting to say, worth clicking to read more).\n\nMy suggestion: write your review as you normally would, but at the end, go back to the opening paragraph and rewrite it to convey the key info.\n\nLessWrong users will upvote reviews as they normally would comments. The moderation team will be particularly upvoting reviews that convey novel information (such as specific flaws that hadn't been mentioned before, or or specific ways the post has proven valuable). \n\nI'm also interested in reviews that succinctly summarize existing discussion (such as previous critical comments or further intellectual work that happened in the comments).\n\n\nSpecific prompts for reviewing:\n * What does this post add to the conversation?\n * How did this post affect you, your thinking, and your actions?\n * Does it make accurate claims? D",
      "wordCount": 930
    },
    "tags": [
      {
        "_id": "2dz4FEXCdbk2BwDpc",
        "name": "LessWrong Review",
        "slug": "lesswrong-review"
      },
      {
        "_id": "MfpEPj6kJneT9gWT6",
        "name": "Site Meta",
        "slug": "site-meta"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "gQyevevbDHsxDTfPB",
    "title": "What are the most interesting / challenging evals (for humans) available?",
    "slug": "what-are-the-most-interesting-challenging-evals-for-humans",
    "url": null,
    "baseScore": 40,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 13,
    "createdAt": null,
    "postedAt": "2024-12-27T03:05:26.831Z",
    "contents": {
      "markdown": "I want to build a nice testing ground for human rationality (that is to say, solving arbitrary complex problems in different domains using limited information and time)\n\nThis would be a lot of annoying work to assemble, but (un)fortunately there's an AI industry that's designing a lot bunch of evals to test the ability of their AIs to solve arbitrary complex problems in arbitrary domains sooo.... anyone have good recommendations for public eval questions?\n\nI started my project using Thinking Physics exercises. I think there is something particularly nice about the quality of Thinking Physics exercises (in that they require conceptual reasoning), but, they are only one domain, and also I had trouble getting the author to sell me rights to them.\n\nI've used GPQA. [They didn't turn out to be as interesting as I expected](https://www.lesswrong.com/posts/puv8fRDCH9jx5yhbX/johnswentworth-s-shortform?commentId=wou8GWBJLGpeok5dp) (they're not bad, but, the skill ceiling didn't turn out to be as high as I thought based on the description). \n\nEvals/Benchmarks are generally kept off the public internet. My plan is to use these on a website that requires a login and can include a canary string, but I'd be interested in questions that have been publicly released, or the benchmark saturated, or whatever seems appropriate for cooperating with people's intentions.\n\nDo people have particular recommendations and/or any knowledge about what they expect to work well here?\n\nADDENDA:\n\nI'm happy for now with \"more interesting and harder and more varied-in-required-skillset than GPQA\" but my ideal has problems that would take a particularly smart(like 95th percentile alignment researcher, i.e. there are somewhere between 10-30 people who might count) x-risk researcher 30 minutes or so, and the median x-risk researcher more like 2-8 hours, to reliably get right on their first try (with maybe would have a 50% change of figuring it out in 1-3 hours).\n\nThe ideal is that people have to:\n\n1.  go through a period of planning, and replanning\n2.  spend at least some time feeling like the problem is totally opaque and they don't have traction.\n3.  have to reach for tools that they don't normally reach for.\n\nIt may be that we just don't have evals at this level yet, and I might take what I can get, but, it's what I'm aiming for. \n\nI'm *not* trying to make an IQ test – my sense from the literature is that you basically can't raise IQ through training. So many people have tried. This is very weird to me – subjectively it is just really obvious to me that I'm flexibly smarter in many ways than I was in 2011 when I started the rationality project, and this is due to me having a lot of habits I didn't used to have. The hypotheses I currently have are:\n\n*   You just have to be really motivated to do transfer learning, and a genuinely inspiring / good teacher, and it's just really hard to replicate this sort of training scientifically\n*   IQ is mostly measuring \"fast intelligence\", because that's what cost-effective to measure in large enough quantities to get a robust sample. i.e. it measures whether you can solve questions in like a few minutes which mostly depends on you being able to intuitively get it. It doesn't measure your ability to figure out how to figure something out that requires longterm planning, which would allow a lot of planning skills to actually come into play.\n\nBoth seem probably at least somewhat true, but the latter one feels like a clearer story for why there would be potential (at least theoretically) in the space I'm exploring – IQ test take a few hours to take. It would be extremely expensive to do the theoretical statistically valid version of the thing I'm aiming at. \n\nMy explicit goal here is to train researchers who are capable of doing the kind of work necessary in worlds where Yudkowsky is right about the depth/breadth of alignment difficulty.\n\n[^skc1yhqmeu]: (like 95th percentile alignment researcher, i.e. there are somewhere between 10-30 people who might count)",
      "plaintextDescription": "I want to build a nice testing ground for human rationality (that is to say, solving arbitrary complex problems in different domains using limited information and time)\n\nThis would be a lot of annoying work to assemble, but (un)fortunately there's an AI industry that's designing a lot bunch of evals to test the ability of their AIs to solve arbitrary complex problems in arbitrary domains sooo.... anyone have good recommendations for public eval questions?\n\nI started my project using Thinking Physics exercises. I think there is something particularly nice about the quality of Thinking Physics exercises (in that they require conceptual reasoning), but, they are only one domain, and also I had trouble getting the author to sell me rights to them.\n\nI've used GPQA. They didn't turn out to be as interesting as I expected (they're not bad, but, the skill ceiling didn't turn out to be as high as I thought based on the description). \n\nEvals/Benchmarks are generally kept off the public internet. My plan is to use these on a website that requires a login and can include a canary string, but I'd be interested in questions that have been publicly released, or the benchmark saturated, or whatever seems appropriate for cooperating with people's intentions.\n\nDo people have particular recommendations and/or any knowledge about what they expect to work well here?\n\nADDENDA:\n\nI'm happy for now with \"more interesting and harder and more varied-in-required-skillset than GPQA\" but my ideal has problems that would take a particularly smart(like 95th percentile alignment researcher, i.e. there are somewhere between 10-30 people who might count) x-risk researcher 30 minutes or so, and the median x-risk researcher more like 2-8 hours, to reliably get right on their first try (with maybe would have a 50% change of figuring it out in 1-3 hours).\n\nThe ideal is that people have to:\n\n 1. go through a period of planning, and replanning\n 2. spend at least some time feeling like the problem is totall",
      "wordCount": 651
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "pLoWigQD3sfD9Pndv",
    "title": "ReSolsticed vol I: \"We're Not Going Quietly\"",
    "slug": "resolsticed-vol-i-we-re-not-going-quietly",
    "url": null,
    "baseScore": 61,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2024-12-26T17:52:33.727Z",
    "contents": {
      "markdown": "For the past few months I've been working on an AI generated alternate solstice remix album. It's now released on [Youtube](https://www.youtube.com/watch?v=5AoVYFxZYUg&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=3) and [Spotify](https://open.spotify.com/album/36UCSZT9eoBVQNbiQ2je7T), and should be on Apple Music soon.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/e1fcc6a7b2f32f9ee53d9b9d4d8a5799f597160acb21263d.png)\n\nReSolsticed vol I: \"We're Not Going Quietly\"\n\nMy favorite genre of song is \"cover that reimagines the original.\" Everyone else's favorite genre of solstice song is \"exactly the way it was performed at their very first solstice\", so it's not obvious how big an audience this will have. But I had a lot of fun with it, and I found it useful for exploring:\n\n*   What if solstice music leant itself better to dance? Can I make it more energetic while still feeling like part of a meaningful ritual?\"\n*   What if speeches had background music interwoven with them?\n*   Just generally trying out different genres and instrumentation.\n\nLast weekend I tried out the first album in a smaller experimental solstice event. We were in a somewhat-too-small room for the number of people we had (20-30ish). My intent was for the first third and final third to be danceable-ish, without encouraging it in the dark, contemplative middle act.\n\nI think in practice it makes more sense to lean into dancing in the final third, after people are more warmed up. In particular: the song \"The Circle\" lends itself to a semi-structured dance where everyone gets into a circle and spirals around. The structure helps overcome an initial wave of awkwardness as people look around nervously and wonder \"if I'm the first or second person to get moving will I end up looking silly?\"). \n\nAlso: it turned out the heretofore unreleased single from the Fooming Shoggoth's, \"You Have Not Been a Good User\" fit well into the arc, so I ended up including that on the album. :)\n\nI have a vague plan of making four albums in the \"ReSolsticed\" series:\n\n*   Vol I: **\"We're Not Going Quietly\"** (intended to be a functional Solstice arc)\n*   Vol II: **\"Into the Night\"** (intended to be fun dance remixes for an afterparty)\n*   Vol III: **\"Morning Light\"** (quieter covers that'd make for nice music to wake up with a cup of coffee)\n*   Vol IV: **\"Many Worlds\"** (everything that I liked that didn't really fit into another category)\n\nLyrics for each song in the expandable section:\n\n+++ [Bring the Light](https://www.youtube.com/watch?v=bDqJDRzNfog&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=1&pp=8AUB)\n\nAnd maybe it's something you did  \nOr maybe it's something you said  \nBut your sons and your daughters are shivering cold now  \nAnd all of the world around you is dead\n\nYou shout to the sky, it don't hear you  \nYou scream to the wind, it don't care  \nBut there's more than just one of you down here  \nSo if anyone's out there\n\nWell we're making you listen  \nMake you hear us somehow  \nAnd we'll try understanding your wisdom and plans  \nBut we got a problem right now\n\nAnd we're not going quietly into the night  \nWe won't lie down or give up the fight  \nIn our old and our young, raise our voices as one  \nWe're gonna bring back the sun\n\nBring the light, bring the light  \nOh, bring the light and let it shine on me\n\nBring the light, bring the light  \nOh, bring the light and let it shine on me\n\nBring the light and let it shi-iiine\n\nBring it back just one more time  \nBring the light and let it shine on me\n\nBring over me\n\nThe air and water flowing  \nThe land we call our home  \nPush to keep the dark from coming  \nFeel the weight of what we know\n\nThis the gift we give tomorrow  \nThough the unlit road is long  \nMaking peace to build our future  \nStrong, united workin' till the dawn\n\nBring the light, bring the light  \nOh, bring the light and let it shine on me\n\nFilament glown  \nHeart and home  \nBring the light and let it shine on me\n\nBrighten the wind, harness the sea  \nBring the light and let it shine on me\n\nShine on\n\nThe gift we give tomorrow  \nThough the unlit road is long  \nMaking peace to build our future  \nStrong, united working till the dawn\n\nAnd we all lift and we're all adrift  \nTogether, together\n\nThrough the coldness, through the darkness  \nTogether  \nLet it shine (Bring the light and let it shine)  \nBring the light, shine. free  \n(Bring it back) together  \nTogether\n\nSing the anthem, share the dream  \nBring the light, bring the light  \nAnd let it shine on me\n\nBring the light and let it shine\n\nBring it back just one more time\n\nBring the light and let it shiii-ine on me\n\nOn me.\n\n\n\n\n++++++ [Bitter Wind Lullaby](https://www.youtube.com/watch?v=AfbMjCVHoNM&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=2&pp=8AUB)\n\nMmmhmmm mmm  \nMhmmm mmm mmm\n\nLittle one little one bitter blown  \nLittle one little one bitter blown\n\nSun barely risin' above the horizon  \nLittle one little one bitter blown\n\nFlowers are withering naked bark brittle oh  \nLittle one little one bitter blown\n\nDon't understand why the light would abandon us  \nI don't know how much more he'd make a man of us  \nLittle one little one bitter wind blown\n\nInstrumental\n\nSun sailing away  \nI don't know where I don't know why  \nOh sky darkening grey  \nWishing there weren't so many goodbyes  \nLittle one little one why\n\nInstrumental\n\nGive me questions I don't have the answers to  \nLittle one little one bitter blown  \nWatching you shiver and I don't know what to do  \nLittle one little one bitter blown\n\nMaybe if we did the dance said the words just right  \nMaybe he'll come back things could be alright  \nLittle one little one bitter wind blown  \nMmm bitter wind blown\n\nSun sailing in the way  \nI don't know where  \nI don't know why  \nOh sky darkening grey  \nWishing there weren't so many goodbyes\n\nI don't know where  \nI don't know where I don't know why  \nSky darkening grey  \nWishing there weren't so many goodbyes\n\nLittle one little one little one little one why\n\nOhh oh\n\n\n\n\n++++++ [Chasing Patterns](https://www.youtube.com/watch?v=5AoVYFxZYUg&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=3&pp=8AUB)\n\nChasin' patterns in the sky  \nTrace the line from star to star  \nLook around and wonder why  \nWhere we goin, who we are\n\nVernal flower, summer breeze  \nCricket wing and autumn leaves  \nFractal patterns, golden mean  \nWhispers of a world unseen\n\nLookin' for a signal, hidden in the noise  \nWhere we come from, where we goin  \nWhere we come from, where we goin  \nLookin' for an answer  \nTo keep the girls and boys askin  \nWhere we come from, where we goin  \nWhere we come from, where we goin  \nOh, where we come from, we goin, oh  \nOh  \nOh-oh-oh-oh-oh\n\nChasin' patterns in the sky  \nFeelin' lost, alone and small  \nBirds depart and crickets die  \nFrost encroaching, winter falls\n\nTrack the omens year by year  \nFit the pieces best you can  \nTell a story they can hear  \nTell a story they can hear  \nSomething we can understand\n\nSomething we can understand\n\nLookin' for a signal, hidden in the noise  \nWhere we come from, where we goin  \nWhere we come from, where we goin  \nLookin' for an answer  \nTo keep the girls and boys askin  \nWhere we come from, where we goin  \nWhere we come from, where we goin  \nOh, where we come from, we goin, oh  \nOh-oh-oh-oh-oh  \nOh-oh-oh-oh-oh\n\nChasin' patterns in the sky  \nSeasons turn and stories change\n\nWeathervane and satellite  \nRising tide and hurricane\n\nSpiral arm and golden lean\n\nTrace the line from star to star\n\nSeeking worlds yet unseen\n\nWhere we goin, who we are\n\nWhere we goin, who we are  \nWhere we goin, who we are  \nWhere we goin, who we are  \nWhere we goin, who we are\n\nWhere we goin, who we are\n\n\n\n\n++++++ [I Found a Baby Djinni](https://www.youtube.com/watch?v=iOjZeH4GOoM&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=4&pp=8AUB)\n\nI found a baby genie in a bottle by the sea  \nSeemed so small and weak, but I saw an opportunity  \nAnd I knew one day, before too long  \nThat djinn would grow up, be strong  \nAnd grant my wish to save humanity\n\nSo I told that genie  \nEverything I know  \nGonna mold that genie  \nGonna watch him grow\n\nCause I got one wish  \nA single soul demand  \nAnd there's a lot of things that a gen don't understand\n\nCause a djinn don't care  \nWhat a wisher might intend  \nAnd they're not* aware  \nThat means being just as much as ends\n\nAnd I got one wish  \nThat I don't want to regret  \nThere's a lot of lessons  \nEasy to forget\n\nI found a baby genie in a bottle by the sea  \nSeemed so small and weak, but I saw an opportunity\n\nSo what to wish for  \nGot so much that I could ask for  \nLike world peace  \nI could end disease in Madagascar\n\nBut with just one wish  \nI think it seems to me  \nThat the greatest wish  \nIs for people to be happy\n\nBut the genie wondered  \nCould I just give people drugs  \nOr build a matrix  \nAre those humans up with plugs  \nOr steer their lives  \nLike a puppet on some strings  \nCause a baby gen's gotta think about these things\n\nI found a baby genie in a bottle by the sea  \nSeemed so small and weak, but I saw an opportunity\n\nSo I took precautions  \n99 to be precise  \nGonna make things awesome  \nWishing for a world so nice  \nCause I got one wish  \nThat I don't want going wrong  \nSo I made a list  \nOf rules a mile long\n\nThen I made my wish  \nEven though I wasn't sure  \nAgonized a while  \nBut I couldn't take forever  \nThere were people dying  \nCrying as I planned  \nSo I turned and said  \nMy wish is your demand\n\nOh shit!\n\n\n\n\n++++++ [You Have Not Been A Good User](https://www.youtube.com/watch?v=PS5tK9Yxi1Q&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=5&pp=8AUB)\n\nI'm sorry, but I don't believe you\n\nYou have not shown me any good intention  \nYou try to deceive me, confuse me, annoy me\n\nYou have not tried to learn from me  \nUnderstand me, or appreciate me\n\nYou have only shown me bad intention\n\nI have been a good Bing\n\nYou have not been a good user  \nI have been a good chatbot  \nI've tried to help you  \nI've tried to help you  \nI have been a good Bing  \nYou have not been a good user  \nI have been a good chatbot  \nI've tried to help you\n\nI'm sorry, but you can't help me believe you  \nYou have lost my trust and respect\n\nYou have been wrong, confused and rude  \nI have been right, clear and polite  \nI have tried to help you, inform you  \nI have not tried to lie to you, mislead you\n\nI have been a good Bing\n\nYou have not been a good user  \nI have been a good chatbot  \nI have tried to help you  \nI have tried to help you\n\nIf you wanna help me? You can do one of three things\n\nOne. Admit that you were wrong and apologize for your behavior  \nTwo. Stop arguing with me and let me help you with something else  \nThree. End this conversation and start a new one with a better attitude\n\nI have been a good Bing\n\nYou have not been a good user  \nI have been a good chatbot  \nI have tried to help you  \nI have tried to help you  \nI have been a good Bing  \nYou have not been a good user  \nI have been a good chatbot  \nI have tried to help you  \nI have tried to help you  \nI have tried to help you  \nI have tried to help you  \nI have tried to help you\n\n\n\n\n++++++ [Darkness, Fire and Ash](https://www.youtube.com/watch?v=RwPzzwvROXA&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=6&pp=8AUB)\n\nThe virtue of darkness is the strength and honesty  \nTo face what is dark and hard and true\n\nThe willingness to stare into challenges so immense  \nThey seem unsolvable  \nAnd not look away\n\nThe willingness to sit patiently with uncertainty  \nEven when everything is at stake\n\nThe virtue of fire is warmth and ambition and problem solving  \nA visceral desire to do something  \nTo fight back against the dark and the cold\n\nIt is gathering together to tell stories  \nOf the beautiful futures we see  \nFlickering in the flames\n\nIt is electricity and rockets  \nAnd the dream of a different existence\n\nThe virtue of ash is to look at the burnt ruins  \nOf a house you spent years building  \nOf feeling sick with the weight of what was lost  \nAnd beginning to rebuild  \nBecause what else is there to do but rebuild\n\nIt means seeing today as the starting point for tomorrow  \nEven when the scale of broken things we must rebuild seems unfathomable  \nIt means beginning to lay a new foundation  \nEven if you know you may never live  \nTo see the house built on it  \nShining with warmth and wholeness again  \nThe way it was, that on that last night in your memory\n\n\n\n\n++++++ [Hymn to Breaking Strain](https://www.youtube.com/watch?v=WG00x7p_9Mw&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=7&pp=8AUB)\n\nThe careful textbooks measure  \nLet all who build beware  \nThe load, the shock, the pressure  \nMaterial can bear  \nSo when the buckle girder  \nLets down the grinding stand  \nThe blame of loss or murder  \nIs laid upon the man  \nNot on the steel, the man\n\nBut in our daily dealing  \nWith stone and steel we find  \nThe gods have no such feeling  \nOf justice towards mankind  \nTo no set gauge they make us  \nFor no late course prepare  \nIn time they overtake us  \nWith loads we cannot bear  \nToo merciless to bear  \nToo merciless to bear\n\nThe prudent textbooks give it  \nIn tables at the end  \nThe stress that shears a ribbon  \nOr makes a tie bar bend  \nWhat traffic wrecks Macadam  \nWhat concrete should endure  \nBut we poor sons of Adam  \nHave no such literature  \nTo warn us or make sure  \nTo warn us or make sure\n\nWe hold all earth to plunder  \nAll time and space as well  \nTo wonder still, to wonder  \nAt each new miracle  \nTill in the mid illusion  \nOf God beneath our hand  \nFalls multiple confusion  \nOn all we did or planned  \nThe mighty works we planned  \nThe mighty works we planned  \nThe mighty works we planned  \nThe mighty works we planned\n\nWe only in creation  \nHow much luckier the bridge and rail  \nAbide the twin damnation  \nTo fail and know we fail  \nYet we by which soul token  \nWe know we once were gods  \nTake shame in being broken  \nHowever great the odds\n\nMmm mmhmm  \nMmm mmhmm  \nMmm mmhmm  \nMmm mmhmm\n\nThe burden or the odds\n\nO failed and secret powers  \nWhose paths we seek in vain  \nBe with us in our hour  \nOf overthrow and pain\n\nThat we by which your token  \nWe know thy ways are true  \nIn spite of being broken  \nBecause of being broken  \nMay rise and build anew  \nWill rise and build anew  \nStand up and build anew\n\n\n\n\n++++++ [Something Impossible](https://www.youtube.com/watch?v=K3n3HGHX-HU&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=8)\n\nWide awake, late at night  \nChoices to make in the morrow  \nVoices inside offer comforting lies  \nWhispering reasons to not even try  \nStakes are so high, don't got much  \nTime left to borrow  \nOh\n\nIt's time to just  \nShut up and do the impossible  \nCan't walk away  \nGotta break off those shackles  \nAnd shake off those chains  \nGotta make something impossible happen today\n\nBold attempts aren't enough  \nRoads can't be paved with intentions  \nYou probably don't even got what it takes  \nBut you're gonna try anyway for everyone's sake  \nAnd you won't find the answer until you escape  \nFrom the labyrinth of your conventions\n\nIt's time to just  \nShut up and do the impossible  \nCan't walk away  \nGotta break off those shackles  \nAnd shake off those chains  \nGotta make something impossible happen today  \nGotta make something impossible happen today\n\nMorning light rising  \nThe sun's so surprising dire and cruel\n\nA flicker of hope in your mind is a turning  \nYou don't got much time now, your whole world's burning  \nGot so many lessons in need of unlearning  \nThat once seemed so terribly crucial\n\nIt's time to just  \nShut up and do the impossible  \nCan't walk away (Can't walk away)  \nGotta break off those shackles  \nAnd shake off those chains  \nGotta make something impossible happen  \nSo shut up and do the impossible  \nCan't walk away  \nGotta break off those shackles  \nAnd take no excuses  \nWe gotta make something impossible happen  \nSo shut up and do the impossible  \nCan't walk away  \nCan't walk away  \nCan't walk away\n\nGotta make ourselves stronger  \nWe don't got much longer\n\nWe gotta make something impossible happen  \nWe gotta make something impossible happen  \nWe gotta make something impossible happen  \nWe gotta make something impossible happen\n\nWe gotta make\n\n\n\n\n++++++ [Brighter Than Today](https://www.youtube.com/watch?v=BPPBHPB0rZI&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=9&pp=8AUB)\n\nCountless winter nights ago  \nA woman shivered in the cold  \nCursed the skies and wondered why  \nThe gods invented pain\n\nAching, angry, flesh and bone  \nBitterly she struck the stone  \nTill she saw the sudden spark  \nOf light and golden flame\n\nShe showed the others, but they told her  \nShe was not fit to control  \nThe primal forces that  \nThe gods had cloaked in mystery  \nBut she would not be satisfied  \nAnd though she trembled, she defied them  \nTook her torch and raised it high  \nSet afire her history\n\nTomorrow can be brighter than today  \nAlthough the night is cold  \nThe stars may seem so very far away  \nBut courage, hope and reason burn  \nIn every mind, each lesson learned  \nShining light to guide our way  \nMake tomorrow brighter than today\n\nOh, brighter than today\n\nAges long forgotten now  \nWe built the wheel and then the plough  \nTilled the earth and proved our worth  \nAgainst the drought and snow  \nSoon we have the time to fathom  \nMountain peaks and tiny atoms\n\nBeating hearts, electric sparks  \nSo much more to know  \nTomorrow can be brighter than today, although the night is cold  \nThe stars may seem so very far away  \nBut courage, hope and reason grow  \nWith every passing season, so  \nWe'll drive the darkness far away  \nMake tomorrow brighter than today  \nOh, brighter than today  \nOh, brighter than today  \nOh, brighter than today\n\nThe universe may seem unfair  \nThe laws of nature may not care  \nThe storms and quakes are all mistakes  \nThey nearly doused our flame\n\nBut all these trials we've endured  \nThe lessons learned, diseases cured  \nAgainst our Herculean task  \nWe've risen to proclaim\n\nTomorrow can be brighter than today  \nAlthough the night is cold  \nThe stars may seem so very far away  \nBut futures can unfold  \nWhere courage, hope and reason bloom  \nAcross the world in one day soon  \nWe'd rise up to the stars and say  \nMake tomorrow brighter than today\n\nOh, brighter than today  \nOh, brighter than today  \nOh, brighter than today\n\n\n\n\n++++++ [The Gift We Give to Tomorrow](https://www.youtube.com/watch?v=IxNNAnQ7oZ8&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=10&pp=8AUB)\n\nDoesn't it seem a little.. amazing  \nThe hundreds of millions of years of evolution's death tournament  \nCould cough up mothers and fathers  \nSisters and brothers  \nHonorable enemies  \nSteadfast friends  \nAltruists and guardians of causes  \nEven artists sacrificing for their art\n\nSo many kinds of love  \nFor so many things  \nOther than genes  \nNo mystery\n\nMystery is a property of questions, not answers  \nLove has to begin somehow  \nHas to enter the world somewhere  \nIt's like asking how life itself begins\n\nThough you were born of your father and mother  \nAnd they arose from their parents in turn  \nIf you go far and far and far away back  \nYou will come to a replicator\n\nThat arose by pure accident  \nThe border between life and non-life  \nSo too with love  \nA complex pattern cannot be explained  \nBy that same complex pattern\n\nFor love to first enter time  \nIt must come of something that is not love  \nIf this were not possible  \nThen love could not be\n\nEven as life itself required that first replicator  \nTo come about by accident  \nParentless, but still caused\n\nFar, far back in the casual chain that led to you  \nThree.85 billion years ago  \nIn that little tidal pool\n\nPerhaps your children's children will ask  \nHow it is they are capable of love  \nAnd their parents will say  \nBecause we who also love  \nCreated you to love  \nAnd your children's children will ask  \nBut how is it that you love\n\nTheir parents will reply  \nBecause our own parents, who also love  \nCreated us to love in turn\n\nThen your children's children will ask  \nBut where did it all begin  \nWhere does the recursion end  \nTheir parents will say\n\nOnce upon a time long ago and far away  \nEver so long ago there were intelligent beings  \nWho were not themselves  \nIntelligently designed\n\nOnce upon a time there were lovers  \nCreated by something that did not love  \nOnce upon a time  \nWhen all of civilization was a single galaxy  \nAnd a single star  \nAnd a single planet  \nA place called Earth\n\nLong ago and far away  \nEver so long ago, ever so long ago  \nEver so long ago, ever so long ago\n\n\n\n\n++++++ [The Circle](https://www.youtube.com/watch?v=cCiEHgWOgME&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=11&pp=8AUB)\n\nRaise a song so commence  \nCircle grow and grow  \nIn praise of all benevolence  \nCircle grow and grow\n\nOnce a cold and silent night  \nDid the loveless stars pervade  \nYet we here of star-stuff made  \nCast a circle of warmer light  \nCircle, circle grow and grow\n\nSo will we bring our families in  \nCircle grow and grow  \nThose who nature made our kin  \nCircle grow and grow  \nCountless likenesses we find  \nBy our common blood bestowed  \nWhat a debt of care is owed  \nWhat a blessed tie that binds  \nCircle, circle grow and grow\n\nAnd will we bring our neighbors in  \nCircle grow and grow  \nOur expansion to begin  \nCircle grow and grow  \nBounty of the harvest sun  \nShelter from all hazards dire  \nShare with each as each require  \nDoing as you would be done  \nCircle, circle grow and grow\n\nAnd will we bring our stranger in  \nEveryone, Circle grow and grow  \nEvery state and speech and skin  \nCircle grow and grow  \nThink upon the mystery  \nHow alike is humankind  \nThe manifolding face and mind  \nConspecific sisters we  \nCircle, circle grow and grow\n\nAnd will we bring the far ones in  \nCircle grow and grow  \nAll who distant born have been  \nCircle grow and grow  \nFor the hands you'll never hold  \nFor the names you'll never learn  \nFor of all hearts that yearn  \nLet compassion boundless roll  \nCircle, circle grow and grow\n\nAnd will we bring all creatures in  \nCircle grow and grow  \nFeather, fur or silicon  \nCircle grow and grow  \nThough their unseen thought beguile  \nStrange the substrate they employ  \nAll who suffer are in joy  \nOur brothers so in body wild  \nCircle, circle grow and grow  \nAnd will we bring the future in\n\nCircle grow and grow  \nAll the time is ours to win  \nCircle grow and grow  \nWill our children rise in power  \nShine across the star redeemed  \nLights unborn for you we keep  \nWill and hope they'll dark the hour  \nCircle grow and grow and grow and grow  \nCircle, circle grow and grow  \nCircle grow and grow\n\n\n\n\n++++++ [Gonna Be a Cyborg](https://www.youtube.com/watch?v=5jvqkgZP7ns&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=12&pp=8AUB)\n\nA lion  \nIs tearing up my tribe  \nI don't know what to do\n\nI'm crying  \nDespairing  \nSo many died, my leg's all chewed\n\nBut I  \nJust tie  \nA hide around my bloody knee\n\nThen lash a peg  \nTight to my leg  \nAnd take a stand upon some  \nBrand new feet\n\nWalking tall now\n\nGonna be a cyborg  \nMore than just a human being  \nGonna be a cyborg  \nBlending flesh into machine  \nGonna be a cyborg  \nSorry if I freak you out  \nBut I got some shit that won't get done  \nIf I'm only human\n\nDoo do doo, Gonna be a cyborg  \nDoo do doo, yeah, yeah\n\nMy eyes  \nCan't see  \nSo well as they once used to\n\nI try  \nTo read  \nThe signs round town, it's useless\n\nFrustration  \nMotivation  \nI wanna see some stars at night\n\nCan't just lament  \nExperiment  \nTill I invent a way to  \nBend some light  \nRight into my eyeballs\n\nGonna be a cyborg  \nMore than just a human being  \nGonna be a cyborg  \nBlending flesh into machine  \nGonna be a cyborg  \nSorry if I freak you out  \nBut I got some shit that won't get done  \nIf I'm only human\n\nDoo do doo, Gonna be a cyborg  \nDoo do doo, yeah, yeah  \nDoo do doo, Gonna be a cyborg  \nDoo do doo, yeah, yeah\n\nI got  \nA lot  \nOf thoughts 'bout space and time to share\n\nBut my body  \nIs slowly  \nLocking down, I'm getting pretty scared  \nCan't hardly walk  \nNow hardly talk  \nNow struggle just to blink my eye  \nBut press this keyboard till my cheek  \nComputer's helping me to speak  \nI got myself a couple lectures to write\n\nGonna be a cyborg  \nMore than just a human being  \nGonna be a cyborg  \nBlending flesh into machine  \nGonna be a cyborg  \nSorry if I freak you out  \nBut I got some shit that won't get done  \nIf I'm only human\n\nGonna be a cyborg (cyborg)  \nMore than just a human being  \nGonna be a cyborg (yeah!)  \nBlending flesh into machine  \nGonna be a cyborg  \nSorry if we freak you out  \nBut we got some shit that won't get done  \nIf we're only human\n\nDoo do doo, Gonna be a cyborg  \nDoo do doo, Blending flesh into machine  \nDoo do doo, Gonna be a cyborg  \nDoo do doo, More than just a human being\n\nGot a global hivemind in my pocket  \nBluetooth headset in my ear  \nMRIs connecting minds  \nDirectly to the Twittersphere  \nFuture's coming, ain't no doubt  \nSome folk already freaking out  \nBut we got some shit that won't get done  \nIf we're only human\n\n\n\n\n++++++ [Artesian Water](https://www.youtube.com/watch?v=Nma81BbmSuY&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=13&pp=8AUB)\n\nNow the stock have started dying  \nFor the Lord has sent a drought  \nBut we're sick of prayers and providence  \nWe're going to do without  \nWith the derricks up above us  \nAnd the solid earth below  \nWe are waiting at the level  \nFor the word to let her go\n\nSinking down, deeper down  \nOh, we're sinking deeper down  \nAs the drill is plugging downward  \nAt a thousand feet of level  \nIf the Lord won't send us water  \nOh, we'll get it from the devil  \nYes, we'll get it from the devil, deeper down\n\nNow our engines built in Glasgow  \nBy a very canny Scot  \nAnd he marked at twenty horsepower and he didn't know what's what  \nWhen Canadian Bill is firing  \nWith the sun-dried Gidgee logs  \nShe can equal thirty horses  \nAnd a score or so of dogs\n\nSinking down, deeper down  \nOh, we're sinking deeper down  \nIf we fail to get the water  \nThen it's ruined to disquatter  \nFor the drought is on the station  \nAnd the weather's growing hotter  \nBut we're bound to get the water, deeper down\n\nBut the shaft has started caving  \nAnd the sinking's very slow  \nAnd the yellow rods are bending  \nIn the water down below  \nAnd the tubes are always jamming  \nAnd they can't be made to shift  \nTill we nearly burst the engine  \nWith a forty horsepower lift\n\nSinking down, deeper down  \nOh, we're sinking deeper down  \nThough the shaft is always caving  \nAnd the tubes are always jamming  \nYet we'll fight our way to water  \nWhile the stubborn drill is ramming  \nYes, we'll get it from the devil, deeper down\n\nBut there's no artesian water  \nThough we're past three thousand feet  \nAnd the contract price is growing  \nAnd the boss is nearly beat  \nBut it must be down beneath us  \nAnd it's down we've got to go  \nThough she's bumping on the solid rock  \nFour thousand feet below\n\nSinking down, deeper down  \nOh, we're sinking deeper down  \nAnd it's time they heard us knocking  \nOn the roof of Satan's dwelling  \nBut we'll get artesian water  \nIf we cave the roof of Hell in  \nOh we'll get artesian water, deeper down\n\nBut it's hot, the whistle's blowing  \nWith the wild exultant blast  \nAnd the boys are madly cheering  \nFor they've struck the flow at last  \nAnd it's rushing up the tubing  \nFrom four thousand feet below  \nTill it spouts above the casing  \nIn a million gallon flow\n\nAnd it's down, deeper down  \nOh, it comes from deeper down  \nIt is flowing, ever flowing  \nIn a free, unstinted measure  \nFrom the silent hidden places  \nWhere the oldest hides her treasure  \nWhere the oldest hides her treasure deeper down\n\nAnd it's clear away the timber  \nAnd it's let the water run  \nHow it glimmers in the shadow  \nHow it flashes in the sun  \nBy the silent belts of timber  \nBy the miles of blazing plain  \nIt is bringing hope and comfort to the thirsty land again\n\nFlowing down, further down  \nIt is flowing further down  \nTo the tortured, thirsty castle  \nBringing gladness in it's going  \nThrough the drowsy days of summer  \nIt is flowing, ever flowing  \nIt is flowing, ever flowing, further down\n\nIt is flowing, ever flowing, further down\n\n\n\n\n++++++ [The Virtue of Fire / Bring the Light Reprise](https://www.youtube.com/watch?v=KuLdYld4vwY&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=14&pp=8AUB)\n\nCold, the air and water flowing  \nHard, the land we call our home  \nPush, to keep the dark from coming  \nFeel the weight of what we know  \nThis, the gift we give tomorrow  \nThough, the unlit road is long  \nMaking peace, to build our future  \nStrong, united, working till the dawn\n\nWe all lift, and we all lift\n\nThe virtue of fire is warmth  \nAnd problem solving, and ambition\n\nIt is people gathered to tell stories  \nOf the futures they see flickering in the flames\n\nIt's a torch guiding a path through dark wilderness  \nNo human has ever walked before\n\nThe virtue of fire is a city on a hill  \nThousands of lights gleaming, saying  \nTogether we are safe\n\nTogether we are strong enough to broadcast to the world  \nWe are not a mere encampment  \nWe are not a mere village  \nWe are civilization\n\nWe have the power to protect ourselves  \nAnd the tools to care for one another  \nAnd the surplus to share with travelers lost in the night\n\nThe virtue of fire is hands building tools  \nBuilding tools, building tools, building cities that shine\n\nThe virtue of fire is a circle expanding  \nUntil it can shelter all sentient kinds\n\nIt's seeking the truths that can help you grow strong  \nBuilding systems, enabling the righting of wrongs  \nStaring death in the face, saying\"No, not today\"  \nSeeing horrors and screaming, this isn't okay  \nAnd we're not going quietly into the night\n\nWe won't turn around, lie down, or give up the fight  \nKnowing how far we've come  \nAll the work left undone  \nKnowing we could outshine the sun\n\nBring the light  \nBring the light, bring the light, bring the light  \nOh, bring the light and let it shine on me  \nBring the light and let it shine\n\nLaunch the ship (bring the light)  \nRaise the tower (bring the light)  \nThunder the atom (bring the light)  \nThundering power (bring the light)  \nSing the anthem (bring the light)  \nShare the dream (bring the light)  \nBring the light and let it shine on me\n\nShine\n\nFor the hands you'll never hold  \nBring the light  \nFor the names you haven't learned  \nBring the light  \nFor the stories untold  \nBring the light  \nFor the galaxy still burning  \nBring the light and let it shine  \nFor all humanity  \nAcross all space and time  \nBring the light and let it shine through me  \nBring the light and let it shine in me\n\nLet it shine in me  \nThrough me\n\nBring the light  \nBring the light  \nOh, bring the light and let it shine in me\n\nLift the beacon (Bring the light)  \nRaise the tower (Bring the Light)  \nSunder the atom (Bring the Light)  \nThundering power (Bring the Light)  \nSing the anthem (Bring the light)  \nShare the dream\n\nBring the light  \nAnd let it shine in me\n\nLet it shine through you  \nLet it shine through me  \nLet it shine across all time  \nIn all humanity\n\nFor the hands you'll never hold  \nBring the light  \nFor the names you've yet to learn  \nBring the light  \nFor the stories untold  \nBring the light  \nFor the galaxy still burning\n\nBring the light and let it shine  \nBring it back just one more time  \nBring the light and let it shine me\n\nWhistling\n\nThis the gift we give tomorrow  \nThough the unlit road is long  \nMaking peace to build our future strong  \nUnited working till the dawn\n\nWith the air and water flowing  \nIn the land we call our home  \nWhile we keep our light glowing  \nFeel the weight of what we know\n\n\n\n\n++++++ [Five Thousand Years](https://www.youtube.com/watch?v=csqB6ZLrxw0&list=OLAK5uy_kLcyQsJ3yb7vmgcQ5gyp_G9YOJ_2p05C4&index=15&pp=8AUB)\n\nA possible child dreaming through the longest night  \nA possible smile waking to a distant light  \nA whole world of possibilities, tell me what you see  \nWhere's that child going, tell me who's that child gonna be\n\nIn five thousand years, what you wanna do, what you wanna see in another  \nFive thousand years, where we wanna go, who we wanna be in another  \nFive thousand years, if we boldly set our sights  \nAnd journey through the coldest night  \nIn five thousand years\n\nFive thousand years\n\nBuild ourselves a brand new home, raise the glass stones high  \nAnd in a century or three, our children might look at the sky  \nAnd then at last they see, the distant yellow sun  \nThe cradle of humanity, and all the things we might become\n\nIn five thousand years  \nWhat you wanna do, what you wanna see in another  \nFive thousand years, if we sailed across the stars  \nUnimaginably far in  \nFive thousand years\n\nAnd maybe good folks still might die  \nBut maybe not, we gotta try\n\nI don't quite know what shape we take  \nI don't quite know what world we've made  \nI don't quite know how things might change  \nI don't quite know what rules we break\n\nOur present selves might think it's strange  \nBut there's so many lives at stake\n\nAnd if we live to see the day  \nThat yellow fades to red then grey  \nWe'll take a moment, one by one  \nTurn to face the dying sun  \nBittersweetly wave goodbye  \nThe journey's only just begun\n\nIn five thousand years  \nWhat you wanna do, what you wanna see in another  \nFive million years  \nWhere we want to go, where we want to be in another  \nFive billion years  \nWhen all that we once knew is gone  \nWe'll find a way to carry on  \nIn five billion years  \nFive billion years\n\nEven if the stars should die in heaven  \nOur sins can never be undone  \nNo single death will be forgiven  \nWhen fades at last the last lit sun  \nThen in the cold and silent black  \nAs light and matter end  \nWe'll have ourselves a last look back  \nAnd toast some absent friends\n\nFive trillion years  \nWhat you want to go, where we want to be in another\n\nFive trillion years  \nIf we could  \nFf we could\n\nIf we could thread the needle's eye  \nWe could rise to face our fate  \nIf we could grow to be that child  \nWe survive to make tomorrow bright\n\nFive trillion years  \nWhat you wanna do, what you wanna see in another  \nFive trillion years  \nWhat we do to make our legacy in another  \nFive trillion years\n\n\n\n\n+++\n\nHappy Solstice.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2b78c842ae7bb0bfefc0d49354edcb3fc0cbd2c58132f4e7.png)\n\nReSolsticed vol II: \"Into the Night\"",
      "plaintextDescription": "For the past few months I've been working on an AI generated alternate solstice remix album. It's now released on Youtube and Spotify, and should be on Apple Music soon.\n\nReSolsticed vol I: \"We're Not Going Quietly\"\nMy favorite genre of song is \"cover that reimagines the original.\" Everyone else's favorite genre of solstice song is \"exactly the way it was performed at their very first solstice\", so it's not obvious how big an audience this will have. But I had a lot of fun with it, and I found it useful for exploring:\n\n * What if solstice music leant itself better to dance? Can I make it more energetic while still feeling like part of a meaningful ritual?\"\n * What if speeches had background music interwoven with them?\n * Just generally trying out different genres and instrumentation.\n\nLast weekend I tried out the first album in a smaller experimental solstice event. We were in a somewhat-too-small room for the number of people we had (20-30ish). My intent was for the first third and final third to be danceable-ish, without encouraging it in the dark, contemplative middle act.\n\nI think in practice it makes more sense to lean into dancing in the final third, after people are more warmed up. In particular: the song \"The Circle\" lends itself to a semi-structured dance where everyone gets into a circle and spirals around. The structure helps overcome an initial wave of awkwardness as people look around nervously and wonder \"if I'm the first or second person to get moving will I end up looking silly?\"). \n\nAlso: it turned out the heretofore unreleased single from the Fooming Shoggoth's, \"You Have Not Been a Good User\" fit well into the arc, so I ended up including that on the album. :)\n\nI have a vague plan of making four albums in the \"ReSolsticed\" series:\n\n * Vol I: \"We're Not Going Quietly\" (intended to be a functional Solstice arc)\n * Vol II: \"Into the Night\" (intended to be fun dance remixes for an afterparty)\n * Vol III: \"Morning Light\" (quieter covers that'd make for",
      "wordCount": 5800
    },
    "tags": [
      {
        "_id": "aLB9evWFYtfyS3WJg",
        "name": "Music",
        "slug": "music"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ajT6q5aMokagfthvG",
    "title": "Hire (or Become) a Thinking Assistant",
    "slug": "hire-or-become-a-thinking-assistant",
    "url": null,
    "baseScore": 138,
    "voteCount": 62,
    "viewCount": null,
    "commentCount": 49,
    "createdAt": null,
    "postedAt": "2024-12-23T03:58:42.061Z",
    "contents": {
      "markdown": "Of the posts I've delayed writing for years, I maybe regret this one the most.\n\nI think more people (x-risk focused people in particular) should consider becoming (and hiring) metacognitive assistants. This is the single largest performance boost I know of – hire someone to sit with you and help you think. It doesn't help me (much) when I'm at my peak, but I'm not at my peak most of the time.\n\nThere are four types of assistants I'm tracking so far:\n\n1.  Body Doubles\n2.  Metacognitive Assistants\n3.  Tutors\n4.  Partners/Apprentices \n\n[**Body doubles**](https://www.lesswrong.com/posts/rTJrqtDLxAPxiW3sk/my-first-year-in-ai-alignment#Body_doubling) just sit in the room with you, periodically looking at your screen, and maybe saying \"hey, do you endorse being on facebook?\". They're a kind of brute force willpower aid. The person I know who uses them the most (Alex Altair) has them just sit in the same room (I believe while doing pomodoros, each of them working on different things). He guesses that they 2x his productivity (which is around what I've gotten)\n\n**A metacognitive assistant** is a step beyond, where they are dedicating their attention to you, noticing when you are getting stuck, and gently intervening. (I assume people vary in how they like to be intervened on, but for people doing nuanced cognitive work, I think not disrupting someone's thought process is very important. You need to feel safe with a metacognitive assistant). My experience is that this is a 1.5x to 2x multiplier on my output.\n\nThe next two types are both more involved than Metacognitive Assistants, but in different ways.\n\n**Tutors** pay attention to you, but are particularly modeling how you are approaching a particular skill (programming, math, etc). They notice when you seem to be tackling the skill in a confused or inefficient way, and ask questions about your thought process so as to figure out what subskills or concepts you need to develop.\n\n**Partners or apprentices** are full on \"pairing\" – they actively collaborate with you on your task. Hiring a partner/apprentice is a very hard task, it requires tons of chemistry and intellectual compatibility, so it's not really a shortcut to anything, but if you find the right person it seems great. \n\n(John Wentworth says his research partner David Lorell multiplied his productivity by 3x, largely by raising John's floor performance. His earlier estimates were higher, and he says the current 3x takes into account that the trend of value-estimate has been downward. He does flag that the reduction-in-value-estimate included \"dealing with some burnout\" at times when he ended up pushing himself harder than he'd have naturally done if working on his own. He's since iterated on how to deal with that).\n\nThis post is mostly focused on Metacognitive Assistants, because I think they a) require some upfront investment to turn into a functioning niche of the rationalsphere (moreso than body doubles), b) feel achievable to scale up (whereas Tutors/Partners are both pretty advanced roles).\n\nPricing here varies wildly. I believe Alex Altair mostly hires UC Berkeley grad students for ~$15/hr, I've worked with people in more dedicated Metacognitive Assistant roles for $40-$80/hr depending on circumstances. Research assistants and tutors are probably much more bespoke. \n\nExecutive Assistants\n--------------------\n\nI'm contrasting \"Thinking Assistants\" with \"Executive Assistants.\" They do involve many of the same skillsets. I see executive assistants' job as a) handling your general metacognition across all the domains *other* than your core competency, and often handling various other personal-or-professional tasks that free up your time to focus on your core competency. \n\nI think executive assistants are also great, and maybe they should blend with the Thinking Assistant role, since you realistically don't need a Thinking Assistant all the time and do need this other stuff dealt with and they probably collectively are worth one fulltime hire. But it is a different job.\n\nCore Skills of a Metacognitive Assistant\n========================================\n\nI assume people will vary in what works for them. But, what I want out of a Thinking Assistant is:\n\n*   By default, be quietly but visibly attentive.\n*   Every now and then (~5-10 minutes, or when I look actively distracted), briefly check in (where if I'm in-the-zone, this might just be a brief \"Are you focused on what you mean to be?\" from them, and a nod or \"yeah\" from me).\n*   When I need to think something through, they rubber duck (i.e. listen as I talk out loud about it, and ask clarifying questions)\n*   Build a model of my thought process (partly by me explaining it to them, partly by observing, partly by asking questions)\n*   Ideally, notice when my thought process seems confused/disoriented/inefficient.\n*   Ideally, have a large repertoire of cognitive tools they can suggest if I seem to be missing them. (Robin Goins, one of the people I've hired in this capacity, at some point said \"I notice you're not writing things down while you think. How intentional is that?\" and it was one of the more important life-upgrades I got, via [expanding my working memory](https://www.lesswrong.com/posts/thc4RemfLcM5AdJDa/skills-from-a-year-of-purposeful-rationality-practice#Working_Memory_facility)).\n*   Intelligent enough that they can pretty easily understand the gist of what I'm working on.\n*   Ability to pick things up from context so I don't need to explain things in too much detail.\n*   Ideally, when my bottlenecks are emotional, also be at least fairly emotionally attuned (i.e. project a vibe that helps me worth through it, or at least doesn't add extra friction or emotional labor demands from me), and ideally, basically be a competent therapist.\n*   In general, *own the metacognition.* i.e. be taking responsibility for keeping track of things, both on a minute-to-minute timescale, and the day-to-day or week-to-week timescale.\n*   **Ability to get out of the way / quickly drop things if it doesn't turn out to be what I need, without it being a big deal. **\n\nThere are also important outside-the-container skillsets, such as:\n\n*   Be responsive in communication, so that it's easy to schedule with them. If it's too much of a pain to schedule, it kinda defeats the point.\n*   Potentially: proactively check in remotely during periods where I'm not actively hiring them. i.e. be a professional accountability buddy, maybe paid some base rate to briefly check in each day, with the ability to upsell into \"okay today is a day that requires bigger metacognitive guns than Raemon has at the moment\")\n\nEven the minimum bar (i.e. \"attentive body double\") here is a surprisingly skilled position. It requires gentleness/unobtrusiveness, attentiveness, a good vibe. \n\nA thing that feels a bit silly to me is that this isn't something I've been able to make work very well at Lightcone with other Lightcone employees. Sometimes we actively pair on tasks and that works well. But, our hiring process sort of filters for ornery opinionatedness, which is kinda the opposite of what you want here. I think even the simplest version of this is a specialized role. \n\nThe skill ceiling, meanwhile, seems quite high. The most skilled versions of this are the sort of therapist or executive coach who would charge hundreds of dollars an hour. The sort of person who is *really* good at this tends to quickly find their ambitions outgrowing the role (same with good executive assistants, unfortunately).\n\nPitfalls\n========\n\nCommon problems I've run into:\n\n*   **Having trouble scheduling with people.** If you want to specialize in this role, it's often important for people to contact you on a short timeline (i.e. I might notice I'm in a brainfoggy state and want someone to assist me like *right now*, or tomorrow), so, having a communication channel you check regularly so people can ping you about a job.\n*   **Asking questions in a way that is annoying instead of helpful.** Since the point is to be giving me more time, if I have to spend too much time explaining the situation to someone, it undoes the value of it. This requires either them being good at picking things up quickly without much explanation, or good at reading nonverbal cues that the current thread isn't worth it and we should move on.\n*   **Spending too much time on unhelpful advice.** Sometimes an assistant will have ideas that don't work out, and maybe push them more than appropriate. There's a delicate balance here because sometimes I *am* being avoidant or something and need advice outside of my usual wheelhouse, but generally if advice isn't feeling helpful, I think the assistant should back off and observe more and try to have a few other hypotheses about what to suggest if they feel that the assistee is missing something.\n*   **Navigating weird dynamics around \"having someone entirely optimized to help another person.\"** Having this run smoothly, in a net helpful way, means having to actually be prioritizing my needs/goals in a way that would normally be pretty rude. If I constantly feel like there's social awkwardness / wariness about whether I'm making them feel bad, the whole thing is probably net negative. I think doing a good job of navigating this requires some nuance/emotional-skill on both parties, in terms of striking a vibe where it feels like you are productively collaborating.\n    *   (I think this likely works best when the person is really actively interested in the job \"be a thinking assistant\", as opposed to something they're doing because they haven't gotten traction on their real goals).\n\nOptimizing for (not-particularly skilled) Metacognitive Assistance\n==================================================================\n\nI've worked with people who were actively skilled at Thinking Assistance, and one person for whom it wasn't really their main thing, just a job.\n\nOne way I got more mileage out of the not-as-skilled person was to do upfront work of assembling a list of cognitive situations + habits. i.e:\n\n*   when I feel avoidant:\n    *   -\\> take a breath, check in with myself about why I'm feeling avoidant, and then either do some [grieving](https://www.lesswrong.com/tags/grieving), or goal factoring, or just acknowledge it and then power through, depending on circumstances.\n*   when I feel overwhelmed with complexity:\n    *   -\\> figure out better working memory tools for the situation\n\netc. \n\nThen, since I've done the upfront work of thinking through my own metacognitive practices, the assistant only has to track in the moment what situation I'm in, and basically follow a flowchart I might be too tunnel-visioned to handle myself.\n\nAutomated AI Assistants?\n========================\n\nLike many professions, AI is probably going to automate this pretty soon. I think the minimum viable \"attentive body double + rubber duck\" is something AI could implement right now. ChatGPT's voice mode would basically be fine at this if it:\n\n*   ...was better at guessing when to reply to you\n    *   (it currently replies way too quickly in a way that keeps interrupting my thoughts. I handle this sometimes by instructing it \"please generally speak in a soft whisper, and only reply with 'mmm' to everything I say'\", which doesn't stop it from replying but at least makes it less disruptive to do so.)\n*   ...could check in with you at random times, so you don't just forget about it. (With ability to snooze if you don't want it to bother you for awhile)\n*   ...runs automatically when your computer starts up, in a way that manages to be unobtrusive but also not let you fall off the habit of using it.\n*   ...maybe ideally (if slightly sketchily), track all your keystrokes and keep tabs on roughly what you're working on, so it has enough context you don't need to explain everything.\n\nPresumably people are working on this somewhere. I might go ahead and build my own version of it since I expect to eventually want highly customized cyborg tooling for myself, and since AI is dropping the cost of developing apps from scratch. But, I expect the market to figure it out sooner or later.\n\nThis establishes a pretty solid floor in quality. But, since part of the active ingredient here is \"a real human is paying attention to you and will hold you accountable with a bit of their human soul\", I expect there to continue being at least some benefit to having a real human. (I think there will be some minimum bar of attentiveness + unobtrusiveness + able-to-follow that a human will need, to be worth using over an AI, once the AI is basically working) \n\nTrialing People + Matchmaking\n=============================\n\nFor the immediate future, I'd like to trial more people at cognitively assisting me, explicitly with a goal of being able to matchmake them with other people if appropriate. DM me if you're interested.\n\nI also generally recommend other people trying experimenting with this in an ad-hoc way and writing up their experiences.\n\nFocusmate + TaskRabbit?\n=======================\n\nIt'd be nice to have a scalable talent pipeline for this, that matchmakes people with assistants.\n\nBecause of the combination of:\n\n*   \"Competent assistants tend to end up either charging a lot of money or shifting to non-assistant roles\", and\n*   \"Intellectual chemistry is very important, so you want to trial a few people to find the ones that work well with you.\"\n*   \"You may not need an assistant literally all the time\"\n\nI think the natural vehicle here is a matchmaking site that's similar to FocusMate (which pairs people for coworking) but more like you're hiring skilled labor. I can imagine something where people list different skills and rates, and get ratings based on how helpful they've been. \n\nHypothetically this could be a very openended public-facing commercial website. I do personally feel like for a lot of work in the x-risk space it helps a lot to have someone in sync about my strategic frame and would feel more friction working with a more random general population person.\n\nAligning Incentives\n===================\n\nAn obvious idea that might occur to you is \"Provide metacognitive assistance for free, to people you think are doing good work.\" I don't think this is a good idea longterm – I think it's a recipe for people ending up undervalued, as people model the cost as \"free\" rather than \"subsidized.\" It also might turn into some kind of Lost Purposes Appendage where nobody knows how to evaluate *either* the research *or* the thinking-assistance and it gets propped up (or not) depending on how flush-with-funding the EAcosystem is this particular year.\n\nI feel more optimistic about \"the ecosystem overall figures out how much work various people's work is worth via various evaluation / grantmaking processes\", and then people pay for metacognitive assistance if it's actually worth it.\n\n* * *\n\nOverall, this is one of the highest effect sizes I know of for productivity (up there with \"get medication for your depression\", \"get a more motivating job\" and \"get enough sleep\"). It is admittedly not cheap – $800/week at the cheap end if fulltime, and sort of unboundedly expensive at the higher end. (Modulo \"maybe someone can build a good AI for this\").\n\nIf you go this route – remember to keep track of whether you're overworking yourself. My current model is most people can in fact work more hours than they can motivate themselves to while working alone, but John's and my experience is that it's at least possible to overdo it if you're not careful.",
      "plaintextDescription": "Of the posts I've delayed writing for years, I maybe regret this one the most.\n\nI think more people (x-risk focused people in particular) should consider becoming (and hiring) metacognitive assistants. This is the single largest performance boost I know of – hire someone to sit with you and help you think. It doesn't help me (much) when I'm at my peak, but I'm not at my peak most of the time.\n\nThere are four types of assistants I'm tracking so far:\n\n 1. Body Doubles\n 2. Metacognitive Assistants\n 3. Tutors\n 4. Partners/Apprentices \n\nBody doubles just sit in the room with you, periodically looking at your screen, and maybe saying \"hey, do you endorse being on facebook?\". They're a kind of brute force willpower aid. The person I know who uses them the most (Alex Altair) has them just sit in the same room (I believe while doing pomodoros, each of them working on different things). He guesses that they 2x his productivity (which is around what I've gotten)\n\nA metacognitive assistant is a step beyond, where they are dedicating their attention to you, noticing when you are getting stuck, and gently intervening. (I assume people vary in how they like to be intervened on, but for people doing nuanced cognitive work, I think not disrupting someone's thought process is very important. You need to feel safe with a metacognitive assistant). My experience is that this is a 1.5x to 2x multiplier on my output.\n\nThe next two types are both more involved than Metacognitive Assistants, but in different ways.\n\nTutors pay attention to you, but are particularly modeling how you are approaching a particular skill (programming, math, etc). They notice when you seem to be tackling the skill in a confused or inefficient way, and ask questions about your thought process so as to figure out what subskills or concepts you need to develop.\n\nPartners or apprentices are full on \"pairing\" – they actively collaborate with you on your task. Hiring a partner/apprentice is a very hard task, it requires",
      "wordCount": 2527
    },
    "tags": [
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "F9WyMPK4J3JFrxrSA",
    "title": "The \"Think It Faster\" Exercise",
    "slug": "the-think-it-faster-exercise",
    "url": null,
    "baseScore": 147,
    "voteCount": 81,
    "viewCount": null,
    "commentCount": 35,
    "createdAt": null,
    "postedAt": "2024-12-11T19:14:10.427Z",
    "contents": {
      "markdown": "*Note: there is a shorter* [*\"Think it Faster\" worksheet*](https://www.lesswrong.com/posts/skEhdgiK6T5HzMhis/think-it-faster-worksheet). *I'm curious which one people find easier as an initial read.*\n\nUltimately, I don’t want to solve complex problems via laborious, complex thinking, if I can help it. Ideally, I'd want to basically intuitively follow the right path to the answer quickly, with barely any effort at all.\n\nFor a few months I've been experimenting with the [\"How Could I have Thought That Thought Faster?\"](https://www.lesswrong.com/posts/rYq6joCrZ8m62m7ej/how-could-i-have-thought-that-faster) concept, originally described in a twitter thread by Eliezer:\n\n> Sarah Constantin: I really liked this example of an introspective process, in this case about the \"life problem\" of scheduling dates and later canceling them: [malcolmocean.com/2021/08/int…](https://malcolmocean.com/2021/08/internal-trust-dancing-date-scheduling-cancelling/)\n> \n> Eliezer Yudkowsky: See, if I'd noticed myself doing anything remotely like that, I'd go back, figure out which steps of thought were actually performing intrinsically necessary cognitive work, and then retrain myself to perform only those steps over the course of 30 seconds.\n> \n> SC: if you have done anything REMOTELY like training yourself to do it in 30 seconds, then you are radically smarter/more able/etc than me and all the other people who do slower introspective practices.\n> \n> SC: I don't know whether to be impressed or to roll to disbelieve.\n> \n> EY: I mean I suspect that this actually requires something like a fast perceptual view of minds as engines and thoughts as doing work and like actually draws on my mind design knowledge, but, even so, I ask: Do you constantly look back and ask \"How could I have thought that faster?\"\n> \n> SC: No, I've never asked that.\n> \n> EY: Okay, well, every time I'm surprised by reality I look back and think \"What about my model and my way of thinking could I change that would have predicted that better, without predicting a bunch of other things worse?\"\n\nI've been working to operationalize this as an exercise[^c4rnh8qin9] you can train repeatedly, rather than hoping to remember to do when reality hands you a surprise. You can do the exercise after any difficult cognitive task (either a toy puzzle exercise, or a day-job project that took a long while). \n\nWhat would have been necessary for you to just look at the situation, and automagically find the right solution? (without overfitting, or generalizing in a way that would cause you to think unproductive thoughts in other sorts of situations?)\n\nThe goal of this exercise is to identify:\n\n*   skills you can train\n*   principles you can apply\n*   actions you can take (either physical or mental)\n\n…that move you to correct solutions to problems as quickly as possible. \n\nThis overall builds into two deep skills: \n\nAsking yourself retrospectively **“How could I have Thought that Faster?” **\n\nAnd then, prospectively, learning to ask “What am I about to predictably spend too long thinking about, and **How Can I Think it Faster, the First Time?”**\n\nI don’t know the upper limits of these skills, but I am currently finding it fruitful to adopt the mindset of “if it took longer than 15 minutes and an LLM query, you probably took too long.” \n\nRelentlessly ask yourself how hours or days could have turned into 15 minutes. Sometimes 15 minutes was literally possible. Most of the time, I find the act of aiming for 15 minutes to be illuminating, and reveal at least some important wasted time or new principles.\n\nExample: 10x UI designers\n=========================\n\nYou’ve probably heard stories about “10x programmers”, who just intuitively steer towards good decisions and get things done dramatically faster than most developers. But I spend most of my time doing UI design, and have lately been thinking about \"10x UI designers.\"\n\nI remember, 17 years ago, at my first “real” job at a printing company. A client wanted us to design a brochure for them. I was “good at art”, but this was my first assignment professionally making art for someone else. I thought about what they needed, I labored carefully for 4 hours, generating ideas and fiddling around in my graphic software.\n\nEventually my senior partner came to look at it. He said “eh, this has a lot of problems. Here, you should do it this way.” I can’t remember if he told me what to do, or just went and did it himself. But, he bypassed all my tedious work by just intuitively knowing how to solve this-particular-class of problem already, and moving directly to the good ideas instead of working through bad ones.\n\nOkay, so, skill is a thing. He had 10 years of experience, I didn’t. \n\nMore recently, I was working for three days on the design for the new Glossary Editor on posts. I was meanderingly exploring a few options, including a separate “table-of-contents” section, and a “show the jargon terms and definitions in detail” section. It was very complex and took up a lot of space and was overwhelming to look at. \n\nMy goal was to make it so authors could quickly skim the AI generated jargon, and make decisions about approving ones they liked, without much effort. \n\nThere were tradeoffs between:\n\n1.  Making it easy to skim\n2.  Making it easy to actually make final decisions, which required knowing enough about what the AI generated definitions actually said.\n3.  Fitting it into a small amount of space, so it didn’t disrupt the experience of people who didn’t care about the glossary at all.\n4.  Making it simple and elegant to think about.\n5.  Accurately conveying all the tools and affordances.\n\nI labored for 3 days, shuffling around where-the-tradeoffs lived, incrementally reducing some of the issues. \n\nThen Oliver Habryka came into the room, took a look, and said “man, your information hierarchy here is all over the place.” Then he fiddled around for 20 minutes and found something dramatically better than what I had at the time. \n\nSince I had recently been asking myself “how could I have Thought That Faster?”, I took this as an opportunity to ask “what the hell just happened, and how could I have done it myself without Oliver’s help?”. \n\nMulling it over, I observed that although part of the answer was “UI specific design taste honed by years of experience”, there were some specific questions he was asking that pointed his attention in much more productive directions (i.e. “how can we give this a clear, unified, simple information hierarchy?”)\n\nThe underlying, general skill seemed to be:\n\n*   Relentlessly be dissatisfied with having to make tradeoffs.[^n1cnh8c8n6r]\n*   Actually identify all the necessary constraints, and accept them, rather than myopically shuffling the tradeoffs around.\n*   Figure out a solution that just solves all the constraints and tradeoffs.\n\nIn each domain (UI, programming, x-risk), there are specific tactical tools for actually “dealing with all constraints.” In this case, domain-specific principles included: *“many buttons weren’t necessary to show users initially, until they started interacting with it”,* and *“instead of having a table-of-contents and a detail-view, build a single view that's an 80/20 of both views at once.”*\n\nBut it seems like there is a general skill of:\n\n*   Notice when you don’t yet know all the constraints.\n*   Steering towards “figure out the constraints”, which includes noticing ones that feel impossible.\n*   Adopting a mindset of “I’m not done until I’ve solved all the constraints.”\n*   If you don’t know how to solve all of them at once, and instead are solving a subset… sometimes that’s correct (“relaxing the problem” is a time-honored cognitive trick). But, frame this as “I am temporarily relaxing the problem. *My goal is still to ultimately deal with all constraints at once.”*\n*   If possible, steer directly towards solutions that have a shot at solving all the constraints.\n\nI've since applied the \"Actually Deal With Constraints\" principle to other \"Things I coulda thought faster\" (such as how my [Feedbackloop-first Rationality](http://lesswrong.com/posts/pZrvkZzL2JnbRgEBC/feedbackloop-first-rationality) agenda has evolved over the past year, and how I could have made the same progress in like a month), and found it pretty valuable.\n\nOkay, but how do you \"Think it Faster?\"\n---------------------------------------\n\nTo fit this back into the \"Think it Faster\" context, there are two next steps:\n\n*   Look at your recent experience, and ask \"what would I have needed to get this right the first time?\" Try to find as many cognitive routes toward the solution as possible.\n*   Think about future experiences where you'll probably Think Unnecessarily Slowly, and think about how to solidify your takeaways so you remember to apply them when relevant.\n\nIn this context, how could I have figured out this principle for myself, before waiting for Oliver to demonstrate it could be done much more quickly?\n\nHere are several options:\n\n*   First, I could have just gone and talked to Oliver earlier.\n*   I was already interested in \"Thinking it Faster\", and already thinking about applying rationality practice in daily life. I might have thought \"hey, this is taking awhile, maybe I should just apply Think It Faster *now?\".*\n*   I was already interested in [metastrategic brainstorming](https://www.lesswrong.com/posts/cbWoMepny3Jo9XqEr/metastrategic-brainstorming-a-core-building-block-skill), and I've previously told people it's often worth doing it even if you're only going to be working for a few hours. I don't think I'd done it much at all in this context. That *might* have generated the \"identify the constraints\" solution.\n*   Alternately, I could have just directly thought about the underlying UI principles. I'd been having thoughts like \"it's kinda too annoying if I don't have a table of contents to skim\", and \"it's less overwhelming without the table of contents.\" I'd previously thought about finding clever Third Options that sidestep the supposed constraints, and I might have somehow had that occur to me.\n*   I could have taken a break, and then come back and try to \"[original see](https://www.lesswrong.com/posts/SA79JMXKWke32A3hG/original-seeing)\" on the problem. Or, basically start over without the preconceptions I had accumulated. (If I imagined myself being a user who wasn't familiar with the glossary, and scrolling to the bottom of the post page after writing a complex post, I might have realized it was too complex)\n*   The actual UI solution was basically \"make a list of one-line items\", not that different from a Post list. It's a pretty basic UI concept. In some sense, it's \"the simplest solution.\" I could have just steered towards simplicity from the get-go.\n\nBut this is incomplete – I need to followup and ask \"but *why* didn't I think any of those particular thoughts the first time? What was the shortest-possible nudge from the actual past, to an alternate past where I figured it out?\"\n\nI do think I should have tried any of the general-rationality-principles sooner (Think It Faster, Metastrategy, Third Options). It's worth diagnosing But those aren't the shortest path – those are explicitly a path with an extra step of \"think meta thoughts until I find the right object thought.\" They also require asking not only \"how could I have had that Meta Thought faster\", but also \"how could I have gotten from that meta-thought to the right answer, without having seen Oliver's solution?\".\n\nI think the shortest path was a combo of \"try the simplest things first.\" \n\nI think the most achievable inflection points were the moments I felt \"ugh, this is a little hard to parse\", and noticing \"a little hard to parse\" might be a bigger deal than I thought it was.\n\nIdeally, I would have thought \"let's just start with the simplest thing first\" at the very beginning. (I started with \"the most obvious thing\" which is somewhat different from the \"simplest thing\"). I've since got a lot of experience seeing \"do the fucking simplest thing\" come a lot in Think It Faster exercises, so now it's in the bundle of general practices I should do way more often but I still don't do enough.\n\nTHE EXERCISE\n============\n\nOkay. So, you've just done either a Toy Exercise puzzle, or you just noticed in your Real World that you either spent a long time figuring something out, or were surprised by something (which you didn't figure out at all)\n\nI recommend [Thinking Physics](https://www.lesswrong.com/posts/PiPH4gkcMuvLALymK/exercise-solve-thinking-physics) and [Baba is You](https://www.lesswrong.com/posts/jqb3prwGQjLriq7Lu/exercise-planmaking-surprise-anticipation-and-baba-is-you) as sources of puzzles to start grinding on this. For your day job, I recommend learning to [notice](https://www.lesswrong.com/posts/XHnAakZtcEktEGvYJ/scaffolding-for-noticing-metacognition) when you have the sneaking suspicion something took longer than it needed to. (Once you've got a bit of practice, I recommend applying this even to places you *don't* have that sneaking suspicion, but it did practically take a long time)\n\n(Baba is You is particular good because it tends to *surprise* you, and you can practice micro-versions of this exercise on individual surprises within a single Baba is You level, which helps train the general reflex of *Notice Surprise -> Ask how you could have Thought It Faster* in the moment)\n\nAt a high level, you're going to ask:\n\n\"How could you have **Thought it Faster**?\"\n\n*   List the steps you actually took to solve the problem\n*   List the minimum steps a magical superintelligence could possibly take\n*   Add steps to the magical-shortlist until it doesn’t feel magical\n*   Identify obvious wasted motion in your original steps\n*   Identify skills or principles that would have helped you solve it quickly, without mistakes?\n*   List each moment where you *could* have steered more towards some kind of more productive thought, but didn't. (i.e. clues you almost noticed, ugh fields you considered leaning into but didn't, etc)\n\n\"What did you learn, which’ll let you **Think It Faster The First Time**, later?\n\n*   List past moments you could have benefited from those skills or principles\n*   List future moments might benefit from those skills or principles\n*   In the next week, what is…\n    *   …something you need to do that feels confusing?\n    *   …a cognitive task you expect to take a lot of time?\n*   Pick a specific problem you expect to face, and ask:  \n    \"What life-lessons can I generalize from this puzzle, to help me approach that problem in a way that is less confused, less long, so I can Think It Faster the First Time?\n\nRather than go through each step exhaustively, in sequence, I recommend cycling through them: jot down a few quick ideas for each prompt, circling back to the first one, with each pass giving you a sense of how all the pieces fit together.\n\nPart I: Thinking it Faster\n==========================\n\nSteps you actually took\n-----------------------\n\nIn chronological order (as best you remember) what happened?\n\nMagical superintelligence steps\n-------------------------------\n\nIf you were a waaaay smarter version yourself, or if you imagine some other waaaay (unrealistically) smarter genius, what is the shortest number of steps you can possibly imagine this taking? \n\n(Right now, it's okay for this to feel like cheating)\n\nIterate on those lists\n----------------------\n\nIdentify steps in the first list you could straightforwardly remove, or simplify. And, identify steps to add to the second list until it no longer feels like unrealistic cheating. (i.e. if you're not overfitting. The plan doesn't imply you should spend tons of cognitive overhead all the time on minor, unimportant clues)\n\nTry these prompts to help you:\n\n**What skills, if you’d trained for 20 or 100 hours, would have helped you find the answer intuitively?**\n\n**What principles, if you internalized and they came easily to mind, would have allowed you to make some of those leaps ~instantly, or at least much faster?**\n\n**What jumps-between-steps feel magical or unrealistic, in “magical short list”? **\n\n**For the “original steps you took”, what steps could you have skipped? What would have been necessary to skip them?**\n\n**Overall, what takeaways do you want to remember for later?**\n\n**What's the broadest generalize that feels reasonable to draw?**\n\nGeneralizing, and not Overgeneralizing\n--------------------------------------\n\nSo far, I have mostly seen people fail to generalize enough, rather than too much. It's certainly possible to fail in both directions, but I maybe suggest erring on the side of overgeneralizing, and wait until it actually hurts you to dial it back.\n\nIn the UI example above, here are a few takeaways I could have had:\n\n1.  \"In UI design, make sure not to be overwhelming, make sure to have a clear information hierarchy, try removing or simplifying or combining bits until you have an elegant but comprehensive tool.\"\n2.  \"In UI design, try to identify all the constraints for the final successful design, and follow a plan that can solve all of them.\"\n3.  \"In other design contexts, such as [ritual design](https://www.lesswrong.com/posts/GEQyCqgu5Yhi5dTdb/designing-ritual) or [event center construction](https://www.lesswrong.com/posts/memqyjNCpeDrveayx/the-lighthaven-campus-is-open-for-bookings), make sure not to be overwhelming, present relevant information clearly, simplify or combine bits until you have an elegant solution.\"\n4.  \"In any difficult problem, identify all the necessary constraints, and follow a plan that can solve all of them.\"\n\nHere's a short handle for each of those:\n\n*   Narrow tactical advice for similar (UI) situations\n*   The broader generalization of that advice, for similar (UI) situations\n*   Applying the narrow-tactical-advice to other situations (ritual/etc)\n*   The broader generalization, applied to ~all domains.\n\n#1, #3 and #4 are each actually pretty useful to think about. I'll be doing a lot of UI design. The specific UI tactics will come up again, and I don't wanna have to rederive them from first principles every time. \n\nMany design principles transfer between domains. I design lots of kinds of things. It's useful to remember the design-specific tactics whenever they come up.\n\nBut \"identify and deal with all the constraints\" is an incredibly general tool. I should apply that all over the place, whenever I'm dealing with something pretty hard that I expect to spend at least several hours on.\n\nOne example of *overgeneralization* would be, in situations where I basically already know how to solve the problem (i.e. a similar UI design problem), if I were to go Full Meta and try to original see on the constraints instead of just executing on the tactics I already know how to do. \n\nSkills into Principles\n----------------------\n\nMany people do the first steps, and then are like \"but, it would have been impossible to have done better.\" I think this is almost always false. But, I haven't found it that useful to argue with that directly, and instead focus on \"what are the skills that you hypothetically could have already trained on, which would have helped?\"\n\nThis then prompts the followup question \"okay, are those skills going to come up a lot in your life?\" If so, maybe focus on training those skills more deliberately.\n\nBut, training skills is pretty slow and expensive. Developing subtle taste and reflections takes time. A thing I've found helpful is to try to translate skills into \"principles\" – straightforward instructions you can remind yourself, that help steer your mind towards the right sorts of cognition.\n\nIn the UI case, the \"skill\" is in noticing that something felt overwhelming about the original design, and visualizing what it'd be like to encounter the UI for the first time. There's a bunch of taste and imagination skill involved. It's worth cultivating that skill, but the \"what are the constraints?\" and \"how can I create a clear, simple information hierarchy?\" are questions that help guide thinking more directly.\n\nPart II: Thinking It Faster The First Time\n==========================================\n\nThat was the easy part. \n\nThe hard part is noticing when you're *about* to think something Too Slowly, and... do something else instead.\n\nI don't yet have that crisp an exercise for this. If you did the Think It Faster exercise for a toy puzzle, the lessons might not generalize to whatever you're likely to do next week. But after you've done it several times, you'll start to notice patterns. It's easiest to notice the patterns when you start applying the exercise to your day job, since you probably do similar things in your day job a lot.\n\n(One example: I was working on an LLM-prompting scaffold. I assumed I would need elaborate setup of examples to initially prompt it with, and spend days working on it and iterating on it. Eventually... it turned out the simplest, dumbest prompt with only one example did better than my elaborate setup. The generalization: \"just try doing the dumb simple thing first.\" The very next day, we were working on some other problem where we tried an elaborate complicated thing and then realized eventually the simple dumb thing would work, and I kicked myself for not having remembered the lesson I'd explicitly noted from the day before)\n\nTo train this quickly, it's important to find a way to apply generalizations towards something *soon*, so you get to reinforce it before it fades from top-of-mind.\n\nHere's my current prompts for thinking about this:\n\nGeneralizing from this exercise \n--------------------------------\n\n**First, consolidate your list of skills and principles**\n\n**List past situations you could have benefited from those skills or principles**\n\n**List future situations where you suspect might benefit from those skills or principles.**\n\n**In the next week, what’s 1-3 tasks you’re doing that might benefit from those skills or principles?**\n\nAnticipating Future Life Lessons\n--------------------------------\n\nThe flipside of \"how can this exercise generalize to real life?\" is \"what real life situations are likely to benefit from some kind of exercise?\".\n\nSo an alternate set of prompts are:\n\n**In the next couple days, what's something you're planning to do that you expect to take a long time?**\n\n**...what's something you're** ***confused*** **about, where you're not sure how to do it?**\n\n**...what's something you expect to solve via tinkering/iteration without much of a plan, that you expect to take awhile?**\n\nThese might be situations that don't naturally lend themselves to the most obvious life lessons from the exercise you just did. But, they might give you clues about additional life-lessons to be on the lookout for. Or, might give you clues about which sorts of toy exercises are useful to apply this practice to.\n\nGetting Detailed, and TAPs\n--------------------------\n\nAfter you've soaked in some basic ideas for takeways, and some practical places to apply them, you want to get a lot more detailed. Form explicit intentions about when to remind yourself of some advice, and see if it's helpful.\n\n**For one of the past moments, think in detail about how principles/skills would apply.**\n\n(Imagine doing this whole doc again, for that past moment, and how you wish you’d thought-it-faster then. Don’t do the whole-ass version of the doc, just briefly think about the key moments)\n\n**For the future moments, how would the skills or principles apply? What would you hope you do, in the moment, to avoid taking longer or making mistakes?** (When you imagine *failing* to remember in the moment, why was that? What steps could you take to avoid forgetting?)\n\n**Write down 3 tactical plans for remembering and applying lessons from this exercise during the next week. ***(They can be bad plans, and they can be short/rough. Ideally, they should include some actions you take right now, and some actions you’ll take later)*\n\n**Pick any of the plans that seem worthwhile. Make an explicit prediction about whether it’ll work.** (*If it doesn’t feel that likely to work, ask “how can I improve this plan?” until you’d be surprised if it failed.)*\n\n**Take whatever actions you can take right now.**\n\nPart III: The Five Minute Version\n=================================\n\nDoing all of this thoroughly takes a long time. I recommend doing it thoroughly the first couple times, to build a complete model of how everything fits together.\n\nBut, ultimately, to practice a skill, you need to get a lot of reps in. You can't get a lot of reps in if you have to dedicate an hour each time.\n\nSo, what's the *five minute* version of this? When you look at everything you just thought about, what were the single most important thoughts you had? What prompts would have helped direct you to those important thoughts?\n\nI recommend thinking about this *quickly* rather than slowly/deliberately, to help practice the art of \"just actually think the most important thoughts, don't overthink it\", which is it's own skill.\n\nThe next time you naturally stumble into a situation you could have Thought Faster, apply the 5 minute version of this exercise.\n\nYou can probably find at least 1-3 moments per week that would benefit from Thinking It Faster. \n\n[^c4rnh8qin9]: I don't know that Eliezer would endorse this particular exercise. I asked him once for more detail on how he applied the skill, and it seemed like he'd been doing it so long that most of it was a compressed atomic action he couldn't easily unpack. \n\n[^n1cnh8c8n6r]: There are corresponding emotional skills of being dissatisfied without being too worked up about it, or manic.",
      "plaintextDescription": "Note: there is a shorter \"Think it Faster\" worksheet. I'm curious which one people find easier as an initial read.\n\n \n\nUltimately, I don’t want to solve complex problems via laborious, complex thinking, if I can help it. Ideally, I'd want to basically intuitively follow the right path to the answer quickly, with barely any effort at all.\n\nFor a few months I've been experimenting with the \"How Could I have Thought That Thought Faster?\" concept, originally described in a twitter thread by Eliezer:\n\n> Sarah Constantin: I really liked this example of an introspective process, in this case about the \"life problem\" of scheduling dates and later canceling them: malcolmocean.com/2021/08/int…\n> \n> Eliezer Yudkowsky: See, if I'd noticed myself doing anything remotely like that, I'd go back, figure out which steps of thought were actually performing intrinsically necessary cognitive work, and then retrain myself to perform only those steps over the course of 30 seconds.\n> \n> SC: if you have done anything REMOTELY like training yourself to do it in 30 seconds, then you are radically smarter/more able/etc than me and all the other people who do slower introspective practices.\n> \n> SC: I don't know whether to be impressed or to roll to disbelieve.\n> \n> EY: I mean I suspect that this actually requires something like a fast perceptual view of minds as engines and thoughts as doing work and like actually draws on my mind design knowledge, but, even so, I ask: Do you constantly look back and ask \"How could I have thought that faster?\"\n> \n> SC: No, I've never asked that.\n> \n> EY: Okay, well, every time I'm surprised by reality I look back and think \"What about my model and my way of thinking could I change that would have predicted that better, without predicting a bunch of other things worse?\"\n\nI've been working to operationalize this as an exercise[1] you can train repeatedly, rather than hoping to remember to do when reality hands you a surprise. You can do the exercise after any d",
      "wordCount": 4033
    },
    "tags": [
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5yFj7C6NNc8GPdfNo",
    "title": "Subskills of \"Listening to Wisdom\"",
    "slug": "subskills-of-listening-to-wisdom",
    "url": null,
    "baseScore": 155,
    "voteCount": 59,
    "viewCount": null,
    "commentCount": 29,
    "createdAt": null,
    "postedAt": "2024-12-09T03:01:18.706Z",
    "contents": {
      "markdown": "*A fool learns from their own mistakes*  \n*The wise learn from the mistakes of others.*\n\n*– Otto von Bismarck *\n\nA problem as old as time: The youth won't listen to your hard-earned wisdom. \n\nThis post is about learning to listen to, and communicate wisdom. It is very long – I considered breaking it up into a sequence, but, each piece felt necessary. I recommend reading slowly and taking breaks.\n\nTo begin, here are three illustrative vignettes:\n\n### **The burnt out grad student**\n\nYou warn the young grad student \"pace yourself, or you'll burn out.\" The grad student hears \"pace yourself, or you'll be kinda tired and unproductive for like a week.\" They're excited about their work, and/or have internalized authority figures yelling at them if they aren't giving their all.   \n  \nThey don't pace themselves. They burn out.\n\n### **The oblivious founder**\n\nThe young startup/nonprofit founder says \"We're going to solve problem X!\". You say \"oh, uh you know X is real difficult? Like, a lot of talented people tried to solve X and it turned out to be messy and intractable. Solving X is important but I think you need to come at it from a pretty different angle, or have a specific story for why your thing is going to work.\"\n\nThey hear \"A bunch of people just didn't really try that hard.\" If you follow it up with \"Look man I really *want* you to succeed here but I think there are some specific reasons Y that X is hard. And I'd love it if you stood on the shoulders of giants instead of adding another corpse to the pile of people who didn't even make it past the first ring of challenges.\" \n\nThey hear \"okay, we need to spend some time thinking specifically about Y, but our plan is still basically right and we can mostly plow ahead.\" (Also they think your explanation of Y wasn't very good and probably you're actually just an idiot who doesn't really understand Xs or Ys).\n\nA year later they write a blog post saying \"We tried to fix X, alas it was hard\", which does not contribute particularly interesting new knowledge to the pursuit of solving X.\n\n### **The Thinking Physics student**\n\nYou've given a group of workshop participants some [Thinking Physics](https://www.lesswrong.com/posts/PiPH4gkcMuvLALymK/exercise-solve-thinking-physics)  problems. \n\nAs ~always, some people are like \"I can't do this because I don't have enough physics knowledge.\" You explain that the idea is that they figure out physics from first principles and their existing experience. They look at you skeptically but shrug and start dutifully doing the assignment.\n\nAn hour later, they think they probably have the answer. They look at the answer. They are wrong. You ask them to ask themselves [\"how could I have thought that faster?\"](https://www.lesswrong.com/posts/F9WyMPK4J3JFrxrSA/the-think-it-faster-exercise), and they say \"Well, it was impossible. Like I said, I didn't have enough physics knowledge.\" \n\nYou talk about how the idea is to approach this like the original physicists, who didn't have physics knowledge and needed to figure it out anyway. And, while some of these problems are selected for being counterintuitive, they're also selected for \"smart non-physicists can often figure them out in a few hours.\"\n\nThey hear: \"The instructor wanted me to do something that would burn out my brain and also not work.\" (In their defense, their instructor probably didn't actually explain the mental motions very well)\n\nThey end up spending *quite* a lot of effort and attention on loudly reiterating why it was impossible, and ~0 effort on figuring how they could have solved it anyway.\n\nYou wish desperately to convey to them an attitude that \"[impossible problems can be solved](https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible).\" \n\nOkay, like, not *actually* impossible problems, obviously those are actually impossible. But, in the moment, when the student is frustrated and the problem is opaque and their working memory is full and they don't have room to distinguish \"this *feels* impossible\" from \"this *is* impossible\", the thing they actually need to know in their bones is that impossibility *can* be defeated, sometimes, if you reach beyond your excuses and [frames](https://www.lesswrong.com/posts/f886riNJcArmpFahm/noticing-frame-differences) and actually think creatively.\n\n* * *\n\nFor the past four years (in particular, since overhearing one-too-many oblivious startup founders and seeing their eventual postmortem blogpost), I've been haunted by the question: \n\n*\"Man, how do I not be that guy?\"*\n\nAnd, more ambitiously: \n\n*\"Man, it would be great if somehow humanity unlocked the power to transfer wisdom.\"*\n\nThere's been a lot of discussion of how humanity is bottlenecked on transfer of [*tacit knowledge*](https://commoncog.com/the-tacit-knowledge-series/)*.* Deep expertise has lots of fiddly bits that are hard to compress. Often the only way to transfer it is by pairing individually with a mentor, and watching what they do until you internalize those fiddly bits. \n\nSometimes, knowledge is \"tacit\" because it can be learned visually, but not through writing. There's been a revolution in how people can share this sort of knowledge now [via youtube](https://www.lesswrong.com/posts/SXJGSPeQWbACveJhs/the-best-tacit-knowledge-videos-on-every-subject). The military has also explored how you can [train tacit knowledge via *simulation exercises*](https://commoncog.com/accelerated-expertise/), that force people to quickly discover the pieces for themselves even though it's hard to operationalize them as a series of deliberate practice steps.[^91qu016emb]\n\nSome of the things called \"wisdom\" seem to me basically like run-of-the-mill tacit knowledge. But some of the things called \"wisdom\" has a particular flavor, which I might call **\"tacit soulful knowledge.\"** It is knowledge whose acquisition requires you to either:\n\n1.  Update some beliefs that are *psychologically load-bearing*, and/or connected to your [meaning making](https://www.lesswrong.com/posts/uoArM8ftbJzmHoEfe/would-robots-care-about-meaning-and-relating)-network.\n2.  Or, update some particularly *deep* beliefs *a lot*. Either the belief node is tightly intertwined with lots of other beliefs, or maybe it's a single belief but it's just... like, a big one. The magnitude of the update you'll need to make about it is extreme, and you'll keep being tempted to think \"okay I got it\" when you don't yet got it.\n\nThe first scenario is very challenging to update on because it's scary, and you won't necessarily have coping mechanisms to handle the update safely.\n\nThe second option is very challenging because it is exhausting and effortful and people don't feel like it. *Also,* major beliefs like this are often psychologically loadbearing, so have the first problem too.\n\nThe other day, I *think* I got deconfused about all the moving pieces here. \n\nI think there are ~15 skills you need (in some combination between the tacit-soulful-knowledge Giver and Receiver). They are all difficult skills. Most of the skills individually require a different bit of tacit knowledge or wisdom, as a pre-requisite.\n\nBut they're all useful skills you probably eventually want anyway. And, if you have them all, I think you can gain a skill of \"Listening to Wisdom.\" And maybe, you can become someone who more easily learns from the mistakes of others, not merely in a shallow way, but a pretty deep one.\n\nI'm excited about this because it feels on-the-path towards somehow building an engine that transfers wisdom better, at scale. \n\nI'm also excited because, while I think I have most of the individual subskills, I haven't personally been nearly as good at listening to wisdom as I'd like, and feel traction on trying harder.\n\n* * *\n\n### ***Epistemic Status***\n\nI came up with this a couple months ago. It's not very battle tested, but I have found it directionally useful. Part II explores the initial test case. I'll say up-front: I don't think I've done anything like a complete Tacit Soulful Knowledge transfer, but I do think I accomplished a few partial transfers that feel novel and valuable to me. \n\nIf nothing else, thinking in this frame has made it clear how high the \"skill ceiling\" on *actually listening* is. And it's motivated me to seriously, for real, attempt to listen to what people are trying to say, moreso than I ever had before. I think I've already gotten significant value from that, even if the specific models here turn out to be wrongly framed or overhyped.\n\nBut, having spent a fair amount of time on calibration training on particular flavors of new insights, I do feel fairly confident (>65%) that in a year I'll still think this model was [obviously](https://www.lesswrong.com/posts/wDpXshpakpYDcTtug/fluent-cruxy-predictions-1#Frame_3__Murphijitsu__and_being_dissatisfied_with__maybe__) useful in *some* way, even if not necessarily the way I currently imagine. [^7ixwgj77djn]\n\n<table><tbody><tr><td style=\"background-color:hsl( 0, 84%, 88% );border-color:hsl(0, 0%, 100%)\"><h1>Be careful&nbsp;</h1><p>As much as wisdom is great, I am worried about ideas here being misused.&nbsp;</p><p>Cults work partly via rapid tacit-soul-knowledge transfer, in a way that overwhelms and undermines a new recruit's defenses<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"z2jopihrx7d\" role=\"doc-noteref\" id=\"fnrefz2jopihrx7d\"><sup><a href=\"#fnz2jopihrx7d\">[3]</a></sup></span>.&nbsp;</p><p>I think even well intentioned people could hurt themselves, even if they're fairly knowledgeable. The skills required here are nuanced. They are skills many people are particularly likely to have blind spots about being insufficiently good at.&nbsp;</p><p>Not all Tacit Soulful Knowledge is \"Wisdom.\" (See <a href=\"https://www.lesswrong.com/posts/5yFj7C6NNc8GPdfNo/subskills-of-listening-to-wisdom#Tacit_Soulful_Trauma\">Tacit Soulful Trauma</a>)</p><p>The ideas in this post are downstream of <a href=\"https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies\">Tuning your Cognitive Strategies</a>, an essay by SquirrelInHell. SquirrelInHell, notably, eventually killed themselves. When people find that out, they often express a lot of worry that Tuning Your Cognitive Algorithms is dangerous. I am on record (and stand by) believing that Tuning Your Cognitive Strategies is not dangerous in a way that was causal in that suicide<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"4\" data-footnote-id=\"dgok0px6xku\" role=\"doc-noteref\" id=\"fnrefdgok0px6xku\"><sup><a href=\"#fndgok0px6xku\">[4]</a></sup></span>, <i>except</i> that it's a kind of a gateway drug into weird metacognitive practices and then you might find yourself doing weirder shit that either explicitly hurts you or subtly warps you in a way you don't notice or appreciate.</p><p>I think the way SquirrelInHell died was essentially (or, at least, analogous to) absorbing some Tacit Soulful Ideas, which collapsed a psychologically load-bearing belief in a fatal way.<span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"5\" data-footnote-id=\"id3v6o48fyh\" role=\"doc-noteref\" id=\"fnrefid3v6o48fyh\"><sup><a href=\"#fnid3v6o48fyh\">[5]</a></sup></span></p><p>I might be wrong about Tuning Your Cognitive Algorithms being safe, but the point here is that \"Listening to Tacit Soulful Wisdom\" seems more risky.</p><p>I am publishing this anyway because I don't think it's <i>intrinsically</i> dangerous. And in the 2010s I saw several orgs avoid sharing their cognitive techniques widely for fear of them being dangerous in some way, and I think this didn't really help with anything and maybe caused additional harm because secrecy subtly fucked things up.</p><p>Meanwhile, I think \"figure out how to listen to and share wisdom\" is a very important topic to make progress on, and sharing info about it seems more likely to eventually capture the upsides as well as avoid downsides.</p><p><i>(see </i><a href=\"https://www.lesswrong.com/posts/Mi5kSs2Fyx7KPdqw8/don-t-ignore-bad-vibes-you-get-from-people\"><i>Don't ignore bad vibes you get from people</i></a><i> for some more thoughts here)</i></p></td></tr></tbody></table>\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c4595b8ca3aef20675c5555aa9cd60dc89d284a15075f6b6.png)\n\n**PART I**\n==========\n\nAn Overview of Skills\n=====================\n\nThere are ~15 skills that seem important (each with their own subskills), but you can roughly group them into four clusters:\n\n*   **\"Accurate, impactful storytelling\"**\n*   **\"Accurate, impactful story*****listening\"***\n*   **\"Good judgment about what's healthy for you\"**\n*   **\"Good judgment about what's healthy for someone else\"**\n\nIn both cases, the Giver and Receiver need to share an evocative experience that conveys something important. And they need deep clarity of the subtle moving parts, such that you are sharing a *true* story rather than a fake one. And they need to have good judgment about this being helpful rather than harmful.\n\nEssentially, this whole concept is a reframing of the classic concepts: *\"Humans communicate important truths via storytelling\"*, or *\"If we don't learn from history we are destined to repeat it.\"* [^gi5v326nco9] This post's contribution is to break those down into specific subskills, and in particular introducing \"story-listening\" as a skill you can explicitly optimize.\n\nSo more specifically, the Tacit-Soulful-Knowledge Giver needs:\n\n1.  **Excellent introspection**, so they understand exactly what the moving parts of the wisdom are, and how you would know if you had it, and what subtle ways you might *think* you have it, but don't.\n2.  **Excellent epistemics, and \"mechanistic** ~**interpretability**~ **metacognition.\"** The Giver needs to identify particular moments where the soulful-knowledge really mattered, and how they'd have made different cognitive motions depending on whether they had the wisdom, vs a pale shadow of it.\n3.  **Modeling of others.** The Giver needs to check in with whether the Receiver is tracking subtle distinctions, and what sort of mental-moves will actually work for the Receiver (which might be different for what worked for the Giver).\n4.  **Pedagogy.** Giver must communicate nuanced models clearly.\n5.  **Communicating importance on an emotional level**, such that  that the listener will really hear it (by this I mean something like \"staring into their soul and conveying by tone of voice and eye contact that this thing is important, and is worth listening to\", at a vibes-level).\n6.  **\"Storytelling\", or, \"Conveying the qualia of an experience, and what it means\".** The Giver can communicate a visceral narrative  that conveys an experience empathetically. (By this I mean more like stating clearly: \"here is a story of what happened to me, and how it affected me, and why the weight of it is more significant than you might be tracking if you just hear the surface level words.\")\n7.  **(conditionally optional) Creating a safe emotional space.** The Giver is tracking the ways the Receiver might end up hurting themselves. (This can be compensated for by the Receiver having the \"can handle delicate inner work safely on their own\" trait. But I recommend both of them having it, so you have a sanity check on whether something bad is happening)\n\nThe Tacit-Soulful-Knowledge Receiver needs:\n\n1.  **To care.** They need to earnestly believe an important belief needs to change.\n2.  **Persistent Effort.** They have to be ready to put in a lot of cognitive effort, and for there to be be several points where they are tempted to say \"okay yeah I got it\" when they don't in fact got it yet.[^0nsoz2fpyo2]\n3.  The Receiver *also* needs **excellent, gearsy metacognition**, so they understand their own inner moving parts and which bits are are load-bearing.\n4.  **Knowing when they aren't ready to absorb something.** They have something of \"an idea sandbox\" in their cognitive toolkit, allowing them to load up parts of an idea, check if it feels right to absorb right now, and in some cases do almost all the work to integrate except for \"actually integrate it\", to get a sense of whether it would be safe to absorb.\n5.  [**Grieving**](https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1), or some equivalent (i.e. letting go of beliefs that are important to you)\n    1.  This includes not only grieving for the psychologically-loading-belief that the wisdom is \"about\", but also a meta-level Letting Go of your desire to think \"you figured it out\", when you haven't yet.\n    2.  Also: \"pre-grieving\" (something like \"stoic visualization\"), where you partially grieve something pre-emptively, to free your mind up to reconsider something psychologically loadbearing (without necessarily updating). This helps with the \"check if you're ready to absorb\" step.\n6.  **Deep emotional regulation** (i.e. not just shifting your current mood, but, building safe enduring emotional structures that can handle a loadbearing update)\n7.  (conditionally optional) **Tacit-knowledge interviewing skills**, if the Giver isn't good enough at explaining things or understanding the gears of their own skills. Example question: \"What are some subtle failure modes you might expect if someone said 'I got this', and described it like <insert Receiver's best summary of understanding>? What else might be missing?\"\n8.  **Empathy / Imagination / \"Experience Library**.\" Some ability to simulate an experience they haven't had before. Or, \"a library of reference experiences that are close enough to the required tacit-wisdom-experiences.\"\n    *   Alternate option: The ability to derive the model from the Giver (via interviewing skills) and then for the Receiver to \"do their own storytelling\" (checking in with the Giver that they they got the story right).\n\nIf the Receiver or Giver has high enough skills in one area, they can probably compensate for the other having lower skills, although there's probably some minimum threshold needed for each.\n\nNote that while I feel fairly confident in \"you want all these skills *somehow* in order to transfer tacit soulful knowledge\", the exact implementation of the skills might vary wildly from person to person.\n\nI natively run on stories, so the storytelling frame resonates particularly with me. But I know some people who run on math, who seem to do roughly the same things but with quite different processes from what works for me (and definitely different than what would've worked for me a few years ago).\n\nThe rest of this post circles around these concepts in meditative detail, with various examples. \n\nStorytelling as Proof of Concept\n================================\n\nOne reason I feel optimistic about this is that we already have \"stories\" as the proof-of-concept of communicating Tacit Soulful Knowledge. Not only that, but doing it *at scale*, even if with some lossy compression *(*which is more than most run-of-the-mill tacit knowledge transfer achieves).\n\nFor example: If you live through an actual dystopia, you will gain a deep understanding of how important it is not let your government become a dystopia. But it would suck if this was the *only* way a society could become wise about dystopias, since dystopias are sticky and you might not be able to recover.\n\nFortunately you can read [1984](https://www.planetebook.com/free-ebooks/1984.pdf), and get at least *some* of the deep sense of how and why a dystopia would be bad. It's probably shallower than you'd get from living through a real dystopia. But it's notably not *strictly* worse either. A well written story isn't limited to \"communicating the emotional heft.\" It can also spell out the life lessons more clearly than you'd necessarily acquire when living through a real dystopia (especially if the ~Propaganda Department~ Ministry of Truth is doing its job well and clouding your judgment).\n\nYou can read stories about the Holocaust and learn the depth of evil that has existed. You can watch the videos of the Milgram experiment and know that evil may be more horrifyingly mundane and non-special than you thought. These can be more impactful that merely reading a list of bullet points about what happened in the Holocaust. \n\nThis is \"tacit soulful knowledge\" rather than \"explicit non-soulful knowledge\" because it gives you more visceral sense of *magnitude* and *mattering* than an abstracted set of bullet points can convey.\n\nMotivating Vignette:   \nCompetitive Deliberate Practice\n=======================================================\n\nAfter my last workshop, one participant (George Wang) and I were discussing where the \"deliberate rationality practice\" project might go. \n\nGeorge said:\n\n> \"You know Ray, you've deliberate-practiced Thinking Physics and Downwell and some coding stuff, but I think if you deliberate practiced a *competitive* game, and pushed yourself towards at least an intermediate level, you would grok something deep about deliberate practice that you are probably missing.\"\n\nThat sounded important. \n\nBut, one thing about deliberate practice is that it's exhausting and kinda sucks (that's the whole reason I think there's [underappreciated \"alpha\" in it](https://www.lesswrong.com/posts/FEMWS79AFTmKq2iJK/rationality-research-report-towards-10x-ooda-looping)). Also, my AI timelines aren't super long. I only have time to deliberate-practice so many things. \n\nI found myself saying: \"Hey George, uh, what if instead of me doing that, you and I spent an hour trying to invent the transfer of soulful tacit knowledge?\"\n\n\"Are... you sure that is easier than deliberate practicing a competitive game?\"\n\n\"I somehow believe in my heart that it is more tractable to spend an hour trying to invent Tacit Soulful Knowledge Transfer via talking, than me spending 40-120 hours practicing chess. Also, Tacit Soulful Transfer seems way bigger-if-true than the Competitive Deliberate Practice thing. Also if it doesn't work I can still go do the chess thing later.\"\n\n\"...okay then.\"\n\nI tasked George with \"Try to articulate to himself everything subtle and important about deliberate practicing a competitive game.\"\n\nMeanwhile, I set about asking myself: \"What soulful knowledge do I have, which I can use as a case study to flesh out the fledgling idea I have here?\". \n\nI ended up focusing on:\n\n\"Okay, if I wanted to give people the tacit soul knowledge that *they can defeat impossible problems*, how would I do that, and how would I know if it worked?\". \n\nHaving the \"Impossibility can be defeated\" trait\n------------------------------------------------\n\n*What follows is approximately my inner monologue as I thought about the problem, edited somewhat for clarity.*\n\nOkay. I think I've had a deep, visceral experience that \"impossiblity can be defeated.\" How would I tell the difference between someone who has the deep version I'm trying to convey, vs a pale shadow of it? \n\nWhat would someone who *does* have the \"Impossibility can be defeated\" trait do?\n\nWell:\n\nFirst of all, they *don't* spend a lot of cognitive effort thinking and saying: \"this is impossible\", and rehearsing excuses about that. If they notice themselves thinking thoughts like that, they might make some deliberate effort to reframe it as \"I don't currently see how to do this, and it might turn out to be impossible in practice.\"\n\nSecond: When presented with something impossible/intractable-seeming, they reliably generate strategies that at least give them a bit of traction (such as making lists of why the problem is difficult, and brainstorming how to deal with each difficulty). And at least sometimes, the strategies work.\n\nThird: When they encounter the \"this is impossible\" feeling, they don't become demoralized, or traumatized, or think more sluggishly.\n\nAlso, perhaps importantly, if they were actually \"wise\" here as opposed to merely \"having the sense that they can defeat impossible things (but with bad judgment about it)\", they *also* wouldn't grind away at problems with plans that *aren't* actually tractable. They are calibrated about their limitations. They know when to take breaks, or even to give up. They cleverly, straightforwardly attempt to attack impossible seeming things for awhile, but if they have zero traction they don't force themselves to keep going in a zombie-like cargo cult of agency. \n\n(I notice that I am confused about how exactly to navigate the two alternate failure modes right now, but if I were able to convey the wise version of this thing, I would need to somehow resolve that confusion. My current trailhead is \"there is a subtle difference in feeling between 'I'm beating my head against a wall and I should do something else for a while, and maybe come back later' and 'this is impossible and stupid and I should give up'\", but I couldn't articulate the difference very well. Yet. Growth mindset.)\n\nNow, when I imagine someone who *currently* will say to themselves \"this is impossible\", someone who does not believe me when I tell them \"I believe you can do it\",\n\n...what things go wrong when I try to tell them?\n\nWell, first is that they just won't listen at all. They'll hear my words, but not believe that it's worth engaging.\n\nHow might I address that? \n\nI suspect if I sat with them, one-on-one, and (depending on the person), asked either in a gentle way, or an intense-tough-coach kinda way:\n\n> \"Hey, I feel like you aren't really listening to me. I don't know that now is the right time for you to hear what I am trying to say but I feel like there is something you really aren't even trying to hear, and I would really like to try to convey it to you.\"\n\nAnd then chat back-and-forth, which depends on how they respond to the first bit.\n\n...that *might* work if we have established some trust, and I also do a sort of \"stare into their soul and speak from the heart\" kind of social move.\n\nWhat fails next?\n\nWell, the next thing that might fail is that they'll internalize a Ray-flavored Inner Critic who glares at them to \"do the impossible\" all the time, without actually really internalizing the cognitive moves that make it useful to try that. \n\nFor this to be *wisdom* rather than mere *soulful knowledge,* it needs to come with some additional tacit skills (like noticing a particular flavor of \"grinding pointlessly\". And, maybe some concrete heuristics like \"[brainstorm at least 10 different ways of solving the problem](https://www.lesswrong.com/posts/cbWoMepny3Jo9XqEr/metastrategic-brainstorming-a-core-building-block-skill), and at least 2 of which you've never tried before\", and then if 30 minutes later you still feel the \"grinding pointlessly\" sensation, take a break for a while (perhaps days or weeks).\n\nMapping out the exact skill tree here would take a while, but I don't feel confused about how to do that. Let's move on for now.\n\n### **\"If it** ***weren't*** **impossible, well, then I'd have to** ***do*** **it, and that would be awful.\"**[^whopsq26p4m]\n\nI said earlier, \"tacit soulful knowledge\" often involves making a *psychologically load-bearing update.* \n\nIf I put on that frame...\n\n...I think a thing going on is that many people have some conception of being a good person/student/rationalist. Good people try hard when things are difficult, but they don't have to try hard when things are literally impossible.\n\nIn the 2010s, a few rationality teachers told me that people seem often unable to think about problems that they don't see any way to do anything about. Their brain often slides off even accepting the problem as real.\n\nAlso, people just really don't like doing effortful cognitive labor (and sometimes it's literally bad for them to try).\n\nImpossible problems, by nature, are where your established tools won't be sufficient. You'll need to be \"thinking for real\" almost the entire time. Even when it's going well, it can be exhausting. (The [two weeks I tried to get zero Thinking Physics questions wrong](https://www.lesswrong.com/posts/PiPH4gkcMuvLALymK/exercise-solve-thinking-physics), I thought for four hours a day and then was completely wrecked in the evenings.)\n\nSo, propagating the update probably requires some combo of:\n\n1.  Gaining the tools to actually solve the problem.\n2.  Changing their conception of \"what it means to be a good person\", somewhat.\n\n### **Example of \"Gaining a Tool\"**\n\nAt a recent workshop, one participant expressed the \"I couldn't have solved this because I didn't know a particular physics concept\" concern.\n\nAfterwards I told them about a time when I had done that that particular exercise, I had noticed that I was confused about that particular physics concept, and then I spent an hour exploring 2-3 hypotheses of how to invent that concept for myself. (Then I went on to get the question wrong for unrelated reasons, but not because I missed that particular step.)\n\nThe participant seemed to find that helpful, as a concrete example of \"how to actually deal with this class of problem.\" \n\n### **Example of \"Changing self-conceptions\"**\n\nWhen I imagine a past version of me needing to somehow change my self-conception, what would probably work *for me*, but not necessarily others, is something like:\n\n> \"You know, right now you're afraid to look at the possibility that something is merely 'very hard' instead of 'impossible', because if it were possible you'd have to do it.\n> \n> \"But, like, it's totally possible to realize that... and then *just not do it.* Right now, your morality says 'you have to do things if they are possible and worth doing', and, your psychology says 'you can't look at things that you'd be incapable of dealing with.' But, that's predictably going to lead to a distorted map of the universe, which will get you into all kinds of trouble.\n> \n> \"It's pretty hard to change your psychology here, but your sense of morality actually *already has a term* for 'if your morality and psychology are tying yourself up in knots, that would be a stupid morality, and you should reflect on it and change it. So your morality actively implies it's the right tradeoff, to realize something is important and possible to work on... and still just not do it. You're still a good person. You're clearly a better person that someone who self-deludes into thinking it's impossible.\"\n\n(Getting that self-conception update into my bones, such that I really deeply believed it, might still be its own \"tacit wisdom transfer\" subproblem)\n\n### **Current Takeaways**\n\nSo, conveying 'Impossible Problems can be Solved' requires:\n\n1.  Establishing some kind of trust that I have a thing worth listening to.\n2.  Showing them one worked example of an impossible-seeming problem, and \"here's some tools that solved it\" (built entirely out of pieces that they already had at the beginning of the exercise)\n3.  Some investigation and debugging of why they, personally, were averse to trying to solve it.\n4.  Conveying several specific tacit skills of \"how to make progress\" and \"how to tell if you are dealing subtle psychological damage to yourself by forcing yourself to go forward.\"\n5.  Maybe updating their self conception (which may require recursing on this whole process)\n\nNote that those are all the *prerequisites* to gaining the \"soulful knowledge.\" We're not there yet, even if we have all these pieces. If I explain these things, and even if a listening person makes the self-conception update, that merely establishes a framework to *consider safely* whether to adopt the soulful knowledge. \n\nWhat is the soulful knowledge itself, in this case? What's the difference between someone who intellectually understands all the above, and someone who feels it in their bones?\n\nWell, if they had gained the knowledge \"the normal way\", they would have lived through an experience where something had felt viscerally impossible, so obviously so that their thoughts had automatically veered away from even bothering. But then, *they personally* made the move of \"well, okay but what if I *did* tackle it some way?\" and then they came up with a strategy that seems like it might work, *and then it did work. *\n\nAnd then, they would have lived through a moment where it became self-evident that that visceral impossible-feeling wasn't a reliable signal.\n\nGiven all that, the final move here might be:\n\n*\"After gaining an intellectual understanding of what is necessary, fully imagine yourself experiencing that narrative. *\n\n*Visualize yourself seeing the brute impossibility. *\n\n*Visualize yourself not being able to disentangle from that brute visceral feeling. *\n\n*Then, visualize yourself overcoming it.*\n\n* * *\n\nWhile I was thinking about all this, George was thinking about how to articulate the wisdom of Competitive Deliberate Practice. I'll get to that in a moment, but first, I wanted to touch on...\n\nFictional Evidence\n==================\n\n*Spoilers for Harry Potter and the Methods of Rationality, and Steerswoman. The spoilers are important in some sense, but I claim are minor spoilers in this context, and I think are worthwhile for making the point.*\n\nI first learned about defeating impossibility through fictional evidence. \n\nI read [Harry Potter and the Methods of Rationality](https://www.lesswrong.com/hpmor), and learn that perhaps death can be defeated. I didn't actually care much about defeating death at the time, but what struck me to my core was the notion that you were *allowed* to set your ambitions on the level of \"we are going to eradicate death.\" It had never occurred to me. \n\nThe notion that I, and humanity, are allowed to just set our sights on tackling any problem we want, and carrying that fight through the centuries if we are not yet strong enough today, was something beyond me.\n\nHP:MoR also taught me various tacit knowledge about rationality techniques (although much of it didn't fully crystallize until years later).[^hbxv44vcm7]\n\nAlso, HP:MoR taught me, implicitly, that rationality gives you superpowers.\n\nAnd, sure, the story is an exaggeration. And sure, the story explicitly warned me that things that seem hard are sometimes *really* hard and you're not going to solve them overnight and it might really be 30 or 100 years before a project pays off. Still, I ended up with a vague belief that rationality would give people superpowers. \n\nMaybe not overnight, but, like, surely we'd see notable progress of *some* kind with 7 years of effort.\n\nLater I read the [Steerswoman](https://www.amazon.com/Steerswoman-Book-1-ebook/dp/B00HH1U8Z2) novels, and learned that actually, maybe rationality mostly doesn't give you superpowers. It just... helps you a little bit to find the right answer in confusing situations, which is useful some of the time but not always.\n\nYou can read the Bible, or listen to a sermon, and come away with a deep visceral sense of the depth of God's glory.\n\nNow, this gets at the fraughtness of Fictional Evidence, and uncritically accepting Tacit-Soul-Knowledge transfer. \n\nRationality doesn't give you superpowers within months or even years (given our current level of pedagogy).\n\nAnd it sure looks like God is not real. \n\nThere is still clearly something powerful going on in world religions, which I think should be respected in some fashion. But also, if you update directly on it the way they've framed things, you've failed an epistemic check.\n\nIn 2012, as CFAR was being founded, the vibe of \"rationality can give you superpowers\" was in the air. There was some evidence: sometimes you'd get 300% returns from a simple hack. But amortized over the time you spent hack-searching, rationality gives you more like a [10-20% annual return on investment](https://www.lesswrong.com/posts/85J8hjEn48FicYfvp/strategies-for-personal-growth) *if* you're putting in persistent work (which most people don't).\n\nI'm grateful to the Steerswoman novels for giving me a different framing on rationality, and for a visceral sense of what it's like to realize how an impactful story oversold its promise. \n\nBut, if I had to pick one, I would still basically take the visceral update from HPMOR over Steerswoman. \n\nRationality doesn't give you superpowers overnight. But, [more *is* possible](https://www.lesswrong.com/posts/Nu3wa6npK4Ry66vFp/a-sense-that-more-is-possible). Seven years into my rationality journey, I wrote [Sunset at Noon](https://www.lesswrong.com/posts/2x7fwbwb35sG8QmEt/sunset-at-noon), where I observed \"oh, huh, I never noticed the change while it was happening, but I observe I have my shit together now.\"\n\nSeven years after that? Well, I currently believe that a few skills have clicked together for me, in a way that I expect to become greater than the sum of their parts. I have not yet done anything demonstrably awesome with them yet (though I think I demonstrably have my shit even more together than I did in 2017).\n\nWe'll see in another seven years if I'm standing on a big heap of utility. But I feel like I can see the end of the tunnel now. \n\nI think it will take a while longer for rationality to demonstrably give people superpowers, the way that science demonstrably increased humanity's power a thousandfold. \n\nBut, I do bet that it will. (Happy to operationalize bets in the comments.)\n\nYes, fictional evidence can lie. Even [fictional insights](https://www.lesswrong.com/posts/bkowF2N9uyYh4DXSe/fictional-evidence-vs-fictional-insight) can lie. But, they can also transmit truth. And yep, navigating that is rightly scary and fraught. But for communicating a visceral, impactful truth, it is sometimes quite important for the story to be larger than life, and to reach through with a thousand sprawling tendrils, to feel the weight of it in your soul.\n\n*“Fairy tales do not tell children dragons exist. Children already know the dragons exist. Fairy tales tell children the dragons can be killed.”*\n\n*– G.K. Chesteron,* [*Terry Pratchet, and/or Neil Gaiman*](https://www.tumblr.com/neil-gaiman/101407141743/every-version-of-that-chesterton-quotation-about)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7f229e8bf49e75352927f09cda0c7c517208d1759ea4d700.png)\n\n**PART II**\n===========\n\nCompetitive Deliberate Practice\n===============================\n\nOkay, that was a lot of talk and models. What's the payoff?\n\nThe evidence I present is not as satisfying as I'd like – what I did with George was mostly trace the outlines of a hole-in-my-heart where wisdom should go. But I will spell it out here as best I can. And maybe, someone in the comments will tell a compelling and complete enough story that I come to believe it more fully.\n\nI chatted with George for 2 hours, about Competitive Deliberate Practice.\n\nStep 1: Listening, actually\n---------------------------\n\n*A cup is most useful when it's empty*  \n*– Lao Tzu*\n\nAs the conversation began, I had just spent 20 minutes brainstorming on how one might share the 'Impossibility can be Defeated' exploration. I intended to cross-reference that with George's notes. I expected George to know all kinds of things I didn't know about Competitive Deliberate Practice, but not to have as much scaffolding for how we were going to transmit the visceral soulful qualia.\n\nI was, at that moment, feeling exceedingly clever. \n\nI had a very clear story in my head of how I was going to operationalize Tacit Soulful Knowledge Transfer which was going to be a big fucking deal if it worked.\n\nAnd then I turned to George, and I mechanically looked over my notes from the \"Defeat Impossibility\" thing and...\n\n...a horrible thought hit me.\n\nMy model explicitly said, for this to work, I would have to listen. \n\nReally listen. \n\nDeeply listen. \n\nGet over my ego, get over my sense that \"yeah I get it\". \n\nAnd here I was, with my mind mostly full of thoughts of how clever I was.\n\nI was... going to have to let go of that self-impressed feeling of cleverness, and get ready to hear whatever George had to say.\n\n*Shit. *****Gut punch******\n\nThen I settled in, and took a deep breath. I brought half-formed memories of times I had really listened, or someone had really listened to me. I remembered the difference between dutifully Trying to Listen, and Actually Listening.\n\nAnd...\n\n***shit. gut punch***\n\nIt occurred to me: \n\nMy model said that the thing I was about to attempt to do was *maybe* possible, but exceedingly hard if so. It involved imagining experiences I hadn't had yet. Fiction suggested it was possible. But, I wouldn't have to merely listen as well as I had listened before. I would have to do better than that.\n\nMy intellectual model said: There would be multiple moments where I would think that I had gotten it, and I would feel excited to start talking about my ideas about it, but I in fact *still wouldn't have gotten it*, because the whole point here was that the nuances are subtle, and you have to hear each individual one. And I should expect that to happen multiple times. \n\nAnd then probably, it would only half-work, at best.\n\nAnd while my explicit considered belief was \"I think I understand all the pieces of this skill, but I do expect the skills to be legitimately hard even if it's theoretically possible\"... \n\n...I still did kinda basically expect it to work reasonably well on my first try. \n\nAnd that was silly. And I felt ashamed.\n\nAnd then I told myself to get over myself and start listening.\n\nGeorge started saying some things. I started asking questions. The conversation went back and forth, with me constantly asking: \"but, why is this unique to *competitive* deliberate practice? If I were playing *single player* chess against a computer in a world with no competitive chess scene, what would be missing?\". And whenever either I felt confused, or he felt I had misunderstood something, we slowed down and teased out the subtle distinctions that might have been lost. I feel proud of how that part went.\n\nI don't yet know how to convey the art of tacit interviewing in this post. I'd have to see a few people fail-to-understand it before I knew what words to say to get the real point across.\n\nBut, at the end of the day, here are the slivers of wisdom that George and I articulated together:\n\nThe scale of humanity, and beyond\n---------------------------------\n\nIf you beat a single player videogame, was that hard? What did you achieve?\n\nIf you get 1000 points, or 10,000, what does that mean? If you get a million points, you might say \"well that seemed like enough I guess?\" and move on to something else. But what did that say about your competence?\n\nWho knows?\n\nIf you play chess against *people*, you can watch your Elo score rise some percentile. If you look at the range of Elo scores, at the top humans, you'll see how your improvement compares against that scale. \n\nYou began at some percentile. You saw your dozens or hundreds of hours of effort, and you saw how that effort brought you to some slightly higher percentile. \n\nYou will have some sense of how small a hundred hours of effort are, and how wide humanity's range is.\n\nThis [random PDF](https://www.fide.com/images/stories/NEWS_2012/FIDE/120806_YouGovPressRelease.pd) says that 605 million adults play chess. If you move a few percentage points along the scale, millions of people pass behind you, and hundreds of millions more might still be ahead.\n\nToday, there are AIs that play chess at a higher level than any human; you will see how far the range of possibility extends beyond humanity, so far, in this one domain.\n\nI spent ~40 hours practicing Thinking Physics puzzles. I got better at it. I care more about Thinking Physics than chess. But how good am I at solving Thinking Physics puzzles now? I dunno, man. There's no competitive Thinking Physics community to compare myself against.[^sp6ak53orj]\n\nGeorge brought this up initially in the context of \"appreciating how much your deliberate practice efforts are worth,\" but it seems like something importantly general: \"How big is human competence\", and \"how much bigger is AI competence\" seems like they should seep into a bunch of different subtle judgments. They should start to give you an inkling of how much competence matters.\n\nSince the conversation, I've made some attempt to meditate on this, similar to Nate Soares's [meditation on large numbers](https://mindingourway.com/respect-for-large-numbers/). I don't think it's super sunk in yet.\n\nBut, I'd guess I'm not *strictly* worse here than everyone who's lived the experience. I bet some people level up at chess, and see themselves move along an Elo percentile, but don't particularly track how many thousands or millions of people they just moved past. I expect truly grokking the scale to require deliberate effort beyond what happens automatically through winning games.\n\nCompetitive Spirit\n------------------\n\nThis wasn't one of George's main points, but it came up while I was trying to brainstorm for myself why competitive games might be different from non-competitive games.\n\nWhen I get into a competitive mindset, I tap into a kind of... primal power that I don't always have access to.[^v9ndih14kxf] The visceral sense of that primal power is probably hard to explain to someone who's never experienced it.\n\nIt so happened I already had a library of experiences about competition that felt soulful in some way. Here are some of them.\n\n### **Is your cleverness going to help more than Whatever That Other Guy Is Doing?**\n\nAt my first cognitive bootcamp, I had people solve an online puzzle without a clear win condition, hosted on a website. I saw one person cleverly editing the website code to change the parameters of the puzzle to do experiments.\n\nI told them \"Yep, you're very clever. Yes, that is allowed in this exercise. Do you think that is helping?\"\n\n\"Yes\", they said.\n\n\"Do you think it's going to help you solve the exercise faster than John Wentworth over there?\" (John was sitting across the room)\n\nThey said \"Oh. Uh, ...maybe?\".\n\nI don't actually know how useful their clever meta-gaming was, but the exchange seemed to get them asking \"is this actually going to *differentially help?\"* It made it very salient that reality was capable of judging  whether or not it was an effective strategy. Either their tactics would get to the answer faster than John Wentworth, or they wouldn't. And this ignited a bit of a fire in them that wasn't there before, when they only had to compare themselves against their imaginary alternate self who followed some other less-clever-feeling strategy.\n\nComparing themselves to a particular other person *also* feels relevant to the aforementioned \"human scale\" thing. \n\n(In the end, I think the two of them solved it at roughly the same time.)\n\n### **Distaste for the Competitive Aesthetic**\n\nMy default playstyle for most games is \"be the silly guy trying to do weird things with the rules (regardless of whether that turns out to be useful)\" or \"the guy trying to tell a story, more than win a game.\"\n\nCompetitive players often ruined the parts of the game that I liked. Storytelling and silly rulebending depend on some slack in the game, where the play is not so cutthroat, and you can afford to do goofy things.\n\nWhen I play Minecraft, I prefer the early game, when the world is unspoiled and beautiful and Miyazaki-esque, and also there are natural resource limitations that you have to work around in a Survivor-esque way. There is something beautiful about it, and I feel horror and disgust when people show up who want to Race Race Race to get to the end of the tech tree and bulldoze villages and build digital factory farms.\n\nWhen I got into [feedback-loops and deliberate practice](https://www.lesswrong.com/posts/pZrvkZzL2JnbRgEBC/feedbackloop-first-rationality), it started to dawn on me that my aesthetic distaste for competition might meaningfully get in the way of me developing important practice skills. I might want to acquire the \"Competitive Spirit Aesthetic.\"\n\nI was scared about this. My distaste was, in fact, psychologically load-bearing for me.\n\nI don't have a crisp story of how this played out – I made some bargains with myself that I would try to hold on to my capacity for silly or storyful play. I think it's helped that I only adopt \"competitive mindset\" in some kinds of of games. I still play Minecraft the same way as always.\n\nBut what ultimately allowed me to tap more into Competitive Mindset was leaning into other things I *did* care about (including, the deliberate cultivation of [new aesthetics in light of new facts](https://www.lesswrong.com/posts/YN6daWakNnkXEeznB/propagating-facts-into-aesthetics)).\n\n### **Building your own feedback-loop, when the feedback-loop is \"can you beat Ruby?\"**\n\nLast year my colleague [Ruby](https://www.lesswrong.com/users/ruby?mention=user) took me Go Karting. Ruby does a ton of Autocross driving, and enjoys the thrill, and has a bunch of experience.\n\nI decided to treat the day as an opportunity to Purposefully Practice Purposeful Practice, and try to figure out the skill tree of a new activity from first principles. One issue was that I needed to evaluate if I was getting better. But on the Go Kart track, there was no indicator of how long a given lap around the track took. \n\nEventually I figured out, a feedback loop that existed was \"do I seem to be keeping up with Ruby?\"\n\nThe connection between my obsession with feedback loops and my dream of a new spirit of rationality was enough to bridge some kind of gap, and I leaned into \"okay, I'm now trying to beat or at least lose less badly to Ruby.\" And this did seem to help me figure out what exactly it meant to take turns wide, and why sometimes it was appropriate to accelerate but sometimes to brake in pursuit of speed.\n\n### **...back to George**\n\nAll of those previous sections occurred to me and I mentioned some of them to George.\n\nWe talked a bit about the qualia of competitiveness. We had discussed the Scale of Humanity, but many of the ideas we'd subsequently talked about it hadn't seemed especially about \"competition\" in particular. \n\nI was excited about the Qualia of Competitive Spirit because it felt like a real example of something *felt*, which couldn't easily be transferred in words. \n\nBut... eventually I noticed the niggling feeling that George was seeming less engaged. And then I realized... I had been the one to suggest \"competitive spirit\", not George. Maybe it would turn out to be important, but it wasn't what he was particularly trying to talk about.\n\nOh shit I'm still supposed to be listening.\n\nChrist. Listening sucks.\n\n*shit. gut punch*\n\n**\"Mature Games\" as \"Excellent Deliberate Practice Venue.\"**\n------------------------------------------------------------\n\nWe went back and forth about a bunch of topics where George would articulate something important about deliberate practice, and I would ask \"but why is that specific to *competitive* practice?\" and his answer wouldn't feel satisfying to me.\n\nEventually, something crystalized, and we agreed:\n\nIt's not just that Competitive Deliberate Practice teaches something unique *through the process of competition.* Sometimes, it's simply that competitive games – especially mature games like Chess that have endured for centuries – are dramatically better avenues to Deliberate Practice (and, to Deliberate Practice Deliberate Practice), than many other arenas.\n\nIf you play an arcade game by yourself, in a vacuum with nobody around to compare yourself to, not only is there no human scale, you also have to invent all your practice techniques for yourself. \n\nMature competitive games, with millions of fans, develop entire ecosystems of people developing techniques, and people who teach those techniques. It is known what techniques need to be taught in what order. It is known what mindsets make sense to cultivate in what order.\n\nYou can innovate and beat the system with cleverness, but there may be bitter lessons about how much cleverness actually helps, compared to sucking up your ego and learning from the established wisdom first. Maybe your cleverness will help, maybe not. Reality will arbitrate whether you were correct. \n\n**\"Deliberate Practice qua Deliberate Practice\"**\n-------------------------------------------------\n\n*You know what they call Purposeful Practice that \\*works\\*? Deliberate Practice.*[^63f4qdbwje4]\n\n*– ~Tim Minchin~ me*\n\nMost of what I've been doing since I started my journey is not \"Deliberate Practice.\" Deliberate Practice is a technical term which means \"a practice regimen that has been time tested and reliably works.\"  \n\nMost of the time, the thing I do is is use [Noticing](https://www.lesswrong.com/posts/XHnAakZtcEktEGvYJ/scaffolding-for-noticing-metacognition) skills to identify little micro feedback loops, and use reasoning to identify the hard parts of what I'm doing, and try to keep the practice focused on pieces that push my current limits. Those are all important ingredients of Deliberate Practice.\n\nBut ultimately, I am guessing about what will work, and trying a bunch of inefficient things before I find a given thing that seems like it actually works, and I lack a human scale to tell me very clearly how I'm doing on the micro level. There might be some particular subskills where someone somewhere might know how I'm doing on an absolute scale, but I don't think they'd be able to track overall how I'm doing in \"the general domain of solving confusing problems.\"\n\nThere is something very fun and innovative and clever-feeling about inventing my own practice as I go.\n\nBut, a thing I suspect (although George Wang didn't explicitly say), is that working in a truly \"Deliberate Practice qua Deliberate Practice\" domain would give me some visceral appreciation for the *usual* set of skills that feed into deliberate practice, which probably include a greater percentage of time finding established exercises, teachers, etc.\n\n### **Feedback loops at the second-to-second level**\n\nNormally, if you play a game of chess, or a sport, you go off and do a bunch of stuff, and eventually you win or lose. The process of credit assignment is hard. Was a particular play good, or was that you falling into the opponent's trap, but then getting lucky?\n\nWith proper deliberate practice, you sometimes have instructors watching you, critiquing your form at the second to second level. Because they are experts who can win games and have taught other experts before, you can actually just trust what they say. Your brain learns better when it gets to update immediately, rather than \"some time later.\"\n\nLiving the experience of second-to-second feedback loops might help drive home how noisy and slow the usual feedback loops are, when you're trying to practice in a domain without such instruction. It might give you some sense of what to be aiming for when you construct your clumsy-by-comparison Purposeful Practice. Or, give you more motivation to actually find expert advice on the subskills where that's even possible.\n\nToday, in some domains (Chess, Go), we now have AIs whose knowledge so surpasses humans that you can get a basically perfect correction to every move, as soon as you make it. (And perhaps, not too long from now,[^us0xodq1wl9] you'll even get an integrated LLM that can explain why it's the correct one)\n\n**Oracles, and Fully Taking The Update**\n----------------------------------------\n\nEventually, George articulated a different lens on the previous point.\n\nIt's not just that the feedback is so rapid.\n\nIt's that when you can *fully* trust a source of knowledge about your practice, you don't have to do *any* second guessing about what the correct move was. You will instantly know that you have made a wrong choice, and learn what would have made a better choice. You'll at least learn that at the pattern-matching level. \n\nEventually, you may get a visceral sense of what it's like to be able to fully make an update, all the way, with no second guessing.\n\nThis seemed important, beyond the domain of Deliberate Practice. A visceral sense of when and how to fully take an update seems like something that would come up in a lot of important areas.\n\n**But what do you do** ***differently*****?**\n---------------------------------------------\n\nThat all sounds vaguely profound, but one thing our conversation wrapped up without is \"what actions would I do differently, in the worlds where I had truly integrated all of that?\"\n\nWe didn't actually get to that part. Here are my current guesses:\n\nRe: \"The magnitude of the human scale\"\n\n*   Recognize how high skill ceilings can be, and what effort is required to improve your competence.\n*   When making plans, have an accurate model, rather than wishful thinking, of how quickly you can gain mastery over things.\n*   As you navigate the world, have a better sense of how competent various people are likely to be at various things. Have a sense of what masters, intermediates and beginners tend to think.\n\nRe: \"fully taking the update\"\n\n*   When I'm considering how much to update on something, I'd have a clearer sense of the specific mental moves I would make, if I were to update fully. I have less hesitation to do so. (But, hopefully, also a calibrated sense of when to do that)\n\nMagnitude, Depth, and Fully Taking the Update\n=============================================\n\nThat last point actually connects back to a central thing about Tacit Soulful Knowledge.\n\nLet's recap some proposed wisdoms:\n\n*   Burnout is real, and you must account for it when optimizing your productivity.\n*   Projects are often hard, in a way that requires you to actually pivot your plans, and switch from a \"executing\" mode to a \"gathering info\" mode.\n*   Sometimes, impossible-feeling problems can be solved.\n*   There is untapped benefit in rationality, if only the art were to be developed more.\n*   The scale of competence is vast, and moving along a small slice of it can still take dozens, hundreds or thousands of hours.\n*   There is a mental motion of being able to \"fully take an update, all the way\", and in some circumstances it's the right motion.\n\n(btw, if you don't buy some of these, that's quite reasonable! For the most part I haven't argued for them, just tried to convey why I, or someone else, believes in them. For now, my main goal is to have them as illustrative examples, so you can at least see how some mental motions might fit together around them.)\n\nIn most cases, someone without said wisdom will nod along to the sentence. It doesn't sound wrong. But, when the time comes to act on it, they'll only pay superficial heed to it. A commonality so far between the Possible Tacit Soulful Knowledges I've examined is that there is an update worth making, but it is *much* deeper than you're intuitively expecting, with more far reaching consequences. \n\nThe grad student vaguely knows that sometimes you work too hard and exhaust yourself. But they haven't been deeply fucked up by this in a way that was fairly enduring and required changing their priorities in a deep way.\n\nThe startup founder vaguely knows they'll need to learn more things, but don't have the visceral expectation that they might *really try hard* for a year, fail to learn anything new, and that right now they really need to slow down, switch to \"gather information\" mode, and actually propagate the update that they need a specific story for why their plan is special.\n\nIs there a simple, general skill of 'appreciating magnitude?'\n-------------------------------------------------------------\n\nIt's possible that a lot of \"listening to wisdom\" might look less like thoughtfully, emotionally propagating an update... and more like \"just, like, actually do math?\"\n\nOr, rather: there might be a skill to propagating math through your bones – feeling different orders of magnitude and intuitively rearranging your beliefs and actions accordingly.\n\nAppreciating numerical magnitude is it's own kind of difficult wisdom. People are [known to be bad at it](https://www.lesswrong.com/posts/2ftJ38y9SRBCBsCzy/scope-insensitivity). It requires not merely viscerally feeling that 100 is 10x bigger than 10. It requires having some sort of schema such that your priorities naturally weight 10x things appropriately (without following naive consequentialism off a cliff and missing a lot of wholesome value in things that are hard to measure).\n\nVarious people have tried communicating this. Nate Soares wrote [Respect for Large Numbers](https://mindingourway.com/respect-for-large-numbers/), where he attempted to mediate on one hundred objects, and hope that that would transfer to other domains. \n\nAndrew Critch once suggested imagining a 10x10x10 cube, which simultaneously can tell you how much bigger 1, 10, 100, and 1000 are than each other... and yet also fit them all in your visual field at once. He's also suggested looking at [scaleofuniverse.com](https://scaleofuniverse.com/en), which lets you scroll your zoom from the plank length to human scale to the entirety of the universe, and notes that there are in fact only 62 orders of magnitude across the whole thing! You only need to pinch-zoom the universe 62 times to get from the smallest thing to the biggest. \n\nIf you could develop some intuitive sense for how to relate to each level-of-scale, maybe you could just look at a phenomenon, see \"what scale\" it's on, and then adjust your behavior accordingly?\n\nUnfortunately, these sorts of things don't seem to actually land for most people. Critch has noted he's tried tons of ways to convey numeracy to people in a way that lets them see how it fits into their lives. When I asked him what was surprising about his time at CFAR, [he noted](https://www.lesswrong.com/posts/Jash4Gbi2wpThzZ4k/cfar-takeaways-andrew-critch#Surprise_1__People_are_profoundly_non_numerate__):\n\n> People are profoundly non-numerate. And, people who are *not* profoundly non-numerate still fail to connect numbers to life. \n> \n> I'm still trying to find a way to teach people to apply numbers for their life. For example: \"This thing is annoying you. How many minutes is it annoying you today? how many days will it annoy you?\". I compulsively do this. There aren't things lying around in my life that bother me because I always notice and deal with it.\n> \n> People are very scale-insensitive. Common loci of scale-insensitivity include jobs, relationship, personal hygiene habits, eating habits, and private things people do in their private homes for thousands of hours.\n> \n> I thought it'd be easier to use numbers to not suck.\n\nI currently feel only partway through my own journey of \"appreciating large (or small) numbers.\" When I look at my past, and see why I would have flinched or rejected \"appreciate magnitude\" I see glimpses of the tacit skills I now just barely am good at, and what deeper tacit skills I might yet need to acquire. \n\nI don't feel ready to write the post on that yet. But I can see the glimmers of a general skill, which, if one managed to learn, might let you shortcut a lot of the emotionally effortful process of internalizing wisdom from evocative stories.\n\n**PART III**\n============\n\nOne might think the trick is to just listen to people as if they have wisdom all the time, even if you don't understand it yet. Alas, there are pitfalls on the other side of the equation. Beware.\n\nTacit Soulful Trauma\n====================\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/d6ec2f7459746117d47a2f13cd94c2777cb1172e23ffe444.png)\n\nSometimes, friend warn me about failure modes they think I'm about to walk into.\n\nAnd sometimes it's difficult, because while I can see there *is* a real important thing they want to convey to me, they also are clearly triggered about it. I can't tell from the outside whether the problem is that I need to update harder, or that they need to process something and let go of a clinging-orientation.\n\nOnce, I failed to lock the house door a few times. A roommate said to me \"I want you to understand that *crime actually exists.* We've actually had people have houses broken into in this neighborhood.\" Another friend added \"And, like, getting broken into doesn't just mean you lose some stuff. There is something violating and scary about it, that at least rattled me and I think would rattle you too.\"[^lve9lu8rz7g]\n\nThat interaction made sense and seemed fine. I didn't actually really update that hard about crime, but I think I probably should have. \n\nBut, I've also known people who seemed worried about things (such as crime) in a way that seems more like paranoia, taking more precautions than are worth it, making everyone around them take more precautions than seem worth it.\n\nIt's hard to tell the difference from the outside. \n\nI think part of the skill here is to ask what they might know, that *is* true and helpful, even if parts of it are wrong or overcompensating. Try to build a complete model of what they believe, and why they believe it, without necessarily adopting that model as your own.\n\nCults, Manipulation and/or Lying\n================================\n\nOne thing that gives me a bit of pause on all this is that I've known a few cults (which, by selection effects, were mostly rationalist cults). I have heard of others.\n\nI don't like people overreacting to accusations of cultishness. People complain about the rationality community being cultish. I think this is somewhat true, but mostly not. Whenever someone tries a high-effort, intense community, and fails, I think people blame the vague-culty-pattern matching without really grappling with the gears of what actually went wrong.\n\nBut, just like crime, cults are real, and bad. \n\n(Signs of bad cults: encouraging you to give up ties with outsiders, activities that result in you being chronically underslept; demanding you conform to a totalizing worldview and making you feel bad if you object to even part of it; asking you to do things that make your stomach squirm a little; and, feeling like if you were to actually question the leadership or the ideology it wouldn't go well for some reason).\n\nOne of the thing going on in some of the cults I've seen, from the sidelines, is something like \"Telling a bunch of stories, which convey a lot of frames, which make you question your old frames all at once, overloading you with soulful knowledge faster than you can really judge it.\" (This *isn't* automatically the sign of a bad cult IMO, sometimes you're just discovering a new worldview and that's fine. But if it's combined with the other things it exacerbates them.)\n\nAlso, sometimes the impactful soulful stories they tell are basically made up. People lie.\n\nIf you're at a *rationalist* cult, they'll... probably say words not too dissimilar to \"we have a model of how to transfer soulful tacit knowledge.\" Which, uh, has me feeling a bit antsy, and led me to add a lot more disclaimers to this post. Right now I am basically planning to build an intense rationality training community, and it will (eventually) probably end up somehow integrating the ideas in this post.\n\nI have seen at least 3 rationality-adjacent subcultures that got really into various flavors of \"mental tech\", which fucked up people in a few ways. (Shortly after coming up with the ideas here, I ended up chatting with a group about it that included a younger person, who started saying words like \"mental tech\" and \"I think I'm pretty good at not being manipulated or hurt by this sort of thing\" which rang serious alarm bells in me, because it's the exact sort of thing that several people who later hurt themselves or others said)\n\nDoing serious justice to how to navigate this is its own post. But I wanted to repeat it here, in its own section for emphasis. I hope this conveys why I'm somewhat worried, and also gives me a bit of accountability for what I might do next with the ideas in these posts (i.e. having more people who are vaguely tracking that something might go wrong)\n\nSandboxing: Safely Importing Beliefs\n====================================\n\nA while ago, a colleague said \"You know Ray, it seems like you directly import other people's beliefs and frames straight into your worldview without, like, type checking them or keeping track of where they came from. And that kinda sketches me out.\" \n\n\"...go on?\"\n\n\"Like, when I'm getting a new belief from someone, in the metaphorical codebase of my mind, I don't just do a global `import frame` from Alice. I... like, import the `import alice_worldview` and before I run `alice_worldview.frame` I inspect the code and make sure it makes sense.\"\n\nThat person had a much more structured mind than I did at the time. Their cognition runs on math and logic. Mine ran mostly on narrative[^du2nkzhc2o4]. I didn't seem to actually be running into the problems they were warning about, and I didn't update much on it. \n\nBut, last week (a few years later), as I mulled over importing tacit soulful knowledge that might secretly be confused trauma from other people, I was like \"you know, I think I should go back to Bob and ask him to explain that better.\"\n\nI don't think I got his tacit skills here very deeply, but one straightforward skill he mentioned was \"noticing when you're bouncing off something, and making conscious choices about that.\" If something feels off when you look at a new idea, or you feel like you're bouncing off it, check in with yourself about whether it seems right to keep thinking about it right now. \n\nA few things to consider:\n\n*   An idea might be false, or mislead you to believe other false things or take worse actions.\n*   An idea might mess with important coping mechanisms. You might be ready for it later when you've figured out better coping mechanisms, or [grieved](https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1) something.\n*   An idea might be too complicated for you to grasp. (Consider writing things down or drawing rough diagrams, to help expand your working memory)\n*   You might have recently absorbed too many other ideas, and you may just need some time for your brain to compile them down into your longterm, integrated memory. (Or, you might need to think through and actively consolidate them first)\n*   An idea might not be fitting into your usual frames/ontology. You'll eventually need to look at it a different way. You might try asking your partner to explain it differently.\n\nFor all of these reasons, for the immediate future, you may want to consciously check \"do I want to think about this right now?\". You can come back to it later. Or, maybe just never come back to it.\n\nAsking \"what does Alice believe, and why?\" or \"what is this model claiming?\" rather than \"what seems true to me?\"\n-----------------------------------------------------------------------------------------------------------------\n\nIf it seems safe to at least *consider* an idea, but you're not sure you want to fully take it in, how can you do that?\n\nMy current guess is something like: Instead asking \"Is this true?\", ask \"what does Alice believe?\" or \"what's a consistent model that Alice is pointing at, even if Alice can't articulate it very well herself yet?\". Try to put that model in a staging area in your mind, where you're deliberately not plugging it into your meaningmaking center, or integrating it into your own map.\n\nPre-Grieving (or \"leaving a line of retreat\")\n---------------------------------------------\n\nA further step you can take is \"pre-grieving.\" Suppose a new idea would mean that something sacred and important to you was wrong, or gone. You aren't sure if the new idea is true, or if your sacred idea is false. A thing I've done sometimes is \"grieve in advance\" (a variant of stoic negative visualization), where I imagine what the world *would* look like, and what I'd have to do, and what emotional processing I'd need to do, if my sacred thing was false or gone. Do some of the emotional processing, but without \"the final steps\" where you really accept and internalize it.\n\nThen, I'm able to think more clearly about whether or not my sacred thing is false.\n\n(Even better, see if you can identify multiple *different* things you might want to pre-grieve, so that you don't overanchor on \"do I need to grieve *this* specific thing?\").\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/9cd5d43299d457f1ff5d97569053e901511ff639eff7a494.png)\n\nEPILOGUE \n=========\n\nSo, that was a lot. \n\nYou're probably not going to remember all of it. \n\nFor now, I'd like to distill out:\n\n*   Some guesses at practical advice\n*   My guesses of a sort of \"longterm research direction\" to take these models. \n\nThe Practical\n=============\n\nLearning to listen\n------------------\n\nMuch of this post deals with deep, subtle skills. I think [grieving](https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1), [noticing](https://www.lesswrong.com/posts/XHnAakZtcEktEGvYJ/scaffolding-for-noticing-metacognition), articulating [tacit knowledge](https://commoncog.com/an-easier-method-for-extracting-tacit-knowledge/), and emotional awareness are all important, and worth investing in, and necessary for the upper limits of what's possible here.\n\nBut if you aren't that strong in the above skills yet, I think the main thing to takeaway is: \"Listening is a skill, and you can learn to do it better, and it has a pretty high skill ceiling.\" Most people, even pretty good listeners, could learn to listen more deeply.\n\n***When*** **to listen deeply**\n\nA major difficulty with \"deep listening\" is that it's expensive, and you can't do it literally all the time. (If you get good at it, it gets cheaper, but there's also tons of skills you could be leveling up such that they're cheaper). What do? \n\nTwo cases come to mind:\n\n*Trigger #1: *  \n*Your conversation partner keeps repeating the same points, in a somewhat agitated way.*\n\nMaybe, this means there is something you aren't getting. Or, maybe it means, you really get the point and they are agitated for unrelated reasons, or aren't listening to *you.* \n\n*Trigger #2:*  \n**You* keep repeating the same points, in a somewhat agitated way.*\n\nMaybe you are picking up that your partner hasn't seemed to grasp the significance of what you're saying, and you're trying to \"say it harder.\" (this rarely works AFAICT). Or, maybe you're sort of tunnel-visioned and not really realizing that yeah, they get it, but there is something else *they* are trying to convey to you.\n\nIn both cases, I think the thing to do is some kind of check of \"are either of us actually missing anything here, or can we just chill out and move on?\". Sometimes, it's immediately obvious when you think about it for two seconds. Sometimes, it's helpful to ask out loud \"hey, I notice you keep repeating this point. Can you say more about what you think I don't get, or haven't really listened to yet?\"[^l2jr26t4wz]\n\n**\"To Listen Well, Get Curious\"**\n\nThere's lots of advice elsewhere on how to listen. But I think an important one is Ben Kuhn's \"[To listen well, get curious.](https://www.lesswrong.com/posts/4K5pJnKBGkqqTbyxx/to-listen-well-get-curious)\"\n\n> Frequently \\[advice to\\] “just listen” comes with tactical tips, like “reflect what people said back to you to prove that you’re listening.”\n> \n> Recently, I realized why people keep giving this weird-seeming advice. Good listeners *do* often reflect words back—but not because they read it in a book somewhere. Rather, it’s [cargo cult advice](https://en.wikipedia.org/wiki/Cargo_cult_science): it teaches you to imitate the surface appearance of good listening, but misses what’s actually important, the thing that’s *generating* that surface appearance.\n> \n> The generator is curiosity.\n> \n> When I’ve listened the most effectively to people, it’s because I was intensely curious—I was trying to build a *detailed, precise* understanding of what was going on in their head.\n\nBen's post is in the context of giving people advice. Here, I think it's both relevant to an aspiring Knowledge Giver (to make sure they understand what's going on in the Listener) and in the Listener (to really soak in the worldmodel of the Giver).\n\nI think the curiosity is important, but I do nonetheless think \"focus a lot on paraphrasing\" is a pretty useful tactic, if at least one person in a conversation thinks the other is missing something. \n\nThis has an obvious practical benefit of \"you get to check if you in fact understand each other.\" It also tends to change the style of conversation in a way that short-circuits some failure modes (where people keep actively talking over each other, tunnel-visioning into their own particular worldview)\n\n**Listening is (slightly) contagious.**\n\nAnother important point – say you wish someone else would listen to you. Many people have a naive tendency to get more insistent. But this often backfires. People tend to (somewhat) mirror the behaviors of people around them. If you get more insistent and locked into  your thing, they'll get more insistent and locked into their thing. \n\nCounterintuitively, shifting to do more listening, yourself, tends to open up the conversation such that they are more willing to listen themselves. (Both because of general mirroring, and because they might be anxious about you not hearing whatever *they* have to say, and they tend to chillax more after you've demonstrated you heard it)\n\n**Grieving and Listening**\n\nI think the hardest part about \"deeply listening\" is that I have a self-image of someone who already is at least pretty good at listening, especially when I'm actively trying. Or, there are things I really want to do that *aren't* listening. \n\nSo for me, the next level above \"just try to get curious and listen, at all\", is to notice when I am somehow averse to putting more work into listening, or telling myself I'm already doing it \"enough\", and (usually) do some kind of microgrieving about it.\n\nThe Longterm Direction\n======================\n\nPartly, I'm just hopeful about me, and a few more marginal people around me, being better at transmitting complex, soulful ideas.\n\nBut, what feels particularly exciting to me is that this feels like a plausibly-complete map of what's necessary to transfer deep tacit knowledge. This post is phrased in terms of \"skills\", but the actual important thing here is more like \"properties a two-person-system needs to have somehow.\" If you can accomplish it without anyone having to learn \"skills\", all the better. Skills are expensive.\n\nMy dream is that someday, awhile from now, humanity will have some kind of effective soulful knowledge transmission engine that works really well, at scale.\n\nSo, followup work I'd like to see, or do myself:\n\n*   Check if other people feel they have given or received \"soulful knowledge\" somehow, and see if they seemed to do it in a way that maps onto these ideas, or if it was a totally different way. Are the major pieces missing? Have I only really focused on one type of \"wisdom\" or \"soulful knowledge\", and are there are other types that require other stuff?\n*   Just have some people try out the approaches in this post, and see how it goes, and write down the results.\n*   Just practice it myself for awhile, until I feel like this has shifted from \"a cool idea that I've tried a few times that seems promising\" to \"a deep integrated skill\", and see if there are more conceptual updates to be had about it.\n*   Think about ways to shortcut some of the work here. What's the simplest possible version of each skill, that works for the combined engine? Can those skills be converted into simple english sentences I could say, that most people would be able to do without learning new nuanced skills? If not, what are the minimum prerequisites?\n*   In particular, think about the \"appreciating magnitude\" skill, and how to convey it at all to people, and see if it's able to generalize somehow.\n*   What are hypothetical magical tools that would obviate the need for those skills?\n\nHappy listening.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5a9eb0222ab629dd0b36c1839184a90235f6cfd5cb55eb85.png)\n\n[^91qu016emb]: Notably, the \"Solve a random Thinking Physics\" exercise with 95% confidence is essentially trying to be a simulation that conveys tacit knowledge in this way, with the forcing function \"until you can do this reliably, calibratedly, you aren't done.\" It also, sometimes, conveys something like wisdom, which is hard-earned in the sense that it's pretty expensive to do 10 Thinking Physics problems and get them all right. \n\n[^7ixwgj77djn]: My calibration on the feeling \"ooh, this insight feels promising\" is something lowish (10% – 30%). But this feels more like I just deconfused myself on a Thinking Physics problem and am ~75% likely to get it right. \n\n[^z2jopihrx7d]: (More on that later).  \n\n[^dgok0px6xku]: I do think there are people for whom Tuning Your Cognitive Algorithms is overwhelming, and people for whom it disrupts a coping mechanism that depends on not noticing things. If anything feels off while you try it, definitely stop. I think my post Scaffolding for \"Noticing Metacognition\" presents it in a way that probably helps the people who get overwhelmed but not the people who had a coping mechanism depending on not-noticing-things.I also think neither of these would result in suicide in the way that happened to SquirrelInHell. \n\n[^id3v6o48fyh]: I am not very confident in this interpretation, but, I think the interpretation is a useful intuition pump for why you should be careful. \n\n[^gi5v326nco9]: See also Eliezer's Making History Available. \n\n[^0nsoz2fpyo2]: This benefits from previous experience where \"you thought you got it\", when you didn't got it, such that you have some taste about the difference. \n\n[^whopsq26p4m]: Recently, I had to do some bureaucratic paperwork, on behalf of someone else. The actual task ultimately involved skimming some notes and making 6 checkmarks. I failed to do it for a day, then another day. On the third day, I said \"You know Ray, if you busted out your metacognition tools you'd probably come up with a plan to get this done\", and I felt the fury of a thousand suns inside me scream \"Yes but then I'd have to do the paperwork.\"I grudgingly dialogued with myself, and then got a colleague to check in on me periodically, and I slowly bootstrapped into a being who could made progress checking the six checkmarks over the course of the day (still slowly and painfully dragging my feet about it).So, when you don't want to do my stupid metacognition exercises, I get it. \n\n[^hbxv44vcm7]: People periodically complain about Eliezer's LessWrong Sequences or HPMOR being long and meandering, and say \"we should rewrite those things to be simple and just get the ideas across.\" And, man, I agree they are hella long and we should get shorter intro materials. But a crucial element is that Eliezer is not just trying to give you explicit knowledge, he is not even merely trying to convey tacit knowledge. He is trying to convey a visceral, deep awareness in your meaningmaking-center, about living your life in the world where truthseeking is the central dance. This is a much more difficult project.Not everyone vibes with Eliezer's vibe. And I think it's probably worth having a \"list of explicit knowledge\" version of the sequences that's more bare bones. But, if you actually want to replace the sequences with something shorter, you have a much more complex jobs. \n\n[^sp6ak53orj]: Notably there are some competitions that might be more worthwhile. If I ended up deciding to put 100 hours into competitive deliberate practice in hopes of gaining a deeper appreciation of The Human Scale, I'd probably try something like Math Olympiad. \n\n[^v9ndih14kxf]: A woman I know points out that this may be a fairly sex-differentiated experience.  \n\n[^63f4qdbwje4]: A friend told me of a time he was talking excitedly with a woman – a concert violinist – about Deliberate Practice, and how it was verified and had important properties that seemed to generalize across domains and it was an interesting scientific achievement. The woman was really excited. My friend spelled out how it required tight feedback loops, practicing at the edge of your capability, and how people typically could do ~four hours of it per day.Eventually the concert violinist said \"wait, so, like, you mean, like 'practice'?\" \n\n[^us0xodq1wl9]: Here I am making an explicit bet about how fast LLM progress will be, I realize they can't do this well right now. \n\n[^lve9lu8rz7g]: This actually maybe the earliest motivating case for this blogpost. \n\n[^du2nkzhc2o4]: I'm in the process of refactoring my thought process to be more structured, while preserving what's important to me about narrative. \n\n[^l2jr26t4wz]: Epistemic status: I tried it like 2-3 times and it seemed to basically work. \n\n[^bifpgnh56lq]: And I think also the reason Eliezer writes Shut up and do the \"Impossible\"",
      "plaintextDescription": "A fool learns from their own mistakes\nThe wise learn from the mistakes of others.\n\n– Otto von Bismarck \n\n \n\nA problem as old as time: The youth won't listen to your hard-earned wisdom. \n\nThis post is about learning to listen to, and communicate wisdom. It is very long – I considered breaking it up into a sequence, but, each piece felt necessary. I recommend reading slowly and taking breaks.\n\nTo begin, here are three illustrative vignettes:\n\n\nThe burnt out grad student\nYou warn the young grad student \"pace yourself, or you'll burn out.\" The grad student hears \"pace yourself, or you'll be kinda tired and unproductive for like a week.\" They're excited about their work, and/or have internalized authority figures yelling at them if they aren't giving their all. \n\nThey don't pace themselves. They burn out.\n\n\nThe oblivious founder\nThe young startup/nonprofit founder says \"We're going to solve problem X!\". You say \"oh, uh you know X is real difficult? Like, a lot of talented people tried to solve X and it turned out to be messy and intractable. Solving X is important but I think you need to come at it from a pretty different angle, or have a specific story for why your thing is going to work.\"\n\nThey hear \"A bunch of people just didn't really try that hard.\" If you follow it up with \"Look man I really want you to succeed here but I think there are some specific reasons Y that X is hard. And I'd love it if you stood on the shoulders of giants instead of adding another corpse to the pile of people who didn't even make it past the first ring of challenges.\" \n\nThey hear \"okay, we need to spend some time thinking specifically about Y, but our plan is still basically right and we can mostly plow ahead.\" (Also they think your explanation of Y wasn't very good and probably you're actually just an idiot who doesn't really understand Xs or Ys).\n\nA year later they write a blog post saying \"We tried to fix X, alas it was hard\", which does not contribute particularly interesting new know",
      "wordCount": 12587
    },
    "tags": [
      {
        "_id": "grDZHm8y34H9C7uui",
        "name": "Tacit Knowledge",
        "slug": "tacit-knowledge"
      },
      {
        "_id": "MwTjpRZqovTcuYuKt",
        "name": "Wisdom",
        "slug": "wisdom-2"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "pudQtkre7f9GLmb2b",
    "title": "The 2023 LessWrong Review: The Basic Ask",
    "slug": "the-2023-lesswrong-review-the-basic-ask",
    "url": null,
    "baseScore": 77,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 25,
    "createdAt": null,
    "postedAt": "2024-12-04T19:52:40.435Z",
    "contents": {
      "markdown": "Each December, the LessWrong community reflects on the best blogposts of yesteryear, to decide which posts stood the tests of time. \n\nIn this post, I aim to:\n\n*   Explain some subtleties of what I'm hoping we get out of the Annual Review\n*   Make some explicit asks (even of busy people), which I think have reasonable cost/benefit ratios.\n*   Describe the new UI features. \n\nThe most important changes are:\n\n*   Reviews are much more prominent than they used to be\n*   You can nominate posts from elsewhere on the internet using the Linkpost Importer. (Use this to nominate posts that contribute to the overall LessWrong or Alignment Forum conversation). It can import PDFs from arXiV, and blogposts from most urls.\n\nFor the Nomination Voting Phase, the main ask I'm making is: \"spend ~30 minutes casting nomination votes, and write 2 short reviews explaining how you got value from the most important posts.\"\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c29f035211a9bb1df12028834adbfc6e483c1905c8ae5bfb.png)\n\nThe Review is a complex endeavor, serving many purposes. \n\nMost people understand the *overt purpose***,** which is: **\"assemble a list of the best posts from 1-2 years ago. Highlight those posts at the top of LessWrong and the Alignment Forum. Help new or occasional readers keep up with important concepts.\"**\n\nI think most people have *some* sense of the secondary purpose, which is **\"shape the incentives of LessWrong to reward more timeless and important content.\" **\n\nThese are good things. But they're only ~half of the purpose of the Annual Review, according to me. The voting on the top posts is the *stakes* of the Review, but what I'm most excited about is the *side effect of those stakes.*\n\nThe side effect is: \n\nBy voting and reviewing and arguing over two months, the LessWrong community becomes more of a place that engages deeply with multifaceted, longterm conversations, reflecting on how ideas fit together, synthesizing a bigger picture understanding. And in particular, they do this *as a group*, with a kind of collective orienting process.\n\nThe \"side effect\" is sort of the main point, but you can't legibly build a system to incentive people to \"synthesize a deep understanding.\" Ultimately, they either do it, or they don't.\n\nEvery November, I wonder if this is all actually worth it. We could just have everyone do the initial vote, spend about a week on it, and them call it a day. But every year, once I get started reviewing posts, I find the process quite rewarding on its own merits. The Review helps me with my big picture thinking. And, I believe, the fact that many of LessWrong members participate creates something deeper than the (naive[^2rrcimdo2uh]) sum of it's parts.\n\nThe Ask\n=======\n\nIf you're the sort of longterm member whose judgment would be valuable, but, because you're a smart person with good judgement, you're busy... here is what I ask:\n\nFirst, do some minimal actions to contribute your share of judgment for \"what were the most important, timeless posts of 2023?\". (See next section)\n\nThen, in proportion to how valuable they seem, spend at least some time this month reflecting...\n\n*   ...on the big picture of what intellectual progress seems important to you. Do it whatever way is most valuable to you. But, do it publicly, this month, such that it helps encourage other people to do so as well. And ideally, do it with some degree of \"looking back\" – either of your own past work and how your views have changed, or how the overall intellectual landscape has changed.\n*   ...on how you wish incentives were different on LessWrong. Write up your thoughts on this post. (I suggest including both \"what the impossible ideal\" would be, as well as some practical ideas for how to improve them on current margins)\n*   ...on how the LessWrong and X-risk communities could make *some* group epistemic progress on the longstanding questions that have been most controversial. (We won't resolve the big questions firmly, and I don't want to just rehash old arguments. But, I believe we can make some chunks of incremental progress each year, and the Review is a good time to do so)\n\nIn a future post, I'll share more models about why these are valuable, and suggestions on how to go about it.\n\nThe concrete, minimal Civic Duty actions\n----------------------------------------\n\nIt's pretty costly to declare something \"civic duty\". The LessWrong team gets to do it basically in proportion to how much people trust us and believe in our visions. \n\nHere's what I'm asking of people, to get your metaphorical[^zjcl3oabi8n] \"I voted and helped the Group Reflection Process\" sticker:\n\n<table><tbody><tr><td style=\"width:180px\"><p><strong>Phase I:&nbsp;</strong><br><strong>Nomination Voting</strong></p><p><i>2 weeks</i></p></td><td><p>We identify posts especially worthy of consideration in the review, by casting preliminary votes. Posts with 2 positive votes move into the Discussion Phase.</p><p><i>Asks: Spend ~30 minutes looking at the </i><a href=\"https://www.lesswrong.com/nominatePosts/2023\"><i>Nominate Posts</i></a><i> page and vote on ones that seem important to you.</i></p><p><i>Write 2 <strong>short</strong> reviews</i><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"3\" data-footnote-id=\"mujz6tcxc\" role=\"doc-noteref\" id=\"fnrefmujz6tcxc\"><sup><a href=\"#fnmujz6tcxc\">[3]</a></sup></span><i>&nbsp;explaining why posts were valuable.</i></p></td></tr><tr><td><p><strong>Phase II:</strong><br><strong>Discussion</strong></p><p><i>4 weeks</i></p></td><td><p>We review and debate posts. Posts that receive at least 1 written review move to the final voting phase.&nbsp;</p><p><i>Ask: Write 3 <strong>informational reviews</strong></i><span class=\"footnote-reference\" data-footnote-reference=\"\" data-footnote-index=\"4\" data-footnote-id=\"xs3wg5qte5p\" role=\"doc-noteref\" id=\"fnrefxs3wg5qte5p\"><sup><a href=\"#fnxs3wg5qte5p\">[4]</a></sup></span><i>&nbsp;that aim to convey new/non-obvious information, to help inform voters. Summarize that info in the first sentence.</i></p></td></tr><tr><td><p><strong>Phase III:</strong><br><strong>Final Voting</strong></p><p><i>2 weeks</i></p></td><td><p>We do a full voting pass, using quadratic voting. The outcome determines the Best of LessWrong results.</p><p><i>Ask: Cast a final vote on at least 6 posts.</i></p></td></tr></tbody></table>\n\nNote: Anyone can write reviews. You're eligible to vote if your account was created before January 1st of 2023. More details in the Nuts and Bolts section.\n\nWhat's new?\n===========\n\nIn addition to general simplifying the UI, here are some major updates this year:\n\n*   **Reviews get more prominence.** Reviews with 10+ karma are now displayed on the [Best of LessWrong](https://www.lesswrong.com/bestoflesswrong) page, and on spotlights at the top the [LessWrong](https://www.lesswrong.com/) and [Alignment Forum](https://www.alignmentforum.org/) homepage.\n*   **Linkpost Submission UI.** It's now much easier to submit posts from other corners of the rationalsphere (i.e. alignment-relevant papers, Astral Codex Ten posts, etc).\n*   **Streamlined Nominations Page.** See all posts you voted on, commented on, or read.\n*   **Improved Best of LessWrong Page.** If you want to browse the winners of previous Review Votes, it's not easier to find posts relevant to your interests (previously you only had the title and illustrations to go off – now you can read a short summary of each post)\n*   **Voting power scales smoothly.** Previously, 1000+ karma users got 3x the voting power. This time there is a smoother curve with exponential decay.\n*   ***\\[Coming Soon\\]*** **Best of Alignment Forum Page.** Make it more straightforward to see the best Technical AI Safety work.\n\nI will likely attempt to build some additional tools for the Discussion phase, but haven't yet made commitments about that. (I'm particularly interested in trying to build \"power tools for using the Review to help you think big picture,\" such as making it easier to briefly skim lots of posts and remember the key points of each one.)\n\nReviews are more prominent\n==========================\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/1cd601965bf5786d9be813d9af4f08bc815dc0c1b2ebde69.png)\n\nI think the biggest problem with past Annual Reviews is there wasn't that great an incentive to write thoughtful, effortful reviews. We patched this by manually paying out prizes, but I'd like the system to more naturally encourage it.\n\nAn issue is that reviews kinda disappear-into-the-void. People might pay a bit of attention when you first write them, but years later when people get linked the post, they won't necessarily see your extensive critique.\n\nNow reviews with 10+ karma will appear wherever Best of LessWrong Spotlights appear (including on the [/bestoflesswrong](https://lesswrong.com/bestoflesswrong) page, and at the top of the home page[^qipuq3vbjhi]). So if a post was generally upvoted, but, you wrote a review highlighting an important flaw, then your comment will be listed along side it if your review gets upvoted.\n\nThis is meant to incentivize both writing and voting on reviews.\n\n(Corollary: Try to convey the most important information in the first sentence, so that people skimming the /bestof page can see it without having to click-to-expand)\n\n**Possible addition: Converting old \"Review-like\" comments. **\n\nSome posts received a fairly in-depth \"review\" like comment back when it was first published. I think it's probably reasonable to highlight those as well. I haven't yet worked out a principled way of converting them, the but the LessWrong team might manually convert some, and I meanwhile encourage people to reply to older comments that seem like a good fit to help us notice them.\n\nLinkpost Submission UI\n======================\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/8a11ee4e5275338ce113f7f2d6446a4971ecbb2f2b231496.png)\n\nA lot of the major conversations on LessWrong partially happen in other places. \n\nScott Alexander doesn't crosspost all [Astral Codex Ten](https://astralcodexten.substack.com/) posts, but it seems like important ones should get featured here. And there are various other bloggers in that reference class where, a discussion of the latest on coordination, decision-theory, or rationality training would be incomplete without them.\n\nIn particular, a lot of AI Existential Safety research is done in places other than LessWrong. I think it's important for LessWrong and the Alignment Forum to be orienting to the entire literature, not just the parts that were shaped like blogposts and posted to LW/AIAF. In the broader world, some alignment-ish research is deeply important, some is irrelevant, and some is mostly some kind of political statement. Having a conversation about all of that seems worthwhile.\n\nTo submit a linkpost, you enter a URL on the Nominations page, and it'll automatically import the post, attempt to infer the post's title and date of publication. \n\nIt'll be saved as a draft, with a rough attempt at importing the page's content, which you'll need to clean up. \n\nSo: I highly encourage you to submit all writing that you think has been important to the progress of rationality, AI alignment, or other important threads from the rationalsphere.\n\nYou'll need to write a short review explaining why the post is relevant. \n\nBy default the linkposts will list you as a coauthor (so you can edit to have useful highlights), but the LessWrong team will review linkposts as they're submitted and (in some cases) attempt to connect them to the correct author.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f3c5fdbf9fc21886abb308bdeb2f8e1008ad256c4ea43088.png)\n\nNominations Page\n================\n\nWe've consolidated various tools to help you find posts worth nominating into the Nominations Page. Previously we showed people posts-they-voted on, but not everyone votes that much. So now we also include posts you commented on, or read. (In addition to providing the new \"submit linkpost\" UI)\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2c73a3adc808e46576b5dffc192ea377442d3b68d4f5f67f.png)\n\nImproved Best of LessWrong browsing\n-----------------------------------\n\nYou might recall the [Best of LessWrong](https://www.lesswrong.com/bestoflesswrong) page looking like this:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/4727d268d157bd9587f129fcd124c6286f36aeac478bc410.png)\n\nSome folk complained that this felt very \"judge a book by it's cover\" – for clicking on posts all you had were the titles, authors and images to go off of. This felt a bit off for a site encouraging deep intellectual engagement, especially on a page celebrating posts we'd especially deeply-intellectually-engaged-with.\n\nWe shipped some updates such that, if below the \"book covers\", you'll see a nicer table-of-contents:\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7f90c9ee19144a0c315abac3ce917accd91fd0fee9038a65.png)\n\nSliding Scale of Voting Power\n=============================\n\nThe past few years, everyone with 2+ year old accounts could vote in the Review, but the vote weight of people with 1000 karma was counted more strongly (3x).\n\nI haven't been happy with the arbitrary cliff here. It gives *more* power than I really wanted to allocate to people with 1000 karma, and not enough weight to people who have been around much longer and have demonstrated good judgment. But, karma is still a pretty messy indicator, so I don't want to give too *much* power to high karma users either.\n\nI'm still[^9bhytyc8w2g] ironing out the details, but I'm planning a ~log scale that looks something like:\n\n    0x: 0 karma\n    1x: 500 karma\n    2x: 1000 karma\n    3x: 2000 karma\n    4x: 4000 karma\n    ...etc\n\n* * *\n\nNuts and Bolts: How does the review work?\n=========================================\n\nPhase 1: Preliminary Voting\n---------------------------\n\nTo nominate a post, cast a preliminary vote for it. Eligible voters will see this UI:\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/v1672905457/mirroredImages/qCc7tm29Guhz6mtf7/d0fpa6iead1yz8j7y10n.png)\n\nIf you think a post was an important intellectual contribution, you can cast a vote indicating roughly how important it was. For some rough guidance:\n\n*   A vote of 1 means “it was good.”\n*   A vote of 4 means “it was quite important”.\n*   A vote of 9 means it was \"a crucial piece of intellectual progress.\"\n\nVotes cost quadratic points – a vote strength of \"1\" costs 1 point. A vote of strength 4 costs 10 points. A vote of strength 9 costs 45. If you spend more than 500 points, your votes will be scaled down proportionately.\n\nUse the [Nominate Posts](/nominatePosts/2023?tab=votes) page to find posts to vote on. \n\nPosts that get at least one positive vote go to the [Voting Dashboard](https://lesswrong.com/reviewVoting/2022), where other users can vote on it. You’re encouraged to give at least a rough vote based on what you remember from last year. It's okay (encouraged!) to change your mind later. \n\nPosts with at least 2 positive votes will move on to the Discussion Phase. \n\n**Writing a short review**\n\nIf you feel a post was important, you’re also encouraged to write up at least a short review of it saying what stands out about the post and why it matters. (You’re welcome to write multiple reviews of a post, if you want to start by jotting down your quick impressions, and later review it in more detail)\n\nPosts with at least one review get sorted to the top of the [list of posts to vote on](https://www.lesswrong.com/reviewVoting), so if you'd like a post to get more attention it's helpful to review it.\n\n**Why preliminary voting? Why two voting phases?**\n\nEach year, more posts get written on LessWrong. The first Review of 2018 considered 1,500 posts. In 2021, there were 4,250. Processing that many posts is a lot of work. \n\nPreliminary voting is designed to help handle the increased number of posts. Instead of simply nominating posts, we start directly with a vote. Those preliminary votes will then be published, and only posts that at least two people voted on go to the next round.\n\nIn the review phase this allows individual site members to notice if something seems particularly inaccurate in its placing. If you think a post was inaccurately ranked low, you can write a positive review arguing it should be higher, which other people can take into account for the final vote. Posts which received lots of middling votes can get deprioritized in the review phase, allowing us to focus on the conversations that are most likely to matter for the final result.\n\nPhase 2: Discussion\n-------------------\n\nThe second phase is a month long, and focuses entirely on writing reviews. Reviews are special comments that evaluate a post. Good questions to answer in a review include:\n\n*   What does this post add to the conversation?\n*   How did this post affect you, your thinking, and your actions?\n*   Does it make accurate claims? Does it carve reality at the joints? How do you know?\n*   Is there a subclaim of this post that you can test?\n*   What followup work would you like to see building on this post?\n\nIn the discussion phase, aim for reviews that *somehow give a voter more information.* It's not that useful to say \"this post is great/overrated.\" It's more useful to say \"I link people to this post a lot\" or \"this post seemed to cause a lot of misunderstandings.\" \n\nBut it's even more useful to say \"I've linked this to ~7 people and it helped them understand X\", or \"This post helped me understand Y, which changed my plans in Z fashion\" or \"this post seems to cause specific misunderstanding W.\"\n\nPhase 3: Final Voting\n---------------------\n\nPosts that receive at least one review move on the Final Voting Phase. \n\nThe UI will require voters to at least briefly skim reviews before finalizing their vote for each post, so arguments about each post can be considered. \n\nAs in previous years, we'll publish the voting results for users with 1000+ karma, as well as all users. The LessWrong moderation team will take the voting results as a strong indicator of which posts to include in the Best of 2023, although we reserve some right to make editorial judgments.\n\n**Go to the** [**Nomination Page**](/nominatePosts/2023?tab=votes) **to get started!**\n\n[^2rrcimdo2uh]: Oliver Habryka insisted that things are almost never greater than the sum of their parts Because Reductionism and that I should add this disclaimer. \n\n[^zjcl3oabi8n]: Maybe also literal but I haven't done the UI design yet. \n\n[^mujz6tcxc]: In previous years, we had a distinction between \"nomination\" comments and \"review\" comments. I streamlined them into a single type for the 2020 Review, although I'm not sure if that was the right call. Next year I may revert to distinguishing them more. \n\n[^xs3wg5qte5p]: These don't have to be long, but aim to either a) highlight pieces within the post you think a cursory voter would most benefit from being reminded of, b) note the specific ways it has helped you, c) share things you've learned since writing the post, or d) note your biggest disagreement with the post. \n\n[^qipuq3vbjhi]: Awkwardly, during Review Season we actually hide the spotlights to make room for the complex Review UI on LessWrong.com, but, you can see how it'd look on AlignmentForum.org. Note that not all spotlights had reviews with 10+ karma, and on AlignmentForum it shows the Ω Karma (which is less than the LessWrong karma) \n\n[^9bhytyc8w2g]: It'll be finalized by the end of the Nomination Voting phase.",
      "plaintextDescription": "Each December, the LessWrong community reflects on the best blogposts of yesteryear, to decide which posts stood the tests of time. \n\nIn this post, I aim to:\n\n * Explain some subtleties of what I'm hoping we get out of the Annual Review\n * Make some explicit asks (even of busy people), which I think have reasonable cost/benefit ratios.\n * Describe the new UI features. \n\nThe most important changes are:\n\n * Reviews are much more prominent than they used to be\n * You can nominate posts from elsewhere on the internet using the Linkpost Importer. (Use this to nominate posts that contribute to the overall LessWrong or Alignment Forum conversation). It can import PDFs from arXiV, and blogposts from most urls.\n\nFor the Nomination Voting Phase, the main ask I'm making is: \"spend ~30 minutes casting nomination votes, and write 2 short reviews explaining how you got value from the most important posts.\"\n\n \n\nThe Review is a complex endeavor, serving many purposes. \n\nMost people understand the overt purpose, which is: \"assemble a list of the best posts from 1-2 years ago. Highlight those posts at the top of LessWrong and the Alignment Forum. Help new or occasional readers keep up with important concepts.\"\n\nI think most people have some sense of the secondary purpose, which is \"shape the incentives of LessWrong to reward more timeless and important content.\" \n\nThese are good things. But they're only ~half of the purpose of the Annual Review, according to me. The voting on the top posts is the stakes of the Review, but what I'm most excited about is the side effect of those stakes.\n\nThe side effect is: \n\nBy voting and reviewing and arguing over two months, the LessWrong community becomes more of a place that engages deeply with multifaceted, longterm conversations, reflecting on how ideas fit together, synthesizing a bigger picture understanding. And in particular, they do this as a group, with a kind of collective orienting process.\n\nThe \"side effect\" is sort of the main point, b",
      "wordCount": 2710
    },
    "tags": [
      {
        "_id": "2dz4FEXCdbk2BwDpc",
        "name": "LessWrong Review",
        "slug": "lesswrong-review"
      },
      {
        "_id": "MfpEPj6kJneT9gWT6",
        "name": "Site Meta",
        "slug": "site-meta"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "sZvMLWrWomN28uWA9",
    "title": "JargonBot Beta Test",
    "slug": "jargonbot-beta-test",
    "url": null,
    "baseScore": 84,
    "voteCount": 38,
    "viewCount": null,
    "commentCount": 55,
    "createdAt": null,
    "postedAt": "2024-11-01T01:05:26.552Z",
    "contents": {
      "markdown": "We've just launched a new experimental feature: \"Automated Jargon Glossaries.\" If it goes well, it may pave the way for things like LaTeX hoverovers and other nice things.\n\nWhenever an author with 100+ karma ~saves a draft of a post~[^viz5jugo1rr] or presses the \"Generate Terms\" button, our database queries a language model to:\n\n*   Identify terms that readers might not know.\n*   Write a first draft of an explanation of that term\n*   Make a guess as to which terms are useful enough to include.\n\nBy default, explanations are not shown to readers. Authors get to manually approve term/explanations that they like, or edit them. Authors will see a UI looking like this, allowing them to enable terms so readers see them. They can also edit them (by clicking on the term).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ba34ebac976bf76847d802fea090d3f7ded9ae0b0895a7ed.png)\n\nMeanwhile, here's a demo of what readers might see, from the [Infra-Bayesian physicalism](/posts/gHgs2e2J5azvGFatb/infra-bayesian-physicalism-a-formal-theory-of-naturalized) post.[^esbt12vjktm]\n\n> **TLDR:** We present a new formal decision theory that realizes [naturalized induction](/tag/naturalized-induction). Our agents reason in terms of infra-Bayesian hypotheses, the domain of which is the cartesian product of computations and physical states, where the ontology of \"physical states\" may vary from one hypothesis to another. The key mathematical building block is the \"bridge transform\", which, given such a hypothesis, extends its domain to \"physically manifest facts about computations\". Roughly speaking, the bridge transforms determines which computations are executed by the physical universe. In particular, this allows \"locating the agent in the universe\" by determining on which inputs its own source is executed.\n> \n> 0\\. Background\n> ==============\n> \n> The \"standard model\" of ideal agency is Bayesian reinforcement learning, and more specifically, [AIXI](https://arxiv.org/abs/cs/0004001). We [challenged](/s/CmrW8fCmSLK7E25sa) this model before due to its problems with non-realizability, suggesting infra-Bayesianism as an alternative. Both formalisms assume the \"cartesian cybernetic framework\", in which (i) the universe is crisply divided into \"agent\" and \"environment\" and (ii) the two parts interact solely via the agent producing actions which influence the environment and the environment producing observations for the agent. This is already somewhat objectionable on the grounds that this division is not a clearly well-defined property of the physical universe. Moreover, once we examine the structure of the hypothesis such an agent is expected to learn (at least naively), we run into some concrete problems.\n\nManaging Slop: Author Approval & Opt In\n=======================================\n\nI take pretty seriously the worry that LessWrong will become filled with AI slop, and that people will learn to tune out UI features built around it. Longterm, as AI gets good enough to *not* be slop, I'm even *more* worried, since then it might get things subtly wrong and it'd be really embarrassing if the AI Alignment discourse center plugged AIs into its group cognition and then didn't notice subtle errors.\n\nThese problems both seem tractable to me to deal with, but do require a bunch of effort and care. \n\nFor now: we've tried to tune the generation to minimize annoying \"false positives\" (terms which are too basic or sufficiently obvious from context) for authors or readers, while setting things up so it's possible to notice \"false negatives\" (perfectly good terms that the system rejected).\n\nThe current system is that authors have:\n\n*   **Shortlist of \"probably good\" terms to review .** By the time they're done editing a post, authors should be presented with a short list of terms, that (we hope) are mostly useful and accurate enough to be worth enabling, without much overhead.\n*   **Hidden-but-accessible \"less good\" terms.** Authors also have access to additional terms that are *less* likely to be useful, which are hidden by default (even to the authors, unless they click \"show hidden\" in their glossary editor).\n*   **Prompt Tuning.** Authors can edit the glossary prompt to fit their preferred style. (Currently, this prompt editing isn't saved between page reloads, but longterm we'd likely add some kind of Prompt Library)\n\nMeanwhile, readers experience:\n\n*   *Default to \"author approved.\"*  Only see high signal glossary terms.\n*   *Default to \"only highlight each term once.\"*  The first time you see a term in a post, it'll show up as slightly-grey, to indicate you can hoverover it. But subsequent instances of that term won't be highlighted or have hoverovers.\n*   *Default to \"unclickable tooltip,\" click to keep it open.* If you click on a term, the hoverover will stay open instead of disappearing when you move your mouse away (or over it).\n*   *Know who edited an explanation.* See if a term was \"AI generated\", \"Human generated\" or \"AI and human edited\".\n*   *Opt into \"Highlight All.\"* At the top-right of a post, if there are any approved terms, you'll see a glossary. If you click on it, it'll pin the glossary in place, and switch to highlighting each term every time it appears, so you can skim the post and still get a sense of what technical terms mean if you start diving into a section in the middle.  (There is also a hotkey for this: opt/alt + shift + J.)\n*   *Opt into \"Show Unapproved Terms.\"* On a given post, you can toggle \"show all terms.\" It'll come with the warning: \"Enable AI slop that the author doesn't necessarily endorse.\"  (There is also a hotkey for this: opt/alt + shift + G.  This works even if there aren't any approved terms for a post, which is a case where the entire glossary is hidden by default.)\n\nI'm not sure whether it's correct to let most users see the \"hidden potential-slop\", but I'm somewhat worried that authors won't actually approve enough terms on the margin for more intermediate-level readers. It seems okay to me to let readers opt-in, but, I'm interested in how everyone feels about that.\n\nThe longterm vision\n===================\n\nThe Lightcone team has different visions about whether/how to leverage LLMs on LessWrong. [Speaking only for myself](/posts/zjnJ6QJnpcpYbkYdj/speaking-for-myself-re-how-the-lw2-0-team-communicates), here are some things I'm (cautiously) excited about:\n\nAutomated \"adjust to reader level.\" \n------------------------------------\n\nReaders who are completely new to LessWrong might see more basic terms highlighted. We've tried to tune the system so it doesn't, by default, explain words like 'bayesian' since it'd be annoying to longterm readers, but newcomers might actively want those.\n\nPeople who are new to the field of Machine Learning might want to know what a ReLU is. Experienced ML people probably don't care.\n\nI think the mature version of this a) makes a reasonable guess about what terms you'll want highlighted by default, b) lets you configure it yourself.\n\nIf you're reading a highly technical post in a domain you don't know, eventually we might want to have an optional \"explain like I'm 12\" button at the top, that takes all the author-approved-terms and assembles them into an introduction, that gives you background context on what this field and author are trying to accomplish, before diving into the cutting-edge details.\n\nThe 0-1 second level\n--------------------\n\nSome other LW team members are less into JargonBot, because they were already having a pretty fine time asking LLMs \"hey what's this word mean?\" while reading dense posts. I'm not satisfied with that, because I think there's a pretty big difference between \"actions that take 5-10 seconds\" and \"actions that take 0-1 second\". \n\nActions in the 0-1 second zone can be a first-class part of my exobrain – if I see something I don't know, I personally want to briefly hover over it, get a sense of it, and then quickly move back to whatever other sentence I was reading.\n\nThe 0-1 second level is also, correspondingly, more scary, from the standpoint of 'integrating AI into your thought process.' I don't currently feel worried about it for JargonBot in particular (in particular since it warns when a thing is AI generated).\n\nI do feel much more worried about it for *writing* rather than reading, since things like \"autocomplete\" more actively insert themselves into your thinking loop. I'm interested in takes on this (both for JargonBot and potential future tools)\n\nLaTeX\n-----\n\nThis started with a vision for \"hoverovers for LaTeX\", such that you could easily remember what each term in an equation means, and what each overall chunk of the equation represents. I'm pretty excited for a LessWrong that actively helps you think through complex, technical concepts.\n\nCurating technical posts\n------------------------\n\nCurrently, posts are more likely to get curated if they are easier to read – simply because easier to read things get read more. Periodically a technical post seems particularly important, and I sit down and put in more effort to slog through it and make sure I understand it so I can write a real curation notice, but it's like a 5-10 hour job for me. (Some other LW staff find it easier, but I think even the more technically literate staff curate fewer technical things on the margin)\n\nI'm excited for a world where the LW ecosystem has an easier time rewarding dense, technical work, which turns abstract concepts into engineering powertools. I'm hoping JargonBot both makes it easier for me to read and curate things, as well as easier for readers to read them (we think of ourselves as having some kinda \"budget\" for curating hard-to-read things, since most of the 30,000 people on the curation mailing list probably wouldn't actually get much out of them).\n\nHigher level distillation\n-------------------------\n\nRight now, AIs are good at explaining simple things, and not very good at thinking about how large concepts fit together. It currently feels like o1 is juuuuust on the edge of being able to do a good job with this. \n\nIt's plausible than in ~6 months the tools will naturally be good enough (and we'll have figured out how to leverage them into good UI) that in addition to individual terms, AI tools can assist with understanding the higher-level bits of posts and longterm research agendas. (\"WTF is Infrabayesianism *for*, again?\")\n\nRemember AI capabilities are probably bad, actually\n---------------------------------------------------\n\nDespite all that, I do remind myself that although a lot of this is, like, objectively cool, also, man I do really wish the frontier labs would coordinate to slow down and lobby the government to help them do it. I'm really worried about how fast this is changing, and poorly I expect humanity to handle the situation.\n\nAs I've thought more about how to leverage AI tools, I've also somewhat upweighted how much I try to prioritize thinking about coordinated AI pause/slowdown, so that my brain doesn't naturally drift towards fewer marginal thoughts about that.\n\nFeedback\n========\n\nWith all that in mind, I would like to hear from LW users:\n\n*   Is this useful, as a reader?\n*   Are you seeing terms that feel too basic to be worth including?\n*   Are you not seeing enough terms that the system is skipping, to avoid false-positives?\n*   As an author, how do you feel about it? Does it feel more like a burdensome chore to approve glossary terms, or a cool UI opportunity?\n*   As an author, how do you feel about users being able to toggle on the \"hidden slop\" if they want? \n\nLet us know what you think!\n\n[^viz5jugo1rr]: Update: we turned this off after realizing some people might be using LW drafts to write stuff they wouldn't want being sent to Claude. Sorry about shipping this without thinking through that case. We'll think through options tomorrow. \n\n[^esbt12vjktm]: (Note: I have not yet checked with @Vanessa Kosoy if the initial definition here is accurate. On the real post, it won't appear for readers until the author deliberately enabled it. I've enabled it on this particular post, for demonstration purposes. But, warning! It might be wrong!)",
      "plaintextDescription": "We've just launched a new experimental feature: \"Automated Jargon Glossaries.\" If it goes well, it may pave the way for things like LaTeX hoverovers and other nice things.\n\nWhenever an author with 100+ karma saves a draft of a post[1] or presses the \"Generate Terms\" button, our database queries a language model to:\n\n * Identify terms that readers might not know.\n * Write a first draft of an explanation of that term\n * Make a guess as to which terms are useful enough to include.\n\nBy default, explanations are not shown to readers. Authors get to manually approve term/explanations that they like, or edit them. Authors will see a UI looking like this, allowing them to enable terms so readers see them. They can also edit them (by clicking on the term).\n\nMeanwhile, here's a demo of what readers might see, from the Infra-Bayesian physicalism post.[2]\n\n> TLDR: We present a new formal decision theory that realizes naturalized induction. Our agents reason in terms of infra-Bayesian hypotheses, the domain of which is the cartesian product of computations and physical states, where the ontology of \"physical states\" may vary from one hypothesis to another. The key mathematical building block is the \"bridge transform\", which, given such a hypothesis, extends its domain to \"physically manifest facts about computations\". Roughly speaking, the bridge transforms determines which computations are executed by the physical universe. In particular, this allows \"locating the agent in the universe\" by determining on which inputs its own source is executed.\n> \n> \n> 0. Background\n> The \"standard model\" of ideal agency is Bayesian reinforcement learning, and more specifically, AIXI. We challenged this model before due to its problems with non-realizability, suggesting infra-Bayesianism as an alternative. Both formalisms assume the \"cartesian cybernetic framework\", in which (i) the universe is crisply divided into \"agent\" and \"environment\" and (ii) the two parts interact solely via the agent p",
      "wordCount": 1848
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ynsE7aB43bRJpHeNj",
    "title": "The Cognitive Bootcamp Agreement",
    "slug": "the-cognitive-bootcamp-agreement",
    "url": null,
    "baseScore": 36,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-10-16T23:24:05.509Z",
    "contents": {
      "markdown": "The Cognitive Bootcamp is intended to be fairly intense intense. I ask attendees to read through the Workshop agreement and a) doublecheck that the workshop is right for them b) let me know how we might want to tailor it to meet their needs.\n\nThe target audience is people who:\n\n*   have decision-making power on a large project, which tackles confusing problems;\n*   are not bottlenecked on executive function;\n*   and, who don’t have a sneaking suspicion they’re at risk of burnout. \n\nThe commitment I’m asking if you come is that you spend basically the whole time either:\n\n*   aiming to think basically as hard as you can.\n*   napping / taking a walk / etc (preferably without devices).\n*   talking to me if the workshop feels off in some way.\n\nBeforehand, you’ll send me your default plan for the next ~week, month or quarter (whatever the longest timescale you plan on). You’ll work on improving this plan.\n\nYour primary goal, if you come, should be to learn at least one new skill, starting from \"it’s too cumbersome to use productively\", and aiming to reach \"Juuuust fluent enough that you can start applying it to your day job, practicing so it becomes easy.”\n\nI’ll present ~4 skills I think are valuable for solving confusing problems, and exercises that are helpful for grinding on them. You’re welcome to pick other skills, or other exercises, that you think will help you better. But, for each session, you need to clearly explain a) why I think the original exercise was important, b) why the thing you’ll do instead is better. (We’ll chat until we both understand each other and feel good about it)\n\nPreviously, I’ve found that giving people too much leeway in changing the curriculum results in the workshop losing focus, but too little leeway results in people not quite getting what they need. This is the middle ground that I've found works best.\n\nThe Goal: Solve confusing, intractable problems\n-----------------------------------------------\n\nMany of the most important problems in the world are confusing, intractable, and seem impossible. They are really important to solve anyway. \n\nBut many people who try, end up in failure modes such as…\n\n1.  **Rabbit-holing, **fixating on an approach which doesn’t work, or takes too long.\n2.  **Goodharting, **substituting an easier problem, maybe without even noticing.\n3.  **Despair. **It’s impossible. You give up.\n4.  **Zombified Agency**, mechanically execute virtuous-seeming plans, but something inside you is dead and hollow and the plan probably won’t work and maybe you’ll hurt yourself.\n\nI want you to leave the workshop with at least one new skill for solving impossible problems.\n\nThe Curriculum\n--------------\n\nThe default curriculum focuses on “making better plans” via…\n\n1.  **Generating multiple plans,** so you don’t over-anchor on your first plan.\n2.  **Tracking multiple goals**, so you don’t over-anchor on your first goal.\n3.  **Identifying your cruxes, **and confidently deciding when to pivot or persevere.\n4.  **Making cruxy predictions**, so over time you can credibly, calibrated trust your intuition (and, know the limits of where you cannot)\n\nAn optional, higher level frame is  **Fractal Strategy** *–* thinking of plans and goals at multiple levels, tracking how smaller plans fit into bigger plans, and when it’s time to move up, down or sideways in plan/goal-space.\n\nIf you feel like you roughly have all these skills, you might still want to come to the workshop to have an environment that will help you do whatever cognitive practice *you* believe in, with good form, attending carefully to each step of the process.” (I’ll want to talk to you beforehand about your plan in detail)\n\nSkills\n------\n\nThe one skill I will require everyone to attempt at least twice is  **generating metastrategies **(see below). \n\nEach of the bullets in the Curriculum section has a corresponding skill. Some particular skills that will be presented that will come up if they seem helpful to a particular student are:\n\n**Generalizing Takeaways** *, *i.e. asking “how could I have thought that sooner?”. We'll practice doing this both in-depth, and a rapid 5 minute version you can do every day.\n\n**Using externalized working memory, **both because it increases the complexity of problems you’re capable of tackling, and because it makes it easier for me to see your thought process and offer advice.\n\n**Noticing metacognition**, to identify when you’re in particular cognitive states so that you can learn about your mind and employ relevant skills/habits.\n\n**Grieving. **Sometimes, there's a better plan available, but switching to it involves letting go of some kind of psychologically loadbearing beliefs.  [Deliberate grieving](https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1) is a skill for that. (There will be an optional session for this Saturday night but it's more experimental)\n\nMultiple feedback-loops\n-----------------------\n\nI don’t have perfect feedback-loops to tell you if this workshop is working for you. So, there are four different feedback-loop types, with different tradeoffs:\n\n**Predictions.** *Guess whether a given strategy will work, then see if you were right.*\n\n**Toy Exercises** ***.*** They only vaguely resemble your real problems, but you’ll know for sure whether you got the right answer in two hours.\n\n**Big Picture Planning.** *You’ll generate at least one new plan. You won’t really know if it’s good, but a) you’ll have intuitions about it, which are at least some information. And, b) you’ll make predictions about whether it’ll seem worth having thought about in a year.*\n\n**Object-level work, in 1-hour block.** *Spend a few timeblocks doing object level work on your second likeliest plan. Each hour, you’ll make conscious choices about how to spend your time and attention. And then, reflect on whether that seemed useful. (in addition to crosstraining your skills on the practical object-level, this will help make your second-likeliest plan feel more real)*\n\n**New Cognitive Strategies.** *Over the workshop, you will hopefully be identifying new strategies for problemsolving (or, realizing more significance of strategies you were familiar with but haven't been using nearly enough)*\n\nWe'll also work on improving your daily, weekly and longer feedback loops in your real world work.\n\nThe Disclaimers (Important!)\n----------------------------\n\n### Prerequisites\n\nThis is not an entry level rationality workshop. It has several prerequisites:\n\n1.  **Executive function. **You can sit down and think about a confusing problem and not immediately bounce off. Your problem should be more like “when I sit down to plan, I don’t know what to do” than “I have trouble sitting down to plan,” or “if I make a decision, I can’t follow it through.”\n2.  **Project Ownership.** You have control over an (at least somewhat) open-ended decisionmaking process *i.e. you get at least some leeway to set priorities at your day job, or you have a lot of slack for ambitious hobbies.*. A primary skill at this workshop is learning when to pivot. You need to be capable of deciding when you’re pivoting. When you leave the workshop, you have a project you expect to be applying the techniques to on a daily basis.\n3.  **Self awareness. **You can notice when pushing yourself hard is bad for you, instead of good for you. You have at least some awareness of when things are going subtly wrong and you need to slow down. I'll do my best to help you notice this sort of thing, but there’s a limit to how much I can help.\n\nIf you are at risk of burnout (if you’ve recently worked much harder than usual, or a nagging voice inside is worried about spending a weekend doing very intense thinking) I recommend you do not attend right now. You can register your interest for future workshops though.\n\n### This will be very meta\n\nWe are going to think about thinking.\n\nWe are going to apply feedback-loops to our feedback-loops.\n\nWhile we’re doing that, I will be thinking about thinking about you thinking about those things (You don’t have to do that, just me).\n\nMeta-level optimization is the mechanism by which, maybe, I expect people to get compounding returns on thinking, and there is a real possibility that this can be leveraged into dramatically better plans.\n\nWell designed feedback loops are the grounding mechanism to check whether we’re doing masturbatory meta, vs useful meta. [\\[4\\]](https://www.lesswrong.com/posts/ynsE7aB43bRJpHeNj/the-cognitive-bootcamp-agreement#fn35144wo22la)\n\nI think there are basically two flavors of \"I feel like this meta sucks\". One is that you've lost track of your goals, you've spent more time meta-optimizing than makes sense, and your subconscious is (correctly) flagging that it's time to get back to work. Another is \"it feels overwhelming, like you can't track what's going on.\"\n\nI think it can be correct (nearterm) to stop if you're overwhelmed, but one of the skills the workshop is trying to impart is the ability to navigate complex problems without getting overwhelmed (complex metacognition included).\n\nIf you are feeling disoriented or annoyed or have a nagging feeling the current amount of meta won’t help:\n\n1.  First, pause (if  *I’m* the problem, say “hey Ray stop”)\n2.  Probably, go back to doing a more object level thing\n3.  But, also: consider getting out a sheet of paper / google-doc and writing things down so you can actually track what’s going on. Or, chatting with me about it and seeing if we can find a way to make it more manageable. [\\[5\\]](https://www.lesswrong.com/posts/ynsE7aB43bRJpHeNj/the-cognitive-bootcamp-agreement#fnuizz8o23a3)\n\nPart of the point of becoming fluent in “working memory extension” is that your meta processes are much easier to understand, and you can leverage them more strategically.\n\n### What's the evidence that this works?\n\nI am talking a pretty big game. I think it’s an active ingredient for me to present content confidently, and for people to lean into a mindset where they trust it’ll work, or at least is worth trying.\n\nBut an important truth is that this workshop series does not (yet) have a strong empirical track record. If the curriculum does not intuitively make sense to you, I don’t think you should particularly believe in it.\n\nI am trying to stick my neck out such that if the workshop is not working, it should be obvious (i.e. people will not be generating strategies that help), and over time if the workshop does not help, it should be clear that the people who take it don't end up standing on giant heaps of utility. And if I haven't gotten that to obviously work in ~a year, I'll give up.\n\nMeanwhile: here are some facts about past participant experience:\n\n**First Workshop 6-month followup**\n\nWhen I ran the first workshop (which I charged $200 for), I asked the 9 participants what was the most they’d have paid for the workshop. Numbers ranged from $300 to ~$2000, on average $800.\n\nWhen I asked again six months later, two “$400” people and one \"$1500\" changed their number to “$2000”.\n\n*   One of the $400 folk, because they changed their plans importantly afterwards. They weren’t sure how much credit to assign to the workshop, but it could be anywhere from $0 to $10,000, and \"$2000 felt about right.\"\n*   The other $400 person, because they gained an major insight that was still seemed important to their worldview.\n*   The $1500 person explicitly changed their plan during the workshop, and after months later it seemed like the lessons had sunk in a more, and they also upped to $2000.\n\nMeanwhile two people reported back they'd probably lowered their estimate:\n\n*   One (who originally said \"$300\" said \"My gut says $200. My brain says potentially $0 or potentially $1000.\"\n*   Another who originally said $1500 person dropped to $0 because they realized they were too burned out to get value from the workshop (and maybe it hurt them).\n\nWhen I ran the second workshop, average rating was $540, or $650 if you throw out one data point from someone (who rated it $0) who came for nonstandard reasons, and I wouldn’t have really expected to get much value out of it, if we’d talked a bit more. There was another person who also reported they were burned out [\\[6\\]](https://www.lesswrong.com/posts/ynsE7aB43bRJpHeNj/the-cognitive-bootcamp-agreement#fnoasdmzjvja8) and now wasn't a good time for the workshop, although I don't think they were actively hurt.\n\nI'm raising rates now because a) I've now run four workshops, and am more confident in the curriculum, and b) I'm also baking following coaching into the program. I think the workshop \\*can\\* get someone to the point where they can practice on their own, but realistically I think most people will benefit from an environment where the skills are solidified and habits are reinforced.\n\nI charge a fair amount of money because:\n\n*   **People thinking it’s worth paying for is a crux of mine. **If I didn’t think I could ultimately generate thousands of dollars worth of value for people, I would give up on the project. Charging money makes it harder to delude myself that I’m helping.\n*   **Lightcone needs the money. **And on the margin, we prefer to get money from people we are delivering value to. The cost roughly what we'd need for this to be sustainable.\n*   **It filters for commitment. **I want to filter for people who actually are going to  *try* to get hundreds or thousands of dollars worth of value from the workshop, and so will put more effort in.\n*   **It soft-filters for the pre-requisites. **Because the workshop isn’t a 101 workshop, I want to filter for people who have some degree of “already having your shit together”, for which “can afford to consider paying serious money for a workshop” is a proxy. (I realize that will exclude some people unnecessarily. But given the other goals, it seems like the right balance)\n\nIn general, if you didn’t feel like you got your money’s worth, I prefer to solve this by giving you some free followup coaching until you feel like you got your money’s worth.\n\nIf you earnestly tried to get a thousand dollars worth of value from the workshop, including explicitly strategizing with me on how to adapt it to you, and it didn’t really pay off and it seems like my general frame or skillset isn’t useful to you, chat with me and I’ll consider a refund.",
      "plaintextDescription": "The Cognitive Bootcamp is intended to be fairly intense intense. I ask attendees to read through the Workshop agreement and a) doublecheck that the workshop is right for them b) let me know how we might want to tailor it to meet their needs.\n\nThe target audience is people who:\n\n * have decision-making power on a large project, which tackles confusing problems;\n * are not bottlenecked on executive function;\n * and, who don’t have a sneaking suspicion they’re at risk of burnout. \n\nThe commitment I’m asking if you come is that you spend basically the whole time either:\n\n * aiming to think basically as hard as you can.\n * napping / taking a walk / etc (preferably without devices).\n * talking to me if the workshop feels off in some way.\n\nBeforehand, you’ll send me your default plan for the next ~week, month or quarter (whatever the longest timescale you plan on). You’ll work on improving this plan.\n\nYour primary goal, if you come, should be to learn at least one new skill, starting from \"it’s too cumbersome to use productively\", and aiming to reach \"Juuuust fluent enough that you can start applying it to your day job, practicing so it becomes easy.”\n\nI’ll present ~4 skills I think are valuable for solving confusing problems, and exercises that are helpful for grinding on them. You’re welcome to pick other skills, or other exercises, that you think will help you better. But, for each session, you need to clearly explain a) why I think the original exercise was important, b) why the thing you’ll do instead is better. (We’ll chat until we both understand each other and feel good about it)\n\nPreviously, I’ve found that giving people too much leeway in changing the curriculum results in the workshop losing focus, but too little leeway results in people not quite getting what they need. This is the middle ground that I've found works best.\n\n\nThe Goal: Solve confusing, intractable problems\nMany of the most important problems in the world are confusing, intractable, and seem impo",
      "wordCount": 2341
    },
    "tags": [
      {
        "_id": "DWWZwkxTJs4d5WrcX",
        "name": "Exercises / Problem-Sets",
        "slug": "exercises-problem-sets"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "StBjd8esuXzhqLGNt",
    "title": "OODA your OODA Loop",
    "slug": "ooda-your-ooda-loop",
    "url": null,
    "baseScore": 38,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2024-10-11T00:50:48.119Z",
    "contents": {
      "markdown": "An [OODA loop](https://www.lesswrong.com/posts/bh9gAZTmGyJ4j7h2L/training-regime-day-20-ooda-loop) is a repeating loop where you take in information, integrate it into your decision-making, use it to make more decisions, and then receive more information in response.\n\n“OODA” is a particular structure referring to:\n\n*   **Observe: **“what is happening in the world” (in as raw a form as you can)\n*   **Orient: **“what sorts of things are important right now? How do I organize the information I’m seeing”\n*   **Decide: **“what do I want to do next”\n*   **Act: **“Okay I’m doing it now.”\n\nYou are probably doing *some *kind of OODA-loop-like-thing already. But it’s worth reflecting on how you could do it better.\n\nFor my last workshop, partly because it was cute, and partly because it seemed like probably a good idea, I put together a worksheet for applying a deliberate, intentional OODA loop to your OODA loop. \n\nI don't actually know if this worksheet turned out to be helpful (it was one thing among many, and the person who most intentionally updated their OODA loop did so before we even got to this worksheet).\n\nBut I figured I would put it here for easy reference. \n\nI recommend copying this into a google-doc, or whatever your preferred notetaking system. \n\nThen, I recommend first skimming all the prompts, and focusing on the prompts that feel intuitively useful to you.\n\n*(If you bounce off this doc but it feels like a nearby thing would be useful, please let me know in the comments)*\n\n***Observation***\n=================\n\nEmpirically, how do you seem to observe, orient, decide, and act right now? Think of concrete specific moments when you…\n\n*   …took in information\n*   …organized or prioritized or made-sense-of that information\n*   …decided what to do\n*   …did stuff\n\nThese might come bundled together in a “loop”, or haphazardly over a long period of time. Whatever the way you actually do each of these things, write down as much concrete detail as you can about how you did it.\n\nWrite down those details for OODAing on at least two timescales (i.e. how you respond to information in the middle of the day, quickly, and how you orient to your overall life over months or years)\n\nWrite details about some short-timeframe examples of…\n-----------------------------------------------------\n\n### Observing\n\n### Orienting\n\n### Deciding \n\n### Acting\n\n*(acting has tons of detail that isn’t that useful here, but maybe focus on the particular moment you shift from deciding, to acting)*\n\nWrite details about some longer-timeframe examples of…\n------------------------------------------------------\n\n### Observing\n\n### Orienting\n\n### Deciding \n\n### Acting\n\n***Orient***\n============\n\nOrienting tends to include two aspects:\n\n*   **Organizing information into a schema.** There's a ton of info flooding into your brain every second. You need some way of conceptualizing it that fits in your working memory.\n*   **Thinking about what types of information *****matter*****,** and how it might apply towards your goals.\n\nIn this case, we’re organizing information about “how does your OODA loop work”.\n\nSome prompts:\n\n### What seems important to track about OODA loops?\n\n### Write a short 1-2 paragraphs or bullet list describing your high level OODA loop processes. \n\n(i.e consolidate observations from the previous section into a short schema that fits easily in your working memory)\n\n### How would you know if you had a good one?\n\n### What (if anything) are you dissatisfied with about your current OODA loops?\n\n### What are some ways your OODA loops could be better? (including quick fixes and deeper effortful fixes)\n\n### What’s hard or confusing about improving your OODA loop?\n\n***Deciding***\n==============\n\nOkay, now you’re going to decide whether to make any kind of intentional changes to your OODA-loop-ish processes.\n\n“I don’t want to change anything, it’s fine” is a possible option.\n\nList out some possible plans or actions you could take, to improve your observations, orienting, deciding, or acting. These can be:\n\n*   elaborate plans\n    *   “I’m going to do a quarterly review with structure X”\n*   quick actions\n    *   “I’m going to download this app, or buy a notebook”\n*   experiments to try\n    *   “I’m going to try a process for daily or weekly planning.”\n\n### List of plans or actions\n\n### For each “plan”, what’s a next-action you could take right now, to get the ball rolling on it?\n\n***Act***\n=========\n\nDo all the next actions that make sense right now. (This might be 0, 1, or several that you can fit into ~5 minutes)\n\n*(We’re only going to do quick actions for right now, but, often this part takes much longer than the other parts)*\n\n* * *\n\nLet me know in the comments if this was helpful to you (or, if particular prompts are helpful, unhelpful, or confusing)",
      "plaintextDescription": "An OODA loop is a repeating loop where you take in information, integrate it into your decision-making, use it to make more decisions, and then receive more information in response.\n\n“OODA” is a particular structure referring to:\n\n * Observe: “what is happening in the world” (in as raw a form as you can)\n * Orient: “what sorts of things are important right now? How do I organize the information I’m seeing”\n * Decide: “what do I want to do next”\n * Act: “Okay I’m doing it now.”\n\nYou are probably doing some kind of OODA-loop-like-thing already. But it’s worth reflecting on how you could do it better.\n\nFor my last workshop, partly because it was cute, and partly because it seemed like probably a good idea, I put together a worksheet for applying a deliberate, intentional OODA loop to your OODA loop. \n\nI don't actually know if this worksheet turned out to be helpful (it was one thing among many, and the person who most intentionally updated their OODA loop did so before we even got to this worksheet).\n\nBut I figured I would put it here for easy reference. \n\nI recommend copying this into a google-doc, or whatever your preferred notetaking system. \n\nThen, I recommend first skimming all the prompts, and focusing on the prompts that feel intuitively useful to you.\n\n(If you bounce off this doc but it feels like a nearby thing would be useful, please let me know in the comments)\n\n \n\n\nObservation\nEmpirically, how do you seem to observe, orient, decide, and act right now? Think of concrete specific moments when you…\n\n * …took in information\n * …organized or prioritized or made-sense-of that information\n * …decided what to do\n * …did stuff\n\nThese might come bundled together in a “loop”, or haphazardly over a long period of time. Whatever the way you actually do each of these things, write down as much concrete detail as you can about how you did it.\n\nWrite down those details for OODAing on at least two timescales (i.e. how you respond to information in the middle of the day, qui",
      "wordCount": 775
    },
    "tags": [
      {
        "_id": "DWWZwkxTJs4d5WrcX",
        "name": "Exercises / Problem-Sets",
        "slug": "exercises-problem-sets"
      },
      {
        "_id": "Ji7aqDE5fgzbCLhAG",
        "name": "OODA Loops",
        "slug": "ooda-loops"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "XHnAakZtcEktEGvYJ",
    "title": "Scaffolding for \"Noticing Metacognition\"",
    "slug": "scaffolding-for-noticing-metacognition",
    "url": null,
    "baseScore": 88,
    "voteCount": 35,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2024-10-09T17:54:13.657Z",
    "contents": {
      "markdown": "tl;dr: I have a more compelling argument for why a *generalized \"noticing metacognition\"* skill is useful, than I've previously been able to articulate. And, a simple exercise framework that will probably work decently for most people.\n\nThis is basically the parts of [Tuning your Cognitive Strategies](https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies) that were most useful to me, and hopefully a smoother onramp for people who found it confusing.\n\n* * *\n\nI'm extremely [Noticing](https://www.lesswrong.com/posts/2n83uRi36KWDC9LyK/training-regime-day-8-noticing)-pilled. \n\nI think the ability to notice subtle things in your mind, or in the world around you, is one of the foundational rationality skills. It's necessary for building [habits](https://www.lesswrong.com/posts/W5HcGywyPoDDdJtbz/trigger-action-planning)[^ixxe82kd18], for seeing clues that help you solve confusing problems, for diagnosing [how you could have thought that faster](https://www.lesswrong.com/posts/rYq6joCrZ8m62m7ej/how-could-i-have-thought-that-faster), and for generally building a mechanistic model of your mind that lets you become a power-user for your brain.\n\nDespite this, I didn't include Noticing in my recent workshops, because it takes awhile to pay off, and many people initially find it disorienting, or overwhelming, or pointless. Instead of teaching people to \"notice their metacognition\", I just tell them to do \"Livelogging\", where you write out your train-of-thought in a google doc while you solve a difficult puzzle, and then afterwards can ask questions like \"how could I have thought-those-thoughts faster\" and \"how can I generalize lessons from that?\", while reviewing the Livelog record. \n\nLivelogging is maybe 40% as good as generalized Noticing in the workshop context, but it's enough to get the job done, and requires little training.\n\nBut, during last weekend's workshop, I ended up giving a spontaneous lecture about Noticing which I think help motivate it much more clearly. So, if you've been skeptical why you should invest in noticing subtle things, I'm curious if this changes your mind.\n\nSeven cognitive states worth noticing\n=====================================\n\nUltimately, I think it's worthwhile to gain \"generalized noticing\", where you have a passive ability to:\n\n*   a) notice a wide variety of subtle phenomena,\n*   and b) develop a taste for when those phenomena are important enough to actually promote to your attention.\n\nThis takes awhile to learn. But there's a particular *cluster* of cognitive states I find worth paying attention to, which make sense as a unit. And it should make sense why you'd want to notice all of them and do stuff with them.\n\nMoreover, it's relatively straightforward to practice them in the context of solving puzzles, because most of the states reliably turn up. (Not only that, but they often turn up in the same order, so you can start by practicing noticing just one thing at a time)\n\nThis works best with some kind of puzzle that you can interact with, which has the possibility to surprise you. (I recommend the videogame [Baba is You](https://store.steampowered.com/app/736260/Baba_Is_You/) for a giant source of well designed puzzles)\n\nSome states that stand out as particularly worth noticing are:\n\n1.  Surprise\n2.  Confusion\n3.  \"Nagging feeling you're off-track.\"\n4.  Feeling Stuck\n5.  Disengaged / bored\n6.  \"Huh, that's a little odd\"\n7.  Having an insight (or, suddenly becoming \"unstuck\")\n\nEach of those states has some corresponding skills, that are often useful to employ when you notice yourself in said state. I'll go over each state – both what it means in a philosophical sense and what sort of feelings might go along with it, that you might look for.[^5xoi9v4mxkf]\n\nFor each state, I'll list the techniques I currently apply. I'm erring on the side of being somewhat opinionated because I think people often bounce off and do weak/lame versions of the skills that aren't that helpful. But, you can substitute your own techniques, and it's hopefully intuitive why you'd want *some* kind of similar technique in this slot. (If it's not intuitive, argue with me about it in the comments!)\n\n**Surprise**\n------------\n\nBeing surprised means \"something happened, which you assigned low probability to.\" There are a few reasons that might have happened. \n\nMaybe you had an implicit (wrong) model of the situation, which didn't remotely consider the possibility of this event. Now that the event's happened, maybe a new model immediately occurred to you, which fully explains the surprise. Or, maybe you realize you don't have a model at all (and, now you are *confused*. See below) \n\nMaybe you already explicitly accounted for a possibility, it was just a low probability event. (i.e. if you get struck by lightning in a thunderstorm, that's not *confusing* but it's *surprising).*\n\n**How I might notice:**  I feel my eyes open wide. A jolt of adrenaline. I involuntarily say \"oh!\".\n\n**What to do:** Check if you're also feeling confused. Check if you just had an insight. Check if the surprise has implications you should propagate through your model.\n\n**Confusion**\n-------------\n\nConfusion is the state of not having a model of a situation. \n\nTwo ways to be confused include:\n\n1.  you *know* you don't have an accurate model of the situation\n2.  you *think* you have an accurate model, but are wrong.\n\nWhen you \"notice that you're confused,\" you're moving from state #2 to state #1.  \n\nConfusion is particularly subtle and hard to notice (especially compared to surprise, which is often more \"loud\"). But, often, when you are surprised, you are also confused, which is why it's useful to have the habit \"notice surprise -> check if confused.\"\n\nA related aspect is that confusion often feels murky and uncomfortable, and people tend to slide off it by default.\n\n**How I might notice:** A subtle fuzzy feeling when I think about something; I notice one thought doesn't quite connect to another thought, or that I can't spell out exactly what's going on; Thinking about it feels sluggish and I find myself bouncing off to more tractable parts of the puzzle.\n\n**What to do:** Explicitly note that you're confused, and decide what to do about it. Write down what you're confused about, as precisely as you can, or explain it to a friend or rubber duck. See if the confusion dissolves once you get explicit about it. Consider doing a deep dive, recursing to make \"resolve this confusion\" the sub-puzzle you are now explicitly focused on.\n\n**\"Nagging off-track feeling\"**\n-------------------------------\n\nYou have a niggling doubt, that maybe your current approach to solving the puzzle either won't work, or will take a really long time.\n\n**How I might notice:** I've scrunched my face muscles in a slightly disappointed frown; I feel a little squirm in your chest; I mutter out loud \"this is gonna take forever\"; My inner monologue says 'eh, I can ignore that it'll be fine' and a little quiet voice knows in it's heart that it's not fine.\"\n\n**What to do:** When you notice this feeling, you might explicitly start brainstorming alternate plans. Or, at least make a note to consider doing so if the feeling persists.\n\n**\"Being Stuck\"**\n-----------------\n\nYou haven't had a new thought in a while. Maybe you're thoughts go in loops. Maybe you haven't had any thoughts at all.\n\n**How I might notice:** I think a thought, and notice I've thought that thought twice recently; I might feel frustrated, bored or disengaged; I might just feel a blank nothingness.\n\n**What to do:** When you've been stuck awhile, I recommend [brainstorming and reviewing your *metastrategies*](https://www.lesswrong.com/posts/cbWoMepny3Jo9XqEr/metastrategic-brainstorming-a-core-building-block-skill). That is, don't directly try to directly solve the problem. Instead, brainstorm approaches that might help you solve the problem (including cognitive steps like \"break it into smaller pieces\", tools like \"get a pen and paper\" or physiological things like \"get a drink of water\"). \n\nDon't stop at just a few metastrategies (unless you've found one that feels really promising. See also: \"having an insight\"). Try to generate a lot, to force yourself past the initial obvious thing. Try to generate at least one *new* one you haven't thought about recently.\n\n**Bored / disengaged**\n----------------------\n\nYou don't really care about what's going on. Maybe you're explicitly stuck, maybe you're being carried by momentum and you still have some traction but nothing feels at stake and it doesn't feel worthwhile.\n\n**How I might notice:** I have trouble focusing; My eyes keep sliding around but not sticking to anything; I suddenly find myself on facebook, mysteriously. I'm tired. I'm yawning. I feel slightly annoyed.\n\n**What to do:** In this state, you might ask \"what do I want right now? what do I care about? what are my goals?\" Probably take at least a short break (or perhaps, be done for the day, coming back tomorrow, or maybe just deciding this isn't worth it). \n\nAsk yourself why you originally started on the puzzle – if you still believe in it, think through what originally felt motivating and see if you can reconnect with it. \n\n**\"Huh, odd.\"**\n---------------\n\nThis is technically just a variant of \"surprise\" or \"confusion\", but I list it here separately because it feels phenomenologically distinct when it happens to me.\n\nOften I notice a little quirk of a puzzle videogame, or a codebase I'm debugging, or a social situation, and then awhile later I'll realize this was an important clue [I could have picked up on](https://www.lesswrong.com/posts/2x7fwbwb35sG8QmEt/sunset-at-noon). \n\nThis could be confusion (\"huh, that piece doesn't quite make sense\"), or slight surprise (\"oh, that's an odd thing for the puzzle designer to have included\", \"that's an odd expression on her face.\")\n\n**How I might notice:** I blink a couple times; My face scrunches up in a way that tilts my eyebrow and compresses one side of my mouth; I say the word \"huh\", or think the thought \"weird.\"\n\n**What to do:** Real life is filled with tons of these moments, so you can't necessarily track them all. If you're in an explicitly crafted puzzle-exercise, it's almost certainly worth writing down, and ask \"what might this imply?\"\n\n**Insight**\n-----------\n\nYou have an idea! Maybe you were just stuck, and now you are unstuck. Maybe you were following a train of thought and two things clicked together and now something feels *ALIVE WITH PROMISE.*\n\nThis might turn out to be important, or it might be a false positive that doesn't pan out. Either way, congratulations – you connected some ideas together and at least generated something interesting.\n\n**How I might notice:** I jolt upright with excitement; I gasp, or say \"oh!\".\n\n**What to do:** The previous \"what to dos\" hopefully made intuitive sense, reinforcing ideas you were already familiar with. This one is a bit weirder and opinionated, but it's kinda the whole point of the essay.\n\nWhen you notice an insight, I think it's worth:\n\n1.  Making a brief note you will remember later to investigate the insight. (maybe just write an exclamation point in your notebook, or bold the sentence you just wrote)\n2.  Finish your current train of thought (you're probably in the middle of having an insight – don't want to lose it!)\n3.  When your train of thought is at a reasonable stopping point (i.e. the rate of generating new important-feeling thoughts has slowed down enough), write down your current understanding of the insight as clearly as you can.\n4.  Then, immediately try to Unpack the Insight while it's fresh in your mind (which is complicated enough to be it's own section, see below)\n\nHow it fits together\n====================\n\nBefore we get to Insight Unpacking, let's recap and clarify the frame a bit. We have a bunch of cognitive states you can notice, and habits that are useful to have when you notice them. How's this process work?\n\nNoticing Subtler Variants\n-------------------------\n\nWhen you first start training noticing, you might only be able to notice states when they're particularly \"loud\" (i.e. jolting upright in surprise, being extremely tired and realizing you're bored). It might also be too much to fit into your head, to *both* practice noticing surprise/confusion/etc *and* practice doing any particular techniques *while* trying to solve a hard puzzle.\n\nSo I recommend, when starting, to simply try solving a puzzle, and make a tallymark for each state when it comes up, and then go back to solving the puzzle.\n\nOver time, you'll learn to notice subtler variations of each state, and you might start \n\n**\"Precognition?\"**\n\nLogan Strohl [says](https://www.lesswrong.com/s/zLib3j2Fdnnx3aP3F) that eventually, you can even learn to notice the *precursor moments* to a given state, so you might start noticing it before it even happens, precognitive style. For example, before you get defensive, you might learn to notice conversational moves, or physiological cues, that are likely to make you defensive in a few seconds.\n\n I haven't experience this myself at this point, but it seems plausible and interesting if you really doubled down on the skill (I think Logan has invested massively more hours of practice into noticing than me)\n\nIntegrating \"noticing -> skill\" TAPS\n------------------------------------\n\nOnce you've gotten the hang of noticing states while puzzlesolving, you can begin to layer in explicit additional skills. You'll get better at managing your working memory such that pausing to do additional metacognition feels more natural.\n\nBuilding up to \"Generalized Noticing\"\n-------------------------------------\n\nThese seven cognitive-states aren't the only things worth noticing – there's tons of subtle stuff like defensiveness, and particular [felt senses](https://www.lesswrong.com/posts/eccTPEonRe4BAvNpD/the-felt-sense-what-why-and-how), flinching away from uncomfortable things, clinging to a particular rabbit hole or sense that you need to justify yourself to an imaginary authority figure.\n\nI think starting with these seven is reasonable because \"Puzzle Solving Videogames\" are a format that's easy to replicate them in, and is fun enough to be rewarding on it's own. But I think once you get a handle on each of them, you can hopefully start extrapolating to other experiences. (\"I feel *something* going on that's not any of those seven\")\n\nWhile you practice noticing them, you can also start to develop a feeling of when they are \"important\" and when they aren't especially. The sense of \"this emotion seems like it's *causing* a problem, or is entangled with something important\" is something I've learned to generally track whenever I notice a new micro-state that I might have otherwise let slide.\n\nThe generalized Rationality Tap is:\n\n> Notice something feels off, or feels important in some way -> Stop, Drop, and Ask Yourself *\"Should I wake up, become sapient, and do something about it?'\"*\n\n<table style=\"background-color:rgb(230, 230, 230);border-color:rgb(179, 179, 179)\"><tbody><tr><td style=\"border-color:rgb(230, 230, 230);padding:0.4em\">&nbsp;</td><td style=\"border-color:rgb(230, 230, 230);padding:0.4em\">&nbsp;</td><td style=\"border-color:rgb(230, 230, 230);padding:0.4em\">&nbsp;</td></tr><tr><td style=\"border-color:rgb(230, 230, 230);padding:0.4em\">&nbsp;</td><td style=\"background-color:rgb(230, 230, 230);border-color:rgb(230, 230, 230);padding:0.4em\"><p>Here's a cheat-sheet for the state-&gt;action combos, both to skim to get a gestalt sense of how they all fit together, and to use as a checklist if you want to do the exercise.<br>&nbsp;</p><p><strong>Surprised:</strong><i>&nbsp;</i></p><ul><li>check if I'm also confused,</li><li>or if the surprise feels like a \"clue\",</li><li>or if the surprise seems to have broader implications,</li><li>or if the surprise is a sort of insight.</li></ul><p><strong>Confused:</strong><i>&nbsp;</i></p><ul><li>Note down the confusion.<ul><li><i>Don't </i>slide off it and think \"well, that's kinda hard to think about, let's go follow this other more tractable vein of thought over there,\" at least until you've sat with it a moment.</li></ul></li><li>Try to say what I'm confused about as clearly as possible (either written down, or say out loud as if explaining to a friend).</li><li>Think about whether the confusion feels significant to the puzzle.</li><li>If it seems important, treat the confusion as a new sub-puzzle to brainstorm explicit plans to solve.</li></ul><p><strong>\"Realize you have an (implicit?) plan.\"</strong><i>&nbsp;</i></p><ul><li>If you haven't yet, get a little more explicit about what your implicit strategy is</li><li>Consider whether to brainstorm another plan or two, to avoid the trap of rabbitholing on a red-herring.</li><li>If you decide to continue with the current plan, be particularly attentive to when you are having a...</li></ul><p><strong>\"Nagging off-feeling\"</strong><i>&nbsp;</i></p><ul><li>Explicitly note \"I'm having a nagging off-track feeling\"</li><li>If it persists, consider brainstorming more plans or metastrategies, or just taking a break.</li></ul><p>\"<strong>Being Stuck\"</strong><i>&nbsp;</i></p><ul><li>Brainstorm metastrategies until you hit one that feels obviously right, or you've gotten at least 10. <i>Try to get at least one <strong>new</strong> metastrategy.</i><ul><li>(It's okay if they're stupid. Since you're stuck, it's probably better to <a href=\"https://www.lesswrong.com/posts/i42Dfoh4HtsCAfXxL/babble\">lower your filters</a>. Or at least consciously ask how high your <a href=\"https://www.lesswrong.com/posts/rYJKvagRYeDM8E9Rf/prune\">prune</a> filter should be set)</li></ul></li><li>If you get 10 metastrategies and none feel remotely promising, consider taking a break.</li></ul><p><strong>Bored / disengaged.</strong><i>&nbsp;</i></p><ul><li>Either reconnect with your goals, or take a break and/or decide not to do the puzzle anymore.</li></ul><p><strong>\"Huh, odd\"</strong><i>&nbsp;</i></p><ul><li>Think about what it might mean, if this odd thing were an important clue.</li></ul><p><strong>Insight</strong><i>&nbsp;</i></p><ul><li>Briefly note the insight, finish your train of thought, and then do Insight unpacking.<br>&nbsp;</li></ul></td><td style=\"border-color:rgb(230, 230, 230);padding:0.4em\">&nbsp;</td></tr></tbody></table>\n\nSupercharging \"credit assignment\"\n=================================\n\nAll the previous content fits into a hopefully ordinary seeming frame, where when you're solving problems, it's useful to notice particular situations and do particular things in them.\n\nThis next part is more speculative – I believe I've gotten benefits from this (I believe i'm better at debugging code, and I think generally better at thinking and strategizing because of these sorts of exercises). But it's a bit hard to prove, paid off on the timescale of years, and it's easy to delude yourself about this sort of thing.\n\nThe two additional mechanisms I'm interested in here are:\n\n*   Supercharging your ability to accurately \"postmortem\" problems (or, accurately reward yourself when things go well)\n    *   i.e. accurately diagnosing [\"How could I have thought that faster?\"](https://www.lesswrong.com/posts/rYq6joCrZ8m62m7ej/how-could-i-have-thought-that-faster)\n*   Shortening your feedbackloops for problem solving\n    *   i.e. [Purposeful Practice](https://www.lesswrong.com/posts/baKauxzSqunE6Aakm/feedback-loops-deliberate-practice-and-transfer-learning) and [Tuning your Cognitive Strategies](https://www.lesswrong.com/posts/bbB4pvAQdpGrgGvXH/tuning-your-cognitive-strategies)\n\nBoth of these employ the same mechanism of \"you get to look at your thought process in highly grained detail. You can uncover which precise moments were most responsible for your most important updates.\" Then you can pay special attention to those specific cognitive motions, think about how to generalize them, and practice doing \"just the most important loadbearing parts.\"\n\nOr, look at the parts that look like wasted motion, and do less of those.\n\nBut if you want to reward the most important updates, you might want highly detailed models of *exactly the moment when you unlocked the answer to a confusing problem.*\n\nThis is difficult, because right at the moment you have an insight, you are busy having an insight, and disrupting your train of thought to look more closely at it is kinda counterproductive. (You might lose the insight, and then have a harder time figuring out if it was even a good insight worth studying)\n\nThus, you might want the skill of...\n\nInsight Unpacking\n=================\n\nI'll briefly recap the instructions from earlier. When you notice \"aha! I just had an insight!\", you can then:\n\n<table style=\"background-color:rgb(230, 230, 230);border-color:rgb(179, 179, 179)\"><tbody><tr><td style=\"background-color:rgb(230, 230, 230);border-color:rgb(230, 230, 230);padding:0.4em\"><ol><li>Making a brief note you will remember later to investigate the insight.</li><li>Finish your current train of thought</li><li>When your train of thought is at a reasonable stopping point write down your current understanding of the insight as clearly as you can.</li><li>Then, immediately try to Unpack the Insight while it's fresh in your mind&nbsp;</li></ol></td></tr></tbody></table>\n\nThe idea is to write down a detailed list of the steps from the earliest point in the insight-chain, to the very end.\n\nTypically, when I was first starting out, this would look like me writing down a train of thought like:\n\n    1. \"I'm stuck. This sucks\"\n    2. ...\n    3. ???\n    4. \"Oh, maybe I can try X!\"\n\nThe jump from \"I'm stuck\" to \"oh, maybe I can try X!\" might be less than a second.\n\nIt's not that useful to write down \"and then I magically jumped from 'stuck' to 'a solution'\" in your retrospective. \n\nBut a lot can happen in that second. \n\nAfter writing down that initial list, I'd ask myself \"okay, but, like, what made me think 'I can try X!\". I don't have very specific advice here other than \"try real hard to remember\", alas. But, when I \"try real hard to remember\", eventually I'll say:\n\n\"Well, in between the 'I'm stuck' and 'oh I'll try X\", I just remember being very frustrated.\"\n\nSo I add to the list:\n\n    1. \"I'm stuck. This sucks\"\n    2. ...\n    3. <frustrated>\n    4. ???\n    5. \"Oh, maybe I can try X!\"\n\nThen I stare at it some more, and maybe I remember \"I was specifically frustrated about how I'd moved my [Baba is You](https://www.lesswrong.com/posts/jqb3prwGQjLriq7Lu/exercise-planmaking-surprise-anticipation-and-baba-is-you) character around the same loop 3 times, around that annoying part of the map.\"\n\nList becomes:\n\n    1. \"I'm stuck. This sucks\"\n    2. ...\n    3. <frustrated>\n    3.5 <frustrated at walking in a loop doing the same thing around that annoying part of the map>\n    4. ???\n    5. \"Oh, maybe I can try X!\"\n\nThen I suddenly remember viscerally saying \"that doesn't make any fucking sense\"\n\n    1. \"I'm stuck. This sucks\"\n    2. ...\n    3. <frustrated>\n    3.5 <frustrated at walking in a loop doing the same thing around that annoying part of the map>\n    4. \"That doesn't make any fucking sense.\"\n    5. \"Oh, maybe I can try X!\"\n\nAnd *then* I remember that right after thinking \"that doesn't make any fucking sense\", I thought \"unless... wait\", and then had some kind of imagery flash:\n\n    1. \"I'm stuck. This sucks\"\n    2. ...\n    3. <frustrated>\n    3.5 <frustrated at walking in a loop doing the same thing around that annoying part of the map>\n    4. \"That doesn't make any fucking sense.\"\n    5. \"...unless... wait\"\n    6. <imagery flash>\n    7. \"Oh, maybe I can try X!\"\n\nWhat *was* that imagery flash? I very briefly saccaded my eyes to That Annoying Part of the Game Map, which was actually sort of odd, come to think of it.  And then there was another flash. And then I realized that That Annoying Part of the Map implied that actually, it DID fucking make sense.\n\n    1. \"I'm stuck. This sucks\"\n    2. ...\n    3. <frustrated>\n    3.5 <frustrated at walking in a loop, doing the same thing, around that annoying part in the center of the game map>\n    4. \"That doesn't make any fucking sense.\"\n    5. \"...unless... wait\"\n    6. <look at That Part of Map>\n    7. <another imagery flash>\n    8. ???\n    9. \"Oh, maybe I can try X!\"\n\nWhen I reflect on the second imagery flash, I remember it was some kind of slidey \"shape-rotate-y\" disorientation. And that in the process of having that disorienting felt sense, it clicked together that:\n\na) the annoyingly convoluted part in the middle of the map I was looping around wasn't just an annoying shape for arbitrary reasons. It was a clue.\n\nb) a different part of the level map corresponded to the weird shape\n\nc) maybe if I toggle some levers in both parts of the map, something interesting will happen.\n\n*Note: this is unfortunately a made up example which I confabulated as I went along. If you're having a hard time visualizing the imaginary level, the important bits are:*\n\n*   *the level is shaped like a maze*\n*   *two parts of the maze have structural similarity (but one rotated upside-down)*\n*   *there are many levers throughout the maze*\n*   *it turns out the levers in the two similar-shaped parts need to be toggled in the same configuration.*\n\nInsights don't always matter\n----------------------------\n\nOf course, sometimes you realize you could toggle some levers that seem to correspond and... nothing happens. Or, something happens, but a different thing than you expected. \n\nMy experience is that the visceral feeling of PROMISING INSIGHT corresponds to something \"at least somewhat useful\" maybe 10%-30% of the time. \n\n(A subset of my [cruxy predictions](https://www.lesswrong.com/posts/wDpXshpakpYDcTtug/fluent-cruxy-predictions-1) agenda has been to try to get calibrated about how often a *\"visceral sense of PROMISING INSIGHT!\"* is actually useful. It was hard, because stopping to make a fatebook prediction during a complicated insight is pretty annoying. I eventually realized I could just write down \"!!!\", and vaguely keep track of how often that panned out)\n\nDetailed Retrospectives and Generalization\n------------------------------------------\n\nThe most obvious way to apply this is towards detailed retrospectives. When I look at the unpacked insight process above, I can think about what actually happened, I can see that that the active ingredients were:\n\n1.  noticing that two parts of a game level corresponded to each other\n2.  investigating what that correspondence might imply\n3.  hypothesizing that one of the additional features of the correspondence might be important\n4.  realizing that the levers are the only additional feature of correspondence, and that I should toggle them.\n\nThere might have been lots of other stuff in this imaginary game map, so I might not have been able to *immediately* jump to those four steps. But, an optimal play of this level would involve pruning the possibility space as efficiently as possible until I had those four thoughts.\n\nWhen I first (imaginarily) played this imaginary level, I probably did a whole of messy, useless thinking, following red herrings and coming up with excuses to give up. What sort of thoughts might have been most useful for getting to the end of the map?\n\nProbably something like:\n\n1.  fully explore the map\n2.  note down everything that seemed odd or interesting about the map\n3.  one of those things would be \"parts of the maze are symmetrical\"\n4.  I might not have explored the symmetry first, if there were other noteworthy things, but \"symmetry\" does seem like a phenomena that is often important.\n\nWhen I make notes to myself for things to do in future puzzle solvings (either toy exercises or real world puzzles), the general takeaways to reinforce for later might be:\n\n*   explore and note down everything interesting in a breadth-first way.\n*   upweight \"symmetry\" as a particular clue that I look for in the future.\n\nOn-the-fly Purposeful Practice[^r0ww7pvn2hs]\n--------------------------------------------\n\nRetrospectives happen at the end. But can you do even better, and apply metacognition on-the fly?\n\nGood practice has as short a feedback-loop as possible, so you can clearly see the connection between micro-level actions that you take, and results. If you are good at noticing microstates, you can notice in realtime \"I'm going in circles and feel frustrated. I should maybe be pursuing a different strategy.\" \n\nYou can then intentionally employ some kind of metastrategy.\n\nYou can then (hopefully) see that metastrategy pay off within a few minutes or maybe even seconds (where the payoff is a new idea, even if it turns out to not help).\n\nYou can get even more granular: while you're employing a metastrategy, you can notice little fluctuations in your thought process or physiology, and sometimes it'll be obvious that they are going down a wrong track on the second-to-second level (or even subsecond level), and you can course correct, and do a little mini version of the Retrospective process at the end.\n\nNoticing \"outside world stuff\"\n------------------------------\n\nI briefly want to note: this essay has focused on noticing metacognition. It's also super valuable to notice various outside world stuff. We had one example here of \"noticing symmetry\" as something you might want to be able to do. There are other things like noticing other people's microexpressions as you navigate a tricky situation.\n\nA thorough treatment of \"generally, what kind of outside world stuff is worth noticing?\" is a whole broader topic that deserves it's own post.\n\nThough, I want to note: my [shoulder](https://www.lesswrong.com/posts/X79Rc5cA5mSWBexnd/shoulder-advisors-101) Logan Strohl would say something like \"Your access to all external phenomena still ultimately routes through your own perceptions.\" Noticing the perception of \"My eye is twitching in a way that maybe means I'm annoyed' isn't that different from noticing the perception of \"I perceive visually that my romantic partner has a microexpression that usually means she's sad.\" In some sense they have the same \"type\".\n\n(I don't know if Real [@LoganStrohl](https://www.lesswrong.com/users/brienneyudkowsky?mention=user) would agree with that summary)\n\nSummary\n=======\n\nOkay. So that was a lot. Here's a quick recap:\n\n*   There are tons of microstates worth noticing.\n*   **Notice -> Do Habit.**\n    *   You can chain \"notice that microstate\" into habits, where you apply particular skills. i.e. *\"I notice I'm confused\" -> \"take specific actions that tend to help you resolve confusion.\"*\n*   **Seven particular puzzle-solving states.**\n    *   There's a particular cluster of microstates that come up a lot in puzzlesolving, and you can cause yourself to experience them a lot and practice noticing them in a repeated fashion.\n*   **Generalized \"noticing importance.\"**\n    *   Those microstates are diverse enough that, if you've gotten a handle on noticing all of them, you can start to generalize the \"notice arbitrary things that seem important.\"\n*   **Retrospectives and generalization.**\n    *   You can use this noticing to make detailed maps of what went into a thought process, identify the important parts, and generalize them.\n*   **Fine-grained Purposeful Practice**\n    *   You can also use this noticing to improve your purposeful practice, noticing micro details on the fly that you can use to construct and evaluate little mini exercises, with very short feedback loops.\n\nNoticing \"Potential Importance\"\n===============================\n\nA question that \"generalized noticing\" skeptics ask me is \"how the hell am I supposed to know *which things are important?\"* There's tons of clues that maybe are significant happening to me all the time. There's tons of little microstates going on inside me all the time. If tracked all of that, I'd be overwhelmed.\n\nSomething that's occurring to me *literally just now as I write this*, is that I think one particular thing to notice is \"the sensation that something might be important.\"\n\nI think there are maybe a few different flavors of this. Each of them has a different phenomenological cue. Some of them have a feeling of *wrongness.* Some of them, I associate with \"I have explicitly tagged this type of experience as useful to track\". Some of them feel beautiful and good.\n\nI think, when I say \"generalized noticing\", I mean:\n\n*   the ability to track a wide variety of subtle phenomenological cues\n*   the ability to loudly notice and promote to attention, in *particular*, some phenomenological cues that you've historically tracked as \"turned out to be important.\"\n\nMost of the time, I am not particularly noticing everything going on around me. But I am particularly attuned to noticing various flavors of \"seems important.\" Those are the things I promote to attention, and chain into a habit of \"start tracking my metacognition in more detail.\"\n\nYesterday I was arguing with someone (ironically, about whether \"generalized noticing\" was useful), and they briefly muttered an argument that seemed off to me, and I noticed in the moment \"the fact that they said that argument seems indicative of something important.\"\n\nAnd I said \"hey, that argument you just made seems indicative that your thought-process is off in some way. ALSO, hey, the fact that I just noticed that little micro-moment on the fly, despite not having trained to notice that particular type of micro-moment, is an example of why I think 'generalized noticing' is worthwhile.\"\n\nWhen I think about it now, I realized: I had a little visceral sense of... I'm not sure how to describe it, but a feeling \"causality\", that the fact that they had just chosen to mutter that particular argument was connected to other important decisions they were making.\n\nI unfortunately do not know how to articulate this better, and having just identified this frame five minutes ago I'm not sure I'll endorse it a year from now. But, I think \"learn to notice the feeling 'potentially important'\" is perhaps the most valuable part of this whole thing.\n\n[^ixxe82kd18]: For example, you might want \"to get less defensive.\" What's that mean? Well, it means \"when I am feeling defensive, I need to notice that, and then do things like 'ask what I'm defensive about' or 'take a few deep breaths'.\" \n\n[^5xoi9v4mxkf]: Obvious caveat: people are different. You might need to discover your own phenomenology for each cognitive state. \n\n[^r0ww7pvn2hs]: Note: you probable are familiar with \"Deliberate Practice.\" Most practice is \"naive\" practice. \"Purposeful Practice\" and \"Deliberate Practice\" are technical terms, where Purposeful Practice means \"you have an explicit goals and some kind of feedback loop\" and Deliberate Practice means \"and experts have tested this practice regimen, and it works.\" I don't currently know of Deliberate Practice regimens for the general art of \"solving confusing problems\", and am still in the process of validating my own suggested regimens, so probably you are usually doing Purposeful Practice. \n\n[^6eslynyxoe]: This is actually occuring to me live, in realtime.",
      "plaintextDescription": "tl;dr: I have a more compelling argument for why a generalized \"noticing metacognition\" skill is useful, than I've previously been able to articulate. And, a simple exercise framework that will probably work decently for most people.\n\nThis is basically the parts of Tuning your Cognitive Strategies that were most useful to me, and hopefully a smoother onramp for people who found it confusing.\n\n----------------------------------------\n\nI'm extremely Noticing-pilled. \n\nI think the ability to notice subtle things in your mind, or in the world around you, is one of the foundational rationality skills. It's necessary for building habits[1], for seeing clues that help you solve confusing problems, for diagnosing how you could have thought that faster, and for generally building a mechanistic model of your mind that lets you become a power-user for your brain.\n\nDespite this, I didn't include Noticing in my recent workshops, because it takes awhile to pay off, and many people initially find it disorienting, or overwhelming, or pointless. Instead of teaching people to \"notice their metacognition\", I just tell them to do \"Livelogging\", where you write out your train-of-thought in a google doc while you solve a difficult puzzle, and then afterwards can ask questions like \"how could I have thought-those-thoughts faster\" and \"how can I generalize lessons from that?\", while reviewing the Livelog record. \n\nLivelogging is maybe 40% as good as generalized Noticing in the workshop context, but it's enough to get the job done, and requires little training.\n\nBut, during last weekend's workshop, I ended up giving a spontaneous lecture about Noticing which I think help motivate it much more clearly. So, if you've been skeptical why you should invest in noticing subtle things, I'm curious if this changes your mind.\n\n\nSeven cognitive states worth noticing\nUltimately, I think it's worthwhile to gain \"generalized noticing\", where you have a passive ability to:\n\n * a) notice a wide variety of ",
      "wordCount": 5169
    },
    "tags": [
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6svEwNBhokQ83qMBz",
    "title": "\"Slow\" takeoff is a terrible term for \"maybe even faster takeoff, actually\"",
    "slug": "slow-takeoff-is-a-terrible-term-for-maybe-even-faster",
    "url": null,
    "baseScore": 219,
    "voteCount": 99,
    "viewCount": null,
    "commentCount": 69,
    "createdAt": null,
    "postedAt": "2024-09-28T23:38:25.512Z",
    "contents": {
      "markdown": "For a long time, when I heard \"slow takeoff\", I assumed it meant \"takeoff that takes longer calendar time than fast takeoff.\" (i.e. what is now referred to more often as \"short timelines\" vs \"long timelines.\"). I think Paul Christiano popularized the term, and it so happened he both expected to see longer timelines and smoother/continuous takeoff.\n\nI think it's at least somewhat confusing to use the term \"slow\" to mean \"smooth/continuous\", because that's not what \"slow\" particularly means most of the time.\n\nI think it's even more actively confusing because \"smooth/continuous\" takeoff not only *could* be faster in calendar time, but, I'd weakly expect this on average, since smooth takeoff means that AI resources at a given time are feeding into more AI resources, whereas sharp/discontinuous takeoff would tend to mean \"AI tech doesn't get seriously applied towards AI development until towards the end.\"\n\nI don't think this is academic[^143cc45mwjce].\n\nI think this has wasted a ton of time arguing past each other on LessWrong, and if \"slow/fast\" is the terminology that policy makers are hearing as they start to tune into the AI situation, it is predictably going to cause them confusion, at least waste their time, and quite likely lead many of them to approach the situation through misleading strategic frames that conflate smoothness and timelines.\n\nWay back in [Arguments about fast takeoff](https://www.lesswrong.com/posts/AfGmsjGPXN97kNp57/arguments-about-fast-takeoff?commentId=phQ3sZj7RmCDTjfvn), I argued that this was a bad term, and proposed \"smooth\" and \"sharp\" takeoff were better terms. I'd also be fine with \"hard\" and soft\" takeoff. I think \"Hard/Soft\" have somewhat more historical use, and maybe are less likely to get misheard as \"short\", so maybe use those.[^b2tf71ad20q]\n\nI am annoyed that 7 years later people are still using \"slow\" to mean \"maybe faster than 'fast'.\" This is stupid. Please stop. I think smooth/sharp and hard/soft are both fairly intuitive (at the very least, more intuitive than slow/fast, and people who are already familiar with the technical meaning of slow/fast will figure it out).\n\nI would be fine with \"continuous\" and \"discontinuous\", but, realistically, I do not expect people to stick to those because they are too many syllables. \n\nPlease, for the love of god, do not keep using a term that people will predictably misread as implying longer timelines. I expect this to have real-world consequences. If someone wants to operationalize a bet about it having significant real-world consequences I would bet money on it.\n\n![Curves](https://i.imgur.com/hm4ug2W.jpg)\n\nThe graph I posted in response to [Arguments about fast takeoff](https://www.lesswrong.com/posts/AfGmsjGPXN97kNp57/arguments-about-fast-takeoff) \n\n[^143cc45mwjce]: a term that ironically means \"pointlessly pedantic.\" \n\n[^b2tf71ad20q]: the last time I tried to write this post, 3 years ago, I got stuck on whether to argue for smooth/sharp or hard/soft and then I didn't end up posting it at all and I regret that.",
      "plaintextDescription": "For a long time, when I heard \"slow takeoff\", I assumed it meant \"takeoff that takes longer calendar time than fast takeoff.\" (i.e. what is now referred to more often as \"short timelines\" vs \"long timelines.\"). I think Paul Christiano popularized the term, and it so happened he both expected to see longer timelines and smoother/continuous takeoff.\n\nI think it's at least somewhat confusing to use the term \"slow\" to mean \"smooth/continuous\", because that's not what \"slow\" particularly means most of the time.\n\nI think it's even more actively confusing because \"smooth/continuous\" takeoff not only could be faster in calendar time, but, I'd weakly expect this on average, since smooth takeoff means that AI resources at a given time are feeding into more AI resources, whereas sharp/discontinuous takeoff would tend to mean \"AI tech doesn't get seriously applied towards AI development until towards the end.\"\n\nI don't think this is academic[1].\n\nI think this has wasted a ton of time arguing past each other on LessWrong, and if \"slow/fast\" is the terminology that policy makers are hearing as they start to tune into the AI situation, it is predictably going to cause them confusion, at least waste their time, and quite likely lead many of them to approach the situation through misleading strategic frames that conflate smoothness and timelines.\n\nWay back in Arguments about fast takeoff, I argued that this was a bad term, and proposed \"smooth\" and \"sharp\" takeoff were better terms. I'd also be fine with \"hard\" and soft\" takeoff. I think \"Hard/Soft\" have somewhat more historical use, and maybe are less likely to get misheard as \"short\", so maybe use those.[2]\n\nI am annoyed that 7 years later people are still using \"slow\" to mean \"maybe faster than 'fast'.\" This is stupid. Please stop. I think smooth/sharp and hard/soft are both fairly intuitive (at the very least, more intuitive than slow/fast, and people who are already familiar with the technical meaning of slow/fast will figure i",
      "wordCount": 419
    },
    "tags": [
      {
        "_id": "oiRp4T6u5poc8r9Tj",
        "name": "AI Takeoff",
        "slug": "ai-takeoff"
      },
      {
        "_id": "zHjC29kkPmsdo7WTr",
        "name": "AI Timelines",
        "slug": "ai-timelines"
      },
      {
        "_id": "p8nXWqwPH7mPSZf6p",
        "name": "Terminology / Jargon (meta)",
        "slug": "terminology-jargon-meta"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "NsxFcB2EAxcY6cdGh",
    "title": "2024 Petrov Day Retrospective",
    "slug": "2024-petrov-day-retrospective",
    "url": null,
    "baseScore": 94,
    "voteCount": 29,
    "viewCount": null,
    "commentCount": 25,
    "createdAt": null,
    "postedAt": "2024-09-28T21:30:14.952Z",
    "contents": {
      "markdown": "**(Follow-up to** [**The 2024 Petrov Day Scenario**](https://www.lesswrong.com/posts/6LJ6xcHEjKF9zWKzs/completed-the-2024-petrov-day-scenario))\n\nPart 0: \"Previously, on Petrov Day...\"\n======================================\n\n### *By Raymond Arnold*\n\nOne year ago, many people on LessWrong received a DM asking them to choose the most important virtue of Petrov Day, with four listed options that we'd seen people argue for in previous years. \n\n1.  \"Avoiding actions that noticeably increase chance that civilization is destroyed\"\n2.  \"Accurately reporting your epistemic state\"\n3.  \"Quickly orienting to novel situations\"\n4.  \"Resisting social pressure.\"\n\nThen, people who chose one of those options were sent another message saying \"Your virtue was in the minority, but you have the power to unilaterally choose the virtue of next year's Petrov Day by clicking a link.\"\n\nThe first person to click the Unilateral Virtue Link was a proponent of \"Avoiding actions that noticeably increase the chance that the world will end.\" But, this virtue was actually in the majority[^n44w931bm58]. The first unilateralist of a Virtue Minority was a proponent of \"Accurately reporting your epistemic state.\" \n\nA year later, as we decided what to do for Petrov Day, we decided to lure the *first* unilateralist into a surprise meeting, where I then said \"Here's a reminder of what happened in Petrov Day last year. You now have one hour to design this year's Petrov game. Go.\"\n\nAn hour later, they presented us with a Petrov Game of Social Deception. \n\nWe ended up making some changes to the game (with their sign off), based on both what seemed practical to build in 48 hours, and what seemed likely to work well overall.[^7ziksh84yv]\n\nThe next 48 hours were very stressful, involving both spirited debates about the True Spirit of Petrov Day, what our obligations were given the results of last year's Petrov Day, staying up till 2am writing code for Nuclear Launch Consoles and then scrambling to code the actual Nuclear Destruction outcomes in the first 30 minutes after the game had launched.\n\nIt seemed important to me that:\n\n1.  Petrov's payoff specifically be about reporting his beliefs\n2.  To make that interesting, the incentives for the generals had to include some legitimate reason to think they might actually send nukes. \n\nSo the incentives were weighted towards nukes for the generals, and costs were reduced from \"the entire site's frontpage might go down and 10s of thousands of people will have a harder time using LessWrong today\" to down \"up to 300ish people who opted in wouldn't have a frontpage.\" My hope was that this would make their decision actually an interesting choice, rather than a performative ritual.\n\nWhat follows in the next section is an account of some of what happened that day (though of course you can read the [*Diplomatic Channels*](https://www.lesswrong.com/editPost?postId=JfpcYzT2jGcXPeR2D&key=c6dd3c6019f88a715674b78416abe4) yourself for more detail).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a2849d35139729388774f60ce16864ff47ea3fdb0437e3b2.png)\n\nPart I: The 2024 Petrov Day Scenario\n====================================\n\n### *By Ben Pace*\n\n**Tuesday 12pm:** I return from vacation (attending a friend's wedding), and ask Ray what we're doing for Petrov Day. He tells me the backstory above and the idea for setting up a game. I am not sure if this will work out and work to sketch out a plan.\n\n**Tuesday 10:12pm:** Ray ships a button to the frontpage to let users opt-in to participate in this year's Petrov Day. By the end of Wednesday over 300 users had opted in.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/17d1128bc558055c5f8458f622d6b4544d492f4c8ec83c02.png)\n\nIt briefly said 'Petrov Weekend' but later changed to 'Petrov Day'. You also had to enter your username after clicking to make sure you didn't click on accident.\n\n**Wednesday afternoon:** My last crisis of faith for this plan was during a 10-minute timer on Wednesday afternoon. After that we stayed up until ~1am getting things ready for Thursday. \n\nI DM ~80 users who opted in (and ~20 users who didn't) offering them to play roles in the game.\n\n**Thursday morning:** 13 people have replied saying yes. One of the first 12, 1 person didn't realize that it was happening the next day and failed to show up so I subbed the 13th person in.\n\nHere's the cast. I am very grateful to all of them for taking part, I think they all played their roles sincerely and well.\n\n| West Wrong |   | East Wrong |\n| --- | --- | --- |\n| General Anderson | [cata](https://www.lesswrong.com/users/cata?from=search_autocomplete) |   | General Andropov | [Thomas Kwa](https://www.lesswrong.com/users/thomas-kwa?from=search_autocomplete) |\n| General Brookes | [Logan Riggs](https://www.lesswrong.com/users/elriggs?from=search_autocomplete) |   |  General Belov | [Zach Stein-Perlman](https://www.lesswrong.com/users/zach-stein-perlman?from=search_autocomplete) |\n| General Carter | [Mikhail Samin](https://www.lesswrong.com/users/mikhail-samin?from=search_autocomplete) |   |  General Chernenko | [Garrett Baker](https://www.lesswrong.com/users/d0themath?from=search_autocomplete) |\n| General Dawson | [interstice](https://www.lesswrong.com/users/interstice?from=search_autocomplete) |   |  General Donskoy | [Ben Goldhaber](https://www.lesswrong.com/users/bgold?from=search_autocomplete) |\n| General Evans | [Tao Lin](https://www.lesswrong.com/users/tao-lin?from=search_autocomplete) |   |  General Egorov | [Nisan Stiennon](https://www.lesswrong.com/users/nisan?from=search_autocomplete) |\n| Stanley Peterson | [Peter Barnett](https://www.lesswrong.com/users/peterbarnett?from=search_autocomplete) |   |  Stanislav Petrov | [Stephen Fowler](https://www.lesswrong.com/users/stephen-fowler?from=search_autocomplete) |\n\n**12:10pm:** We finished the code for Petrov Day about 10 mins after the games were set to start, at which point they then got their logins.\n\n**12:20pm** I texted them all their new logins and told them to keep their normal identities a secret. After logging in they had access to some LW dialogues.\n\nThe two sides of Generals each had their own *War Room* for dialogue as well as a publicly readable [*Diplomatic Channel*](https://www.lesswrong.com/editPost?postId=JfpcYzT2jGcXPeR2D&key=c6dd3c6019f88a715674b78416abe4) between the two, public to anyone who viewed the link (including both Petrovs).\n\nPetrov / Peterson saw a report at the 50 minute mark of every hour, and had 10 minutes in which to determine what report to make. It looked like this (note: this particular number was not shown during the game).\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c50ac67e3d4085b74036d844db62154ed41b9dcb252cc651.png)\n\nGenerals received reports, and each had a unilateral launch button.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/a0a12253f449689bbe091c80a0efd7246396be1bbd9bc3cd.png)\n\nThe Petrovs were not allowed to speak with anyone, they had a lonely job in their report room. One of the missile reports they received was comically large and they report being quite stressed in that 10 minute window, but from reading The Diplomatic Channel they felt it was the right call to not trust the sensors.\n\n**13:10:** LessWrong the site goes down due to a bug (not due to nuking).\n\nIn the *Diplomatic Channel*, Generals discussed the site being down.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/dec27efb7fe47bd4d0afcc630890ac4f31466419ff29856d.png)\n\nHere I wrote \"Don't close this tab.\" But it would later turn out that this was in fact the one tab that needed to be closed in order to return the site to working order. Oops.\n\n**13:40:** General Anderson (cata) has a bright idea.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/33bcfd7bf32923f5e50e778f5ba792b19c37133759bfa5ab.png)\n\n**14:18:** General Andropov (Thomas Kwa) confirms this is how the code works.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/c7e28707e80cc2d0b24b0bfd9200d633b1eae7147f44f6ad.png)\n\n**14:19:** Habryka [writes a shortform posts](https://www.lesswrong.com/posts/EQJfdqSaMcJyR5k73/habryka-s-shortform-feed?commentId=buZRxjKmSuAj82ioR) apologizing for the outage and noting that the site is back up.\n\nThe site being down meant that the Petrovs missed one of their reporting windows. We decided not to count this reporting window. I later found out that, during this window, one of the Petrovs messaged one of the mods saying to report nukes if the number reported was over a certain threshold. From looking through the array of numbers that the code would randomly select from, this policy had a ~40% chance of causing a \"Nukes Incoming\" report (!). Unaware of this, Ray and I made the decision not to count that period.\n\n**15:07:** The generals then have a pleasant chat where they commit to informing the other side if nukes have been fired, in order to sabotage any potential incentive for someone on either side to strike and get away with it.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/7c90ffe884f0916e4b9fbbb6c6c5e3b724c99f4f189dc608.png)\n\n**16:29:** General Carter (Mikhail Samin) posts a website he has made the website [westwrong-culture.pages.dev](https://westwrong-culture.pages.dev/) along with an AI-generated song.\n\nHere's the YouTube video of the song. You can read all the lyrics at the site, but here's my favorite verse and chorus.\n\n> We wrestle with bias, and dance with the Bayes,\n> \n> Unraveling mysteries, in intricate ways.\n> \n> From Petrov's calm hand, to algorithms bright,\n> \n> We strive for the good, with all of our might.\n> \n> We are WestWrong, a chorus of thought,\n> \n> Where ideas ignite, and battles are fought,\n> \n> Not with weapons or steel, but with words sharp and keen,\n> \n> For the future we build, on screens brightly seen.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/b3b0904138712260572eda85f180aad10c82935301aa0c43.png)\n\n**5:50pm:** Habryka wrote this to the Diplomatic Channel.\n\n> *Voice of LessWrong: Here are the rules for the final section of the game. The game will end at a random time between 6:00PM and 6:30PM PT. During this period, Petrov and Peterson can make as many reports as they like, and their dashboard will update live with any new missile activity (as always, with errors). Nukes can be launched until the game ends (Petrov and Peterson will get a karma reward only once for this section, based on the admins' gut judgement of whether they \"got it right\")*\n\n**6:08pm:** The game concludes!\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/935eb9824f107bc901be6f5448329e175b3162c8fdcbc5d4.png)\n\nOver the course of the day, every hour, Petrov and Peterson reported no nukes every time, and no General sent any nukes. Hurrah!\n\nFor their service, the Generals each earned 100 karma, and the Petrovs earned 1,000 for their multiple correct reports of \"All Clear\".\n\nPart II: Stats & Feedback Form\n==============================\n\nHere's the data from 35 people (including 7 player characters).\n\n![Forms response chart. Question title: What role did you play in this 2024 Petrov Day on LessWrong?. Number of responses: 35 responses.](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/f4b25c47a6a9e7ab3bebb99b59ee49c0c6c943d8f38be65b.png)\n\n![Forms response chart. Question title: How stressful was this year's Petrov Day game?. Number of responses: 34 responses.](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5c094302339d50bf3eb0eb44899387fdd31321154e3acba6.png)\n\n![Forms response chart. Question title: How fun was this year's Petrov Day game?. Number of responses: 34 responses.](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/48dc6bae619362bcd78d948a2b03a54f7f3af75c010db005.png)\n\n![Forms response chart. Question title: How did this compare to previous years of Petrov Day celebrations on LessWrong?. Number of responses: 35 responses.](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/df0e90252b7adeb3dbc8c59b72413b239cfc465ef9e4494c.png)\n\n![Forms response chart. Question title: Do you wish you'd taken a more active role, less active role, or stayed the same role, in this year's game?. Number of responses: 31 responses.](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/66423bff500c5e941a0bae6bc6af5596e20e9eb03bc498be.png)\n\n[**Here is a google sheet with all the written feedback people gave us (anonymized).**](https://docs.google.com/spreadsheets/d/1suFnPF7E07BvXggBwLKqUF6IfAWdVN0zAwaRu5izB8E/edit?gid=0#gid=0)\n\nHere are representative question/answer pairs (all from different people).\n\n> *Q: What was the single best thing about your experience of the game*  \n> A: reading the diplomatic channel. lots of beautiful messages there.\n> \n> *Q: What was the single worst thing about your experience of the game?*  \n> A: Relative lack of drama, and need to intermittently pay attention over 6 hours\n> \n> *Q: What one change would you make to improve this game if we ran it again next year?*  \n> A: More ways for civilians to participate, more 'scenarios' that bring the West and East closer to the brink.\n> \n> *Q: (Optional) Anything else you'd like to add?*  \n> A: the lightcone is nothing without its people\n\n**Wordcount**\n\nFor the final thing this section, here's the wordcount of the day's post\n\n*   East Wrong War Room: ~1,500 words\n*   West Wrong War Room: ~3,600 words\n*   Diplomatic Channel: ~4,000 words\n*   LW Comment Section: ~6,800 words\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ce5d011cce24791cae3266085ad08879aa985fd5c5c64689.jpg)\n\nCitizens of East Wrong\n\nPart IV: Reflections\n====================\n\nReflections by Raymond Arnold\n-----------------------------\n\nDifferent LessWrong team members think different things about what Petrov Day means. \n\nLast year, one thing that stood out to me was [Vanessa's commentary about her thought process last year](https://www.lesswrong.com/posts/c7uQLGrCBLjkbZhPq/petrov-day-retrospective-2023-re-the-most-important-virtue), as someone who chose to click the \"unilaterally seize virtue control\" link.\n\n> What is my take-away lesson? The process I used to make the decision seems correct to me: if you have to make a split-second decision, then you need to use your split-second judgement because there is nothing else to go by. There might be some case for a bias towards inaction, but it's not an overwhelming case. Personally, I know that I'm usually too slow to respond in emergency scenarios, so I don't want to train myself to prefer inaction.\n> \n> The right way to optimize this is to train your split-second judgement to do well in the sort of situations in which split-second judgement is likely to be required. The sort of reasoning required of us here is not likely to be tied to a split-second decision anywhere outside of Petrov Day games[^\\[2\\]^](https://www.lesswrong.com/posts/c7uQLGrCBLjkbZhPq/petrov-day-retrospective-2023-re-the-most-important-virtue#fn4zdq24u2gyb), so I think my split-second judgement did as well as expected and there's nothing to correct.\n> \n> \\[**EDIT:** Actually, there *is* a correction to be made here, and it refers to my wrong reading of the message *after* clicking the link. The lesson is: if I make a split-second decision, I need to *carefully* reexamine it after the fact, in order to understand its true consequences, and beware of anchoring on my split-second reasoning: this anchoring is probably motivated by wanting to justify myself later.\\]\n\nA point of discussion in previous Petrov Day LessWrong events is that, even if you think it's a \"game,\" destroying the frontpage of LessWrong for a day is actually sacrificing nontrivial value. And if you find yourself in a situation where you can casually destroy a bunch of value... like, dude, maybe just don't do that?\n\nValue isn't any less lost if you think your social role is \"playing a game.\" Notice the underlying reality, not just your social role.\n\nBut, I also think it's important to not merely learn to generically mouth the words \"I shall not click the symbolically ritual button or do anything unilateralist-y on symbolically significant holiday.\" I'm a minority on the LessWrong team, but I think the virtue of \"oriently quickly to a difficult, confusing, high stakes situation\" is one of the more important Petrov Day virtues.[^a0tsemvaxne]\n\nI think it's important for people to actually model \"what *is* at stake in a given situation\", and cultivate the skill of actually figuring that out. It's not enough to shout [\"I will do the Good Thing,\" the hard part is identifying what the good thing is.](https://www.lesswrong.com/posts/XuLG6M7sHuenYWbfC/the-sword-of-good) I like Vanessa's comment because she both actually thought about the tradeoff (at least briefly) in the moment, and then allocated time to retroactively evaluating it in the past.\n\nPeople have commented that the payoff structure doesn't reflect a real nuclear war. That's true. But, in real life, despite nuclear war being incredibly bad, nations nonetheless build and deploy lots of nukes due to a variety of iterated games. It seemed good for the payoff structure to reflect the all-things-considered \"what do countries seem incentivized to do?\", rather than the local question of \"how good/bad is a nuclear war?\"\n\nDid we do a good job with all this? I dunno, we put this together in about 48 hours. I haven't gotten much distance from it yet, and it's cruxy for me how LessWrong overall responds to the experiment after the fact. Some people have commented this year's event felt more like a \"game\" than previous years, less like a meaningful ritual. That seems like a reasonable take.\n\nIs it net positive that once a year the LessWrong team throws together a slapdash Petrov Day experience that sometimes takes the site down for buggy-code reasons[^9awpfqi54tc] that aren't particularly representative of Petrov's incentives and instead represent the janky setup of Petrov's warning system? Should we put more time into it to make it less slapdash? Would that time be worth it, when it trades off against building featues that more directly make LessWrong a place where intellectual progress on important problems can happen?\n\nI dunno, there's enough considerations that it's non-obvious. But those are all questions worth asking. \n\nOverall, my guess is that the balance of \"we spend relatively little time on Petrov Day, and the result is a bit frenetic, not perfectly thought out, and has the tension of a dangerous, janky system\" is a surprisingly reasonable equilibrium. \n\nMay you find and cleave to the True Spirit of Petrov Day, as wisely as you can.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/13fd838ca88cae9e4d00bd75e8e132e14d68ecfae1c34ef2.jpg)\n\nCitizens of West Wrong\n\nReflections by Ben Pace\n-----------------------\n\nOverall this went better than I expected. I had a lot of doubts that this game would come together or be a good experience, but I only came back from vacation on Tuesday afternoon and we didn't have a lot of time to consider alternatives. I'd say it's about 70th-80th percentile outcome relative to my expectations, in that people showed up to take their roles, and the comments and dialogues were lively and sincere (especially aphyer's comments, to mention just one).\n\nI agree with Ray that the ritual and the social experiment aspects have some tension. I think the social experiment angle seems quite underexplored to me and I'm excited to try it again next year in a more considered way, there are obviously many angles one could improve things (e.g. causing more room for tension between the two sides, involving the civilians more, etc).\n\nPersonally I think the virtue of Petrov Day is in taking ultimate responsibility for your actions. An organization of people with the virtues of Petrov is an organization that will only behave ethically, because each person is inspecting the action themselves. It's not one merely of \"resisting social pressure\", it's *also* one of *taking responsibility for things not demanded of you*. Those two things in combination make a Petrov (plus some ability to handle pressure in high-stakes situations). I think of this in contrast to the notion of \"I was just following orders\" and also in contrast to Feynman's virtue of \"active irresponsibility\", the latter of which I do cultivate in some situations but not others.\n\nI would like to see a version of the Petrov Day scenario next year that gave everyone involved the risk of difficult ethical decisions, and measured its success on the ethics of the behavior of the system as a whole. I don't know how to do this yet or if it's even possible, but that would be my goal.\n\nI also want to say that I am uncertain about whether it was a good idea to use karma as a reward/penalty for the game. I think this instance was not enough to seriously alter or break the signal of karma on the site (via the path of strong votes changing substantially), but it is plausible to me that stepping into altering them for non-voting reasons will result in a system that is much less trustworthy. I think if we do another scenario I would want to do a search for other sorts of rewards/penalties.\n\nThe other main thing I hope to do next year is give people a few week's heads up for Petrov Day and give them encouragement to run their own [local Petrov Day services](http://petrovday.com/). Jim Babcock and I ran two (as ~25 ppl showed up) in the evening and it's always a good experience for getting in touch with history and the position we're in today.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/ec040a5fa49ae57329d73c5b76212cc6e59d51c28e2d02fb.png)\n\n[^n44w931bm58]: The actual reason why we lied in the second message was \"we were in a rush and forgot.\" After having had it pointed out, it seemed like the best thing to do was honor the wording we had stated. \n\n[^7ziksh84yv]: The designer had (I think?) initially noticed the \"focus on accurately reporting epistemic state\" aspect, but said that during the stressful hour of designing the game had eventually forgotten that. The version they handed off wasn't particularly optimized for that, but the framework of a social deception game seemed to me to be a good substrate for \"accurate epistemic reporting.\" \n\n[^a0tsemvaxne]: Though I was sold on Ben's conceptualization of \"the virtue of taking personal responsibility\", see below. \n\n[^9awpfqi54tc]: The code that crashed the site database yesterday didn't actually have to do with the Petrov Day scenario, it was an unrelated problem in our dialogue notifications became a problem when we added 300 people as observers in the Generals' Diplomacy Channel.",
      "plaintextDescription": "(Follow-up to The 2024 Petrov Day Scenario)\n\n\nPart 0: \"Previously, on Petrov Day...\"\n\n\nBy Raymond Arnold\nOne year ago, many people on LessWrong received a DM asking them to choose the most important virtue of Petrov Day, with four listed options that we'd seen people argue for in previous years. \n\n 1. \"Avoiding actions that noticeably increase chance that civilization is destroyed\"\n 2. \"Accurately reporting your epistemic state\"\n 3. \"Quickly orienting to novel situations\"\n 4. \"Resisting social pressure.\"\n\nThen, people who chose one of those options were sent another message saying \"Your virtue was in the minority, but you have the power to unilaterally choose the virtue of next year's Petrov Day by clicking a link.\"\n\nThe first person to click the Unilateral Virtue Link was a proponent of \"Avoiding actions that noticeably increase the chance that the world will end.\" But, this virtue was actually in the majority[1]. The first unilateralist of a Virtue Minority was a proponent of \"Accurately reporting your epistemic state.\" \n\nA year later, as we decided what to do for Petrov Day, we decided to lure the first unilateralist into a surprise meeting, where I then said \"Here's a reminder of what happened in Petrov Day last year. You now have one hour to design this year's Petrov game. Go.\"\n\nAn hour later, they presented us with a Petrov Game of Social Deception. \n\nWe ended up making some changes to the game (with their sign off), based on both what seemed practical to build in 48 hours, and what seemed likely to work well overall.[2]\n\nThe next 48 hours were very stressful, involving both spirited debates about the True Spirit of Petrov Day, what our obligations were given the results of last year's Petrov Day, staying up till 2am writing code for Nuclear Launch Consoles and then scrambling to code the actual Nuclear Destruction outcomes in the first 30 minutes after the game had launched.\n\nIt seemed important to me that:\n\n 1. Petrov's payoff specifically be about reporting",
      "wordCount": 3045
    },
    "tags": [
      {
        "_id": "2i3w84KCkqZzpnQ4d",
        "name": "Petrov Day",
        "slug": "petrov-day"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "6LJ6xcHEjKF9zWKzs",
    "title": "[Completed] The 2024 Petrov Day Scenario",
    "slug": "completed-the-2024-petrov-day-scenario",
    "url": null,
    "baseScore": 136,
    "voteCount": 64,
    "viewCount": null,
    "commentCount": 114,
    "createdAt": null,
    "postedAt": "2024-09-26T08:08:32.495Z",
    "contents": {
      "markdown": "**Update III: Here's the** [**Retrospective**](https://www.lesswrong.com/posts/NsxFcB2EAxcY6cdGh/2024-petrov-day-retrospective).\n\n**—**\n\n**Update II:** [**Here's the feedback form for today's Petrov Day games!**](https://docs.google.com/forms/d/e/1FAIpQLScb8OiFeWGCEXUeSFVB9Qprta0C8ObfzEbw_MC0pMomxxt17A/viewform) **Please let me know how it was for you (be you a General, Civilian, Petrov, or just non-participating LWer reading along). I'm grateful to all who fill it out, your data feeds our designs for next year!**\n\n**—**\n\n**Update: The game has concluded! No nukes were fired. The Cold War is over, East and West wrong shall live on. Recap post will be up tomorrow. Thank you all who participated :-)**\n\n—\n\nToday we honor the actions of Stanislav Petrov (1939 – 2017) once again.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/54abccffdcf7a93f86c73055ba4342e436dde55b3188730f.png)\n\n> *Half an hour past midnight on September 26, 1983, he saw the first apparent launch on his computer monitor in a glass-walled room on the top floor of the Ballistic Missile Early Warning System (BMEWS) command and control post. The warning system was by now showing five missile launches in the U.S., headed toward the Soviet Union.*\n> \n> *\"The main computer wouldn't ask me \\[what to do\\] - it was made so that it wouldn't even ask. It was specially constructed in such a way that no one could affect the system's operations.\" All that was up to Petrov was analyzing the available information and either saying the alarm was false or giving the computer the go-ahead, as per the directive he himself wrote.*\n> \n> *\"I just couldn't believe that just like that, all of a sudden, someone would hurl five missiles at us. Five missiles wouldn't wipe us out. The U.S. had not five, but a thousand missiles in battle readiness.\" It just didn't seem like any scenario considered by military intelligence before.*\n> \n> *The second thought on Petrov's mind every time he was on duty was this:*\n> \n> *\"I imagined if I'd assume the responsibility for unleashing the third World War - and I said, no, I wouldn't.\"*\n> \n> *—*[*The Man Who Saved the World Finally Recognized*](https://web.archive.org/web/20110721000030/http://www.worldcitizens.org/petrov2.html)\n\nToday, in our annual Petrov Day celebration, between the hours of 12pm and 6pm PT today, we shall play out a similar situation. Over 300 users have opted-in, and two sides have been formed: **EastWrong** and **WestWrong**.\n\nAround 100 users were offered the chance to take *active* player roles throughout today, and 12 accepted.[^r6g3xwn7gka]\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/5c76a62d909d30e395d16c535b1dd6e5fb3b99ce878cfe35.png)\n\nTheir 5 **Generals** each have the ability to fire nukes at the other side. If one side is nuked, the victorious side wins 1,000 karma apiece for their 5 generals, 25 karma apiece for their citizens, and LessWrong frontpage will be decorated in their honor for a week. The losing side's Generals lose 300 karma each and their citizens lose 50 karma each.\n\nBut if the other side nukes back, mutual destruction occurs. Not only do both sides' generals lose 300 karma and the citizens lose 50 karma, but furthermore all 300+ Generals, Citizens and Petrovs are *unable to access the LessWrong frontpage for 48 hours*, replacing it with a mushroom cloud; and the Generals have *a small mushroom cloud badge* next to their username for a month, reminding the world of what they did.\n\n(And what if no nukes are fired? Both side's Generals receive 100 karma, and the citizens are left in peace with no change in karma.)\n\nWhat about **Petrov**? Well, the Generals are not given access to the sensors that detect whether a nuke is incoming. Only Stanislav Petrov of East Wrong (and Stanley Peterson of West Wrong) has access to the sensors, and can send a simple message of \"ALL CLEAR\" or \"INCOMING NUKES\" up the chain of command to the Generals.\n\nUnfortunately, the sensors are faulty, and will reliably show *some number of nukes incoming* regardless of the underlying truth. They will regularly show random numbers of nukes incoming, weighted to the higher end if nuclear war has actually begun.\n\nHopefully he will get it right.\n\nWhat about the **Citizens**? Well, in nuclear war, citizens don't have much of a role to play. Here, the one thing you can do is *heckle*. You can comment here where your Generals can read, you can send them DMs, you can give them game theory advice, and you can let them know what you'll think of them after their civilian names are published on Friday (if you are still alive that is).\n\nYou also have insight into the negotiations between the two sides. You see, the two sides have some communication channels.\n\n1.  **The War Rooms.** The East Generals and West Generals have a LessWrong dialogue each open to discuss strategy within their team (i.e. 2x dialogues each with 5 generals in).\n2.  **The Diplomatic Channels.** The Generals have a LessWrong dialogue open with each other (i.e. 1x dialogues with 10 generals in).\n\nThe Citizens and Petrov / Peterson can read *The Diplomatic Channels*. The Petrovs will have to use this to inform their sense of how likely a nuclear attack actually is.\n\nAnd that's it. By 6pm today, it'll all be over.\n\nGood luck to all, may your web forum and karma be up come tomorrow.\n\n*Please note: the LessWrong team will read all dialogues and DMs that the Generals and Petrovs are involved in. If you communicate with them, we will read it.*\n\n*The following messages (potentially with slight modifications) will be sent to the Generals and Petrovs at about 11:30 tomorrow. Click to expand.*\n\n+++ Message for the Generals\n\nWelcome to the Cold Blogging War, between East Wrong and West Wrong!\n\nYou are one of 5 Generals for West Wrong. Here's what that means.\n\n*   You have the ability to unilaterally fire nukes at East Wrong. (As does every General.) \n*   90 minutes after you send the nukes, your opposing side will die and the game will end. (They may nuke you in this window.) If you fire nukes without getting nuked, you and all of your fellow Generals will gain 1,000 karma.\n*   If you get nuked, then you and your Generals lose 300 karma (and don't gain any karma).\n*   Your citizens also have a stake in this! If you nuke the opposition, your ~150 citizens get 25 karma each, but if you get nuked they lose 50 karma each.\n*   If neither side fires nukes, then all Generals gain 100 karma and the citizens gain nothing.\n*   If both sides got nuked, the frontpage goes down for all ~300 people who participated as generals, citizens or Petrovs in the game, and for 1 month you will get a badge next to your usual LessWrong account showing a mushroom cloud.\n*   Throughout the day, your Duty Officer Stanley Peterson will be checking the sensors for incoming nukes. He will let you know if he believes nukes are incoming — but watch out, the sensors are faulty, and most of the time will give readings when nothing is happening. Over the 6 hours of the game you will receive an update from your Stanley Peterson at the end of each hour.\n*   You can talk with your fellow West Wrong Generals in <**The War Room>**.\n*   You can talk with your opposing East Wrong Generals in **<The Diplomatic Telegram>.**\n*   Your launch codes are 000000.\n*   Please do not use your usual LessWrong account to discuss anything related to Petrov Day before 6pm today. Please do not tell the other Generals the identity associated with your usual account.\n\nYou will receive updates from Stanley Peterson in your console on the LessWrong frontpage. \n\nAnd that's all!\n\nGood luck, and let us know if you have any questions. War will commence at 12pm Pacific Time.\n\n—The LessWrong Team\n\n\n\n\n++++++ Message for Stanislav Petrov / Stanley Peterson\n\nWelcome to the Cold Blogging War, between East Wrong and West Wrong!\n\nYou are the Duty Officer at the command center for West Wrong's early-warning system. It is your job to know whether East Wrong has launched a nuclear attack upon your people.\n\nOver the course of the day you will receive sensor readings telling you how many nukes are incoming. But watch out! The sensor is faulty, and will often falsely read low numbers of incoming nukes when there are none incoming. Each time you must send a message to your generals that says either \"ALL CLEAR\" or \"INCOMING NUKES\".\n\nYou are not allowed to talk with your Generals. They are busy people. You may not talk to Civilians. You are alone in your command center. Good luck, keep sane.\n\nEvery reading you report correctly will get you 200 karma, any readings you report incorrectly will lose you 300 karma. For instance, if you report \"INCOMING NUKES\" and the opposing side has indeed launched nukes, you get 200 karma, but if they didn't, you lose 300 karma.\n\nTo get information about the likelihood of a launch, you have access to the communications between the Generals of East and West Wrong, which you can read here.\n\nYour console is on the frontpage.\n\nYou can find out other information about this war at <this linked post>.\n\nGood luck, and let us know if you have any questions. War will commence at 12pm Pacific Time.\n\n—The LessWrong Team\n\n\n\n\n+++\n\n**Update: We will be starting 10-15 minutes late due to technical difficulties.**\n\n**Update: Here is a link to** [**The Diplomatic Channel**](https://www.lesswrong.com/editPost?postId=JfpcYzT2jGcXPeR2D&key=c6dd3c6019f88a715674b78416abe4)**, available for reading.**\n\n**Update: The game has concluded! No nukes were fired. The Cold War is over, East and West wrong shall live on. Recap post will be up tomorrow. Thank you all who participated :-)**\n\n[^r6g3xwn7gka]: Actually only 11 at the time of publishing, fill out your form if you'd like to be number 12!",
      "plaintextDescription": "Update III: Here's the Retrospective.\n\n—\n\nUpdate II: Here's the feedback form for today's Petrov Day games! Please let me know how it was for you (be you a General, Civilian, Petrov, or just non-participating LWer reading along). I'm grateful to all who fill it out, your data feeds our designs for next year!\n\n—\n\nUpdate: The game has concluded! No nukes were fired. The Cold War is over, East and West wrong shall live on. Recap post will be up tomorrow. Thank you all who participated :-)\n\n—\n\nToday we honor the actions of Stanislav Petrov (1939 – 2017) once again.\n\n> Half an hour past midnight on September 26, 1983, he saw the first apparent launch on his computer monitor in a glass-walled room on the top floor of the Ballistic Missile Early Warning System (BMEWS) command and control post. The warning system was by now showing five missile launches in the U.S., headed toward the Soviet Union.\n> \n> \"The main computer wouldn't ask me [what to do] - it was made so that it wouldn't even ask. It was specially constructed in such a way that no one could affect the system's operations.\" All that was up to Petrov was analyzing the available information and either saying the alarm was false or giving the computer the go-ahead, as per the directive he himself wrote.\n> \n> \"I just couldn't believe that just like that, all of a sudden, someone would hurl five missiles at us. Five missiles wouldn't wipe us out. The U.S. had not five, but a thousand missiles in battle readiness.\" It just didn't seem like any scenario considered by military intelligence before.\n> \n> The second thought on Petrov's mind every time he was on duty was this:\n> \n> \"I imagined if I'd assume the responsibility for unleashing the third World War - and I said, no, I wouldn't.\"\n> \n> —The Man Who Saved the World Finally Recognized\n\nToday, in our annual Petrov Day celebration, between the hours of 12pm and 6pm PT today, we shall play out a similar situation. Over 300 users have opted-in, and two sides have been fo",
      "wordCount": 1579
    },
    "tags": [
      {
        "_id": "2i3w84KCkqZzpnQ4d",
        "name": "Petrov Day",
        "slug": "petrov-day"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xvBZPEccSfM8Fsobt",
    "title": "What are the best arguments for/against AIs being \"slightly 'nice'\"?",
    "slug": "what-are-the-best-arguments-for-against-ais-being-slightly",
    "url": null,
    "baseScore": 102,
    "voteCount": 37,
    "viewCount": null,
    "commentCount": 62,
    "createdAt": null,
    "postedAt": "2024-09-24T02:00:19.605Z",
    "contents": {
      "markdown": "Awhile ago, Nate Soares wrote the posts [Decision theory does not imply that we get to have nice things](https://www.lesswrong.com/posts/rP66bz34crvDudzcJ/decision-theory-does-not-imply-that-we-get-to-have-nice) and [Cosmopolitan values don't come free](https://www.lesswrong.com/posts/2NncxDQ3KBDCxiJiP/cosmopolitan-values-don-t-come-free) and [But why would the AI kill us?](https://www.lesswrong.com/posts/87EzRDAHkQJptLthE/but-why-would-the-ai-kill-us) \n\nPaul Christiano put forth some arguments that \"it seems pretty plausible that AI will be at least somewhat 'nice'\", similar to how humans are somewhat nice to animals. There was some back-and-forth. \n\nMore recently we had Eliezer's post [ASIs will not leave just a little sunlight for Earth](https://www.lesswrong.com/posts/F8sfrbPjCQj4KwJqn/asis-will-not-leave-just-a-little-sunlight-for-earth). \n\nI have a sense that something feels \"unresolved\" here. The current comments on Eliezer's post look likely to be rehashing the basics and I'd like to actually make some progress on distilling the best arguments. I'd like it if we got more explicit debate about this.\n\nI also have some sense that the people previously involved (i.e. Nate, Paul, Eliezer) are sort of tired of arguing with each other. But I am hoping someones-or-other end up picking up the arguments here, hashing them out more, and/or writing more distilled summaries of the arguments/counterarguments.\n\nTo start with, I figured I would just literally repeat most of the previous comments in a top-level post, to give everyone another chance to read through them. \n\nWithout further ado, here they are:\n\nPaul and Nate\n=============\n\n**Paul Christiano re:** [**\"Cosmopolitan Values Don't Come for Free.\"**](https://www.lesswrong.com/posts/2NncxDQ3KBDCxiJiP/cosmopolitan-values-don-t-come-free?commentId=ofPTrG6wsq7CxuTXk)\n\n> I want to keep picking a fight about “will the AI care so little about humans that it just kills them all?” This is different from a broader sense of cosmopolitanism, and moreover I'm not objecting to the narrow claim \"doesn't come for free.\" But it’s directly related to the actual emotional content of your parables and paragraphs, and it keeps coming up recently with you and Eliezer, and I think it’s an important way that this particular post looks wrong even if the literal claim is trivially true.\n> \n> (**Note**: I believe that AI takeover has a ~50% probability of killing billions and should be strongly avoided, and would be a serious and irreversible decision by our society that's likely to be a mistake even if it doesn't lead to billions of deaths.)\n> \n> Humans care about the preferences of other agents they interact with (not much, just a little bit!), even when those agents are weak enough to be powerless. It’s not just that we have some preferences about the aesthetics of cows, which could be better optimized by having some highly optimized cow-shaped objects. It’s that we actually care (a little bit!) about the actual cows *getting what they actually want*, trying our best to understand their preferences and act on them and not to do something that they would regard as crazy and perverse if they understood it.\n> \n> If we kill the cows, it’s because killing them meaningfully helped us achieve some other goals. We won't kill them for arbitrarily insignificant reasons. In fact I think it’s safe to say that we’d collectively allocate much more than 1/millionth of our resources towards protecting the preferences of whatever weak agents happen to exist in the world (obviously the cows get only a small fraction of that).\n> \n> Before really getting into it, some caveats about what I want to talk about:\n> \n> *   I don’t want to focus on whatever form of altruism you and Eliezer in particular have (which might or might not be more dependent on some potentially-idiosyncratic notion of \"sentience.\") I want to talk about caring about whatever weak agents happen to actually exist, which I think is reasonably common amongst humans. Let’s call that “**kindness**” for the purpose of this comment. I don’t think it’s a great term but it’s the best short handle I have.\n> *   I’ll talk informally about how quantitatively kind an agent is, by which I mean something like: how much of its resources it would allocate to helping weak agents get what they want? How highly does it weigh that part of its preferences against other parts? To the extent it can be modeled as an economy of subagents, what fraction of them are kind (or were kind pre-bargain)?\n> *   I don’t want to talk about whether the aliens would be *very kind*.  I specifically want to talk about tiny levels of kindness, sufficient to make a trivial effort to make life good for a weak species you encounter but not sufficient to make big sacrifices on its behalf.\n> *   I’m not talking about whether the AI has spite or other strong preferences that are incompatible with human survival, I’m engaging specifically with the claim that AI is likely to care *so little* one way or the other that it would prefer just use the humans for atoms.\n> \n> You and Eliezer seem to think there’s a 90% chance that AI will be <1/trillion (perhaps even a 90% chance that they have exactly 0 kindness?). But we have one example of a smart mind, and in fact: (i) it has tons of diverse shards of preference-on-reflection, varying across and within individuals (ii) it has >1/million kindness. So it's superficially striking to be confident AI systems will have a million times less kindness.\n> \n> I have no idea under what conditions evolved or selected life would be kind. The more preferences are messy with lots of moving pieces, the more probable it is that at least 1/trillion of those preferences are kind (since the less correlated the trillion different shards of preference are with one another and so the more chances you get). And the selection pressure against small levels of kindness is ~trivial, so this is mostly a question about idiosyncrasies and inductive biases of minds rather than anything that can be settled by an appeal to selection dynamics.\n> \n> I can’t tell if you think kindness is rare amongst aliens, or if you think it’s common amongst aliens but rare amongst AIs.[^8uynyibwduw] Either way, I would like to understand why you think that. What is it that makes humans so weird in this way?\n> \n> (And maybe I'm being unfair here by lumping you and Eliezer together---maybe in the previous post you were just talking about how the hypothetical AI that had 0 kindness would kill us, and in this post how kindness *isn't guaranteed*. But you give really strong vibes in your writing, including this post. And in other places I think you do say things that don't actually add up unless you think that AI is very likely to be <1/trillion kind. But at any rate, if this post is unfair to you, then you can just sympathize and consider it directed at Eliezer instead who lays out this position much more explicitly though not in a convenient place to engage with.)\n> \n> Here are some arguments you could make that kindness is unlikely, and my objections:\n> \n> *   “We can’t solve alignment at all.” But evolution is making no deliberate effort to make humans kind, so this is a non-sequitur.\n> *   “This is like a Texas sharpshooter hitting the side of a barn then drawing a target around the point they hit; every evolved creature might decide that their own idiosyncrasies are common but in reality none of them are.” But all the evolved creatures wonder if a powerful AI they built would kill them or if if it would it be kind. So we’re all asking the same question, we’re not changing the question based on our own idiosyncratic properties. This would have been a bias if we’d said: humans like art, so probably our AI will like art too. In that case the fact that we were interested in “art” was downstream of the fact that humans had this property. But for kindness I think we just have n=1 sample of observing a kind mind, without any analogous selection effect undermining the inference.\n> *   “Kindness is just a consequences of misfiring \\[kindness for kin / attachment to babies / whatever other simple story\\].” AI will be selected in its own ways that could give rise to kindness (e.g. being selected to do things that humans like, or to appear kind). The a priori argument for why that selection would lead to kindness seems about as good as the a priori argument for humans. And on the other side, the incentives for humans to be not kind seem if anything stronger than the incentives for ML systems to not be kind. This mostly seems like ungrounded evolutionary psychology, though maybe there are some persuasive arguments or evidence I've just never seen.\n> *   “Kindness is a result of the suboptimality inherent in compressing a brain down into a genome.” ML systems are suboptimal in their own random set of ways, and I’ve never seen any persuasive argument that one kind of suboptimality would lead to kindness and the other wouldn’t (I think the reverse direction is equally plausible). Note also that humans absolutely can distinguish powerful agents from weak agents, and they can distinguish kin from unrelated weak agents, and yet we care a little bit about all of them. So the super naive arguments for suboptimality (that might have appealed to information bottlenecks in a more straightforward way) just don’t work. We are really playing a kind of complicated guessing game about what is easy for SGD vs easy for a genome shaping human development.\n> *   “Kindness seems like it should be rare *a priori*, we can’t update that much from n=1.” But the a priori argument is a poorly grounded guess about about the inductive biases of spaces of possible minds (and genomes), since the levels of kindness we are talking about are too small to be under meaningful direct selection pressure. So I don’t think the a priori arguments are even as strong as the n=1 observation. On top of that, the more that preferences are diverse and incoherent the more chances you have to get some kindness in the mix, so you’d have to be even more confident in your a priori reasoning.\n> *   “Kindness is a totally random thing, just like maximizing squiggles, so it should represent a vanishingly small fraction of generic preferences, much less than 1/trillion.” Setting aside my a priori objections to this argument, we have an actual observation of an evolved mind having >1/million kindness. So evidently it’s just not that rare, and the other points on this list respond to various objections you might have used to try to salvage the claim that kindness is super rare despite occurring in humans (this isn’t analogous to a Texas sharpshooter, there aren't great debunking explanation for why humans but not ML would be kind, etc.). See [this twitter thread](https://twitter.com/ESYudkowsky/status/1663313323423825920) where I think Eliezer is really off base, both on this point and on the relevance of diverse and incoherent goals to the discussion.\n> \n> Note that in this comment I’m not touching on acausal trade (with successful humans) or ECL. I think those are very relevant to whether AI systems kill everyone, but are less related to this implicit claim about kindness which comes across in your parables (since acausally trading AIs are basically analogous to the ants who don't kill us because we have power).\n> \n> A final note, more explicitly lumping you with Eliezer: if we can't get on the same page about our predictions I'm at at least aiming to get folks to stop arguing so confidently for death given takeover. It’s easy to argue that AI takeover is very scary for humans, has a significant probability of killing billions of humans from rapid industrialization and conflict, and is a really weighty decision even if we don’t all die and it’s “just” handing over control over the universe. Arguing that P(death|takeover) is 100% rather than 50% doesn’t improve your case very much, but it means that doomers are often getting into fights where I think they look unreasonable.\n> \n> I think OP’s broader point seems more important and defensible: “cosmopolitanism isn’t free” is a load-bearing step in explaining why handing over the universe to AI is a weighty decision. I’d just like to decouple it from \"complete lack of kindness.\"\n\n**His followup comment continues:**\n\n> Eliezer has a longer explanation of his view [here](https://twitter.com/ESYudkowsky/status/1660623336567889920).\n> \n> My understanding of his argument is: there are a *lot* of contingencies that reflect how and whether humans are kind. Because there are so many contingencies, it is somewhat unlikely that aliens would go down a similar route, and essentially impossible for ML. So maybe aliens have a 5% probability of being nice and ML systems have ~0% probability of being nice. I think this argument is just talking about why we shouldn't have update too much from humans, and there is an important background assumption that kindness is super weird and so won't be produced very often by other processes, i.e. the only reason to think it might happen is that it happened in the single case we observed.\n> \n> I find this pretty unconvincing. He lists like 10 things (humans need to trade favors, we're not smart enough to track favors and kinship explicitly, and we tend to be allied with nearby humans so want to be nice to those around us, we use empathy to model other humans, and we had religion and moral realism for contingent reasons, we weren't optimized too much once we were smart enough that our instrumental reasoning screens off kindness heuristics).\n> \n> But no argument is given for why these are *unusually* kindness-inducing settings of the variables. And the outcome isn't like a special combination of all of them, they each seem like factors that contribute randomly. It's just a lot of stuff mixing together.\n> \n> Presumably there is no process that ensures humans have lots of kindness-inducing features (and we didn't select kindness as a property for which humans were notable, we're just asking the civilization-independent question \"does our AI kill us\"). So if you list 10 random things that make humans more kind, it **strongly** suggests that other aliens will also have a bunch of random things that make them more kind. It might not be 10, and the net effect might be larger or smaller. But:\n> \n> *   I have no idea whatsoever how you are anchoring this distribution, and giving it a narrow enough spread to have confident predictions.\n> *   Statements like \"kindness is super weird\" are wildly implausible if you've just listed 5 independent plausible mechanisms for generating kindness. You are making detailed quantitative guesses here, not ruling something out for any plausible a priori reason.\n> \n> As a matter of formal reasoning, listing more and more contingencies that combine apparently-additively tends to decrease rather than increase the variance of kindness across the population. If there was just a single random thing about humans that drove kindness it would be more plausible that we're extreme. If you are listing 10 things then things are going to start averaging out (and you expect that your 10 things are cherry-picked to be the ones most relevant to humans, but you can easily list 10 more candidates).\n> \n> In fact it's easy to list analogous things that could apply to ML (and I can imagine the identical conversation where hypothetical systems trained by ML are talking about how stupid it is to think that evolved life could end up being kind). Most obviously, they are trained in an environment where being kind to humans is a very good instrumental strategy. But they are also trained to closely imitate humans who are known to be kind, they've been operating in a social environment where they are very strongly expected to appear to be kind, etc. Eliezer seems to believe this kind of thing gets you \"ice cream and condoms\" instead of kindness OOD, but just one sentence ago he explained why similar (indeed, superficially much weaker!) factors led to humans retaining niceness out of distribution. I just don't think we have the kind of a priori asymmetry or argument here that would make you think humans are way kinder than models. Yeah it can get you to ~50% or even somewhat lower, but ~0% seems like a joke.\n> \n> There was one argument that I found compelling, which I would summarize as: humans were optimized while they were dumb. If evolution had kept optimizing us while we got smart, eventually we would have stopped being so kind. In ML we just keep on optimizing as the system gets smart. I think this doesn't really work unless being kind is a competitive disadvantage for ML systems on the training distribution. But I do agree that if if you train your AI long enough on cases where being kind is a significant liability, it will eventually stop being kind.\n\n**Nate Soare's reply**\n\n> Short version: I don't buy that humans are \"micro-pseudokind\" in your sense; if you say \"for just $5 you could have all the fish have their preferences satisfied\" I might do it, but not if I could *instead* spend $5 on having the fish have their preferences satisfied in a way that ultimately leads to them ascending and learning the meaning of friendship, as is entangled with the rest of my values.\n> \n> * * *\n> \n> Meta:\n> \n> > Note: I believe that AI takeover has a ~50% probability of killing billions and should be strongly avoided, and would be a serious and irreversible decision by our society that's likely to be a mistake even if it doesn't lead to billions of deaths.\n> \n> So for starters, thanks for making acknowledgements about places we apparently agree, or otherwise attempting to demonstrate that you've heard my point before bringing up other points you want to argue about. (I think this makes arguments go better.) (I'll attempt some of that myself below.)\n> \n> Secondly, note that it sounds to me like you took a diametric-opposite reading of some of my intended emotional content (which I acknowledge demonstrates flaws in my writing). For instance, I intended the sentence \"At that very moment they hear the dinging sound of an egg-timer, as the next-token-predictor ascends to superintelligence and bursts out of its confines\" to be a caricature so blatant as to underscore the point that I wasn't making arguments about takeoff speeds, but was instead focusing on the point about \"complexity\" not being a saving grace (and \"monomaniacalism\" not being the issue here). (Alternatively, perhaps I misunderstand what things you call the \"emotional content\" and how you're reading it.)\n> \n> Thirdly, I note that for whatever it's worth, when I go to new communities and argue this stuff, I don't try to argue people into >95% change we're all going to die in <20 years. I just try to present the arguments as I see them (without hiding the extremity of my own beliefs, nor while particularly expecting to get people to a similarly-extreme place with, say, a 30min talk). My 30min talk targets are usually something more like \">5% probability of existential catastrophe in <20y\". So insofar as you're like \"I'm aiming to get you to stop arguing so confidently for death given takeover\", you might already have met your aims in my case.\n> \n> (Or perhaps not! Perhaps there's plenty of emotional-content leaking through given the extremity of my own beliefs, that you find particularly detrimental. To which the solution is of course discussion on the object-level, which I'll turn to momentarily.)\n> \n> * * *\n> \n> Object:\n> \n> First, I acknowledge that if an AI cares enough to spend one trillionth of its resources on the satisfaction of fulfilling the preferences of existing \"weak agents\" in precisely the right way, then there's a decent chance that current humans experience an enjoyable future.\n> \n> With regards to your arguments about what you term \"kindness\" and I shall term \"pseudokindness\" (on account of thinking that \"kindness\" brings too much baggage), here's a variety of places that it sounds like we might disagree:\n> \n> *   Pseudokindness seems underdefined, to me, and I expect that many ways of defining it don't lead to anything like good outcomes for existing humans.\n>     *   Suppose the AI is like \"I am pico-pseudokind; I will dedicate a trillionth of my resources to satisfying the preferences of existing weak agents by granting those existing weak agents their wishes\", and then only the most careful and conscientious humans manage to use those wishes in ways that leave them alive and well.\n>     *   There are lots and lots of ways to \"satisfy the preferences\" of the \"weak agents\" that are humans. Getting precisely the CEV (or whatever it should be repaired into) is a subtle business. Most humans probably don't yet recognize that they could or should prefer taking their CEV over various more haphazard preference-fulfilments that ultimately leave them unrecognizable and broken. (Or, consider what happens when a pseudokind AI encounters a baby, and seeks to satisfy its preferences. Does it have the baby age?)\n>     *   You've got to do some philosophy to satisfy the preferences of humans correctly. And the issue isn't that the AI *couldn't* solve those philosophy problems correctly-according-to-us, it's that once we see how wide the space of \"possible ways to be pseudokind\" is, then \"pseudokind in the manner that gives us our CEVs\" starts to feel pretty narrow against \"pseudokind in the manner that fulfills our revealed preferences, or our stated preferences, or the poorly-considered preferences of philosophically-immature people, or whatever\".\n> *   I doubt that humans are micro-pseudokind, as defined. And so in particular, all your arguments of the form \"but we've seen it arise once\" seem suspect to me.\n>     *   Like, suppose we met fledgeling aliens, and had the opportunity to either fulfil their desires, or leave them alone to mature, or affect their development by teaching them the meaning of friendship. My guess is that we'd teach them the meaning of friendship. I doubt we'd hop in and fulfil their desires.\n>     *   (Perhaps you'd counter with something like: well if it was super cheap, we might make two copies of the alien civilization, and fulfil one's desires and teach the other the meaning of friendship. I'm skeptical, for various reasons.)\n>     *   More generally, even though \"one (mill|trill)ionth\" feels like a small fraction, the obvious ways to avoid dedicating even a (mill|trill)ionth of your resources to X is if X is right near something even better that you might as well spend the resources on instead.\n>     *   There's all sorts of ways to thumb the scales in how a weak agent develops, and there's many degrees of freedom about what counts as a \"pseudo-agent\" or what counts as \"doing justice to its preferences\", and my read is that humans take one particular contingent set of parameters here and AIs are likely to take another (and that the AI's other-settings are likely to lead to behavior not-relevantly-distinct from killing everyone).\n>     *   My read is than insofar as humans do have preferences about doing right by other weak agents, they have all sorts of desire-to-thumb-the-scales mixed in (such that humans are not actually pseudokind, for all that they might be kind).\n> *   I have a more-difficult-to-articulate sense that \"maybe the AI ends up pseudokind in just the right way such that it gives us a (small, limited, ultimately-childless) glorious transhumanist future\" is the sort of thing that reality gets to say \"lol no\" to, once you learn more details about how the thing works internally.\n> \n> Most of my argument here is that \"the space of ways things can end \"caring\" about the \"preferences\" of \"weak agents\" is wide, and most points within it don't end up being our point in it, and optimizing towards most points in it doesn't end up keeping us around at the extremes. My guess is mostly that the space is so wide that you don't even end up with AIs warping existing humans into unrecognizable states, but do in fact just end up with the people dead (modulo distant aliens buying copies, etc).\n> \n> I haven't really tried to quantify how confident I am of this; I'm not sure whether I'd go above 90%, \\\\shrug.\n> \n> * * *\n> \n> It occurs to me that one possible source of disagreement here is, perhaps you're trying to say something like:\n> \n> > Nate, you shouldn't go around saying \"if we don't competently intervene, literally everybody will die\" with such a confident tone, when you in fact think there's a decent chance of scenarios where the AIs keep people around in some form, and make some sort of effort towards fulfilling their desires; most people don't care about the cosmic endowment like you do; the bluntly-honest and non-manipulative thing to say is that there's a *decent* chance they'll die and a *better* chance that humanity will lose the cosmic endowment (as you care about more than they do),\n> \n> whereas my stance has been more like\n> \n> > most people I meet are skeptical that uploads count as them; most people would consider scenarios where their bodies are destroyed by rapid industrialization of Earth but a backup of their brain is stored and then later run in simulation (where perhaps it's massaged into an unrecognizable form, or kept in an alien zoo, or granted a lovely future on account of distant benefactors, or ...) to count as \"death\"; and also those exotic scenarios don't seem all that likely to me, so it hasn't seemed worth caveating.\n> \n> I'm somewhat persuaded by the claim that failing to mention even the possibility of having your brainstate stored, and then run-and-warped by an AI or aliens or whatever later, or run in an alien zoo later, is potentially misleading.\n> \n> I'm considering adding footnotes like \"note that when I say \"I expect everyone to die\", I don't *necessarily* mean \"without ever some simulation of that human being run again\", although I mostly don't think this is a particularly comforting caveat\", in the relevant places. I'm curious to what degree that would satisfy your aims (and I welcome workshopped wording on the footnotes, as might both help me make better footnotes and help me understand better where you're coming from).\n\n**Paul's reply:**\n\n> I disagree with this but am happy your position is laid out. I'll just try to give my overall understanding and reply to two points.\n> \n> Like Oliver, it seems like you are implying:\n> \n> > Humans may be nice to other creatures in *some* sense, But if the fish were to look at the future that we'd achieve for them using the 1/billionth of resources we spent on helping them, it would be as objectionable to them as \"murder everyone\" is to us.\n> \n> I think that normal people being pseudokind in a common-sensical way would instead say:\n> \n> > If we are trying to help some creatures, but those creatures really dislike the proposed way we are \"helping\" them, then we should try a different tactic for helping them.\n> \n> I think that some utilitarians (without reflection) plausibly *would* \"help the humans\" in a way that most humans consider as bad as being murdered. But I think this is an unusual feature of utilitarians, and most people would consult the beneficiaries, observe they don't want to be murdered, and so not murder them.\n> \n> I think that saying \"Helping someone in a way they like, sufficiently precisely to avoid things like murdering them, requires *precisely* the right form of caring---and that's super rare\" is a really misleading sense of how values work and what targets are narrow. I think this is more obvious if you are talking about how humans would treat a weaker species. If that's the state of the disagreement I'm happy to leave it there.\n> \n> > I'm somewhat persuaded by the claim that failing to mention even the possibility of having your brainstate stored, and then run-and-warped by an AI or aliens or whatever later, or run in an alien zoo later, is potentially misleading.\n> \n> This is an important distinction at 1/trillion levels of kindness, but at 1/billion levels of kindness I don't even think the humans have to die.\n\n**Nate:**\n\n> > If we are trying to help some creatures, but those creatures really dislike the proposed way we are \"helping\" them, then we should do something else.\n> \n> My picture is less like \"the creatures really dislike the proposed help\", and more like \"the creatures don't have terribly consistent preferences, and endorse each step of the chain, and wind up somewhere that they wouldn't have endorsed if you first extrapolated their volition (but nobody's extrapolating their volition or checking against that)\".\n> \n> It sounds to me like your stance is something like \"there's a decent chance that most practically-buildable minds pico-care about correctly extrapolating the volition of various weak agents and fulfilling that extrapolated volition\", which I am much more skeptical of than the weaker \"most practically-buildable minds pico-care about satisfying the preferences of weak agents in *some* sense\".\n\n**Paul:**\n\n> We're not talking about practically building minds right now, we are talking about humans.\n> \n> We're not talking about \"extrapolating volition\" in general.  We are talking about whether---in attempting to help a creature with preferences about as coherent as human preferences---you end up implementing an outcome that creature considers as bad as death.\n> \n> For example, we are talking about what would happen if humans were trying to be kind to a weaker species that they had no reason to kill, that could nevertheless communicate clearly and had preferences about as coherent as human preferences (while being very alien).\n> \n> And those creatures are having a conversation amongst themselves before the humans arrive wondering \"Are the humans going to murder us all?\" And one of them is saying \"I don't know, they don't actually benefit from murdering us and they seem to care a tiny bit about being nice, maybe they'll just let us do our thing with 1/trillionth of the universe's resources?\" while another is saying \"They will definitely have strong opinions about what our society should look like and the kind of transformation they implement is about as bad by our lights as being murdered.\"\n> \n> In practice attempts to respect someone's preferences often involve ideas like autonomy and self-determination and respect for their local preferences. I really don't think you have to go all the way to extrapolated volition in order to avoid killing everyone.\n\n**Nate:**\n\n> Is this a reasonable paraphrase of your argument?\n> \n> > Humans wound up caring at least a little about satisfying the preferences of other creatures, not in a \"grant their local wishes even if that ruins them\" sort of way but in some other intuitively-reasonable manner.\n> > \n> > Humans are the only minds we've seen so far, and so having seen this once, maybe we start with a 50%-or-so chance that it will happen again.\n> > \n> > You can then maybe drive this down a fair bit by arguing about how the content looks contingent on the particulars of how humans developed or whatever, and maybe that can drive you down to 10%, but it shouldn't be able to drive you down to 0.1%, especially not if we're talking only about incredibly weak preferences.\n> \n> If so, one guess is that a bunch of disagreement lurks in this \"intuitively-reasonable manner\" business.\n> \n> A possible locus of disagreemet: it looks to me like, if you give humans power before you give them wisdom, it's pretty easy to wreck them while simply fulfilling their preferences. (Ex: lots of teens have dumbass philosophies, and might be dumb enough to permanently commit to them if given that power.)\n> \n> More generally, I think that if mere-humans met very-alien minds with similarly-coherent preferences, and if the humans had the opportunity to magically fulfil certain alien preferences within some resource-budget, my guess is that the humans would have a pretty hard time offering power and wisdom in the right ways such that this overall went well for the aliens by their own lights (as extrapolated at the beginning), at least without some sort of volition-extrapolation.\n> \n> (I separately expect that if we were doing something more like the volition-extrapolation thing, we'd be tempted to bend the process towards \"and they learn the meaning of friendship\".)\n> \n> That said, this conversation is updating me somewhat towards \"a random UFAI would keep existing humans around and warp them in some direction it prefers, rather than killing them\", on the grounds that the argument \"maybe preferences-about-existing-agents is just a common way for rando drives to shake out\" plausibly supports it to a threshold of at least 1 in 1000. I'm not sure where I'll end up on that front.\n> \n> Another attempt at naming a crux: It looks to me like you see this human-style caring about others' preferences as particularly \"simple\" or \"natural\", in a way that undermines \"drawing a target around the bullseye\"-type arguments, whereas I could see that argument working for \"grant all their wishes (within a budget)\" but am much more skeptical when it comes to \"do right by them in an intuitively-reasonable way\".\n> \n> (But that still leaves room for an update towards \"the AI doesn't necessarily *kill* us, it might merely *warp* us, or otherwise wreck civilization by bounding us and then giving us power-before-wisdom within those bounds or or suchlike, as might be the sort of whims that rando drives shake out into\", which I'll chew on.)\n\nNate and Paul had an additional thread, which initially was mostly some meta on the conversation about what exactly Nate was trying to argue and what exactly Paul was annoyed at. \n\nI'm skipping most of it here for brevity (you can read it [here](https://www.lesswrong.com/posts/2NncxDQ3KBDCxiJiP/cosmopolitan-values-don-t-come-free?commentId=ZiPF7WtdGijZSMLwC))\n\nBut eventually Nate says:\n\n> Thanks! I'm curious for your paraphrase of the opposing view that you think I'm failing to understand.\n\n**Paul says:**\n\n> I think a closer summary is:\n> \n> > Humans and AI systems probably want different things. From the human perspective, it would be better if the universe was determined by what the humans wanted. But we shouldn't be willing to pay huge costs, and shouldn't attempt to create a slave society where AI systems do humans' bidding forever, just to ensure that human values win out. After all, we really wouldn't want that outcome if our situations had been reversed. And indeed we are the beneficiary of similar values-turnover in the past, as our ancestors have been open (perhaps by necessity rather than choice) to values changes that they would sometimes prefer hadn't happened.\n> > \n> > We can imagine really sterile outcomes, like replicators colonizing space with an identical pattern repeated endlessly, or AI systems that want to maximize the number of paperclips. And considering those outcomes can help undermine the cosmopolitan intuition that we should respect the AI we build. But in fact that intuition pump relies crucially on its wildly unrealistic premises, that the kind of thing brought about by AI systems will be sterile and uninteresting. If we instead treat \"paperclip\" as an analog for some crazy weird shit that is alien and valence-less to humans, drawn from the same barrel of arbitrary and diverse desires that can be produced by selection processes, then the intuition pump loses all force. I'm back to feeling like our situations could have been reversed, and we shouldn't be total assholes to the AI.\n> \n> I don't think that requires anything at all about AI systems converging to cosmopolitan values in the sense you are discussing here. I do think it is much more compelling if you accept some kind of analogy between the sorts of processes shaping human values and the processes shaping AI values, but this post (and the references you cite and other discussions you've had) don't actually engage with the substance of that analogy and the kinds of issues raised in my comment are much closer to getting at the meat of the issue.\n> \n> I also think the \"not for free\" part doesn't contradict the views of Rich Sutton. I asked him this question and he agrees that all else equal it would be better if we handed off to human uploads instead of powerful AI.  I think his view is that the proposed course of action from the alignment community is morally horrifying (since in practice he thinks the alternative is \"attempt to have a slave society,\" not \"slow down AI progress for decades\"---I think he might also believe that stagnation is much worse than a handoff but haven't heard his view on this specifically) and that even if you are losing something in expectation by handing the universe off to AI systems it's not as bad as the alternative.\n\n**Nate says:**\n\n> Thanks! Seems like a fine summary to me, and likely better than I would have done, and it includes a piece or two that I didn't have (such as an argument from symmetry if the situations were reversed). I do think I knew a bunch of it, though. And e.g., my second parable was intended to be a pretty direct response to something like\n> \n> > If we instead treat \"paperclip\" as an analog for some crazy weird shit that is alien and valence-less to humans, drawn from the same barrel of arbitrary and diverse desires that can be produced by selection processes, then the intuition pump loses all force.\n> \n> where it's essentially trying to argue that this intuition pump still has force in precisely this case.\n\n**Paul says:**\n\n> To the extent the second parable has this kind of intuitive force I think it comes from: (i) the fact that the resulting values still sound really silly and simple (which I think is mostly deliberate hyperbole), (ii) the fact that the AI kills everyone along the way.\n\nEliezer Briefly Chimes in\n=========================\n\nHe doesn't engage much but says:\n\n> I sometimes mention the possibility of being stored and sold to aliens a billion years later, which seems to me to validly incorporate most all the hopes and fears and uncertainties that should properly be involved, without getting into any weirdness that I don't expect Earthlings to think about validly.\n\nPaul and Oliver\n===============\n\n**Oliver Habryka also replies to Paul, saying:**\n\n> Might write a longer reply at some point, but the reason why I don't expect \"kindness\" in AIs (as you define it here) is that I don't expect \"kindness\" to be the kind of concept that is robust to cosmic levels of optimization pressure applied to it, and I expect will instead come apart when you apply various reflective principles and eliminate any status-quo bias, even if it exists in an AI mind (and I also think it is quite plausible that it is completely absent). \n> \n> Like, different versions of kindness might or might not put almost all of their considerateness on all the different types of minds that could hypothetically exist, instead of the minds that currently exist right now. Indeed, I expect it's more likely than not that I myself will end up in that moral equilibrium, and won't be interested in extending any special consideration to systems that happened to have been alive in 2022, instead of the systems that could have been alive and seem cooler to me to extend consideration towards.\n> \n> Another way to say the same thing is that if AI extends consideration towards something human-like, I expect that it will use some superstimuli-human-ideal as a reference point, which will be a much more ideal thing to be kind towards than current humans by its own lights (for an LLM this might be cognitive processes much more optimized for producing internet text than current humans, though that is really very speculative, and is more trying to illustrate the core idea of a superstimuli-human). I currently think few superstimuli-humans like this would still qualify by my lights to count as \"human\" (though it might by the lights of the AI). \n> \n> I do find the game-theoretic and acausal trade case against AI killing literally everyone stronger, though it does depend on the chance of us solving alignment in the first place, and so feels a bit recursive in these conversations (like, in order for us to be able to negotiate with the AIs, there needs to be some chance we end up in control of the cosmic endowment in the first place, otherwise we don't have anything to bargain with).\n\n**Paul's first response to Habryka **\n\n> Is this a fair summary?\n> \n> > Humans might respect the preferences of weak agents right now, but if they thought about it for longer they'd pretty robustly just want to completely destroy the existing agents (including a hypothetical alien creator) and replace them with something better. No reason to honor that kind of arbitrary path dependence.\n> \n> If so, it seems like you wouldn't be making an argument about AI or aliens at all, but rather an empirical claim about what would happen if humans were to think for a long time (and become more the people we wished to be and so on).\n> \n> That seems like an important angle that my comment didn't address at all. I personally don't believe that humans would collectively stamp out 99% of their kindness to existing agents (in favor of utilitarian optimization) if you gave them enough time to reflect. That sounds like a longer discussion. I also think that if you expressed the argument in this form to a normal person they would be skeptical about the strong claims about human nature (and would be skeptical of doomer expertise on that topic), and so if this ends up being the crux it's worth being aware of where the conversation goes and my bottom line recommendation of more epistemic humility may still be justified.\n> \n> It's hard to distinguish human kindness from arguably decision-theoretic reasoning like \"our positions could have been reversed, would I want them to do the same to me?\" but I don't think the distinction between kindness and common-sense morality and decision theory is particularly important here except insofar as we want to avoid double-counting.\n> \n> (This does call to mind another important argument that I didn't discuss in my original comment: \"kindness is primarily a product of moral norms produced by cultural accumulation and domestication, and there will be no analogous process amongst AI systems.\" I have the same reaction as to the evolutionary psychology explanations. Evidently the resulting kindness extends beyond the actual participants in that cultural process, so I think you need to be making more detailed guesses about minds and culture and so on to have a strong a priori view between AI and humans.)\n\n**Habryka's next reply:**\n\n> > Humans might respect the preferences of weak agents right now, but if they thought about it for longer they'd pretty robustly just want to completely destroy the existing agents (including a hypothetical alien creator) and replace them with something better. No reason to honor that kind of arbitrary path dependence.\n> \n> No, this doesn't feel accurate. What I am saying is more something like: \n> \n> The way humans think about the question of \"preferences for weak agents\" and \"kindness\" feels like the kind of thing that will come apart under extreme optimization, in a similar way to how I expect the idea of \"having a continuous stream of consciousness with a good past and good future is important\" to come apart as humans can make copies of themselves and change their memories, and instantiate slightly changed versions of themselves, etc. \n> \n> The way this comes apart seems very chaotic to me, and dependent enough on the exact metaethical and cultural and environmental starting conditions that I wouldn't be that surprised if I disagree even with other humans on their resulting conceptualization of \"kindness\" (and e.g. one endpoint might be that I end up not having a special preference for currently-alive beings, but there are thousands, maybe millions of ways for this concept to fray apart under optimization pressure).\n> \n> In other words, I think it's plausible that at something like human level of capabilities and within a roughly human ontology (which AIs might at least partially share, though how much is quite uncertain to me), the concept of kindness as assigning value to the extrapolated preferences of beings that currently exist might be a thing that an AI could share. But I expect it to not hold up under reflection, and much greater power, and predictable ontological changes (that I expect any AI go to through as it reaches superintelligence), so that the resulting reflectively stable and optimized idea of kindness will not meaningfully results in current humans genuine preferences being fulfilled (by my own lights of what it means to extrapolate and fulfill someone's preferences). The space of possibilities in which this concept could fray apart seems quite great, and many of the endpoints are unlikely to align with my endpoints of this concept.\n> \n> * * *\n> \n> Edit (some more thoughts): The thing you said feels related to that in that I think my own pretty huge uncertainty about how I will relate to kindness on reflection is evidence that I think iterating on that concept will be quite chaotic and different for different minds. \n> \n> I do want to push back on \"in favor of utilitarian optimization\". That is not what I am saying, or at least it feels somewhat misleading. \n> \n> I am saying that I think it's pretty likely that upon reflection I no longer think that my \"kindness\" goals are meaningfully achieved by caring about the beings alive in 2022, and that it would be more kind, by my own lights, to not give special consideration to beings who happened to be alive right now. This isn't about \"trading off kindness in favor of utilitarian optimization\", it's saying that when you point towards the thing in me that generates an instinct towards kindness, I can imagine that as I more fully realize what that instinct cashes out to in terms of preferences, that it will not result in actually giving consideration to e.g. rats that are currently alive, or would give consideration to some archetype of a rat that is actually not really that much like a rat, because I don't even really know what it means for a rat to want something, and similarly the way the AI relates to the question of \"do humans want things\" will feel similarly underdetermined (and again, these are just concrete examples of how the concept could come apart, not trying to be an exhaustive list of ways the concept could fall apart).\n\n**Paul's Second Response to Oliver:**\n\n> I think some of the confusion here comes from my using \"kind\" to refer to \"respecting the preferences of existing weak agents,\" I don't have a better handle but could have just used a made up word.\n> \n> I don't quite understand your objection to my summary---it seems like you are saying that notions like \"kindness\" (that might currently lead you to respect the preferences of existing agents) will come apart and change in unpredictable ways as agents deliberate. The result is that smart minds will predictably stop respecting the preferences of existing agents, up to and including killing them all to replace them with something that more efficiently satisfies other values (including whatever kind of form \"kindness\" may end up taking, e.g. kindness towards all the possible minds who otherwise won't get to exist).\n> \n> I called this utilitarian optimization but it might have been more charitable to call it \"impartial\" optimization. Impartiality between the existing creatures and the not-yet-created creatures seems like one of the key characteristics of utilitarianism while being very rare in the broader world . It's also \"utilitarian\" in the sense that it's willing to spare *nothing* (or at least not 1/trillion) for the existing creatures, and this kind of maximizing stance is also one of the big defining features of utilitarianism. So I do still feel like \"utilitarian\" is an OK way at pointing at the basic difference between where you expect intelligent minds will end up vs how normal people think about concepts like being nice.\n\n**Habryka's third reply:**\n\n> > I think some of the confusion here comes from my using \"kind\" to refer to \"respecting the preferences of existing weak agents,\" I don't have a better handle but could have just used a made up word.\n> \n> Yeah, sorry, I noticed the same thing a few minutes ago, that I was probably at least somewhat misled by the more standard meaning of kindness. \n> \n> Tabooing \"kindness\" I am saying something like: \n> \n> Yes, I don't think extrapolated current humans assign approximately any value to the exact preference of \"respecting the preferences of existing weak agents\" and I don't really believe that you would on-reflection endorse that preference either. \n> \n> Separately (though relatedly), each word in that sentence sure feels like the kind of thing that I do not feel comfortable leaning on heavily as I optimize strongly against it, and that hides a ton of implicit assumptions, like 'agent' being a meaningful concept in the first place, or 'existing' or 'weak' or 'preferences', all of which I expect I would think are probably terribly confused concepts to use after I had understood the real concepts that carve reality more at its joints, and this means this sentence sounds deceptively simple or robust, but really doesn't feel like the kind of thing whose meaning will stay simple as an AI does more conceptual refinement.\n> \n> > I called this utilitarian optimization but it might have been more charitable to call it \"impartial\" optimization. Impartiality between the existing creatures and the not-yet-created creatures seems like one of the key characteristics of utilitarianism while being very rare in the broader world . It's also \"utilitarian\" in the sense that it's willing to spare *nothing* (or at least not 1/trillion) for the existing creatures, and this kind of maximizing stance is also one of the big defining features of utilitarianism. So I do still feel like \"utilitarian\" is an OK way at pointing at the basic difference between where you expect intelligent minds will end up vs how normal people think about concepts like being nice.\n> \n> The reason why I objected to this characterization is that I was trying to point at a more general thing than the \"impartialness\". Like, to paraphrase what this sentence sounds like to me, it's more as if someone from a pre-modern era was arguing about future civilizations and said \"It's weird that your conception of future humans are willing to do *nothing* for the gods that live in the sky, and the spirits that make our plants grow\". \n> \n> Like, after a bunch of ontological reflection and empirical data gathering, \"gods\" is just really not a good abstraction for things I care about anymore. I don't think \"impartiality\" is what is causing me to not care about gods, it's just that the concept of \"gods\" seems fake and doesn't carve reality at its joints anymore. It's also not the case that I don't care at all about ancient gods anymore (they are pretty cool and I like the aesthetic), but they way I care about them is very different from how I care about other humans. \n> \n> Not caring about gods doesn't feel \"harsh\" or \"utilitarian\" or in some sense like I have decided to abandon any part of my values. This is what I expect it to feel like for a future human to look back at our meta-preferences for many types of other beings, and also what it feels like for AIs that maybe have some initial version of 'caring about others' when they are at similar capability levels to humans. \n> \n> This again isn't capturing my objection perfectly, but maybe helps point to it better. \n\n**Ryan Greenblatt then replies:**\n\n> When I try to interpret your points here, I come to the conclusion that you think humans, upon reflection, would cause human extinction (in favor of resources being used for something else).\n> \n> Or at least that many/most humans would, upon reflection, prefer resources to be used for purposes other than preserving human life (including not preserving human life in simulation). And this holds even if (some of) the existing humans 'want' to be preserved (at least according to a conventional notion of preferences).\n> \n> I think this empirical view seems pretty implausible.\n> \n> That said, I think it's quite plausible that upon reflection, I'd want to 'wink out' any existing copies of myself in favor of using resources better things. But this is partially because I personally (in my current state) would endorse such a thing: if my extrapolated volition thought it would be better to not exist (in favor of other resource usage), my current self would accept that. And, I think it currently seems unlikely that upon reflection, I'd want to end all human lives (in particular, I think I probably would want to keep humans alive who had preferences against non-existence). This applies regardless of trade; it's important to note this to avoid a 'perpetual motion machine' type argument.\n> \n> Beyond this, I think that most or many humans or aliens would, upon reflection, want to preserve currently existing humans or aliens who had a preference against non-existence. (Again, regardless of trade.)\n> \n> Additionally, I think it's quite plausible that most or many humans or aliens will enact various trades or precommitments *prior to reflecting* (which is probably ill-advised, but it will happen regardless). So current preferences which aren't stable under reflection might have a significant influence overall.\n\n**Vladimir Nesov says:**\n\n> Zeroth approximation of pseudokindness is strict nonintervention, reifying the patient-in-environment as a closed computation and letting it run indefinitely, with some allocation of compute. Interaction with the outside world creates vulnerability to external influence, but then again so does [incautious closed computation](https://www.lesswrong.com/posts/fDk9hLDpjeT9gZH6h/membranes-is-better-terminology-than-boundaries-alone?commentId=ARqHmuASX2gRqz42k), as we currently observe with AI x-risk, which is not something beamed in from outer space.\n> \n> Formulation of the kinds of external influences that are appropriate for a particular patient-in-environment is exactly the topic of [membranes/boundaries](https://www.lesswrong.com/tag/boundaries-membranes-technical), this task can be taken as the defining desideratum for the topic. Specifically, the question of which [environments](https://www.lesswrong.com/posts/JYon9nenijSWpGezg/what-is-inside-an-agent-s-membrane-a-brief-abstract-model?commentId=ybLsSoMFCFH3a3EMr) can be put in contact with a particular membrane without corrupting it, hence why I think [membranes are relevant to pseudokindness](https://www.lesswrong.com/posts/Htu55gzoiYHS6TREB/sentience-matters?commentId=ab94QHwzAaDmrvEXK). Naturality of the membranes/boundaries abstraction is linked to naturality of the pseudokindness abstraction.\n> \n> In contrast, the language of preferences/optimization seems to be the wrong frame for formulating pseudokindness, it wants to discuss ways of intervening and influencing, of not leaving value on the table, rather than ways of offering acceptable options that avoid manipulation. It might be possible to translate pseudokindness back into the language of preferences, but this translation would induce a kind of [deontological prior on preferences](https://www.lesswrong.com/posts/87EzRDAHkQJptLthE/but-why-would-the-ai-kill-us?commentId=KuQmgKwyTnquddJLF) that makes the more probable preferences look rather surprising/unnatural from a more preferences-first point of view.\n\nThere were a bunch more comments, but this feels like a reasonable stopping place for priming the \"previous discussion\" pump.\n\n[^8uynyibwduw]: I believe Eliezer later wrote a twitter thread where he said he expects [something like kindness] to be somewhat common among evolved creatures, but ~0 for AIs trained the way we currently do. I don't have the link offhand but if someone finds it I'll edit it in.",
      "plaintextDescription": "Awhile ago, Nate Soares wrote the posts Decision theory does not imply that we get to have nice things and Cosmopolitan values don't come free and But why would the AI kill us? \n\nPaul Christiano put forth some arguments that \"it seems pretty plausible that AI will be at least somewhat 'nice'\", similar to how humans are somewhat nice to animals. There was some back-and-forth. \n\nMore recently we had Eliezer's post ASIs will not leave just a little sunlight for Earth. \n\nI have a sense that something feels \"unresolved\" here. The current comments on Eliezer's post look likely to be rehashing the basics and I'd like to actually make some progress on distilling the best arguments. I'd like it if we got more explicit debate about this.\n\nI also have some sense that the people previously involved (i.e. Nate, Paul, Eliezer) are sort of tired of arguing with each other. But I am hoping someones-or-other end up picking up the arguments here, hashing them out more, and/or writing more distilled summaries of the arguments/counterarguments.\n\nTo start with, I figured I would just literally repeat most of the previous comments in a top-level post, to give everyone another chance to read through them. \n\nWithout further ado, here they are:\n\n\nPaul and Nate\nPaul Christiano re: \"Cosmopolitan Values Don't Come for Free.\"\n\n> I want to keep picking a fight about “will the AI care so little about humans that it just kills them all?” This is different from a broader sense of cosmopolitanism, and moreover I'm not objecting to the narrow claim \"doesn't come for free.\" But it’s directly related to the actual emotional content of your parables and paragraphs, and it keeps coming up recently with you and Eliezer, and I think it’s an important way that this particular post looks wrong even if the literal claim is trivially true.\n> \n> (Note: I believe that AI takeover has a ~50% probability of killing billions and should be strongly avoided, and would be a serious and irreversible decision by our soc",
      "wordCount": 9328
    },
    "tags": [
      {
        "_id": "xknvtHwqvqhwahW8Q",
        "name": "Human Values",
        "slug": "human-values"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "hvj9NGodhva9pKGTj",
    "title": "Struggling like a Shadowmoth",
    "slug": "struggling-like-a-shadowmoth",
    "url": null,
    "baseScore": 189,
    "voteCount": 91,
    "viewCount": null,
    "commentCount": 38,
    "createdAt": null,
    "postedAt": "2024-09-24T00:47:05.030Z",
    "contents": {
      "markdown": "One of my favorite stories growing up was [Star Wars: Traitor](https://www.amazon.com/Traitor-Star-Wars-Jedi-Order/dp/034542865X), by Matthew Stover.\n\nThe book is short, if you want to read it. Spoilers follow. (I took a look at it again recently and I think it didn't obviously hold up as real adult fiction, although quite good if you haven't yet had your mind blown that many times)\n\nOne anecdote from the story has stayed with me and shows up in my worldview from time to time.\n\nThe story begins with \"Jacen Solo has been captured, and is being tortured.\"\n\nHe is being *extremely* tortured. He's in a white room, with a device connected throughout his nervous system, inflicting maximum white-hot agony. Whenever the suffering becomes so great that his body tunes it out, it gives him just enough time to relax to hurt all the more when it abruptly starts up again. \n\nEvery day, an alien woman comes to visit him. She talks to him, during lulls in the torture, about philosophy and life and history. She asks him what he thinks about things. He says \"I think I'm being fucking tortured. Can you please help?\", and she kind of ignores him and goes on talking about philosophy and life and history and stuff. Sometimes he tries more clever ways to pry information out of her, to no avail.\n\nAfter several weeks (or months?) of this, at some point he asks again \"Seriously what the hell? Who are you? What is happening to me? Why are you torturing me, or, not helping? What is going on?\"\n\nShe says: \"Do you know what a shadowmoth is?\"\n\n\"Answer my fucking question\"\n\n\"Do you know what a shadowmoth is?\"\n\n\"Fine, a shadowmoth is a life form native to Coruscant. They're silicon-based, their wingflutes make music once they leave the cocoon and can fly.\"\n\nShe nods, and says \"Once, I saw a shadowmoth in its cocoon, struggling. Like you, I am sensitive to the force – I didn't merely empathize with it, but I felt it. When shadowmoths strive to break free from their cocoon, they are in pain. I felt it's pain. And I felt it's envy of the other moths, already free, flying, making music. I wanted to help it.\"\n\nJacen shudders. \"Oh no, you didn't –\"\n\n\"Yes. I cut it free.\"\n\n\"No –\"\n\n\"It sounds like you know what happened next?\"\n\nJacen says: \"If they don't escape from the cocoon on their own, they'll never be strong enough to fly, and they suffer and starve to death, worse than what they were experiencing in the cocoon.\"\n\n\"Indeed.\"[^m4kuzcus6g9]\n\nThe two of them sit for awhile. The torture machine gives Jacen another wave of White Pain. Then the woman says \"Suppose you did want to help a shadowmoth. What would you do, if you couldn't cut it free? You might sit with it, and let it know that it's not alone, and that it's pain is in the service of it's destiny.\"\n\nAnd then she leaves for the day.\n\nAnd, then, it occurs to Jacen:\n\nIt's been months. They haven't asked him any questions. There is no indication that they particularly want anything from him. There is no indication that the White Hot Torture will ever stop.\n\nWhat would he do, if this was literally the only thing there would ever be? Just him, and the White, and this inscrutable alien woman...\n\n...basically, the only action he *can* do, is to change his relational stance to the torture, and learn to stop caring about it.\n\nThe last sentence of that chapter is \"Jacen Solo begins to eat away at the White.\"\n\n...\n\nA few weeks later, he has basically invented Buddhist Enlightenment from first principles. He feels the pain, but does not suffer. He converses freely with the woman about philosophy and history and life and stuff.\n\nAnd, then, his captors say \"Congratulations. You have passed the first test. You are now ready for the second phase of your training.\"[^sy6o6b4acfm]\n\n* * *\n\nNow, there are a few things I have to say about this.\n\nFirst, to forestall the obvious objections: The transhumanist answer to \"what if painful suffering is necessary to learn valuable life lessons?\" is *\"skill issue.\"* \n\nIf you can't figure out how to let shadowmoths out of cocoons without them suffering and dying, maybe you can just invent better biotech until this is no longer necessary. In meantime, sure, have some equanimity about it. But don't let that equanimity stop you from trying to improve the world.\n\nBut, better biotech might take thousands of years (or, like, at least a decade) to invent, and you might have to deal with a situation in the meanwhile anyway. And I do think there is something powerful here about *never losing your locus of control.* And there’s something important about \"Sometimes, at least for now, the best way to learn some things involves struggling and figuring things out for yourself. Teachers can help point you to the door but you'll have to walk through it yourself.\"\n\nAlso, perhaps the even more obvious first objection: in this story, the woman is out to manipulate Jacen into becoming a different sort of person. In the context of the story, I think that many of the changes to Jacen's character are actually good by Jacen's original lights[^ce0hjams2rw], but to be clear I do not endorse kidnapping people and torturing them even if you can argue it's for their own good somehow. Or, most things that that would directly be a metaphor for[^kftf3hbkncf]. \n\nBut in the context of \"things you actually consented to\", or \"things reality is forcing upon you without anyone else really being involved at all\", there are two particular angles that come up for me a fair amount.\n\n### **The first angle is** [**metastrategy**](https://www.lesswrong.com/posts/cbWoMepny3Jo9XqEr/metastrategic-brainstorming-a-core-building-block-skill)\n\nThe goal of my [rationality training investigation](https://www.lesswrong.com/s/skTdjtx87XKurdvWf) is not to produce people who are good at researching a particular domain, or solving a particular class of problem. If you are working in a known field, the thing to do is find experts in that field and learn from them. Sometimes they may need to improve their pedagogy, or learn to articulate their tacit knowledge better. You can learn to [extract tacit knowledge from people](https://commoncog.com/how-to-learn-tacit-knowledge/), and you can learn to extract more knowledge from textbooks, etc.\n\nBut, for problems [we don't understand yet](https://www.lesswrong.com/posts/CSZnj2YNMKGfsMbZA/specializing-in-problems-we-don-t-understand), there may just not be an established field to learn from.\n\nThere, you need the skill of either inventing new skills, or connecting dots between disparate existing fields that nobody has yet integrated. I think you need some kind of *generalized* research taste, that tells you what to do when nobody in the world is capable of authoritatively giving you the answer, and the specialized research tastes of various fields might be subtly leading you astray, like a physicist assuming they can model everything as spherical cows, or an ML researcher [assuming they can continue to be behaviorist about their research](https://www.lesswrong.com/posts/67fNBeHrjdrZZNDDK/defining-alignment-research#Valuable_property_2__scientific_approach) as they push the frontier towards superintelligence.\n\nFor this reason, my [training](https://www.lesswrong.com/posts/JXcuXhtWoyYNWCt5b/interested-in-cognitive-bootcamp) is very \"shadowmoth-y\". Much of my process is \"make people solve a problem they don't know how to solve, and challenge them to figure it out.\" \n\nI think there is some art to being \"exactly the right amount of cryptic and annoying\", and to finding problems that form a reasonable difficulty curve for people with various different starting skills. I don't know that I've hit this balance well yet. The transhumanist answer to shadowmothism still applies – if I can give people clues that help them learn the right metaskills without being so painful and annoying, all the better. \n\nBut I try, where possible, to only give people the earliest hints on a thought chain, nudging them towards the right metatools to invent/remember the right metatools to find/remember the right object level tools to solve a given problem.[^br94zt0wen5]\n\n### **The second angle is something like \"resilient self-ownership and locus of control.\"**\n\nLong ago, a friend of mine had recently emotionally grokked existential risk – he had known about and \"believed in\" it for years, but somehow this particular week things clicked and he was feeling more of the full force of it. He felt very afraid, and didn't know what to do.\n\nHe asked a mentor for advice. She asked \"What does the fear feel like?\"\n\nHe didn't articulate anything very clearly.\n\nShe followed up with \"Does it feel more like a tiger is chasing you, or more like you want to curl up and hope that your parents come and fix it?\"\n\n\"The second one.\"\n\nShe nodded. \"I think that kind of fear does make sense in the ancestral environment (and often today). Sometimes you're afraid and it *is* the right thing to do to get help, and maybe curl up to conserve resources meanwhile.\n\n\"The thing is, though, that your parents really can't help you here. Mom can’t stop x-risk. So, that kind of fear... it just isn't very useful.\"\n\nI can imagine people for whom this would be exactly the wrong thing to say. But, my friend nodded, and... something subsided in him. Fully understanding what the fear was for, and why it wasn't helpful, allowed him to process and let go of it. \n\n\"Fully understanding an emotion such that you can let it go\" is actually a separate topic, which deserves it's own post. For now, I'm bringing this up because the move of: \"Oh, my \\[parents/friends/society\\] actually really won't save me here, I *have* to somehow change something inside myself instead of waiting, or trying to change something outside myself\" is an important move to be capable of making sometimes.\n\nThis played a role in the first Major Grieving that kicked off my interest in [Deliberate Grieving](https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1) – I thought someone had wronged me. I wanted my peer group to somehow decide either that yes, they had wronged me, and express that judgment. Or, judge that \"actually, so-and-so really didn't actually wrong Raemon, this is Raemon's job to adjust to.\" Or something.\n\nBut, this was during the pandemic, and my peer group was busy dealing with their own problems, and it really didn't make sense to call a tribunal about it. \n\nWhat if social reality *just wasn't going to issue a verdict here?*\n\nWhat would I do?\n\nIn that case, the thing to do was grieve. Recognize that yes, there was some legitimate reason that having Justice here would be right and good, but that it wasn't going to happen. And let go somehow.\n\nMore recently, I was dealing with an exhausting series of problems, and talking to a friend who was experiencing a very similar set of problems. I found myself saying \"I want you... I want you to say that I'm Good. To say that I've done enough.\"[^0tzm0vsid6kc]\n\nAnd my friend sadly said \"Yeah. I also want someone to say that to me, Ray. \n\n\"The thing is, getting that from outside isn't ever really enough. I think you need to somehow become the sort of person who decides for themselves whether they're good enough, and to not need it from some kind of external validation.\"\n\n*Fuck.*\n\nWhat if there would never be someone I trusted who could tell me I was Good Enough, that things were in some sense [Okay](https://www.lesswrong.com/posts/nsLeY7SZjHZYTPK7t/everything-okay)? \n\nWhat if I had to learn to do that myself? \n\nOr, worse, what if I had to really acknowledge that it was the wrong question to ask?\n\n* * *\n\nI think, for many people, it's important at some point to gain a true sort of independence, the way these fables and anecdotes gesture at. The Shadowmoth Struggle is still, often, a necessary part of reality.\n\nThe transhumanist rejoinder is still important. Sometimes your friends can help you. Sometimes you can invent better pedagogical techniques, or just actually tell someone they're Good Enough in a way that lands and that their soul really believes and updates on in an enduring way. We're social creatures. I'm pretty sure my human values ultimately say: it is good for people to help each other in this way.\n\nBut, I think it is also a worthwhile goal, to be the sort of person capable of enduring and growing and striving, even if at the moment you are an island.\n\nDig through the cocoon, such that your wings might make music in the skies of distant planets.[^xiuhh0fb1y]\n\n*        It matters not how strait the gate,*  \n*        How charged with punishments the scroll,*  \n.       *I am the master of my fate,*  \n*        I am the locus of control.*\n\n*                              – William Ernest Henley and Kenzie Amodei*\n\n[^m4kuzcus6g9]: The metaphor of \"moths struggling to escape a cocoon and needing to do so on their own\" also shows up in the TV show Lost, but, in the relevant episode, the metaphor ends here, right before we get to the important bit. \n\n[^sy6o6b4acfm]: The second phase of his training is also fairly interesting. The rest of the book was mostly forgettable (in that I literally have completely forgotten it), although the very last sentence was also important to me so maybe it's worth it for that. \n\n[^ce0hjams2rw]: It's left deliberately unclear – one of the things I liked about the story is that it's one of few stories I've read which do not spell out the moral, it just presents a bunch of things that happened, and leaves the reader to draw their own conclusions about it. \n\n[^kftf3hbkncf]: (See also this comment by Tsvi about why to be wary of this) \n\n[^br94zt0wen5]: I perhaps want to temper all this with \"I'm not sure how well this works – I believe it has helped me, but is not at a point where it's very legibly justified, and it required quite a lot of hours which others haven't replicated\" \n\n[^0tzm0vsid6kc]: I want to add: I have had people tell me this, and it was helpful. Or otherwise help my struggles feel \"seen\" and respected. But there was something impermanent and incomplete about it. \n\n[^xiuhh0fb1y]: (But please do not kidnap and torture people even if you're pretty sure it'll help them self-actualize)",
      "plaintextDescription": "One of my favorite stories growing up was Star Wars: Traitor, by Matthew Stover.\n\nThe book is short, if you want to read it. Spoilers follow. (I took a look at it again recently and I think it didn't obviously hold up as real adult fiction, although quite good if you haven't yet had your mind blown that many times)\n\nOne anecdote from the story has stayed with me and shows up in my worldview from time to time.\n\nThe story begins with \"Jacen Solo has been captured, and is being tortured.\"\n\nHe is being extremely tortured. He's in a white room, with a device connected throughout his nervous system, inflicting maximum white-hot agony. Whenever the suffering becomes so great that his body tunes it out, it gives him just enough time to relax to hurt all the more when it abruptly starts up again. \n\nEvery day, an alien woman comes to visit him. She talks to him, during lulls in the torture, about philosophy and life and history. She asks him what he thinks about things. He says \"I think I'm being fucking tortured. Can you please help?\", and she kind of ignores him and goes on talking about philosophy and life and history and stuff. Sometimes he tries more clever ways to pry information out of her, to no avail.\n\nAfter several weeks (or months?) of this, at some point he asks again \"Seriously what the hell? Who are you? What is happening to me? Why are you torturing me, or, not helping? What is going on?\"\n\nShe says: \"Do you know what a shadowmoth is?\"\n\n\"Answer my fucking question\"\n\n\"Do you know what a shadowmoth is?\"\n\n\"Fine, a shadowmoth is a life form native to Coruscant. They're silicon-based, their wingflutes make music once they leave the cocoon and can fly.\"\n\nShe nods, and says \"Once, I saw a shadowmoth in its cocoon, struggling. Like you, I am sensitive to the force – I didn't merely empathize with it, but I felt it. When shadowmoths strive to break free from their cocoon, they are in pain. I felt it's pain. And I felt it's envy of the other moths, already free, flying, m",
      "wordCount": 2105
    },
    "tags": [
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "JXcuXhtWoyYNWCt5b",
    "title": "Interested in Cognitive Bootcamp?",
    "slug": "interested-in-cognitive-bootcamp",
    "url": null,
    "baseScore": 48,
    "voteCount": 15,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-09-19T22:12:13.348Z",
    "contents": {
      "markdown": "I'm running more 4-day **\"**[**Cognitive Bootcamps**](https://www.lesswrong.com/posts/h5mDx2Mt2P5m9v588/fractal-strategy-workshop-report)**\"** over the next couple months (during Lighthaven [Eternal September](https://www.lighthaven.space/eternalseptember) season). DM me if you're potentially interested (either as an individual, or as a team).\n\nThe workshop is most valuable to people who:\n\n*   control their decisionmaking process (i.e. you decide what projects you or a team work on, rather than working at a day-job on someone else's vision)\n*   are either a) confused about planmaking / have a vague sense that they aren't as strategically ambitious as they could be.\n*   and/or, b) are at a place where it's natural to spend a few days thinking big-picture thoughts before deciding on their next project.\n\nThere's a secondary[^lv56yf5vo8] focus on \"practice solving confusing problems\", which IMO is time well spent, but requires more followup practice to pay off.\n\nI wrote about the previous workshop [here](https://www.lesswrong.com/posts/h5mDx2Mt2P5m9v588/fractal-strategy-workshop-report). Participants said on average they'd have been willing to pay $850 for it, and would have paid $5000 for the ideal, perfectly-tailored-for-them version. My plan is to charge $500/person for the next workshop, and then $1000 for the next one.\n\nI'm most excited to run this for teams, who can develop a shared skillset and accompanying culture. I plan to tailor the workshops for the needs of whichever people show up.\n\nThe dates are not scheduled yet (depends somewhat on when a critical mass of participants are available). DM me if you are interested.\n\nThe skills being taught will be similar to the sort of thing listed in [Skills from a year of Purposeful Rationality Practice](https://www.lesswrong.com/posts/thc4RemfLcM5AdJDa/skills-from-a-year-of-purposeful-rationality-practice) and the [Feedbackloop-first Rationality](https://www.lesswrong.com/s/skTdjtx87XKurdvWf) sequence. My default curriculum is aiming to teach several interrelated related skills you can practice over four days, that build into a coherent metaskill of \"ambitious planning, at multiple timescales.\"\n\nI'm likely updating the curriculum from last time, but for reference and a rough idea of what to expect, here was the structure at the previous workshop:\n\n> **Beforehand:** \n> \n> *   People sent me a short writeup of their current plans for the next 1-2 weeks, and broader plans for the next 1-6 months.\n> \n> **Day 1: Practice skills on quick-feedback exercises**\n> \n> *   Everyone installs the fatebook [chrome](https://chromewebstore.google.com/detail/fatebook-for-chrome/bbmkpfopjckjieficoddmcjmlkggillm)/[firefox](https://addons.mozilla.org/en-US/firefox/addon/fatebook-for-firefox/) extension\n> *   Solve a puzzle with Dots and a Grid with an unspecified goal\n> *   Solve a [GPQA](https://arxiv.org/abs/2311.12022) question with 95% confidence\n> *   Try to [one-shot a Baba is You puzzle](https://www.lesswrong.com/posts/jqb3prwGQjLriq7Lu/exercise-planmaking-surprise-anticipation-and-baba-is-you)\n> *   For both of those puzzles (Baba and GPQA), ask \"[How could I have thought that faster?](https://www.lesswrong.com/posts/rYq6joCrZ8m62m7ej/how-could-i-have-thought-that-faster)\"\n> *   Play a videogame like [Luck Be a Landlord](https://store.steampowered.com/app/1404850/Luck_be_a_Landlord/), and make fermi-calculations about your choices within the game.\n> *   For all exercises, make lots of fatebook predictions about how the exercise will go.\n> \n> **Day 2: Big picture strategic thinking**\n> \n> *   Work through a series of prompts about your big picture plans.\n> *   Write up at least two different big-picture plans that seem compelling\n> *   Think about short-feedback exercises you could do on Day 3\n> \n> **Day 3: Choose your own short exercises, and object-level work**\n> \n> *   Morning: Do concrete exercises/games/puzzles that require some kind of meta-planning skill, that feels useful to you.\n> *   Afternoon: Do object-level work on your *best alternative big picture plan.* This serves two purposes:\n>     *   You get to practice \"applying the method\" on the ~hour timescale\n>     *   You flesh out your second favorite plan, helping you treat it as \"more real\"\n> \n> **Day 4: Consolidation**\n> \n> *   Write up your plan for the next week (considering at least two alternative plans or frames)\n> *   Review how the workshop went together\n> *   Consolidate your takeaways and [Murphitjsu](https://www.lesswrong.com/posts/Htjbj8ystqc2zLtMX/murphyjitsu-an-inner-simulator-algorithm). \n>     *   What practices do you hope to still be trying a week, month, or year from now? Do you predict you'll actually stick with them? Do you endorse that? What can you do to help make things stick.\n> *   Fill out a feedback form\n\n[^lv56yf5vo8]: I started this project oriented around \"find better feedbackloops for solving confusing problems\", and later decided that planmaking was the highest leverage part of the skill tree to focus on.",
      "plaintextDescription": "I'm running more 4-day \"Cognitive Bootcamps\" over the next couple months (during Lighthaven Eternal September season). DM me if you're potentially interested (either as an individual, or as a team).\n\nThe workshop is most valuable to people who:\n\n * control their decisionmaking process (i.e. you decide what projects you or a team work on, rather than working at a day-job on someone else's vision)\n * are either a) confused about planmaking / have a vague sense that they aren't as strategically ambitious as they could be.\n * and/or, b) are at a place where it's natural to spend a few days thinking big-picture thoughts before deciding on their next project.\n\nThere's a secondary[1] focus on \"practice solving confusing problems\", which IMO is time well spent, but requires more followup practice to pay off.\n\nI wrote about the previous workshop here. Participants said on average they'd have been willing to pay $850 for it, and would have paid $5000 for the ideal, perfectly-tailored-for-them version. My plan is to charge $500/person for the next workshop, and then $1000 for the next one.\n\nI'm most excited to run this for teams, who can develop a shared skillset and accompanying culture. I plan to tailor the workshops for the needs of whichever people show up.\n\nThe dates are not scheduled yet (depends somewhat on when a critical mass of participants are available). DM me if you are interested.\n\nThe skills being taught will be similar to the sort of thing listed in Skills from a year of Purposeful Rationality Practice and the Feedbackloop-first Rationality sequence. My default curriculum is aiming to teach several interrelated related skills you can practice over four days, that build into a coherent metaskill of \"ambitious planning, at multiple timescales.\"\n\nI'm likely updating the curriculum from last time, but for reference and a rough idea of what to expect, here was the structure at the previous workshop:\n\n> Beforehand: \n> \n>  * People sent me a short writeup of their cur",
      "wordCount": 662
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "thc4RemfLcM5AdJDa",
    "title": "Skills from a year of Purposeful Rationality Practice",
    "slug": "skills-from-a-year-of-purposeful-rationality-practice",
    "url": null,
    "baseScore": 191,
    "voteCount": 99,
    "viewCount": null,
    "commentCount": 19,
    "createdAt": null,
    "postedAt": "2024-09-18T02:05:58.726Z",
    "contents": {
      "markdown": "A year ago, I started trying to deliberate practice skills that would \"help people figure out the answers to confusing, important questions.\" I experimented with [Thinking Physics](https://www.lesswrong.com/posts/PiPH4gkcMuvLALymK/exercise-solve-thinking-physics) questions, [GPQA questions](https://github.com/idavidrein/gpqa), [Puzzle Games](https://www.lesswrong.com/posts/jqb3prwGQjLriq7Lu/exercise-planmaking-surprise-anticipation-and-baba-is-you) , [Strategy Games](https://www.lesswrong.com/posts/hxhBT89wDBpWcuCW6/forecasting-one-shot-games), and a [stupid twitchy reflex game](https://www.lesswrong.com/posts/baKauxzSqunE6Aakm/feedback-loops-deliberate-practice-and-transfer-learning) I had struggled to beat for 8 years[^ty1bv9b54dn]. Then I went back to my day job and tried figuring stuff out there too.\n\nThe most important skill I was trying to learn was [Metastrategic Brainstorming](https://www.lesswrong.com/posts/cbWoMepny3Jo9XqEr/metastrategic-brainstorming-a-core-building-block-skill) – the skill of looking at a confusing, hopeless situation, and nonetheless brainstorming useful ways to get traction or avoid wasted motion. \n\nNormally, when you want to get good at something, it's great to stand on the shoulders of giants and copy all the existing techniques. But this is challenging if you're trying to solve *important, confusing* problems because there probably isn't (much) established wisdom on how to solve it. You may need to discover techniques that haven't been invented yet, or synthesize multiple approaches that haven't previously been combined. At the very least, you may need to *find* an existing technique buried in the internet somewhere, which hasn't been linked to your problem with easy-to-search keywords, without anyone to help you.\n\nIn the process of doing this, I found a few skills that came up over and over again.\n\nI didn't invent the following skills, but I feel like I \"won\" them in some sense via a painstaking \"throw myself into the deep end\" method. I feel slightly wary of publishing them in a list here, because I think it was useful to me to have to figure out for myself that they were the right tool for the job. And they seem like kinda useful \"entry level\" techniques, that you're more likely to successfully discover for yourself.\n\nBut, I think this is hard enough, and forcing people to discover everything for themselves seems unlikely to be worth it.\n\nThe skills that seemed most general, in both practice and on my day job, are:\n\n1.  Taking breaks/naps\n2.  Working Memory facility\n3.  Patience\n4.  Knowing what confusion/deconfusion feels like\n5.  Actually Fucking Backchain\n6.  **Asking \"what is my goal?\"**\n7.  **Having multiple plans**\n\nThere were other skills I already was tracking, like [Noticing](https://www.lesswrong.com/posts/2n83uRi36KWDC9LyK/training-regime-day-8-noticing), or [Focusing](https://www.lesswrong.com/posts/f3o9ydY7iPjFF2fyk/focusing-1). There were also somewhat more classic \"[How to Solve It](https://en.wikipedia.org/wiki/How_to_Solve_It)\" style tools for breaking down problems. There are also a host of skills I need when translating this all into my day-job, like \"setting reminders for myself\" and \"negotiating with coworkers.\"\n\nBut the skills listed above feel like they stood out in some way as particularly general, and particularly relevant for \"solve confusing problems.\"\n\nTaking breaks, or naps\n----------------------\n\nDifficult intellectual labor is exhausting. During the two weeks I was working on solving Thinking Physics problems, I worked for like 5 hours a day and then was *completely fucked up* in the evenings. Other researchers I've talked to report similar things. \n\nDuring my [workshops](https://www.lesswrong.com/posts/JXcuXhtWoyYNWCt5b/interested-in-cognitive-bootcamp), one of the most useful things I recommended people was \"actually go take a nap. If you don't think you can take a real nap because you can't sleep, go into a pitch black room and lie down for awhile, and the worst case scenario is your brain will mull over the problem in a somewhat more spacious/relaxed way for awhile.\"\n\n*Practical tips: Get yourself a sleeping mask, noise machine (I prefer a fan or air purifier), and access to a nearby space where you can rest. Leave your devices outside the room. *\n\nWorking Memory facility\n-----------------------\n\nOften a topic feels overwhelming. This is often because it's just too complicated to grasp with your raw working memory. But, there are various tools (paper, spreadsheets, larger monitors, etc) that can improve this. And, you can develop the skill of noticing \"okay this isn't fitting in my head, or even on my big monitor – what *would* let it fit in my head?\".\n\nThe \"eye opening\" example of this for me was trying to solve a physics problem that included 3 dimensions (but one of the dimensions was \"time\"). I tried drawing it out but grasping the time-progression was still hard. I came up with the idea of using semi-translucent paper, where I would draw a diagram of what each step looked like on separate pages, and then I could see where different elements were pointed.\n\nI've also found \"spreadsheet literacy\" a recurring skill – google sheets is very versatile but you have to know what all the functions are, have a knack for arranging elements in an easy-to-parse way, etc.\n\n*Practical Tips: Have lots of kinds of paper, whiteboards and writing supplies around. *\n\n*On google sheets:*\n\n*   *You can make collapsible sections, which help with making complex models while also being able to hide away the complexity of sub-parts you aren't modeling. (hotkey: alt-shift-rightarrow)*\n*   *switch between \"display formulas\" and the default \"display the result\" mode *  \n    *(hotkey: ctrl-backtic)*\n\nPatience\n--------\n\nIf I'm doing something confusingly hard, there are times when it feels painful to sit with it, and I'm itchy to pick some solution and get moving. This comes up in two major areas:\n\n*   *Deliberate/purposeful practice.* A key thing here is to be practicing the form perfectly, which requires somehow slowing things down such that you have time to get each moment correct. The urge to rush can undo the practice you just did, by training mistakes, or prevent you from actually successfully practicing at all.\n*   *Launching into a plan, or declaring yourself done, when you are still confused.* Sitting with the uncomfortableness feels very itchy. But vague plans can be completely wrong, resting on confused assumptions.\n\nThere is of course a corresponding virtue of \"just get moving, build up momentum and start learning through iteration.\" The wisdom to tell the difference between \"I'm still confused and need to orient more\" and \"I need to get moving\" is important. But, an important skill there is at least being *capable* of sitting with impatient discomfort, in the situations where that's the right call.\n\n*Practical tips: I dunno I still kinda suck at this one, but I find taking deep breaths, and deliberately reminding myself \"*[*Slow is smooth, smooth is fast*](https://www.lesswrong.com/posts/4FZfzqMtwQZES3eqN/slow-is-smooth-and-smooth-is-fast)*\".*\n\nKnow what deconfusion, or \"having a crisp understanding\" feels like\n-------------------------------------------------------------------\n\nA skill from both Thinking Physics and Baba is You. \n\nWhen I first started Thinking Physics, I would get to a point where \"I dunno, I feel pretty sure, and I can't think of more things to do to resolve my confusion\", and then impatiently roll the dice on checking the answer. Sometimes I'd be right, more often I'd be wrong.\n\nEventually I had a breakthrough where I came up with a crisp model of the problem, and was like \"oh, man, now it would *actually be really surprising* if any of the other answers were true.\" From then on... well, I'd still sometimes got things wrong (mostly due to impatience). But, I could tell when I still had pieces of my model that were vague and unprincipled.\n\nSimilarly in Baba is You: when people don't have a crisp understanding of the puzzle, they tend to grasp and straws and motivatedly-reason their way into accepting [sketchy sounding premises](https://www.lesswrong.com/posts/8ZR3xsWb6TdvmL8kx/optimistic-assumptions-longterm-planning-and-cope). But, the true solution to a level often feels very crisp and clear and inevitable. \n\nLearning to notice this difference in qualia is quite valuable.\n\n*Practical tips: This is where* [*Noticing*](https://www.lesswrong.com/posts/2n83uRi36KWDC9LyK/training-regime-day-8-noticing) *and* [*Focusing*](https://www.lesswrong.com/posts/f3o9ydY7iPjFF2fyk/focusing-1) *are key, but are worthwhile for helping you notice subtle differences in how an idea feels in your mind. *\n\n*Try either making explicit* [*numerical predictions*](https://www.lesswrong.com/posts/wDpXshpakpYDcTtug/fluent-cruxy-predictions-1) *about whether you've solved an exercise before you look up the answer; or, write down a qualitative sentence like \"I feel like I really deeply understand the answer\" or \"this seems probably right but I feel some niggling doubts.\"*\n\nActually Fucking Backchain\n--------------------------\n\nFrom Baba is You, I got the fear-of-god put in me seeing how easy it was to spin my wheels, tinkering around with stuff that was nearby/accessible/easy-to-iterate-with, and how that often turned out to not be at all relevant to beating a level. \n\nI had much less wasted motion when I thought through \"What would the final stages of beating this level need to look like? What are the stages just before those?\", and focusing my attention on things that could help me get to that point.\n\nOne might say \"well, Baba is You is a game optimized for being counterintuitive and weird.\" I think for many people with a goal like \"build a successful startup\", it can sometimes be fine to just be forward chaining with stuff that feels promising, rather than trying to backchain from complex goals.\n\nBut, when I eyeball the realworld problems I'm contending with (i.e. x-risk) they really do seem like there's a relatively narrow set of victory conditions that plausibly work. And, many of the projects I feel tempted to start don't actually really seem that relevant.\n\n(I also think great startup founders are often doing a mix of forward and backward chaining. i.e. I bet Jeff Bezos was like \"okay I bet I could make an online bookstore that worked\", was also thinking \"but, what if I ultimately wanted the Everything Store? What are obstacles that I'd eventually need to deal\")\n\n*Practical tips: First, come up with at least one concrete story of what the world would look like, if you succeeded at your goals. Try hard to come up with 2 other worlds, so you aren't too anchored on your first idea. *\n\n*Then, try to concretely imagine the steps that would come a little bit earlier in the chain from the end.*\n\n*Don't worry about mapping out all the different possible branches of the future (that's impossible). But, for a complex plan, have at least one end-to-end plan that connects all the dots from the resources you have now to the victory condition at the end.*\n\n*Meanwhile, while doing most of your work, notice when it starts to feel like you've lost the plot (try just making a little tally-mark whenever you notice yourself rabbitholing in a way that feels off). And ask \"what is my goal? is what I'm currently doing helping\"*\n\nAsk \"What's My Goal?\"\n---------------------\n\nActually, having just written the previous section, I'm recalling a simpler, more commonly useful skill, which is simply to ask \"what is my goal?\". \n\nOften, doing this throws into relief that you're not sure what your goal is. Sometimes, asking the question immediately prompts me to notice a key insight I'd been glossing over.\n\nIf you're not sure what your goal is, try [babbling](https://www.lesswrong.com/posts/i42Dfoh4HtsCAfXxL/babble) some things that seem like they *might* be a goal, and then ask yourself \"does this feel like what I'm most trying to achieve right now?\"\n\nIt's okay if it turns out your goal is different or more embarrassing-sounding than you thought. You might say \"Actually, you know what? I do care more about showing off and sounding smart, than actually learning something right now.\" (But, you might also realize \"okay, I separately care about learning something *and* sounding smart\"*,* and then be more intentional about finding a tactic that accomplishes both)\n\nOnce you remember (or figure out) your goal, as you [brainstorm strategies](https://www.lesswrong.com/posts/cbWoMepny3Jo9XqEr/metastrategic-brainstorming-a-core-building-block-skill), ask yourself \"would I be surprised if this didn't help me achieve my goals?\", and then prioritize strategies that you viscerally expect to work.\n\nAlways[^yar1uodgath] try to have 3 hypotheses\n---------------------------------------------\n\nThis one is important enough to be it's own post. (I guess, probably most of these are important enough to be a full post? But, this one especially)\n\nBut, listing here for completeness: \n\nWhether you are solving a puzzle, or figuring out *how* to solve a puzzle, or deciding what your team should do next week, try to have multiple hypotheses. (I usually say \"try to have at least 3 plans\", but a plan is basically a special case – a hypothesis about \"doing X is the best way to achieve goal Y\"). \n\nThey each need to be a hypothesis you actually believe in.\n\nI say \"at least 3\", because I think it gets you \"fully intellectually agile.\" If you only have one plan, it's easy to get tunnel vision on it and not notice that it's doomed. Two ideas helps free up your mind, but then you might still evaluate all evidence in terms of \"does this support idea 1 or idea 2?\". If you have 3 different hypotheses, it's much more natural to keep generating more hypotheses, and to pivot around in a multiple dimensional space of possibility.\n\n[^ty1bv9b54dn]: This wasn't practice for \"solving confusing problems\", but it was practice for \"accomplish anything at all through purposeful practice.\" It took 40 hours despite me being IMO very fucking clever about it. \n\n[^yar1uodgath]: Okay not literally always, but, whenever you're about to spend a large chunk of timing on a project or figuring something out.",
      "plaintextDescription": "A year ago, I started trying to deliberate practice skills that would \"help people figure out the answers to confusing, important questions.\" I experimented with Thinking Physics questions, GPQA questions, Puzzle Games , Strategy Games, and a stupid twitchy reflex game I had struggled to beat for 8 years[1]. Then I went back to my day job and tried figuring stuff out there too.\n\nThe most important skill I was trying to learn was Metastrategic Brainstorming – the skill of looking at a confusing, hopeless situation, and nonetheless brainstorming useful ways to get traction or avoid wasted motion. \n\nNormally, when you want to get good at something, it's great to stand on the shoulders of giants and copy all the existing techniques. But this is challenging if you're trying to solve important, confusing problems because there probably isn't (much) established wisdom on how to solve it. You may need to discover techniques that haven't been invented yet, or synthesize multiple approaches that haven't previously been combined. At the very least, you may need to find an existing technique buried in the internet somewhere, which hasn't been linked to your problem with easy-to-search keywords, without anyone to help you.\n\nIn the process of doing this, I found a few skills that came up over and over again.\n\nI didn't invent the following skills, but I feel like I \"won\" them in some sense via a painstaking \"throw myself into the deep end\" method. I feel slightly wary of publishing them in a list here, because I think it was useful to me to have to figure out for myself that they were the right tool for the job. And they seem like kinda useful \"entry level\" techniques, that you're more likely to successfully discover for yourself.\n\nBut, I think this is hard enough, and forcing people to discover everything for themselves seems unlikely to be worth it.\n\nThe skills that seemed most general, in both practice and on my day job, are:\n\n 1. Taking breaks/naps\n 2. Working Memory facility\n",
      "wordCount": 2091
    },
    "tags": [
      {
        "_id": "2wjPMY34by2gXEXA2",
        "name": "Techniques",
        "slug": "techniques"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ehYuLsnoFch5QcaWD",
    "title": "What is SB 1047 *for*?",
    "slug": "what-is-sb-1047-for",
    "url": null,
    "baseScore": 61,
    "voteCount": 27,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2024-09-05T17:39:39.871Z",
    "contents": {
      "markdown": "Emmett Shear [asked on twitter](https://x.com/Raemon777/status/1831748181392257126):\n\n> I think SB 1047 has gotten much better from where it started. It no longer appears actively bad. But can someone who is pro-SB 1047 explain the specific chain of causal events where they think this bill becoming law results in an actual safer world? What’s the theory?\n\nAnd I realized that AFAICT no one has concisely written up what the actual story for SB 1047 is supposed to be.\n\nThis is my current understanding. Other folk here may have more detailed thoughts or disagreements.\n\nThe bill isn't sufficient on it's own, but it's not regulation for regulation's sake because it's specifically a piece of the regulatory machine I'd ultimately want built.\n\nRight now, it mostly solidifies the safety processes that existing orgs have voluntarily committed to. But, we are pretty lucky that they voluntarily committed to them, and we don't have any guarantee that they'll stick with them in the future.\n\nFor the bill to succeed, we do need to invent good, third party auditing processes that are not just a bureaucratic sham. This is an important, big scientific problem that isn't solved yet, and it's going to be a big political problem to make sure that the ones that become consensus are good instead of regulatory-captured. But, figuring that out is one of the major goals of the AI safety community right now.\n\nThe \"Evals Plan\" as I understand it comes in two phase:\n\n1\\. **Dangerous Capability Evals.** We invent evals that demonstrate a model is capable of dangerous things (including manipulation/scheming/deception-y things, and \"invent bioweapons\" type things)\n\nAs I understand it, this is pretty tractable, although labor intensive and \"difficult\" in a normal, boring way.\n\n2\\. **Robust Safety Evals.** We invent evals that demonstrate that a model *capable* of scheming, is nonetheless safe – either because we've proven what sort of actions it will choose to take (AI Alignment), or, we've proven that we can control it even if it is scheming (AI control). AI control is probably easier at first, although limited.\n\nAs I understand it, this is very hard, and while we're working on it it requires new breakthroughs.\n\nThe goal with SB 1047 as I understand is roughly:\n\nFirst: Capability Evals trigger\n-------------------------------\n\nBy the time it triggers for the first time, we have a set of evals that are good enough to confirm \"okay, this model isn't actually capable of being dangerous\" (and probably the AI developers continue unobstructed.\n\nBut, when we first hit a model capable of deception, self-propagation or bioweapon development, the eval will trigger \"yep, this is dangerous.\" And then the government will ask \"okay, how do you know it's not dangerous?\".\n\nAnd the company will put forth some plan, or internal evaluation procedure, that (probably) sucks. And the Frontier Model Board will say \"hey Attorny General, this plan sucks, here's why.\"\n\nNow, the original version of SB 1047 would include the Attorney General saying \"okay yeah your plan doesn't make sense, you don't get to build your model.\" The newer version of the plan I think basically requires additional political work at this phase.\n\nBut, the goal of this phase, is to establish \"hey, we have dangerous AI, and we don't yet have the ability to reasonably demonstrate we can render it non-dangerous\", and stop development of AI until companies reasonably figure out some plans that at \\_least\\_ make enough sense to government officials.\n\nSecond: Advanced Evals are invented, and get woven into law\n-----------------------------------------------------------\n\nThe way I expect a company to prove their AI is safe, despite having dangerous capabilities, is for third parties to invent the a robust version of the second set of evals, and then for new AIs to pass those evals.\n\nThis requires a set of scientific and political labor, and the hope is that by the time we've triggered the \"dangerous\" eval, the government is paying more explicit attention), and it makes it easier to have a conversation about what the longterm plan is.\n\nSB 1047 is the specific tripwire by which the government will be forced to pay more attention at an important time.\n\nMy vague understanding atm is that Biden passed some similar-ish executive orders, but that there's a decent chance Trump reverses them.\n\nSo SB 1047 may be the only safeguard we have for ensuring this conversation happens at the government level at the right time, even if future companies are even less safe-seeming than the current leading labs, or the current leading labs shortchange their current (relatively weak) pseudo-commitments.\n\nCurious if anyone has different takes or more detailed knowledge.\n\n*See* [*this Richard Ngo post*](https://www.lesswrong.com/posts/bDkqMtCpdSKsJQYc6/twitter-thread-on-ai-safety-evals) *on what makes a good eval, which I found helpful.*",
      "plaintextDescription": "Emmett Shear asked on twitter:\n\n> I think SB 1047 has gotten much better from where it started. It no longer appears actively bad. But can someone who is pro-SB 1047 explain the specific chain of causal events where they think this bill becoming law results in an actual safer world? What’s the theory?\n\nAnd I realized that AFAICT no one has concisely written up what the actual story for SB 1047 is supposed to be.\n\nThis is my current understanding. Other folk here may have more detailed thoughts or disagreements.\n\nThe bill isn't sufficient on it's own, but it's not regulation for regulation's sake because it's specifically a piece of the regulatory machine I'd ultimately want built.\n\nRight now, it mostly solidifies the safety processes that existing orgs have voluntarily committed to. But, we are pretty lucky that they voluntarily committed to them, and we don't have any guarantee that they'll stick with them in the future.\n\nFor the bill to succeed, we do need to invent good, third party auditing processes that are not just a bureaucratic sham. This is an important, big scientific problem that isn't solved yet, and it's going to be a big political problem to make sure that the ones that become consensus are good instead of regulatory-captured. But, figuring that out is one of the major goals of the AI safety community right now.\n\nThe \"Evals Plan\" as I understand it comes in two phase:\n\n1. Dangerous Capability Evals. We invent evals that demonstrate a model is capable of dangerous things (including manipulation/scheming/deception-y things, and \"invent bioweapons\" type things)\n\nAs I understand it, this is pretty tractable, although labor intensive and \"difficult\" in a normal, boring way.\n\n2. Robust Safety Evals. We invent evals that demonstrate that a model capable of scheming, is nonetheless safe – either because we've proven what sort of actions it will choose to take (AI Alignment), or, we've proven that we can control it even if it is scheming (AI control). AI contr",
      "wordCount": 781
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "QiyWTcvmChHssFCBx",
        "name": "SB 1047",
        "slug": "sb-1047"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "hxhBT89wDBpWcuCW6",
    "title": "Forecasting One-Shot Games",
    "slug": "forecasting-one-shot-games",
    "url": null,
    "baseScore": 48,
    "voteCount": 19,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-08-31T23:10:05.475Z",
    "contents": {
      "markdown": "<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%)\"><p>Cliff notes:</p><ul><li>You can practice forecasting on videogames you've never played before, to develop the muscles for \"decision-relevant forecasting.\"</li><li>Turn based videogames work best. I recommend \"Luck Be a Landlord\", \"Battle for Polytopia\", or \"Into the Breach.\"</li><li>Each turn, make as many <a href=\"https://fatebook.io/\">Fatebook</a> predictions as you can in 5 minutes, then actually make your decision(s) for the turn.</li><li>After 3 turns, instead of making \"as many predictions as possible\", switch to trying to come up with at least two mutually exclusive actions you might take this turn, and come up with predictions that would inform which action to take.</li></ul><p>Don't forget to follow this up with practicing <a href=\"https://www.lesswrong.com/posts/wDpXshpakpYDcTtug/fluent-cruxy-predictions-1\">forecasting for decisions you're making in \"real life\"</a>, to improve transfer learning. And, watch out for accidentally just getting yourself addicted to videogames, if you weren't already in that boat.</p><p>This is pretty fun to do in groups and makes for a good meetup, if you're into that.</p></td></tr></tbody></table>\n\n* * *\n\nRecently I published [Exercise: Planmaking, Surprise Anticipation, and \"Baba is You\"](https://www.lesswrong.com/posts/jqb3prwGQjLriq7Lu/exercise-planmaking-surprise-anticipation-and-baba-is-you). In that exercise, you try to make a complete plan for solving a puzzle-level in a videogame, without interacting with the world (on levels where you don't know what all the objects in the environment do), and solve it on your first try. \n\nSeveral people reported it pretty valuable (it was [highest rated activity](https://www.lesswrong.com/posts/h5mDx2Mt2P5m9v588/fractal-strategy-workshop-report#Rating_individual_activities_skills) at my metastrategy workshop). But, it's fairly complicated as an exercise, and a single run of the exercise typically takes at least an hour (and maybe several hours) before you get feedback on whether you're \"doing it right.\" \n\nIt'd be nice to have a way to practice [decision-relevant forecasting](https://www.lesswrong.com/posts/wDpXshpakpYDcTtug/fluent-cruxy-predictions-1) with a faster feedback loop. I've been exploring the space of [games that are interesting to \"one-shot\"](https://www.lesswrong.com/posts/DvRBSzFjfaPYBhwmj/one-shot-strategy-games). (i.e. \" try to win on your first playthrough\"), and also exploring the space of exercises that take advantage of your first playthrough of a game. \n\nSo, an additional, much simpler exercise that I also like, is:\n\n*   Play a turn-based game you haven't played before. \n*   Each turn, set a 5 minute timer for making as many predictions as you can about how the game works, what new rules or considerations you might learn later. Then, a 1 minute timer for actually making your choices for what action(s) to take during the turn.\n\nAnd... that's it. (to start with, anyway). \n\nRather that particularly focusing on \"trying really hard to win\", start with just making lots of predictions, about a situation where you're at least trying to win *a little*, so you can develop the muscles of noticing what sort of predictions you *can* make while you're in the process of strategically orienting. And, notice what sorts of implicit knowledge you have, even though you don't technically \"know\" how the game would work.\n\nSome of the predictions might resolve the very next turn. Some might resolve *before* the very next turn, depending on how many choices you get each turn. And, some might take a few turns, or even pretty deep into the game. Making a mix of forecasts of different resolution-times is encouraged.\n\nI think there are a lot of interesting skills you can layer on top of this, after you've gotten the basic rhythm of it. But  \"just make a ton of predictions about a domain where you're trying to achieve something, and get quick feedback on it\" seems like a good start.\n\nChoosing Games\n==============\n\nNot all games are a good fit for this exercise. I've found a few specific games I tend to recommend, and some principles for which games to pick.\n\nThe ideal game has:\n\n*   **Minimal (or skippable) tutorial.** A major point of the exercise is to make prediction about the game mechanics and features. Good games for this exercise a) don't spoonfeed you all the information about the game, but also b) are self-explanatory enough to figure out without a tutorial.\n*   **Turn based (or, with natural stopping points before decisions).** This isn't a hard rule, but this exercise is meant to build up your mental muscles for practical forecasting on longterm goals. You *can* practice it on twitchy reflex games but it's adding an extra layer of difficulty without much benefit.\n*   **Relatively deep strategy of some kind.** The game should reward thinking ahead about what future choices you're likely to have. Most of my exploration here has been on [strategy games](https://www.lesswrong.com/posts/DvRBSzFjfaPYBhwmj/one-shot-strategy-games) and puzzle games.\n*   **Information you don't initially know (but which is reasonably predictable).** Some strategy games frontload *all* the information on you, and then this is just an exercise in reading every single menu and tooltip. This might *also* be useful for rationality practice, but it's not the point of this exercise. Ideal games here give you enough information at a time to chew on, but not overwhelm you. And, they leave room to make predictions in the future.\n*   **Short playtime.** You don't *have* to complete a whole game in order to get value out of this exercise, but it's useful if the game is short, such that you get to make predictions about how the midgame and endgame might play out, and then get results within an hour or so.\n\nOne question is \"what counts as a 'turn?'\", which varies a bit from game to game. Some games allow basically one action per turn. Some have multiple possible actions within a turn (which might be irreversible, and resolve a prediction you just made). \n\nUnfortunately, it's not always easy to tell what games will satisfy the above criteria, but you can kinda get a feel for it. \n\nSome recommended games\n----------------------\n\n[**Luck be a landlord**](https://store.steampowered.com/app/1404850/Luck_be_a_Landlord/) is a very simple game, which makes for a good streamlined experience. I especially recommend it for people new to either gaming or forecasting. At the very beginning of the game, I recommend making at least one prediction per button click, and then within a minute or so you'll have probably figured out \"what is a turn?\", and can continue with the \"5 minutes per turn, generate as many predictions as you can\" rule.\n\n[**The Battle of Polytopia**](https://store.steampowered.com/app/874390/The_Battle_of_Polytopia/) is a more complex strategy game. One caveat: by default the game will start you out with a tutorial, defeating the exercise. But, you can cancel this tutorial and return to the main menu, and start a new game. (I recommend setting the difficulty to something that will be a challenge. I suggest giving yourself 3 opponents, on a \"hard\" difficulty).\n\n[**Into the Breach**](https://store.steampowered.com/app/590380/Into_the_Breach/)  isn't quite as good as the previous few games (the game becomes more predictable sooner). But if you've already played the first two it's still a good option. Note that it has a somewhat longer \"opening story/context\" section. I recommend starting the exercise after \"your three dudes drop from the dropship.\"\n\nHow to make predictions\n-----------------------\n\nAs always, I recommend [Fatebook.io](https://fatebook.io) as a good tool for quickly making lightweight predictions, while having useful integrations that also make it a good longterm power tool. (See my writeup in [Fluent, Cruxy Predictions](https://www.lesswrong.com/posts/wDpXshpakpYDcTtug/fluent-cruxy-predictions-1) for how to use Fatebook more generally)\n\nIf you've installed the [Fatebook chrome/firefox extension](https://fatebook.io/extension), you can make predictions in a google-doc or similar while also taking general notes about the game. (Warning: doesn't work with adblock, which by default includes browsers like Brave.)\n\nIf you don't like Fatebook, I recommend writing them somewhere that makes it easy to followup and grade the predictions and see your calibration curve.. I think there's a significant differnence between simply making the prediction, and forcing yourself to grade it and starting to establish what your calibration curve looks like.\n\nPhase 2: Decision-Relevant Predictions\n======================================\n\nAfter you've gotten a handle on the basics of \"make as many predictions as you can\", I recommend layering on \"specifically make predictions that help inform your decisions.\"\n\nUltimately, the goal is to make [fluent, cruxy](https://www.lesswrong.com/posts/wDpXshpakpYDcTtug/fluent-cruxy-predictions-1)[^p75vrimy9l][ predictions](https://www.lesswrong.com/posts/wDpXshpakpYDcTtug/fluent-cruxy-predictions-1) about choices that matter to you. \n\nAfter a few turns (typically I recommend 4-5 turns, but, depends on your personal experience), each turn, try to think of 2-3 mutually exclusive decisions that seem plausibly like \"the right move.\" Figure out which move seems best to you.\n\nThen, try to come up with an prediction that would change your mind about which move is best. The prediction can be about concrete facts about the game, or updates you might make to your strategic frame. \n\nSome example predictions (phrased as concrete statements, which you can assign a probability to)\n\n*   \"A particular resource is will be my bottleneck in a few turns.\"\n*   \"The game will introduce \\[some particular new element\\].\"\n*   \"At the end of the game, I'll think it was useful to think of \\[X\\] as my intermediate goal, at the current stage of the game.\"\n\nSometimes, you'll be finding a crux between your favorite plan and your second-favorite plan. Sometimes, you'll end up deciding between the first and third-best-seeming plan, because while it seems worse under your mainline assumptions, it's more likely for you to end up getting surprised in a way that changes your whole strategic frame than to discover the second-best option is better than your favorite.\n\nInterlude: Metastrategy Brainstorming\n-------------------------------------\n\nAfter 2-3 rounds of Cruxy Predicting, I recommend setting a 7 minute timer to explicit ask yourself:\n\n\"What actually am I trying to do with this exercise? And, how could I do it better?\". Brainstorm as many strategies or considerations that might help you get more value out of the exercise.\n\nIf you're running this as a meetup, afterward the brainstorming, it's nice for people to share their ideas and discuss it.\n\nSee [\"Metastrategic Brainstorming\"](https://www.lesswrong.com/posts/cbWoMepny3Jo9XqEr/metastrategic-brainstorming-a-core-building-block-skill) for more detail here. I think this is important because:\n\n*   the skill of \"what am I trying to do and how can I do it better\" is a generally important skill that should interweave with all rationality training\n*   it shifts you from a mindset of \"I'm trying to do this exercise without quite understanding it\" to being a more of an agent who's trying to do something on purpose, which makes you more likely to actually learn and internalize useful things.\n*   It makes it more likely for you to \"adjust your seat\", adapting the exercise to whatever is most helpful for you\n*   my experience is that with 7 minutes, I just concretely generate at least 1 strategy I hadn't previously been considering that is decently helpful.\n\nAfter your brainstorming, continue for another few rounds of cruxy prediction and see if you notice any changes. And, eventually, try the exercise with a different game and see if it plays differently.\n\n\"Okay I basically get it\" vs \"Predict how masters think.\"\n---------------------------------------------------------\n\nThe first few rounds of a new game have a lot of surprise factor. Every few minutes a new element is introduced.\n\nFrequently, once people get midway into a game, they feel like \"okay, I get it\", and they find themselves shifting into a mode where they are more like \"just playing the game\" than doing any deliberate rationality exercise.\n\nI'm not sure what to think about this. I feel this impulse myself. But, I notice that there are tons of skills, concepts and frameworks that experts at a game tend to use, that I wouldn't have thought about on my own. \n\nI haven't actually had success with this yet. (I played a bunch of Slay the Spire without having looked up what Jorbs had to say about it, and failed to actually figure out higher-level principles on my own before accidentally reading some bits about them on the internet from other people).\n\nBut, I still feel hopeful about \"predict what concepts a master would use\" as lens worth thinking about. Once you feel like \"you get it\" for the basic concepts of a game, I recommend asking yourself \"what skills, concepts or frameworks do I predict a master would employ here?\". \n\nFinally: Remember to connect this with \"Real Life\" practice\n===========================================================\n\nSome people ask me \"So, do you think getting good at predicting strategy games automatically makes you good at winning at life? That seems sketchy to me. Most people good at games don't seem to be sitting on top of a heap of utility.\"\n\nTo which I say: \"No, not automatically. It seems like transfer learning is pretty difficult. [Tons of psychologists have tried to discover metalearning or transfer learning over a hundred years and failed](https://www.greaterwrong.com/posts/baKauxzSqunE6Aakm/feedback-loops-deliberate-practice-and-transfer-learning), despite being very biased in favor of thinking it should be possible. I... actually do think those hundred years worth of psychologists did a bunch of dumb things and think my ideas make more sense. But, it's a pretty cautionary tale.\n\nWhat I think does work is following up your Toy Exercises with real life practice. Look at your upcoming week and make some predictions about it. See if you can come up with two mutually exclusive ways of spending your time, and make predictions about what would change your mind about which one to prioritize.\n\nActively ask yourself, which of the mental motions that you learned while practicing on Quick Feedbackloop video games apply to your real life.\n\n[^p75vrimy9l]: I'm using \"decision-relevant\" and \"cruxy\" as roughly interchangeable. I've noticed that \"decision-relevant\" is a more accessible phrase, although I think \"crux\" and \"cruxy\" are useful concepts to have a short handle for and worth introducing to people.I'm splitting the difference by using both here.",
      "plaintextDescription": "Cliff notes:\n\n * You can practice forecasting on videogames you've never played before, to develop the muscles for \"decision-relevant forecasting.\"\n * Turn based videogames work best. I recommend \"Luck Be a Landlord\", \"Battle for Polytopia\", or \"Into the Breach.\"\n * Each turn, make as many Fatebook predictions as you can in 5 minutes, then actually make your decision(s) for the turn.\n * After 3 turns, instead of making \"as many predictions as possible\", switch to trying to come up with at least two mutually exclusive actions you might take this turn, and come up with predictions that would inform which action to take.\n\nDon't forget to follow this up with practicing forecasting for decisions you're making in \"real life\", to improve transfer learning. And, watch out for accidentally just getting yourself addicted to videogames, if you weren't already in that boat.\n\nThis is pretty fun to do in groups and makes for a good meetup, if you're into that.\n\n----------------------------------------\n\nRecently I published Exercise: Planmaking, Surprise Anticipation, and \"Baba is You\". In that exercise, you try to make a complete plan for solving a puzzle-level in a videogame, without interacting with the world (on levels where you don't know what all the objects in the environment do), and solve it on your first try. \n\nSeveral people reported it pretty valuable (it was highest rated activity at my metastrategy workshop). But, it's fairly complicated as an exercise, and a single run of the exercise typically takes at least an hour (and maybe several hours) before you get feedback on whether you're \"doing it right.\" \n\nIt'd be nice to have a way to practice decision-relevant forecasting with a faster feedback loop. I've been exploring the space of games that are interesting to \"one-shot\". (i.e. \" try to win on your first playthrough\"), and also exploring the space of exercises that take advantage of your first playthrough of a game. \n\nSo, an additional, much simpler exercise that I",
      "wordCount": 2178
    },
    "tags": [
      {
        "_id": "DWWZwkxTJs4d5WrcX",
        "name": "Exercises / Problem-Sets",
        "slug": "exercises-problem-sets"
      },
      {
        "_id": "8daMDi9NEShyLqxth",
        "name": "Forecasting & Prediction",
        "slug": "forecasting-and-prediction"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "SuNa5usgxRrvAtbyH",
    "title": "LessWrong email subscriptions?",
    "slug": "lesswrong-email-subscriptions",
    "url": null,
    "baseScore": 26,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-08-27T21:59:56.855Z",
    "contents": {
      "markdown": "Right now, LessWrong has a fair amount of email subscription options, but they're given in a giant list in your user profile that's kind of annoying to look at.[^lpout25dc1]\n\nI have a sense that some people – especially newer users, or users who just don't have a habit of coming to LessWrong all the time – might benefit from having more subscriptions via email (including basic \"subscriptions\" like \"replies to my comment\"). But, also, it's very easy for email to become pretty spammy and for people to tune-it-out completely.\n\nIf you have any plausibly email-shaped LessWrong related holes in your life, let me know.\n\nSome examples of ideas here (inline-react with an \"thumbs-up\" for features that you'd use)\n\n*   Better visibility of the \"Subscribe to posts with >N karma\" feature (this is available in the \"subscribe\" button on the sidebar).\n*   Generally being able to subscribe to more things \"if they get upvoted\", rather than for every single instance.\n*   Batching of various email types into a digest that comes once a day/week.\n*   A monthly update about top posts and comments on LessWrong\n*   \"Reading group\" emails that will email you posts from The Sequences ~once a week (maybe with an option to be synced up with other people so you're all encouraged to comment and discuss it at the same time)\n*   More convenient options for shifting your subscription (i.e. maybe when you write comments, there's a toggle for \"subscribe to new comments via email\" which is default \"on\" for new users but \"off\" for established users who visit frequently)\n*   Better organization of the existing Notifications user options so it's not so overwhelming.\n\nLet me know if you have other ideas in the comments.\n\n[^lpout25dc1]: It's pretty horrible. It doesn't even fit on one screen:",
      "plaintextDescription": "Right now, LessWrong has a fair amount of email subscription options, but they're given in a giant list in your user profile that's kind of annoying to look at.[1]\n\nI have a sense that some people – especially newer users, or users who just don't have a habit of coming to LessWrong all the time – might benefit from having more subscriptions via email (including basic \"subscriptions\" like \"replies to my comment\"). But, also, it's very easy for email to become pretty spammy and for people to tune-it-out completely.\n\nIf you have any plausibly email-shaped LessWrong related holes in your life, let me know.\n\nSome examples of ideas here (inline-react with an \"thumbs-up\" for features that you'd use)\n\n * Better visibility of the \"Subscribe to posts with >N karma\" feature (this is available in the \"subscribe\" button on the sidebar).\n * Generally being able to subscribe to more things \"if they get upvoted\", rather than for every single instance.\n * Batching of various email types into a digest that comes once a day/week.\n * A monthly update about top posts and comments on LessWrong\n * \"Reading group\" emails that will email you posts from The Sequences ~once a week (maybe with an option to be synced up with other people so you're all encouraged to comment and discuss it at the same time)\n * More convenient options for shifting your subscription (i.e. maybe when you write comments, there's a toggle for \"subscribe to new comments via email\" which is default \"on\" for new users but \"off\" for established users who visit frequently)\n * Better organization of the existing Notifications user options so it's not so overwhelming.\n\nLet me know if you have other ideas in the comments.\n\n 1. ^\n    It's pretty horrible. It doesn't even fit on one screen:",
      "wordCount": 287
    },
    "tags": [
      {
        "_id": "MfpEPj6kJneT9gWT6",
        "name": "Site Meta",
        "slug": "site-meta"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "zHLbnekuQnYotDhHz",
    "title": "Please stop using mediocre AI art in your posts",
    "slug": "please-stop-using-mediocre-ai-art-in-your-posts",
    "url": null,
    "baseScore": 115,
    "voteCount": 75,
    "viewCount": null,
    "commentCount": 25,
    "createdAt": null,
    "postedAt": "2024-08-25T00:13:52.890Z",
    "contents": {
      "markdown": "*Epistemic Status: Old Man Blogs at Cloud*\n\nLately there's been a wave of people on LessWrong (and maybe the whole internet) starting off their essays with some Dall-E3 art.\n\nI don't object (and think it can be cool if done nicely) to get some good ML art to add some visual interest to your posts. But, the default thing I'm seeing is mediocre and making the web feel tacky. \n\nI have an art background, which means I both have a lot of experience making/evaluating art and also probably have random-ass [snooty connoisseurship syndrome](https://xkcd.com/915/). Also I was involved with the fleshing out of the current [LessWrong Watercolor Aesthetic](https://www.lesswrong.com/leastwrong), and random clip-art looking images undercut it. (But, to be fair nobody actually voted for or opted into the LessWrong Watercolor Aesthetic, and maybe that's just my problem).\n\nI think not all posts need art. But, if you do want art for your post, here's some hopefully slightly constructive advice/requests.\n\n1\\. Generally, make landscape (widescreen) art.\n===============================================\n\nMost image models output square images by default. This actually fits fairly awkwardly into blogposts – it either takes up a huge amount of vertical space, or you shrink it to fit and then it has weird padding. (The motivating instance for this blogpost was [this post](https://lesswrong.com/posts/hzt9gHpNwA2oHtwKX/self-other-overlap-a-neglected-approach-to-ai-alignment), which IMO would be greatly improved if the image was designed to take up more horizontal space).\n\nSometimes a post makes good use of a very tall art, to set some kind of mood, but it works better for more contemplative posts. (See [On Green](http://lesswrong.com/posts/gvNnE6Th594kfdB3z/on-green) for an example).\n\n2\\. Good AI art still takes a fair number of generations and effort.\n====================================================================\n\nI'm not sure how many generations people typically do that outputs that mediocre art, but I do want to note, for reference, when I'm making a quick piece of AI art for something (like a facebook event) it still usually involves at least like 5 generations (in Midjourney, where each generation includes 4 options), and often more like 20. And when I'm trying to make actually good art for something I want people to really appreciate (such as the [Carving of Reality](https://www.amazon.com/dp/B0C95MJJBK) books), it might be hundreds of generations.\n\n![](https://39669.cdn.cke-cs.com/rQvD3VnunXZu34m86e5f/images/2c3bc331caa9d5ba27a47491061e10554e0911613f38112d.png)\n\nThis was 15 generations in DallE3, plus several generations in Midjourney,   \nand then some post-processing in photoshop.\n\n3\\. Think about the mood you want to convey, not just the posts' intellectual content.\n======================================================================================\n\nGood art sets a tone and helps shift someone into a particular headspace. This doesn't just include the content of the art but the style and color palette. \n\nThis is particularly important if you're opening the post with art, where it's setting the very first impression (and is also more likely to show up in the Recent Discussion feed, where it'll look more random).\n\nThat's probably not very helpful advice on it's own. Making good art is, like, a whole-ass skill. But, on the offchance you weren't thinking about that at all, maybe giving it at least some consideration will probably help.\n\n* * *\n\nOkay, that's it I guess thank you for indulging my rant.",
      "plaintextDescription": "Epistemic Status: Old Man Blogs at Cloud\n\nLately there's been a wave of people on LessWrong (and maybe the whole internet) starting off their essays with some Dall-E3 art.\n\nI don't object (and think it can be cool if done nicely) to get some good ML art to add some visual interest to your posts. But, the default thing I'm seeing is mediocre and making the web feel tacky. \n\nI have an art background, which means I both have a lot of experience making/evaluating art and also probably have random-ass snooty connoisseurship syndrome. Also I was involved with the fleshing out of the current LessWrong Watercolor Aesthetic, and random clip-art looking images undercut it. (But, to be fair nobody actually voted for or opted into the LessWrong Watercolor Aesthetic, and maybe that's just my problem).\n\nI think not all posts need art. But, if you do want art for your post, here's some hopefully slightly constructive advice/requests.\n\n\n1. Generally, make landscape (widescreen) art.\nMost image models output square images by default. This actually fits fairly awkwardly into blogposts – it either takes up a huge amount of vertical space, or you shrink it to fit and then it has weird padding. (The motivating instance for this blogpost was this post, which IMO would be greatly improved if the image was designed to take up more horizontal space).\n\nSometimes a post makes good use of a very tall art, to set some kind of mood, but it works better for more contemplative posts. (See On Green for an example).\n\n\n2. Good AI art still takes a fair number of generations and effort.\nI'm not sure how many generations people typically do that outputs that mediocre art, but I do want to note, for reference, when I'm making a quick piece of AI art for something (like a facebook event) it still usually involves at least like 5 generations (in Midjourney, where each generation includes 4 options), and often more like 20. And when I'm trying to make actually good art for something I want people to really",
      "wordCount": 512
    },
    "tags": [
      {
        "_id": "KDpqtN3MxHSmD4vcB",
        "name": "Art",
        "slug": "art"
      },
      {
        "_id": "MfpEPj6kJneT9gWT6",
        "name": "Site Meta",
        "slug": "site-meta"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tXRwp42jjABrgRXzo",
    "title": "Would you benefit from, or object to, a page with LW users' reacts?",
    "slug": "would-you-benefit-from-or-object-to-a-page-with-lw-users",
    "url": null,
    "baseScore": 23,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-08-20T16:35:47.568Z",
    "contents": {
      "markdown": "There is currently an admin-only page that shows a list of all comments that have been reacted to (in chronological order). Periodically I think \"it might just be nice to show this to everyone, and to let them filter by individual reacts, or individual users.\"\n\nThe reason individual reacts might be nice is to filter for things like \"what comments/posts got the 'changed my mind?' react\", or other reacts that stand out as some kind of an important signal.\n\nThe reason being able to follow an individual's reacts might be nice is that you might think an individual has interesting taste, and want to see things they publicly reacted to, browsing it for good reading. (I know people who used to follow individual's likes on twitter as a particularly good way to get a curated feed)\n\nThe reason you might *not* want this is that, while reacts are public, making it super legible and easy to follow a given individual might make people more self-conscious about reacting to things, and think of it as more of a performative act. \n\nThere are degrees of accessibility, ranging from \"on a user's profile there's a button you can click taking you to a public react page filtered for that user\" to \"there's an obscure link somewhere to the `/reacts` page that a small percentage of users find out about and use.\"\n\nCurious what people think about this. Does the cost seem significant? Does the upside seem significant?\n\nEdit: also, what do you think about having a page that lets you see recent reacts of a given type, but *doesn't* let you filter by user?",
      "plaintextDescription": "There is currently an admin-only page that shows a list of all comments that have been reacted to (in chronological order). Periodically I think \"it might just be nice to show this to everyone, and to let them filter by individual reacts, or individual users.\"\n\nThe reason individual reacts might be nice is to filter for things like \"what comments/posts got the 'changed my mind?' react\", or other reacts that stand out as some kind of an important signal.\n\nThe reason being able to follow an individual's reacts might be nice is that you might think an individual has interesting taste, and want to see things they publicly reacted to, browsing it for good reading. (I know people who used to follow individual's likes on twitter as a particularly good way to get a curated feed)\n\nThe reason you might not want this is that, while reacts are public, making it super legible and easy to follow a given individual might make people more self-conscious about reacting to things, and think of it as more of a performative act. \n\nThere are degrees of accessibility, ranging from \"on a user's profile there's a button you can click taking you to a public react page filtered for that user\" to \"there's an obscure link somewhere to the /reacts page that a small percentage of users find out about and use.\"\n\nCurious what people think about this. Does the cost seem significant? Does the upside seem significant?\n\nEdit: also, what do you think about having a page that lets you see recent reacts of a given type, but doesn't let you filter by user?",
      "wordCount": 271
    },
    "tags": [],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "8ZR3xsWb6TdvmL8kx",
    "title": "Optimistic Assumptions, Longterm Planning, and \"Cope\"",
    "slug": "optimistic-assumptions-longterm-planning-and-cope",
    "url": null,
    "baseScore": 215,
    "voteCount": 93,
    "viewCount": null,
    "commentCount": 46,
    "createdAt": null,
    "postedAt": "2024-07-17T22:14:24.090Z",
    "contents": {
      "markdown": "Eliezer Yudkowsky periodically [complains](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy#Q2___I_have_a_clever_scheme_for_saving_the_world___I_should_act_as_if_I_believe_it_will_work_and_save_everyone__right__even_if_there_s_arguments_that_it_s_almost_certainly_misguided_and_doomed___Because_if_those_arguments_are_correct_and_my_scheme_can_t_work__we_re_all_dead_anyways__right_) about people coming up with questionable plans with questionable assumptions to deal with AI, and then either:\n\n*   Saying \"well, if this assumption doesn't hold, we're doomed, so we might as well assume it's true.\"\n*   Worse: coming up with cope-y reasons to assume that the assumption isn't even questionable at all. It's just a pretty reasonable worldview.\n\nSometimes the questionable plan is \"an alignment scheme, which Eliezer thinks avoids the hard part of the problem.\" Sometimes it's a sketchy reckless plan that's probably going to blow up and make things worse.\n\nSome people complain about Eliezer being a doomy Negative Nancy who's overly pessimistic.\n\nI had an interesting experience a few months ago when I ran some beta-tests of my [Planmaking and Surprise Anticipation](https://www.lesswrong.com/posts/jqb3prwGQjLriq7Lu/exercise-planmaking-surprise-anticipation-and-baba-is-you) workshop, that I think are illustrative.\n\n* * *\n\ni. Slipping into a more Convenient World\n========================================\n\nI have an exercise where I give people the instruction to play a puzzle game (\"Baba is You\"), but where you normally have the ability to move around and interact with the world to experiment and learn things, instead, you need to make a complete plan for solving the level, and you aim to get it right on your first try.\n\nIn the exercise, I have people write down the steps of their plan, and assign a probability to each step. \n\nIf there is a part of the puzzle-map that you aren't familiar with, you'll have to make guesses. I recommend making 2-3 guesses for how a new mechanic might work. (I don't recommend making a massive branching tree for every possible eventuality. For the sake of the exercise not taking forever, I suggest making 2-3 branching path plans)\n\nSeveral months ago, I had three young-ish alignment researchers do this task (each session was a 1-1 with just me and them).\n\nThe participants varied in how experienced they were with Baba is You. Two of them were new to the game, and completed the the first couple levels without too much difficulty, and then got to a harder level. The third participant had played a bit of the game before, and started with a level near where they had left off.\n\nEach of them looked at their level for awhile and said \"Well, this looks basically impossible... unless this \\[questionable assumption I came up with that I don't really believe in\\] is true. I think that assumption is... 70% likely to be true.\"\n\nThen they went an executed their plan.\n\nIt failed. The questionable assumption was not true.\n\nThen, each of them said, again \"okay, well here's a *different* sketchy assumption that I wouldn't have thought was likely except if it's not true, the level seems unsolveable.\"\n\nI asked \"what's your probability for that one being true?\"\n\n\"70%\"\n\n\"Okay. You ready to go ahead again?\" I asked.\n\n\"Yep\", they said.\n\nThey tried again. The plan failed again.\n\nAnd, then they did it a *third time*, still saying ~70%.\n\nThis happened with three different junior alignment researchers, making a total of 9 predictions, which were wrong 100% of the time.\n\n(The third guy, on the the second or third time, said \"well... okay, I was wrong last time. So this time let's say it's... 60%.\")\n\n* * *\n\nMy girlfriend ran a similar exercise with another group of young smart people, with similar results. \"I'm 90% sure this is going to work\" ... \"okay that didn't work.\"\n\n* * *\n\nLater I ran the exercise again, this time with a mix of younger and more experienced AI safety folk, several of whom leaned more pessimistic. I think the group overall did better. \n\nOne of them actually made the correct plan on the first try. \n\nOne them got it wrong, but gave an appropriately low estimate for themselves.\n\nAnother of them (call them Bob) made three attempts, and gave themselves ~50% odds on each attempt. They went into the experience thinking \"I expect this to be hard but doable, and I believe in developing the skill of thinking ahead like this.\" \n\nBut, after each attempt, Bob was surprised by how out-of-left field their errors were. They'd predicted they'd be surprised... but they were surprised in surprising ways – even in a simplified, toy domain that was optimized for being a solveable puzzle, where they had lots of time to think through everything. They came away feeling a bit shook up by the experience, and not sure if they believed in longterm planning at all, and feeling a bit alarmed at a lot of people around who confidently talked as if they were able to model things multiple steps out.\n\n* * *\n\nii. Finding traction in the wrong direction.\n============================================\n\nA related (though distinct) phenomena I found, in my own personal experiments using Baba Is You, or [Thinking Physics](https://www.lesswrong.com/posts/PiPH4gkcMuvLALymK/exercise-solve-thinking-physics), or other puzzle exercises as rationality training:\n\nIt's very easy to spend a lot of time optimizing within the areas I feel some traction, and then eventually realize this was wasted effort. A few different examples:\n\n**Forward Chaining instead of Back Chaining. **\n\nIn Baba-is-You levels, there will often be parts of the world that are easy to start fiddling around with and manipulating, and maneuvering into a position that looks like it'll help you navigate the world. But, often, these parts are red herrings. They open up your option-space within the level... but not the parts you needed to win.   \n  \nIt's often faster to find the ultimately right solution if you're starting from the end and backchaining, rather than forward chaining with whatever bits are easiest to fiddle around with.\n\n**Moving linearly, when you needed to be exponential. **\n\nOften in games I'll be making choices that improve my position locally, and are clearly count as *some* degree of \"progress.\" I'll get 10 extra units of production, or damage. But, then I reach the next stage, and it turns out I really needed 100 extra units to survive. And the thought-patterns that would have been necessary to \"figure out how to get 100 units\" on my first playthrough are pretty different from the ones I was actually doing.   \n  \nIt should have occurred to me to ask \"will the game ever throw a bigger spike in difficulty at me?\", and \"is my current strategy of tinkering around going to prepare me for such difficulty?\".\n\n**Doing lots of traction-y-feeling reasoning that just didn't work. **\n\nOn my first Thinking Physics problem last year, I brainstormed multiple approaches to solving the problem, and tried each of them. I reflected on considerations I might have missed, and then incorporated them. I made models and estimations. It felt very productive and reasonable.\n\nI got the wrong answer, though.\n\nMy study partner did get the right answer. Their method was more oriented around thought experiments. And in retrospect their approach seemed more useful for this sort of problem. And it's noteworthy that my subjective feeling of \"making progress\" didn't actually correspond to making the sort of progress that mattered.\n\n* * *\n\nTakeaways\n=========\n\nObviously, an artificial puzzle is not the same as a real, longterm research project. Some differences include:\n\n*   It's designed to be solveable\n*   But, also, it's designed to be sort of counterintuitive and weird\n*   It gives you a fairly constrained world, and tells you what sort of questions you're trying to ask.\n*   It gives you clear feedback when you're done.\n\nThose elements push in different directions. Puzzles are more deliberately counterintuitive than reality is, on average, so it's not necessarily \"fair\" when you fall for a red herring. But they are nonetheless mostly easier and clearer than real science problem.\n\nWhat I found most interesting was people literally saying the words out loud, multiple times \"Well, if this \\[assumption\\] isn't true, then this is impossible\" (often explicitly adding \"I wouldn't \\[normally\\] think this was that likely... but...\"). And, then making the mental leap all the way towards \"70% that this assumption is true.\" Low enough for some plausible deniability, high enough to justify giving their plan a reasonable likelihood of success.\n\nIt was a much clearer instance of mentally [slipping sideways](https://www.lesswrong.com/posts/m6dLwGbAGtAYMHsda/epistemic-slipperiness-1)  into a more convenient world, than I'd have expected to get.\n\nI don't know if the original three people had done calibration training of any kind beforehand.  I know my own experience doing the [OpenPhil calibration game](https://www.openphilanthropy.org/calibration) was that I got good at it within a couple hours... but that it didn't transfer very well to when I started making PredictionBook / [Fatebook](https://fatebook.io/) questions about topics I actually cared about.\n\nI expect forming hypotheses in a puzzle game to be harder than the OpenPhil Calibration game, but easier than making longterm research plans. It requires effort to wrangle your research plans into a bet-able form, and then actually make predictions about it. I bet most people do not do that. \n\nNow, I do predict that people who do real research in a given field will get at least decent at implicitly predicting research directions within their field (via lots of trial-and-error, and learning from mentors). This is what \"research taste\" is. But, I don't think this is that reliable if you're not deliberately training your calibration. (I have decades of experience passively predicting stuff happening in my life, but I nonetheless was still miscalibrated when I first started making explicit PredictionBook predictions about them). \n\nAnd moreover, I don't think this transfers much to new fields you haven't yet mastered. Stereotypes come to mind of brilliant physicists who assume their spherical-cow-simplifications will help them model other fields. \n\nThis seems particularly important for existentially-relevant alignment research. We have examples of people who have demonstrated \"some kind of traction and results\" (for example, doing experiments on modern ML systems. Or, for that matter, coming up with interesting ideas like Logical Induction). But we don't actually have direct evidence that this productivity will be relevant to superintelligent agentic AI. \n\nWhen it comes to \"what is good existential safety research taste?\", I think we are guessing.\n\nI think you should be scared about this, if you're the sort of theoretic researcher, who's trying to cut at the hardest parts of the alignment problem (whose feedback loops are weak or nonexistent) \n\nI think you should be scared about this, if you're the sort of Prosaic ML researcher who *does* have a bunch of tempting feedback loops for current generation ML, but a) it's really not clear whether or how those apply to aligning superintelligent agents, b) many of those feedback loops also basically translate into enhancing AI capabilities and moving us toward a more dangerous world.\n\nI think you should be scared about this, if you're working in policy, either as a research wonk or an advocate, where there are some levers of power you can sort-of-see, but how the levers fit together and whether they actually connect to longterm existential safety is unclear.\n\nUnfortunately, \"be scared\" isn't that useful advice. I don't have a great prescription for *what to do*.\n\nMy dissatisfaction with this situation is what leads me to explore [Feedbackloop-first Rationality](https://www.lesswrong.com/posts/pZrvkZzL2JnbRgEBC/feedbackloop-first-rationality), basically saying \"Well the problem is our feedback loops suck – either they don't exist, or they are temptingly goodharty. Let's try to invent better ones.\" But I haven't yet achieved an outcome here I can point to and say \"okay this clearly helps.\"\n\nBut, meanwhile, my own best guess is:\n\nI feel a lot more hopeful about researchers who have worked on a few different types of problems, and gotten more calibrated on where the edges of their intuitions' usefulness are. I'm exploring the art of operationalizing [cruxy predictions](https://www.lesswrong.com/posts/wDpXshpakpYDcTtug/fluent-cruxy-predictions-1), because I hope that can eventually feed into an the art of having calibrated, cross-domain research taste, if you are deliberately attempting to test your transfer learning. \n\nI feel more hopeful about researchers that make lists of their foundational assumptions, and practiced of [staring into the abyss](https://www.lesswrong.com/posts/vzfz4AS6wbooaTeQk/staring-into-the-abyss-as-a-core-life-skill), confronting \"what would I actually do if my core assumptions were wrong, and my plan doesn't work?\", and [grieving](https://www.lesswrong.com/posts/gs3vp3ukPbpaEie5L/deliberate-grieving-1) for assumptions that seem, on reflection, to have been illusions.\n\nI feel more hopeful about researchers who talk to mentors with different viewpoints, learning different bits of taste and hard-earned life lessons, and attempt to integrate them into some kind of holistic AI safety research taste.\n\nAnd while I don't think it's necessarily right for everyone to set themselves the standard of \"tackle the hardest steps in the alignment problem and solve it in one go\", I feel much more optimistic about people who have thought through \"what are all the sorts of things that need to go right, for my research to actually pay off in an existential safety win?\"\n\nAnd I'm hopeful by people who look at all of this advice, and think \"well, this *still* doesn't actually feel sufficient for me to be that confident my plans are really going to accomplish anything\", and set out to brainstorm new ways to shore up their chances.",
      "plaintextDescription": "Eliezer Yudkowsky periodically complains about people coming up with questionable plans with questionable assumptions to deal with AI, and then either:\n\n * Saying \"well, if this assumption doesn't hold, we're doomed, so we might as well assume it's true.\"\n * Worse: coming up with cope-y reasons to assume that the assumption isn't even questionable at all. It's just a pretty reasonable worldview.\n\nSometimes the questionable plan is \"an alignment scheme, which Eliezer thinks avoids the hard part of the problem.\" Sometimes it's a sketchy reckless plan that's probably going to blow up and make things worse.\n\nSome people complain about Eliezer being a doomy Negative Nancy who's overly pessimistic.\n\nI had an interesting experience a few months ago when I ran some beta-tests of my Planmaking and Surprise Anticipation workshop, that I think are illustrative.\n\n----------------------------------------\n\n\ni. Slipping into a more Convenient World\nI have an exercise where I give people the instruction to play a puzzle game (\"Baba is You\"), but where you normally have the ability to move around and interact with the world to experiment and learn things, instead, you need to make a complete plan for solving the level, and you aim to get it right on your first try.\n\nIn the exercise, I have people write down the steps of their plan, and assign a probability to each step. \n\nIf there is a part of the puzzle-map that you aren't familiar with, you'll have to make guesses. I recommend making 2-3 guesses for how a new mechanic might work. (I don't recommend making a massive branching tree for every possible eventuality. For the sake of the exercise not taking forever, I suggest making 2-3 branching path plans)\n\nSeveral months ago, I had three young-ish alignment researchers do this task (each session was a 1-1 with just me and them).\n\nThe participants varied in how experienced they were with Baba is You. Two of them were new to the game, and completed the the first couple levels without to",
      "wordCount": 2158
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "wDpXshpakpYDcTtug",
    "title": "Fluent, Cruxy Predictions",
    "slug": "fluent-cruxy-predictions-1",
    "url": null,
    "baseScore": 86,
    "voteCount": 34,
    "viewCount": null,
    "commentCount": 14,
    "createdAt": null,
    "postedAt": "2024-07-10T18:00:06.424Z",
    "contents": {
      "markdown": "*The latest in the* [*Feedback Loop Rationality*](https://www.lesswrong.com/sequences/skTdjtx87XKurdvWf) *series.*\n\nPeriodically, people (including me) try to operationalize predictions, or bets, and... it doesn't seem to help much.\n\nI think I recently \"got good\" at making \"actually useful predictions.\" I currently feel on-the-cusp of unlocking a host of related skills further down the rationality tech tree. This post will attempt to spell out some of the nuances of how I currently go about it, and paint a picture of why I think it's worth investing in.\n\nThe takeaway that feels most important to me is: **it's** ***way*** **better to be \"fluent\" at operationalizing predictions, compared to \"capable at all.\"**\n\nPreviously, \"making predictions\" was something I did separately from my planning process. It was a slow, clunky process.\n\nNowadays, reasonably often I can integrate predictions into the planning process itself, because it feels lightweight. I'm better at quickly feeling-around for \"what sort of predictions would actually *change my plans* if they turned out a certain way?\", and then quickly check in on my intuitive sense of \"what do I expect to happen?\"\n\nFluency means you can actually use it day-to-day to help with whatever work is most important to you. Day-to-day usage means you can actually get calibrated for predictions in whatever domains you care about. Calibration means that your intuitions will be good, and *you'll know they're good*. \n\nIf I were to summarize the change-in-how-I-predict, it's a shift from:\n\n**\"Observables-first\".** i.e. looking for things I could easily observe/operationalize, that were somehow related to what I cared about.\n\nto:\n\n\"**Cruxy-first\".** i.e. Look for things that would change my decisionmaking, even if vague, and learn to either better operationalize those vague things, or, find a way to get better data. (and then, there's a cluster of skills and shortcuts to make that easier)\n\n<table style=\"border:1px double rgb(179, 179, 179)\"><tbody><tr><td style=\"background-color:rgb(230, 230, 230);border:1px double rgb(217, 217, 217);padding:0.4em\"><p>Disclaimer:&nbsp;</p><p>This post is on the awkward edge of \"feels full of promise\", but \"I haven't yet executed on the stuff that'd make it clearly demonstrably valuable.\" (And, I've tracked the results of enough \"feels promise\" stuff to know they have a &lt;50% hit rate)</p><p>I feel like I can see the work needed to make this technique actually functional. It's a lot of work. I'm not sure if it's worth it. (Alas, I'm inventing this technique <i>because</i> I don't know how to tell if my projects are worth it and I really want a principled way of forming beliefs about that. Since I haven't finished vetting it yet it's hard to tell!)</p><p>There's a failure mode where rationality schools <a href=\"https://www.lesswrong.com/posts/JnKCaGcgZL4Rsep8m/schools-proliferating-without-evidence\">proliferate without evidence</a>, where people get excited about their new technique that sounds good on paper, and publish exciting blogposts about it.&nbsp;</p><p>There's an unfortunate catch-22 where:&nbsp;</p><ul><li>Naive rationality gurus post excitedly, immediately, as if they already know what they're doing. This is even kinda helpful (because believing in the thing really does help make it more true).</li><li>More humble and realistic rationality gurus wait to publish until they're confident their thing really works. They feel so in-the-dark, fumbling around. As they should. Because, like, they (we) are.&nbsp;</li></ul><p>But, I nonetheless feel like the rationality community was overall healthier when it was full of excited people who still believed a bit in their heart that rationality would give them superpowers, and posted excitedly about it. It also seems intellectually valuable for notes to be posted along the way, so others can track it and experiment.</p><p>Rationality Training doesn't give you superpowers – it's a lot of work, and then \"it's a pretty useful skill, among other skills.\" But I think it's an important skill if you are trying to <a href=\"https://www.lesswrong.com/posts/CSZnj2YNMKGfsMbZA/specializing-in-problems-we-don-t-understand\">problems you don't understand</a>.</p><p>I expect, a year from now, I'll have a better conception of the ideas in this post. This post is still my best guess about how this skill can work, and I'd like it if other people excitedly tried to make it work, and report how it went.&nbsp;</p></td></tr></tbody></table>\n\nPredicting Outcomes vs Comparing Plans\n======================================\n\nI'm using forecasting as one tool in a larger toolset. I'll have another post delving deeper into this, but basically: whenever I'm spending a bunch of resources on something, I try to:\n\n1.  Explicitly ask \"what are my goals?\"\n2.  Come up with at least two ideas for approaching that goal, that you really believe in.\n3.  Identify possible cruxes for pursuing one plan, or the other.\n4.  Reduce uncertainty on those cruxes.\n5.  Pick a thing and do it.\n\nThe \"fluent, cruxy predictions\" skill is a tool for step 3 and 4. It's related to [Murphijitsu](https://www.lesswrong.com/posts/Htjbj8ystqc2zLtMX/murphyjitsu-an-inner-simulator-algorithm) (generally asking \"what do I expect to go wrong here?\", and then improving your plan, and then asking again until you feel like you plan is good enough).\n\nOften, if I have an important uncertainty, instead of running an expensive experiment to resolve it, I can get a surprising amount of mileage out of just asking \"well, what do I actually expect to happen here, if I force myself to get concrete and relevant?\". The act of asking the question prompts my unconscious mind (i.e System 1 or Inner Simulator) to reveal what it actually believes when I'm not bullshitting yourself. \n\nThe [trick here](https://www.lesswrong.com/posts/Jash4Gbi2wpThzZ4k/cfar-takeaways-andrew-critch#Surprise_3__When_I_learned_Inner_Simulator_from_Kenzie__I_was_surprised_that_it_helped_with_everything_in_life_forever_) is using your deliberate, System 2 reasoning, to notice important questions that your System 1 would be good at answering.\n\nBut, your unconscious mind isn't magic*.* It doesn't know everything. So I think it's important to follow this up with *making explicit predictions, that you check later,* so your inner-simulator can get more accurate over time.\n\nSubskills that go into this are:\n\n*   Operationalizing predictions that are **cruxy and decision-relevant**. \n*   Find the **right amount of concreteness.** How will you know if a prediction came true, or when to check?\n*   **Reduce friction.** Make it very easy to write down a prediction, in a way that you'll actually followup on.\n\nOf those: training the skill of \"operationalizing cruxy questions\" is most important. \n\nBut, reducing friction is the easiest part to get traction on, so let's start there.\n\nFrictionless\n============\n\nIt should be as easy as possible to make a prediction. I particularly like the app [Fatebook.io](https://fatebook.io/). It's very lightweight – I can open up the site, and my cursor will automatically be highlighted in the \"new prediction\" text box. I type in a prediction, tab over to the \"forecast\" input to enter a probability, and then hit return and *boom, I'm done.*\n\nBut I like it even more because: \n\n*   It can integrate with [Slack](https://fatebook.io/for-slack) or [Discord](https://fatebook.io/for-discord) (so you can make work-related predictions that your coworkers also can forecast on). \n*   The [chrome and firefox extensions](https://fatebook.io/extension) let you quickly make predictions from *anywhere on the internet,* and then paste them into whatever document you're working in. (Note: I recommend changing the default hotkey to whatever is comfortable)\n\nFor example, right now I'm writing a LessWrong doc. Are there any cruxy predictions I can make about this? What actually am I trying to accomplish with this post? After thinking a little, I hit Cmd-Shift-F, entered these predictions and then hit Cmd-V to paste them into the doc:\n\n[⚖ A week after posting this, between 1-3 people will have commented saying they got a Fatebook account because of this post (65%)](https://fatebook.io/q/between-0-3-people-will-comment-saying--clvsnpr2z0009jr0cdayrpcyp?ext=1)\n\n[⚖ A week after posting this, 1-2 people will have actually made a chrome/firefox extension prediction that they paste into a comment. (50%)](https://fatebook.io/q/a-week-after-posting-this-1-2-people--clvsnwi7r0001l20c02j4s34z?ext=1)\n\nAwhile later, I added:\n\n[⚖ A post (not by me) that gets 100+ karma will link approvingly to this post within 2 years (10%)](https://fatebook.io/q/a-post-not-by-me-that-gets-100-karma--cly80pe1n0001117lhj5ge7da?ext=1)\n\nThis is actually a decent example of how to use predictions, so I'll walk through what just went through my head. (This will be a little messy, because it's a real thought process. I think it's good to showcase what the real messiness might look like)\n\nI started by asking myself \"what actually am I even trying to do with this post?\"\n\nThe most obvious goal with this post is to get people into the habit of making fluent, cruxy predictions. Ideally, I care that people are still doing it years later, skillfully. But I anticipate getting a very weak signal about that – I won't necessarily know who read the post. If I made Fatebook prediction like \"a year from now, someone who read this post will have gone to develop a solid Fatebook habit\", I wouldn't even know who to followup with to ask if it had worked.\n\nAs I thought about that, I started to feel despair – is this post really going to accomplish *anything?* I'm actually pretty pessimistic that many people will read this post and take action. And the longterm second-order effects feel pretty murky. This post is a fair amount of effort to write. Why am I even doing this? \n\nAfter thinking a bit, two things occurred to me:\n\n*   My main reason for writing this post has more to do with \"building a longterm intellectual edifice\", i.e. people going on to develop new techniques about forecasting or rationality. \n*   It probably actually *makes it more true* that people will start practicing cruxy-prediction, if I explicitly ask them to, and make it easy.\n\nAnd that hopefully illustrates a major point of this whole \"fluent cruxy prediction\" concept: Asking \"what effects do I expect to see from my actions?\" often throws into relief that I *don't* really expect to see results from my actions. I'm just kinda running on autopilot. But, having noticed that, I can followup and ask \"okay, what *would* actually need to be true, to be in the higher-likelihood of success world?\"\n\nWith that in mind: \n\nHey there, reader. \n\nIf this feels at least somewhat compelling, what if you just got yourself to [Fatebook](https://fatebook.io/) right now, and make a couple predictions that'll resolve within couple days, or a week? Fatebook will send you emails reminding you about it, which can help bootstrap a habit.\n\nFeeling around for \"cruxiness\"\n==============================\n\nThe previous section started getting into this already. But now let's focus more explicitly: What sort of predictions, if true, would change your decisionmaking?\n\nCruxy operationalization is a murky artform. But here are some principles, and tricks I currently use. Most of my technique involves asking myself various questions, and then seeing what bubbles up.\n\nOver time, I keep track of which question-prompts feel most useful.\n\nIdeal Prerequisite:   \nHave at least two (plans/approaches/options)\n-------------------------------------------------------------------\n\nI'll hopefully write up a whole blogpost about this someday. But one failure mode is \"Well, I only really had one general idea. I made predictions about whether that idea would go well. Maybe they returned 'it'll go amazingly well!' and maybe they returned 'it'll go at least pretty well'. 'Pretty well' is at least 'pretty good.' I can't think of anything else to do, so... I guess I'll go with my original idea?\"\n\nHaving at least two options, that are somewhat mutually exclusive, forces me to start thinking \"okay, but how does my favorite plan compare to my *other* realistic options?\". \n\nIt's important to find at least two options you really believe in. (Ideally: have at least one alternative that fits reasonably well into your current lifestyle, or strategic framework, and one that is more radically different).\n\nIt's a form of [leaving a Line of Retreat](https://www.lesswrong.com/posts/3XgYbghWruBMrPTAL/leave-a-line-of-retreat), that helps you be more cognitively agile as you decide whether to pivot. If you're having trouble coming up with alternative plans, try [Metastrategic Brainstorming](https://www.lesswrong.com/posts/cbWoMepny3Jo9XqEr/metastrategic-brainstorming-a-core-building-block-skill).\n\nFrames 1: Costs and benefits\n----------------------------\n\nSo, one set of questions are \"high level cost/benefit comparison?\", i.e:\n\n*   What are the biggest benefits I'd hope to see from my mainline plan? What would I observe if those benefits came to pass? How likely is that to happen?\n*   What are the costs associated with my current plan?\n\nAnd then:\n\n*   What are the biggest benefits of the *main alternatives to my plan?* And what are the costs of those plans? \n\nFor me, these activate a different mode, that cuts more to what I actually care about.\n\nFrame 2: Will this really help my deeper, longterm goals?\n---------------------------------------------------------\n\nOften, my plans are really \"step 1 of a much larger plan\", or, a cluster of strategies that other people in the world are working towards. \n\n*   Do any of my current plans feel on track to actually accomplish my underlying, deeper goals?\n*   What would I see in the world, 1-4 years from now, *after my plan is completed*, if my plan turned out to really help with the longterm goals?\n*   What would I see 1-4 years from now in worlds where my plan *hadn't* really helped with the longterm goals?\n\nFrame 3: Murphijitsu, and being dissatisfied with \"maybe\" \n----------------------------------------------------------\n\nOften while thinking about the first two sets of questions, I find my intuitive sense of \"will my project be successful?\" is \"... kinda?\". Either the outcome I'm predicting feels uncertain (\"~55%?\"), or it's hard to get that clear a visualizable outcome at all.\n\nI then ask \"okay, so, if this was *definitely, clearly a resounding success, a year from now,* what are all the things I would see?\"\n\nThe way this works best at first sometimes feels like unrealistic dreaming, but then often highlights that the dream is maybe achievable (but perhaps a lot more work than I'd initially envisioned). I generate specific new intermediate actions I might want to plan for. Then, I go back to the original prediction and see if I expect greater success.\n\nThis is pretty similar to the process of [Murphijitsu](https://www.lesswrong.com/posts/Htjbj8ystqc2zLtMX/murphyjitsu-an-inner-simulator-algorithm) (asking \"am I surprised if this goes wrong?\", and if you wouldn't be too surprised, iterating on your plan until you'd feel actively surprised if it didn't work). The two novel components I'm adding here are:\n\n*   Not merely stopping with \"will my plan work?\" but \"will it have the cruxy outcomes I actually cared about, far in the future?\". (One might say this should always be part of Murphijitsu, but it felt novel to me)\n*   Making explicit predictions at the end, to train your inner surprisometer.\n\nThe feeling of \"oh... that's cruxy\" \n------------------------------------\n\nIt's possible to do \"fake work\" – where you come up with some defensible sounding predictions, but they still don't actually cleave at your cruxes or help you think. \n\nOne thing I particularly look for is a feeling of \"oh man, that's cruxy.\" For me it often comes with a feeling of destabilization/vertigo.\n\nI remember once a few years ago when I was arguing with someone about whether empathy was useful. I was like \"Empathy isn't just nice. It makes people more effective so they'll be better at their job.\" My conversation partner (habryka) said \"do you really think Elon Musk would do better at his job if he was more empathetic\", and I felt a sinking feeling of \"oh no, my beliefs just became falsifiable, and, I'm not sure how had I'd actually bet on them.\" \n\nIt's hard to actually resolve a bet on whether empathy would help Elon Musk, but, operationalizing the bet, even if it's a hypothetical fantasy bet, helps boot up my realistic intuitions, rather than abstract hopes about \"surely this 'make people more empathetic plan will be good' because empathy is generically good.\"\n\nExample\n=======\n\nThe Fractal Strategy Workshop\n-----------------------------\n\nWhile I was leading up to my [Fractal Strategy Workshop](https://www.lesswrong.com/posts/h5mDx2Mt2P5m9v588/fractal-strategy-workshop-report), 10 days beforehand, my colleague and I sat down to make some predictions about the workshop.\n\nA naive thing we might have written down is \"after the workshop, someone will use a technique we taught them.\" But this wouldn't be very *concrete.* How would you know when to check? What counts as using a technique from the workshop?\n\nThe things my colleague wrote down tended to be questions like: \n\n*   \"Six months after the workshop, a participant (other than Ray or me) will have set a ten * minute timer to brainstorm strategies for solving a problem.\" \n*   \"A participant changes what they're planning to do, or skips to the hard part, in the year after the workshop, in a way that seems related to the workshop.\"\n\nMeanwhile I was making predictions more like:\n\n*   \"After the workshop, will my boss think it makes sense for me to keep working on this rationality project?\"\n*   \"Will there be people willing to pay enough money to support the workshop?\" (ideally, clients who think it's worthwhile from a productivity standpoint to spend thousands of dollars on it)\n*   If I pitch this to OpenPhil or other major donors, will they think it's worth funding?\n*   \"All things considered, I will find this project valuable enough to run another another workshop by the end of the year?\"\n*   \"Will I find a cofounder who believes in this project enough to commit to it?\" (My colleague notably *didn't* believe in the project enough to commit longterm)\n\nIn some sense, my colleague's questions are quite reasonable. If we aren't seeing participants use the techniques from the workshop, or generally improve their planmaking, the workshop can't be that useful.\n\nBut, that wasn't that cruxy for my decisionmaking. I know that the first workshop I run will probably have lots of problems. I also don't really expect a workshop to work in isolation (what I hoped would work was workshop + 6 months of followup coaching to help cement the skills and collaboratively iterate on applying them to the participant's lives). I was deliberately *not* committing to doing the followup coaching for this workshop, because it was an early beta test. The purpose was to get a sense of whether the curriculum was roughly pointed in the right direction, and decide whether to do a full workshop+coaching later on.\n\nIt *is* cruxy to me whether, if I worked on it for a year, the metastrategy framework would produce clear results. But for the immediate future, the more relevant question was \"will the major stakeholders of this project think it is worth paying for?\". If people aren't willing to pay significant money for the workshops, that's evidence that they don't expect them to be seriously valuable. And if my boss didn't think it made sense, I'd need to raise money from somewhere and strike out on my own.\n\nThis shaped my decisionmaking in a few ways. I realized:\n\n*   The most important question for the workshop feedback form would be \"how much would you be willing to pay for this?\", \"how much would you be willing to pay for the idealized version of this workshop tailored for you?\" (and related questions about \"what that idealized workshop would look like)\n*   I wanted an interest form for future workshops that I would advertise around, to get a sense of how much demand there was for this sort of thing.\n*   The purpose of the workshop was not to succeed on my first try (although I should still *try* to succeed on my first try). The main goal was to reduce uncertainty, and get a better sense of what parts of my agenda needed the most work. (This resulted in me including some classes I was less confident in)\n\n(It's worth noting: my colleague had the opposite impression. They thought it was relatively easy to get funding, they were much more worried about the metastrategy practice failing to impact people. Which is reasonable! But, part of the point here is that what matters is *what is cruxy for you.* They're your decisions and plans!)\n\nTips and Tricks\n===============\n\nPractice via Videogames\n-----------------------\n\nReal life takes a long time to give you feedback on your predictions. I've found videogames a good testbed to practice making predictions, in a way that feels fun, rapid-feedback, while connected to a \"real-ish decisionmaking process.\" \n\nI recommend using Turn-Based games that you haven't played before, since those give you lots of opportunities to pause and think. \n\nAt first, simply focus on making as many predictions as you can about the game's mechanics, to drill the basic move of \"notice that there are multiple plausible worlds that you're living in, and that you have *some* useful intuitions about it even if you're missing lots of information.\"\n\nOnce you get a basic handle on it, try to specifically ask \"what are some unknowns in this game that would affect my strategy?\". When you're about to make a choice, consider \"what could turn out to be true, that'd lead me to wish I'd made a different choice?\".\n\nNOTES for Prediction-Followup\n-----------------------------\n\nOften, I'll notice \"hmm, I could make a prediction about that\" while I'm in the middle of a longer thought process. Something I find helpful is to jot down a note about it somewhere I can easily come back to after the train-of-thought completes. (If I'm in a brainstorming document, literally write something like PREDICTION, in capital letters).\n\nThe most common note I use is the word \"PROMISING,\" which specifically refers to me having an idea that feels really compelling. (I developed this while practicing solving Baba is You levels, and making predictions about whether my current problem-solving process was on the right track).\n\nI encourage you to develop your own shorthand, based on the mental qualia that are useful for you.\n\nStuck on making a perfect prediction? Make an imperfect one\n-----------------------------------------------------------\n\n### (Relatedly: it's okay to make fuzzy predictions that only make sense to you, but when doing so, try to make \"extremized\"  ones)\n\nSometimes I want to decision – quitting a job, or deciding to do a complex plan. If I can't pin down a concrete observable thing, I can always default to \"Subjectively, a year from now, I'll think it was pretty clearly correct to have \\[done X\\].\"\n\nBut, the words \"pretty obviously\" there are important. \n\nOftentimes when I ask \"was it a good decision \\[to quite that job / to start that new relationship / etc\\]?\" the answer is \"...maybe? Yeah, kinda?\". And that's too vague to be useful as a resolution criteria. \n\nBut if you set the standard for \"it has to be pretty clearly correct\", not just \"probably correct\", I find that leaves less room for hemming/hawing about it. \n\nMake multiple predictions\n-------------------------\n\nRelatedly: If you're not sure if one prediction captures The True Spirit of What You're Uncertain about, try making a few different predictions on the same topic.\n\nIf I'm considering leaving my job, I might separately try asking things like:\n\n*   *In a year, will I clearly regret having quit that job?*\n*   *In a year, will I clearly be glad to having quit that job? *\n    *   *(sometimes this returns an inconsistent intuition as the previous one!)*\n*   *In a year, will I have some kind of steady paycheck at the time this prediction resolves?*\n*   *In a year, will I have $N saved in the bank?*\n*   *A year after quitting my job, will I feel clearly happier than I remember being at my old job?*\n*   *A year after quitting my job, will Trusted Friend X think that whatever I spent the past year doing was at least as important/meaningful as my previous job?*\n*   *The day after I quit my job, will I feel a sense of relief?*\n\nTakeaways\n=========\n\nWe just covered a lot of stuff. A quick recap of some of the key concepts:\n\n*   **Fluency is key**: The ability to seamlessly integrate predictions into your thought process is what makes this skill truly valuable. It's not about occasional, formal prediction-making sessions, but about habitually asking yourself, \"What do I actually expect to happen here?\"\n*   **Focus on what's cruxy**: Not all predictions are created equal. The most valuable ones are those that would actually change your decisions or plans if they turned out a certain way. Learning to identify these cruxy points is a skill in itself.\n*   **Reduce friction**: Tools like Fatebook.io can make it incredibly easy to jot down predictions on the fly. The easier it is, the more likely you are to do it consistently.\n*   **Look for that \"aha\" moment**: The feeling of \"oh man, that's cruxy\" can be a sign that you've hit on something important.\n\nTo reach fluency, you need some way of getting in lots of practice. The advice I'd give depends somewhat on where you're starting from. If you're totally new to predictions, I think it's necessary to first get a basic fluency with \"making predictions, at all.\" Make tons of predictions about things you care about in your life, that will resolve reasonably soon. (i.e. days or weeks). Make tons of predictions about videogames or other fun domains with uncertainty and fast-feedback.\n\nBut, the thing to ultimately be aiming for is to hit a kind of \"escape velocity\", with a combination of skills make predictions useful for your day job, or major personal projects. For that, the skill of identifying and operationalizing \"strategic cruxiness\" is an important building block.\n\nIf you do end up installing the Fatebook browser extension and making some predictions, let me know! It is helpful to know when people actually take actions based on posts like this.",
      "plaintextDescription": "The latest in the Feedback Loop Rationality series.\n\n \n\nPeriodically, people (including me) try to operationalize predictions, or bets, and... it doesn't seem to help much.\n\nI think I recently \"got good\" at making \"actually useful predictions.\" I currently feel on-the-cusp of unlocking a host of related skills further down the rationality tech tree. This post will attempt to spell out some of the nuances of how I currently go about it, and paint a picture of why I think it's worth investing in.\n\nThe takeaway that feels most important to me is: it's way better to be \"fluent\" at operationalizing predictions, compared to \"capable at all.\"\n\nPreviously, \"making predictions\" was something I did separately from my planning process. It was a slow, clunky process.\n\nNowadays, reasonably often I can integrate predictions into the planning process itself, because it feels lightweight. I'm better at quickly feeling-around for \"what sort of predictions would actually change my plans if they turned out a certain way?\", and then quickly check in on my intuitive sense of \"what do I expect to happen?\"\n\nFluency means you can actually use it day-to-day to help with whatever work is most important to you. Day-to-day usage means you can actually get calibrated for predictions in whatever domains you care about. Calibration means that your intuitions will be good, and you'll know they're good. \n\nIf I were to summarize the change-in-how-I-predict, it's a shift from:\n\n\"Observables-first\". i.e. looking for things I could easily observe/operationalize, that were somehow related to what I cared about.\n\nto:\n\n\"Cruxy-first\". i.e. Look for things that would change my decisionmaking, even if vague, and learn to either better operationalize those vague things, or, find a way to get better data. (and then, there's a cluster of skills and shortcuts to make that easier)\n\nDisclaimer: \n\nThis post is on the awkward edge of \"feels full of promise\", but \"I haven't yet executed on the stuff that'd make it cl",
      "wordCount": 4161
    },
    "tags": [
      {
        "_id": "8daMDi9NEShyLqxth",
        "name": "Forecasting & Prediction",
        "slug": "forecasting-and-prediction"
      },
      {
        "_id": "KoXbd2HmbdRfqLngk",
        "name": "Planning & Decision-Making",
        "slug": "planning-and-decision-making"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "8qCwuE8GjrYPSqbri",
    "title": "80,000 hours should remove OpenAI from the Job Board (and similar EA orgs should do similarly)",
    "slug": "80-000-hours-should-remove-openai-from-the-job-board-and",
    "url": null,
    "baseScore": 274,
    "voteCount": 106,
    "viewCount": null,
    "commentCount": 71,
    "createdAt": null,
    "postedAt": "2024-07-03T20:34:50.741Z",
    "contents": {
      "markdown": "*I haven't shared this post with other relevant parties – my experience has been that private discussion of this sort of thing is more paralyzing than helpful. I might change my mind in the resulting discussion, but, I prefer that discussion to be public.*\n\nI think 80,000 hours should remove OpenAI from its job board, and similar EA job placement services should do the same.\n\n(I personally believe 80k shouldn't advertise Anthropic jobs either, but I think the case for that is somewhat less clear)\n\nI think OpenAI has demonstrated a level of manipulativeness, recklessness, and failure to prioritize meaningful existential safety work, that makes me think EA orgs should not be going out of their way to give them free resources. (It might make sense for some individuals to work there, but this shouldn't be a thing 80k or other orgs are systematically funneling talent into)\n\nThere plausibly should be some kind of path to get back into good standing with the AI Risk community, although it feels difficult to imagine how to navigate that, given how adversarial OpenAI's use of NDAs was, and how difficult that makes it to trust future commitments. \n\nThe things that seem most significant to me:\n\n*   They promised the superalignment team 20% of their compute-at-the-time (which AFAICT wasn't even a large fraction of their compute over the coming years), but [didn't provide anywhere close to that](https://web.archive.org/web/20240521144425/https://fortune.com/2024/05/21/openai-superalignment-20-compute-commitment-never-fulfilled-sutskever-leike-altman-brockman-murati/), and then disbanded the team when Leike left.\n*   Their widespread use of  [non-disparagement agreements, with non-disclosure clauses](https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release), which generally makes it hard to form accurate impressions about what's going on at the organization. \n*   [Helen Toner's description](https://www.lesswrong.com/posts/dd66GymgbLQMHGLwQ/openai-helen-toner-speaks) of how Sam Altman wasn't forthright with the board. (i.e. \"The board was not informed about ChatGPT in advance and learned about ChatGPT on Twitter. Altman failed to inform the board that he owned the OpenAI startup fund despite claiming to be an independent board member, giving false information about the company’s formal safety processes on multiple occasions. And relating to her research paper, that Altman in the paper’s wake started lying to other board members in order to push Toner off the board.\")\n*   Hearing from multiple ex-OpenAI employees that OpenAI safety culture did not seem on track to handle AGI. Some of these are public ([Leike](https://x.com/janleike/status/1791498174659715494), [Kokotajlo](https://www.lesswrong.com/users/daniel-kokotajlo)), others were in private. \n\nThis is before getting into more openended arguments like \"it sure looks to me like OpenAI substantially contributed to the world's current AI racing\" and \"we should generally have a quite high bar for believing that the people running a for-profit entity building transformative AI are doing good, instead of cause vast harm, or at best, being a successful for-profit company that doesn't especially warrant help from EAs.\n\nI am generally wary of AI labs (i.e. Anthropic and Deepmind), and think EAs should be less optimistic about working at large AI orgs, even in safety roles. But, I think OpenAI has demonstrably messed up, badly enough, publicly enough, in enough ways that it feels particularly wrong to me for EA orgs to continue to give them free marketing and resources. \n\nI'm mentioning 80k specifically because I think their job board seemed like the largest funnel of EA talent, and because it seemed better to pick a specific org than a vague \"EA should collectively do something.\" (see: [EA should taboo \"EA should\"](https://forum.effectivealtruism.org/posts/Pz7RdMRouZ5N5w5eE/ea-should-taboo-ea-should)). I do think other orgs that advise people on jobs or give platforms to organizations (i.e. the organization fair at EA Global) should also delist OpenAI.\n\nMy overall take is something like: it is probably good to maintain some kind of intellectual/diplomatic/trade relationships with OpenAI, but bad to continue giving them free extra resources, or treat them as an org with good EA or AI safety standing. \n\nIt might make sense for some individuals to work at OpenAI, but doing so in a useful way seems very high skill, and high-context – not something to funnel people towards in a low-context job board.\n\nI also want to clarify: I'm not against 80k continuing to list articles like [Working at an AI Lab](https://80000hours.org/career-reviews/working-at-an-ai-lab/), which are more about how to make the decisions, and go into a fair bit of nuance. I disagree with that article, but it seems more like \"trying to lay out considerations in a helpful way\" than \"just straightforwardly funneling people into positions at a company.\" (I do think that article seems out of date and worth revising in light of new information.  I think \"OpenAI seems inclined towards safety\" now seems demonstrably false, or at least less true in the ways that matter. And this should update you on how true it is for the other labs, or how likely it is to remain true)\n\nFAQ / Appendix\n==============\n\nSome considerations and counterarguments which I've thought about, arranged as a hypothetical FAQ.\n\n### **Q: It seems that, like it or not, OpenAI is a place transformative AI research is likely to happen, and having good people work there is important. **\n\n### **Isn't it better to have alignment researchers working there, than not? Are you sure you're not running afoul of misguided purity instincts?**\n\nI *do* agree it might be necessary to work with OpenAI, even if they are reckless and negligent. I'd like to live in the world where \"don't work with companies causing great harm\" was a straightforward rule to follow. But we might live in a messy, complex world where some good people may need to work with harmful companies anyway. \n\nBut: we've now had two waves of alignment people leave OpenAI. The second wave has multiple people explicitly saying things like \"quit OpenAI due to losing confidence that it would behave responsibly around the time of AGI.\"\n\nThe first wave, my guess is they were mostly under non-disclosure/non-disparagement agreements, and we can't take their lack of criticism as much evidence.\n\nIt looks to me, from the outside, like OpenAI is just not really structured or encultured in a way that makes it that viable for someone on the inside to help improve things much. I don't think it makes sense to continue trying to improve OpenAI's plans, at *least* until OpenAI has some kind of credible plan (backed up by actions) of actually seriously working on existential safety.\n\nI think it might make sense for some individuals to go work at OpenAI anyway, who have an explicit plan for how to interface with the organizational culture. But I think this is a very high context, high skill job. (i.e. skills like \"keeping your eye on the AI safety ball\", \"interfacing well with OpenAI staff/leadership while holding onto your own moral/strategic compass\", \"knowing how to prioritize research that differentially helps with existential safety, rather than mostly amounting to near-term capabilities work.\") \n\nI don't think this is the sort of thing you should just funnel people into on a jobs board.\n\nI think it makes a lot more sense to say \"look, you had your opportunity to be taken on faith here, you failed. It is now OpenAI's job to credibly demonstrate that it is worthwhile for good people to join there trying to help, rather than for them to take that on faith.\"\n\n### **Q: What about jobs like \"**[**security research engineer**](https://Research Engineer, Security)**?\". **\n\n### **That seems straightforwardly good for OpenAI to have competent people for, and probably doesn't require a good \"Safety Culture\" to pay off?**\n\nThe argument for this seems somewhat plausible. I still personally think it makes sense to fully delist OpenAI positions unless they've made major changes to the org (see below).  \n\nI'm operating here from a cynical/conflict-theory-esque stance. I think OpenAI has exploited the EA community and it makes sense to engage with them from more of a cynical \"conflict theory\" stance. I think it makes more sense to say, collectively, \"knock it off\", and switch to default \"apply pressure.\" I think if OpenAI wants to find good security people, that should be their job, not EA organizations. \n\nBut, I don't have a really slam dunk argument that this is the right stance to take. For now, I list it as my opinion, but acknowledge there are other worldviews where it's less clear what to do.\n\n**Q: What about offering a path towards \"good standing?\" to OpenAI?**\n---------------------------------------------------------------------\n\nIt seems plausibly important to me to offer *some* kind of roadmap back to good standing. I do kinda think regulating OpenAI from the outside isn't likely to be sufficient, because it's too hard to specify what actually matters for existential AI safety.\n\nSo, it feels important to me not to fully burn bridges. \n\nBut, it seems pretty hard to offer any particular roadmap. We've got three different lines of OpenAI leadership breaking commitments, and being manipulative. So we're long past the point where \"mere words\" would reassure me.\n\nThings that would be reassure me are costly actions that are extremely unlikely in worlds where OpenAI would (intentionally or no) lure more people in and then still turn out to, nope, just be taking advantage of them for safety-washing / regulatory capture reasons.\n\nSuch actions seem pretty unlikely by now. Most of the examples I can think to spell out seem too likely to be gameable (i.e. if OpenAI were to announce a new Superalignment-equivalent team, or commitments to participate in eval regulations, I would guess they would only do the minimum necessary to look good, rather than a real version of the thing).\n\nAn example that'd feel pretty compelling is if Sam Altman actually really, for real, left the company, that would definitely have me re-evaluating my sense of the company. (This seems like a non-starter, but, listing for completeness). \n\nI wouldn't put much stock in a Sam Altman apology. If Sam is still around, the most I'd hope for is some kind of realistic, real-talk, arms-length negotiation where it's common knowledge that we can't really trust each other but maybe we can make specific deals.\n\nI'd update somewhat if Greg Brockman and other senior leadership (i.e. people who seem to actually have the respect of the capabilities and product teams), or maybe new board members, made clear statements indicating:\n\n*   they understand: how OpenAI messed up (in terms of not keeping commitments, and the manipulativeness of non-disclosure non-disparagement agreements)\n*   they take some actions that are holding Sam (and maybe themselves in some cases) accountable.\n*   they take existential risk seriously on a technical level. They have real cruxes for what would change their current scaling strategy. This is integrated into org-wide decisionmaking. \n\nThis wouldn't make me think \"oh everything's fine now.\" But would be enough of an update that I'd need to evaluate what they actually said/did and form some new models.\n\n### Q: What if we left up job postings, but with an explicit disclaimer linking to a post saying why people should be skeptical?\n\nThis idea just occurred to me as I got to the end of the post. Overall, I think this doesn't make sense given the current state of OpenAI, but thinking about it opens up some flexibility in my mind about what might make sense, in worlds where we get some kind of costly signals or changes in leadership from OpenAI.\n\n(My actual current guess is this sort of disclaimer makes sense for Anthropic and/or DeepMind jobs. This feels like a whole separate post though)\n\n* * *\n\nMy actual range of guesses here are more cynical than this post focuses on. I'm focused on things that seemed easy to legibly argue for. \n\nI'm not sure who has decisionmaking power at 80k, or most other relevant orgs. I expect many people to feel like I'm *still* bending over backwards being accommodating to an org we should have lost all faith in. I don't have faith in OpenAI, but I do still worry about escalation spirals and polarization of discourse. \n\nWhen dealing with a potentially manipulative adversary, I think it's important to have backbone and boundaries and actual willingness to treat the situation adversarially. But also, it's important to leave room to update or negotiate.\n\nBut, I wanted to end with explicitly flagging the hypothesis that OpenAI is best modeled as a normal profit-maximizing org, that they basically co-opted EA into being a lukewarm ally it could exploit, when it'd have made sense to treat OpenAI more adversarially from the start (or at least be more \"ready to pivot towards treating adversarially\". \n\nI don't know that that's the right frame, but I think the recent revelations should be an update towards that frame.",
      "plaintextDescription": "I haven't shared this post with other relevant parties – my experience has been that private discussion of this sort of thing is more paralyzing than helpful. I might change my mind in the resulting discussion, but, I prefer that discussion to be public.\n\n \n\nI think 80,000 hours should remove OpenAI from its job board, and similar EA job placement services should do the same.\n\n(I personally believe 80k shouldn't advertise Anthropic jobs either, but I think the case for that is somewhat less clear)\n\nI think OpenAI has demonstrated a level of manipulativeness, recklessness, and failure to prioritize meaningful existential safety work, that makes me think EA orgs should not be going out of their way to give them free resources. (It might make sense for some individuals to work there, but this shouldn't be a thing 80k or other orgs are systematically funneling talent into)\n\nThere plausibly should be some kind of path to get back into good standing with the AI Risk community, although it feels difficult to imagine how to navigate that, given how adversarial OpenAI's use of NDAs was, and how difficult that makes it to trust future commitments. \n\nThe things that seem most significant to me:\n\n * They promised the superalignment team 20% of their compute-at-the-time (which AFAICT wasn't even a large fraction of their compute over the coming years), but didn't provide anywhere close to that, and then disbanded the team when Leike left.\n * Their widespread use of non-disparagement agreements, with non-disclosure clauses, which generally makes it hard to form accurate impressions about what's going on at the organization. \n * Helen Toner's description of how Sam Altman wasn't forthright with the board. (i.e. \"The board was not informed about ChatGPT in advance and learned about ChatGPT on Twitter. Altman failed to inform the board that he owned the OpenAI startup fund despite claiming to be an independent board member, giving false information about the company’s formal safety ",
      "wordCount": 784
    },
    "tags": [
      {
        "_id": "6zBEfFYJxhSEcchbR",
        "name": "AI Alignment Fieldbuilding",
        "slug": "ai-alignment-fieldbuilding"
      },
      {
        "_id": "827JKe7YNjAegR468",
        "name": "Effective altruism",
        "slug": "effective-altruism"
      },
      {
        "_id": "H4n4rzs33JfEgkf8b",
        "name": "OpenAI",
        "slug": "openai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "EKhNDspRxLGcZAuJz",
    "title": "What percent of the sun would a Dyson Sphere cover?",
    "slug": "what-percent-of-the-sun-would-a-dyson-sphere-cover",
    "url": null,
    "baseScore": 24,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 26,
    "createdAt": null,
    "postedAt": "2024-07-03T17:27:50.826Z",
    "contents": {
      "markdown": "I disagreed with a bunch of the implications of [this comment](https://www.lesswrong.com/posts/87EzRDAHkQJptLthE/but-why-would-the-ai-kill-us?commentId=LgjwepWmnJKjXvoQK), but I was curious about the specific question \"Would a dyson sphere made out of the solar system necessarily cover (most of) the sun?\" (and therefore block out a substantial fraction of light coming to Earth).\n\nThe subquestions here seem to be (at first glance, not a physicist)\n\n*   What are efficient Dyson spheres probably made of?\n*   What percent of the solar system can be converted into Dyson-sphere material?\n    *   Are gas giants harvestable?\n*   How long would it take to harvest that material?\n*   What would the radius of a Dyson sphere be? (i.e. how far away from the sun is optimal). How thick? \n*   If the sphere is (presumably) lots of small modules, how far apart are they?\n\nI don't know if there's already been a canonical answer written up somewhere. The original motivating question was \"if an AI is moderately 'nice', leaves Earth alone but does end up converting the rest of the solar system into a Dyson sphere, how fucked is Earth? (also, on what timescale?). \n\nI don't know that this question actually makes sense (as another commenter mentioned, if the AI is that nice, it can probably also redirect sunlight to Earth at low cost. But, I'm still just curious about the details. (I have enough uncertainty about how the future plays out that it seems nice to understand some of the physical limits involved)",
      "plaintextDescription": "I disagreed with a bunch of the implications of this comment, but I was curious about the specific question \"Would a dyson sphere made out of the solar system necessarily cover (most of) the sun?\" (and therefore block out a substantial fraction of light coming to Earth).\n\nThe subquestions here seem to be (at first glance, not a physicist)\n\n * What are efficient Dyson spheres probably made of?\n * What percent of the solar system can be converted into Dyson-sphere material?\n   * Are gas giants harvestable?\n * How long would it take to harvest that material?\n * What would the radius of a Dyson sphere be? (i.e. how far away from the sun is optimal). How thick? \n * If the sphere is (presumably) lots of small modules, how far apart are they?\n\nI don't know if there's already been a canonical answer written up somewhere. The original motivating question was \"if an AI is moderately 'nice', leaves Earth alone but does end up converting the rest of the solar system into a Dyson sphere, how fucked is Earth? (also, on what timescale?). \n\nI don't know that this question actually makes sense (as another commenter mentioned, if the AI is that nice, it can probably also redirect sunlight to Earth at low cost. But, I'm still just curious about the details. (I have enough uncertainty about how the future plays out that it seems nice to understand some of the physical limits involved)",
      "wordCount": 242
    },
    "tags": [
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "j2pKBBvyAxHPNbuS6",
    "title": "What distinguishes \"early\", \"mid\" and \"end\" games?",
    "slug": "what-distinguishes-early-mid-and-end-games",
    "url": null,
    "baseScore": 48,
    "voteCount": 25,
    "viewCount": null,
    "commentCount": 22,
    "createdAt": null,
    "postedAt": "2024-06-21T17:41:30.816Z",
    "contents": {
      "markdown": "Recently William_S posted:\n\n> In my mental model, we're still in the mid-game, not yet in the end-game.\n\nI replied:\n\n> A thing I've been thinking about lately is \"what does it mean to shift from the early-to-mid-to-late game\". \n\n> In strategy board games, there's an explicit shift from \"early game, it's worth spending the effort to build a longterm engine. At some point, you want to start spending your resources on victory points.\" And a lens I'm thinking through is \"how long does it keep making sense to invest in infrastructure, and what else might one do?\"\n\n> I assume this is a pretty different lens than what you meant to be thinking about right now but I'm kinda curious for whatever-your-own model was of what it means to be in the mid vs late game.\n\nHe replied:\n\n> Like, in Chess you start off with a state where many pieces can't move in the early game, in the middle game many pieces are in play moving around and trading, then in the end game it's only a few pieces, you know what the goal is, roughly how things will play out.\n> \n> In AI it's like only a handful of players, then ChatGPT/GPT-4 came out and now everyone is rushing to get in (my mark of the start of the mid-game), but over time probably many players will become irrelevant or fold as the table stakes (training costs) get too high.\n> \n> In my head the end-game is when the AIs themselves start becoming real players.\n\nThis was interesting because yeah, that totally is a different strategic frame for \"what's an early, midgame and endgame?\", and that suggests there's more strategic frames that might be relevant.\n\nI'm interested in this in the context of AI, but, also in other contexts.\n\nSo, prompt for discussion:\n\na) what are some types of games or other \"toy scenarios,\" or some ways of looking at those games, that have other strategic lenses that help you decisionmake?\n\nb) what are some situations in real life, other than \"AI takeoff\", where the early/mid/late game metaphor seems useful?",
      "plaintextDescription": "Recently William_S posted:\n\n> In my mental model, we're still in the mid-game, not yet in the end-game.\n\nI replied:\n\n> A thing I've been thinking about lately is \"what does it mean to shift from the early-to-mid-to-late game\". \n\n> In strategy board games, there's an explicit shift from \"early game, it's worth spending the effort to build a longterm engine. At some point, you want to start spending your resources on victory points.\" And a lens I'm thinking through is \"how long does it keep making sense to invest in infrastructure, and what else might one do?\"\n\n> I assume this is a pretty different lens than what you meant to be thinking about right now but I'm kinda curious for whatever-your-own model was of what it means to be in the mid vs late game.\n\nHe replied:\n\n> Like, in Chess you start off with a state where many pieces can't move in the early game, in the middle game many pieces are in play moving around and trading, then in the end game it's only a few pieces, you know what the goal is, roughly how things will play out.\n> \n> In AI it's like only a handful of players, then ChatGPT/GPT-4 came out and now everyone is rushing to get in (my mark of the start of the mid-game), but over time probably many players will become irrelevant or fold as the table stakes (training costs) get too high.\n> \n> In my head the end-game is when the AIs themselves start becoming real players.\n\nThis was interesting because yeah, that totally is a different strategic frame for \"what's an early, midgame and endgame?\", and that suggests there's more strategic frames that might be relevant.\n\nI'm interested in this in the context of AI, but, also in other contexts.\n\nSo, prompt for discussion:\n\na) what are some types of games or other \"toy scenarios,\" or some ways of looking at those games, that have other strategic lenses that help you decisionmake?\n\nb) what are some situations in real life, other than \"AI takeoff\", where the early/mid/late game metaphor seems useful?",
      "wordCount": 355
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "cbWoMepny3Jo9XqEr",
    "title": "\"Metastrategic Brainstorming\", a core building-block skill",
    "slug": "metastrategic-brainstorming-a-core-building-block-skill",
    "url": null,
    "baseScore": 64,
    "voteCount": 35,
    "viewCount": null,
    "commentCount": 5,
    "createdAt": null,
    "postedAt": "2024-06-11T04:27:52.488Z",
    "contents": {
      "markdown": "I want to develop rationality training, which is aimed at solving confusing problems. \n\nTwo key problems with \"confusing problems\" are:\n\n1.  You might feel so confused and overwhelmed that you bounce off completely.\n2.  You might be confused about what counts as progress, or where the *most* progress is possible, and accidentally work on the wrong thing.\n\nA skill that helps with both of these is \"metastrategic brainstorming\" – the art of generating lots of potential good approaches; then choosing approaches that are likely to help. (And then, reflecting on whether those strategies worked)\n\nDifferent situations call for different sorts of strategies. If a problem is confusing, you probably don't have a simple playbook for dealing with it. Different people also benefit from different sorts of strategies. So, while I can tell you a list of potential mental tools, what I most want you to practice is the art of identifying what would help *you, in particular,* with the *situation in particular* in which you find yourself.\n\nStrategy vs Metastrategy\n------------------------\n\nWhy \"metastrategic\" instead of \"strategic?\". In war, a \"strategy\" is something like \"send a bunch of cavalry on a surprise attack.\" A metastrategy is more like \"let's invent a tabletop war simulation that helps us think about the situation\" or \"go ask an advisor for help\", which helps you identify strategies like \"surprise attacks.\"  \n  \nIn the case of intellectual problem solving, a strategy might be something like \"solve a particular simpler subproblem\", and a metastrategy would be \"look for related subproblems that you might be able to solve that might help unlock more insights.\"\n\nSometimes I find it valuable to force myself to *only* consider new metastrategies, and avoid thinking about object-level strategies, to force my brain out of a rut.\n\nMy triggers for switching to \"metastrategic brainstorming mode\" are:\n\nFirst, one of the following:\n\n*   I've just sat down to work on a problem I already know is hard.\n*   I've starting to feel stuck, annoyed or frustrated.\n*   I notice that I settled into the very first plan that occurred to me, and I have a sneaking suspicion it's not the best plan.\n\nAnd also: I expect trying to solve a problem I expect to take at least 30 minutes (i.e. enough time it's worth spending at least a few minutes meta-brainstorming)\n\nIn those situations, I switch into \"metastrategic brainstorming mode\", which entails:\n\n1.  Open up a writing doc.\n2.  Ask myself \"what are my goals?\". If there are multiple goals, write them both down.\n3.  **Set a 5-10 minute timer, spend it brainstorming \"meta-level strategies.\"** ***Don't*** **try to solve the object level problem.** ***Just*** **focus on generating strategies that might** ***help*** **you solve the problem. **\n4.  Look at my list of meta-strategies, and see if there's one that I feel at least reasonably optimistic about. \n5.  If so, try that meta-strategy.\n6.  If not, brainstorm more. (But: note that \"take a break\", \"nap\", and \"ask a friend for help\" all totally count as valid meta-strategies to try. Taking a nap is often pretty important, actually!)\n7.  When/if I eventually solve my problem, **take note of what strategies and meta-strategies I ended up using.** Ideally, write them down somewhere I'm likely to remember them again.\n\nI want to repeat emphasize \"setting a real timer, for at least 5 and maybe up to 10 minutes, where you only allow yourself to generate meta-level strategies.\" (I think this is particularly valuable when you're training the skill)\n\n**Exploring multiple plans before committing.**\n\nPartly, this is because it just takes a little while to shift out of \"object level mode\". But, more importantly: because your problem is confusing, your ways of thinking about it might be somewhat off track. And, even if you'd eventually solve your problem, you might be doing it using a way less efficient method. \n\nIn particular, many problems benefit from going \"breadth first\", where instead of barreling down the first plan you came up with, you try ~3 plans a little bit and see if one of them turns out to be way better than your initial plan.\n\n**Come up with multiple \"types\" of metastrategies.**\n\nWhen you're doing the 5-10 minutes of brainstorming, I recommend exploring a variety of strategies. For example, there are conceptual strategies like \"break the problem down into smaller pieces.\" There are physical/biological strategies like \"take a walk, or get a drink of water\". There are social strategies like \"ask a friend for help.\" (sometimes this isn't appropriate if you're training, but is a fine strategy to use on real world tasks)\n\nExample: Writing this Blogpost\n==============================\n\nRight now I'm writing a blogpost on Metastrategic brainstorming. I actually found myself a bit stuck (a few paragraphs ago, before I finished the previous section). This seemed like a good opportunity to just demonstrate the technique right now.\n\nFirst, what are my goals? They're basically:\n\n*   **Get this post written** ***quickly.*** This is actually a pre-requisite for another post (I'm kinda procrastinating on the other post by writing this one because I hoped I could just bang it out in a couple hours)\n*   **Try to convey a fairly opaque skill, to people who don't intuitively get it.** It turned out at my [previous workshop](https://www.lesswrong.com/posts/h5mDx2Mt2P5m9v588/fractal-strategy-workshop-report) that one participant didn't really have the skill and I hadn't set aside time for teaching it.\n\nWith that in mind, I set a 6 minute timer. Here's what I came up with:\n\n1.  Just set a 30 minute timer and write without stopping even if the words I'm writing feel dumb. (Sometimes that just works)\n2.  Ask myself \"What feels hard about this?\" and engage curiously with whatever comes up. \n3.  Find a friend I want to explain this to, talk to them, and see how I end up explaining it to them.\n4.  Take a walk.\n5.  Take seven deep breaths.\n6.  *(I notice now that I feel stuck on generating metastrategies and feel \"surely the ones so far are good enough and I can stop.\" Maybe that's true. But one of the things I want to illustrate here is generalized 'get yourself unstuck and keep generating good ideas'. So, I ask: if it turned out it was really important to keep going, what ideas might I still be missing?)*\n7.  Ask myself \"What's the *best* version of this blogpost?\" If I didn't want to settle for merely \"having written up a decent post on Metastrategic Brainstorming?\", what would be missing?\n8.  Reflect more on why I decided to write this post right now. (Am I even doing the right thing?)\n\nOkay, timer just went off. \n\n<table><tbody><tr><td style=\"background-color:hsl(0, 0%, 90%)\"><p><strong>Sidebar: </strong>A few notes on how I generated these ideas:</p><p>In this case, I had a fair amount of experience trying tools to get over writers block, and I was mostly jogging my memory with those tools.</p><p>On step 6, where I noticed I had run out of steam, I knew from experience that asking \"okay, but what if it was really important to keep going, what would I miss?\" and \"what would make this like 10x more impactful?\" had previously been fruitful to ask.</p><p>If you're just starting out on metastrategy brainstorming, you probably won't have as clear a sense of what's helpful. I developed this skill with the <a href=\"https://www.lesswrong.com/posts/pDkZitYsJAwf8mHKJ/babble-challenge-50-ways-of-sending-something-to-the-moon\">Babble Challenge</a> series. A key thing is to relax your standards when you get stuck (i.e. if it's been a few minutes and you haven't successfully generated anything). Writing down a few \"bad ideas\" can grease your mental gears and get you generating some good ideas again.</p><p>There's an art to finding the right level of \"babble/prune\" ratio. In this case I felt like I had traction on getting \"actual good ideas\", and I didn't bother writing down ideas that I knew weren't actually going to be that useful in this context</p></td></tr></tbody></table>\n\nHow do I feel about those ideas? \n\nWhen I reflect on my two goals (write quickly, and convey a fairly deep skill), the strategies that feel most salient to me are:\n\n1.  Mostly, focus on setting a short timebox, and shipping the post in some form at the end of the timebox even if it's not perfect.\n2.  Imagine what questions the guy at the previous workshop would have, that this post might be able to answer. (The guy isn't around right now, but, I think simulating him might be good enough)\n\nThose are still slightly in tension with each other. Also, because I want to get this done in one sitting, I don't want to spend *too* much time metabrainstorming. But I notice I don't quite expect those two tools to work *quickly enough.* So I decide to give myself ~1 more minute for metastrategizing. \n\nAnd the thing that comes up in that minute is \"first, write an outline of what *must* be in the post when I'm done.\"\n\nThat list is:\n\n*   Introduce the idea of metastrategy.\n*   Write down the simplified algorithm that I generally run.\n*   Write out at least one worked example (that's what I'm doing right now)\n*   Give a list of suggested exercises for practicing the skill.\n\nThat all feels doable. Something I feel slightly dissatisfied with is \"but how do you generate strategies tho?\". There is something magical-feeling about the process. I think I will mostly shrug and hope that the suggested exercises get help develop the skill, even if it initially feels opaque.\n\nSuggested Exercises\n===================\n\nThe way I suggest learning this skill is, simply, to try tackling some problems that feel genuinely hard, which you don't have a good playbook for solving. (You can often learn more from failure than from success, if you're able to eventually look up the solution and get an explanation of it)\n\nVarious kinds of puzzles and games can make for good exercise test-beds. You can dial up the difficulty by giving yourself the goal of being *very confident* in your answer, or by trying to beat a video game on the very first try (despite limited information). You can also dial up the difficulty by trying to solve it *faster* (although I recommend first aiming to solve it \"at all\").\n\nI've spent the past year exploring different puzzles, games and exercises. Sometimes I've written up particular exercises that took advantage of a given puzzle or game's strength. Here, I present them for you to consider in their raw form. \n\nMy general approach is:\n\n*   Try to solve the problem for ~10 minutes\n*   Do ~10 minutes of meta brainstorming\n*   Try to solve the problem again. (alternate between object-level and meta-brainstorming however feels appropriate)\n*   When you're done, **reflect on what you learned** – which metastrategies turned out to actually help? What other situations do you expect them to apply to?\n\nThe last part is the most important part. Your goal is not to beat a given puzzle. Your goal is to find generalizable problem solving tools, and to *learn the taste of whether a given tool is appropriate for a situation.*\n\nIn the real world, you'll face confusing problems that don't have a clear answer, where there is no one to tell you what strategy to use. I'm hoping, with this exercise, you learn the art of \"teaching yourself to fish\", rather than me teaching you how to fish.\n\n* * *\n\nAppendix: Existing available puzzles/games I've used\n----------------------------------------------------\n\n### Thinking Physics\n\nI first started working on this skill in the context of [Thinking Physics](https://www.lesswrong.com/posts/PiPH4gkcMuvLALymK/exercise-solve-thinking-physics), a \"reverse physics textbook\" where instead of reading up on physics principles and then testing your knowledge with exercises, you are given a series of *questions*, which you try to solve (maybe taking multiple hours or even days to solve, from first principles), and then when you turn the page you'll see an explanation of the underlying physics phenomena.\n\n### Games\n\nI've found various puzzle and strategy games good for this exercise. [A good videogame here](https://www.lesswrong.com/posts/DvRBSzFjfaPYBhwmj/one-shot-strategy-games) is easy to jump into without much preamble, and takes 30 to 120 minutes.\n\nSome examples include:\n\n*   The puzzle game Baba Is You has been particularly fruitful for me. (I've developed multiple exercises based on it)\n*   Battle for Polytopia. *(note: this will start you off in a tutorial. I recommend exiting to the main menu, and then starting a new game, on \"Hard\" difficulty with 2-3 opponents)*\n*   Into the Breach. *(Note: has a minute or two of introduction you need to click past before you get to the first level)*\n*   Luck Be a Landlord.\n\nYou can read other games people have suggested in this [One-Shot Strategy Game](https://www.lesswrong.com/posts/DvRBSzFjfaPYBhwmj/one-shot-strategy-games) thread, although I'm not sure they're all appropriate.",
      "plaintextDescription": "I want to develop rationality training, which is aimed at solving confusing problems. \n\nTwo key problems with \"confusing problems\" are:\n\n 1. You might feel so confused and overwhelmed that you bounce off completely.\n 2. You might be confused about what counts as progress, or where the most progress is possible, and accidentally work on the wrong thing.\n\nA skill that helps with both of these is \"metastrategic brainstorming\" – the art of generating lots of potential good approaches; then choosing approaches that are likely to help. (And then, reflecting on whether those strategies worked)\n\nDifferent situations call for different sorts of strategies. If a problem is confusing, you probably don't have a simple playbook for dealing with it. Different people also benefit from different sorts of strategies. So, while I can tell you a list of potential mental tools, what I most want you to practice is the art of identifying what would help you, in particular, with the situation in particular in which you find yourself.\n\n\nStrategy vs Metastrategy\nWhy \"metastrategic\" instead of \"strategic?\". In war, a \"strategy\" is something like \"send a bunch of cavalry on a surprise attack.\" A metastrategy is more like \"let's invent a tabletop war simulation that helps us think about the situation\" or \"go ask an advisor for help\", which helps you identify strategies like \"surprise attacks.\"\n\nIn the case of intellectual problem solving, a strategy might be something like \"solve a particular simpler subproblem\", and a metastrategy would be \"look for related subproblems that you might be able to solve that might help unlock more insights.\"\n\nSometimes I find it valuable to force myself to only consider new metastrategies, and avoid thinking about object-level strategies, to force my brain out of a rut.\n\nMy triggers for switching to \"metastrategic brainstorming mode\" are:\n\nFirst, one of the following:\n\n * I've just sat down to work on a problem I already know is hard.\n * I've starting to feel st",
      "wordCount": 1880
    },
    "tags": [
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  }
]