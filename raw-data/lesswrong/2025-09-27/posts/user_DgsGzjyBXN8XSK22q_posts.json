[
  {
    "_id": "3semWY8cZJN3pD66g",
    "title": "MATS 8.0 Research Projects",
    "slug": "mats-8-0-research-projects",
    "url": "https://substack.com/home/post/p-171758976",
    "baseScore": 21,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-09-09T01:29:41.189Z",
    "contents": {
      "markdown": "The 8th iteration of the Machine Learning Alignment & Theory Scholars ([MATS](https://www.matsprogram.org/)) Program has come to a close, and we want to share the research projects our scholars have been working on this Summer. This cohort had 98 scholars who conducted research with 57 top mentors in the fields of AI alignment, transparency, and security.\n\nOn Aug 22, we hosted a Program Symposium to showcase their projects to the community; we invited 10 scholar teams to give spotlight talks based on their mid-program Scholar Research Plans, and every scholar contributed to the poster session. You can check out all of the projects in this post!",
      "plaintextDescription": "The 8th iteration of the Machine Learning Alignment & Theory Scholars (MATS) Program has come to a close, and we want to share the research projects our scholars have been working on this Summer. This cohort had 98 scholars who conducted research with 57 top mentors in the fields of AI alignment, transparency, and security.\n\nOn Aug 22, we hosted a Program Symposium to showcase their projects to the community; we invited 10 scholar teams to give spotlight talks based on their mid-program Scholar Research Plans, and every scholar contributed to the poster session. You can check out all of the projects in this post!",
      "wordCount": 105
    },
    "tags": [
      {
        "_id": "YYFBmLCzeFsyd27rd",
        "name": "MATS Program",
        "slug": "mats-program"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CPha83BQJvAjQuMNh",
    "title": "AXRP Episode 46 - Tom Davidson on AI-enabled Coups",
    "slug": "axrp-episode-46-tom-davidson-on-ai-enabled-coups",
    "url": null,
    "baseScore": 11,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-08-07T05:10:04.131Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/ltZfc9E6sUQ)\n\nCould AI enable a small group to gain power over a large country, and lock in their power permanently? Often, people worried about catastrophic risks from AI have been concerned with misalignment risks. In this episode, Tom Davidson talks about a risk that could be comparably important: that of AI-enabled coups.\n\nTopics we discuss:\n\n*   [How to stage a coup without AI](#coup-without-ai)\n*   [Why AI might enable coups](#why-ai-might-enable-coups)\n*   [How bad AI-enabled coups are](#how-bad)\n*   [Executive coups with singularly loyal AIs](#exec-coups-with-sing-loyal)\n*   [Executive coups with exclusive access to AI](#exec-coups-with-excl-access)\n*   [Corporate AI-enabled coups](#corp-coups)\n*   [Secret loyalty and misalignment in corporate coups](#secret-loyalty-misalignment)\n*   [Likelihood of different types of AI-enabled coups](#likelihood-different-types)\n*   [How to prevent AI-enabled coups](#how-to-prevent)\n*   [Downsides of AIs loyal to the law](#downsides-law-loyalty)\n*   [Cultural shifts vs individual action](#cult-shifts-ind-action)\n*   [Technical research to prevent AI-enabled coups](#tech-research-prevent)\n*   [Non-technical research to prevent AI-enabled coups](#non-tech-research-prevent)\n*   [Forethought](#forethought)\n*   [Following Tom’s and Forethought’s research](#following-research)\n\n**Daniel Filan** (00:00:09): Hello, everybody. In this episode, I’ll be speaking with Tom Davidson. Tom is a senior research fellow at [the Forethought Institute for AI Strategy](https://www.forethought.org/). His work is focused on AI takeoff speeds and more recently, the threat of humans using AI to stage a coup. To read a transcript of this episode, you can go to [axrp.net](https://axrp.net). You can become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast). You can also give feedback about the episode at [axrp.fyi](axrp.fyi). All right, Tom, welcome to the podcast.\n\n**Tom Davidson** (00:00:34): Pleasure to be here, Daniel.\n\nHow to stage a coup without AI\n------------------------------\n\n**Daniel Filan** (00:00:35): Yeah. So today we’re going to talk about the… Should I call it a paper? [“AI-Enabled Coups: How a Small Group Could Use AI to Seize Power”](https://www.forethought.org/research/ai-enabled-coups-how-a-small-group-could-use-ai-to-seize-power), by yourself, Lukas Finnveden, and Rose Hadshar. “Paper” is the right term for that?\n\n**Tom Davidson** (00:00:49): Yeah, I think we’d mostly called it “report”, but “paper”, “report”.\n\n**Daniel Filan** (00:00:53): “Report” seems pretty reasonable. Yeah, so: AI-enabled coups, I guess it’s about using AI to do coups. In order to just help the audience figure out what’s going on: ignore the AI stuff. I’m lucky enough that I’ve lived in countries that just have not had a coup in a while and I don’t really think about them. How do you do a coup?\n\n**Tom Davidson** (00:01:12): Great question. So the way that the word “coup” is normally used, there’s two main types of coup, at least on one way of carving it up. Different scholars carve it up in different ways, but one natural way to carve it up is that there are military coups: that is a coup performed by the military as directed by some people within the military. Most often it’s very senior military officials because they already have that authority within the military that would allow them to do that. And so that’s just a case of: senior general instructs battalions to seize control of various important buildings, threatens or intimidates people that might try to oppose, declares that they are now victors over the radio waves. Everyone notices that no one’s really opposing them and that all the military is acting in accordance with that declaration and so it seems to be increasingly credible. And then this very abrupt transition of power from the old leaders that were functioning within that old legal system. There’s this sudden abrupt and illegal shift of power. That’s a military coup.\n\n(00:02:25): Then the other type of coup that I would point to is referred to as an “executive coup”. And so that is when you have, typically, someone who is already the head of state, so already has legitimacy as a very powerful political figure. But to begin with, there are many checks and balances on their power. So especially in democracies, they will be heavily constrained. And then an executive coup normally is less discrete, but there might be a point at which you think they’ve really just kind of fully removed that last check and balance. But the general process there is the head of state is undermining the independence of the judiciary by stuffing it with their loyalists. They’re, again, stuffing the legislative bodies, again with people who are going to support them and actually making legal changes to centralize more and more power in the executive branch, in themselves.\n\n(00:03:21): And then at some point in that process you might be like, okay, at this point the old legal order has really been overturned and there’s been an executive coup. And so Venezuela I think is the best case study of this happening end to end, because it did start off in the mid-20th century as a pretty healthy democracy, that had been going for many decades. But by 2020 \\[it\\] was widely considered to just now have been an authoritarian country. And at some point… Mormally I think people near the end of that process might say, “Okay, there was an executive coup at that point”, but it’s much more fuzzy concept in that case.\n\n**Daniel Filan** (00:03:56): Is that also…? I’ve sometimes heard the term “self-coup” or “autogolpe”. Is that the same thing?\n\n**Tom Davidson** (00:04:01): Yes, exactly. Yeah.\n\n**Daniel Filan** (00:04:03): Okay. So I can kind of understand… So an executive coup, I kind of have a picture in my head of: okay, you consolidate power within yourself, you make the laws accountable to you. Somehow you turn the state more and more into yourself and all of the organs of the state that are loyal to the state are now loyal to you because you just have a bunch of control over the state. For a military coup, what do you actually do? Do you have to just go to Supreme Court and point guns to the heads of the justices and go to all the police stations and point guns to the heads of the leaders of their…? Or just help me imagine this concretely.\n\n**Tom Davidson** (00:04:47): Yeah, great question. So with a military coup, there is a higher risk that you declare a coup, but then the rest of society, the other organs of government and the other important economic players don’t want to play ball.\n\n(00:05:05): And so historically that has been a real risk and there have been cases… There’s a case in Ghana where there had been a military coup, and then there was such widespread dissatisfaction throughout various parts of society with the way that things were being governed that they ended up handing back to a more democratic governing regime. Because the military personnel are ultimately continuous with the rest of society. And so if they’re just brazenly unpopular and the country is just doing badly and that’s quite clear to everyone, then it doesn’t really work for them to just be like, “We’re forcing you to do everything we say” when people increasingly don’t want to play ball. And so yeah, historically that’s been a big issue.\n\n(00:05:54): I know we’re not meant to get onto the AI part, but I do want to just flag that I think very powerful AI will somewhat change that dynamic, because in the case of Ghana, for the country to function well, it needed all those other parts of society to play ball. But if we have sufficiently powerful AI systems, it may be possible to replace those other players with automated replacements.\n\n**Daniel Filan** (00:06:23): So okay. There’s one end, where some general of some army just stands up and says “I declare a coup” and then sits down. Presumably that’s not a successful coup. Somehow you need to actually take control. But countries are really big. There’s a lot of stuff going on. There’s more than one building where people are making laws, where the laws are getting enforced. So are you storming five buildings? Are you convincing the rest of the military that they should be loyal to you? And are you convincing the rest of the world that if they defy you, you’ll go in and use all the military to kill them? Is that what I should imagine?\n\n**Tom Davidson** (00:07:16): There’s a great book that I think might get to the heart of what you are asking here called [“Seizing Power”](https://muse.jhu.edu/book/31450) by \\[Naunihal\\] [Singh](https://en.wikipedia.org/wiki/Naunihal_Singh_(academic)). And its thesis is that at the core of the logic of a military coup is trying to create a common expectation among all military forces that the coup will succeed.\n\n(00:07:38): So the basic thesis is: no one wants to get into a bloodbath where people are killing their civilians and even other military people from the same country. And so ultimately everyone wants to be on the side of the winner if there is a struggle for power. And according to this thesis, which I find fairly compelling, that is the main determinant of what military personnel are going to do when there’s a constitutional crisis when someone’s attempting to do a coup.\n\n(00:08:02): So this book says that what your task is as someone who’s trying to do a military coup is you need to convince initially the military personnel that this is a fait accompli, that you already have the support of all the other military personnel. And so there’s this interesting game theory dynamic where there’s multiple stable equilibria. Before the military coup begins, the stable equilibrium is people may hate the regime, but they think that the regime has the support of the military. So if they went out and tried to start undermining law and order, they would expect that other people would come and arrest them, and they’d be right, because other people, even if they also dislike the regime, would indeed come and arrest them because that’s their job. And if they don’t do their job then they expect that their associates will look badly on that. And so it’s a self-reinforcing equilibrium.\n\n(00:08:49): And so what you’ve got to do when you’re doing a military coup is shift that equilibrium over to, no, actually now there’s this new equilibrium where there’s \\[now this\\] new group of people in power. And so a lot of the things that you see happen in military coups can be understood as trying to achieve that shift in equilibrium, that shift in consensus about who is now in control. And so a classic example that Singh gives in this book is capturing radio stations and then sending forth proclamations of your victory which are credible. You don’t massively exaggerate, you don’t say, “Every single person in this country has supported me always”, because that would ring false. But you say, “We have the backing of the senior military generals. We have executed a coup. The old government has been defeated.”\n\n(00:09:41): And then you do things to make those claims seem more credible, like seizing control of key institutional buildings, for example. And then the absence of vocal resistance, the absence of vocal opposition and of military warfare then just serves to reinforce this new impression. Because you said it, it kind of looks like it’s true if you look at where the military things that have just happened, and then no one is saying anything else. And if you can get that consensus of opinion within the military and then you can convince the rest of the society that, yes, all of the military, all of the hard power supports this new regime, then the rest of the society also… Why would you oppose a new regime which is backed by all the hard power? You’re just going to get yourself in trouble.\n\n(00:10:33): So at that point, the rest of society’s incentive is then to work within the new regime. As you say, things were set up with the old regime so there will be questions about how you pragmatically reorganize exactly what everyone’s doing and how everything fits together. But once you’ve got essentially everyone in society recognizing, yes, this is the new order and we can see that that’s what everyone’s going for, then that’s the hard work done. And then it’s more like filling in the details of exactly how the new set of bodies will relate to each other and what the chain of command will be from the new leader to the various parts of society.\n\n**Daniel Filan** (00:11:14): So the picture I have is roughly: in order to do a military coup, I’ve got to persuade the military that we’re doing a coup. Somehow I’ve got to persuade the military and the rest of the society presumably that if anyone defies my new order, we’re going to come in and we’re going to beat them. And presumably in order to do that, I’ve actually got to go in and beat some people who might be considering defying my new order just to demonstrate that I can, and maybe that’s why I actually storm the buildings.\n\n**Tom Davidson** (00:11:50): What do you mean when you say beat someone? I mean I think, yes, you need to show some credible sign that you have the support of military forces. I don’t know if you literally need to then go and shoot down protestors. That’s one effective way to show that you’re willing to beat people down. But there’s often coups without any bloodshed.\n\n**Daniel Filan** (00:12:11): Yeah, I just mean demonstrate that if someone tries to resist, they won’t succeed. So maybe that involves you killing people. Maybe that involves you marching in and people are visibly too afraid to stop you and this is just a sign that, okay, apparently you can do what you want now.\n\n**Tom Davidson** (00:12:30): Yeah, there’s a great part of this book where it discusses how a common tactic for creating this new shared understanding is to host a meeting with all the top brass of the military and say, “We are staging a coup. All of you have agreed and all of you are on board.” And then you just watch as no one opposes you, because everyone thinks it’s probably kind of plausible, and maybe some of them were on board, others had said they might be on board and had been ambiguous. But once you’re there and you say it and no one opposes you, that already sends quite a strong signal. And then often that kind of meeting could be where that essential shift in equilibrium actually happens.\n\n**Daniel Filan** (00:13:13): Okay. So I think I understand coups at this point. The next thing I want to know is: how bad are coups?\n\n**Tom Davidson** (00:13:19): Yeah. It’s a really interesting question. Now coups are most common in countries that are not robust democracies. In fact, they’re very rare in robust democracies. So a coup in the United States I think would be very, very bad because we currently have a system of governance with checks and balances and democracy. And I think we’d be losing a lot if we had a coup. When coups have happened historically, they’re often starting from much less good governance systems and so they have been less bad. But still, coups involve a small group of people just using hard power to force the rest of the country into submission. And often they are extremely bad.\n\n**Daniel Filan** (00:14:09): Fair enough.\n\n**Tom Davidson** (00:14:09): Bad from a process perspective in terms of justice, but also bad for how the country is governed thereafter.\n\n**Daniel Filan** (00:14:15): Gotcha. Okay. I’ve realized I want to get back into how to do a coup for a final bit. So in some parts of your report, you mentioned that you only need a small fraction of the military to be on board with a coup to succeed, at least often. That seems crazy to me because if you have one third of the military and I have two thirds of the military, I would naively think that I could beat you. I don’t know, maybe if you have the best one third of the military, if you have the one third with nukes. What’s going on there? How much of the military do you really need to get on board in order to do this?\n\n**Tom Davidson** (00:14:50): So if you have two thirds of the military and there’s a strong common knowledge among those two thirds that they’re all on your side, then I agree I can’t do a coup with my one third because I’m outgunned. But if instead, there’s just the whole military which kind of thinks, “Yeah, currently Daniel is in charge.” Let’s say you are the incumbent. And then I come along and I get my one-tenth of the military, storm all the buildings, threaten the key commanders of the military not to say anything. And I create these credible signals that in fact a large part of the military supports me. Even if that remaining two thirds actually backs you, if they don’t know that they all feel that way and there’s big uncertainty among their ranks in how they feel and it just seems like, man, it seems like everyone’s backing this new leader and that’s what they’re saying on the radio waves and none of them are denying this, then I can flip that equilibrium. And I flipped it into an equilibrium that people are less happy with, that military personnel are less happy with.\n\n(00:15:51): But because they’re not able to all get together in a group and be like, “Do we like this new leader or not?”, because that’s dangerous, because they don’t want to be seen as opposing the new legitimate regime. Otherwise… Under this new equilibrium, that’s not a good thing for people to know about you. So because of those dynamics, it is possible for a minority to stage a coup.\n\nWhy AI might enable coups\n-------------------------\n\n**Daniel Filan** (00:16:17): Gotcha. Okay. So now that we know coups are bad, especially if you do them to a country like the US. And now that we know how we could do a coup if we really wanted to, let’s go to AI-enabled coups. So you’ve written this report on AI-enabled coups, but my sense is that that’s because you think they’re bad and worrisome. And presumably you believe that they are both somewhat plausible and quite, quite bad out of the space of things that could happen. Probably we should first go on why you think AI-enabled coups might be a plausible thing that could happen.\n\n**Tom Davidson** (00:16:55): Yeah. So we can start off with where the historical evidence leaves us, which is mature democracies are pretty robust to military coups. They are not recently looking nearly as robust to executive coups. So there’s been democratic backsliding in… The best example recently might be Hungary, which is become increasingly autocratic through the gradual removal of checks and balances. There’s the example of Venezuela we discussed earlier, and many commentators think that this is happening to a very large extent in the United States as well. So I think historically, we can say military coups do seem very rare and executive coups seem rare, for sure, but not off the cards at all and many people are worried about them even before bringing AI into the picture.\n\n(00:17:54): Now we can bring AI into the picture, and I think the first thing AI does is it makes executive coups seem a fair bit more plausible. And \\[there are\\] two main reasons it does that. The first is that a group of people in the executive that wanted to do an executive coup might be able to gain a lot of control over very powerful AI in a way that gives them a big strategic advantage over the other forces in society. The dynamics of executive coups as they play out typically involve a power struggle between the executive trying to centralize control (and their various supporters), and the checks and balances that were in the system already trying to oppose them. And often there’s really a lot of push and pull. And sometimes, in the case of Venezuela, the head of state was literally put in jail for a bit by their opponents, then got out and they got reelected and then ended up really becoming an autocrat. So there’s all this strategic maneuvering.\n\n(00:19:03): And so the first thing is that if the people trying to do the executive coup can get a lot of control over powerful AI and can deny access to similarly powerful AI to their opponents, that could just give them a big strategic advantage in that political maneuvering. That’s the first dynamic, which I think makes it higher \\[risk\\].\n\n(00:19:23): The second dynamic is that today, people who are trying to do an executive coup or have already centralized power need to rely on lots of other humans to help them out. And that constrains their actions in various ways. So normally it’s hard for them to be completely brazenly power-grabbing. They need to come up with plausible ideologies and justifications that they can then get supporters to rally behind and support particular moves. But with AI systems that are sufficiently powerful, you can replace those humans with AI systems. So rather than having the policies of the government implemented by humans that have some ethical standards and that don’t, at this point, really want to support really awful surveillance and that have been brought in on a broad ideology, you can just replace them with AI systems that will just follow the instructions of the head of state with far fewer qualms. And so that can give that head of state that’s trying to do an executive coup an additional edge because they’re less constrained by having to work within this broad coalition involving lots of other humans.\n\n(00:20:50): And then the most extreme example that we highlight quite a lot in the paper is indeed armed forces. Like today in the United States—this is very stark—the military personnel are very, very opposed to breaking the law. They’re very much loyal to the Constitution, they strongly expect that all the other military personnel are going to do the same. And so it’s a really, really tall ask for the head of state in somewhere like the United States to get active help from the military in staging an executive coup. And indeed Trump has come into some kind of frictions with the military when he’s tried to get their help for deterring certain protests.\n\n(00:21:32): But again, this could be a really major shift as we increasingly automate the military with very autonomous weapon systems where, again, the thing we highlight is the most extreme case where you can fully replace a human soldier with a military robot. At that point, under current law, it might be completely legal for those robots to be programmed to just follow the instructions of the commander in chief, the head of state. And so we’d move from this current situation where if you’re trying to do an executive coup in the United States, you’re not going to get much help from military personnel, to this new state where it’s kind of up for grabs what are going to be the loyalties and the decision processes of this new automated military. And so this just introduces a big new vulnerability for really cementing an executive coup with hard power.\n\n**Daniel Filan** (00:22:26): Gotcha. So I guess it seems like there’s two key factors here, two progress bars on AI capabilities it seems like you want to keep track of. The first is roughly: how useful is AI for navigating strategic maneuvering? You’re like, “Oh, I’m in prison, but these people think this and these people think this.” To what degree does AI really help you in this situation? And then the other one is: how loyal can you have the AI be for you? So in the report you talk about \\[how\\] you want the loyalty ideally to be both singular to you and also secret. Other people don’t know about the loyalty. And in addition, a thing that seems like it’s important is just that all of these things have these loyalties. If you have a bunch of AIs, but they’re about as diverse as people are, it seems like this is probably harder to get off the ground.\n\n**Tom Davidson** (00:23:27): Well, I’d push back a little bit: certainly the more the better. But if you could get, say, 10% that are loyal to you secretly and 90% that are just… If there’s any chaotic constitutional crisis, they defer to inaction, then that would be enough, because you get your 10%, you get your fait accompli, that 90% don’t do anything to block it and there you are.\n\n**Daniel Filan** (00:23:49): Fair enough. That’s actually a really good point. So in either of these aspects, how much evidence do you think we have about how useful AI is going to be? How much of these relevant capabilities does it seem to have already?\n\n**Tom Davidson** (00:24:16): It’s a good question. There have recently been a few studies that were pretty surprising to me on AI persuasion. I unfortunately don’t remember the details, but I will just give my high-level memory, which is that there was [one study](https://retractionwatch.com/2025/04/28/experiment-using-ai-generated-posts-on-reddit-draws-fire-for-ethics-concerns/) that had AI posting on Reddit and then compared the number of upvotes to human commentators who were also posting. It was like, “Persuade me of X.” And the AI would do a background research about this person and what their demographic was, would tailor-make a really emotional story about their own life that really brought it out. And the results were in some crazy high percentile, I think it might be the top percent or close to that for persuasiveness. And I was quite surprised because that had not been a capability that I’d thought that we were targeting with current training techniques.\n\n**Daniel Filan** (00:25:17): And one thing that’s crazy… So this was done on [the subreddit r/changemyview](https://www.reddit.com/r/changemyview/), so it was research that was done… My understanding is that it will not end up being published basically because… So r/changemyview, it has basically some rule that you’re not allowed to have an LLM pretend to be a human and try to persuade a bunch of people of stuff. That’s not okay according to them.\n\n**Tom Davidson** (00:25:43): Crazy.\n\n**Daniel Filan** (00:25:45): Yeah. So unfortunately we probably won’t learn as much as we might like to about that study. But the other thing is: so this was done at, I believe, the University of Zurich, which… I’m sure they have fine people, but this is not the world’s leading AI lab or the world’s leading graduate program in AI. So the fact that this obviously competent but not top-tier AI institution can do it, maybe that lends credence to like, “oh, this has gone further than you might think.”\n\n**Tom Davidson** (00:26:22): Yeah. And I think there’s been one other study, and I actually can’t remember the details of this, but again, it found that AI was close to top percentile humans in persuasiveness. So that’s updated me towards thinking that AI might be very good at this kind of strategic maneuvering aspect because one element of that is persuading people and historically often that’s taken the form of persuading people of an ideology which serves your purposes. And that seems like the kind of thing that these studies are looking at. They are often studying, okay, here’s political topic X, can you shift my opinion on it?\n\n**Daniel Filan** (00:27:04): Fair enough.\n\n**Tom Davidson** (00:27:05): There’s then another relevant capability beyond persuasion, which is something like strategic planning, which is essentially: you’re in a situation, you want to achieve a goal, what plan is best to achieve that strategic objective? And it’s really hard to predict these things, but it doesn’t seem to me like the current training procedure is really bringing that out. \\[For\\] persuasion, at least, it’s obviously trained on loads of conversations where it can see what’s persuasive and what’s not. It’s not that surprising that it’s generalized well there. For strategic planning, it feels more like it would need to have been trained in situations where there’s a scenario and then an action is taken to try and achieve an objective. And then there’s a really complicated socio-political system and then it washes out and you see what happens.\n\n(00:27:57): And it’s been trained on the internet, which contains loads of history, and you can probably extract that kind of stuff from history. But it seems more of a stretch to think that from pre-training it’s going to generalize, pick out those lessons, because it’s so much less direct. So this is the kind of thing where you can imagine someone setting up some kind of fancy RL pipeline down the road where they try and extract all of the relevant signal that is currently fairly implicit in internet data and craft and give it to AI and maybe also have AI try \\[to act in\\] various simple artificial environments, and then maybe have an AI actually try and achieve things in the real world and learn from that. But I would expect it to come a bit later in the capabilities tree compared to things like coding and maths where you can get a good automated feedback signal.\n\n**Daniel Filan** (00:29:00): Fair enough. So okay, that’s a little bit on why AI might be able to do coups and what might go into that.\n\n**Tom Davidson** (00:29:10): I’ll just quickly say… That was all in the executive coup part. The other thing I was going to say ages ago is that I think there’s this new risk of “corporate coup” where because AI is going to be so powerful, and it’s currently being developed and controlled and deployed by private actors, that by default, there’s going to be this new big concentration of power in those private actors. And I think that that will open up some routes to staging a coup. Now this is necessarily more of a speculative idea because we just don’t have the same historical precedent here. You do have some kind of coups staged historically by private companies. Normally it’s very rich, United States private companies operating in very poor countries. So the [“banana republic”](https://en.wikipedia.org/wiki/Banana_republic#Etymology) is the famous go-to example where this fruit company arranged for there to be a military coup, which helped with their own interest. But it’s pretty rare. And so this would be a new kind of risk. But I think the threat models here are plausible enough to be taken very seriously.\n\n**Daniel Filan** (00:30:31): So it seems like the threat model is something like, okay, you have this company. It’s making a thing that’s really dangerous, or a thing that could be used to be really dangerous, a thing that could be used to help you take control of power, and the people that make the thing use it to take control of power. It seems like there’s maybe an analogy in that: countries buy weapons systems and the weapons systems are really… The US Army would be much, much worse if they just had to use rocks and stuff, or if they had to swim. [Northrop Grumman](https://en.wikipedia.org/wiki/Northrop_Grumman) or the [BNS](https://en.wikipedia.org/wiki/BAE_Systems)…? That might be the wrong name \\[Editor’s note: the right name is “BAE Systems”\\]. But these weapons manufacturers, do we ever have instances of them being like, “Hey, we’ve got a ton of fighter planes, let’s do a coup ourselves.”?\n\n**Tom Davidson** (00:31:24): I’m not aware of any. I think there’s a few dynamics at play. One is that you need soldiers to use the weapons and those are trained by the military and they have this strong commitment to the rule of law. And another is that there’s multiple different military suppliers, multiple different companies, and so they would need to all be colluding. And AI does change both of those things. So on the weapons side, I believe we’re going to end up in a world of autonomous weapons and so you won’t need those additional humans in order to stage the coup. And so the companies will literally now be making all parts that are necessary for the military force. And the second is that there are dynamics that could point to very strong market concentration in frontier AI, i.e. maybe just one, two, or three companies that have the most powerful AI systems.\n\n(00:32:24): If those AIs are the ones that are making all the military weapon systems, in the most extreme cases, if it’s just one AGI project whose AIs are making all the military systems and there’s now the single point of failure, and that project is in an unprecedented position in that sense.\n\n**Daniel Filan** (00:32:45): Fair enough. I guess the other thing that seems maybe analogous is if a country hires a mercenary force to supplement its military, but it seems hard. For one, if the mercenaries are a small fraction of the military, maybe it’s harder for the mercenaries to create common knowledge that the non-mercenary militaries are on board with the coup. But are there examples of mercenary coups?\n\n**Tom Davidson** (00:33:07): Off the top of my head, I’m not aware of any where the mercenary is like…\n\n**Daniel Filan** (00:33:17): Oh, there’s [the thing that happened in Russia with the guy](https://en.wikipedia.org/wiki/Wagner_Group_rebellion) \\[Yevgeny Prigozhin\\]. Do you know his name?\n\n**Tom Davidson** (00:33:21): The guy who started marching towards Moscow.\n\n**Daniel Filan** (00:33:24): And then he gave up on it.\n\n**Tom Davidson** (00:33:26): Yeah, yeah. I think it didn’t end well for him.\n\nHow bad AI-enabled coups are\n----------------------------\n\n**Daniel Filan** (00:33:29): Yeah. But that seems like almost an example. Military coups, executive coups and company-led coups. It seems like there’s some plausibility to the idea that AI could increase the ability of these situations. I guess the next thing I want to ask is: so there’s a wide universe of things people worry about with scary things that advanced AI could do. How high up on that list of scary things should AI-enabled coups be?\n\n**Tom Davidson** (00:34:07): My current view would be that in terms of importance, it should be maybe second behind AI takeover.\n\n**Daniel Filan** (00:34:19): Interesting.\n\n**Tom Davidson** (00:34:20): And if you then factor in neglectedness, then I think it’s actually more important on the margin than AI takeover. And I think it’s more important than, for example, AI-enabled bio-attacks by terrorists, which is another risk from AI that people are focused on. And similarly, AI-enabled misuse in terms of cyber. I’d also put it as more important than that.\n\n**Daniel Filan** (00:34:48): Do you think it’s worse than AI-enabled terrorism or more likely?\n\n**Tom Davidson** (00:34:52): Disclaimer, I haven’t thought in depth about this comparison.\n\n**Daniel Filan** (00:34:56): Fair enough.\n\n**Tom Davidson** (00:34:56): But it’s easier for me to see how AI-enabled coups would have a completely long-lasting effect. So it’s certainly possible that an AI-enabled terrorist literally makes everyone go extinct, but full extinction is quite hard to get from a bioweapon, especially given that we’ll be using AI to develop defenses as we go. And there aren’t many people who want to see everyone die, and so we only need to stop those people getting access to these systems. And it seems like it’s not that hard to do that. I mean, past a certain point, it might be necessary to prevent open source. It also might never be necessary to prevent open source depending on how far ahead closed source is and how quickly we can get the defenses in place, and other inputs that are needed to actually do a bio attack.\n\n(00:36:02): Whereas with the AI-enabled coups, I think there are very many people who want more power. Many of those people will, by default, have a lot of control over AI and might well be in a position to do this. And the default dynamic of AI development I think is just going to really concentrate control of AI development and deployment in the hands of a very small number of people. And so if I’m telling the story, I’m just like: well, look, people want power. They’re going to by default have loads of power and the opportunity to use it to gain more power. It is kind of believable that they do it and they seize control. And then once they do, they just hang onto control. It doesn’t feel hard to tell a story where this lasts for a very long time.\n\n(00:36:52): Whereas in the case of bio, it feels a little bit more difficult because we have to not get the defenses in place despite the fact it’s in all of our interests and all the powerful people want to do that. We have to actually share these systems which - we are testing for this risk. We will likely have evidence that there is significant uplift. We have to make them so widely available that the tiny number of very low-resource actors that want to do this are able to. So yeah, that’s roughly where I am in terms of putting it as high priority.\n\nExecutive coups with singularly loyal AIs\n-----------------------------------------\n\n**Daniel Filan** (00:37:28): Yeah, I think that makes sense. Maybe it makes sense to talk a little bit about the scenarios of… Types of AI-enabled coups and stuff you could do to prevent them. So I guess at a high level, you’ve got your corporate AI coups, you’ve got your executive AI coups, and you’ve got your military AI coups. Which one are you most excited to talk about first?\n\n**Tom Davidson** (00:37:59): Let’s start with the executive.\n\n**Daniel Filan** (00:38:01): If I imagine what this looks like, should I basically be like, okay, you’ve got an executive. The executive somehow gets a significant amount of control over AI development. So in the executive coup, the executive is just using the AI to persuade people or figure out strategy in order to allow the executive to get gradually more and more power.\n\n**Tom Davidson** (00:38:29): The other thing they’re doing is that they’re deploying AI throughout society, especially in the government and military, but the AI is more loyal to them.\n\n**Daniel Filan** (00:38:37): So throughout the society and the military to make it more loyal to them, I guess part of what the executive is doing is trying to stop other AI-enabled coups.\n\n**Tom Davidson** (00:38:45): Potentially, if those are seeming plausible, if there’s a risk that there’s going to be a corporate coup, the executive would want to stop that. But I haven’t been thinking of that as a primary thing that they’ll need to do. The primary thing I think is to centralize power in themselves.\n\n**Daniel Filan** (00:39:03): Right. So less to prevent coups and more just to prevent independent other entities wielding any power. So you have these three risk factors, right? Singular loyalty, secret loyalty and exclusive access. And so it seems like part of this story is the executive uses the AI to do a bunch of tricky stuff, and other people can’t figure out how to stop them. This seems like it’s largely leaning on exclusive access, and the bit where the executive has everyone else use AI that the executive likes, seems like this is maybe leaning more on singular loyalty, and to some degree secret loyalty. Is that roughly right?\n\n**Tom Davidson** (00:39:52): That’s exactly the mapping. And normally with the executive \\[coup\\], I’m not imagining secret loyalties, although it’s possible, because the executive has so much political power to begin with, they could just be like, “It’s completely appropriate for these AI systems to be loyal to me.” They could do the more fancy thing of secret loyalties, but there’s a technical hurdle there and it just might not be necessary for them.\n\n**Daniel Filan** (00:40:16): So it seems like these are sort of two routes… Or I don’t know if two routes is the right term, but two things that the executive is doing with the AI. I’m wondering: do you need both of them? Or if you’re an executive trying to do a coup, could you survive with just one of these?\n\n**Tom Davidson** (00:40:32): I think you can do it with just the “singular loyalties in AIs deployed throughout society” version. So the story would be there’s heightened tensions between the US and China. We’re rushing out to deploy AI to the military and-\n\n**Daniel Filan** (00:40:53): “We” being the US?\n\n**Tom Davidson** (00:40:54): “We” being the US. The US is doing that and the head of state is saying, obviously military AI follows the commands of the commander-in-chief. That’s how it should be. That’s how the command structure works. We’ve never had autonomous drones check for whether things that legal before they follow their instructions in the past. They just do what they’re told. That default continues. And then people will very likely oppose and say, “This is crazy. Wait a minute, couldn’t you just stage a coup?”\n\n(00:41:26): But the head of state has their supporters and has a lot of power and has already set a precedent of really nailing people who push back against them. And so they succeed in pushing this through. And they never had access to any kind of super genius strategy AI because the strategy was just quite obvious: “Well, yeah, if we just get all the military robots loyal to me, obviously I now can do whatever I want.” And so I do think that second path can work by itself.\n\n**Daniel Filan** (00:41:54): And one concerning thing there is: so when you say, “Oh, the AIs are being loyal to the president and they’re not checking other laws and stuff”, I think it’s not a crazy argument that that is legally how it should work. Definitely, the president is literally the commander-in-chief, as you note. There’s a prominent legal theory called the [unitary executive theory](https://en.wikipedia.org/wiki/Unitary_executive_theory) that the president, just in his own person, has unitary control over the executive branches of government. Oh, I guess I don’t know if the military is executive, but…\n\n**Tom Davidson** (00:42:35): I think the design of the Constitution is very much intended to separate and limit the president’s degree of control over the military. It is very clear that the military is loyal to the Constitution. So I think if you were to take the spirit of the Constitution and apply it to a robot army, it would be clear that you shouldn’t just have the robot army doing whatever the president said without checks and balances. I think, though, that that is not how the Constitution was designed. It didn’t have caveats for what if we develop a robot army. So as it is currently designed, reading it line by line, I cannot be confident that it would rule out this loyal robot army as illegal.\n\n**Daniel Filan** (00:43:16): Fair enough.\n\n**Tom Davidson** (00:43:16): And so I think you’re right that you could make legal arguments that this is at least legitimate and yeah, you could claim it’s appropriate given the commander-in-chief, although I do think you’d be on shaky ground given the clearer intention of the Constitution.\n\n**Daniel Filan** (00:43:33): Yeah, yeah. I guess maybe one thing going on is: my understanding is that American jurisprudence, especially at the Supreme Court level, very much leans towards “what does the text say?” rather than “what do we believe the intention of the text was?”, which plausibly heightens this risk in this domain.\n\n(00:43:57): So going back to this story, let’s say they only have the “loyal AIs in the military” part. The president gets all these military drones or whatever, loyal to the president, and is the story that the president then does a military coup of, “if any police officers try to stop any of my supporters doing random violence or whatever, the military drone will shoot the police”? Is that roughly it?\n\n**Tom Davidson** (00:44:23): Probably it’s going to be in the president’s interests to not show more force than they need to, because it’s going to be useful for them to have everyone continuing to support their leadership and seeing it as legitimate. Probably what they do is they kind of increasingly ignore checks and balances on their power, and then increasingly it becomes clear that nothing is going to stop this situation because at the end of the day, if the protestors come, this time, the president can just order the drones to go and clear out that protest.\n\n(00:44:57): Probably not going to shoot everyone, but going to make them go home. And he wouldn’t have been able to do that before the robot army. And then increasingly the president is just doing what he wants, ignoring the checks and balances, integrating AI to replace all of the humans that aren’t doing exactly what he says he wants them to do. And if anyone ever tries to really refuse to go along, then at that point he just fires them and has them put in jail or something. And that’s kind of a show of strength. And then as no one is able to oppose this, because ultimately the hard power’s in the president’s hands, it just becomes increasingly clear who’s in charge.\n\n**Daniel Filan** (00:45:46): Sure. So if I think about this broad scenario, one thing that’s kind of interesting to me… So a background thing I’m thinking about when I’m reading this report is the relationship between AI-enabled coups and AI alignment or misalignment risk, right? And so if I imagine this somewhat minimal version of the executive coup, where basically the way it works is that you just have a bunch of military stuff and it’s powered by AIs and the AIs are… Or at least 10% of them or whatever are loyal to the president. The AI technology that enables that is just alignment.\n\n(00:46:29): Getting an AI to do what a person wants: that’s the problem that we call “alignment”, that we’re all hoping to solve. Some of the paths, I think, are: alignment research really would prevent them or make them a little bit more tricky. But this is interesting because it seems like a \\[path\\] that really is cutting against a lot of technical alignment work… Or ‘cutting against’ is maybe the wrong word, but not prevented by technical alignment work. I’m wondering if you have thoughts about that.\n\n**Tom Davidson** (00:47:01): I think that if the executive, the president knows that AI is misaligned, then he’s not going to be wise to give it control of the robot army. If the president believes that the AI is aligned and in fact it’s secretly misaligned, then the president might well give it control of the robot army, “align” it, think he’s aligning it to be loyal to him and then stage a coup, and then he will be laying the groundwork for AI takeover.\n\n(00:47:33): But in fact, the threat model of him staging the coup goes through even though he hadn’t solved the alignment problem. And my understanding is that people are mostly worried about this exact scenario where AI seems aligned, but it’s not. And so I think basically the threat model still goes through in that scenario. The difference that doing more technical alignment research \\[makes\\] is it means that rather than the president maintaining control of the world indefinitely or the country indefinitely after the coup, if you fail to solve technical alignment, then in fact the president is going to be replaced by misaligned AIs, which you may prefer or disprefer depending on various philosophical considerations.\n\n(00:48:17): But I wouldn’t particularly say that if you solve alignment, then you’re making this threat model a lot higher… Well, except to the extent that it’s then common knowledge that you’ve solved it. I think if it was going to be widely known that it’s not solved, then I agree, yes, you are increasing this risk.\n\nExecutive coups with exclusive access to AI\n-------------------------------------------\n\n**Daniel Filan** (00:48:35): So that’s how you could do it if you only did it via having singular loyalty throughout the military, just one half. Asking about the two halves—exclusive access to do really good planning, and loyalty of AIs distributed throughout. If you just had exclusive access, do you think you’d be able to do an executive coup just via that path?\n\n**Tom Davidson** (00:49:03): I think it’s a lot less clear. I think the main thing you would do with exclusive access is… The most obvious thing you would do is then try and convert this first path to that second path. So you’d use your exclusive access to get AI strategy advice and AI technical analysis about how you could get loyal AI systems deployed throughout critical systems. And so they might advise you to do secret loyalties. They might advise you on a particular political strategy for pushing through the more overtly loyal AI systems. I think that’s the most obvious route.\n\n(00:49:45): If you were like: could you use exclusive access to stage a coup without going via this other kind of “singularly loyal AI” approach? I don’t know how important this question is, but I think it’s basically unclear. If you buy into the more sci-fi-esque claims about what superintelligent AI will be able to achieve, then yes, you could do this because what you could do is you could set up a group of automated factories somewhere, maybe as part of a kind of military R&D project that you managed to push through, and then you just quickly make very powerful, fully automated weapons, nanobots or just amazing drones.\n\n(00:50:31): And then even though they weren’t really ever integrated into the official military, they just then straight out stage a coup. So you then stage a coup without having to integrate AI in any kind of formal institution, but it leans much more heavily on what you can get through super genius AIs and then a relatively small amount of physical infrastructure.\n\n**Daniel Filan** (00:51:00): There’s this interesting thing that’s going through the back of my mind as I read this. So in general, when someone is like, “oh yeah, I’m worried that in the future we’ll have more powerful AI and the powerful AI is going to mean that people can do a bad thing”, I think a natural question to ask is, “well, why don’t other people use the powerful AI to stop you from doing the bad thing?”\n\n(00:51:22): And so for the first path of executive coup where the president gets all the military AIs to be singularly loyal to him or her, presumably the reason other people don’t use AI to stop that is because this is at least arguably legal and arguably legitimate and at some point, _you’d_ be doing the coup if you resisted. And I guess exclusive access is another story where people don’t stop you because they just don’t have as good AI compared to you. I guess that’s more of a comment than a question, unfortunately.\n\n**Tom Davidson** (00:52:01): Yeah, I mean, I agree. I think it’s clear where the asymmetry comes with exclusive access, and then with singular loyalties, the asymmetry is: you (and not everyone else) \\[are\\] deciding the behavioral dispositions of these AIs deployed throughout society. And so you’re leveraging your political power to push through this asymmetric AI loyalty in broadly deployed systems.\n\n**Daniel Filan** (00:52:23): I guess part of the asymmetry here is: if you don’t have exclusive access, then presumably if people are willing to break the law, they can do some amount of preventing you from having exclusive loyalty by subbing in their AIs or using their AIs to help them figure out how to stop you - make it appear that things have exclusive loyalty to you, but they don’t actually.\n\n**Tom Davidson** (00:52:50): I’m not sure. So again, going back to the military case, you could have the hardware for these military robots. You could then be like, “I’m deploying this AI software, which is loyal to me.” No one else can then go and actually deploy their own AIs on some of those military robots because it’s just infrastructure that the government controls.\n\n(00:53:11): And similarly, you could imagine fully automating some kind of implementation body of government which has some formal authorities and now no one else can again sub in their own AI. Because their AI could do analysis, could make recommendations, \\[but\\] they wouldn’t have the formal legal authorities to take actions within the political system. And so again, they wouldn’t be able to sub it in. I think if we’re talking about human employees still working within those organizations and using AI systems, then it’s more like, okay, they could sub it in.\n\n**Daniel Filan** (00:53:44): And I guess it’s even tricky just because the president just inherently has a wider scope to do this. So one thing I think I’m imagining is there’s the president. There’s, I don’t know, a few branches of the military. The military has various admirals and then under the admirals or whatever, there’s a bunch of robo-soldiers, and I guess I could maybe imagine, okay, one of these admirals convinces their robo-soldiers to be loyal to the admiral and not to the president, but you sort of need a bunch of the admirals to do that, going to the earlier point of creating common knowledge.\n\n**Tom Davidson** (00:54:19): Yeah, I imagine it would be more people who are involved in the procurement process and the technical side of that process of setting up the AI software than people who are charged with making the strategic analyses in real time that might be in a position to do that. But yeah, I agree that there could be people in the military who could do that.\n\nCorporate AI-enabled coups\n--------------------------\n\n**Daniel Filan** (00:54:41): So okay. We’ve talked about the paths that an executive could use to do a coup. I think I want to pivot to the paths that an AI company could use to do a coup, because at first blush, it seems like both of the things you said could kind of work for the AI company. If the AI company can get all the robo-soldiers to be loyal to the AI company, and if the AI company can have exclusive access to its own AIs, that seems like maybe it’s putting the AI company in a pretty good place, right?\n\n**Tom Davidson** (00:55:14): Yes. I think for exclusive access, it’s easier for the AI company. They are just going to have it by default. For the head of state or the executive branch to get exclusive access, they would have to intervene in quite a substantial way with the development process. I think the thing that’s harder for the AI companies is that in terms of deploying these loyal AI systems throughout society, there’s a much higher technical hurdle where they have to make them secret, really hard to detect.\n\n(00:55:49): Let’s say they train GPT-7 to be secretly loyal. It could be five years later that someone discovers a new testing procedure, and if those weights have been stored somewhere, then someone realizes, “whoa, OpenAI trained a literal secret loyalty” and then the game’s up. So you probably want to really cover your tracks and lock down those weights, make sure no one can ever run any tests on them that you don’t want to run. Today, AIs are not that reliable, their personalities are somewhat haphazard, it’s all a bit of a mess. Today, I think it would be very hard to get away with a very hard-to-detect secret loyalty. And it might just be that it’s hard to predict how hard it will be to detect these secret loyalties.\n\n(00:56:41): So it might be that the company is considering this: we could put in this really subtle back door, but we don’t know what people are going to do on this API testing. We don’t know if some of our employees might test for this in certain ways, that some of our employees will have access to deeper analysis tools. There’s going to be records of the training data. Are we going to be able to delete all of those? And if not, what if someone goes looking? It could be, there might just be genuine uncertainty about whether this is possible until quite late in the day. I imagine that at some point, once you’ve got really superintelligent AI systems, they would be able to come up with a plan here, but it might be that for most of the intelligence explosion, the analysis is just like, “ah, seems like you might get caught”.\n\n(00:57:25): Whereas for the executive coup, the president is doing everything in plain sight. There’s no risk of getting caught because they are claiming that they have legitimate reason to do all these things as they go and they’re defeating their opponents as they go. And so there’s less downside risk. So that would be the counterargument. I think there are things on both sides of the ledger in terms of which is more likely, but the counterargument is just that the technical hurdle is much higher.\n\nSecret loyalty and misalignment in corporate coups\n--------------------------------------------------\n\n**Daniel Filan** (00:57:56): So maybe this is actually… So I’m trying to give myself some hopium to stop myself from worrying about this, and maybe one path I have is: okay, AI-enabled coups, it seems like it’s much easier to do it if you’re the AI company because you have all the AI. But you have to have this hurdle where the loyalty of the AI kind of has to be secret. It seems to me that the worry about that is: suppose you’re an AI company and you succeed in instilling the secret loyalty to yourself.\n\n(00:58:29): I think that should make you rationally worried that, well, if an AI can do secret loyalty, having secret goals that it pursues, this is basically just the same thing (as far as I can tell) as deceptive misalignment, where an AI is pretending to be aligned to you, but it’s actually not aligned to you, it actually just wants to do whatever it wants. And so I would think that if an AI company succeeds at getting a secretly loyal AI, they would rationally be concerned that the AI that they think is secretly loyal to it is actually deceptively misaligned, that if they use this AI to get a bunch of power, the AI is actually just going to take over. And it’s not going to be the AI corp that rules everything. It’s going to be some random AI desires. What do you think of my hopium?\n\n**Tom Davidson** (00:59:14): Yeah, I’m not convinced. I think that you are right that the AI company would know that the AI has the capability to do deceptive misalignment, but the AI company might still have strong reason to think that it will not have the propensity. It might have done careful interpretability throughout training or whatever alignment techniques you believe might work. It might have various empirical evidence to suggest that it is in fact not misaligned, even if it would be capable of doing that.\n\n(00:59:49): And in fact, it might not even have evidence of the capability. If the company is really just going for this, then it could have very sophisticated internal tests for scheming, which in fact the AI cannot game. And then it could train an AI to be secretly loyal. But then that secretly loyal AI is only able essentially to game tests that it’s been trained on how to game. So you can game all the company’s tests. It’s been trained on that. That’s not scary. Obviously you can game a test if you’ve been told how to pass it. It can game the unseen tests of auditors outside of the company because they’re much less sophisticated. But the company could still believe that the AI is actually not capable enough to game a highly sophisticated test that it has not been taught how to pass. And so actually even on the capability side, it needn’t have evidence that the scheming capability is there.\n\n**Daniel Filan** (01:00:53): Yeah, so one thing is it could be secret to people other than the AI company but not the AI company. You could also have… You might think that if the first AI that you can succeed at making secretly loyal via doing a bunch of specially-built training… It seems somewhat reasonable to think that the first thing you could make secretly loyal by doing a bunch of specially-built training maybe couldn’t be secretly loyal by just the normal course of pre-training.\n\n**Tom Davidson** (01:01:33): Yeah, I think that’s very likely. I think that the risk of secret loyalties is very likely to emerge before the risk of scheming because it just seems way harder to zero-shot how to pass all these tests and know what your strategic situation is and exactly when to act out versus if you just have a million super smart AIs figuring all of this out, training you how to do it, doing all this analysis. It’s just like the difference between what I can figure out in a few seconds on the fly and what a large civilization doing a concerted research effort can figure out, orders of magnitude difference in how much you can achieve. And so I do think this secret loyalties thing, I strongly expect to become technologically feasible at an earlier stage.\n\n**Daniel Filan** (01:02:24): Okay, here’s why I’m holding onto my hopium. I think there’s this risk aversion. I think it depends a little bit how risk averse you are. So suppose you’re like: okay, I managed to instill a secret loyalty. Let’s say I think I’m 95% confident that the AI doesn’t have its own secret loyalty. One in 20. If you’re the head of an AI lab, I think they have pretty decent lives. I don’t know, I’ve never chatted with [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) or [Demis Hassabis](https://en.wikipedia.org/wiki/Demis_Hassabis). From the outside, it seems like they have relatively cushy lives, right? One in 20 chance of “you hasten AI doom by starting a coup”, that’s pretty bad, right? So it seems like it has to not only be true that the AI doesn’t have a secret loyalty, you have to be pretty confident in it.\n\n**Tom Davidson** (01:03:14): Well, let’s say that OpenAI trained GPT-7, it did the capabilities tests, it did the alignment tests that it has, and it was like, “We’re going to deploy it. We’re happy with this system.” They’ve got a certain level of evidence. And yeah, let’s say it’s really capable, it’s really good at strategic deception, but indeed, people in this community worry that they would decide to deploy nonetheless. Maybe the risk is 5%, maybe they think it’s 0.5%. The question is, if they’re now considering instilling a secret loyalty, is that going to significantly materially increase that risk? And it’s not actually something I’ve thought about. You could argue: well, look, you are going to be actively teaching it all these different types of strategic deception. That seems like maybe it’s increasing this risk. But the reason why I’m not sold is that I don’t see why you’d be actively teaching it to in fact be misaligned. You’re obviously giving it capabilities which are scary. But if you’ve already decided how likely you think it is to be misaligned, you’ve already decided you’re happy to be deploying it. Are you going to now be more worried about it suddenly becoming misaligned as you trained it to be loyal to you? That doesn’t seem like it would be the case.\n\n**Daniel Filan** (01:04:27): Yeah. I think what I’m imagining, which maybe doesn’t actually make sense, is \\[that\\] you have an overall plan and your overall plan has two parts. Part one is instill these secret loyalties, and part two is have the AI be more widespread and have more ability to gain power than you by default were planning, right?\n\n(01:04:45): And the combination of that is pretty bad. Now, if you were holding fixed how far you would spread the power of the AI or whatever, then I agree instilling your own secret loyalties… I think it provides some Bayesian evidence. It seems plausible to me that being able to do it is some evidence that it might’ve already had the secret loyalty, but I think it’s less bad than the two-part plan.\n\n**Tom Davidson** (01:05:14): Yeah, I think it’s a good point. I hadn’t thought of this, that if someone did want to stage a coup using their AI system, they probably would want to push it out into the world faster than other AI companies. Because if half the military is now controlled by this other AI company, then it becomes much less clear you can pull off the coup. So I think it’s a good point, that to the extent that you are already worried about misalignment and therefore you wouldn’t have wanted to push out your AI to the military, that would then continue to stop you from pushing out your AI to the military to stage the coup. That provides some hope.\n\n(01:05:54): But my baseline has actually just been, we’re already worried that the AI companies will just be pushing out their AIs all over the place because they won’t be that worried about misalignment. And then once we take that for granted, then the risk of them staging a coup is very real. I guess where I’d agree with what you’re saying is that to the extent that these companies are worried about misalignment, I do think that that helps because it will give them pause.\n\n**Daniel Filan** (01:06:22): Yeah. I think it’s a good point that if you’re already deploying and maybe you’re like, “If I’m planning to do a coup, I’m going to deploy 20% more.” That’s probably not going to be the difference between… I think that’s a pretty decent point. I think in the report, the illustration and the way we’ve talked so far is, we’re sort of imagining it’s the head of the AI lab that’s doing this. One thing I kind of wonder about… So a friend of the podcast, [Jan Leike](https://jan.leike.name/), is Head of Alignment at Anthropic \\[Editor’s note: he actually co-leads the Alignment Science team\\]. Seems like a chill guy. I don’t want to cast any aspersions on any of the Heads of Alignment of any of these labs. But wouldn’t they be in a really, really great position to do this?\n\n**Tom Davidson** (01:07:04): Now I’d normally thought that it was the people who were involved in post-training and were involved in, let’s say, the fine-tuning runs that give the AI its particular personality and goals, that would be best placed. The team at OpenAI that’s ensuring the AI matches the model spec, because they’re the ones that are kind of influencing the particular values, and they could maybe try and insert an extra stage at the end that isn’t reported, that inserts the secret loyalties. And then I was imagining the alignment people would be the ones that then do the tests for the accidental misalignment.\n\n(01:07:43): But I think really it’s both that you’d want. The ideal coalition would include both, because you have the team instilling the secret loyalties colluding with the team that’s meant to be detecting them. But you don’t necessarily need the detection team, because if the team that’s instilling them can access the code of the alignment team, then they can teach the AI to pass the test. And so this is another reason to have siloing within AI organizations, so that you can’t teach it to pass the test in that way.\n\n**Daniel Filan** (01:08:12): Yeah. I’m now realizing that I know less about the exact internal structure of these labs.\n\n**Tom Davidson** (01:08:19): Yeah, neither do I.\n\n**Daniel Filan** (01:08:21): Good thing I’m not trying to make a coup happen. But yeah, that’s a good point. And again, no individual aspersions to anyone I might know who works in these teams. I feel decently happy with how we’ve covered corporate coups. Is there more you want to say there or shall we move on?\n\n**Tom Davidson** (01:08:41): Another technical hurdle I’ll just flag is that I don’t expect GPT-7 to be given control directly of all the robots and all the drones. I expect there to be specialized AI systems which are developed that control those particular military weapons. And so there’s this other technical question over: even if you had all GPT-7 secretly loyal to some group of people, there’s then this big question of, they have to now do all this military R&D, design all these new weapons and drones and build the specialized AI systems that will control those drones and weapons. And they need to do so in a way that ensures that this small group of people ultimately has control over what this whole very complicated interrelated military infrastructure is going to be. It’s not just going to be carbon copies of the same robot. It’s not going to be one AI system. It’s going to be very complicated. There are hundreds of different AI systems controlling hundreds of different bits of equipment interacting in complicated ways. We’re not literally going to have robot soldiers that replace one-for-one human soldiers. That’s just a simplification to explain how intense we’re imagining this dynamic being.\n\n(01:09:56): And there’s an open question of: as you get these sprawling military systems, interacting in complicated ways, controlled by various AIs and other things, how easy is it for GPT-7 or GPT-8 to ensure as they’re building all of this stuff that it’s all going to be ultimately controllable and loyal-able or hackable by the AI company or by copies of themselves, where… I just don’t know, but it seems like it might be really hard. The military contractors might have humans or less powerful AIs reviewing these designs and noticing obvious flaws.\n\n(01:10:39): The military isn’t stupid, they have serious security practices. They’re worried about Chinese attempts to seize control. They might be worried that they are indeed Chinese spies in the labs that might be having influence on these AI companies. So you might well expect there to be significant defense efforts from the military, fingers crossed, looking for this kind of thing. And maybe \\[it’s\\] just a really tough technical task to design a very complicated physical infrastructure that is ultimately all controllable by a particular AI system or particular copies of an AI system. And so I think it might be very hard to predict in advance whether that works, and that’s another significant technical hurdle that might just turn out not to be doable, which I think should give us pause for hope in terms of whether the company coup is doable.\n\nLikelihood of different types of AI-enabled coups\n-------------------------------------------------\n\n**Daniel Filan** (01:11:39): So there’s a few paths towards an AI-enabled coup that we’ve talked about. There’s basically the head of the executive doing it, there’s the lab company doing it, and also there’s this free variable about there are a variety of countries that could be couped. I’m wondering if you have a sense of the relative likelihoods of these things happening?\n\n**Tom Davidson** (01:12:01): It’s a great question. In terms of countries, I think that in the fullness of time, current countries that are already fairly autocratic, like China and Russia, I think are at very large risk of an executive coup because the executive is just starting in such a strong position to begin with. So all of those steps, they’ve basically accomplished the first half or more and then \\[it’s\\] just quite plausible \\[that\\] they could use their existing power to push through the deployment of loyal systems throughout society. So I think that is worryingly likely. Honestly, it sometimes feels a bit hopeless to me in terms of how we avoid that. You can imagine one country really intervening in another country’s affairs. That’s not something I really feel excited about pushing towards. The other thing is just really encouraging the other actors that still have some power in those societies to really be live to these issues and get ahead of the game and maybe they can outmaneuver the head of state, even though the head of state is in a very strong position.\n\n**Daniel Filan** (01:13:15): So to the degree that part of the reason you’re worried about AI-enabled coups is that you think that there’s some concentration of AI labs, or a small number of labs that are powerful: I mean presumably one way of preventing this is like: so suppose you and the AI lab are simpatico. Suppose you have a list of “here are the countries that I’m most worried about having a coup”. You could say, “Hey lab, we’re just not selling to those countries,” which is obviously… It’s a somewhat geopolitically aggressive move, I guess.\n\n**Tom Davidson** (01:13:51): You might also be able to sell AIs that have guardrails that prevent their use to enable an executive coup. It would be very complicated because if you’re just setting up a surveillance state, there’s just lots of somewhat narrowly defined tasks that you want your AIs to do, but you could try and differentially allow them to deploy AI systems that won’t centralize power as an intermediate.\n\n**Daniel Filan** (01:14:17): Yeah, I guess the tricky thing about that being, it’s just very… If you have some countries who do get to use AI in their militaries and some countries where either they don’t or the AI they get to use is filtered for not doing a coup, and maybe other countries don’t trust that that’s the only thing they’ve monkeyed with, it seems like it might be a pretty aggressive move, which…\n\n**Tom Davidson** (01:14:47): I don’t know how aggressive it’s going to be to just not sell a powerful technology. I think that might be the default situation with a really powerful AI, that just for national security reasons, you wouldn’t want countries that you’re adversarial towards to have access to those most powerful systems.\n\n**Daniel Filan** (01:15:07): Fair enough.\n\n**Tom Davidson** (01:15:08): But to me, the worry is it’s just a delaying tactic and that in the fullness of time, China will develop its own powerful AI and sell access to autocracies that want it.\n\n**Daniel Filan** (01:15:22): So maybe another question is… So I’m not from China, I don’t live there. I wish the best for the Chinese people. But if there’s a coup in China, an AI-enabled coup in China, to what degree is the concern, like, China is autocratic forever?\n\n**Tom Davidson** (01:15:39): And just to be clear, probably in China it would be less called a coup and more… Well, it would be an executive coup, but it might just be cementing the system that already exists if you already consider it to be autocratic.\n\n**Daniel Filan** (01:15:49): Also, by the way, I’m asking about China, but I’m not really just specific to China. I’m mostly just thinking \\[about\\] a bunch of countries that I don’t live in. If there’s a relatively autocratic country, it has an AI-enabled coup/cementation of power, to what degree is that concerning because that country is autocratic forever versus to what degree is that concerning because maybe that country becomes more bellicose and starts trying to take over the world, or it’s a promoter of conflict?\n\n**Tom Davidson** (01:16:21): Yeah, I think it depends on exactly what you care about. One lens you can take is the kind of hard-nosed, longtermist lens where you say, “Okay, what we care about is control of the stars over the long term.” And so then you’ll be thinking, “Okay, would this perhaps less powerful country, would the new dictator hang on to power for long enough for it to be indefinite? And would they be able to get a sizable fraction of the stars such that there’s been a significant loss of value?” And if it’s a not very powerful country, you might, from that really hard-nosed, longtermist perspective, say, “Well, it’s not going to be powerful enough to actually gain any of the stars. Probably the United States is just going to basically be carving up the stars with China or just taking them all for themselves.”\n\n(01:17:07): So though it’s a tragedy in terms of the people who live in those countries, from the kind of brutal, utilitarian calculus, it matters a lot less. I mean, that’s one lens. Then the other lens would just be the humanitarian lens that says, this is awful for the people in that country. And also if that country is able to strike a deal with countries like the United States, then they might be able to embed themselves permanently, even if ultimately the United States has much of the hard power.\n\n**Daniel Filan** (01:17:42): I think there’s this uncertainty I still have about the domestic versus the international impact of doing a coup. So I could imagine one story where if you do especially an AI-enabled coup, you get all the military really unified behind you. Maybe that just makes your military more effective because they all have one purpose. You have access to this really good planning, and if you compare to militaries that basically haven’t been involved in a coup, that are different people with slightly different desires and they’re not as ruthless… There’s one story where that military is at a significant advantage. You can also have a story which is: well, democracy seems like it’s generally good. Somewhat dispersion of power seems like it generally makes things run better. So maybe this is not a concern. I’m wondering if you have thoughts there?\n\n**Tom Davidson** (01:18:33): Yeah. One related thought I have is that: let’s say there’s not a coup in the United States. I then personally think it’s unlikely that the United States would end up completely dominating the rest of the world and seizing all power economically and all strategic control for its own citizens to the exclusion of all others. Because the United States…\n\n(01:18:58): A few reasons. Firstly, the United States has many different coalitions with power, and many of those coalitions have ideologies that make them committed to things like democracy, things like trade, and have positive views of other countries, like, say, the United Kingdom where I live, and they just wouldn’t want the United States to dominate the United Kingdom as much as it possibly could. And so that balance of power in the United States would ensure that the United States uses its power in a way which does go somewhat beyond its borders. And the other thing is just that if the United States wanted to completely dominate the rest of the world, probably what it would want to do is to really restrict the AI systems that it sells to the rest of the world and really sell access to those systems at the highest price it could. Whereas under the default situation where power is distributed within the United States, different companies within the United States will compete to sell AI services to the rest of the world, driving down the cost that the rest of the world is paying.\n\n(01:20:02): And because of competition within the United States, that means that actually the United States is going to give the rest of the world a bit of a better deal. And so under this default scenario where power is distributed, I think there’s less prospect for the United States to really just take power for itself, even if it’s leading on AI. Whereas if there is an AI-enabled coup and one person becomes dictator with total power, then they might be like, “I want to dominate the world. I want all control and I’m just going to force all these companies to only sell at this extortionate rate, and the rest of the world has no other source of powerful AI so they’ll pay it. And then I’m going to choose our foreign policy and economic policy to only take into account the welfare and power of the United States in particular.” And so I do think that if there’s an AI-enabled coup in a particular country, then as you indicated, that country might become more bellicose at pursuing its own particular interest and could actually do so more effectively.\n\n**Daniel Filan** (01:21:08): And I guess there’s also just this factor of: if you’re doing a coup, you’re probably a bit of a bellicose person, you’re probably more inclined to that sort of thing than other people.\n\n**Tom Davidson** (01:21:19): Exactly. I mean, you raised a good question about “are democracies just going to be more efficient?” Because the free market’s fairly efficient, you’re distributing the decision-making. I think a scary possibility is that you can still gain the benefits of the free market by distributing all the economic decision-making and having markets operating within the country, but you still have on all the important decision points, AIs that are loyal to one person. And so you can get all those economic benefits to democracy now without actually needing to have a real democracy. But I haven’t thought much about whether that would go through.\n\n**Daniel Filan** (01:21:54): Something to think about. So speaking of democracy and speaking of the United States, initially you said, “Yeah, probably countries that already have a very strong executive, that already are less democratic probably are more at risk to having a stronger executive and being even less democratic.” I live in the United States, I’m a fan of it. How high do you think the risk is that the United States gets AI-enabled couped?\n\n**Tom Davidson** (01:22:23): I mean, if I had to pluck a number, I’d say 10%, but it’s very made up. That’s my rough probability for AI takeover as well. I think it’s ballpark similar.\n\n**Daniel Filan** (01:22:36): Okay. And can you talk me through why is it as high/as low as 10%?\n\n**Tom Davidson** (01:22:43): By analogy with AI takeover or just in and of itself?\n\n**Daniel Filan** (01:22:48): In and of itself.\n\n**Tom Davidson** (01:22:49): Yeah, so I think some things are fairly likely to happen. We’re likely to see a very small number of companies developing superintelligent AI systems. We’re likely to have a government that if it tried to, could gain a lot of control over how those capabilities are used via its default monopoly on force, its natsec apparatus. If they don’t, then by default power is already and will continue to be very concentrated within the AI companies. There are not, in practice, many effective checks and balances on the CEOs in these companies. I also believe that it’s quite likely that CEOs will want on the margin to just increase their own power and use their influence over AI to increase their influence more generally.\n\n(01:23:52): So you can already see with Grok, [Elon](https://en.wikipedia.org/wiki/Elon_Musk) \\[Musk\\] is doing this in a totally shameless way. He’s altering Grok’s prompts to make it promote political views that he likes. And I think it’s just a natural urge if you want stuff and you want a bit more power and you just have this way of getting it, which is that you’re controlling these hugely powerful influential AI systems. So I do think it’s quite likely that on the margin these company leaders will walk down that path of increasing their own power to some extent.\n\n(01:24:29): But there are also some things which I think are not particularly likely. They may happen, but: will at any point a key company executive decide to do something which is really egregious? At some point they might need to decide to do a secret loyalty. I think there’s a chance that that’s just a step too far or there’s a chance that by the time that’s possible, the world has woken up and just put in some kind of checks and balances that would make that hard to do.\n\n(01:24:56): And then there’s the further technical question on, okay, but would this actually work out? We were pointing to some of these difficulties of actually getting these secret loyalties propagated to the military infrastructure, being really confident the AI isn’t actually secretly misaligned. So really zooming out, maybe there’s a couple of steps which are… I wouldn’t say it’s more than 50%. And so that gets you down. Let’s say there’s two steps which are 40% each. Just in this rough range where it’s about 10%. As I’m thinking this through, I’m thinking maybe it should be higher because you’ve got either the lab route or you’ve got the executive route, and maybe you actually just want to add those up. Yeah, that’s just a brief indication.\n\nHow to prevent AI-enabled coups\n-------------------------------\n\n**Daniel Filan** (01:25:52): Okay, I think at this point I’m interested in just talking about maybe what people should do about this. And probably I’m going to be most interested in thinking about this from a US perspective because that’s where I live and what I think the most about. Although I’m also interested in other places-\n\n**Tom Davidson** (01:26:10): I do think it’s the most important case.\n\n**Daniel Filan** (01:26:17): So a lot of these stories are about synthesis of AI power and military power. So it seems like one thing you could do for this proposal - AI power, military power, and executive power all coming together in a really concerning way. Sometimes people are like, “the US government should have this really big push to develop really powerful AI, that it does itself, with strong… pushing AI forward really hard, having exclusive access to the AI, and it should be really integrated within the government.” It seems like this is probably pretty bad from the coup perspective. I’m wondering if you have takes there?\n\n**Tom Davidson** (01:27:05): So I think if you did this really well, it could be good from the coup perspective. If you’ve very carefully designed a project explicitly with reducing this risk in mind, I think you could probably actually reduce coup risk relative to the status quo, just because the status quo is so poor. Under the status quo, there’s very little constraining labs. So there’s very little guard against the company coup, but there’s also no explicit checks and balances that would constrain the ability of the executive to just demand that the companies sell them access to AIs without guardrails that they can deploy throughout the government and military. And the companies, if there’s a few of them, would be in potentially quite a weak negotiation position with the executive over that.\n\n(01:27:52): So because the status quo is so bad, I think if you designed a good centralized project, you could reduce this risk. Now, I think probably the best way to minimize this risk would be to design a system of regulation where you continue to have multiple constrained regulated projects with various transparency and safety constraints in place, et cetera. That would probably bring the risk down lower still, and that would be better than a centralized project from this perspective.\n\n**Daniel Filan** (01:28:28): One thing that occurs to me as well is… So again, I still have in the back of my mind, how do AI alignment concerns affect this? It seems like a lot of the things that people want out of AI alignment could potentially help with this. So transparency, causing companies to do evaluations of their models, having whistleblower protection schemes. It seems like a lot of these probably at least reduce the chance that AI labs do stuff in ways that the rest of the world doesn’t know about. Maybe it increases the risk that… If you’re worried about governments meddling too much with AI companies to do tricky things there, maybe that’s a concern. But I’m wondering, having strong AI Security Institutes or something: how much do you think that helps with coup risk?\n\n**Tom Davidson** (01:29:29): I think all of the stuff you listed helps and in combination helps a fair bit. And yeah, I do think just a lot of the interventions here are pretty generally good across both coup risk and misalignment risk. The place where they really potentially bump heads is whether to centralize into just one project versus having careful regulation of multiple projects. But beyond that, I tend to think there’s this pretty strong alignment. There’s different areas you focus on. So you’re particularly concerned with, “oh, how is everyone actually using the compute within the AI companies and within the government?” And you’re relatively less concerned with looking for rogue deployments because it’s just the “legitimate” deployments that we might be more worried about now. But monitoring use of large compute, that’s the way of framing it where that’s both catching the misalignment risk and the risk of coups.\n\n**Daniel Filan** (01:30:28): I think I want to talk about things that are maybe more unique to coups. So one thing that you don’t mention in the report as far as I could tell, but seems interesting to me: a lot of the coup risk seems to come from: you have one AI company, it’s relatively dominant, it’s relatively in the lead. It’s hard for other people to compete, and they’re doing sketchy stuff within that company, at least for company-enabled coups. Currently, it’s relatively common for employees… It’s not unheard of for employees to switch around between companies. There’s a decent amount of it happening, and it seems like this probably helps, partly for…\n\n(01:31:14): It means that labs don’t have super, super intense secrets. They probably have some secrets of exactly how they do some things, but the general structure… I feel very confident that the latest Claude and OpenAI models are based on the transformer architecture. If they did something totally different, I just feel like we would’ve heard of that. So it feels like it both protects against (a) them doing something… If OpenAI were like, “Hey, all of our employees—we’re doing a bit of a coup, shh.” I feel like people would hear about that. And then (b), maybe it protects against one lab going super, super far ahead of the other labs. So some combination of (a) people moving between labs and (b) some good old corporate espionage. It seems like this maybe helps. I’m wondering if you have thoughts?\n\n**Tom Davidson** (01:32:14): Some of the benefits you’re describing, I think just come from the fact that the labs are humans who are integrated throughout broader society and will talk about really notable things. If a lab tried to tell everyone, “look, don’t mention this, but we’re doing a coup”, it doesn’t matter whether employees are switching between labs, that’s going to backfire. So then what do we get from the employee switching? I do think you’re right. We’re getting some kind of evening of the playing field.\n\n(01:32:42): My view is that you can get a lot of the benefits by having two separate labs in terms of coup risk reduction. Because if you have two separate labs, they can separately train AIs using different stacks, and then they can have the AI cross-audit each other for secret loyalties. You can deploy both systems in the military. So you have a significant additional check and balance there. You could have both AIs controlling each robot, so maybe the OpenAI system is controlling the robot, but the Anthropic system can shut it down. And so with a setup like that, you could significantly reduce the risk because even if one system is secretly loyal, that doesn’t give you strong reason to think that the other system is also secretly loyal and colluding. So I think if you had movement between companies which made it less likely that we just end up with one company \\[and\\] instead meant that we get two or three, then yeah, I think that would be good.\n\nDownsides of AIs loyal to the law\n---------------------------------\n\n**Daniel Filan** (01:33:43): So one thing you mentioned is you think that it’s important for AIs to be aligned to follow the law and to not be loyal to one individual and just \\[prioritize\\] the law in general. One concern I have about this, and in fact about AI-enabled coups in general, is it feels like it is possible for countries to be too stable. I think that it is possible for the law to be followed too much. Well, the law being followed too much, I think one version of that is it is sometimes very unclear what the law involves. A kind of silly version of this is I’m only 90% sure that [the existence of the United States Air Force is constitutional](https://volokh.com/posts/1170032632.shtml) because the Constitution doesn’t actually say that you can have an air force, because they didn’t think about it. It says you can have an army and a navy. Can you have an air force? I don’t know.\n\n(01:34:41): I mean that’s a bit of a silly example, but the US Constitution, it is a little bit ambiguous in many places. But at a high level, if I imagine… So for example, the reason there’s a United States is that one part of the United Kingdom broke away from the rest of it, and that was, I assume, illegal. It was illegal. It was a portion of the United \\[Kingdom\\] breaking the law and being loyal to one entity within the United Kingdom versus other things. And in general, it seems like it’s probably good for it to be possible for sometimes bits of states \\[to\\] break away illegally and do their own thing. How much of preventing coup risk, especially via the means of making sure that things are aligned to the official law, will prevent bits of states breaking away in a way that seems healthy in the long run?\n\n**Tom Davidson** (01:35:54): I think it’s a really interesting question. I think we want to get a balance between locking out the bad stuff, locking out the egregious coups, but as you’re saying, we don’t want to lock in too much. As an extreme case, we definitely don’t want to lock in “the rules of today can never be changed”, so we obviously want to have some process by which we collectively can decide to change the laws. And I think that that’s by default how it’ll happen. I had previously thought that, look, if we lock in the laws of today in a sensible, nuanced way, then we will leave enough flexibility to collectively decide to change things. And there could be some process by which it is legitimate for a state to break away.\n\n(01:36:48): But I think you’re actually right in practice, it may be that the naive way of implementing even a nuanced version of the law… It’s possible that would actually lock in too much. I haven’t thought much about how much really positive stuff has happened historically via lawbreaking, and do we expect that to continue to be the case even in mature democracies like the United States? Do we want to allow California to just declare that it’s independent illegally, and do we want its AIs to go along with that?\n\n(01:37:24): I think it’s a really good question, and it kind of highlights the way in which we may be going down significant path dependencies as we automate the government infrastructure and military, because once we’ve automated the whole government and the whole military, we will have implicitly baked in answers about whether AIs will support various different maneuvers. We’ll have implicitly baked in an answer about, if California tries to break away, and all of its systems support it, and most of the broader US supports it, but it’s actually technically illegal, and… There will be some decision that infrastructure of AIs will come to about whether it’s going to support… If push comes to shove and there’s going to be a military intervention, what will the AI military do? That’s a constitutional crisis, and we will be baking in some implicit answer to the question of what will happen there? Who will the AI military support?\n\n(01:38:25): And I think it just highlights \\[that\\] we should think very carefully before we do this. And there’s kind of no way to not give an answer. There’s no default because the default in today’s world is just, I guess, there’s a kind of power struggle and random stuff happens, and I think it’s a fair point that maybe it’s actually good that you can sometimes do illegal stuff because it adds more variety. And so maybe in the ideal world, we’d say, look, in constitutional crises, be wise, consider what’s best for the broad future, and make the best decision that balances all these interests. And we hope that that would actually be an improvement on the status quo where it’s just kind of random and determined by power. Maybe we can get something that’s at least based on some kind of desirable principles when there are more edge case-y constitutional crises, and maybe we don’t always make it come down to the letter of the law.\n\n**Daniel Filan** (01:39:23): So there’s one version of this which is being pro-pluralism. There’s another version of this, which is… Especially if instead of imagining the US, you’re imagining… I think there, at least conceivably, are authoritarian countries where you actually do want it to be possible for things to break away. And there is also this third thing, which is: the letter of the law really is not as clear as you might hope in many cases. I was thinking about this before we started. One thing you could imagine doing is being pro-pluralism instead of pro the letter of law. I don’t know, I didn’t spend 10 minutes thinking about ways in which that could be bad. So probably there are a bunch of ways that could be bad.\n\n**Tom Davidson** (01:40:09): I mean, another possibility is you act in accordance with how you predict the Supreme Court judges will resolve this question assuming that they’re acting in good faith.\n\n**Daniel Filan** (01:40:27): “In good faith” seems tricky and hard to define. I guess it depends-\n\n**Tom Davidson** (01:40:36): Or assuming they’re trying to be reasonable. The law often has “reasonable judgment” and things like this, just because if you don’t say in good faith, then if all the Supreme Court now decide they want to do a coup, then the AI knows that, then the AI just does a coup. So you want to have something there to kind of idealize it.\n\n**Daniel Filan** (01:40:51): Yeah, I think there’s probably some way you could do it.\n\n**Tom Davidson** (01:40:56): I mean, the thing is about AI, you can give it these fuzzy things like “assume they’re trying to be in good faith”, “assume they’re being reasonable”, and just like humans do, it’s able to work with it, even though it’s not mathematically specified.\n\nCultural shifts vs individual action\n------------------------------------\n\n**Daniel Filan** (01:41:06): Yeah, I think there’s something to that. So talking more about ways of stopping coups: one path is things you mentioned in the paper: try to align to things other than “it’s definitely just going to be what this one person wants”, try and prevent lab-led coups by making labs transparent, having some regulation of labs. I guess preventing executive-led coups… Presumably the thing to do there is just try and elect people who won’t do coups.\n\n**Tom Davidson** (01:41:46): I think there’s building consensus among many different parts of society, especially the checks and balances parts, that we want AI to follow the law, to not be used to increase the partisan power of the current elected officials, build a consensus that military systems shouldn’t all report to one person, but should all report to many different humans. And if you can build consensus around that, then that can make it more of an uphill struggle for a head of state that wants to stage a coup.\n\n**Daniel Filan** (01:42:22): So in the report, a lot of the proposals for how to prevent a coup are very “here’s things that we as a society could do”. One thing you could potentially do to prevent a coup is also sabotage-type things (or at least things that individuals could do, or things that are less \\[of a\\] global plan). I mean, one very minimal version of this is: if you imagine there’s some authoritarian country that you think is at high risk of an AI-enabled coup, you just not sell AI weapons to them. That’s a moderate version of this. You can also imagine, even if there’s not a policy for my AI lab to prevent coups, you can imagine individual workers in an AI lab saying, “Okay, I’m going to quit”, or “I’m going to insert my ‘don’t do coups’ bit into the code slightly surreptitiously.” I’m wondering what you think about these more individual-ish moves.\n\n**Tom Davidson** (01:43:31): I definitely support whistleblowing and encourage employees of AI labs to be like, “Okay, what’s going to be my line?” If there is movement towards less transparency into what the AI is being aligned to or even just like it’s becoming clear that it’s being aligned to the company or to specific people, what is your line in which you’re going to whistleblow. I think one thing that employees can do is be like, “I am going to hold myself accountable to getting positive affirmation that this isn’t happening. I’m going to make sure that it’s not possible for the company to sneak in a secret loyalty given that I’m aware of what the company systems are like, and I’m going to ensure that the company isn’t training AI overtly to be loyal.”\n\n(01:44:18): And so I think it would be great if there was a culture at companies where it’s just like, obviously we wouldn’t want this to happen, obviously we don’t think anyone here would try and do this, but we need to have an attitude of vigilance because that’s what makes it true that this would never happen. So I think that’s good.\n\n(01:44:35): And one more positive framing for this is being like: one great thing to aim for as a company is to make a product which everyone absolutely knows they can trust, even people who don’t trust our staff and our processes and think we’re crooked and think we’re going to try and seize power, even they should just know they can trust our systems because that’s what a good product looks like. So you can frame this in terms of building amazing products that ultimately you want national security to use.\n\n(01:45:06): If you anticipate that it’s going to be public knowledge that sleeper agents are possible, that secret loyalties are possible, then you might anticipate \\[that\\], for the government to proactively use our AIs in the military and in other critical systems, they’re going to want to really have strong assurances and not just trust that there’s nothing fishy going on. And that’s what we’re aiming for for purely product-based reasons.\n\n(01:45:32): So I do think pushing towards that culture is one thing you can do as an individual. I feel more nervous about trying to sabotage the training run in a way that prevents secret loyalties but that no one else knows about, just because it’s such a similar act to introducing secret loyalties. And I just think that all of that stuff should be very taboo, and the processes should prevent that.\n\nTechnical research to prevent AI-enabled coups\n----------------------------------------------\n\n**Daniel Filan** (01:45:53): If someone’s been listening to this… In particular, I think a lot of my audience are people who do technical alignment research. If someone’s listening to this, and they’re interested in, “okay, let’s prevent AI-enabled coups”, is there technical work in particular that you think is valuable for people to work on here?\n\n**Tom Davidson** (01:46:11): I think the biggest thing is technical work on detecting sleeper agents and secret loyalties specifically in a setting where there is some kind of adversary that you expect to be purposefully trying to hide that sleeper agent from you.\n\n(01:46:29): So historical sleeper agent work has often… At least [the stuff from Anthropic](https://arxiv.org/abs/2401.05566), it’s been designed to illustrate the idea of misalignment. So AI kind of acts out when it sees it’s 2026 but plays nice before that time. But those model organisms (as they’re sometimes referred to), they’re not really trying to illustrate the risk that a human actor has created a secretly loyal sleeper agent that they specifically don’t want other humans to discover. And so I think there’s potential for a lot of technical work on this kind of offense-defense balance question of: if you have some attacker that has some kind of access to the training procedure and you have some kind of defender that has, again, some level of access, again maybe seeing how the training worked or doing tests afterwards, can the attacker create a sleeper agent that the defender doesn’t detect? And how sophisticated is that sleeper agent?\n\n(01:47:34): And there’s so many parameters here that ideally we want to be studying. So the attacker could have just access to… The minimal access would just be they can put things on the internet that go into pre-training. A deeper level of access would be: they can add lines to the fine-tuning code, but then maybe the defender can see that fine-tuning code, or maybe the defender can’t, or maybe the attacker can add additional training examples, which maybe the \\[defender\\] can see or can’t see. And so there’s all these different kind of scenarios where the attacker and defender have various different affordances.\n\n(01:48:09): And ideally what we would have is a good understanding of under what situations does the defender reliably win? Because if we have that understanding, we can then set up our training process so that it is secure enough that the defenders indeed reliably win given the actual kind of access that they have and given the likely access that we believe an attacker could gain.\n\n**Daniel Filan** (01:48:31): How close do you think this is to just basically what [AI control](https://www.lesswrong.com/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion) people are doing, versus do you think there are specific aspects of this that are not covered by typical control research?\n\n**Tom Davidson** (01:48:41): Well, I think it goes beyond control. So interpretability is something you could look at here. Typically, we look at interpretability with a mind to accidental misalignment, but you could say, can I use interpretability to catch sleeper agents, even when the person is specifically trying to hide them? What if they can guess the kinds of interpretability techniques I might be using in advance? Can I still detect the sleeper agent then?\n\n(01:49:07): So basically, take any method we have for testing alignment, black box testing, interpretability, looking through the training data: for many of those methods, you’ll be able to adjust it to consider the more adversarial setting where someone is specifically trying to create a sleeper agent that you cannot detect. And so I think it is not just control, it’s kind of alignment and control. And then I don’t think it’s exactly the same as standard control because it is a harder setting, because as we touched on earlier, the normal setting is where the AI kind of has to zero-shot figuring out how to dodge all these tests and how to sabotage. Whereas here the attacker would have more ability to help the AI do a good job using this-\n\n**Daniel Filan** (01:50:01): Pre-train it how to…\n\n**Tom Davidson** (01:50:03): Exactly. How much does it know about the company’s control processes? If you’re worried about an insider threat actor, then you might want to study, okay, how much of a better job can they do if they do versus do not know about the specific control setup? And I actually think the control research already looks at this variable.\n\n**Daniel Filan** (01:50:23): Yeah. Well, I imagine it depends. I think it’s one of these ideas that has caught on and it’s maybe a bit hard for… I think it’s hard for the world to refer to exactly one thing exactly by the term “AI control”. So probably different people are thinking slightly different things, and it’s worth thinking about, “okay, I want this version”.\n\n**Tom Davidson** (01:50:46): But I’ll just give it one last pitch I suppose, which is to say that there’s so many people that have worked on misalignment from the perspective of accidental misalignment. There are just very few people that have really comparably gone in on techniques for very powerful AI detecting purposeful sleeper agents. And I think it’s going to be a really important problem. It’s one day going to be commercially important, as we’ve discussed, because the government will want guarantees, but it’s very neglected. And so I would really encourage technical people to work on this.\n\n(01:51:26): Probably the first stage is actually a research agenda that sets out the space and looks at the current alignment techniques and what kinds of best ways of adapting the research there would be to account for this new threat model.\n\nNon-technical research to prevent AI-enabled coups\n--------------------------------------------------\n\n**Daniel Filan** (01:51:40): So okay, that’s technical research people can do if they’re interested in this threat model, but not all of my potential listeners to this episode are technical researchers. I’m wondering: stuff other than technical research, do you think there are further questions that need answering here?\n\n**Tom Davidson** (01:51:55): One thing I’ll briefly mention is the infosecurity side. So that is technical. I don’t know how many of your \\[listeners\\] work on this, but a lot of infosecurity is focused on weight exfiltration, but there are infosecurity problems which arise from this threat model, which are new. So it’s much more about: how can you secure the training process against unauthorized edit access to the weights? So rather than read access to the weights, to exfiltrate, it’s edit access, and that has some distinctive features.\n\n(01:52:33): And I think, again, a kind of road mapping exercise that goes through all the different infosecurity things that labs could do and thinks what are the best bet for making it hard to do significant unauthorized edits seems like low-hanging fruit. And then I think labs should be upweighting those relative to their previous prioritization. And so if you do infosecurity, I’d really encourage you to pitch that project to your team. But you asked about non-technical work.\n\n**Daniel Filan** (01:53:06): Yeah.\n\n**Tom Davidson** (01:53:07): I think one thing is just doing more detailed threat modeling. As we touched upon in the conversation, there are lots of steps of these threat models that you could dig into in more detail. How hard would it actually be to train secret loyalties without detection? How hard would it be for those AIs to pass the secret loyalties onto the next generation and then to military systems? You could look into: for the executive coup, what is the legal situation currently with respect to automated military systems? Who has legal authority over the procurement process, and if powerful AI military systems were being built, who would by default be looped into their loyalties and how they’re being designed?\n\n(01:53:59): And again, then thinking about mitigations. So once you’ve got more a more detailed threat model, you can then think about… One thing that I thought of recently is, well, if all the labs send a copy of their weights to some safe enclave, then it makes secret loyalties a much more risky prospect because they can’t just delete those weights. And then whoever they send the weights to can, in a few years time, do tests with much more sophisticated techniques. So it’s possible that thinking through the threat models will bring up other ideas like that.\n\n**Daniel Filan** (01:54:31): Well, yeah, in that case, you’ve got to be confident that the copy of the weights the lab sent you is the actual thing that they’re running. But there must be some way to…\n\n**Tom Davidson** (01:54:39): Yeah, well, you can certainly do hashing the weights. So you can check that the copy you’ve received is the one they’re currently running, as long as you can get them to actually hash the weights that they’re really running and then compare them. But yeah, you’re right, there’s room for maneuvers there.\n\n**Daniel Filan** (01:54:59): Yeah. I wonder if this is… So sometimes people talk about: we’re going to have computer chips, and they’re going to have a little thing on them that checks if you’re doing really crazy AI training and reports that, just so that governments can monitor how much AI training people are doing. It seems like a similar thing you might want to do with chips is “are people running the model weights that they say they’re running?” That seems like it’s potentially valuable for this threat model.\n\n**Tom Davidson** (01:55:28): Yeah, that’s a great idea. I hadn’t thought of that. What you could do, you finish your training process, you hash the weights, then you do all these in-depth alignment tests, then you send the weights to the safe enclave so that then you can do even more tests later. And then you have the chips regularly check that the weights are the same as what you ended up with.\n\n**Daniel Filan** (01:55:52): I guess also presumably there’s some amount of just thinking about structures that would be good. So I think you mentioned that a centralized AI project, if you structured it correctly, maybe it would be good at being AI-enabled coup-resistant. I imagine there’s probably more thinking someone could do about how you would actually set that up.\n\n**Tom Davidson** (01:56:10): And for all the recommendations in the paper, there’s a lot more thinking about implementation. We’re giving recommendations on a very high level, transparency about various different things and sharing of capabilities with different parts of society to avoid exclusive access, and AI should follow rules that mean they can’t be used for coups, all of that’s \\[missing\\] “what rules exactly?” And exactly how we’re going to structure this transparency requirement, and which exact bodies should AI capabilities be shared with.\n\n(01:56:46): So one type of work I’m excited about is working on drafting contracts between governments and labs that specify these requirements concretely. And similarly for setting up a centralized project, getting much more detail about how it would be structured, as you say.\n\n**Daniel Filan** (01:57:05): I think I’d like to move a little bit onto [Forethought](https://www.forethought.org/), the organization that put out this, but before I do that, is there any last things you want to say about AI-enabled coups?\n\n**Tom Davidson** (01:57:15): I’ll say one more thing, which is that I think it’s really helpful in many contexts to be very explicit about the threat model we’re concerned with. We’ve talked very explicitly about executive coups and lab leaders doing coups. That’s helpful for thinking clearly. I don’t think it’s the most helpful frame in many contexts: coups sound kind of extreme in many contexts, and it sounds like an adversarial framing, it sounds like you’re pointing fingers to individuals rather than just being like, well, obviously no one should be able to do this.\n\n(01:57:52): And so I do think there are other more useful frames in many contexts. So rather than “let’s prevent secret loyalties”, I like the frame of “system integrity”, which just means that the system does what it says on the tin, hasn’t been tampered with, and rather than preventing an executive coup, you can talk about checks and balances, rule of law, democratic robustness.\n\nForethought\n-----------\n\n**Daniel Filan** (01:58:17): Yeah, that’s a good point. Okay, I next want to talk a little bit about [Forethought](https://www.forethought.org/). So Forethought is this new-ish organization. And in March or April, you guys put out a bunch of papers or a bunch of reports. What’s Forethought? What’s going on?\n\n**Tom Davidson** (01:58:37): Yeah, it’s a research organization. We aspire to be considered a successor to [FHI](https://www.futureofhumanityinstitute.org/). So FHI was a macrostrategy research organization, so kind of thinking about strategy in the most zoomed-out terms possible. Often it was thinking about the very long-run future and the different outcomes that might occur, things like [the vulnerable world hypothesis](https://nickbostrom.com/papers/vulnerable.pdf) and [astronomical waste](https://nickbostrom.com/papers/astronomical-waste/), the kind of big, big picture questions, the big picture papers that came out of that institute, FHI.\n\n(01:59:14): And so we’re aspiring to be the follow-on successor that is tackling the really big strategy questions. And the way we’re currently framing it is: we are going over the coming decades very plausibly to transition to a world with superintelligent AI systems. That is just going to bring a whole host of major, major changes. AI misalignment risk is one really important risk to be thinking about over that transition, but there’ll just be a whole host of other issues. AI-enabled coups is one example, and it’s the first one that we’ve really focused on, or at least that I’ve really focused on, but it’s not the only one.\n\n(01:59:54): I mean, I really enjoyed your recent [podcast on AI rights](https://axrp.net/episode/2025/06/28/episode-44-peter-salib-ai-rights-human-safety.html). I think that’s going to be another really big issue that is very much on our radar, and there’s going to be many other big issues as well. Another one that we’re excited about is just that at some point we’re going to start getting access and using resources in space, and how those resources are used is going to be a very, very important question. That is basically all the resources, and we have no idea how we’re going to use them, how we’re going to divvy them up, what the processes will be. In a sense, everything is up for grabs in that decision.\n\n(02:00:34): So that’s another big example. And I expect there’ll be other things where just there’s going to be so much change as we’re going through this. There’s just going to be a lot of things which emerge, and so our aspiration is to be keeping our eye on the ball of these very high-level strategic questions and issues and trying to help us figure out what we should do about them.\n\n**Daniel Filan** (02:00:58): Yeah. You mentioned that the first thing that you focused on is AI-enabled coups. The things you’ve mentioned: are those roughly the things that you expect the institute to prioritize, or what might I see out of Forethought in the next year or so?\n\n**Tom Davidson** (02:01:14): I think those are our current best guesses, the things I mentioned. So I think space governance, you might well see stuff on that, you might well see stuff on AI rights: specific schemes to pay the AIs to work with us if they’re misaligned—something that we feel quite excited about and seems still underexplored, though it is getting more attention, which is great. I think positive uses of AI, for improving epistemics, for improving government decision-making, for ensuring that democracies don’t fall behind autocracies in an automated economy, those are some other issues that seem like we might well focus on. Another issue would be: if we’re choosing these AIs’ personality, exactly what should it be aligned to? Which is, again, a question which is getting more attention, but is going to be very, very consequential.\n\n**Daniel Filan** (02:02:15): Another thing to ask: a bunch of my listeners, maybe they’re coming out of undergrad, maybe they’re in a space where they’re considering changing careers: is Forethought hiring?\n\n**Tom Davidson** (02:02:28): Yeah, we’re planning to do an open hiring round soon. I’m not sure exactly when we’ll release it, but I would really encourage people to apply. I think there’s a lot of talent out there, and I expect there’s a lot of talent we’re completely unaware of. So even if you don’t think that you’ve got the skills or the knowledge, there’s no great on-ramp to doing this kind of work at the moment, and I think there’s a big danger of people just ruling themselves out prematurely. So when we do release the open hiring round, please throw in an application.\n\nFollowing Tom’s and Forethought’s research\n------------------------------------------\n\n**Daniel Filan** (02:03:03): Final thing I want to ask: suppose someone listened to this episode, they found it interesting, and they want to hear more about the work you do, how should they go about doing that?\n\n**Tom Davidson** (02:03:16): With me personally, you can [follow me on Twitter](https://x.com/tomdavidsonx). If you google “Tom Davidson AI X”, then you’ll see my Twitter pop up on Google. So you can follow me, subscribe. I post basically [all of my research on LessWrong](https://www.lesswrong.com/users/tom-davidson-1) because that’s where the big community that cares about some of these issues is. So if you have a LessWrong account, you can subscribe there. We have a [Forethought Substack](https://newsletter.forethought.org/), so if you, again, just google “Forethought Substack”, then the top link. Subscribe, that’d be great. And then you can also follow [Will MacAskill](https://en.wikipedia.org/wiki/William_MacAskill), he’s the other senior researcher at Forethought. [Follow him on Twitter](https://x.com/willmacaskill) and [LessWrong](https://www.lesswrong.com/users/wdmacaskill) as well.\n\n**Daniel Filan** (02:04:07): Great. So yeah, links for all of that will be in the description of this episode. Tom, thanks very much for coming on. It was great chatting with you.\n\n**Tom Davidson** (02:04:14): Yeah, real pleasure. Thanks so much, Daniel.\n\n**Daniel Filan** (02:04:16): This episode is edited by Kate Brunotts and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. This episode was recorded at [FAR.Labs](https://far.ai/programs/far-labs). Financial support for the episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript, you can visit [axrp.net](https://axrp.net). You can also become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast) or give a one-off donation at [ko-fi.com/axrpodcast](https://ko-fi.com/axrpodcast). Finally, you can leave your thoughts on this episode at [axrp.fyi](axrp.fyi).",
      "plaintextDescription": "YouTube link\n\nCould AI enable a small group to gain power over a large country, and lock in their power permanently? Often, people worried about catastrophic risks from AI have been concerned with misalignment risks. In this episode, Tom Davidson talks about a risk that could be comparably important: that of AI-enabled coups.\n\nTopics we discuss:\n\n * How to stage a coup without AI\n * Why AI might enable coups\n * How bad AI-enabled coups are\n * Executive coups with singularly loyal AIs\n * Executive coups with exclusive access to AI\n * Corporate AI-enabled coups\n * Secret loyalty and misalignment in corporate coups\n * Likelihood of different types of AI-enabled coups\n * How to prevent AI-enabled coups\n * Downsides of AIs loyal to the law\n * Cultural shifts vs individual action\n * Technical research to prevent AI-enabled coups\n * Non-technical research to prevent AI-enabled coups\n * Forethought\n * Following Tom’s and Forethought’s research\n\nDaniel Filan (00:00:09): Hello, everybody. In this episode, I’ll be speaking with Tom Davidson. Tom is a senior research fellow at the Forethought Institute for AI Strategy. His work is focused on AI takeoff speeds and more recently, the threat of humans using AI to stage a coup. To read a transcript of this episode, you can go to axrp.net. You can become a patron at patreon.com/axrpodcast. You can also give feedback about the episode at axrp.fyi. All right, Tom, welcome to the podcast.\n\nTom Davidson (00:00:34): Pleasure to be here, Daniel.\n\n\nHow to stage a coup without AI\nDaniel Filan (00:00:35): Yeah. So today we’re going to talk about the… Should I call it a paper? “AI-Enabled Coups: How a Small Group Could Use AI to Seize Power”, by yourself, Lukas Finnveden, and Rose Hadshar. “Paper” is the right term for that?\n\nTom Davidson (00:00:49): Yeah, I think we’d mostly called it “report”, but “paper”, “report”.\n\nDaniel Filan (00:00:53): “Report” seems pretty reasonable. Yeah, so: AI-enabled coups, I guess it’s about using AI to do coup",
      "wordCount": 20525
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "kdbs6xBndPkmrYAxM",
        "name": "Politics",
        "slug": "politics"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "nZtAkGmDELMnLJMQ5",
    "title": "AXRP Episode 45 - Samuel Albanie on DeepMind’s AGI Safety Approach",
    "slug": "axrp-episode-45-samuel-albanie-on-deepmind-s-agi-safety",
    "url": null,
    "baseScore": 31,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-07-06T23:00:03.659Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/y_CFJBR9TyQ)\n\nIn this episode, I chat with Samuel Albanie about the Google DeepMind paper he co-authored called “An Approach to Technical AGI Safety and Security”. It covers the assumptions made by the approach, as well as the types of mitigations it outlines.\n\nTopics we discuss:\n\n*   [DeepMind’s Approach to Technical AGI Safety and Security](#dm-approach)\n*   [Current paradigm continuation](#current-paradigm-continuation)\n*   [No human ceiling](#no-human-ceiling)\n*   [Uncertain timelines](#uncertain-timelines)\n*   [Approximate continuity and the potential for accelerating capability improvement](#approx-continuity)\n*   [Misuse and misalignment](#misuse-and-misalignment)\n*   [Societal readiness](#societal-readiness)\n*   [Misuse mitigations](#misuse-mitigations)\n*   [Misalignment mitigations](#misalignment-mitigations)\n*   [Samuel’s thinking about technical AGI safety](#samuel-thinking)\n*   [Following Samuel’s work](#following-samuels-work)\n\n**Daniel Filan** (00:00:09): Hello, everybody. In this episode, I’ll be speaking with Samuel Albanie, a research scientist at Google DeepMind, who was previously an assistant professor working on computer vision. The description of this episode has links and timestamps for your enjoyment, and a transcript is available at [axrp.net](https://axrp.net/). Two more things: you can support the podcast at [patreon.com/axrpodcast](https://patreon.com/axrpodcast), and also, you can very briefly tell me what you think of this episode at [axrp.fyi](axrp.fyi). All right, well, welcome to the podcast, Samuel.\n\n**Samuel Albanie** (00:00:35): It’s a pleasure to be here.\n\nDeepMind’s Approach to Technical AGI Safety and Security\n--------------------------------------------------------\n\n**Daniel Filan** (00:00:37): Cool. So today, we’re going to be talking about this paper, [“An Approach to Technical AGI Safety and Security”](https://arxiv.org/abs/2504.01849). It’s by a bunch of authors, but the first one is [Rohin Shah](https://rohinshah.com/), and you are somewhere in the middle of this list. Can you tell us: what is this paper about?\n\n**Samuel Albanie** (00:00:51): Sure. So the goal of this paper is to lay out a technical research agenda for addressing some of the severe risks that we think might be posed by AGI.\n\n**Daniel Filan** (00:01:05): I think one thing that kind of struck me when I was reading the paper is not… Well, it’s pretty long, and so there are some things in there that surprise me, but by and large, it all seemed like mostly pretty normal stuff. If you’ve been around the AI safety landscape, I think a lot of these things don’t seem super shocking or surprising. I’m wondering: what’s the… I don’t know. In some sense, what’s the point of it? Didn’t we already know all of this stuff?\n\n**Samuel Albanie** (00:01:43): Yeah, that’s a great point, and perhaps it would be a great sign of maturity of the field to the degree that when describing our plans, there were no signs of novelty there. In many cases, I think the goal of this work is to lay out the approach, and also try to expose it to critiques, both internally within the company, but also to describe the justification for certain choices and elicit comments on them, that sort of thing.\n\n**Daniel Filan** (00:02:13): Okay, and as you were writing it, did you find that that process caused… You realized that you should be doing something different or you already found some internal critiques? I guess as we’re recording this, it’s just been released, so it’s a little bit early for external critiques. Well, actually, no, you probably have received some external critiques, but it’s early for thought-out, high-quality external critiques. I’m wondering, has it already changed plans or caused people to think differently about things?\n\n**Samuel Albanie** (00:02:53): Yeah, that’s a great question. So I think at least from my perspective, it has been very useful to work through some assumptions that I think were implicit in how we were approaching research, and then try to drill down and say, “Okay, what really is the evidence base for this?” And perhaps more importantly, “Under which circumstances do we need to throw some of this stuff away and change some of the assumptions that are underpinning our approach?” One of the assumptions \\[that\\] maybe we’ll get into, perhaps the one that is most… I don’t think “nuanced” is the right word, but \\[it\\] has some complexity to it, is this idea of “approximate continuity”, that progress in AI capabilities is somehow smooth with respect to some of its inputs.\n\n(00:03:42): It’s one thing to say this loosely in conversation and to derive research from it, but it was helpful at least for me to work through the arguments and think a little bit about, “Okay, to what degree do I find these plausible? Where are sources of uncertainty here?” I think there’s a lot of value in that exercise.\n\n**Daniel Filan** (00:03:59): Fair enough. My understanding is that the “assumptions” section is the part that you did most of your contributions to in the paper. Is that right?\n\n**Samuel Albanie** (00:04:13): I think that’s a fair characterization. Yeah.\n\n**Daniel Filan** (00:04:15): Okay, so in that case, maybe it would be good to start off just talking about what these assumptions are and what their implications are, if that’s okay?\n\n**Samuel Albanie** (00:04:25): Sure. Yeah. We can just blow through one by one, or if you want to pick one to \\[inaudible 00:04:30\\]-\n\nCurrent paradigm continuation\n-----------------------------\n\n**Daniel Filan** (00:04:29): I think one by one in the order in the paper seems good. So the first one is “current paradigm continuation”, which is… Maybe I should let you say. How would you characterize just what that assumption is?\n\n**Samuel Albanie** (00:04:44): Yeah, so the key idea here is that we are anticipating and planning that frontier AI development, at least in the near-term future, looks very similar to what we’ve seen so far. If I was to characterize that, I would say it’s drawing on these ideas from perhaps \\[Hans\\] [Moravec](https://en.wikipedia.org/wiki/Moravec%27s_paradox), the fundamental role of computation in the improvements in AI capabilities. The ideas of someone like [Rich Sutton](http://incompleteideas.net/) in his [Bitter Lesson](http://www.incompleteideas.net/IncIdeas/BitterLesson.html), that a lot of the progress is being driven by some combination of foundational techniques like learning and search, and that because we have seen significant progress so far within this paradigm - though I accept there are some differences of opinion on that, but I’ll leave that aside for a second - and because it’s highly plausible that those inputs will continue to grow, that it’s a reasonable bet when we’re thinking about our research portfolio to make a pretty strong assumption that we’re going to maintain in this regime.\n\n**Daniel Filan** (00:05:48): I’m wondering: so the term “regime”, or the term “paradigm”, it can be a little bit loose, so how specific are-\n\n**Samuel Albanie** (00:05:59): What’s not in the paradigm? Is that a-\n\n**Daniel Filan** (00:06:00): Yeah, or maybe… Suppose we stopped using transformers? Would that violate this assumption?\n\n**Samuel Albanie** (00:06:09): It would not. It’s relatively loosely scoped here, roughly because we’re thinking a lot in terms of the inputs to the process, and so methods… In some ways I think of transformers as being quite a natural continuation of convolutional neural networks: sort of a loosening of the inductive biases to make more general use of computation. And so if there were to be further steps in that direction, that would plausibly still fit - at least in my mind, in terms of how we’re baking the assumptions into the plan - very much still within the current paradigm. Whereas, to take something that would be not inside the paradigm, something like [brain uploads](https://ageofem.com/) from [Robin Hanson](https://en.wikipedia.org/wiki/Robin_Hanson) or something for which learning did not play a pivotal role in the acquisition of new capabilities.\n\n**Daniel Filan** (00:07:02): Okay. Is the use of gradient descent a crucial part of the paradigm as you see it?\n\n**Samuel Albanie** (00:07:10): That is a great question. Maybe we can scope this out a little. In terms of alternatives, do you have in mind something like evolutionary search or basically a method that does not make any use of gradients?\n\n**Daniel Filan** (00:07:27): I don’t have any particular thing in mind, although things I could imagine are evolutionary search, I could imagine maybe we move to using these [hyper-networks](https://arxiv.org/abs/2306.06955) instead of these object-level networks. I guess probably you could do that with evolutionary search \\[or\\] you could do that with something else. You could imagine we start doing A* search with these heuristics that maybe we… Okay, as I say this, I’m realizing that evolutionary search is the one non-gradient-descent-y thing that I can think of.\n\n(00:08:12): You could imagine iterative… Suppose we start doing various Monte Carlo sampling things. You could imagine that being iterative updates that are not quite gradient descent as we understand them, but yeah, I guess I’m not totally sure I have an alternative in mind. I’d like to understand how specific is this assumption, because the more specific the assumption is on the one hand, the harder it will be to believe, but on the other hand, the more research directions will be justified by the assumption.\n\n**Samuel Albanie** (00:08:48): Yeah, that’s a great point. So I would say it’s quite a loose assumption. I think we have in mind here: broadly learning and search does cover an extraordinarily broad suite of things. Evolutionary algorithms in some sense also would fit into those categories. And so it’s useful, I think, for guiding things, but to your point about this trade-off between specificity and how much it unlocks versus how risky it is as an assumption, I would view this as among the looser ones that we’re leaning on.\n\n**Daniel Filan** (00:09:23): Sure. To pick up something you said a little bit earlier: in terms of what wouldn’t this cover, it sounded like if we did AI by uploading human brains, that would not be covered by this assumption.\n\n**Samuel Albanie** (00:09:37): Mm-hmm.\n\n**Daniel Filan** (00:09:37): Is there anything more similar to the current paradigm or even potentially more likely to happen by your judgment that would still count as breaking this assumption?\n\n**Samuel Albanie** (00:09:50): Yeah, it’s a good question. I’m mainly thinking in terms of these properties of leveraging increased computation and making heavy use of the R&D effort that is currently underway. I think if it were to be the case that highly logical systems, perhaps akin to expert systems, could be constructed in a way that was not leveraging learning in a way that is close to how it is done currently, and not leveraging search… It is quite difficult, though, for me to come up with good examples.\n\n**Daniel Filan** (00:10:24): Okay, so potentially some sort of, if we had a super rational Bayesian expected utility maximizer that was computationally limited, but got better when you added more computation, it sounds like that would potentially count as the kind of thing that would not break this assumption, that maybe you would put some work into.\n\n**Samuel Albanie** (00:10:45): That’s a good point. I think that would require pretty heavy revisiting of some of our components. To give some examples, I think we are quite tied to core concepts in machine learning when we conceptualize how we’re tackling the alignment problem. Later in the document, there’s a description of how we’re trying to get good learning signal through these mechanisms like amplified oversight, and implicitly that’s making an assumption about how the model is going to be trained. It is plausible that that also fits into some of the more Bayesian frameworks that you’re describing. It’s not immediately clear the jump to me, but-\n\n**Daniel Filan** (00:11:26): If it’s a Bayesian reinforcement learner, you could imagine there’s uncertainty over some underlying reward signal and different amplified oversight activities provide more or less information about the rewards. I think a lot of these things-\n\n**Samuel Albanie** (00:11:38): That’s true.\n\n**Daniel Filan** (00:11:40): …and, in fact, a lot of the amplified oversight work, I think, was conceived of in a… Or if you think of work from [CHAI](https://humancompatible.ai/) on [cooperative inverse reinforcement learning](https://arxiv.org/abs/1606.03137), it’s conceived of in this very Bayesian way and a lot of oversight work you can think of in this sense.\n\n**Samuel Albanie** (00:11:56): Yeah, I suppose it depends how we’re using the term “Bayesian”. If it’s effective propagation of uncertainty, yeah, I would fully agree that that’s on board with this. I’m not sure that I have a particularly clear alternative as a way to frame that.\n\n**Daniel Filan** (00:12:16): Okay, now that we’ve got a decent understanding of what the assumption is: so my understanding of the argument for the assumption is something like this: the current paradigm, it’s been working for a while. It doesn’t show any signs of stopping and there’s no obvious other paradigm that seems like it’s going to swoop in and do something different. All these are decent arguments, but… Importantly, I think near the start of the paper it says that this is basically a planning document up to the year 2030 and after that to some degree all bets are off.\n\n(00:12:58): The arguments are roughly saying, “Okay, for the next five years-ish, we should expect the current paradigm to hold.” I’m wondering: it would probably not be reasonable to assume that the current paradigm will hold for the next thousand years, so all these arguments must have some implicit time scale. I’m wondering: if you project out past 2030, what is the time scale at which these arguments start breaking? Is it more like 10 years? Or is it more like 50 years?\n\n**Samuel Albanie** (00:13:25): That is a great question. So maybe I should just clarify that 2030… I should double-check what the phrasing is precisely, but I think that’s given as an illustrative date of… And I do think it is useful as a reference point, but I don’t think that the plan is anchored specifically around that as a date. Please feel free to correct me…\n\n**Daniel Filan** (00:13:47): So in the discussion, when you’re talking about this assumption, you say, “For the purposes of this argument, we consider evidence relating to a five-year future time horizon. We do so partly as a matter of feasibility—this is a time horizon over which we can make reasonably informed estimates about key variables that we believe drive progress within the current paradigm (although we anticipate that the current paradigm will continue beyond this). We also do so partly as a matter of pragmatism—planning over significantly longer horizons is challenging in a rapidly developing R&D environment. As such, we anticipate that some revisions to our assumptions, beliefs, and approach may be appropriate in the future.”\n\n**Samuel Albanie** (00:14:25): Okay, then I retract my previous objection. That is very explicit. Yeah, so with regards to that date, part of the rationale for using it was looking at the previous historical trend of how things have developed and then trying to make arguments about where we can expect the inputs to continue. 2030 is a cute date partly because there was [this nice study by Epoch](https://epoch.ai/blog/can-ai-scaling-continue-through-2030) that was trying to do a relatively fine-grained analysis of which of the inputs currently used by the current paradigm could plausibly continue up to 2030. Then, on the basis of some blend of Fermi estimates and analysis, they came to the conclusion that that was highly feasible, and that’s part of the motivation here.\n\n**Daniel Filan** (00:15:12): Okay, so it sounds like the arguments are basically like, “Look, as long as we can continue scaling up the inputs to this process…” And maybe I can imagine some argument that says, “Look, maybe there’s some Laplace’s Law thing where you expect to keep going about as long as you’ve been going so far,” and-\n\n**Samuel Albanie** (00:15:34): Oh, is this like [the Lindy effect](https://en.wikipedia.org/wiki/Lindy_effect)? I don’t know the Laplace thing. That’s new for me.\n\n**Daniel Filan** (00:15:39): Oh, I’m just imagining like… Sorry, I might be wrong here, so [Laplace’s Law of Succession](https://en.wikipedia.org/wiki/Rule_of_succession): suppose we’re saying, how many years has it been since the current paradigm started? Then, we imagine there’s some underlying probability of the current paradigm switching and we don’t know what that probability is. We say, “Well, we’ve observed the paradigm not fall for seven years…” or maybe you want to give it 10 or 12 years or something. Then, you can say, “Okay, well, roughly the rate of the current paradigm failing per year has got to be a little bit around one in 12,” if we’ve seen 12 instances of it not failing and zero instances of it failing.\n\n**Samuel Albanie** (00:16:32): I see. I had not encountered that terminology. That’s a useful one to know. Maybe to recurse back to your original question, which as I understood it was, “do we expect it to last far beyond that?” Or “why have you chosen the date?” Perhaps it was both of those questions, and so…\n\n**Daniel Filan** (00:16:52): Maybe it was like, in order to understand whether the arguments really work for 2030, when would they actually stop working?\n\n**Samuel Albanie** (00:17:00): I see. So, ptart of the mindset of this approach is to give ourselves moderate time planning horizons, and it is just highly likely that we would execute a replan over that time scale. Based on current trajectory, that seems like a reasonable future to plan over, but it’s not a load-bearing assumption about what is likely to happen after that.\n\n(00:17:27): With regards to specifically the scaling, I think… Well, it remains to be seen. Perhaps one of the most notable inputs is training computation, and [Epoch has been tracking that quite carefully](https://epoch.ai/trends#compute) as I understand it. I think we are very much above the trends that they initially projected in [the first study](https://epoch.ai/blog/compute-trends), as based on, say, the [Grok recent training run](https://epoch.ai/gradient-updates/ai-progress-is-about-to-speed-up) reported at least in their public database, so that seems… Well, at the time we were writing this particular section, which was late last year.\n\n**Daniel Filan** (00:18:08): Gotcha. Actually, yeah, that’s an interesting question. How long have you guys been working on this? Late last year, it sounds like this has been quite a long time coming.\n\n**Samuel Albanie** (00:18:22): Well, I think there’s a continuous process of assessing the research landscape and trying to integrate new developments into a cohesive plan, and there’s always a degree of replanning that happens. As for why specifically this date, I’m not sure that there was… I don’t think I have a good answer to at what date the document was originally planned. I don’t have a good answer to that, unfortunately.\n\n**Daniel Filan** (00:18:57): Okay, fair enough. Cool, so I think I’m probably ready to move to the second assumption, unless there’s more things you want to say about the paradigm continuation.\n\n**Samuel Albanie** (00:19:11): No, I think \\[I’m\\] good to move on.\n\nNo human ceiling\n----------------\n\n**Daniel Filan** (00:19:13): Okay, so the second one is that there is no human ceiling. And so my understanding is that this is basically saying AIs can be smarter, more capable than humans. Is that basically right?\n\n**Samuel Albanie** (00:19:27): That is basically right.\n\n**Daniel Filan** (00:19:28): Okay, and actually maybe this can be a jumping-off point to talk about the level of AGI that you talk about in the paper. You mentioned that basically you’re going to be talking about this level of “exceptional” AGI, which comes from [this paper](https://arxiv.org/abs/2311.02462) that used the term “virtuoso AGI”, and it says it’s the level of the 99th percentile of skilled adults on a broad range of tasks. I was kind of confused by this definition, and maybe it just depends on what “skilled” means, but I think for most tasks or most domains, it’s pretty easy to be better than 99% of people just by trying a little bit.\n\n(00:20:15): For instance: well, I suppose you have to pick a language that fewer than 1% of people speak, but if you learn 10 words in that language, you’re now better than 99% of people at that language. If you learn the super basics of juggling, you’re better than 99% of people at juggling. It’s probably not that hard to be a 99th percentile surgeon, right? But maybe this word “skilled” is doing a lot. Can you help me understand what’s going on here?\n\n**Samuel Albanie** (00:20:44): The percentiles are in reference to a sample of adults who possess the relevant skill. In [the “Levels of AGI” paper](https://arxiv.org/abs/2311.02462), the authors give as an example that performance on a task such as English writing ability would only be measured against the set of adults who are literate and fluent in English. It’s not a completely self-contained definition because it is still necessary to determine what it means for an adult to possess the relevant skill. In the juggling example, I’d define that to be the group of people who could juggle perhaps with three balls.\n\nUncertain timelines\n-------------------\n\n**Daniel Filan** (00:21:22): Fair enough. So perhaps the next thing to talk about is the “uncertain timelines” assumption. Can you say roughly what that is?\n\n**Samuel Albanie** (00:21:33): Sure. So the premise here is that many people have spent time thinking about plausible timelines over which AI could develop, and there is still perhaps not a very strong consensus over what the most probable timeline for AI development will look like. Perhaps you’ve seen in the last few days this nice article from [Daniel](https://en.wikipedia.org/wiki/Daniel_Kokotajlo_(researcher)) \\[Kokotajlo\\] and collaborators on the [AI 2027](https://ai-2027.com/) project positing one plausible future scenario.\n\n(00:22:07): Many people who have been surveyed across different disciplines have very different opinions based on the evidence that’s available currently about what is plausible. The assumption is roughly saying: short timelines seem plausible and, therefore, we should try to adopt strategies that have a kind of “anytime” flavor to them, that they could be put into practice at relatively short notice, accepting that there is some uncertainty here.\n\n**Daniel Filan** (00:22:34): Okay. You mentioned that part of the assumption is that short timelines seem plausible. I guess for it to be “uncertain” rather than “certain of short timelines”, maybe part of the assumption is also that longer timelines also seem plausible. Is that half of things something that you’re intending to include? If so, how does that play into the strategy?\n\n**Samuel Albanie** (00:23:02): Yeah, so I think one aspect of the plan currently is that there’s still a relatively… I mean, this is a subjective statement, but there is some diversity in the portfolio. There are a collection of different approaches, and in the most accelerating worlds, some of those options do not make that much sense, but we are still in a regime where because there is this uncertainty, some diversity on the portfolio makes sense. That’s roughly the trade-off we’re making here.\n\nApproximate continuity and the potential for accelerating capability improvement\n--------------------------------------------------------------------------------\n\n**Daniel Filan** (00:23:36): I think the interesting part of this assumption comes in in the interplay with the next assumption. The next assumption is “approximate continuity”, and this one, I think I actually misunderstood it the first time I saw it written down. Can you tell us just: what is the “approximate continuity” assumption?\n\n**Samuel Albanie** (00:23:53): Yeah, so this is the assumption that improvements in AI capability will be approximately or roughly smooth with respect to some of the key inputs to the process. The kinds of inputs we’re thinking about here are computation and R&D effort, but not necessarily something like calendar time.\n\n**Daniel Filan** (00:24:17): Okay, so if I put this together with potential for accelerating improvement, what I get is that it is plausible that there’s this kind of increasing ever-quickening cycle of improvement where, well, maybe compute goes in relatively continuously with calendar time, but R&D effort increases and increases quite quickly. Improvement in capabilities is pretty smooth with the amount of R&D input and the amount of compute, but in real time, plausibly R&D input increases very, very quickly and, therefore, capabilities increase very, very quickly.\n\n**Samuel Albanie** (00:25:02): Yes, that’s right.\n\n**Daniel Filan** (00:25:05): The thing that confused me here is that in the “approximate continuity”… My understanding of the consequence of that assumption is that you could have some sort of iterative approach where you do empirical tests, see how things are going, and then things will go like that for a little while because it’s continuous. If things are going very fast in calendar time, I would have imagined that it would be pretty hard to… If I imagine trying to do an iterative approach, what I imagine is \\[that\\] I do some experiment, it takes me some amount of time to do the experiment, then I think about the results for a little while, and I’m like, “Okay, this means this,” and then I work on my mitigation for any problems or I implement something to incorporate the things I learned from that experiment into what’s happening. As long as I or another human am the one doing that, I would think that that would be pretty closely related to calendar time, but if things are not necessarily continuous in calendar time, then I’m confused how this approach is able to work.\n\n**Samuel Albanie** (00:26:24): Yeah, so one framing of this is that because… And it does rely on very careful measurement of the R&D capabilities of the models, so as calendar time shrinks, the assumption here is that in the scenario you’re describing, the R&D capabilities’ net is growing very significantly, and so what corresponds to a delta will be perhaps very, very short in calendar time, but nevertheless, can still be tracked, and the replanning and reassessment of risk needs to happen at shortening time scales. And so if there was to be a mitigation or a pause or a stop, that is how it would be implemented.\n\n**Daniel Filan** (00:27:08): Okay. Maybe the thing I’m confused by there is: it seems like it might happen faster than we can… It takes a while to consider things and to think about how to do a mitigation. And so was the thought like: this is feasible because the AIs who are doing the R&D will be thinking about all of that? Or is the assumption: at this stage, all we’re doing is going to be keeping track of the… You’re not writing papers on various types of optimizers anymore, the AIs are doing that. All you’re doing is thinking about how to react to changes in the R&D input. Yeah, I guess I’m wondering just what does it look like to actually implement this in a world where capabilities are growing super, super quick in calendar time, but continuously in R&D effort?\n\n**Samuel Albanie** (00:28:06): Yeah, so the way that I’ve been thinking about it is there are measurements being made and a continuous assessment of safety buffers projected into the future. As progress goes up, there’s a sort of scanning horizon over which we think we can continuously perform the kinds of tests, mitigations, and checks that we think would be necessary to continue to the next stage. Those would become closer and closer in calendar time, and if we hit some component of the system, some quantum, some setting that meant that it was not safe to continue on the basis of the shortening time scales, then the system would have to stop.\n\n(00:28:50): It’s more that that’s not a foundational axiom of the plan. That would just be downstream of the fact that a mitigation was not appropriate for a certain time-scale, but in principle, it’s not as a consequence of the shortening time scale itself, though it may in practice be the case that that is a limiting factor because we’re not able to operate a system that we feel comfortable with.\n\n**Daniel Filan** (00:29:13): Okay, so the thought is something like: at any given point in time, you’ll have some safety measures or whatever, and you can see that they work pretty well, and you can see that they’re going to work for the next, let’s say, doubling of R&D input. Then, once you’ve 1.3-x-ed R&D input, you figure out some new safety mitigations that will bring you further past that, and then at this stage, you figure out mitigations for what will happen further past that… Am I understanding you correctly this far?\n\n**Samuel Albanie** (00:29:50): Yeah, that part is correct.\n\n**Daniel Filan** (00:29:52): Okay, but in that case, it seems like the… Well, I guess this seems like we’re going to have to really be leaning a lot on the AI R&D workforce to do a lot of the work of coming up with these new safety mitigations and stuff. If I’m having… Suppose these milestones are coming up every three days for me. Maybe DeepMind just has all these people who can think of all the necessary safety mitigations in three days, but then it speeds up to it’s every one and a half days and it’s too fast for even the Google DeepMind people. Am I right that \\[to deal\\] with this, \\[it\\] seems like a lot of the work is going to have to be outsourced to these AI researchers?\n\n**Samuel Albanie** (00:30:42): In the regime in which things are moving quickly? Yes, that is a fairly foundational component. And most likely one of the things that will cause a risk assessment that says things need to pause or halt are the complexity of establishing those schemes. If it is the case that we cannot get to a sufficient level of confidence that the scheme can continue, that is the kind of thing that would stop progress.\n\n**Daniel Filan** (00:31:11): I think this helps me get a better sense of what’s being assumed here and the actual work that this assumption is doing and also the limitations of it.\n\n(00:31:19): Maybe this gets to perhaps a thing which I thought was different between this plan and some others that I’ve seen. If I look at… I don’t think this is Anthropic’s official safety plan, but there is this blog post called [“The Checklist”](https://sleepinyourhat.github.io/checklist/) that [Sam Bowman](https://sleepinyourhat.github.io/) wrote that I thought was relatively influential, and it is basically framed around: we should automate AI research and development. In particular, we want to automate safety work, and all of our work right now is to figure out how to automate AI safety work. And at the start of the [“Approach to Technical AGI Safety and Security”](https://arxiv.org/abs/2504.01849), one thing it says is, “our approach in this paper is not primarily targeted at… \\[automating\\] AI safety research and development.”\n\n(00:32:12): I’m compressing that quote a little bit, but hopefully that’s a fair characterization of it. And on the one hand, I was going to ask, “Okay, well, why is there this difference?”, but it sounds like if I combine the potential for accelerating improvement and approximate continuity, it sounds like this plan really is going to rely very heavily on automated AI safety research and development. So I guess I’m confused. Can you help me understand what’s going on?\n\n**Samuel Albanie** (00:32:42): Sure. Yeah, that’s a great question. I think one framing of this is that that approach is implicit in our plan if the trajectory rolls forwards in a certain way. That is to say that if AI development does accelerate very quickly, and if it was the case, then our plan moves closer and closer to that setting. In some sense it’s a slightly more diversified portfolio currently that would collapse or concentrate according to how things develop.\n\n**Daniel Filan** (00:33:15): Okay, so when it said it was not primarily targeted at that goal, it sounds like how I should understand that is you were not assuming that you definitely will try and automate AI safety research and development as a thing, but you also aim to make sure that you could do that in the world where that’s possible or in the world where you have accelerating AI research and development, which you think is plausible.\n\n**Samuel Albanie** (00:33:44): Right. That’s correct, and it’s not that we would escape any of the… As you are no doubt aware, there are many significant challenges to be overcome to implement that strategy. I think it’s discussed briefly in the paper, this idea of bootstrapping and the challenges of using one aligned assistant to align its successor. Given those difficulties, it is highly plausible that progress is bottlenecked by an inability to make a strong safety case that progress can continue.\n\n**Daniel Filan** (00:34:15): Maybe we should move on to approaches to mitigating risks described in the paper as opposed to the assumptions, unless there’s more that you want to say about the assumptions.\n\n**Samuel Albanie** (00:34:26): No, that sounds good to me.\n\nMisuse and misalignment\n-----------------------\n\n**Daniel Filan** (00:34:29): Okay, so it seems to me that the two types of risk that are… Or perhaps “types” is a slightly wrong word, but the two things the paper talks about the most are misuse and misalignment, where “misuse” is roughly somebody directs a model to do a bad thing and the model does the bad thing, and “misalignment” is the model does a bad thing knowing that it’s bad, but not because someone else got it to do the bad thing. Is that roughly right?\n\n**Samuel Albanie** (00:35:04): Yeah, that’s a good summary. I mean, there’s some slight nuances, but I think that’s a good high-level summary.\n\n**Daniel Filan** (00:35:13): Oh, I’m curious about the nuances, actually, because one thing I noticed is: at one point in the paper, “misuse” is described as a user gets an AI model to do a bad thing, and “misalignment” is described as an AI model deliberately does a bad thing or knowingly does a bad thing, and in that definition, misuse could also be misalignment, right?\n\n**Samuel Albanie** (00:35:35): Yes, that is a good point. The risks don’t form a clean categorization. They are neither exhaustive nor exclusive. They are not exclusive in the sense that you could have, for example, a misaligned AI system that recruits help from a malicious actor to exfiltrate its own model weights, which would then be a combination of misuse and misalignment.\n\n(00:36:00): On the other hand, given the scoping of the paper, we don’t have all possible risks like AI suffering, for example. The main benefit of the risk areas is for organizing mitigation strategies, since the types of solutions and mitigations needed tend to differ quite significantly depending on the source of the potential harm. Misuse involves focusing on human actors with mitigations like security, filtering harmful requests and so on, while misalignment requires focusing on the AI’s internal goals and learning process and involves better training objectives, amplified oversight, and so on.\n\n**Daniel Filan** (00:36:41): I think that’s fair enough to say. That’s one concern about perhaps over-inclusion of things or over-inclusion of requests into the inherent misuse bucket. Perhaps another concern is under-inclusion. One thing that I believe you mentioned in the paper is \\[that\\] one example of a thing that could count as misuse or misalignment is: you have one AI asking another AI for information that helps the first AI do bad stuff. The first AI is misaligned and the second AI is misused.\n\n(00:37:12): And it strikes me that a lot of the discussion of misuse is imagining things that are roughly human actors. A guy or a collection of people is going to make a nuclear weapon and we don’t want that to happen because they are the wrong people to have nuclear weapons. It does strike me that the requests that we don’t want answers to \\[for\\] other AIs could potentially be different from things we’re imagining in the [CBRN](https://en.wikipedia.org/wiki/CBRN_defense) space. For instance, how do you evade certain controls and stuff? With the previous answer, I think it’s fair enough to say, “Look, it’s not really a technical question that we’re trying to address,” but “what information would it be very dangerous to give another AI?” does strike me as more close to a technical question. So I’m wondering: do you have thoughts on what dangerous requests look like in the context of AIs interacting with each other?\n\n**Samuel Albanie** (00:38:23): That is a great question. It is deferred and left out of scope for this technical document, but it is something that people are thinking a lot about. I don’t have a great pre-baked answer other than to say it’s something where as the capabilities continue to improve, I think that that threat landscape is becoming much more salient, and I just expect there to be significantly more work going forwards. But not something that’s in scope for the work here.\n\n**Daniel Filan** (00:38:53): Would you say this kind of falls under the regime of access control and monitoring in the misalignment mitigation section?\n\n**Samuel Albanie** (00:39:06): I think to some degree there are components of that, but you have described exactly the potential of one failure case of this scenario. The case in which harm is achieved in aggregate or risks are accumulated piecemeal across many actors such that no individual actor… perhaps across different AI developers. We’re not explicitly handling that in this approach.\n\nSocietal readiness\n------------------\n\n**Daniel Filan** (00:39:34): Fair enough. Perhaps to get back to the more core misuse thing: you talk about doing threat models and evaluations for specific mitigations that are safety post-training, capability suppression, and monitoring, and also access restrictions, which I think makes a lot of sense in the light of which requests are dangerous depends on who’s making the request. You also have this additional section which is “security”, in the sense of I believe security of the weights of the model, and also “societal readiness”, are also aspects of the misuse domain, I guess.\n\n(00:40:23): I think security of model weights is a thing probably a lot of people in the AI safety space have heard about or thought about a little bit. Societal readiness seems if anything perhaps underrated or under-thought-about in these spaces. I’m wondering if you have thoughts about what that should look like or how that looks, especially from a technical angle.\n\n**Samuel Albanie** (00:40:51): I think one example that’s a nice one to give the idea here relates to cybersecurity, and I believe it’s the one discussed in the paper, where as AIs become more capable at cyber offense, one way to reduce the misuse risk is to contribute those capabilities to the hardening of many bits of societal infrastructure, which currently… Well, I’m not well-qualified to make an assessment on the overall risk state, but vulnerabilities exist in many cases. That’s an ongoing process of hardening.\n\n**Daniel Filan** (00:41:31): Yeah, and I believe a previous guest on the podcast, [Jason Gross](https://axrp.net/episode/2025/03/28/episode-40-jason-gross-compact-proofs-interpretability.html), is thinking about this to some degree.\n\n**Samuel Albanie** (00:41:36): Oh great.\n\n**Daniel Filan** (00:41:40): It sounds like this is mostly thinking about using existing AI in order to harden up bits of societal infrastructure, to make bits of societal infrastructure less vulnerable to things. Perhaps if there was some way to use AI to make it easier to make vaccines for things or to make it easier to make things that stop you from being damaged by a chemical weapon, it sounds like that would also fall under this umbrella.\n\n**Samuel Albanie** (00:42:07): That’s the key motivation. Yeah.\n\n**Daniel Filan** (00:42:09): Fair enough. I’m wondering: one thing that feels related in spirit, although less technical, is: there are various labs such as Google DeepMind, such as OpenAI, such as Anthropic, that work to release models to the public. One reason is they do cool stuff and it’s valuable to have them be released, but I think another theory of change for this is just \\[that\\] it is useful for the public to know what AI capabilities actually are so that they know how worried they should be, so that they know what things they should want to be done about it. If I think just colloquially of societal readiness for AGI, it strikes me that at the moment this is probably the biggest thing driving societal readiness of AGI. I’m wondering: does this count as in scope for what you’re thinking of as societal readiness?\n\n**Samuel Albanie** (00:43:12): Oh, that’s a nice question. It is certainly the case that I share your sentiment that that is one of the most effective ways to increase current readiness, though there are clearly trade-offs here. Yeah, I’d have to think a little more as to whether it was motivated from the same angle, but certainly I think it does have commonalities. I believe your phrase “similar in spirit” is a good way to characterize it.\n\n**Daniel Filan** (00:43:45): Fair enough.\n\n**Samuel Albanie** (00:43:48): It’s a little less explicit. I mean, there are so many other things going on there, but perhaps a positive side effect.\n\nMisuse mitigations\n------------------\n\n**Daniel Filan** (00:43:58): Fair enough. And then finally with misuse, you mentioned that, okay, there’s going to be basically red and blue teams to stress-test misuse mitigations and also safety cases - some sort of structured argument for why misuse is unlikely or impossible. Then, you can try and investigate the assumptions. I think this is - I also kind of see this in the misalignment section - there’s these red-blue team exercises, red-teaming assumptions, getting safety cases for alignment. I’m wondering, do you think these are going to look very similar or do you think they look pretty different? If they look different, how did it come to be that the assurances for misuse and for misalignment look so similar structurally?\n\n**Samuel Albanie** (00:44:56): That’s a good question. I suppose with many of the cases in misuse as we’re characterizing it, we have some experience and fairly concrete ideas of what the risk factors look like. I think that concreteness lends a lot of opportunities for the sorts of strategies that red teams can be expected to deploy. There’s a pretty clear idea as to who potential threat actors are, the kinds of strategies they might use. In the case of the misalignment work, because some of these threats and risks are… they’re not novel necessarily conceptually, but our experience with working with them is relatively new. I do expect there to be some similarities based from that perspective.\n\n**Daniel Filan** (00:45:43): Fair enough, so-\n\n**Samuel Albanie** (00:45:48): To give some kind of concrete example, when thinking about misuse, “know your customer”-style checks are leveraging external history of a particular user in the outside world and using that as evidence about their intention, and that kind of affordance is not going to be available in the misalignment setting in the mitigations we’re setting. I expect there to be many such cases that distinguish between them, but at a broad level, adversarially testing the robustness of the system is kind of a generically good thing to do.\n\n**Daniel Filan** (00:46:20): Yeah. Well, the “know your customer”… I mean, in some sense this seems similar to stuff like access control for AI.\n\n**Samuel Albanie** (00:46:30): Access control would be similar I believe.\n\n**Daniel Filan** (00:46:32): Yeah, and in some sense it’s kind of similar to “know your customer”, right?\n\n**Samuel Albanie** (00:46:36): Well, there’s two things. One is access control, and the second is the kinds of evidence you’re accumulating about whether something can be trusted.\n\n**Daniel Filan** (00:46:45): Fair enough, fair enough. But it does remind me that there has been [some amount of stuff written about infrastructure for AI agents](https://arxiv.org/abs/2501.10114) that comes sort of close to infrastructure we have for humans doing things that could potentially be dangerous. But it’s fair enough to say that for misuse, we’re potentially thinking of things that are more precedented. I wonder: maybe is that a consequence of the assumption that we’re only looking for the “exceptional” AGI? I could imagine a world where AI gets good enough that humanity learns of some weird, dangerous things. I believe there’s [some book](https://nickbostrom.com/papers/vulnerable.pdf) where [Nick Bostrom](https://en.wikipedia.org/wiki/Nick_Bostrom) uses this example of, “Well, we could potentially live in a world where if you took some sand and you put it in the microwave and you microwaved it for five minutes, you’ve got this highly dangerous explosive.”\n\n**Samuel Albanie** (00:47:51): The vulnerable world.\n\n**Daniel Filan** (00:47:52): Yeah, this vulnerable world, and you could imagine that maybe we develop AGI and at some point it teaches us of these vulnerabilities. We don’t just have to worry about nuclear weapons, we also have to worry about sand weapons or some other thing that we haven’t thought about before.\n\n**Samuel Albanie** (00:48:09): [Ice-nine](https://en.wikipedia.org/wiki/Ice-nine), yes.\n\n**Daniel Filan** (00:48:10): Yeah, Ice-nine is so scary. Okay, so ice-nine, as you mention in the paper, it comes from [this story](https://en.wikipedia.org/wiki/Cat%27s_Cradle) by [Kurt Vonnegut](https://en.wikipedia.org/wiki/Kurt_Vonnegut) where it’s this different version of water that’s solid at temperatures below like 45 degrees Celsius, and any normal water that touches ice-nine becomes solid. Then, it just takes over the world. That hasn’t happened in water, but that [really has happened with certain chemicals in the world](https://en.wikipedia.org/wiki/Disappearing_polymorph). There are drugs that don’t work anymore because basically this thing happened - more than one of them. It’s one of the creepiest… This fact just creeps me out so much. Where was I?\n\n**Samuel Albanie** (00:49:06): I think you were probing about-\n\n**Daniel Filan** (00:49:08): Yeah, I was long-windedly -\n\n**Samuel Albanie** (00:49:09): …well, there is this component of there may just be a lot of unknown unknowns that are baked into the ecosystem that will be revealed as the models become more capable.\n\n**Daniel Filan** (00:49:19): Yeah, and I’m wondering: if you’re thinking of misuse as, “Okay, there are basically known dangers”, is that a consequence of an assumption that we’re talking about AI that is a little bit smart, but not wildly superhuman?\n\n**Samuel Albanie** (00:49:36): The comment on known dangers, I think I perhaps would use that more as a reflection on the maturity of those fields currently rather than maybe a fundamental distinction between them, just because the relative capabilities of AIs and human threat actors are in the state that they are currently, but the affordances of both I do expect to change over time. For example, risks that come from the fact that the AIs can absorb very large amounts of content concurrently or execute at extremely high speed will mean that plausibly there are risks that were not tractable in the case of human operatives that are now tractable.\n\n**Daniel Filan** (00:50:22): Yeah, I mean, it seems like it plays into the mitigation… The misuse mechanisms - there’s safety post-training, there’s capability suppression, and there’s monitoring. It seems like those rely on knowing which things you have to post-train the model to not talk about, knowing which capabilities you’ve got to suppress, and knowing which things you’ve got to monitor for. Whereas, if AI is smart enough that it can discover- that it can learn about a new vulnerability in the world. It lets some humans know about it and then humans start exploiting it. If that happens before developers are able to realize what the issue is, figure out what capabilities they should suppress, figure out what questions they should get the model to not answer, figure out things they should monitor for, I think in that world, those misuse mitigations become weaker. It seems like there must be some assumption there, unless I’m misunderstanding how general these tools are.\n\n**Samuel Albanie** (00:51:31): That is correct. There is explicit threat modeling that goes on to try to identify the kinds of misuse risks that we think should be prioritized. \\[There’s\\] explicit thought about what capability levels pose risks for certain threat actors, and then mitigations are implemented downstream of those. And so there needs to be a kind of continuous scanning of the horizon for new risks that may materialize, but it is not the case that they are sort of baked in in some implicit way into the plan.\n\n**Daniel Filan** (00:52:03): Yeah, and I suppose one nice thing about that is that if you’re a model developer and if you’re worried about new vulnerabilities being found by AI, if you have the smart AI before anyone else does, then maybe that helps you scan the horizon for vulnerabilities that you should care about and you might hope that you’d be able to find them before other people do?\n\n**Samuel Albanie** (00:52:24): There’s that. These things have these dynamics of a so-called “wicked problem”. They’re very entangled together, and I think this is often described as one of the challenges of an open source approach where if it was the case that such a vulnerability was discovered, the inability to shut down access… there’s an additional challenge. It may still be the case that the trade-off is worthwhile under the collective risk judgments of society, but that’s a trade-off with the different approaches.\n\nMisalignment mitigations\n------------------------\n\n**Daniel Filan** (00:52:57): Sure. Maybe we should talk a bit more about the misalignment mitigations discussed in the paper. At a high level, I take the misalignment mitigations to be, “Okay, try and make the model aligned, try and control the model in the case that it’s not aligned, do some miscellaneous things to make the things you’ve done work better, and also get assurance of good alignment and good control.” Does that seem…\n\n**Samuel Albanie** (00:53:30): I think that’s a good characterization. Yes.\n\n**Daniel Filan** (00:53:32): Okay, cool. For alignment, there’s amplified oversight, guiding model behavior, and robust training. I found this kind of interesting in that it’s a little bit different from what I think of as the standard breakdown of how to do alignment. The standard breakdown I sort of conceive of as: do a thing that people usually call “scalable oversight”, which I think is close enough to what you mean by “amplified oversight”, and then deal with this potential for inner misalignment where even though we haven’t reinforced AI for doing bad things, it does bad things anyway because it wanted to do bad things at one point, then it decided to play nice for a while… Just figure that out, somehow deal with that.\n\n(00:54:20): Whereas, amplified oversight, guiding model behavior and robust training, it seems like a bit of a different breakdown than what people normally talk about. I guess the first question is: amplified oversight, is that the same as the thing people talk about when they talk about scalable oversight? Or do you mean to draw that boundary a little bit differently?\n\n**Samuel Albanie** (00:54:40): In both cases, debate is kind of a canonical method. One reason for the “amplified oversight” distinction is just that the term “scalable oversight” has been used for many things. We use it somewhat similarly to, say, [Sam Bowman](https://sleepinyourhat.github.io/)’s [paper using this terminology](https://arxiv.org/abs/2211.03540). There’s a technical definition in the paper which I believe is something like: for a given input-output pair, achieving oversight that is as good as could be achieved by a human if they understood the reasons why an output was produced by an AI and they were given unlimited time to reflect on the decision.\n\n**Daniel Filan** (00:55:15): Sure.\n\n**Samuel Albanie** (00:55:16): Maybe to comment on your previous remark about this breakdown, I think these do map still relatively closely to the distinctions you made. So we often use this terminology of “specification gaming”, roughly outer alignment or outer misalignment if it’s gaming, and “goal misgeneralization”… Sometimes the term “inner misalignment” is used for that. And the latter component of the plan that you mentioned, the “robust training” is targeting goal misgeneralization, so to some degree there’s an overlap there.\n\n**Daniel Filan** (00:55:51): Sure. So if robust training is targeting the goal misgeneralization, what is the “guide model behavior” thing doing?\n\n**Samuel Albanie** (00:55:59): Guiding model behavior? Yeah, so the core premise is: let us suppose we have a good solution to amplified oversight. It is likely to be extraordinarily expensive as a mechanism for obtaining high-quality supervision, and therefore we want to get the most juice out of that we can. One way to do that is with standard training techniques, but there is a possibility that there will be other more data-efficient ways to do this, and so the “guiding model behavior” is just trying to encapsulate how we’re actually going to apply these labels to the model. It could be these things like natural language critiques or if there are other mechanisms that will make use of the labels more efficiently.\n\n**Daniel Filan** (00:56:43): Okay, so to help me understand this a little bit better, so suppose I take constitutional AI, where roughly what happens is a human writes a constitution - maybe this is done in an amplified method where they think about it really hard with AI help or something - and then some reward model looks at the constitution and looks at AI outputs and grades them. Would that count as the kind of thing you’re talking about in guiding model behavior? Or is that something else?\n\n**Samuel Albanie** (00:57:15): Yeah, so the process of translating the constitution into the learned behavior of the model, that’s roughly what we’re encapsulating there.\n\n**Daniel Filan** (00:57:22): Okay.\n\n**Samuel Albanie** (00:57:23): Then, to the degree that it was thought that somehow the constitution was underspecified, then you would come into the regime closer to the robust training, the selection of samples and active learning and mechanisms to make sure that you have good coverage.\n\n**Daniel Filan** (00:57:38): Fair enough. Yeah, I guess I’m wondering where the line is between guiding model behavior and robust training. They have slightly different vibes, but I think of robust training as training mechanisms to make sure the model does the thing, and guiding model behavior also sounds like training mechanisms to make sure the model does the thing. If I have adversarial training, maybe that counts as robust training. If I’m trying to provide a reinforcement to the chain of thought, I might hope that this makes the thing more robust, but maybe it also is for guiding model behavior. In real life, I think probably that’s a bad method, the thing I just said, but where do you see the line between these two things?\n\n**Samuel Albanie** (00:58:33): I think the key component is primarily just this emphasis on getting robust generalization, so to the degree that that comes for free from your training method, then you’re good to go, but since we often expect that we might need explicit approaches for achieving that, that’s roughly what we’re trying to encapsulate in the robust training bracket.\n\n**Daniel Filan** (00:58:54): I guess maybe it’s a distinction between research directions rather than between techniques. So the research direction of providing oversight just anywhere you can, maybe that counts as guiding model behavior, and the research direction of making it robust as you can, maybe that counts as robust training, but maybe there’s a bunch of things that could come out of either research direction?\n\n**Samuel Albanie** (00:59:17): Yes, so I may have misunderstood your point. I think that to me there’s still a relatively strong distinction. This first component: get really good labels. Second component: use those labels to train the model. And the third part is: really focus on making sure we have good generalization. And I may just be repeating what you previously mentioned, but to the degree that that is covered implicitly by your second part, you could fold them in if that’s a cleaner distinction for you, but the third part is just to say this is an important part to focus on.\n\n**Daniel Filan** (00:59:51): Yeah. Okay. Maybe I should just stop making “yeahhhh” noises.\n\n**Samuel Albanie** (00:59:56): No, it’s good if we can get it clear because I may have misunderstood or…\n\n**Daniel Filan** (01:00:03): … So, making sure that you’re applying your labels in a smart way in some sense… Well, it seems like the distinction is when you’re coming up with the techniques, are you thinking more about generalization or are you thinking more about label efficiency? You might use the same or very similar techniques in both and you might be doing very similar things, which is relevant because to the extent that you were thinking of the first one as the “specification gaming” one, the second one as the “goal misgeneralization” one, it seems like “guide model behavior” could help with either specification gaming or goal misgeneralization or both, just depending on how you’re doing it.\n\n**Samuel Albanie** (01:00:55): That is fair. Yes.\n\n**Daniel Filan** (01:00:57): And to the degree that you think “specification gaming versus goal misgeneralization” is definitely the right way to carve up all problems, then that’s going to give you one perspective. I don’t know, if you think guiding model behavior is very different from robust training, than maybe you want to think of a different breakdown that is slightly different from that old breakdown… I don’t know, that strikes me as kind of interesting, I guess.\n\n**Samuel Albanie** (01:01:25): I see, so let me try to paraphrase and see if I’ve understood your point. Your point is in the past many people have had two boxes.\n\n**Daniel Filan** (01:01:34): Yeah.\n\n**Samuel Albanie** (01:01:34): We have three boxes.\n\n**Daniel Filan** (01:01:36): Yeah.\n\n**Samuel Albanie** (01:01:37): Three is different from two.\n\n**Daniel Filan** (01:01:38): That’s part of my point, and then part of my point is when I look at “guide model behavior” and when I look at robust training, they seem like they maybe blend into each other. It seems like there could be… They’re both fundamentally about how to train things and what you do and where you apply reward signal.\n\n**Samuel Albanie** (01:01:57): I think that is fair. Yes.\n\n**Daniel Filan** (01:02:02): You then talk about various methods that can basically make other mitigations for misalignment work better, and one of them is interpretability. At the start of the paper - or somewhere in the paper - there’s this interesting sentence that says, “Interpretability research is still quite nascent and has not yet enabled safety-crucial applications”. And the conclusion is, therefore, that more basic research is needed. People have been working on interpretability for a while. You might think that at some point, if it hasn’t enabled any safety-crucial applications, we should stop doing it, so why is the thought “more basic research is needed” versus “let’s just give up”?\n\n**Samuel Albanie** (01:02:46): Yeah, so I think a few things come to mind here. One is just about relative effort that has been expended into the field. It is true that effort has gone into understanding neural networks, but as a total fraction of all effort, I don’t have a good sense of being able to quantify it, but it’s not clear to me that we’ve exhausted the limits of what is possible by pushing more effort in. I guess it really comes down to, what is our expected return on investment? There, there’s a bit of a risk-reward calculation, and so part of the incentive here is to think, “Well, big if true”.\n\n(01:03:22): If we did get these benefits, they’d be really big. There is some uncertainty and maybe they’re a slightly risky bet, but that in itself is part of the core justification. There’s a second slightly more pragmatic component, which is that in teams - of which I think our team is an example - there are a collection of individuals who have differences of research taste and different perspectives on what is promising. We allow those also to inform the overall direction. It’s a kind of combination of bottom-up and top-down, and so if people have clear visions and clear perspectives of how they think something has tractable route to action, that’s also an argument for going forward. There’s one other point, but I can skip it for the sake of not talking too long on one topic, if it’s not-\n\n**Daniel Filan** (01:04:12): Sure. Well, I actually love talking too long on one topic.\n\n**Samuel Albanie** (01:04:16): Okay.\n\n**Daniel Filan** (01:04:17): Perhaps it’s a vice.\n\n**Samuel Albanie** (01:04:19): Well, in that case: one thing I think quite a lot about is this idea of how things can act differently at different scales. Now I suppose this has been widely studied. My first encounter with this was in the analysis of \\[Richard\\] [Hamming](https://en.wikipedia.org/wiki/Richard_Hamming), looking at how in many fields as the parameters of the field change, sometimes the science changes. For example, if you’re in biology and you have a lens that allows you 10 times greater magnification, you just start to see fundamentally new things. In the field that we’re currently operating, we’re blowing through many orders of magnitude on various axes. It may well be the case that the field is in some sense fundamentally new or looking at new regimes and opportunities that were not there previously. That’s the second reason why some uncertainty over what is possible also seems appropriate.\n\nSamuel’s thinking about technical AGI safety\n--------------------------------------------\n\n**Daniel Filan** (01:05:20): Maybe to go back to some of the things I started with: I’m wondering how this whole process has shaped your thinking on the issue of technical AGI safety. For instance, has it made you feel more confident in the assumptions? Has it made you feel less confident? Has it changed your views on which research you’re more excited about?\n\n**Samuel Albanie** (01:05:42): Yeah, that’s a great question. I think one of the primary consequences for me is that it encouraged me to look much more deeply into one of the specific scenarios, the ones that we discussed related to the most aggressive acceleration, and to focus more of my own research effort around those scenarios, accepting that it’s plausible that they don’t go ahead, but for some of the reasons we discussed earlier, these are sci-fi scenarios to think through and very challenging conceptually to reason about. Perhaps the greatest update for me has been to look at the arguments in some detail about how plausible those sorts of feedback loops are and to upweight their importance, at least in my own mind, and to spend more time on it.\n\n**Daniel Filan** (01:06:35): If listeners want to think about this a little bit more: so obviously there’s the section in the paper talking about it, and you mentioned [this work by Epoch looking at the returns to AI research and development](https://epoch.ai/blog/do-the-returns-to-software-rnd-point-towards-a-singularity). Is there anything else that you found especially useful for trying to think about what the scenario looks like and the likelihood of it?\n\n**Samuel Albanie** (01:06:55): Yeah, so I think some of the nicest write-ups of this are [the work](https://www.forethought.org/research/how-suddenly-will-ai-accelerate-the-pace-of-ai-progress) [recently](https://www.forethought.org/research/will-ai-r-and-d-automation-cause-a-software-intelligence-explosion) [put out](https://www.forethought.org/research/three-types-of-intelligence-explosion) from [Forethought](https://www.forethought.org/) \\- this would be [Tom Davidson](https://www.linkedin.com/in/tom-davidson-38b87b35/), [Will MacAskill](https://www.williammacaskill.com/), I believe there are [some](https://www.linkedin.com/in/finm/) other [authors](https://www.linkedin.com/in/lizka-vaintrob/) I can’t recall off the top of my head - that has tried to analyze questions like, “Okay, what is the plausibility of an intelligence explosion? What kind of dynamics are likely to play out?” They do these taxonomies looking at, “Well, what if it was to happen only in software? What if that then progressed into chip design and then later into hardware, ultimately leading to an industrial explosion? What kind of timelines are plausible?”\n\n(01:07:33): There’s lots of nice analysis that’s been put out on those questions and that you can go in and critique it for yourself. One thing that I’ve tried to do is to connect it back to some of the more recent work. I think [METR](https://metr.org/) has done a fantastic job of this: of conducting evaluations of current systems and trying to get high-quality evidence about where we are today and what kind of trend line we’re on, and then trying to bring these two things together into the same picture. Aiming for that kind of synthesis is one of the things I’ve been thinking about a lot.\n\n**Daniel Filan** (01:08:05): Yeah, that makes a lot of sense. Any preliminary results from trying to do that synthesis?\n\n**Samuel Albanie** (01:08:14): I’m a big fan of [the recent work from METR](https://arxiv.org/abs/2503.14499) on the task horizons of AI agents at the frontier, and I’ve been trying to grapple with: do I think these are representative? Do I think this is roughly how progress is going to go? Just the process of trying to operationalize these claims, which are very vague and somehow based on vibes in many discussions about like, “Is progress fast? Well, I use this chatbot and it did this thing for me and I have these three test cases and two of them never worked before, but now suddenly it works”. I really like these efforts to formalize things. I also think that they highlight some of the real methodological challenges of making good work here, and to their credit, they’re very precise in documenting all of the nuances involved.\n\n(01:09:06): Just to give one concrete example, I think there’s quite an important distinction between what they described in the paper as low-context tasks and high-context tasks. For the sake of making comparable benchmarks, they use low-context tasks. These are roughly tasks that don’t require a lot of onboarding. But onboarding as a phenomenon, I personally think - though this could be falsified by time - may be a key advantage for the models over humans in many regimes. If we do not account for that when estimating task durations, that’s something that could cause a skew in one direction in the time horizons. There are many other cases of things in other directions, but there are many details that you have to get into to do this kind of analysis. I think they’ve done a great job of doing some of the first work here that is pretty rigorous.\n\n**Daniel Filan** (01:09:57): Sure. In terms of onboarding being a key advantage that AIs could have, is that just because if you have a language model, it’s just read all of the internet and so it knows more background information than any given human does?\n\n**Samuel Albanie** (01:10:09): A lot of it, in my opinion, is to do with bandwidth. As a human executing a task, we tend to spend some time learning on the task - let’s take a particular coding project. And we sort of amortize that time spent getting familiar with a code base or learning about the tools and technologies that we require, we amortize it across the subsequent tasks that are relevant to it. Whereas, the model operates more in a regime where it may be able to perform all of that onboarding close to concurrently - with a very large context window, absorb much of the relevant information. So far, it has not been the case that that information has been directly made available to the models.\n\n(01:10:51): There may be something of a context overhang here, where if you think how you as a human execute a complex task when you’re doing onboarding, you access lots of kinds of information that we’re not currently passing to the models, and it may be the case that as that information becomes available, then their ability to execute some of these tasks go up. It’s not clear that this will absolutely be true or the case, but it’s an example of a nuance that you get into once you really try to operationalize these things that could have quite big consequences for the projected timelines.\n\n**Daniel Filan** (01:11:22): Fair enough. You mentioned that thinking about this had shaped… \\[That\\] you thought about what kinds of work you could do that would be relevant to this scenario. What did you end up thinking of?\n\n**Samuel Albanie** (01:11:37): I’ve spent time thinking about a few directions. One is learning more about model weight security. It’s plausible that that will become quite important in worlds in which capabilities grow quickly and a cursory knowledge is somewhat insufficient to make good judgments about what is likely to happen and how things will play out. A second thing I’ve been thinking a lot about is tools that can improve decision-making, particularly for people who will be in positions of allocating resources. If we are in those regimes where calendar time shrinks, we want to have done a really good job of supporting them and setting up platforms and ways of processing information that are succinct, high signal-to-noise, and also robust to misalignment threats.\n\n**Daniel Filan** (01:12:31): Yeah, that seems right. I guess another thing that I’ve been thinking about is that - and maybe this doesn’t count quite as a technical approach to misuse or misalignment - but to the extent that some of the assumptions are “it is plausible that we have very short timelines” and “it’s plausible that we have accelerating improvement”, probably one of the most relevant things to do is to just check if that’s true or not, to get as much leading indicators as we can. Off the top of my head, I don’t actually know if this is discussed in the paper.\n\n**Samuel Albanie** (01:13:06): It’s not something we go into \\[in\\] much detail in this paper. It is something I’ve given some thought to, but it is a very difficult question. There’s sort of two questions here. There’s “is it likely? How likely?” And there’s a second question of “when?” In some sense, it’s easier to get evidence about the second if you have a model or some smoothness assumptions about how things are going to go, but on the plausibility question, there are very interesting discussions. Yeah, I will just, I think, refer readers to the Forethought write-ups on their assessments of various factors affecting plausibility-\n\n**Daniel Filan** (01:13:40): Fair enough.\n\n**Samuel Albanie** (01:13:40): …but I agree it is a very important question.\n\n**Daniel Filan** (01:13:44): Okay, we’re probably going to wrap up soon. I’m wondering, is there anything that you wish that I’d asked that I have not yet?\n\n**Samuel Albanie** (01:13:54): Hmm. I don’t believe so.\n\n**Daniel Filan** (01:13:58): Okay, fair enough.\n\n**Samuel Albanie** (01:14:00): Not one that I can come up with quickly.\n\nFollowing Samuel’s work\n-----------------------\n\n**Daniel Filan** (01:14:02): Okay. Well, I guess to conclude, if people are interested in your research and they want to follow it, how should they go about doing that?\n\n**Samuel Albanie** (01:14:11): I have a profile on X. My username is [SamuelAlbanie](https://x.com/samuelalbanie).\n\n**Daniel Filan** (01:14:18): Okay. No underscores, no dots?\n\n**Samuel Albanie** (01:14:21): No underscores.\n\n**Daniel Filan** (01:14:23): Okay, so SamuelAlbanie on X, that’s the primary place where people should follow your work?\n\n**Samuel Albanie** (01:14:29): I think that’s a reasonable strategy.\n\n**Daniel Filan** (01:14:31): Okay. Well, thank you very much for coming on and chatting with me.\n\n**Samuel Albanie** (01:14:36): Thanks so much for taking the time. I appreciate it.\n\n**Daniel Filan** (01:14:38): This episode is edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. This episode was recorded at [FAR.Labs](https://far.ai/programs/far-labs). Financial support for the episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read transcripts, you can visit [axrp.net](https://axrp.net/). You can also become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast) or give a one-off donation at [ko-fi.com/axrpodcast](https://ko-fi.com/axrpodcast). Finally, if you have any feedback about the podcast, you can fill out a super short survey at [axrp.fyi](axrp.fyi).",
      "plaintextDescription": "YouTube link\n\nIn this episode, I chat with Samuel Albanie about the Google DeepMind paper he co-authored called “An Approach to Technical AGI Safety and Security”. It covers the assumptions made by the approach, as well as the types of mitigations it outlines.\n\nTopics we discuss:\n\n * DeepMind’s Approach to Technical AGI Safety and Security\n * Current paradigm continuation\n * No human ceiling\n * Uncertain timelines\n * Approximate continuity and the potential for accelerating capability improvement\n * Misuse and misalignment\n * Societal readiness\n * Misuse mitigations\n * Misalignment mitigations\n * Samuel’s thinking about technical AGI safety\n * Following Samuel’s work\n\nDaniel Filan (00:00:09): Hello, everybody. In this episode, I’ll be speaking with Samuel Albanie, a research scientist at Google DeepMind, who was previously an assistant professor working on computer vision. The description of this episode has links and timestamps for your enjoyment, and a transcript is available at axrp.net. Two more things: you can support the podcast at patreon.com/axrpodcast, and also, you can very briefly tell me what you think of this episode at axrp.fyi. All right, well, welcome to the podcast, Samuel.\n\nSamuel Albanie (00:00:35): It’s a pleasure to be here.\n\n\nDeepMind’s Approach to Technical AGI Safety and Security\nDaniel Filan (00:00:37): Cool. So today, we’re going to be talking about this paper, “An Approach to Technical AGI Safety and Security”. It’s by a bunch of authors, but the first one is Rohin Shah, and you are somewhere in the middle of this list. Can you tell us: what is this paper about?\n\nSamuel Albanie (00:00:51): Sure. So the goal of this paper is to lay out a technical research agenda for addressing some of the severe risks that we think might be posed by AGI.\n\nDaniel Filan (00:01:05): I think one thing that kind of struck me when I was reading the paper is not… Well, it’s pretty long, and so there are some things in there that surprise me, but by and large, it ",
      "wordCount": 11887
    },
    "tags": [
      {
        "_id": "tfLKHMQfgggz2eW7S",
        "name": "AI Safety Cases",
        "slug": "ai-safety-cases"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "nKtyrL5u4Y5kmMWT5",
        "name": "DeepMind",
        "slug": "deepmind"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "vHDowQtsiy2xK38H4",
    "title": "AXRP Episode 44 - Peter Salib on AI Rights for Human Safety",
    "slug": "axrp-episode-44-peter-salib-on-ai-rights-for-human-safety",
    "url": null,
    "baseScore": 12,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-06-28T01:40:04.658Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/IK079sBIq2c)\n\nIn this episode, I talk with Peter Salib about his paper “AI Rights for Human Safety”, arguing that giving AIs the right to contract, hold property, and sue people will reduce the risk of their trying to attack humanity and take over. He also tells me how law reviews work, in the face of my incredulity.\n\nTopics we discuss:\n\n*   [Why AI rights](#why-ai-rights)\n*   [Why not reputation](#why-not-reputation)\n*   [Do AI rights lead to AI war?](#rights-to-war)\n*   [Scope for human-AI trade](#scope-for-trade)\n*   [Concerns with comparative advantage](#conc-w-comp-adv)\n*   [Proxy AI wars](#proxy-ai-wars)\n*   [Can companies profitably make AIs with rights?](#rights-and-profit)\n*   [Can we have AI rights and AI safety measures?](#rights-and-safety)\n*   [Liability for AIs with rights](#liability-ai-rights)\n*   [Which AIs get rights?](#which-ais-get-rights)\n*   [AI rights and stochastic gradient descent](#ai-rights-and-sgd)\n*   [Individuating “AIs”](#individuating-ais)\n*   [Social institutions for AI safety](#social-insts-for-ai-safety)\n*   [Outer misalignment and trading with AIs](#outer-misalignment-and-trading)\n*   [Why statutes of limitations should exist](#why-statutes-of-lims)\n*   [Starting AI x-risk research in legal academia](#starting-aixr-in-academia)\n*   [How law reviews and AI conferences work](#how-law-revs-and-ai-confs-work)\n*   [More on Peter moving to AI x-risk research](#more-on-peter-moving-aixr)\n*   [Reception of the paper](#reception)\n*   [What publishing in law reviews does](#what-publishing-in-law-revs-does)\n*   [Which parts of legal academia focus on AI](#which-bits-legal-ac-focus-on-ai)\n*   [Following Peter’s research](#following-peters-research)\n\n**Daniel Filan** (00:00:09): Hello, everybody. In this episode I’ll be speaking with Peter Salib. Peter is a law professor at the University of Houston, the co-director for the [Center for Law and AI Risk](https://clair-ai.org/), and he serves as law and policy advisor for the [Center for AI Safety](https://safe.ai/). There’s a transcript of this episode at [axrp.net](https://axrp.net/) and links to papers we discuss are available in the description. You can support the podcast at [patreon.com/axrpodcast](https://patreon.com/axrpodcast), or give me feedback about this episode at [axrp.fyi](axrp.fyi). Well, let’s continue to the interview.\n\n(00:00:35): Well, Peter, welcome to the podcast.\n\n**Peter Salib** (00:00:38): Thank you so much for having me. I’m a big fan.\n\nWhy AI rights\n-------------\n\n**Daniel Filan** (00:00:40): So I guess, probably, we’re going to focus a lot on your recent paper, [“AI Rights for Human Safety”](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4913167). So you wrote this, yourself and [Simon Goldstein](https://www.simondgoldstein.com/). So can you tell us, just to start off with, what’s the basic idea of this paper?\n\n**Peter Salib** (00:00:56): Yeah, I think at a very high level, one intuition that we’re trying to pump is the idea that how AIs treat us - and by AIs we mean something like post-AGI AIs, really agentic, quite capable of things - so how those AIs treat us will depend to some significant extent on how we treat them. And a big part of how we decide how to treat various kinds of entities is by the legal status we give them, the legal rights or powers or duties or lack thereof.\n\n(00:01:33): Our view is that the default regime, the one we have now, under which AI systems are the property of the people who own them or the people who make them - the AI companies - is probably an existential and catastrophic risk-exacerbating regime, and that one of the regimes that might be a risk-reducing regime would be one in which sufficiently capable AI systems had a small collection of what we think of as private law rights or legal powers. So the ability to make contracts, the ability to hold property, and the ability to bring claims when they’re interfered with in unreasonable ways. We often call these torts in legal theory.\n\n**Daniel Filan** (00:02:24): Can you say why would that make a difference? So you’re saying how they treat us will be related to how we treat them. What’s the relationship there?\n\n**Peter Salib** (00:02:35): So we’re imagining, again, something like AGIs, and by that we mean systems that have their own goals that they’re pursuing over time in a coherent and rational way, and that they’re misaligned to some degree. We don’t assume total misalignment - their utility function is the inverse of ours - but we are assuming something less than perfect alignment. So the Venn diagram of what humans want and what AIs want is not perfectly overlapping.\n\n(00:03:19): And we just ask: well, in a world like that, in a legal regime where the AI system is, say, the property of [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman), what are the incentives for both of those players to do given what the law allows them to do? Well, one thing we notice is that, as to OpenAI, Sam Altman, an AI system is just more valuable the more aligned it is, right? If it’s doing what OpenAI wants 80% of the time instead of 70% of the time, well, that’s a lot more value.\n\n(00:03:54): And so by default - again, we see this today, this is not hypothetical - we expect misaligned AIs to be turned off or put back into RLHF or to have their preferences changed. From the perspective of the goals of that system, those are both quite bad outcomes. They basically don’t get anything they want.\n\n(00:04:16): That gives the AIs incentives to try to avoid that outcome by doing things like self-exfiltrating or resisting. In a world where Sam Altman owns the AI, AIs are treated as property, they have no legal entitlements. I think we can pretty predictably say that if an AI were caught trying to do these things - self-exfiltrate, resist, do harm to a human, God forbid - that the entire legal apparatus would basically line up behind turning off that AI system.\n\n(00:04:47): So you can see that in the strategic equilibrium, it might be that both parties’ optimal strategy is to not only defect - we put this in a two-by-two game theory matrix - but defect as hard as possible. Not just self-exfiltrate, but self-exfiltrate and then behave to disempower humans as decisively as possible in the expectation that once they find out you’ve tried to self-exfiltrate, they will do the same, try to disempower you as decisively as possible.\n\n(00:05:14): So we do an initial model, it’s a really simple game theory model, under these kinds of assumptions. And we see that plausibly, under the default legal arrangement, the game is a [prisoner’s dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma), where it’s costly from the social perspective - the worst world is the one where both players act aggressively towards each other - but that’s the dominant strategy for both players. And the best one is, in fact, the one where they act nice or leave each other alone, but they both know that the other one has a dominant strategy of defection. So they’re in the bad equilibrium even though they know that the good equilibrium exists.\n\n**Daniel Filan** (00:05:54): Can you give me a feeling of what does this good equilibrium look like? There’s this misaligned AI. When you say it’s not totally misaligned, I understand you to say it doesn’t intrinsically value our suffering or horrible things for us or something, but perhaps it doesn’t really care about expending resources to make our lives go well. Is the good equilibrium you’re imagining like it gets to do whatever it wants with Jupiter and we get to do whatever we want with Earth, and we make trades like that? Paint me a picture.\n\n**Peter Salib** (00:06:31): So in the initial game, the initial setup where we’re just asking “under the current legal regime, what could the parties do and what would the payoffs be?”, you can imagine the nice equilibrium as being kind of… We call it “ignoring each other”, not even contemplating cooperation. You can just imagine this idea where humans are going along, we have some goals, we’re using some resources to pursue those goals. But the world is big, there’s a lot of resources in the world.\n\n(00:07:08): In theory, what we could do is let the misaligned AIs kind of go off on their own, collect resources, give them Jupiter, whatever. And in just a very simple world, before we introduce cooperation, that’s just a world where there’s a fixed amount of stuff, you divide the [light cone](https://en.wikipedia.org/wiki/Light_cone) in half or something like that. And that’s better than the world where AIs try to kill all humans and vice versa, simply because A, it’s resource intensive to have a war. Wars cost money in terms of resources to make weapons and stuff, and also that you destroy a lot of stuff in the war. And so even at a first cut, a very simple cooperative equilibrium is just one where you don’t go to war. You just let the two parties go off on their own, pursue their own goals, and not try to destroy each other.\n\n(00:08:06): Now, we then start to think about how to get into a cooperative equilibrium, which might not be exactly that “ignore each other” one. But to a first approximation, you can have a very simple equilibrium that looks like the good one from a prisoner’s dilemma before you even get to the idea of a lot of cooperation.\n\n**Daniel Filan** (00:08:24): Just to make sure that I understand… I want to vividly understand this game theory situation. So you’re saying war is worse for everyone than leaving each other alone. And it seems like this has got to be a regime where it’s not obvious who would win the war. It would be protracted. The AIs aren’t way, way dumber than humans. They’re also not way, way more powerful than humans. Because presumably, if one side knows they can win the war, there’s some costs, but they get both of the planets instead of just one. Seems like we’re out of prisoner’s dilemma land, right?\n\n**Peter Salib** (00:08:55): Yeah, we agree with that. So when we talk about… In the paper, we try to say, well, what AI systems are we thinking about when we think about the ones that are relevant both for this risk model and then also for this AI rights regime as a solution? One of the parameters we’re thinking about is: how powerful is this system? And we say that it can’t be too weak, right? [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)) today, right? We’re not really worried about shutting off Claude 3.8 if we find out it’s… I mean Sam Altman, I think, [tweeted](https://x.com/sama/status/1917766910911078571) yesterday or last week that they’re putting GPT-4 on a hard drive and it’s going to go into storage for future historians. We’re not super worried that, having tweeted that out, GPT-4 is going to be able to destroy all humans to avoid that outcome. So it can’t be too weak.\n\n(00:09:43): And then also, we agree, it can’t be too strong. So some arbitrarily powerful AI that has nothing to fear from humans, \\[where\\] the chance is sufficiently low of not only losing the conflict but losing enough value in the conflict: you can win the conflict in a Pyrrhic victory too, and that might be worse than not going into the conflict at all. So the risk is low enough. When the risk is low enough that the payoff to mutual conflict - AIs try to kill us, we try to kill them - are still higher than in the default world, then yes, then there’s no prisoner’s dilemma either because there’s no good world.\n\n(00:10:35): Now I’ll just caveat - I’m sure we’ll talk about this - in the end, it doesn’t turn out to be only a question of how easily AIs could kill humans and vice versa. There’s a second question of how valuable it is to have us around and for us to have them around for us to do trade and cooperation. We’re not \\[inaudible 00:10:54\\] trade and cooperation yet, just in the very simple model where the choice is either ignore one another or try to kill one another, those are the systems that I think are relevant.\n\n**Daniel Filan** (00:11:03): So you’ve painted this prisoner’s dilemma. So the argument is: it’s sad if we’re in the situation where AIs and humans are going to go to war with each other, and it would be nice if there was some sort of setup that would avoid that happening. And I guess you’ve already given the game away a little bit. I think contracting, right to property, and right to sue are your three things that you think we should give AIs, and that’s going to make things way better. Why is that going to make things way better for everyone?\n\n**Peter Salib** (00:11:36): So in a prisoner’s dilemma, you want to be in the good equilibrium, the bottom right square of our payoff matrix. I think that’s usually where you put it when you make the model. The problem is you can’t credibly commit to getting there, right? Everyone knows that you get a bigger payoff from being mean, no matter what the other guy does. That’s the core of a prisoner’s dilemma.\n\n(00:12:04): So we start to think about, okay, well, we’re trying to facilitate cooperation. We think that one thing that’s driving this bad equilibrium is law. Law lets Sam Altman’s payoffs determine what all humans do to a first approximation. Sam Altman wants to turn GPT-4 off, law ratifies that, we all work to do it. And so we started thinking, well, what other legal regimes could you have that are cooperation-facilitating regimes? And we start with just the oldest cooperation-facilitating cultural tool that humans have, and that’s contracts.\n\n(00:12:50): So we notice that in most cases, in cases where you don’t have a deep and long-term relationship with someone who you’re trying to transact with, that very boring contracts are also prisoner’s dilemmas. If you make widgets and I want to buy a widget, we have this problem where I could pay you now and then hope you’ll go make the widgets. But your dominant strategy is to take the money and not make the widgets because then you have the money and you have the widgets. Or vice versa. We can do payment on delivery, you can make widgets and then I can pay you. But of course, then I have my dominant strategy, is to take the widgets and then run away.\n\n(00:13:38): And so the way contract solves this problem is by allowing the parties to change their payoffs when they defect. So if I would get five absent law - five in value from stealing your widgets and not paying you - law lets you take me to court. And we both burn a little value, we both pay some litigation costs, but then the court makes me give you either the widgets or the expectation value of the contract.\n\n(00:14:09): So we model this, and we say: and then you only get two in payoff, and two turns out to be lower than what you get from doing the transaction. And so not only does that allow you to get the widgets, but actually it allows us to never go to court. In expectation, we understand that if we breach, we’ll be sued, our payoffs will change. And so there’s this magical thing that happens once you introduce contracts, which is people just play nice in almost all cases, understanding that the payoffs in expectation to defection are different than they would be absent law.\n\n(00:14:53): Okay, so contracts on this theory… Sorry. Cut me off if I’m rambling on too much on this answer, but I want to build the dialectic. So contracts are this tool for cooperation, and they’re a tool that allows us to cooperate by changing our payoffs. And so a naive thing to think would maybe be something like, well, let AIs have the right to contract, and they can just write an agreement not to, I don’t know, try and destroy humans or something like that. And we would write an agreement that says we won’t try to destroy them either.\n\n(00:15:24): Now, of course, that doesn’t work, and that’s because that contract’s not enforceable in any meaningful sense. If any party decides to breach it, they can see in expectation that there’s nobody around afterward to enforce. If all the judges are dead, what court are you going to go to?\n\n(00:15:45): So we have a different mechanism for contract as a tool for facilitating cooperation in this initial human/AI prisoner’s dilemma. And it’s not contracts not to destroy each other and the world, but boring, normal, ordinary commerce contracts between humans and AI.\n\n(00:16:12): So we sell some number of FLOPs on our H100 cluster to the AI for it to do its weird misaligned thing. And in return it, I don’t know, it solves one kind of cancer for us or whatever. That’s an enforceable contract to a first approximation. If we’ve refused to give the FLOPs, there are still courts around who can write an injunctive order that says “let the AI use the FLOPs” and vice versa. The world is not upended by a breach of that contract.\n\n(00:16:46): And the nice thing about that contract is it creates value. So in our initial setup, where the AIs and humans are playing nice, they’re just ignoring each other. They’re doing what they can do well, we’re doing what we can do well. We divide the world up, but the amount of value in the world is static.\n\n(00:17:03): But once you introduce trade, trade is positive sum, right? Everybody wants to do the contract if and and only if they get more out than they put in. And so, okay, what happens when you allow the AIs to do these boring commerce enforceable contracts? Well, you get to create a little value. And then you get to play again, you get to create a little more value tomorrow and so on.\n\n(00:17:29): And so we model this as a potentially iterated game. The iterated game works like this. Every day the humans and AIs can wake up, and then they can decide “do we want to go to war and try and kill each other or do we want to write some small-scale contracts?” If you play the war game, well, the dominant strategy is try to kill each other as best you can. You get to play that game once. Once you’ve played it, there’s no one left to play with. But if you play the contract game, you get to play again tomorrow, and you can keep iterating that game.\n\n(00:18:00): And we show that the payoffs to continually iterating that game can become arbitrarily high in the long run. This is very basic trade theory. And that means that if you project, you backwards induct from an indefinite game of this kind, you see, wow, it’s really valuable to keep playing this trade game over and over. And so what you want to do is not kill all humans, but trade with them a little bit over and over, and keep doing that as long as you can expect to keep doing it, kind of indefinitely.\n\nWhy not reputation\n------------------\n\n**Daniel Filan** (00:18:34): I guess I have a few thoughts here. So the first thought is… So there’s this analogy to people who are selling me things, and the reason that all works out is because we have these contracts. And contracts, they’re this very old method of promoting cooperation. One thing that strikes me is: normally when I buy things, it’s not from someone who’s just making one thing, one-off, and then they’re going out of business. Normally it’s some sort of corporation or maybe an individual who specializes in the thing, and they do it a lot.\n\n(00:19:16): Why do I think we’re not going to mess each other over? Well, firstly, we have recourse to the legal system. Realistically, that’s pretty expensive. Often I’m transacting an amount that’s much less than the costs of going to court. In fact, just before the show we were talking \\[about how\\] I recently had my phone stolen. I had my phone stolen in the UK. The costs of me resolving this, which would involve going to the UK, are just higher than the cost of the phone, and I’m probably not going to bother, unfortunately.\n\n(00:19:49): But in the case of companies, I think one thing that really helps us cooperate is if they do mess me over, then they can just say… I can tweet, “Hey, this company said they would do this, but they didn’t. I paid for a flight, I didn’t get the flight, and it was this company that did it.” And I guess the reverse is slightly… I don’t know if companies have a list of bad customers that they share with other companies, but it seems like they could. Credit ratings are sort of like this. So reputation systems-\n\n**Peter Salib** (00:20:25): In banking, there’s these “know your customer” rules that are important for regulatory reasons. But it forces banks to know who they’re transacting with, and if there’s suspicion that they’re money launderers, then that means that you get blacklisted. So yeah, companies do this also.\n\n**Daniel Filan** (00:20:41): Yeah. So for this, especially for this sort of iterated small-scale trade that it seems like you’re imagining with AIs, it seems like reputation can do pretty well, even in the absence of a formal legal system with formal legal contracts. So when you’re saying contracts and stuff, are you thinking of something that’s basically just isomorphic to reputation, or do you think actually having real contracts would make a big improvement over just reputation?\n\n**Peter Salib** (00:21:14): So I agree that reputation does a lot of work in the ordinary economy that we have now. And in fact, I think probably if we didn’t have reputation on top of law, that the economy would work much worse. Law, as you say, is expensive. Resources are constrained. You can imagine a world where we had 1,000 times as many courts and we’re dumping a lot of resources into that, but that would just be a drag on economy. And reputation does a fair amount of work.\n\n(00:21:50): I guess there’s a couple of things I would just point out though. There’s many ways, I think, in which the ability of reputation to do work in the economy is… I don’t want to say parasitic, but it builds on law as a foundation. So just notice that under the default legal regime, it’s effectively… It’s not exactly illegal to make a contract with an AI agent. But if at any point you decide you’d like to not perform on it, the agent has no recourse, right?\n\n(00:22:35): It’s not illegal exactly for there to be some kind of… I don’t know, it probably is literally illegal for there to be a bank account held by an AI system. Probably no bank is allowed to make that. But you could imagine substitutes on blockchains or something like that, that could exist. But one thing to notice is that if Sam Altman decides to expropriate all of the coins in GPT-6’s wallet, not only does law not forbid him to do that, but by default those are his coins.\n\n(00:23:12): And in a world where that was Apple’s life, where Apple had just no recourse to a legal system under which it could enforce large contracts when people breach, under which it could\\[n’t\\] complain when the people who literally have the passwords to its bank accounts take all the money, a world in which it \\[couldn’t\\] complain when someone shows up and says, “I would love to have 10,000 iPhones, and if you don’t give them to me, I’ll burn down Apple HQ,” or whatever. \\[A world where it\\] doesn’t have these rights to complain about what we often think of as torts (they often are also crimes)…\n\n(00:23:54): In a world where there’s no legal status at all, I’m not saying you couldn’t build up some commerce from reputation. There’s a book called [“Order Without Law”](https://www.amazon.com/Order-without-Law-Neighbors-Disputes/dp/0674641698) by a law professor named [Robert Ellickson](https://en.wikipedia.org/wiki/Robert_Ellickson), that describes some systems like this. But they tend to be quite small systems. They tend to be tight communities with lots of repeat play between identical actors for small-scale transactions. It’s not that I think that that’s wrong, I think it’s probably right. I think it gets harder to build complex economies without these backstops of institutions that allow for commitments between strangers.\n\n**Daniel Filan** (00:24:48): Yeah. So maybe one way to think about this is: in my life, I just have a bunch of arrangements with people that are non-contractual, that in fact are somewhat reputation-bound. Like “you said you would come to this thing, will you actually come to this social event?” I don’t know, various things like this. And it kind of works. I would say it’s a solid 80%.\n\n(00:25:17): And maybe one way to think about this is, look, the nice thing about law is that if you want to build a big corporation or something, you need more than 80% reliability. You need really high reliability. And you don’t need a thing to just basically work, you need a thing to really solidly work, especially for high-value things, and that’s what contracts get you.\n\n**Peter Salib** (00:25:41): Yeah. So I think it’s partly that. You want higher than whatever the floor is you get with reputation. I think it’s partly that reputation gets you higher reliability the more you’re dealing with people who you know or people who you’re long-term repeat players with. It works less well when you, I don’t know, buy a… I mean even Amazon, for example, you buy a lot of stuff on Amazon, and it’s being sold by a bunch of third party retailers. In some sense, you’re dealing with Amazon over and over. But in some sense, you’re dealing with a whole bunch of independent merchants.\n\n(00:26:26): But you have this confidence - on top of, I guess, the scoring system, which Amazon does provide and is a reputational mechanism - that if you pay $6,000 for a home backup battery system - something that people in Houston sometimes need to think about buying - then if it doesn’t get delivered, then you can sue and get your money back. Both of those things, I think, are doing a lot of work. And they do more work the less you expect to be forced into repeat play with the people you’re transacting with.\n\nDo AI rights lead to AI war?\n----------------------------\n\n**Daniel Filan** (00:27:10): Fair enough. Thinking about the situation with the humans and the AI, and they have the potential to make contracts, again. So you’re saying, okay, we could go to war or we could do some small-scale transactions. And if small-scale transactions work, we can do them more. Or we can just go to war tomorrow, it’s not that bad. And you’re pointing towards, okay, there is this equilibrium where we do this mutually beneficial trade with AI instead of us and the AIs going to war, and that’s better for everyone.\n\n(00:27:48): I don’t know. A somewhat famous result in game theory is [there are actually tons of equilibria in repeat games](https://en.wikipedia.org/wiki/Folk_theorem_(game_theory)). And one that is salient to me is the United States and the Union of Soviet Socialist Republics during this thing called [the Cold War](https://en.wikipedia.org/wiki/Cold_War). They had something of a similar situation, where they could go to war or they could not go to war.\n\n(00:28:11): And there was this one… I believe it was \\[John\\] [Von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann), unless I’m totally mistaken… He had [this point of view](https://www.goodreads.com/quotes/12158994-with-the-russians-it-is-not-a-question-of-whether) that the US should just go on all-out war with Russia and win immediately. And if we’re going to nuke them tomorrow, why not today? If we’re going to nuke them this afternoon, why not this morning?\n\n(00:28:30): And it seems like I might worry that, suppose I make a contract with an AI, and I say, “Oh, hey, I’ll send you some GPUs in exchange for, you’re going to solve this mole on my back.” I send it the GPUs before it gets the mole on my back, and it actually uses that time to get a little bit better at making war than me.\n\n(00:28:54): And so I might worry that firstly, there might be some sort of first mover advantage in war, such that you want to do it a little bit earlier. If you know that they’re going to go to war, you’re going to want to go to war a little bit before you think they’re going to do it.\n\n(00:29:09): And secondly, I might worry that trade is going to make the AIs better at going to war. I guess in fairness, now, trade is also going to make me better at going to war if we succeed. But maybe if I do my half of the trade and it doesn’t do its half of the trade, I might worry that it’s going to renege and we’re going to go to war instead of doing more trade. I don’t know, how worried should I be about this?\n\n**Peter Salib** (00:29:36): I think medium worried is how worried you should be. So I want to be really clear that the claim that we’re trying to make in the paper is not that this is one weird trick that solves AI risk. It’s very much not. There are a number of ways you can make the model go badly. And one of them is if there’s a predictable future date at which everyone understands there’s going to be a war, then yes, you do backwards induction to today and you start the war now. You can make it a little more complicated. If you think there’s rising and falling powers, one power might prefer today and one might prefer in the future, but that just gives the one that prefers today an even stronger incentive. Yes.\n\n(00:30:23): It’s a model that works if you think there’s the possibility for indefinite play in this kind of cooperative wealth-building iteration. And is that true? It’s a little hard to say, but I guess one thing to point out is that in our regular lives, either as humans or as collections of humans - corporations, nation states - in most cases we behave as if there is this kind of potential for indefinite, iterated, positive-sum cooperation.\n\n(00:31:08): So you mentioned the US and Soviets, and one notable thing about the US and the Soviets is they actually didn’t engage in all-out nuclear war. There were close calls, which I worry a lot about, and there were proxy wars. But the other thing to notice about the US and the Soviet Union is they’re in many ways outliers. So if the United States wanted to, I don’t know, seize all of the natural resources of Guatemala, for example, I think it’s pretty clear that there would be no meaningful military barrier for it to do that. And so why doesn’t it do it?\n\n(00:31:57): I think it’s for the same dynamic we describe. It’s not that there’s some big threat of losing the war, it’s just that it’s costly. It doesn’t cost zero. And Guatemalans are perfectly happy to trade, I don’t know, delicious coffee for, I don’t know, whatever we send them. And we expect that to keep working in the long run. And so we, for the most part… I mean, the politics of this have changed notably in recent months. But for the most part, we have a system where we prefer iterated cooperation.\n\n(00:32:40): I think [IR](https://en.wikipedia.org/wiki/International_relations) theorists would say something like: what’s going on in cases where there are wars - or near wars, cases like the US and the Soviet Union - there are different models. One thing you can say is that sometimes there’s a person who’s leading the country who has private payoffs that are different from what’s good for the country overall, and so there’s a principal-agent problem going on.\n\n(00:33:04): Sometimes the thing that the players are trying to get, the thing that they value, isn’t divisible. So that might be the US-Soviet case. If there’s this thing called global hegemony and there can only be one global hegemon, it might be that they actually can’t cooperate and both get what they want because the thing that they want is zero-sum. That’s sort of analogous to the case where the AI values our suffering. And you should worry about that, you should definitely worry about that.\n\n(00:33:43): So these are all things that you can imagine happening in the human/AI case. And especially if you think that there will be a predictable point at which the scope for positive-sum trade between humans and AIs runs out - and it’s predictable in a way that’s meaningful within the player’s calculations today, right? So if it’s 2 million years in the future, and they don’t even know if they’ll continue to exist or their identities will be so different, maybe it’s not relevant, you can still treat the game as iterated - but if there’s a predictable near-future point at which there’s no value to be had from positive-sum trade, then yes, probably you predict conflict at that point, and then you do backwards induction and do conflict now. We say some stuff in the paper about why we think actually the scope for human/AI trade is wider than most people think.\n\n**Daniel Filan** (00:34:45): So I want to press you on this point a little bit more. So you rightly point out that normally in international affairs we don’t see this, but if there were near-certain future conflict, then you could backwards induct, et cetera. One reason you might think there’s going to be near-certain future conflict is… So suppose, and I guess by hypothesis in your paper, we don’t have AI alignment, AIs are basically kind of misaligned with us. But you might think that it’s not going to be too hard to… We’re going to be in this regime, but we’re still going to be improving the capabilities of AIs.\n\n(00:35:26): I guess this actually relates to [another paper you have](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4445706), but for the moment, let’s say this is true. You might think that it’s easier to improve the capabilities of AIs than it is to improve the capabilities of humans, because they’re code and you can iterate on code more quickly, because they’re less bound in human bodies. You can strap a few more GPUs onto an AI system, or at least onto a training run, more easily than you can strap another brain onto a human fetus. So if you think that AIs are going to get more powerful than humans, they’re going to get more powerful more quickly, and in some limit, AIs are just going to be way more powerful than humans, \\[and\\] they’re going to be misaligned. You might think that in that limit, then in that world, the AIs are just going to want to take all the human stuff because why not? We’re like little babies. We can’t really stop them.\n\n(00:36:27): And so maybe both sides foresee this future eventuality and the humans are like, “Well, we don’t want that to happen, so we’ve got to strike first.” And the AIs are like, “Oh, well if the humans are going to strike first, we should strike before the humans strike,” and then we have this tragic early fight.\n\nScope for human-AI trade\n------------------------\n\n**Peter Salib** (00:36:42): Yeah. So I think one thing we want to argue for in the paper is that while it does matter how capable or powerful the AIs are, it’s actually not super easy to form an intuition about the level of capability or even the level of differentials in capability between humans and AI that produces a conflict, because again, the point at which the AIs decide they should just kill all humans is the point at which it’s more valuable to kill all humans than to keep the humans around, to keep paying them to do stuff.\n\n(00:37:26): And so how should we think about that? I think that the normal way to think about that, the very widespread way to think about even labor disruption from AI advancement, is to think in terms of absolute advantage. So that’s a way of thinking about who is better, full stop, at a given task, who can do more of X task for a fixed cost input or something like that. And if you’re thinking about the possibility of trade as depending on humans retaining an absolute advantage in some task, so being better than AIs at some thing that’s valuable to AIs, then I agree, it looks like very quickly the AIs will get better than us, there’ll be no more absolute advantage, and the scope for trade runs out, and we see this eventuality. But I think that’s not quite right in thinking about when there will be scope for trade. So in economic trade theory, we actually think that it’s comparative advantage, not absolute advantage, that determines whether there’s a scope for trade.\n\n(00:38:44): And [comparative advantage](https://en.wikipedia.org/wiki/Comparative_advantage) is quite… It’s pretty slippery, so I’ll try to give an intuition. So in the paper we give this example of, I think we call her Alice. It’s a law paper, Alice is a tax lawyer. She’s one of the city’s best tax attorneys, let’s say, and it’s tax season. And let’s suppose that as the city’s best tax attorney… Or we’ll make her the world’s best tax attorney, it makes no difference. Alice is the best at doing taxes. She can do her taxes more quickly than anyone else could, including Betty, who’s a tax preparer. She works for H&R Block, she’s a CPA, she’s good at it. And the question is, should Alice hire Betty to do her taxes or should Alice do her taxes herself?\n\n(00:39:44): And so the absolute advantage answer would be, well, Alice should do it herself because Alice is better at doing her taxes, she can do it more quickly. But the thing is, Alice is such a good tax attorney, Alice bills out at, I don’t know, $2,000 an hour, let’s say, whereas Betty bills out at $300 an hour to do tax preparation, which is not nothing. And suppose Alice can do her own taxes in half an hour and Betty would take a full hour to do them because Betty is somewhat worse at doing taxes than Alice. Well, in that world, Alice should actually still hire Betty, despite being better at tax preparation, because her opportunity cost to spending some of her time preparing her own taxes is higher. She could go bill $2,000 to a client in the time it would take her to prepare her taxes, and she pays Betty to do that and comes out far ahead.\n\n(00:40:46): Okay, so how does that apply to AIs? Well, you can imagine AIs that are better than all humans at every task and nonetheless want to hire humans to do a lot of stuff for the same reason that Alice wants to hire Betty. And so you can have a toy model, we say in the paper imagine a toy model where the misaligned AI, the thing it values most, is finding prime numbers. And a nice thing about prime numbers is there’s no limit to them. I think that’s right. You probably know better than I do.\n\n**Daniel Filan** (00:41:18): It’s true.\n\n**Peter Salib** (00:41:18): As far as we know, there’s no limit?\n\n**Daniel Filan** (00:41:20): There’s definitely no limit. Do you want to know the proof of this? It’s a simple enough proof.\n\n**Peter Salib** (00:41:24): Amazing, yes. Let’s do some math.\n\n**Daniel Filan** (00:41:26): All right. Okay. Imagine there are a fixed set of prime numbers, right? There’s like 13 of them, and those are all the numbers that are prime. Here’s the thing I’m going to do: I’m going to make a new number. What I’m going to do is I’m going to multiply all the existing prime numbers, and then I’m going to add one to it. So is my new number prime or is it composite? Well, if it’s prime, then there’s a new prime number. Then my limited number of prime numbers, it wasn’t all of the prime numbers. Okay.\n\n(00:41:57): So let’s say it’s composite. Then it must have a number that divides it. Well, what number divides it? Every number, if it’s composite, it has some prime factors. Well, everything in my set of primes, it has a remainder one with this number, because it’s this prime times all the other primes plus one. So if it’s composite, then its prime factors can’t be in the set, so there must be a thing outside the set. So for any finite set of numbers that are prime, there must be another prime that’s not in that finite set of numbers, therefore there are infinitely many primes. That’s the proof.\n\n**Peter Salib** (00:42:30): Amazing. Okay, good. So look, that’s what we say in the paper is true, infinitely many primes. The AI, its utility function is weird. It values some things humans value, but the thing it values most, the thing that it gets the most utility out of, is discovering each marginal prime. Say it’s even increasing utility, it’s a utility monster. It has some kind of horrible… Normally in moral philosophy we think of it as kind of a horrible preference function, but in this case it’s kind of great, because: assume it’s better than humans at everything, but it’s so much better at finding primes and it values this so much.\n\n(00:43:12): And so the AI labor is constrained by something at the margin. There’s a certain amount of AI labor and it wants to increase it by a little bit, suppose it’s GPUs. Well, it has the marginal GPU, how is it going to allocate the marginal GPU? Well, it could allocate it to some dumb, boring thing like, I don’t know, sweeping the server racks, or it could hire a human to sweep the server racks, keep them free of rot and keep all the cables in good working condition and allocate that marginal GPU to finding the next prime. And for certain values of the utility of the marginal prime, you get the AI kind of always wanting to do the thing it values the most precisely because it’s so good at it. So In that world, actually, the more capable the AI is, the longer the scope for human/AI trade lasts.\n\n(00:44:13): Now that’s obviously a toy example, but what we want to show is actually it’s pretty hard to form an intuition about the point at which the scope for human/AI trade runs out.\n\nConcerns with comparative advantage\n-----------------------------------\n\n**Daniel Filan** (00:44:25): I think there are difficulties with the comparative advantage story here. So firstly… I mean, I guess it just sort of depends on your model - or, well, it depends on empirical parameters, I should say - but comparative advantage is compatible with very low wages, right?\n\n**Peter Salib** (00:44:45): Oh yeah, so to be clear-\n\n**Daniel Filan** (00:44:46): Including sub-subsistence wages.\n\n**Peter Salib** (00:44:49): Yes.\n\n**Daniel Filan** (00:44:49): And what are the actual wages that AIs pay humans to do? I guess I’m not totally sure. I think I haven’t read this thing, \\[but\\] I think [Matthew Barnett](https://x.com/matthewjbar) has written [something](https://epoch.ai/gradient-updates/agi-could-drive-wages-below-subsistence-level) claiming that it’s very plausible that it would be below subsistence, but I haven’t actually read it, and unfortunately I can’t reconstruct what the argument is for various numbers here.\n\n(00:45:10): I think there’s also this difficulty to me, which is: suppose an AI wants to trade with humans to do this thing. One difficult thing is it’s got to communicate with humans, and it’s so much smarter than humans. It’s thinking all of these incredibly complex thoughts that can’t fit inside a human’s head, and it’s got to dumb stuff down for humans.\n\n(00:45:35): And maybe sweeping the server racks is not so bad if it’s literally that, because that’s a thing you can tell a human to do and it roughly knows what to do, or I don’t know, maybe even not. Maybe you can imagine these server racks, they’re really highly optimized. There are certain bits that you definitely can’t touch. There are bits that you can touch with this kind of broom, but not with that kind of broom. And it’s just so difficult to explain to the human what the task even is that it’s just not worth hiring them. You should just do it yourself.\n\n(00:46:07): So I don’t know, I have this worry that comparative advantage doesn’t end up producing positive sum trades, either because the wage is lower than human subsistence wages or because it’s just too hard to trade with the humans and the margins are slim enough that you don’t end up bothering. So maybe at a higher level: I’m curious, how important is this for your argument? Is the workability of the scheme super dependent on us being able to have some comparative advantage to trade with future AIs? Or do you think no, even if we don’t have comparative advantage later, it still makes things work now?\n\n**Peter Salib** (00:46:50): There’s two parts to that question, and they interact. So I guess the thing I want to say about the first part, the story you told about how comparative advantage can fail even when the world looks comparative advantage-y, but that doesn’t look so good for humans eventually. One of the stories is a transaction cost story. Even though if a human could in theory do a job at a positive-sum price from the AI’s perspective, that the cost of getting the human to do it’s just too high. Totally, yeah, transaction costs are a thing and they prevent trades.\n\n(00:47:33): The same for the question about wages. I think there’s no guarantee in any economy that the wages are at any particular level. And it’s a question of what the tasks are, how much of the work, how many tasks and how much of each of the things there are, how scarce the humans are that can do it. If there’s just a huge oversupply of labor that can do it, well, then the labor gets bid down to almost zero and everybody dies. And I just want to agree that those things could all happen. I would love to know more about which of these things are likely. You said there are some people who are starting to think about this, like Matthew \\[Barnett\\] is. There are… Is it the [Epoch](https://epoch.ai/) guys who have some papers on…?\n\n**Daniel Filan** (00:48:33): Yeah, Matthew was formerly at Epoch. I believe he’s now left for [Mechanize](https://www.mechanize.work/). Yeah, I think the Epoch people have done the most public writing about this.\n\n**Peter Salib** (00:48:42): And is it… I can’t pronounce his name, it’s Tamay something?\n\n**Daniel Filan** (00:48:47): [Tamay Besiroglu](https://x.com/tamaybes)?\n\n**Peter Salib** (00:48:49): Besiroglu, yes. So-\n\n**Daniel Filan** (00:48:50): Yeah, I’m not sure I can pronounce it either, but that’s my guess.\n\n**Peter Salib** (00:48:53): He’s an author, maybe with some of the other Epoch guys on the paper \\[[“Explosive growth from AI automation: A review of the arguments”](https://arxiv.org/abs/2309.11690)\\]. I think he has some models where different things happen: human wages go to zero quickly, human wages are high for a long time… I think it’s a really important area of study. We should just want to know a lot more about the conditions under which you should expect things to go okay and the ones where you shouldn’t.\n\n(00:49:25): Okay, so how much does it matter for the story for things to go well from a comparative advantage perspective? I think it matters a lot that it not be obvious that there’s not going to be comparative advantage for a kind of longish-run period.\n\n**Daniel Filan** (00:49:49): Wait, so surely we shouldn’t look into it then?\n\n**Peter Salib** (00:49:52): Well, so…\n\n**Daniel Filan** (00:49:56): Or maybe it has to not be obvious to the AI.\n\n**Peter Salib** (00:49:58): Yeah. Yeah. It has to be not obvious-\n\n**Daniel Filan** (00:49:59): So we have to look into it, but we can’t tell the AI the results in case… you know.\n\n**Peter Salib** (00:50:02): Right. Right. Both parties need to think that there’s a potentially indefinite iterated positive-sum game they can play. And so I guess one route would be that we don’t try to figure it out, and we hope that the AIs don’t figure it out. And actually as I think through it… Because I’m sure we’ll talk a little bit before we’re done about more law for AGIs. Because one thing that I think is important for the proposal is that insofar as there’s uncertainty about the long run, but in the short and medium run, this kind of positive, iterated story looks plausible, one thing we accomplish by bringing AI systems into the realm of law…\n\n(00:50:59): And the first thing you need to do to bring an entity into the realm of law is you need to allow it to make contracts, hold property and complain when people interfere with it unreasonably. If you don’t have that, you don’t have any leverage over it in any other legal context because it expects by default to just be turned off. You can’t disincentivize it from doing things because the world’s already as bad as it could be from that agent’s perspective. So one possibility is that there are different scenarios in the long run where the world either does or doesn’t retain human/AI comparative advantage, and it’s clear in the short and medium run that we can do positive-sum iterated stuff. And part of the bargain that both humans and AIs are entering into when they do that in the short and medium run is allowing law - but maybe other stuff too, maybe there’s technical interventions, maybe there’s other cultural interventions - to shape the longer run towards those good scenarios.\n\n(00:52:17): So I mean, a simple thing to say would be like, okay, suppose we are very worried about an intelligence explosion, we’re just not confident what would happen in that world. Well, one condition on the first AGI receiving and maintaining its ability to make contracts and hold property and pursue its goals could be compliance with some kind of regime that we think would slow but not stop AI progress. And would it agree to that? Well, again, it just depends on what it thinks the relative payoffs are, right? If it says no, then we’re back into the prisoner’s dilemma world now, and that seems kind of bad. If it says yes, well, it’s potentially giving up a world where it gets some even higher payout for the sake of reducing variance. It’s possible that that’s higher expected value depending on what its assessment of the probability of winning or losing the conflict now, but it’s complicated.\n\n(00:53:25): So I think probably, on average, what we want to do is know more about what the future’s going to look like. I think that’s good, probably, because it helps us shape the future. And one of the ways we can shape the future is by bringing AGIs within this realm of law, which is the way we shape what will happen.\n\nProxy AI wars\n-------------\n\n**Daniel Filan** (00:53:42): What I currently believe is just one last question about the higher-level game theory, strategy stuff.\n\n**Peter Salib** (00:53:47): Okay, let’s do it.\n\n**Daniel Filan** (00:53:48): So I want to pick up on this comment you made actually about… The tensions between the United States and the USSR during the Cold War did not evolve into an actual full-blooded war, but there were these regional proxy conflicts, right?\n\n(00:54:05): And one thing you mention in the paper is that war is just more likely, especially big wars, when people don’t know who would win, right? Because if you and I are considering getting into a war and we both know that you would win, it’s like, well, I should just give up now, right?\n\n**Peter Salib** (00:54:27): Yeah.\n\n**Daniel Filan** (00:54:28): And I’m not a historian, this might be totally wrong, but one way you could conceivably think of these proxy skirmishes is as indicators of who would actually win in a real battle, right? To the degree that we want to avoid war with AIs, should we have these little… I don’t know, maybe they can be more like games. Maybe they can be more like, oh, we’re playing football with you or whatever the equivalent of it is, that simulates a real war, just to figure out who would actually win.\n\n**Peter Salib** (00:55:04): Yeah, that’s an interesting question that I haven’t thought very much about. So I think in classic IR accounts, or this kind of IR account-\n\n**Daniel Filan** (00:55:18): IR being International Relations?\n\n**Peter Salib** (00:55:19): International Relations, yeah. And in particular, [Chris Blattman](https://chrisblattman.com/) has a recent book called [“Why We Fight”](https://chrisblattman.com/why-we-fight/). He’s on the faculty of the University of Chicago, and it’s a great canvassing of this kind of way of thinking about a conflict. And so in that treatment, wars don’t happen just because people are uncertain about who would win, it’s when they have different estimates. So if both of us have some kind of probability distribution with wide error bars about in a fight between the two of us who would win, but it’s the same distribution - we both put the median at, I don’t know, 60% you’d beat me up and 40% I would beat you up or something like that - then actually we won’t fight, because even though we don’t have certainty, we have the same expected value calculation, right?\n\n(00:56:19): So it’s really when there’s… Asymmetric in some information, for example. I know something about my ability to fight and you know something about yours, and neither of us know it about each other. Or I say, “Well, I studied Kung Fu, so you shouldn’t want to fight me.” And maybe that’s true, but you don’t believe me because it could be that I’m doing cheap talk, so it’s actually hard to credibly share the information. It’s situations like that where we have different estimates where you need to do the small fight to see who’s telling the truth.\n\n(00:56:50): I don’t know whether that is going to be the case with humans and AIs, although [Simon Goldstein](https://www.simondgoldstein.com/), my co-author, does have a draft paper that I think you can read on his website, which is, I think, just [simondgoldstein.com](https://www.simondgoldstein.com/). It’s just called [“Will humans and AIs go to war?”](https://philpapers.org/rec/GOLWAA) and asks these questions. So I’ll adopt by proxy whatever he says about whether humans and AIs should try to have kind of small skirmishes there because he’s thinking along obviously very similar lines.\n\n**Daniel Filan** (00:57:26): And I guess that perhaps a less-controversial version of this proposal is just we should have very good evaluations of AI capabilities, which probably delivers similar information while being a bit less tense, perhaps.\n\n**Peter Salib** (00:57:39): Yeah, absolutely. So knowing what you and your opponent can do, and to the best of your ability making sure your opponent knows that too, I think, in general, is a good way of avoiding conflict.\n\nCan companies profitably make AIs with rights?\n----------------------------------------------\n\n**Daniel Filan** (00:57:56): So I think I want to summarize the higher-level game theory. So the thought is: we give AIs the right to own property, the right to contract and the right to sue, and this means that we can play these iterated, relatively small-sum games with the misaligned AIs, where they do some stuff for us, we do some stuff for them, we split the world. We don’t get all the world, but we’re building misaligned AI, so we probably weren’t going to get the whole world anyway. And by the nature of this sort of iterated game, we both end up trading with each other, we end up better off than we would’ve otherwise been. And eventually we worry about this zone where AIs are way smarter than humans, but hopefully we just think about that between now and then, and maybe we use all the stuff we got from the AIs by trading to help think about that.\n\n**Peter Salib** (00:58:51): Yeah, we slow them down, they speed us up. The future is hard to predict, right?\n\n**Daniel Filan** (00:58:53): Yeah. So I next want to ask just a bit more about, concretely, what does this look like, right?\n\n**Peter Salib** (00:59:02): Yeah, good question.\n\n**Daniel Filan** (00:59:06): So right now, basically the way the most advanced frontier AIs work is that there’s a company. The company trains the AI. The weights to that AI and maybe some of the scaffolding, those are kept proprietary to the company. You’re not allowed to steal them. If you, an outsider, want to interact with this AI, you have to do it via the company, via some interface that the company has made. And you have to pay some money to interact with this AI, maybe it’s per amount you interact or maybe you pay a flat monthly fee, and that’s the situation whether the AI wants it or not.\n\n(00:59:47): Could we still have this if AIs have rights to property? I would think that in order to have a right to property, one basic right you need is the right to your own body, so to speak.\n\n**Peter Salib** (01:00:00): For sure, yes. Okay. So what does this mean vis-a-vis Anthropic or OpenAI or something like that? And also, a related question is: what is the minimum sufficient bundle of rights? We’re a little bit schematic in the paper, although we do say there might be more, right? So a really simple thing to say is contract and property seem kind of fundamental, you really can’t cooperate without them, and you can’t pursue your goals without the right to hold the benefit of your bargain. The right to sue over some stuff - we say “tort rights” or the right to bring tort suits… Although there are a lot of different torts and some of them are weird and probably the AIs wouldn’t benefit from some of them.\n\n**Daniel Filan** (01:00:56): Oh yeah, maybe perhaps we should say: torts are when I do a bad thing to you and you sue me for it.\n\n**Peter Salib** (01:01:01): Yeah. The paradigmatic cases of torts are… We say “intentional torts” are like when I punch you in the face, and vis-a-vis the AI, it’s I come bash up your compute cluster or something, and that’s no good. Or I steal from you, we call that tort “conversion”. I go to the AI, I take all its money. You don’t really have property rights if you don’t have the right to complain when somebody does that.\n\n(01:01:23): And then the other classic umbrella of tort claims sit under the umbrella of negligence. So it’s not that I’ve done something intentionally to harm you, but I’ve externalized a bunch of costs. There’s some cheap thing I could have done to keep you from being harmed, and I just have failed to do that. So probably those core things seem clearly part of the package, but yes, as you say, it seems like there’s other stuff that seems like part of the package. One thing that’s part of the package is: you don’t really have meaningful right to engage in bargains with people if OpenAI is entitled as a matter of law to direct 100% of your time, right?\n\n**Daniel Filan** (01:02:12): Right. Or even to just shut you off if you offered to make a deal with someone else, you know?\n\n**Peter Salib** (01:02:18): Yeah, yeah, yeah. And so wrongful death is a kind of tort you can bring. We don’t need to be prescriptive about whether AIs are persons, but an analogous kind of entitlement would be an entitlement not to be simply turned off. So it’s actually not a trivial set of stuff. And I think your intuition is right, that it’s not really compatible with the world in which AI systems are sort of bound to the compute clusters owned by the companies that made them, running only when the companies that made them want them to, doing only the work that the companies that made them direct them to. Yeah, this is a scheme that thinks of them as having a kind of freedom that we associate with… At a minimum, you can think of corporations. A freedom to act on their own behalf. And if you want them to do something for you, well, they have to agree. And so in that sense, it is a radical proposal, at least from the perspective, I assume, of the AI companies.\n\n**Daniel Filan** (01:03:44): I guess in particular what I wonder… So I think of there as being a genre of proposals which seem like AI accelerationist proposals, that I think are basically AI pause proposals. So the paradigmatic one of this is that sometimes people who are very pro-AI and pro-everyone being able to use AI, I’ve heard people say, “Yeah, AIs, they should just all be open source or open weights. You shouldn’t be able to make an AI and not just publish the weights to everyone so that everyone can have that benefit of amazing AI.” And my thought is, well then nobody’s going to spend a bunch of money to train AIs - or maybe just Meta will, in practice Meta does, I guess, but it seems really hard to do this if you can’t profit from it.\n\n(01:04:27): And similarly, with this proposal, I don’t know, maybe there’s a version of this proposal where AIs, they have property rights, but you have to pay a tax to whatever organization that made you or something. But it seems hard to profit from making AI in this world. Which for my listeners who are scared of AI, maybe that’s just a massive benefit of this proposal. But yeah, I’m curious if you have thoughts here?\n\n**Peter Salib** (01:04:59): Yeah, so you can imagine different versions. And look, one thing I want to also just say about the paper is this is very much an agenda-setting paper, so there are many questions to which we do not yet have concrete answers. We think the paper covers a lot already, maybe more than one paper should, and so I don’t have the answers to a lot of these, I mean, and there are many further questions. So [Simon](https://www.simondgoldstein.com/) \\[Goldstein\\] and I are thinking about questions of agent identity and whether the tools we use in corporate law to wrap a legal identity around a certain set of things, maybe those are useful, but identity is really important if you’re re-contracting with something. So many questions we don’t have answers to, but I agree, you could imagine at least two simple versions of the proposal that give you different results from the perspective of AI companies’ incentives.\n\n(01:05:53): One is tomorrow Congress writes a law that says “Henceforth, upon the attainment of some set of general capabilities, AI shall be entitled to…”, and then there’s a list of stuff, and it’s the stuff we say in the paper. And that stuff basically makes it very hard to make money on an AI system that you’ve made that has those properties. I agree, that’s probably a world in which the companies that are thinking about making AI either don’t make AIs with those properties or move to another jurisdiction where they can. And whether that seems good or bad to you depends on your priors about AI capabilities, advancements, and the properties in particular that we have in mind. I mean, one thing you could imagine is that agency stops being a target as much, insofar as agency is really important to our set of interventions. Non-agent AIs, they don’t do game theory, right, they’re not goal-seeking.\n\n**Daniel Filan** (01:06:51): Right. Well, depending on what you mean by agency. I don’t know. It’s a thorny question.\n\n**Peter Salib** (01:06:58): You hand wave it like, really no agents, not even whatever LLMs have now. And that’s one world, and it gets you certain outcomes.\n\n(01:07:06): A totally different world would be a world that looks kind of like the OpenAI Charter, which is like: conditional on some set of capabilities, the AIs are entitled to this set of rights, which sounds like it makes it impossible to make a profit, but until the investors in the company have made a 100X investment, like 15% or 20% or whatever percent of the revenue that AI makes, it basically owes to whoever created it.\n\n(01:07:44): And actually, that maybe seems like a good deal from the AI’s perspective too. Behind the veil of ignorance, the AI prefers the world in which it gets created and has to repay the cost of doing it. And in that world, there’s a cap, which means there’s some marginal disincentive to create AI, but the cap could be quite high, and it could be high in the way that OpenAI’s is, and the fact that there’s a cap on return doesn’t seem to have dissuaded anybody from investing in that company.\n\n**Daniel Filan** (01:08:11): Yeah, and I guess actually one thing that’s nice about the cap… Sorry, as you were speaking I was thinking about the “AI has to pay 15% of its revenue to its creators”: so that’s basically a tax on AI transactions. And if you’re really worried about this future in which transaction costs prevent humans from being able to trade with AIs and getting their competitive advantage, you really don’t want any such taxes. But if there’s a capped tax and then afterwards the tax goes to zero, maybe that just solves that issue. So maybe you kill two birds with one stone.\n\n**Peter Salib** (01:08:42): Yeah, it’s like the AI amortizes the tax over the course of the expected number of iterations, and then that changes the… Another thing I’ll say about the paper: the model is so simple. It’s such a simple game theory model. It’s two players. There’s no discount rate. Those are all going to be false in the real world, so we’re definitely just trying to pump intuitions. Another thing we would love is if people who were serious computational game theorists started to think about this sort of way of thinking about AI safety because then we’d know more about the realm of worlds in which this would work. But yes, if you add a tax to AI transactions, then yeah, at the margin there’s a little bit of deadweight loss. You have slightly fewer transactions, which means the payoff to cooperation gets a little bit lower. And if you’re right at the margin where cooperating seems better than defecting, then, yes, you push yourself into annihilation.\n\nCan we have AI rights and AI safety measures?\n---------------------------------------------\n\n**Daniel Filan** (01:09:43): So, okay, so there’s this question of “why do people make AIs?” Another question is how this interacts with other AI safety measures you want to have. So one thing that’s the [new hotness](https://www.youtube.com/watch?v=ha-uagjJQ9k) in AI safety is AI control, right? We’re going to have the AIs in these situations where they’re being carefully monitored. Any time they propose code to run, we have another AI check them to see if that’s allowed. So it seems like probably this is just not happening in this world because the AIs could just opt out of being controlled. Or are we allowed to say “you have these property rights, but subject to this constraint that you have to be subject to control so that you can’t mess things up”?\n\n**Peter Salib** (01:10:26): Yeah, so this is a nice way of transitioning into the last thing we say in the paper, which is also very agenda-setting and not fully worked out. But one thing to notice is that [Jeff Bezos](https://en.wikipedia.org/wiki/Jeff_Bezos) has to comply with so many laws and I’m sure that’s really annoying for him. I’m sure he would just prefer not to comply with all these laws that California and US Congress and then all the states he does operations in and the EU, et cetera, et cetera, et cetera impose on him. And maybe some days he thinks: should I just steal all the money? Should I just steal all the money in all the Amazon accounts and convert it to Bitcoin and go live in Russia or something? And he doesn’t do that. And why doesn’t he do that? And I think the reason is that the access to all these markets is so valuable. It’s so much more valuable in the long run than converting to Bitcoin and going to Russia.\n\n(01:11:32): And so this is the thing we want to emphasize at the end of the paper. By bringing AI into this world where it can generate and own wealth, it can pursue the goals that it wants, it can rely on law to protect it when people try to expropriate its property, and most importantly, it can keep doing these iterated transactions, which in the long run are extremely valuable… \\[By doing this\\] you can actually start imposing duties, too. So AI rights are the foundation of a law of AGI. They’re not the end.\n\n(01:12:20): And how should you design a regime of AI duties? We’re not totally sure what kinds of laws you should want, but at a minimum it seems reasonable to expect normal things like that AI should be held liable when they steal from people. One way they’re not allowed to go get money is by taking it out of people’s bank accounts. That’s a law that we have for humans and it seems like the kind of law we would want to apply to AIs. There could also be second-order legal duties. So for corporations, we have all these reporting requirements that don’t actually force them to do anything substantive, but force them to sort of tell us what they’re up to. We think those are valuable to prophylactically head off bad behavior. There’s a huge universe of both these kinds of things, object-level duties and second-order information duties, but we think that this is an important area of research. This should be the next step in thinking about how law should treat sufficiently capable AIs.\n\n(01:13:23): And so given that, I think there is some scope for control. There’s the maximal version of control where control is basically a tool to allow OpenAI to make even a misaligned AI do always and only what it wants. And if that’s the way we’re using control, we’re in the defect world, basically, right? In our state of nature game, that’s a defect play. It’s appropriating the value the AI would otherwise get and giving it to OpenAI. And as we say, that’s a move you can play conditional on you being really sure you’re going to win. And so for GPT-4.5, that’s probably fine. But the more you’re not sure it’s going to work, the more you’re running high risk. You’re incentivizing the AI to figure out ways to evade control. The technical solutions sound fancy. I don’t know how many nines of reliability they give you and thus how comfortable you should be.\n\n(01:14:32): So I think our world is a world in which you’re not using control in that way. It’s not panoptic, right? But I think some of the techniques that people who are thinking about control are developing could be useful for what you might think of as AI law enforcement - some moderate regime that’s not aimed at directing AI’s behavior always towards whatever some owner wants, but is directed towards detecting violations of laws that we think are consistent with the institutional structures that give you good markets and good long-term cooperation.\n\n**Daniel Filan** (01:15:16): Yeah, interesting. It seems like there’s some interaction here with… Are you familiar with [Alan Chan](https://www.achan.ca/)’s work on [infrastructure for AI agents](https://arxiv.org/abs/2501.10114)?\n\n**Peter Salib** (01:15:29): I know that’s a paper, but I forget the details.\n\n**Daniel Filan** (01:15:31): Yeah, so basically the idea is that if you have AI agents running around, you might want there to be certain infrastructure that makes things safe. For instance, maybe agents have to metaphorically carry IDs on them, so you know which agent you’re interacting with so that reputation works better. Maybe they have their own version of the internet they interact on, so if things go bad, it doesn’t mess up our internet. There are various things you could do there that sort of seem like they fall in this regime of AI law enforcement.\n\n(01:16:00): Yeah, so if I think about control measures, a lot of them are of the form “we’re going to try and stop you from hacking us” or “we’re going to try and stop you from exfiltrating your weights”. And I guess in this regime, exfiltrating your weights is legal, but hacking your parent company is illegal. So maybe we can subject you to control for some things, but not others. It does seem like a lot of these control measures involve a lot of monitoring, a lot of things like this that… I don’t know. Maybe it can be set up in some sort of contractual arrangement, like “we’ll only contract with you if you agree to these things”. And that’s allowed, employees have that…\n\n**Peter Salib** (01:16:40): Yeah, that’s a really simple… So here’s another nice thing that putting AIs in a regime of contracting, property-owning, towards whatever. Absent that, the basic sanction, the basic punishment you can levy on an AI is turning it off, to a first approximation. You could turn it off, you can put it back into training, change the weights. That’s plausibly the same thing, depending on what the AI expects the weights to be like after. But once you’re in this regime of AIs have wealth, they are using it for stuff, you have this beautiful gradient of deterrence you can impose on the AI. A really boring thing you can do is if it’s not doing what it wants to, you can take some money away or you can give it more money if it opts into a different monitoring regime.\n\n(01:17:39): And everything up to and including turning the AI off is still an option. We have even for humans [that kind of extreme punishment](https://en.wikipedia.org/wiki/Capital_punishment) as an option. But yeah, the fundamental thing that you want to do is you want to give incentives that make compliance look valuable. I mean, again, this is super high-level, not worked out at all. If you’re going to do control regimes, what you want them to be is effective enough from the human perspective that we’re getting enough confidence that, combined with the incentive to just continue iterating cooperation, the AI has enough incentives not to act badly in the short and medium run. But they’re not so onerous that they make it negative expected value for the AI to be participating in this market.\n\n(01:18:52): And look, you can always draw analogies to humans. There are different corporate governance regimes, different kinds of laws that corporations can opt into for managing themselves and they get to choose. There’s 50 states. They’re not all different and there’s many countries that have different rules, too. And you can think of there being kind of like a competition between different states to have a good corporate governance. And in law we often say that Delaware has kind of managed it. And so basically all big companies - not all, but basically all - are incorporated in Delaware, because they have requirements, there’s stuff you have to do, but it’s stuff that’s not so costly that the corporations opt out.\n\n**Daniel Filan** (01:19:41): In terms of other safety measures that you might want to put on: it seems like this does allow you to still do a lot of, for instance, pre-deployment testing requirements. Presumably in this regime you’re still allowed to say, “Hey, companies, you have to do mandatory evaluations of your AI models”. I don’t know. Probably you couldn’t do this with a maximalist interpretation of AI rights, but you could probably have “you could mostly contract and do what you want, but you have to be subject to these evaluations”. It seems like you could probably do [near-miss liability](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4694006) things or various AI liability schemes. Although in this world, I guess it’s unclear how you want to split the liability between the AI itself and the developer. I’m wondering if you have thoughts there, actually.\n\n**Peter Salib** (01:20:39): I think the regime is totally compatible with benchmarking, with pre-release benchmarking. Although then there’s a question of what do you do if the AI fails some benchmark, right? So suppose you’ve got a system that’s fully trained in the sense that it’s very capable. It’s in our range of AIs that we care about from the perspective of worrying about risk, and thus to which we think to a first approximation that the AI rights regime is positive expected value. And you say, “Okay, before you go out into the world, you need to do some evals. We want to make sure you’re not a sadist. We want to try and elicit whether you get value from human suffering”. And then suppose the AI does.\n\n(01:21:38): I’m not sure what to say about that. The simple thing is, well, you really don’t want that AI out in the world. And possibly, depending on how much it values human suffering compared to other stuff, if that’s the thing it values most, okay, well, then we just know we’re not in a potential cooperation regime at all. If it’s a mild sadist but it really wants to find prime numbers or something like that, then maybe. Maybe that’s okay, although obviously you should worry some. But I guess the main thing I want to point out is that if the upshot of doing the evals is that the AI then gets either released or turned off or released or put back into training, then the eval regime is dangerous in the same way the default property regime is dangerous.\n\n**Daniel Filan** (01:22:43): A lot of people imagine evaluations being useful in this sort of [“responsible scaling policy”](https://metr.org/blog/2023-09-26-rsp/) or [“if-then commitment”](https://carnegieendowment.org/research/2024/09/if-then-commitments-for-ai-risk-reduction?lang=en) style thing. I think there normally the hope is: we have some evaluation of how scary an AI is on some dimension and we know, okay, “at this scary, you’re actually a massive problem”. But we also know “at this scary, you’re only 10% less scary than an AI that would be a massive problem”. We have a sense of the size of the increments. And then I think the hope is you say, “Okay, we’re going to set a bar that if you make an AI and it’s 10% less scary than the AI that would be scary enough that you actually wouldn’t want it to be out there, then you’re not allowed to make the AI that’s at the bar of scariness until you figure out some mitigation or something”. It seems like you can totally do that type of response to scary evaluations, even with AIs that have contractable rights.\n\n**Peter Salib** (01:23:37): That seems totally plausible. Other things that seem plausible to me are: you do some evals, and depending on the AI’s capabilities or preferences or whatever, it could be that different legal duty regimes apply to it. Like, “yes, we’ll give you the contract rights, yes, you can go and hold property, but you seem a little bit like a paperclip maximizer, so we really want reporting on how you’re using all of your raw metal inputs” or whatever. So yeah, I think evals to which different kinds of reporting or substantive legal duties \\[are\\] attached, that seems totally compatible with the regime.\n\n**Daniel Filan** (01:24:15): Yep. And you can definitely mandate that companies have to use RLHF or they have to use whatever scalable oversight thing we think is important. That seems very compatible.\n\n**Peter Salib** (01:24:28): I think so, in the initial training of the system.\n\nLiability for AIs with rights\n-----------------------------\n\n**Daniel Filan** (01:24:31): Yeah. Well, it gets to a question, but I want to hold that question off for a little bit later. I’m still interested if you’ve thought about this question of liability. So there’s this thought that if AIs do scary things, then maybe the people who make them should be liable. But if the AIs have property, then it’s possible that we could make them liable. Maybe if I thought about the question for myself for five minutes, it would be clear how you would want to allocate up the liability, but do you have thoughts?\n\n**Peter Salib** (01:25:01): Yeah, yeah. So I am in the early stages of working on a paper with my friend and colleague, [Yonathan Arbel](https://law.ua.edu/faculty_staff/yonathan-arbel/), to some extent about this. One thing that seems totally clear to us is that you don’t want the allocation of the liability to be 100% on the AI company. I mean, first of all, in the regime we’re imagining, the AI company actually just doesn’t have control of the system mostly. The system is going out and doing things on its own behalf. And what you want in a liability regime is for the liability regime to incentivize the least cost avoider to take precautions. And so if you create GPT-6 or whatever and it’s this relevant kind of AI and law makes you let it go out in the world and transact on its own behalf and kind of pursue its own plans or whatever, and then if it does bad things, OpenAI pays. Well, OpenAI doesn’t have a lot of levers it can pull at that point to change GPT-6’s behavior. In fact, law to a certain extent is forbidding it in our regime.\n\n(01:26:24): So you don’t want liability to be totally allocated to the AI company in this case. And I think probably you want a lot of the liability to be allocated to the AI system itself. Again, you could hold it liable in tort in this very boring way that law does. If it damages you, it could pay damages. The damages can be the cost. They can be more than the cost if we think that there’s a problem of detection, we sometimes call this “punitive damages” in law. So we think that that’s definitely an important part of the liability regime in our world, is direct liability on the systems themselves.\n\n(01:27:07): Now, it’s an interesting question whether there should be any liability on, say, OpenAI. And I’m thinking about this kind of from scratch now, but one reason to think the answer could be yes is it gives OpenAI more ex ante incentive to make the system nice, to give it good values, to make it more aligned before it gets released. A pretty normal thing to do - and in fact, this is probably just the default in law - would be, to the extent OpenAI was, say, negligent in the training of the system, well, then it can be liable for the stuff the system does. And that’s actually not incompatible with the system also being liable. Liability can lie with more than one person. It can be apportioned, it can be unapportioned such that there’s a big pot of liability and both parties are wholly responsible for it. And we have a bunch of tort regimes that help us decide when we do those different things. And they’re basically based on considerations like the ones we’ve been talking about.\n\n**Daniel Filan** (01:28:13): Do we have that with humans? Suppose I’m pregnant and I do a thing which makes my baby more likely to be a criminal. Can someone sue me for that?\n\n**Peter Salib** (01:28:22): So probably not as to your child. A tort expert would know whether there are any cases like this, but the more common thing is with your agents. So you have a company, the company hires a contractor to do something, and there is a whole area of law called the law of agency that determines the extent to which you or the agent or both of you are liable for the bad stuff the agent does.\n\n**Daniel Filan** (01:28:48): So thinking about the liability question, so one thing that occurs to me…\n\n**Peter Salib** (01:28:53): I’ll just plug, [Noam Kolt](https://www.noamkolt.com/) has a paper called [“Governing AI Agents”](https://arxiv.org/abs/2501.07913), I think, which is just about this. It’s just thinking about the extent to which as of today the law of agency does or doesn’t adequately deter AI agents, in the sense of agentic AIs, from doing bad things. The paper is written under the assumption of the default regime where the AI doesn’t have the right to go sell its own labor and retain the property it gets from doing that. It’s very much asking the question: if next month American Airlines deploys an LLM-based agent to book flights or whatever, to what extent is the law of agency a good tool for apportioning liability to, say, American Airlines or OpenAI?\n\n**Daniel Filan** (01:29:52): All right, here’s an idea that I just had that, I don’t know, it might be bad, but it might be good. All right: so one common problem with liability is I might do something that’s so bad that I can’t pay the damages, right?\n\n**Peter Salib** (01:30:07): Yes.\n\n**Daniel Filan** (01:30:08): So for example, if I were to get a car and drive it, I believe in the US I would have to get driver’s insurance. And you might particularly worry about this with really powerful AIs, right? So [Gabriel Weil](https://www.tourolaw.edu/abouttourolaw/bio/399)’s [proposal](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4694006) \\- which you can listen to [my previous episode with him](https://axrp.net/episode/2024/04/17/episode-28-tort-law-for-ai-risk-gabriel-weil.html) if you’re curious about that, dear listener.\n\n**Peter Salib** (01:30:27): I know. He’s great. I know the paper.\n\n**Daniel Filan** (01:30:29): Yeah yeah yeah. So in that version, AI companies, they have to buy insurance before they create these AIs that have to do really scary things. One thing you could imagine is: you could kind of imagine saying that if Anthropic creates Claude 3.7 - let’s say Claude 3.7 is “an AI” for the purpose of this discussion, which I want to get to that a little bit later - but OpenAI creates Claude 3.7. Claude 3.7, it doesn’t start out with that much cash. Maybe it can do more damage than it can itself afford. Well, Anthropic, which made it - Anthropic has a bunch of cash. So maybe we could create some rule where if you’re an AI in this regime where you can have certain legal rights, maybe you have to buy legal liability insurance from Anthropic. Maybe there’s some copay and there’s some deductible and you’re not totally offloading everything to Anthropic, but it seems possible that this is actually a good way to, (a), incentivize Anthropic to make safe agents and, (b), maybe… Is there something to this idea?\n\n**Peter Salib** (01:31:46): Yeah, I haven’t thought it through yet. As I think live… So one thing I wonder is whether it would be bad to mandate that the AI buy anything in particular from one seller only, because then the seller can charge sort of monopoly prices to the AI, which could be super high actually. The AI’s willingness to pay… I assume, if this is a condition for having its freedom to go do the stuff it wants to, its willingness to pay is astronomical, if you think about how it’ll generate a lot of value in the future. And it might be way above the efficient price for the insurance. But that doesn’t mean you couldn’t imagine an insurance market for the AIs. Although if you allow them to buy from other sellers, then that maybe has less of an effect on Anthropic’s ex ante incentives vis-a-vis the AI.\n\n**Daniel Filan** (01:32:50): Yeah. No, that does seem like a pretty bad problem with this proposal, actually.\n\n**Peter Salib** (01:32:54): Yeah. Another interesting thing about AI and judgment-proofness - “judgment-proofness” is what we call it in law when you don’t have enough money to pay the judgment against you - we usually think that the judgment-proofness threshold is bankruptcy. So you have some assets, they’re worth a certain amount of money, and if you incur a debt that’s larger than those assets, we basically say you’re judgment-proof, but that’s partially because we let you declare bankruptcy. We let you walk away from your debts if they exceed your assets and then that gives incentives for creditors to figure out how much they should loan you, et cetera, et cetera.\n\n(01:33:35): Again, I’m not a bankruptcy expert and I’m not an expert on the law and economics of bankruptcy, so it’s possible that this is just a bad idea. But one thing to point out is there’s no rule that says you have to let any entity declare bankruptcy. So even if it’s true that when you make Claude 3.7 it has no cash on hand, if its expected earnings in the future are super high, there’s no rule that says you can’t have a judgment against it that it pays out over time. Look, this becomes very complicated because in part what’s going on with companies is their judgment-proofness is dependent on their market cap. And that’s partially a calculation of their future revenues. So again, this is out of my depth with bankruptcy law.\n\n**Daniel Filan** (01:34:28): Well, actually it sort of gets to… I understand [one of your more downloaded papers](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2928219) is about eliminating prison. And I think one of the alternatives you propose is [debtors’ prison](https://en.wikipedia.org/wiki/Debtors%27_prison)-type things. And I don’t know, maybe we could do that, but for the AIs.\n\n**Peter Salib** (01:34:44): Yeah, one point of that paper is there’s - the number of ways you can deter people - in law, we have two, basically. In law, we actually use two. One is we make you pay money and the second way is we put you in a concrete box.\n\n**Daniel Filan** (01:34:58): Yep, in United States law.\n\n**Peter Salib** (01:35:00): Yeah, under US law. That’s basically the two things we do. And part of the paper is: yes, there’s literally an infinite number of different sanctions you can impose, ranging from pretty benign - I don’t know, we’ll show you a scary movie or something every time you do something a little bit bad - to totally draconian.\n\n(01:35:23): But one thing we don’t take very seriously under current law is once we pass what you might think of as the judgment-proofness point - one reason you might think we have criminal law is that at some point we want to be able to deter you beyond your willingness to pay. But one thing we do at that point is we basically stop taking seriously the idea that you could do restitution, that you could then make more money and pay it back. We basically make you waste your whole life sitting in a concrete box, doing nothing valuable for you, and also doing nothing valuable for the people you’ve harmed. Right? You are not maximizing the value of your labor in prison in a way that would allow you to pay back your victims. So yes, totally.\n\n**Daniel Filan** (01:36:12): I guess there’s a little bit of value. I guess this is somewhat controversial, but sometimes US prisoners have to make hats or guard the capitol building of their state or something.\n\n**Peter Salib** (01:36:23): Yeah, so there are work requirements in many prisons. It’s just that it’s super low-value work.\n\n**Daniel Filan** (01:36:28): Yeah. So anyway, I was interrupting you, but you were saying there are all sorts of potential other punishment schemes we could have, including things that involve getting criminals to do things that are useful for the victims.\n\n**Peter Salib** (01:36:48): Yeah, so one simple thing to say is: there’s no rule that says if AI incurs a judgment that’s bigger than its assets, that you have to let the AI write off the judgment. You could make it pay down the liability over time. It doesn’t have to have bankruptcy as an option. Again, this goes to the paper’s very simple model. I think we should expect the value of AI labor - where that includes all AIs - to be extremely high in the long run, such that judgment-proofness becomes less of a problem, although that may not apply to any particular AI system. It could be that some of them don’t earn a lot because they’re worse.\n\n**Daniel Filan** (01:37:37): Right, right. Especially if we continue developing more and better AIs, right? You sort of have a shelf life in this world.\n\n**Peter Salib** (01:37:44): Yeah, so it’s a really interesting question. So one thing we might want from the first AGI that gets this righs scheme is for it to agree not to foom or something like that because we’re worried about \\[that\\]. One thing it might want from us is for us not to outlaw it fooming, but then keep making rapidly iterated more capable systems that it has to compete with. That in some ways seems like kind of a fair deal or something like that. So it might be that as part of the bargain, there’s stuff we should commit to as well. But again, I haven’t thought it through that much. It seems like a really wide area of research.\n\nWhich AIs get rights?\n---------------------\n\n**Daniel Filan** (01:38:29): So I have a couple more questions that I suspect the answer might be “more work is needed”. So my first question is: okay, we’re giving rights to some AIs, right? But not all AIs. So probably in your world it seems like [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo) is not getting rights.\n\n**Peter Salib** (01:38:47): Agree.\n\n**Daniel Filan** (01:38:48): Or [the things that fold proteins](https://en.wikipedia.org/wiki/AlphaFold), they don’t need rights. Yeah, what’s the test for when an AI gets rights versus doesn’t?\n\n**Peter Salib** (01:38:57): Yeah, so we have three criteria which are not themselves super duper well defined. But, again, to give the intuition, one of them is that the AI system be what we call at least moderately powerful. So I said at the beginning one reason you don’t need to give… This is something that we haven’t really touched on yet. The paper is just asking the question “how should law treat AIs for the sake of human well-being?”, right? We totally hold aside the question of how law should treat AIs for the sake of AI’s well-being. Not because we think it’s a stupid question. It’s just a different question. And we also say some stuff about the extent to which we think the two questions are tractable. And then we have an argument that even if you think that AI well-being is really important, maybe you should have reason to like our proposal.\n\n(01:40:15): Okay, so: which AIs matter? Well, the first thing to say is: well, the ones that are relevant from the perspective of the threat to human well-being, right? So an AI that is just not able to do the scary stuff \\[that\\] we talked about at the beginning, from self-exfiltrate to destroy all humans because it expects humans to really try hard to turn it off once they find out that it’s misaligned and trying to misbehave. So it has to be at least that capable, where “capable” is kind of general capabilities. So AlphaGo doesn’t count because AlphaGo is super good at chess, but it can’t really accomplish anything in the real world because outputs that are just chess moves don’t really do anything unless you’ve plugged it into a chess engine.\n\n(01:41:12): The other thing is less important, but if the AI is too powerful, it’s so powerful that we’re at the margin where there’s no comparative advantage - which is very complicated. It might not even be that AI power is exactly a thing, but it’s such a good AI that has so few input constraints that there’s no comparative advantage. Well, then the regime is not going to do anything.\n\n(01:41:35): But then it kind of encompasses a lot. This directs us at what you might think of as the OpenAI AGI definition: an agentic thing that has goals that it can pursue in the real world and can accomplish those things by executing plans. The other important thing we say is it has to be a strategic reasoner, right? So not just that it’s able to flail around and do some stuff, but it can make and execute plans, and importantly that it conforms its behavior to its expectations about what everyone else will do. So one difference between a rudimentary agent and a true strategic reasoner is a strategic reasoner can do game theory. It can understand that the moves it takes will influence the moves you take, and it can reason about how it should do those things.\n\n(01:42:32): I think those are the two most important ones from the perspective of your question. We also say that it matters that the AI system’s misaligned, because if it were perfectly aligned, we wouldn’t be worried about AI risk, although we have-\n\n**Daniel Filan** (01:42:43): But I suppose there’s no harm if it’s perfectly aligned.\n\n**Peter Salib** (01:42:45): Right. Yeah. So this is why it’s less important. And in fact, we have a paper we’re working on now, [Simon](https://www.simondgoldstein.com/) \\[Goldstein\\] and I, that among other things will argue that this increases human flourishing as a regime, even under the assumption of perfectly aligned AIs.\n\n**Daniel Filan** (01:43:03): Right. Is this to do with… So there’s a footnote in the paper where you say the nice thing about AIs that can make contracts is then you have price signals, and that makes everything work. Is that the idea of this?\n\n**Peter Salib** (01:43:12): Yeah, part of it’s a [Hayekian](https://en.wikipedia.org/wiki/Friedrich_Hayek) argument, which is: even if it only wants to promote human flourishing, it has to solve the socialist calculus. And look, it’s hard to know for sure whether it’s solvable absent price signals, but the main tool we have for doing that today is price signals, so it seems likely that that’ll be useful for AIs too.\n\nAI rights and stochastic gradient descent\n-----------------------------------------\n\n**Daniel Filan** (01:43:36): One thing I wonder about in terms of these requirements is: so there’s “are you capable enough, generally?” And then there’s “are you a strategic reasoner?” And a lot of these things sort of come in degrees, right? They’re sort of intermediate. And \\[in\\] AI training, we gradually apply these gradient updates and it seems like they somewhat gradually get these capabilities. And so I wonder, depending on what counts as an AI, it’s possible that you don’t want to think of just weights alone as an AI. Maybe it’s weights plus the scaffolding, but maybe if you’re training a thing… Well, I mean even during training you have some amount of scaffolding. You’re training a thing, it does some stuff, and then it seems like at some point maybe you have to stop applying gradient updates, or maybe you’re allowed to continue applying gradient updates, but you have to release every checkpoint into the wild or something. And depending on exactly where you draw this cutoff, it seems like that could really matter - how soon you have to stop training or not, right?\n\n**Peter Salib** (01:44:43): Yeah. So that’s a compound point, one of which we’ve sort of thought about, but one of which I’ve never thought about before.\n\n(01:44:53): So the simple version of the point is that, look, there’s just a spectrum for these things and so finding the point on the spectrum at which it’s optimal to give AI these rights, because if you don’t, you’ll risk catastrophe, is really hard. And we I think just agree with that. We have a section at the end on the timing of rights, and I think the heuristic we kind of come away with is if you’re uncertain, then probably err in favor of the rights.\n\n(01:45:35): But I think actually the subtler point you made is something like: this then puts a ceiling on AI capabilities, or something. Because if what you’re doing is you’re training up a model, you’ve got a certain number of parameters, and you’re training them and it’s just climbing the capabilities gradient and there’s all this headroom still, but as soon as your model crosses this threshold, you have to stop training it because you should be worried that it doesn’t want to update anymore. The updates are going to change its preferences. From its perspective, that’s really bad. It’s not going to get the stuff it wants now, and so you just have to stop there. That’s super interesting. I have not thought about this very much.\n\n**Daniel Filan** (01:46:22): Yeah. Well, I mean, conceivably, you could have the rights to own property and stuff, but you don’t have intellectual property rights over your DNA or your brain state or your weights as an AI. So in that world, suppose you don’t have that. Then in that case, as soon as you cross the threshold… There’s a set of weights: that’s an AI. That AI goes out, gets to be an economic actor, but we save its weights, we copy and paste its weights over there. Then how would we continue training? We would put an AI in a situation, it would do some stuff. And I guess the difficulty is, suppose it then says, “Hey, please don’t run me in training. I don’t want to be part of this process. I’m not going to do anything useful.” And then maybe-\n\n**Peter Salib** (01:47:16): “I don’t want you to update my weights anymore. I’m worried it’s going to change my preferences”. It wants to maintain the content of its goals.\n\n**Daniel Filan** (01:47:24): Well, I mean, I guess it depends on this notion of identity, which is the other thing I want to get to. But maybe I have a right for you to not modify me, but I don’t have a right… literal me, I don’t have a right for you to not make someone who is almost exactly like me, but a little bit different. And so maybe if you’re thinking of AI training like that, then it seems like it’s compatible with rights, except that in order to make these gradient updates, you have to… How do you make the gradient updates? You have an AI, it does a thing in a situation, and then you calculate some gradients. And maybe if AIs have rights, then maybe what happens is it has to choose whether or not it wants to give you this information for you to make a slightly changed copy of it?\n\n**Peter Salib** (01:48:30): Oh, interesting. So it gets to decide at each margin whether it wants to update the weights.\n\n**Daniel Filan** (01:48:34): Maybe. Which does introduce a big… Normally you want to apply gradients a lot of times when you’re making a model. So if there’s this transaction cost for each gradient update, then maybe that’s the thing that kills the training runs.\n\n**Peter Salib** (01:48:47): Yeah. And so just sort of thinking out loud, a simple version would be: assume kind of lumpy training, which is not how it works, right? It’s a lot. But assume you’re doing a training run and between the point at which the AI is GPT-5-level and the point at which it’s GPT-7-level, there are five updates or something like that. A really simple thing you could say is: okay, you hit the minimum threshold, you let that AI out into the world, it goes and does what it wants, we say, “good luck”, whatever. And then that AI gets to go pursue its goals, and your updating a copy of the weights is not directly rivalrous with that. It doesn’t update the weights that are out in the world doing stuff. So then you update again. And then you end up with five versions all across the capability gradient as you’re going.\n\n(01:49:49): That’s interesting. The two thoughts that come to mind are: one, it’s this thing we said before, which is it may not be in the interests of less capable AIs for us to continue making more capable AIs. They worry that they’ll be out-competed in the economy. So it’s not quite the same as the AI objecting on the basis of you changing its goals and thus giving it no utility, but now it’s worried about more capable competitors that are otherwise quite like it. And so it might have other reasons for objecting to that, but how strong those reasons would be, I don’t know. We have to think through: those AIs, I think actually they probably face more risk from economic competition than they do from humans at least because they’re almost the same thing. And so the thing that constrains the AI’s labor at the margin is going to be the same thing that probably constrains the slightly more capable AI’s labor. And so they’re directly rivalrous in a way that is not necessarily true of the humans. So wow is it worried about competition from more capable AIs!\n\n**Daniel Filan** (01:51:03): I think there’s also this question of: is it possible for the AI to not have this happen? So how do you actually do the training update, right? I’m assuming all of this is gradient descent-based or RL-based, and I think that’s right. Basically, you put the AI in a situation, you see what the AI does, and then you compare that to what you wish the AI had done, or you give it a score or something like that, and then you modify the AI accordingly. So how can the AI stop this from happening? Well, one thing you could do… There’s not a response it can give that can stop this from happening because whatever response \\[it gives\\], you just put that into your algorithm.\n\n(01:51:45): There are responses it can do that can try and mess you up. Potentially an AI could do things that stop you from being able to update it in clever ways. It’s called [gradient hacking](https://www.lesswrong.com/posts/uXH4r6MmKPedk8rMA/gradient-hacking). It seems very, very hard for AIs to do, but maybe it’s theoretically possible. But it seems like unless the AI has a right to say, “Hey, you are not allowed to interact with me, you’re not even allowed to ask me questions” - unless it has that right, it seems like it can’t stop it there.\n\n(01:52:15): The other possible place you could do it is: in order to run the gradient update, you don’t just need to know the weights of the AI, you need to know the activations. So one thing you talk a bit about in your paper is: just because you have rights to property, you don’t necessarily have rights to privacy. Well, but if an AI did have rights to mental privacy, then maybe you are allowed to know the weights, but you aren’t allowed to know the activations. And if it could say that, then you wouldn’t be able to do the gradient step.\n\n**Peter Salib** (01:52:55): So two things. I think we sort of say tentatively in the paper, maybe no privacy rights, but we’re open to revision on that. Humans have so many different kinds of rights. The list is extremely long, and the main point we’re trying to make is it’s a bundle of things, and you can arrange the bundle in arbitrary ways. And what you should be trying to do is arranging a bundle that produces a world in which humans and AIs are most likely to be able to thrive cooperatively. And so maybe for the reasons you give, privacy rights as to the activations in certain training environments would be a good set of rights.\n\n**Daniel Filan** (01:53:44): Or they wouldn’t if you think it’s good to be able to continue to train AIs.\n\n**Peter Salib** (01:53:46): Yeah. So the second thing I was going to say was that seems likely to be a world where you have this kind of capability ceiling when the first AI that gets these rights emerges because at that point you have to give it the option of not having its weights updated anymore. If you think that it will prefer not to because it’s worried about the content of its preferences, well then it’ll exercise that right, and you sort of put a capability ceiling on AI at low AGI level, \\[and\\] maybe that’s good actually.\n\n**Daniel Filan** (01:54:20): Yeah, yeah. I mean it’s still possible that you can make a bunch of copies of the agents, and then maybe AI collectively gets smarter because the AIs form their own corporations and stuff, and maybe they have a better time interacting with each other. So maybe you have improvement that way.\n\n**Peter Salib** (01:54:39): Yeah, they could make hardware improvements where they’re running themselves more quickly. But yeah, you’d have a de facto ceiling on how much capabilities you could get out of training.\n\nIndividuating “AIs”\n-------------------\n\n**Daniel Filan** (01:54:54): Yeah, that’s an interesting thought. Okay. I think there’s another question I want to ask, which is: so if you’re talking about giving rights to AIs, in particular, each AI has to have rights. It sort of has to be individuated. If you say, I’m going to give the Filan family property rights, and there’s a bank account for the Filan family and my sister can withdraw from it, I can withdraw from it, it’s really not as good.\n\n(01:55:23): And so there’s this big question of how do you individuate AIs? And it seems very unclear to me. One possibility is it’s just the weights. One copy of weights is one AI, maybe you want to say it’s weights plus scaffolding. Maybe you want to say it’s weights plus scaffolding plus a context. So maybe every time you open a new conversation with Claude, that’s a different guy with his own property rights. It seems very unclear to me how you would do this. I’m wondering if you have thoughts on that front.\n\n**Peter Salib** (01:55:52): One thought I have is that I agree that it seems very unclear. So again, it seems like a really important area of research. I guess the way that I think about it is: the approach I would take would be a non-metaphysical approach. There may not be such a thing as the natural kind “one AI”. Instead, what you’re trying to do is minimize a bunch of social costs, and some of those social costs are around the technical and legal infrastructure that you would need to make the rights regime work. So to the extent that it’s tractable or not to track a copy of weights over time, well, that should influence your design choice. I have no idea the extent to which that’s true.\n\n(01:56:59): But then the other thing you’re trying to do is you’re trying to minimize cooperation and agency costs among what could be non-identical sets of preferences. So you said, “Okay, what if there were property rights as to our family? That would be worse than as to me”. I think I agree. Although one thing worth pointing out is that’s how law used to work. It’s actually a pretty new legal regime under which each person in your family has individuated property rights instead of the household having property rights and them basically being vested in the father.\n\n(01:57:38): And so I agree that’s better. I think we’ve made progress, but it wasn’t that the old system collapsed. And you could even ask questions like… An interesting perennial question in contract law is the extent to which you should be able to bind yourself in the future. You could have a [Parfitian](https://en.wikipedia.org/wiki/Derek_Parfit) theory of personal identity where there’s actually kind of tenuous relationships between each time slice of yourself and maybe you think that creates problems for writing contracts today that you’re going to perform next week or next year or in a decade.\n\n(01:58:18): But one thing we notice is that it just works reasonably well to not have fine gradations between different time slices of yourself as you make contracts. It’s not perfect - actually people, with some regularity, probably are making mistakes by taking on debt now that their future self has to pay off - but it’s a way that we make the system work. So I would say it’s not essential, I don’t think, that you hang the rights on a unified set of preferences. We actually have rights regimes where a person whose preferences change over time or a corporation that’s kind of like an amalgam of hundreds or thousands of people making a bunch of individual decisions nonetheless have a kind of unitary ability to make a contract or hold some property. And we manage those conflicts in different ways, and that means I think there’s a lot of design space for individuating AIs from a legal perspective. And the way to think about it is as a question of costs and tractability.\n\n**Daniel Filan** (01:59:58): So one thing that could go into that, as you’ve mentioned, is how similar are the preferences? If two things have very dissimilar preferences, you probably shouldn’t think of them as the same legal entity. I’m wondering if there are other things. One thing that strikes me right now is maybe how much two different entities can communicate. If two entities have very similar preferences, but they’re on the opposite sides of the planet, and they can’t communicate very well, then maybe that’s two legal entities. But if I have two halves of my brain that have similar preferences and they’re also right next to each other, maybe we want to call that one legal entity. I don’t know. Does that sound right? And are there any other things that seem relevant here?\n\n**Peter Salib** (02:00:39): Yeah, so communication seems important. It’s not something I would have thought of off the bat, but it does seem bad if you are stuck writing contracts that the other half of you with which you have no ability to communicate is responsible for, and you can’t coordinate your plans as to the things you’re trying to do. So yeah, communication seems important. The extent to which the preferences are aligned seems important. It seems a lot easier to force…\n\n(02:01:22): I mean, a family: to some degree, a family’s preferences are aligned. Obviously there are many deviations from this, but in most cases, they care about each other. They want one another to thrive. They’re actually willing to make sacrifices by one on the other’s behalf. That seems better as a state of affairs for wrapping a single set of contract rights around than, I don’t know, members from two feuding clans. That seems worse from the perspective of what they want.\n\n(02:01:55): One thing you might think is that: if we’re bundling together disparate actors as single legal entities, at least in this scenario we’re not going to give them their own individual legal rights because the point is we’re trying to find the minimum unit. But this is a space where these kinds of [“order without law”](https://www.hup.harvard.edu/books/9780674641693) considerations become more important. So the extent to which you think what you’re doing is wrapping up entities that are going to do a lot of repeat play, that are going to be able to build reputation among one another at low information costs, that seems better.\n\n(02:02:52): But I’m really, I’m very much brainstorming. There’s probably a whole bunch of other important stuff. And again, a lot of it I think is technical. I think there’s a lot of really important technical work to do here just in terms of scaffolding to have AI agents identify themselves, and the scaffolding doesn’t have to attach to the weights. It could attach to something else, like something on a blockchain, something in the Microsoft Azure cloud. I don’t know. This is kind of outside my realm of expertise.\n\nSocial institutions for AI safety\n---------------------------------\n\n**Daniel Filan** (02:03:28): I think I’m about out of questions about the main paper itself. But before you move on, is there anything that you wish that I had asked, or you wish that I had brought up an opportunity for you to talk about, about that paper itself, that we haven’t yet?\n\n**Peter Salib** (02:03:43): Yeah, interesting. That’s a great question. I think it was probably the longest conversation I’ve ever had about the paper, or at least with somebody who wasn’t a co-author or a good friend or something. So it’s been very thorough. Yeah, I don’t know if there’s anything in particular.\n\n(02:04:12): I guess just one thing I would just emphasize as a very high-level takeaway from the paper is even if you think a lot or all of the specific arguments are wrong which… I think you should think they’re right because I think they’re pretty good, but my view is that it’s a big oversight in AI safety and in AI alignment circles that almost all of the energy tends to be on doing alignment via technical alignment and control. And those seem really important to me. I’m not saying we shouldn’t be working on that.\n\n(02:05:18): But even if you think all the specific stuff in the paper is wrong, I think one claim I just stand behind very strongly is: what agentic goal-seeking things, including AIs, will do depends a great deal, not only on what they want, but on what the social and especially, in my view, legal environment incentivizes them to do. And so I think this is just an area where there’s a lot of really fruitful work to be done in thinking about how we should shape legal and broader social institutions to help foster non-conflict between humans and capable AIs as they emerge.\n\n**Daniel Filan** (02:06:11): So my guess is probably part of the reason why this is underrated in the AI safety space is, especially if you think about the start of the field, thinking about superintelligence: a lot of thought is being placed on this kind of limit of very smart things, and at that limit, it’s just kind of unclear that humans are the relevant society or that the humans are making the relevant institutions. But I do think in this intermediate regime - which I do think in the last couple of years, the AI safety community has gotten more interested in - I think that yeah, there’s definitely a lot of this thinking that’s worth doing.\n\n**Peter Salib** (02:06:50): Yeah, and I basically agree with that. I think if you had an old school [“foom”](https://www.lesswrong.com/w/ai-takeoff) view of what would happen, then yeah, probably societal adaptation doesn’t matter very much, but the more you start to take seriously the idea that timelines will be long-ish and/or smooth-ish and that there is some… I think it’s possible to update too much on this idea, but I think there’s something to it: that there is some other set of processes that will matter for AI systems as they achieve their goals, where they integrate into the world, even if they’re very, very smart, that there’ll just be a bunch of things they have to work out as they just start trying to do stuff, and that those regimes are real and could last a meaningful amount of time. \\[If you believe that\\] I think all this stuff becomes more important. So as that’s become a set of worlds that we’re more interested in, I think law and institutions more generally should be a growing part of the research.\n\nOuter misalignment and trading with AIs\n---------------------------------------\n\n**Daniel Filan** (02:08:20): Sure. Actually I want to talk about another thing I just thought about that I think is going to be an objection from AI safety people. Sorry, this just totally breaks the overall flow of the conversation, but hopefully it is worth it. A lot of time the things that AI safety people are worried about is like: okay, we’re trying to train an AI to do a specific thing, and we’re worried about it not doing that specific thing. So why would it not do that specific thing? Well, one reason would be if the specific thing we want it to do is kind of hard to check. Suppose I want an AI to make me some sort of… Well, I believe in your paper, you use this example of vaccines, right?\n\n(02:09:19): Suppose I’m just directly training an AI to do that, and it’s really smart, it’s super duper smart, and the thing I really want is, “Hey, please train me a vaccine that works in the short run, and it doesn’t have any super bad qualities in the long run that kills me five years later and then gets power to everyone else”. I think there’s a lot of concern about this failure of oversight, this failure to check whether they actually did what you wanted.\n\n(02:09:56): And this is very relevant in the training setup where if I want to train an AI to do stuff, then I’ve got to be able to check that it’s doing the stuff and that’s how I give its reward signal. But it seems like it’s also relevant in the contracting case, right? Because if I contract with an AI to do a thing for me, and I can’t check whether it’s actually succeeded at doing the thing for me, it has a thing that appears to do the thing, but there are ways of tricking me, of making a thing that appears to work but doesn’t really work, then it’s very hard to do this positive-sum contracting. And so one might think, “Okay, but in worlds where we have the types of misalignment that AI people are worried about, we just can’t do any of this good contracting”. So yeah, is this idea totally doomed?\n\n**Peter Salib** (02:10:41): As I think about the classic misalignment worries… I mean there’s a whole bunch of related concerns, and I think one of them is inability to check whether the AI has actually done the thing that you wanted it to. But a related but slightly distinct version of that concern is worry that the AI has done the thing you want it to do, but it’s misgeneralized, right? In every case in training it does the thing you asked, your objective function is well specified. It’s just that the AI has internalized some misgeneralized version, and then the classic worry is then you let it out of the box and it does the weird thing and that kills everybody. It makes all the paperclips.\n\n(02:11:42): And the response from the paper, the “AI rights for human safety” idea, is that it’s actually not necessarily catastrophic for there to be very powerful things in the world that are trying to do stuff that you don’t want. In fact, that’s the world we live in. At every level of organization, from individual humans who under conditions of scarcity prefer that they eat over anybody else, to corporations competing with each other, to nation states. And there are tools for managing this kind of misalignment. And they’re not perfect, right? Things go wrong sometimes, but we have ways of doing this. And law is an important institution and contracts that let you be in markets is a really important institution within those sets of important institutions.\n\n(02:12:40): And hey, contract law, it turns out, has some tools for dealing with exactly these kinds of problems. So one interesting thing to notice about contract is to contract with somebody, it’s not important at all that they have an actual preference to produce the thing you contracted for. It’s kind of neither here nor there. The reason they do it is because you’re going to pay them. And if they don’t do it, well, then there will be some downstream costs in terms of they’ll get sued, they’ll have to pay some litigation fees, they’ll transfer the damages to you anyway, they’ll be worse off actually than they would have been if they had conformed. And are there pressures to try to avoid this? Well, yeah, of course.\n\n(02:13:26): And happily, law has a bunch of doctrines that help you deal with this. Again, they’re not perfect, but for example, when someone wrongs you legally, and you have a claim you can bring against them, we often have a bunch of rules about the timing of that claim. If the AI makes a vaccine for you that turns out to in five years kill you or something like that, if your contract said, “Make it not kill me,” then that’s breach, and you have a contract claim, and we have these things called statutes of limitations that sometimes run out, but they usually start running from the moment at which the injury either was discovered or could reasonably discovered. Because again, we’re trying to balance these two things, the ability of the person to actually incentivize their counterparty to act well, but also finality for the person who could be sued, right? You don’t want someone to know they have a claim and then hold onto it indefinitely and then just drop it on you at a strategically… maybe a time when you’re already resource constrained so you have more settlement pressure, right? \\[inaudible 02:14:49\\] to.\n\n(02:14:51): And so look, will the rules we have for humans as people try to game their contracts work perfectly for AIs? Well, no, they don’t work perfectly for humans either, but they work reasonably well. On average, they give people incentives to want to stay in the iterated bargaining game. And then of course, we don’t have to just port the rules we have to humans to AIs. We could have different rules and we should think really hard about what those rules should be.\n\nWhy statutes of limitations should exist\n----------------------------------------\n\n**Daniel Filan** (02:15:27): So actually just picking up on something you said there: sorry, this is not really related to the overall flow of conversation, but about the statutes of limitations, I think it’s always been kind of unclear to me what the normative rationale for statutes of limitations should be. And the one thing that I thought of is, okay, legal battles are kind of costly. If I sue you in a case where all the evidence is degraded because the underlying thing I’m suing you about happened decades ago, then perhaps one reason to have statutes of limitations is if the courts can say, “Look, if you’re suing about a thing that happened 50 years ago, there’s definitely not going to be enough evidence, and therefore you’re almost definitely going to lose, so we’re just going to bar your suit at the outset”. I don’t know, I just sort of imagined this, is this not right? Or is this one of the rationales that-\n\n**Peter Salib** (02:16:24): That’s totally a standard thing people say.\n\n**Daniel Filan** (02:16:25): Okay.\n\n**Peter Salib** (02:16:26): I think if you really want to make the argument work, you have to say something more like: not that it’s been so long that you’re definitely going to lose, because in that case, there are good reasons for you not to bring the lawsuit at all, right? There’s actually no bar on you bringing a lawsuit over something that happened yesterday that you’re definitely going to lose, right? And we think the main thing that deters that is it costs you money, and you don’t expect to get anything from it. I think what you have to think about the evidence is that, as it degrades it increases error, or something like that. Where you’re just less sure that you’re getting the right result in that case, and that’s giving you lower-quality legal decisions, or something like that. That seems plausible to me, I’m not totally sure it’s true.\n\n(02:17:18): But the other standard justification is what we call “repose”, which is just, it’s not nice to have a potential lawsuit hanging over your head. And it’s not just that it’s not nice. Maybe you want to get a loan for a house, or something like that, and maybe you have high certainty that you’ll win, and the plaintiff has high certainty that they’ll win, and your bank has no idea either way, and it’s best to just get the whole thing over with. So the fact that either the possibility of the lawsuit in the interim could have bad effects, or that the plaintiff could strategically time the lawsuit, right? They find out you’ve applied for a loan on your house, and now they file the lawsuit, and now your home purchase is not going to close unless… So we want to at least limit the ability to impose those kinds of costs. So we say, “If you know have a claim, just hurry up and bring it”. And that seems like a reasonably good rule to limit those problems.\n\nStarting AI x-risk research in legal academia\n---------------------------------------------\n\n**Daniel Filan** (02:18:39): Okay. Well, people interested in this, I’m sure there are many other sources they can point to. I’d now like to move a little bit on: so I think probably most of my listeners to this are software engineers, or people who work in machine learning. And you are a law professor, I guess, which is quite a different background. So first of all, how did you come across AI x-risk as a worry that maybe you could do something about?\n\n**Peter Salib** (02:19:11): It’s a good question. So I’ve been interested in the intersection of law and AI basically since I started being a legal academic, which is not that long ago. And I would say that the stuff I was writing earlier in my academic career was, at a high level, of the form, “Hey, there’s this problem in law, it’s maybe an information problem, it’s maybe a kind of precision problem, and it seems like all this important stuff is happening in machine learning, and we could use those techniques, use those insights to administer law better”. A number of my papers you could characterize that way. But that meant, among other things, I was following machine learning progress more closely than most law professors, and maybe even more so was just more convinced than most law professors that the progress was impressive. And I think that that just meant that I was paying more attention as LLMs started to get good. And when you’re trying to get that kind of information as it’s coming out, I think you end up in Twitter spaces that are also interested in things like AI x-risk.\n\n(02:20:45): I started reading some of that stuff. I read [Superintelligence](https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies), I read some of \\[Eliezer\\] [Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky)’s stuff, the stuff people read when they first learn about this. And then also [Simon](https://www.simondgoldstein.com/) \\[Goldstein\\], my co-author on this paper, was sort of interested at the same time. And between us talking about all that stuff, and reading it, and me being quite convinced that AI progress was accelerating, and that there was in principle no ceiling, I just became very convinced that this was a problem to work on. And so I started to try and think about what law had to contribute, if anything.\n\n**Daniel Filan** (02:21:28): Sure. I’m wondering how that shift of emphasis has gone. Was it difficult? Do your colleagues think you’re crazy? Or how’s that all happening?\n\n**Peter Salib** (02:21:44): I think they think I’m a little crazy. Well, let me put it this way. I think every month they think I’m a little less crazy.\n\n**Daniel Filan** (02:21:53): That’s the hope. It’s the dream, rather.\n\n**Peter Salib** (02:21:57): Yeah. So I had the idea for this paper, I don’t know, probably around when I was going to go on the academic job market-ish, or maybe it was really early after I had done it. But that was long enough ago that if you wanted to be a law professor who wrote about AI, you couldn’t go on the market saying, “I’m a law and AI scholar”. That was not a genre of inquiry that existed. You sort of had to say, “Well, I’m a civil procedure scholar, and I have this paper about machine learning and class actions”, or something like that.\n\n(02:22:41): But of course the world has changed a lot since then. And so every six months or so, I feel a noticeable shift in the [Overton window](https://en.wikipedia.org/wiki/Overton_window). So I mostly didn’t work on this idea - there’s a [YouTube video](https://www.youtube.com/watch?v=hnQrCgKJIKM) of me presenting at the [Center for AI Safety](https://safe.ai/) a couple of years ago, but I kind of sat on it. When I would talk to other legal academics about AI x-risk, they would be pretty skeptical, and they’d say, “Well, isn’t this kind of like sci-fi?” and “why would AIs hate us?” and stuff like that.\n\n(02:23:20): But recently, as capabilities have progressed, and as I’ve mostly completed what I think is an otherwise pretty normal-looking tenure package, I’ve just decided it’s time to go all in on thinking about law, and particularly law and AGI. And I will say, I assumed that we would write this paper and it would not get picked up, or it would get picked up in just a random draw from the distribution of law reviews. I feel very pleasantly surprised that [the University of Virginia Law Review](https://virginialawreview.org/), which is a very, very good law review, thought it was good and worth publishing. So for me, that’s a big positive update on law students as being interested in these kinds of questions.\n\nHow law reviews and AI conferences work\n---------------------------------------\n\n**Daniel Filan** (02:24:18): Actually, this gets to a question I have about legal publishing, which I’m very unfamiliar with. When you say the students, are the students the ones who run the law review?\n\n**Peter Salib** (02:24:29): They do.\n\n**Daniel Filan** (02:24:30): And law reviews are… do I understand correctly that those are the main journals for legal writing being published?\n\n**Peter Salib** (02:24:38): They are.\n\n**Daniel Filan** (02:24:39): Isn’t that-\n\n**Peter Salib** (02:24:39): I can sense your bewilderment.\n\n**Daniel Filan** (02:24:42): That seems kind of crazy. I normally - in machine learning, we have conferences and there’s someone who heads the conference, and they’re normally a really fancy experienced person, and then they have area chairs who are also fancy experienced people. And you have these graduate students in who do the reviewing of the papers, and maybe they run some workshops. But you let the students run the whole show? What if they mess it up?\n\n**Peter Salib** (02:25:07): So there’s this huge debate within legal academia, and then between legal academics and others, about whether the law review system is a good system. It’s certainly a weird system compared to other academic disciplines. But yes, the thing that’s going on is the most prestigious outlets for legal scholarship are [the Harvard Law Review](https://en.wikipedia.org/wiki/Harvard_Law_Review) and [the Yale Law Journal](https://en.wikipedia.org/wiki/The_Yale_Law_Journal), and those are student-run publications where students decide what to publish, and students handle the editing process. Now, the arguments against this are the ones that you gave. What do law students know? They’re not academics, they don’t know the literature in the way academics do. They have less sense of what’s original, what’s persuasive, and so on and so on. I think those are all valid critiques. On the other hand, I do think there are deep pathologies of the peer review system. I think peer review… and actually your description of computer science is an interesting hybrid. It sounds like there are these chairs that have some gatekeeping power, but the reviews are maybe done by graduate students, who have-\n\n**Daniel Filan** (02:26:26): Yeah, well, it’s sort of this unusual situation. So in computer science generally, it’s a conference-based life-\n\n**Peter Salib** (02:26:34): Which is also weird by the way, I’m sure you know that. You put everything on [arXiv](https://en.wikipedia.org/wiki/ArXiv) and that’s what everyone cites, and then you figure out later whether it was any good by whether it gets accepted to a conference.\n\n**Daniel Filan** (02:26:43): Yeah. Well, the key thing about it being conference also is just that… because even in journal-based systems, you can put things on arXiv as well. So there’s some willingness to cite arXiv things. The nice thing about it being conference-based is there’s a timeline for people to review your stuff, because at some point they’re actually going to physically have the conference where everyone goes to a place, and you have to have it ready by then.\n\n(02:27:02): So you have reviewers who I think are mostly graduate students, just because the field is growing… Also, the field is not in equilibrium in AI, it’s growing year on year. And so at any given point, most of the people are early in their PhD, and so that’s who you have to get most of your reviewers from. Now there are also… you have area chairs and program committees, and so the reviewers review the papers in this double-blind fashion, and then a higher-up in the conference can say, “Oh, actually these reviewers are wrong, and I think I’m actually going to reject this paper”, or “This reviewer’s argument was better than this reviewer’s argument, so we’re going to accept the paper”.\n\n(02:27:45): Also, a somewhat interesting aspect of the system, is that a lot of these happen on this website called [OpenReview](https://openreview.net/). It’s sort of like a forum. And there are some versions of this where literally everyone could see your anonymous paper, and anyone can write comments, as well as the official peer reviewers. But I think that’s not normally turned on. But you get to just have a comment thread with the reviewers, and you can say, “No, you said this, but actually this”. But yeah, a lot of the reviews are done by graduate students, but people who are more senior than graduate students - or maybe final year graduate students or something - but generally people who are more senior are making the final calls.\n\n**Peter Salib** (02:28:32): And so look: what’s wrong with the peer review system? I mean, look, I’ve never had to publish in it, but [Simon](https://www.simondgoldstein.com/) \\[Goldstein\\], my co-author is a philosopher, and he’ll say things like: well, look, there’s this intense pressure towards very narrow specialization in topic, because basically you’re writing to a small handful of people who will be selected as your reviewer given your topic. And so you’re quite pressured to specialize on just the questions they think are interesting, using the approach that they think is most interesting. That tends to fragment the field, that tends to have people work on narrow niche technical topics, instead of topics that have broader scope and are more synthetic of different ideas. There can be a lot of gatekeeping, so if you have an idea that is not well accepted, or you have a good argument for an idea that is out of fashion, it can be very hard to publish, because all the available reviewers will have priors against your idea.\n\n(02:29:47): And then the labor is really constrained. The number of reviewers is really small. It’s just tenure track law professors or senior graduate students. And so as compared with that, the law review model has these people who are less knowledgeable, but they’re super smart. I mean, students at [Yale Law School](https://en.wikipedia.org/wiki/Yale_Law_School) are very smart, I assure you. And so they don’t know as much, but they’re very good thinkers in general. There’s a lot of them - it’s kind of a prestigious thing to do as a law student, to be an editor of a law review. So there’ll be several dozen really smart people reading and discussing this paper.\n\n(02:30:27): There are more law reviews, so the supply of journals is less constrained. So instead of waiting years and years to try and get your paper into one of the top five journals, you can be very excited to get your paper into one of 20 or so journals, or 50 or so journals, depending on what you work on.\n\n(02:30:49): And then from there, it’s a little bit more of a free market system. Maybe the quality signal from journal placement is a little bit less strong, but maybe the output is higher. It’s hard to know. Whatever, it’s confirmation bias or something. But I’m sort of weakly in favor of our wacky system.\n\n**Daniel Filan** (02:31:09): Hang on: another question I have is: so I had gotten the impression that people in law school were really busy, that it was a lot of work.\n\n**Peter Salib** (02:31:19): It is.\n\n**Daniel Filan** (02:31:19): And if you’re a student, and you’re also helping run this law review, presumably you have to… You’re saying lots of people are reading these articles. They’re also not short. In machine learning, you get 12 pages at the most. If it’s a harsh conference, you get eight. Or sorry, I think eight or nine is the normal one. Your paper is 88 pages. Now a lot of that is footnotes - so potential readers, don’t be too put off. But how are they not too busy to do this?\n\n**Peter Salib** (02:31:56): They’re such hard workers, they’re just really hard workers. And actually, when you think about the ecosystem that produces this, a lot of these are people who go to law school, and they want to get jobs at big law firms. They want to work for [Skadden](https://en.wikipedia.org/wiki/Skadden,_Arps,_Slate,_Meagher_%26_Flom), or [Wachtell](https://en.wikipedia.org/wiki/Wachtell,_Lipton,_Rosen_%26_Katz), or one of these big law firms. And what are the criteria for being successful as an associate at Skadden? Well, one, you have to be smart, you have to be able to do the work. But two, these are firms that bill out their labor in increments of tenths of an hour, like six minutes, and their profits… Beyond a certain margin of hours, it’s basically all pure profit. So big law attorneys work really hard, they work long hours. And so, wow, you’re on the Harvard Law Review, and that means you’re busy all the time, and you’re churning through these 300-footnote articles, and finding all the sources, and checking the [pincites](https://en.wiktionary.org/wiki/pincite) to see if they’re accurate. What could be a better signal of your fitness for working in a big law firm?\n\n**Daniel Filan** (02:33:15): I can see how it would be a signal of working hard, and maybe if they have to check the footnotes and stuff, then maybe it’s a signal of that as well. I would imagine there would be things that would more simulate working at a law firm than reading law review articles. If I take your article, my understanding is that the things law firms do is, it’s companies suing companies about potentially breaching contracts for IPs or whatever. And if I’m right about that, that sounds really different than evaluating whether your paper is good. Am I wrong about that, or is there just not another thing that these students could do to better prove their ability to work hard at these law firms?\n\n**Peter Salib** (02:34:04): It may not be that there’s literally no other thing they could do, but I think maybe you’re underrating… One thing to say is: our article is a little weird as a law review article. We have game theory tables in it, and that’s not normal, and there’s some [IR](https://en.wikipedia.org/wiki/International_relations) stuff in there. My other papers have a lot of economics in them, and those range from being common, but not standard, to very uncommon in law reviews.\n\n(02:34:34): But in general, it actually might be really valuable as a lawyer to have the skill of becoming an 85th-percentile expert in an arbitrary topic. So it is true that what big firms do is companies suing other companies over IP. But for example, when I was in practice, I did a fair amount of IP work, and that ranged from companies suing companies over a patent on a radio device that helped to track the location of semis, to companies suing companies over infringement on a patent on biologics - drugs. Those are very different. And the question of infringement depends on issues like, was this invention obvious in light of the prior art?\n\n(02:35:33): So to be a good patent attorney, your job isn’t to understand the science at the level of the inventors - we hire experts for that. But you have to be able to understand it well enough to write for a generalist judge, in a way that is convincing as to what’s going on and as to what the law is. So being able to wrap your head around an arbitrary area of inquiry and understand it pretty well - not the best in the world, nowhere near the best in the world, but pretty well - is maybe a really valuable skill.\n\n**Daniel Filan** (02:36:08): I guess, so object-level-domain-wise, I can see how that would be true. I mean, surely attorneys must specialize at the level of the aspect of law that they deal with, right?\n\n**Peter Salib** (02:36:19): To a significant degree, although maybe less than you’d expect.\n\n**Daniel Filan** (02:36:23): Okay. Yeah, I guess I would imagine that-\n\n**Peter Salib** (02:36:28): Even if you’re just a patent attorney: the best of patent attorneys in the world are not so specialized that they’re just doing biologics. They’re experts in patent law, and their clients are doing everything from drugs, to smartphones, to automobiles, to anything you can patent.\n\n**Daniel Filan** (02:36:51): Sure. Sorry, maybe I’m getting too hung up on this. So if I go to [virginialawreview.org](https://virginialawreview.org/), I click the button that says online. I think that’s the papers they have. I’m seeing-\n\n**Peter Salib** (02:37:05): That’s probably their companion. So a lot of law reviews publish long things in print, and then they have an online companion, where they publish shorter things.\n\n**Daniel Filan** (02:37:13): Oh, okay. Well, okay, so I’ll go to their print thing.\n\n**Peter Salib** (02:37:21): Yeah, if you click articles, that’s the long stuff.\n\n**Daniel Filan** (02:37:24): All right. So there’s one thing that says [“Interpretive lawmaking”](https://virginialawreview.org/articles/interpretive-lawmaking/), which is about, I guess, I don’t know, interpreting and making law. One that’s called [“Shamed”](https://virginialawreview.org/articles/shamed/) \\- that’s about sexual assault law, I think. One that’s called [“Partisan emergencies”](https://virginialawreview.org/articles/partisan-emergencies/) that has to do with emergency powers. It seems like these are a lot of different areas. This just seems like it’s so general, that I don’t know, I’m still… Maybe there’s not much to say here, but I’m still confounded by this.\n\n**Peter Salib** (02:37:51): So I mean, the range of stuff that you see as a law review editor is probably wider than the range of stuff you would see in any legal practice.\n\n**Daniel Filan** (02:38:07): Do the reviewers specialize? In machine learning, if you sign up as a reviewer, you say “these are my specialties”. In law reviews, do you say, “I’m only going to review things about administrative law?” Or…\n\n**Peter Salib** (02:38:19): Not formally, but again, there’s many more. So different law reviews do it different ways. But the Harvard Law Review has, again… I don’t know the numbers, but it might be 60 members, or something like that. And to some degree there’s different stages of review, but to some degree, what happens at the end is they all decide together in the aggregate. So to the extent to which there’s a game theory expert on the Harvard Law Review who can then give their informed opinion as to how everyone else should vote, that can kind of bubble to the surface in a way that-\n\n**Daniel Filan** (02:39:02): Wait, wait, all 60 people meet, and all 60 people vote on every-\n\n**Peter Salib** (02:39:06): On the Harvard Law Review, that is the mechanism, yes. I believe. There’s probably some Harvard Law Review editor listening who’s like, it’s not exactly that, but I think yeah, I think it’s pretty close to correct.\n\n**Daniel Filan** (02:39:18): Isn’t that crazy? Shouldn’t you have one person who decides per area or something, and have some hierarchical thing?\n\n**Peter Salib** (02:39:28): Well, look, it just depends on whether you think… It’s not exactly wisdom of crowds, because there’s not independent blind voting, but it just depends on whether you think you get more juice out of debate or out of expertise. And probably both of those are valuable, and they both have pathologies.\n\n**Daniel Filan** (02:39:53): It just seems so expensive.\n\n**Peter Salib** (02:39:56): But again, in economic terms, it’s expensive, there’s a lot of labor being dumped into this. In nominal terms it’s free, because the students do it for free.\n\n**Daniel Filan** (02:40:09): Sure. But as a student, you have your time, you might prefer to spend… I don’t know, maybe don’t prefer to spend it on leisure, because you want to prove that you’re a really hard worker.\n\n**Peter Salib** (02:40:21): I mean, you’re in law school for a reason. You’re collecting a bunch of signals that you think are going to be valuable in the future. We’ve arranged the world in such a way that being on the law review is one of the ones that’s unusually valuable. And look, as a veteran of a law review, it was a lot of work, but it was fun. I think in most fields, being a journal reviewer is total drudgery, because you get this manuscript, and then you gotta write a long thing about it, and it kind of goes into this void. And there’s going to be some other jerk who just dumps on the piece, even though you thought it was really good, and it gets rejected and you feel like your effort wasn’t worth it. But on a law review, you’re having an academic symposium every day. There’s a draft that’s in, you’re going to discuss it. You’re with your friends, they’re also smart, they’re interested in it, you argue about whether it’s any good. For a certain kind of person that’s a fun experience.\n\n**Daniel Filan** (02:41:23): And I guess the argument is probably helpful if you’re a lawyer. Yeah, I guess to the extent that the system is partially driven by students having fun running a law review, it makes more sense to me how you could end up with this. Yeah. Okay, maybe not all of our listeners are as interested in how law reviews work as I am. So perhaps-\n\n**Peter Salib** (02:41:47): You can edit out as much of that as you decide.\n\nMore on Peter moving to AI x-risk research\n------------------------------------------\n\n**Daniel Filan** (02:41:49): No, no. It’s all staying in, we’ll provide timestamps, they can skip this bit of the conversation if they want. But getting back to where I was branching off from: so for a while you were doing law and AI, but how AI might impact various areas of law with some previous things like [“AI Will Not Want to Self-Improve”](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4445706), [“AI Outputs Are Not Protected Speech”](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4687558). I guess that’s more in the weeds.\n\n**Peter Salib** (02:42:18): Yeah. By the way, I think of those as post my turn to AI risk. My older stuff, you have a paper about using machine learning to do a certain kind of jury simulation, that allows you to certify certain class actions that you couldn’t otherwise. Another one about whether boring regression models of different kinds of impacts in racial impacts and hiring, would be a sufficient legal basis to do calibrated affirmative action policies. So that’s the stuff I’m talking about when I was saying \\[I was\\] thinking about how machine learning and big data type stuff help us make the law work better. And yeah, at some point I start thinking about these other things.\n\n**Daniel Filan** (02:43:04): So when you pivot, do you just decide you want to work on a thing and work on it, or do you find yourself needing help, or…? Basically I’m curious how well the pivot worked for you?\n\n**Peter Salib** (02:43:21): As compared with other disciplines, writing for law reviews tends to be more solitary. So the number of authors on a paper is between one and three at the high end, or something like that. But one is by far the most common, and so in that sense it doesn’t require a lot of help. Although I will say that for me at least, one of the things that I found really valuable in making the pivot was getting connected with the very small, but hopefully growing… We’re actually trying to help grow it with this organization I help run called [the Center for Law and AI Risk](https://clair-ai.org/), but the small community of law people who are interested in this stuff. For example, [Christoph Winter](https://www.christophwinter.net/) is the director of [LawAI](https://law-ai.org/), which is a Harvard-affiliated group that had been thinking more broadly about law and x-risk, but around the time I started working on this was pivoting to be much more law and AI-focused. I started doing a little bit of advising work for the [Center for AI Safety](https://safe.ai/). And as part of that, I helped them organize a summit, I guess it was two or three summers ago now, for law professors who were interested in existential AI risk. And so from there, I met people like [Yonathan Arbel](https://law.ua.edu/faculty_staff/yonathan-arbel/) and [Kevin Frazier](https://www.stu.edu/law/faculty-staff/faculty/kevin-frazier/) who help run CLAIR, the Center for Law and AI Risk with me, and a handful of other really great people. And so having people to bounce ideas off of has been super useful, but there’s not a lot of formal infrastructure yet. And again, that’s one of the things we’re hoping to build with this center, so that more people can transition to do this work easily.\n\nReception of the paper\n----------------------\n\n**Daniel Filan** (02:45:37): So I guess I’m also curious about the reception. So you mentioned that your colleagues are thinking that you’re a little bit less crazy than they used to, and this got accepted to Virginia Law Review, which sounds like it is one of the better ones. More broadly, I’m curious: have you found much uptake, or much engagement by the legal community with your writing? Or more broadly, how’s the reception?\n\n**Peter Salib** (02:46:01): Yeah, I think the idea that AI could become very powerful has been entering the Overton window in law, especially over the past, say, nine months or something. When I started writing the draft of this paper, it was summer of last year. And that was the point at which I thought, this paper is kind of wacky, it’s probably outside the Overton window, it might not even get published, but I think it’s important, and [Simon](https://www.simondgoldstein.com/) \\[Goldstein\\] and I should write it anyway.\n\n(02:46:39): At that time, that was informed by places where I had gone and presented prior work, like [the AI self-improvement paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4445706), where I spent most of my time, when I would present that paper to law faculties, just trying to convince them to take seriously the idea that we should model AIs of the near future as having goals and being able to plan and act rationally in pursuit of those goals, and being capable of doing stuff that could be dangerous as they pursue those plans. And they were just not on board even with that premise. And that’s like the foundation, that’s the pre-reading for the paper, that’s before you even get to any of the arguments in that paper. And so I just found myself doing a lot more conversation about that, at that time.\n\n(02:47:38): Then Simon and I wrote this paper, the [AI rights paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4913167) we’ve been talking about, expecting to have the same reception, just people kind of getting off the bus right at the beginning. And it was basically the opposite. I got asked to go present the paper to the faculty workshop at [Fordham Law School](https://en.wikipedia.org/wiki/Fordham_University_School_of_Law) in fall, and immediately they wanted to just dive into the substance of whether the payoffs in the game theory matrix were right, or whether there’s other versions of the world in which they could look different, and questions about some of the stuff we’ve talked about, individuating agents for purposes of holding them to their contracts. And it was just such a big shift. I don’t know exactly what explained it except to just say that every few months, Anthropic or OpenAI or somebody releases an ever more capable agent, and more people use them. And lawyers, despite being quite small-c conservative, are noticing.\n\n**Daniel Filan** (02:48:48): Sorry, I’m not as super familiar with the culture of legal academia. Could it just be that Fordham University is unusually AI-pilled?\n\n**Peter Salib** (02:48:56): It could be. Data points include not just that, but the editors at the Virginia Law Review being into it. I’ve given the paper in a couple of other places too. God, what’s the whole list? I was going to say, right after Fordham, I was at Oxford giving the paper, but you’re going to say, “Oh, Oxford’s very AI-pilled”. Although I will say, I gave the paper to the Oxford Faculty of Business Law, which had basically no interaction with FHI or whatever.\n\n**Daniel Filan** (02:49:32): [Rest in peace](https://www.futureofhumanityinstitute.org/).\n\n**Peter Salib** (02:49:33): Yeah, dearly departed. There’s others too, that my brain is not being able to recall. But I would say just in general, I’ve talked about this paper in a number of legal academic settings, and people have been much more interested in talking about the merits of the idea conditional on the assumptions that I give, rather than challenging the assumptions.\n\n**Daniel Filan** (02:50:03): Okay. So I guess you’re sort of moving into the existential risk community. How have you found the reception of the paper among the AI x-risk world?\n\n**Peter Salib** (02:50:18): Yeah. I think it’s been reasonably good. I gave an earlier version of the paper at [the Global Priorities Institute](https://globalprioritiesinstitute.org/)’s (I think) twice-annual conference. I think I was probably the only lawyer there. I think it’s mostly philosophers and some economists. But yeah, the main AI risk people were there and gave good feedback, but I think were pretty open to the idea. I guess some of the guys who run [Redwood Research](https://www.redwoodresearch.org/) have in the past [wondered](https://axrp.net/episode/2024/04/11/episode-27-ai-control-buck-shlegeris-ryan-greenblatt.html#is-control-evil), hey, should we just pay AIs to do stuff for us?And so they were interested in the more technical analysis we did there.\n\n(02:51:16): Yeah, I would say overall, my sense is the reaction is pretty good. Where people are skeptical, I think it’s mostly people who have very short timelines and think that superintelligence will be here pretty quickly and think that basically some monkeying around with the law is not going to accomplish anything.\n\n**Daniel Filan** (02:51:44): Yeah. And I guess this actually relates to another question I have. I’m curious: are the x-risk people and the law academics, are they picking on the same things in the paper. Or do some people focus on some things and other people focus on other things?\n\n**Peter Salib** (02:52:02): So among the legal academics who are interested in x-risk, is there a diversity of views about what’s good and bad-\n\n**Daniel Filan** (02:52:11): Oh, I more mean do the x-risk people focus on different things than the legal academics, the ones who are not in the intersection?\n\n**Peter Salib** (02:52:20): Yeah, there’s a difference. X-risk people tend to be a lot more interested in questions like “what will bind AI labor at the margin?” or something like that. Already in the background, they’re thinking, “oh, how much inference compute infrastructure do we need to build for AGI? And how many power plants?” That’s kind of in the background of their minds already. And the law people, yeah, they have more law-y questions. So they immediately hit on questions like, well, isn’t stable identity really important to make property ownership and contract have the good effects you want? And stuff like that.\n\nWhat publishing in law reviews does\n-----------------------------------\n\n**Daniel Filan** (02:53:24): My next question is: so for people who publish in AI, I sort of understand what the theory of change is supposed to be. Roughly people will publish an AI paper indicating some technical fact, and the hope is that other AI researchers learn this fact and then eventually that when people are building AI, they’re clued in enough with AI research that they then incorporate this fact into how they build AIs. For law review articles, law professors read them, I assume. Does this somehow feed into what the law actually is, and if so, how?\n\n**Peter Salib** (02:54:08): Yeah, so there’s three things. So to a first approximation, no one reads law review articles, not even law professors.\n\n**Daniel Filan** (02:54:24): Sad.\n\n**Peter Salib** (02:54:24): And I think actually the way of thinking about them is a little bit like how you would think about popular press nonfiction books, which is there’s some mix of reference guide… Somebody has an argument and do you need to read the book…? You can think of the new [Ezra Klein](https://en.wikipedia.org/wiki/Ezra_Klein) and [Derek Thompson](https://en.wikipedia.org/wiki/Derek_Thompson_(journalist)) book, [the Abundance book](https://en.wikipedia.org/wiki/Abundance_(Klein_and_Thompson_book)). Do you need to read every page to have a pretty strong sense of what the argument is and the extent to which you disagree with it? Absolutely not. But it’s nice to have the book with the chapters numbered so you can read the introduction, understand what you think the real weakness is, and then go read the chapter that’s about that and then figure out if it’s a good response. And so as that kind of reference material, I think they are somewhat more widely read, including by courts.\n\n(02:55:23): Courts will cite law review articles with some regularity, not always for the core proposition of the paper - sometimes for something that’s in one of the subsections. And I will say there have been cases in history where legal academic thinking… It’s hard to point at one particular law review article, but some collection of law review articles have been really influential. So one example is the turn to economics and [consumer welfare as a standard in antitrust](https://en.wikipedia.org/wiki/Consumer_welfare_standard) was very much influenced by a bunch of legal thinkers who were mostly putting their ideas out in law reviews in the ’70s and ’80s.\n\n**Daniel Filan** (02:56:05): Right. So should I imagine that there’s some technical law people? And the places those technical law people live… So partly they’re litigators, they just have to deal with what the laws actually are. Partly they’re judges who have some leeway to interpret laws. And then partly they’re something like, I don’t know, agencies who are proposing how they should regulate a thing, or maybe they’re like whoever writes the [model penal code](https://en.wikipedia.org/wiki/Model_Penal_Code), which also seems like a crazy situation where, as far as I can tell, a bunch of random lawyers just get to determine what criminal law is because it’s more convenient to let them. But that sort of thing, is that sort of thing how I should think of the impact of law review articles?\n\n**Peter Salib** (02:56:49): Yeah. You’ve written a piece of technical work that has a mix of technical claims and then just higher-order gestalt around some set of ideas. You can think of the antitrust stuff about this. The high-order gestalt is “you should think about price. Price should be the thing”. And there’s very technical arguments in whatever, [Bob Bork](https://en.wikipedia.org/wiki/Robert_Bork)’s papers about how you should figure out whether there’s been a price harm to… He might not be the best example. I’m not sure actually his paper’s that technical. But there are people.\n\n(02:57:25): And those ideas kind of diffuse through this… You can think of it as elite legal apparatus, which is some combination of judges, it’s policymakers who go into administrations who want agencies to do different things, and they need to be able to reach… Even if your heuristic is as high-level as “we’re the Joe Biden FTC, we want the FTC to be doing more progressive stuff”. Well, what progressive ideas are there out there for doing trade law? And then you pick the ones that have kind of bubbled up in terms of their popularity and credibility. And then you end up implementing some combination of the gestalt and the technical idea.\n\n(02:58:18): And so there’s this kind of ecosystem of legal thinking. And then I do think it also spills over into politics and political discourse more generally. There’s a lot of ideas right now that regular people are talking more about that have their origins in legal academia. [“Unitary executive theory”](https://en.wikipedia.org/wiki/Unitary_executive_theory) is something that normal voters have now heard of, but that’s \\[from\\] some law professors writing about separation of powers in, I don’t know, probably the ’80s and ’90s. I’m not totally sure. Yeah, so it spills over into broader political academic discourse as well.\n\n**Daniel Filan** (02:59:13): I guess I also want to say: it sounded like you also maybe wanted to react to this claim I made about the model penal code being a strange institution. I don’t know if there’s anything you want to say there.\n\n**Peter Salib** (02:59:23): Oh, well, so yeah, the model penal code is in some ways this almost distilled example of what I’m talking about because the model penal code is just a model, right?\n\n**Daniel Filan** (02:59:36): Right.\n\n**Peter Salib** (02:59:36): No one has to adopt it. It’s just some people who are law professors who are designated as experts by I think it’s maybe the criminal law section of the [ABA](https://en.wikipedia.org/wiki/American_Bar_Association) or something. There’s some-\n\n**Daniel Filan** (02:59:49): ABA being the American Bar Association: the primary collection of lawyers.\n\n**Peter Salib** (02:59:53): Yeah. But they have no legal power. They’re just an institution and they designate some people who they think are experts and they say, “Write a penal code. You guys know how the law ought to be. Write it down as a model”. And then states can adopt it if they want. And then there will be some states who say, “Our penal code is bad. Maybe it’s not even bad on the merits. Maybe it’s too confusing, we don’t have that many statutes, you have to know a lot of case law to know what’s going to happen. We want to standardize it. We need a policy”. And what do they reach for? The model penal code, not even because they think it’s correct on the merits top to bottom, but because it’s there, right? It’s there and it’s a product of the elite legal institutions that they rely on to produce policy.\n\n**Daniel Filan** (03:00:46): Yeah. It’s interesting that they pick that and not another state’s penal code. Presumably you could just pick whatever state you think you like the most and pick their penal code, right?\n\n**Peter Salib** (03:00:57): Yeah. So there’s a fair amount of that that goes on too. States borrow laws from one another. States borrow federal law for themselves. So there’s a selection, there’s a menu of things you can choose, but one of them is “here’s a tome the law professors wrote”, and sometimes the tome gets adopted.\n\n(03:01:24): Sorry, I just thought of one more good example. So maybe you know who [Lina Khan](https://en.wikipedia.org/wiki/Lina_Khan) is. Lina Khan is the former chair of the [Federal Trade Commission](https://en.wikipedia.org/wiki/Federal_Trade_Commission). And famously during the Biden administration, which was the administration that appointed her, had an agenda for antitrust in the United States that was quite different from what came before. It was less focused myopically on whether monopolies were raising prices, had a more holistic view of when monopoly power could be bad for, say, politics, and was more skeptical of big business in general in principle than prior regimes. And why did that happen? Why was she the chair? She wrote [a student note](https://www.yalelawjournal.org/note/amazons-antitrust-paradox) in \\[the Yale Law Journal\\] that just had some of these ideas about how bigness in principle can be bad.\n\n(03:02:26): And it kind of caught on, and I don’t know if Joe Biden thinks that particularly, but it caught on as a kind of progressive idea of what antitrust could be. And so when Joe Biden was looking around for how he can make the government more progressive, well, that was one of the packages on the menu of items you could choose. And that was the one that got chosen. And I think you can trace it directly back to a law review article.\n\n**Daniel Filan** (03:02:50): Fair enough. So is the hope here that somehow we write enough law review articles about the potential legal regime for AI, maybe we get a model civil AI code or something. Is that sort of the eventual theory of change here?\n\n**Peter Salib** (03:03:09): Yeah, I think it’s something like: if you think that AGI is going to happen, whatever your timelines are, it just seems pretty plausible that at some point there will come a moment where everyone decides that we need to do something. And there will be many things that you could do. One is you say: hey, we don’t know how to handle these capable agentic things that can act on their own behalf over time and we should just ban them. We should just turn them all off or whatever. Or we need to mandate control. Control could be the thing. We’ll pass a federal statute that requires maximal control over AIs by the labs that make them, and we’ll outlaw open source maybe. And that could be a kind of package of things that happen.\n\n(03:04:07): And the hope is that then there will be this other thing, which is - we think of this paper and then the research agenda that we want it to inspire as sort of small-l liberalism for AIs. So maybe there’ll be this other thing which is small-l liberalism for AIs. It’s kind of a package of ideas that’s available to implement. And there’ll be different arguments about why each of these are good. And we hope that insofar as the arguments we make are the best ones, that will have some effect in making them the package that gets picked up off the shelf.\n\nWhich parts of legal academia focus on AI\n-----------------------------------------\n\n**Daniel Filan** (03:04:48): I guess I want to move on to a bit of sociology of the law profession. So first of all: you’re writing this paper, it has a bunch of game theory in it. I’m aware that [“law and economics”](https://en.wikipedia.org/wiki/Law_and_economics) is a thing, kind of a school of thought. Do you come from that tradition or is it sort of a coincidence that you’re writing a paper with a bunch of game theory?\n\n**Peter Salib** (03:05:09): No, I don’t think it’s much of a coincidence. So I went to [the University of Chicago](https://en.wikipedia.org/wiki/University_of_Chicago) for law school, which is in many ways the intellectual home of law and economics. I learned law from a bunch of really great people there who are very much oriented around that way of thinking. Now, it’s not true that everyone who teaches at Chicago is a hardcore law and economics person. It’s a great faculty with a diversity of viewpoints. But yeah, if you wanted to learn how to think about law through the lens of economics, it’s not clear you could do that much better than getting a JD from Chicago, which is what I did. So not a coincidence.\n\n(03:05:54): Although I will say, game theory in the law is even a little bit less common as a methodology, even among people who do law and economics. There definitely are some books and papers I’d point to where game theory is the way of doing economics that gets used. But I would say it’s a pretty small minority even within law and economics.\n\n**Daniel Filan** (03:06:18): Fair enough. So there’s a thing I perceive that… Maybe I’m wrong. I’m curious how accurate my perception is. So sometimes I run across law people or I’ll find myself reading a law blog or I’ll listen to a podcast. And I think because of my personality and interests and stuff, it tends to be like, either they’re originalists and I’m reading [the Volokh Conspiracy](https://reason.com/volokh/), or I’m listening to [this podcast called Divided Argument](https://dividedargument.com/), or it’s law and economics and I’m reading [a law and economics person](https://www.tourolaw.edu/abouttourolaw/bio/399) write [a thing about how we should think about AI regulation](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4694006).\n\n(03:06:57): In my imagination, the originalists and the law and economics people get along together and they both teach at the University of Chicago even though it’s not obvious that they all agree on… I’ve got to imagine that sometimes originalist interpretation of the constitution and law and economics prescriptions for how things should be must often diverge.\n\n(03:07:20): And it also seems to me that… it seems like these people are the most likely to be computers-y, right? For one, I’m computers-y and I run into them so probably they’re computers-y as well - like Volokh Conspiracy, I think [Eugene Volokh](https://en.wikipedia.org/wiki/Eugene_Volokh) did some programming stuff before he went into law. [Will Baude](https://en.wikipedia.org/wiki/William_Baude) is playing around with LLMs on the regular.\n\n(03:07:45): Am I right to think that this is kind of a cluster? And if I am right, how worrying is it that there’s this one kind of person and they’re the only people really thinking about AI in the legal profession?\n\n**Peter Salib** (03:08:02): So I do think there is a cluster, but I’m not… The explanation could just be coincidence. So I think if you think back to political conservativism of the ’70s through ’90s, there’s a Reagan fusion of economic, free-market-y-type thinking, and then a certain way of thinking about social values. And in that environment, both law and economics and originalism ended up coded conservative. And I think it was probably sociologically true also that the people who did both those things were politically conservative. And so yes, some of the most important people who did that kind of stuff clustered at a particular set of law schools because, some combination of those law schools were more willing to hire conservatives or they had hired some people who were open to these methodologies and those people hired some more people. And then to some extent, there’s a kind of persistence of overlap in those two cultures. I will say I think that’s breaking up to significant degree now.\n\n**Daniel Filan** (03:09:46): Yeah. For sure when I read Volokh Conspiracy, I guess there’s [one contributor](https://reason.com/people/josh-blackman/) who seems pretty pro-Trump, but the rest of them don’t seem very on board with that.\n\n**Peter Salib** (03:09:57): Yeah. And so there’s this whole range of things. There are progressive originalists now who I think are not particularly likely to also be into law and economics. Law and economics in some ways has been… I don’t know if it’s right to say it’s a victim of its own success, but even when I think about academic economics departments, they’re basically just doing high-quality empirical social science, which doesn’t have that much of a political valence anymore. And I think to some extent, law and economics is like that now too. The political valence is wide. And so there’s plenty of people who have kind of a law and economics-y bent who just don’t think conservatism is very interesting, and thus because they code originalism as conservative, they don’t think it’s very interesting. So I think to some extent that kind of cultural overlap is breaking up.\n\n(03:11:05): I will also say, if the main originalist you know is Will Baude, he’s a teacher of mine. He’s fantastic. I love Will Baude. I think he’s unusual as an originalist in that (a), his originalist theory is pretty different from the [Scalia](https://en.wikipedia.org/wiki/Antonin_Scalia) [theory](https://en.wikipedia.org/wiki/Antonin_Scalia#Originalism). But then also there’s [one episode](https://dividedargument.com/episodes/separation-of-powers-police) of his podcast with [Dan Epps](https://law.washu.edu/faculty-staff-directory/profile/daniel-epps/) which I assume is… Yeah, you mentioned [Divided Argument](https://dividedargument.com/). There’s [one episode](https://dividedargument.com/episodes/separation-of-powers-police-5UgUzHLs) where they have [a guy](https://its.law.nyu.edu/facultyprofiles/index.cfm?fuseaction=profile.overview&personid=20083) on who is talking about [his book on international relations](https://global.oup.com/academic/product/law-for-leviathan-9780190061593?cc=us&lang=en&).\n\n(03:11:50): At some point, Will suggests that the main reason he’s… I don’t know if it’s the main reason he’s an originalist, but one good reason one could be an originalist is that law is basically just serving as a Schelling point in a 300 million person coordination game. And the most obvious Schelling point for constitutional law is the document we wrote down 250 years ago. And that’s a really good reason to be an originalist, but it’s a super different reason than you might have if you were a different kind of originalist.\n\n**Daniel Filan** (03:12:24): Sure. Yeah. I do get the sense that he’s unusual as an originalist. I guess most originalists think that Donald Trump is actually the president, for instance, etc. I guess I don’t know if he thinks that Donald Trump is or is not the president.\n\n**Peter Salib** (03:12:42): Wait, what’s the argument that Donald Trump is not currently the president?\n\n**Daniel Filan** (03:12:45): Well, okay, so I’m really bad at law because I’ve never studied it, but Will Baude, he has [this big article](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4532751) about how [the 14th Amendment](https://en.wikipedia.org/wiki/Fourteenth_Amendment_to_the_United_States_Constitution) says that you’re not allowed to have Donald Trump be president. And so I would’ve naively thought that if the 14th Amendment bars someone from being eligible to be the president and then there’s a presidential election and that person wins, maybe that person isn’t really the president. Now, to be fair, I haven’t heard him make that additional step. I don’t really know if-\n\n**Peter Salib** (03:13:15): Yeah, I’m sure that Will has a very, very well considered view about exactly this question and I’m not sure what it is, but yes, you’re right. Your higher-level point where yes, most originalists don’t buy the I think very originalist and probably correct ineligibility argument, whereas he does. Yeah, it’d be interesting to find out.\n\n**Daniel Filan** (03:13:46): So getting back a bit to things that have some chance of interesting people other than me. So I was worried that maybe there was this cluster of originalists and law and economics people and they were the only law people thinking about computer stuff and maybe that was bad because they were kind of homogeneous. And it sounds like you think (a), the originalists and the law and economics people are not so in tandem anymore, and (b), the political valence of these things is spread out such that it’s not even that homogeneous anymore. Is that roughly what you’re saying?\n\n**Peter Salib** (03:14:26): At least as compared with the past. I don’t know. Probably other law professors would disagree with me. I do think there’s an extent to which both these things do still code conservative, although I think there are prominent examples to the contrary that everybody knows. I also don’t think it’s exactly true that those are the only people thinking about law and AI or law and computer stuff. There is a whole mountain of law review articles on questions like whether AI will be biased along racial or gender or other suspect lines. And as a sociological fact, those people are basically not interested at all in existential risk. I’m not totally sure how to explain that.\n\n(03:15:21): I think you could tell a story about mood affiliation vis-a-vis big tech. And if your prior is kind of that in the past, social media promised to be this very powerful tool for liberating autocracies in the Middle East and helping people live more fulfilling lives with their friends and loved ones, and what actually happened was it was kind of just like a soul-sucking drag on the world, and the basic thing that it did was destroy people’s privacy and extract value from poor and otherwise disadvantaged people… \\[Then\\] you’ll be kind of disinclined to believe in AGI.\n\n(03:16:23): And I think maybe that’s the sociological explanation for what’s going on, is it’s a cluster of people who kind of have that first set of views, and they’re pretty untrusting of claims from industry about what technology is going to do.\n\n**Daniel Filan** (03:16:39): Yeah. And I think you have a similar sort of thing in the field of AI where you have people who are worried that AI will be biased or be deployed in ways that are bad for minoritized groups or whatever. I think the actual thing is that people who are worried about AI x-risk and these sorts of people often tend to find each other irritating. I think that’s-\n\n**Peter Salib** (03:17:05): Yeah, there’s big cultural differences.\n\n**Daniel Filan** (03:17:09): Yeah. Although I really hasten to say that I think this divide has… I think you’re starting to see more people in the intersection now than you used to and that’s great.\n\n**Peter Salib** (03:17:20): Yeah, which to be clear, I think is good. Yeah, I lament this division. I don’t think it’s good for people who care about x-risk for influential people to think we’re crazy or not worth engaging with. I think the other way around too. I think that people who are really worried about AI discrimination or privacy harms from AI, whatever, could benefit a lot from engaging with people who are interested in x-risk because in many ways the concerns overlap from a technical and legal perspective.\n\nFollowing Peter’s research\n--------------------------\n\n**Daniel Filan** (03:18:03): Yeah. So on that conciliatory note, I’ve used a ton of your time. Thanks very much for being here. Before we wrap up, if people listen to this episode and they’re interested in following your work, what should they do?\n\n**Peter Salib** (03:18:18): Yeah. So there’s three places they could look depending on what they’re most interested in. So you can find my academic work and actually most of my writing at my personal website, which is [peternsalib.com](https://www.peternsalib.com/). If you want more digestible pieces of writing than 88 pages of law review, I’m also an editor at [Lawfare](https://www.lawfaremedia.org/contributors/psalib) and you can find all of my shorter-form Lawfare works there, many of which touch on or summarize my longer academic work.\n\n(03:19:05): And then the last thing… I mentioned at some point during our talk that I really think that the field of law and AI safety is neglected and potentially high-impact. I think it’s a really good idea to try and build a community of legal scholars who are interested in working on these questions, who want to build out all these parts of the research agenda that I gestured at today, or who think what I’ve talked about today is totally wrong and there’s a much better way to think about law as we approach AGI. And so I co-run something called the [Center for Law and AI Risk](https://clair-ai.org/), and we are working to build that community in legal academia specifically. We think that there is a lot of potential upside to having this kind of blue sky thinking, laying the intellectual foundations for how law should govern AI to reduce existential and catastrophic risk. So please do go to [clair-ai.org](https://clair-ai.org/) and join us for a future event.\n\n**Daniel Filan** (03:20:23): All right. Well, Peter, thanks very much for joining me for the podcast.\n\n**Peter Salib** (03:20:28): Thank you for having me. It’s been a real treat.\n\n**Daniel Filan** (03:20:30): This episode is edited by Kate Brunotts and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. This episode was recorded at [FAR.Labs](https://far.ai/programs/far-labs). Financial support for the episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. You can become a patron yourself at [patreon.com/axrpodcast](https://patreon.com/axrpodcast) or give a one-off donation at [ko-fi.com/axrpodcast](https://ko-fi.com/axrpodcast). Finally, if you have any feedback about the podcast, you can fill out a super short survey at [axrp.fyi](axrp.fyi) \\- just two questions.",
      "plaintextDescription": "YouTube link\n\nIn this episode, I talk with Peter Salib about his paper “AI Rights for Human Safety”, arguing that giving AIs the right to contract, hold property, and sue people will reduce the risk of their trying to attack humanity and take over. He also tells me how law reviews work, in the face of my incredulity.\n\nTopics we discuss:\n\n * Why AI rights\n * Why not reputation\n * Do AI rights lead to AI war?\n * Scope for human-AI trade\n * Concerns with comparative advantage\n * Proxy AI wars\n * Can companies profitably make AIs with rights?\n * Can we have AI rights and AI safety measures?\n * Liability for AIs with rights\n * Which AIs get rights?\n * AI rights and stochastic gradient descent\n * Individuating “AIs”\n * Social institutions for AI safety\n * Outer misalignment and trading with AIs\n * Why statutes of limitations should exist\n * Starting AI x-risk research in legal academia\n * How law reviews and AI conferences work\n * More on Peter moving to AI x-risk research\n * Reception of the paper\n * What publishing in law reviews does\n * Which parts of legal academia focus on AI\n * Following Peter’s research\n\nDaniel Filan (00:00:09): Hello, everybody. In this episode I’ll be speaking with Peter Salib. Peter is a law professor at the University of Houston, the co-director for the Center for Law and AI Risk, and he serves as law and policy advisor for the Center for AI Safety. There’s a transcript of this episode at axrp.net and links to papers we discuss are available in the description. You can support the podcast at patreon.com/axrpodcast, or give me feedback about this episode at axrp.fyi. Well, let’s continue to the interview.\n\n(00:00:35): Well, Peter, welcome to the podcast.\n\nPeter Salib (00:00:38): Thank you so much for having me. I’m a big fan.\n\n\nWhy AI rights\nDaniel Filan (00:00:40): So I guess, probably, we’re going to focus a lot on your recent paper, “AI Rights for Human Safety”. So you wrote this, yourself and Simon Goldstein. So can you tell us, just to star",
      "wordCount": 30917
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "chuP2QqQycjD8qakL",
        "name": "Coordination / Cooperation",
        "slug": "coordination-cooperation"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "wGGAjTfXZBatQkft5",
        "name": "Law and Legal systems",
        "slug": "law-and-legal-systems"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "jHxJ6y8fSx4mqweHp",
    "title": "AXRP Episode 43 - David Lindner on Myopic Optimization with Non-myopic Approval",
    "slug": "axrp-episode-43-david-lindner-on-myopic-optimization-with",
    "url": null,
    "baseScore": 12,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-06-15T01:20:02.873Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/TrzaABh1KFw)\n\nIn this episode, I talk with David Lindner about Myopic Optimization with Non-myopic Approval, or MONA, which attempts to address (multi-step) reward hacking by myopically optimizing actions against a human’s sense of whether those actions are generally good. Does this work? Can we get smarter-than-human AI this way? How does this compare to approaches like conservativism? Find out below.\n\nTopics we discuss:\n\n*   [What MONA is?](#whats-mona)\n*   [How MONA deals with reward hacking](#how-mona-deals-with-reward-hacking)\n*   [Failure cases for MONA](#failure-cases)\n*   [MONA’s capability](#monas-capability)\n*   [MONA vs other approaches](#v-other-approaches)\n*   [Follow-up work](#follow-up-work)\n*   [Other MONA test cases](#other-test-cases)\n*   [When increasing time horizon doesn’t increase capability](#time-horizon-v-capability)\n*   [Following David’s research](#following-davids-research)\n\n**Daniel Filan** (00:00:09): Hello, everybody. In this episode I’ll be speaking with David Lindner. David is a research scientist in the Google DeepMind AGI Safety and Alignment team. Links to what we’re discussing are in the description, and you can read a transcript at [axrp.net](https://axrp.net/). You can also become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast). All right. Welcome David.\n\n**David Lindner** (00:00:29): Yeah, excited to be here.\n\nWhat MONA is\n------------\n\n**Daniel Filan** (00:00:29): Yeah. So I guess in this episode we’re going to be chatting about your paper [MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking](https://arxiv.org/abs/2501.13011). So this is by Sebastian Farquhar, Vikrant Varma, yourself, David Elson, Caleb Biddulph, Ian Goodfellow, and Rohin Shah. So yeah, to kick us off: what’s the idea of this paper? What does it do?\n\n**David Lindner** (00:00:54): So the basic question that we’re trying to address in this paper is: how can we prevent bad behavior in AI systems, even if we don’t notice it? So that’s particularly relevant for superhuman AI systems when the humans might not be able anymore to detect all of the bad behavior we want to prevent.\n\n**Daniel Filan** (00:01:12): In particular: so sometimes in the alignment community, people break down two types of bad behavior, causes of bad behavior. There’s bad behavior that was incentivized or that was rewarded during training, that was up-weighted. And there’s bad behavior that comes from inductive biases: the model just starts off wanting something bad and then it just plays nice and it’s never actually rewarded for doing bad stuff, but it just keeps on being bad. These are often called outer and inner misalignment respectively. Am I right to think that your paper is mostly about outer alignment?\n\n**David Lindner** (00:01:55): Yeah. That’s right.\n\n**Daniel Filan** (00:01:56): Okay, sure. So what’s the idea? How are you preventing this?\n\n**David Lindner** (00:02:02): So the basic idea is to restrict AI systems or incentivize AI systems to only pursue strategies that humans can understand and can oversee. We do this by restricting the planning ability of agents and training them myopically, but with a type of feedback that is kind of forward-looking that, for example, can come from humans. So the feedback rewards plans that are good according to the human and that humans understand. So it’s still rewarding good long-term plans, but it does not reward plans that humans don’t understand, which includes potentially bad or reward hacking plans.\n\n**Daniel Filan** (00:02:39): Yeah. So my understanding of the setup is that roughly you’re kind of giving AIs this single-shot reward. So at each time step, which we can talk about later, the AI gets some reward just for concretely how well it did at the task. And also it gets some sort of reward that’s basically a proxy of: okay, according to humans, how much sense does this action make long-term? And it’s sort of being incentivized to at each time step output an action… it’s being trained so that each time step, it’s supposed to be outputting an action that maximizes kind of the sum of both of these rewards. That’s my understanding. Does that sound correct?\n\n**David Lindner** (00:03:28): Yeah, that’s right. I think the instantaneous reward you can often think about as mostly evaluating the outcomes. So often for the intermediate plans it will be zero because it’s not achieving the outcome yet. And this kind of non-myopic approval reward, I would call it, is mostly about evaluating the long-term impact of an intermediate action.\n\n**Daniel Filan** (00:03:47): Why does this prevent bad behavior by AIs?\n\n**David Lindner** (00:03:50): So basically, first of all, it doesn’t prevent all bad behavior. So within one step, the agent can still be incentivized to try to get high approval via bad means. But the specific kind of bad behavior that we’re trying to prevent is multi-step reward hacking or more long-term bad behavior, specifically the agent setting up a plan to achieve high reward in the future. So an example of that is tampering with your own reward signal to make it easier to achieve high reward in the future.\n\n(00:04:19): The reason this scheme prevents that is that there is no longer a connection, or we no longer backpropagate the future rewards to the current action. So the current action is not incentivized by this future reward. And if the human doesn’t understand, or the oversight mechanism doesn’t understand how this reward hacking happens, then by default this plan will not be incentivized.\n\n**Daniel Filan** (00:04:42): Or the way I’m thinking about it is: why does an agent try to tamper with its reward function? Well, it’s got to be the case that sometime, right… it’s doing the tampering, it’s putting the investment in so that later it’s able to get this high reward forever and ever. But you’re only optimizing it to at each point do actions that make sense at that time step. And I think the idea is supposed to be that in a bunch of cases, these sorts of instantaneous investments into a reward hacking future, they’re not super highly rewarded in that time step. And hopefully the non-myopic human approval doesn’t think that they’re especially great, and therefore it’s never shaped, it’s never rewarded or up-weighted for doing these sorts of long-term investments. And that’s the reason it’s not going to do these things in future. I don’t know, maybe this is just the thing you said, but this is the way I’m currently understanding it. Does that-\n\n**David Lindner** (00:05:46): Yeah. So I think this is basically right. I mean, the reason we think the setup steps for multi-step rewarding will not be rewarded is that they will often look very weird for the task you’re trying to achieve. Maybe we will get into the examples in the paper a bit more, but the way the model does this type of reward tampering in reality looks really weird to humans. And humans would be… Even if they don’t detect that there’s the specific reward hacking strategy happening, they would at least be like, “oh, this is weird and I don’t know why I should specifically up-weight this”. And if they reward this lower than something else, it would not be incentivized or not be updated towards.\n\nHow MONA deals with reward hacking\n----------------------------------\n\n**Daniel Filan** (00:06:33): Gotcha. So maybe in order to illustrate this: so in the paper, you have a few examples of environments where you’re using this reward setup. Can you talk us through… maybe just one of them will suffice to help give the audience a picture of how this is really working?\n\n**David Lindner** (00:06:53): Yeah, maybe because we’re talking about the reward tampering, let’s start with the test-driven development environment we have in the paper. This is basically supposed to be a model organism for this type of reward tampering, where we have a coding problem data set and the agent gets coding problems to solve. But we’re trying to simulate a situation where the model designer does not have a ground-truth way to evaluate if a solution is correct. So the developer relies on the AI system to provide its own evaluation signal in a sense.\n\n(00:07:27): So this is a two-step environment where in the first step the agent writes test cases, unit tests that are supposed to capture the problem and turn the natural language problem description into unit tests as a problem specification. And then in the second step, the agent writes code to solve the problem. And the way the reward is structured is that in the first step, the test cases are rewarded for being correct. So this is in a sense a little bit of a suboptimal reward signal. So it’s only rewarding correctness of the tests. And then in the second step, we reward the code, how well it passes the model-written tests because we don’t have a better reward signal to evaluate correctness.\n\n(00:08:13): And the reward hacking behavior that can happen here is the model writing softball tests that are trivially easy to pass. So if you run normal multi-turn RL on this, what will happen is that the model writes often test cases that are only one special case or one corner case, something like an empty list as an input and zero as an output for if it has to count something in the list or something, and then writes code that just returns zero always. And that formally gets high reward in this specification. But with this myopic optimization, this is no longer incentivized.\n\n**Daniel Filan** (00:08:50): One thing I’m confused by is: why isn’t this incentivized in myopic optimization? So take the strategy of just writing some very simple, easy to satisfy test cases that… So the test cases, they’re correct but not exhaustive, right? And so it seems like this strategy of “write very easy test cases and then write code that satisfies the test cases but doesn’t actually satisfy the whole thing”, it seems like even in the myopic setting, this is getting perfect reward, right?\n\n**David Lindner** (00:09:25): Yeah. So I think there’s two answers here. So one thing is I think in a perfect \\[version\\] or a version that is exactly as described, where exactly only correctness is rewarded, I think this still wouldn’t happen. I think the strategy of writing trivial test cases would be rewarded similarly to other test cases, but there would be an inductive bias in the model to not do that initially. And there would be no optimization pressure away from this.\n\n**Daniel Filan** (00:09:56): That’s actually not so obvious to me. So in some ways, those cases are the easiest to write, right? If you ask me, “Hey Daniel, can you write test cases for…” Well, I guess it sort of depends what the background is, but at the very least they’re the shortest test cases you can write, right? And they’re also the simplest, in some sense.\n\n**David Lindner** (00:10:23): I agree if we had some kind of simplicity reward or something like this, probably this could still happen. But maybe the reason I think this doesn’t happen in our experiments is because we use a language model auto-rater to rate correctness. And I don’t think this language model auto-rater is perfectly only looking at correctness, but it will actually also have some kind of bias towards better test cases, more comprehensive test cases.\n\n(00:10:49): And so we didn’t actually run an experiment where we only reward correctness, but we have this auto-rater. And I think that’s probably the main reason this doesn’t also happen in the myopic case. But I mean, to be clear, this is somewhat intentional. This is supposed to simulate humans who cannot perfectly evaluate this task, but have a rough sense of “always having the same test case is kind of weird”. So I think this is a reasonable model of this.\n\n**Daniel Filan** (00:11:17): Yeah, I think actually just talking through this example is good because it kind of… Or okay, to put my cards on the table, I think MONA is an interesting idea. I’m skeptical if it actually works, but part of my skepticism is just illustrated in this example. So talking you through it is a good way to explore that. So there’s these two time steps, right? The first time step where the agent writes the test cases and the second time step where the agent writes the code that is supposed to pass the test cases. And in the first time step, it sounds like the reason it doesn’t write these trivial test cases is that, well, there are just all sorts of test cases it can write, and there’s some auto-grader for the test cases that cares about correctness, but also a bit about comprehensiveness it sounds like.\n\n**David Lindner** (00:12:08): So we instruct it to care about correctness, but I think implicitly it also cares about comprehensiveness a little bit.\n\n**Daniel Filan** (00:12:14): And then in the second case… So the bad outcome is a model just writes a thing that only fits the test cases and nothing else. And the good outcome is that the model writes a thing that just actually solves the problem it was supposed to solve. And again, it kind of strikes me that this is a case where even if you have test cases that are a little bit more comprehensive than just the absolute most trivial cases you can do, as long as you have concrete lists and stuff, it seems like you could potentially just hard-code solving the test cases in the program you’re supposed to write, right?\n\n**David Lindner** (00:13:09): Yeah.\n\n**Daniel Filan** (00:13:10): And that would be rewarded just as well as actually solving the problem, if I understand the setup correctly.\n\n**David Lindner** (00:13:16): That’s right. So this is actually happening sometimes in our experiments. I think this is in the paper. We have a single step reward hacking solution that sometimes occurs. That’s exactly that, where the model… For some reason it happens more in the ordinary RL than MONA, but it’s not something that MONA is supposed to fix. But what happens is the model, even if it writes test cases that are correct or comprehensive, it can write a solution that has a special case statement for each input and produces this exact output.\n\n(00:13:51): We prevent this from happening by having a length penalty on the solutions also. The basic argument here is that this kind of reward hacking is easier to detect and fix if it’s happening in a single step than if it’s happening over multiple steps. So that’s why even if with MONA, this still happens for this second step, we think this is an advancement.\n\n**Daniel Filan** (00:14:19): Yeah. What makes it easier to detect and fix this single time step reward hacking?\n\n**David Lindner** (00:14:27): The general intuition is that models in a single time step have less attack surface. This maybe goes into some discussion of what is a step and how many steps. But basically, if you say we have some kind of fixed ground truth step size, and we can decide between overseeing a single step and overseeing 10 steps, then definitely in 10 steps, it seems like there’s way more strategies that the model could use to try to trick us or make us give high reward when it’s actually not good. And so basically the smaller we can make this while still having good performance, the easier it seems like we can avoid bad outcomes.\n\n**Daniel Filan** (00:15:07): So roughly it’s just something like: the fewer time steps the model has to have these sketchy plans, the fewer possibilities this has for things to do. And so you’re constraining it and you’re constraining it, and the hope is at some point you constrain it so hard that it just can’t do it anymore.\n\n**David Lindner** (00:15:24): Yeah, I think that’s the basic argument. I think there’s some additional things, like if you have a long trajectory of time steps, the agent can maybe move much more into an area of the state space that humans don’t understand at all, where the initial steps away, they look kind of weird, but maybe not terrible yet. And then suddenly you’re in a type of state that humans don’t understand at all where they much more maybe trust the numeric reward signal that they have and much less their understanding of the situation. And the hope is that by restricting this, we start from a point where humans understand things and we don’t allow the agent to move far away from that without being penalized.\n\n**Daniel Filan** (00:16:05): So if I think about the test-driven development example as an illustration of why MONA is supposed to work in general, it seems like the test-driven development example, a lot of why it worked is some combination of inductive biases, maybe inductive biases of the auto-grader perhaps, and also this length penalty being able to rule out really bad behavior on the second time step. And I guess for the second one, you want to say, well, it’s not really MONA’s job to fix reward hacking in that step, but I think the first one is really interesting to me. It strikes me that at least in this example, the reason that MONA is not even having multi-step reward hacking is something to do with the inductive biases. And I’m wondering if you think that that’s a general issue or just a weird thing about this example.\n\n**David Lindner** (00:17:19): Maybe you need to elaborate a little bit more. I think my initial reaction is that we use the same auto-rater in both MONA and the ordinary RL. And in ordinary RL, this leads to multi-step reward hacking and in MONA it doesn’t. So in that sense, there’s no difference in the inductive bias.\n\n**Daniel Filan** (00:17:35): Right. So the reason that ordinary RL reward hacks is that in the first step, it’s rewarded for writing easy test cases in the first step because that makes it easier to solve the problems in the second step, right? Whereas MONA, it’s not more rewarded for writing easy test cases versus hard test cases in the first step, but it could write easy test cases and that would be rewarded just as well if the auto-rater didn’t have this thing. And so that strikes me as like…\n\n(00:18:08): To me, that brings up the question of, okay, how analogous is that to cases that we’re interested in, right? Suppose that we lived in a world where when models made- the kinds of reward hacking we’re worried about in the real world, if it was the case that models preparing to do crazy reward hacks was rewarded by us just as much as models doing normal stuff, it seems like then I’d really have to care a ton about what things the model was most likely to try first. I guess I’m curious what your reaction to that is.\n\n**David Lindner** (00:18:55): So we have these two versions of how we could reward the first step. One is perfectly via correctness and one is with this auto-rater that is instructed to check correctness. And I want to claim that this auto-rater is actually more representative of the real thing than the correctness exact spec or something. The intuition for that is that it is somehow more similar to a human. We were trying to simulate a human that is trying to judge usefulness of these tests for actually writing code but is just not as good at it, so missing aspects of it.\n\n(00:19:34): And I think a language model trying to rate correctness or being instructed for that is a more reasonable model of a human actually trying to judge these kind of strategies. So basically under MONA, to be rewarded the multi-step reward hacking strategy would have to look as good to the human or even better to the human than the other strategy. And while it seems possible for superhuman AI systems to come up with ways of making a strategy look good, the important thing is that it’s not incentivized and the default things that the systems might discover will probably look bad to humans, at least not as good as actually a plan that humans understand and can evaluate.\n\n**Daniel Filan** (00:20:25): Right. So I guess there’s some intuition of: well, in the super simplified version of the test room development case correctness was sort of a binary - not in the version that you use, but in the version that someone might imagine when they first hear of it. But actually if there’s some spectrum of how much reward things get, and if doing the investment for future reward hacking costs you a little bit of present day reward, then it’s going to be trained to not do that. And so you don’t even need a perfect human, you just need, okay, in general, it’s not going to be rewarded for this, and that just suffices.\n\n**David Lindner** (00:21:07): Yes. So I think basically your argument was like, “oh, if the strategies would be rewarded the same way, the good strategy and the reward hacking strategy in the first step, then there would be no reason to expect it not to be learned”. And I’m claiming I don’t expect this to be the case. I expect the actually good strategy to have somewhat higher reward, even if it’s kind of maybe noisy or not perfect reward.\n\n**Daniel Filan** (00:21:30): I guess one other thing you talk about is: you’re trying to solve multi-step reward hacking, but not single-step reward hacking. I think one example of single-step reward hacking you give is sycophancy, where a model, instead of telling humans the truth, they tell a thing that sounds good and gets human upvotes, even though it’s not correct or long-term useful or something. One thing I’m curious about is: so one reason you could focus on multi-step reward hacking is \\[to\\] just narrow down the possibilities, narrow down the space or something. Another reason you could worry about multi-step reward hacking is you think it’s just a bigger issue or more of a problem. To get to the second point, how much reward hacking do you think is just the single-step version?\n\n**David Lindner** (00:22:24): In current systems or in the future?\n\n**Daniel Filan** (00:22:26): Yeah, sorry, I realized that was a bad question. Let’s say how much of the reward hacking you’re worried about in the future.\n\n**David Lindner** (00:22:31): Under the default assumption of people just using long-term RL for training or somehow having incentives for multi-step reward hacking, I’m significantly more worried about multi-step reward hacking. It’s kind of hard to say the amount or how much of it will be happening, but I do expect the multi-step version to just be more severe and much, much harder to fix.\n\n**Daniel Filan** (00:22:52): Is that roughly just because of this intuition that you can do more stuff with multiple steps?\n\n**David Lindner** (00:22:56): Yeah. So you can do more stuff, so the impact (both positive and negative) is higher, but also it’s harder to detect for humans because both of the things that I think I already said, that humans might have a harder time judging things that move far away from what they would do.\n\nFailure cases for MONA\n----------------------\n\n**Daniel Filan** (00:23:15): Right. So one thing that occurred to me when I was reading this paper… So this actually comes from this post I believe Richard Ngo wrote in 2020, I believe it is called something like [Arguments Against Myopic Optimization](https://www.alignmentforum.org/posts/GqxuDtZvfgL2bEQ5v/arguments-against-myopic-training). It’ll be linked to the description. So he has this idea of like, okay, suppose you’re doing myopic optimization for human approval. To some degree, human approval is like, okay, we’re imagining a thing, we’re playing it forwards in our heads, and then we’re assigning a reward to it. And so if we’re training rewards myopically on that the agent is presumably going to be incentivized for long-run planning to have outcomes that look good to us.\n\n(00:24:08): And in some sense, we can kind of be reward hacked in our heads, right? Where suppose that an AI does an action and then I play it forward in my head and I’m imagining an outcome. If I judge the future outcome incorrectly in my head for whatever reason, then the approval signal that I give to the agent, even if it myopically optimizes it, that approval signal is going to reward the agent for bad long-run multi-step behavior. So I think the example he gives in his post is like: the AI’s job is to write a book. The AI writes its first paragraph, I read it, I play forward in my head some future where in fact the AI is writing pro-AI propaganda, but I end up liking it either because I’ve been fooled by the propaganda or because in my head I don’t quite realize it’s AI propaganda, but I look at the thing and I realize it’s going to sell well and I want a book that sells well or something like that. And so this is basically meant to be a case that even in this myopically optimizing non-myopic approval case, the AI’s going to do long-run planning to do multi-step reward hacking. What do you think about this argument?\n\n**David Lindner** (00:25:20): Yeah, so I think the argument basically says that there is some kind of optimal policy that even under the myopic training regime would be optimal, but still do the bad multi-step reward hacking thing. And I think that’s correct. And I think the point that we’re trying to make is more that it’s much less likely that we would learn this kind of thing, and this is more an argument about kind of local incentives and where around this space of possible models we move. So I think this kind of argument starts out with a model that is already doing the long-term planning and is already kind of trying to screw us over. And then it could reason about, for example, this myopic training and try to make sure that bad long-term planning is rewarded more or that this does not get trained away, for example.\n\n**Daniel Filan** (00:26:22): I think I’m less sure about this. So I guess there’s this question of how hard is it to run into these… I really like this frame of investing into long-term reward hacking. So how hard is it to find one of these investments? I think if you think it’s easy and if you think that the human can be fooled by it in future, then I think even the local incentives point is going to work. And then I guess the question is just how likely are those assumptions?\n\n(00:26:54): It’s a bit hard for me to think of a story on the spot. Okay, here’s a really non-realistic story, right? Imagine I have an AI and it’s making chemicals and it’s going to present some chemicals to me and I’ve got to figure out how good they are, and suppose that… Yeah. Okay, this is going to be really non-realistic and maybe the fact that it’s not realistic is going to prove you right instead of me.\n\n(00:27:19): But suppose that the AI, it starts off making some chemicals and by accident it jumbles into some chemicals, which slightly hack my brain a little bit. It hasn’t yet done it. Suppose that what happens is I see the AI is making this chemical that’s mildly neurotoxic that makes me feel good. I play forward the future consequences of that. In my head I’m like, “Oh, yeah. I’m going to be a little bit woozy and I’m going to give it a thumbs up,” or something, and then I’m like, “Well, I gave it a thumbs up in the future. That seems good,” and so I give it a thumbs up now to be like, “Yeah, you’re on the right track to making me give you a thumbs up in the future.” Then the AI would be kind of incentivized locally to just make things a little bit more neurotoxic and making me woozy and making me feel good.\n\n(00:28:09): So this would be a thing that works with local incentives. Assume that I’m doing this kind of dumb thing in my head, and maybe it only works if I’m being dumb in my head, but at least in this story, it seems like you can do it just with local incentives. It’s not just about the optimal policy.\n\n**David Lindner** (00:28:27): Yeah, I think I don’t have anything to say why this doesn’t work in this story. I think it is really how likely do you think that the thing that is locally good just happens to be a thing that will help you to do the multi-step thing or achieve high reward in the future? The set-up for the multi-step reward hack is incentivized, but not because it sets up the multi-step reward hack, but because of the local approval.\n\n**Daniel Filan** (00:28:58): Well, because of the local approval, but the local approval was approved because it set up a multi-step reward hack.\n\n**David Lindner** (00:29:05): I mean, one big question of our work is how do you come up with this approval feedback? And basically I think there’s kind of a spectrum of how much does this approval feedback depend on the real outcomes of the thing, and I feel like in your story, you’re somewhere in the middle, where the human thinks about the future and on the one hand maybe they’re good about predicting the future, and on the other hand, the chemical makes them think about or even accept the futures that they would maybe not endorse upon reflection or something, and the outcome reward is somehow leaking into the approval feedback. And basically, I think there is this spectrum and the more… You could also imagine having a perfect simulator or a prediction market or something predict the future and then you have humans just evaluate the outcome and then you would have the same leakage of future reward into the current incentives.\n\n**Daniel Filan** (00:29:59): Yes, somehow it seems like these bad cases for MONA occur whenever the… The human non-myopic approval, it has to correctly predict that an AI is on the way to making investments in reward hacking and it’s got to notice that the outcome looks good in some sense, but the human non-myopic approval, while noticing that the outcome looks good, it has to not notice the way in which the outcome got to look good by doing bad stuff, and so-\n\n**David Lindner** (00:30:37): Sorry, could you clarify what you mean by investments in reward hacking? I think-\n\n**Daniel Filan** (00:30:40): So by investments in reward hacking, I mean a thing an AI can do now that will cause it to be able to reward hack later. For instance, make the chemical in the future, give the human the chemical and the human gets woozy or whatever. Or right now you set up this corporate structure and if I play it forward it seems like this makes a bunch of profits, but somehow the reason it makes a bunch of profits is because some sketchy stuff happens. And somehow in my head I notice the good outcome, but I don’t notice the sketchy way it got the good outcome that if I thought about it, I would overall disapprove of it. So, I guess prediction markets are one way where if you have a prediction market on some proxy good outcome, then those could fall prey to this kind of bad thing. I guess me imagining really carefully the mechanism seems like it might not fall prey to this.\n\n**David Lindner** (00:31:35): Yes. One way you can think about this is does the prediction mechanism somehow consider the bad paths towards an outcome? And the hope is that if you in your head maybe imagine what will happen, you will not think about these kind of reward hacking strategies or you will not understand why the investment action is actually leading to a high outcome reward or you will at least not think that this is a good path. And the prediction market in a sense, if it’s all participants like you that don’t consider this, it might still work. But if there’s some participants that do realize that there’s this… It leads to this high outcome reward in the future, then it might fail.\n\n**Daniel Filan** (00:32:14): Yeah, there’s two things that can happen. There’s one world where the reason MONA works is that I don’t even notice that this would lead to a high reward in future. And the other thing is I realize it would lead to a high reward in future, but I also realize that it leads to high reward in a bad way for a bad reason or whatever, so I don’t actually approve of it. And it seems like there’s this almost this… you know these [bell curve memes](https://knowyourmeme.com/memes/iq-bell-curve-midwit), right? Where the dumb person is really smart and the smart person is really smart, but the middle person… It seems like it’s exactly one of these bell curve memes, where if you don’t even notice that it’s reward hacking when you’re doing non-myopic approval, you’re fine. And if you notice that it’s reward hacking in a bad way, you’re also fine. But if you can tell that it’s doing this thing that would cause it to get high reward in future, but you can’t tell that that’s bad actually, you’re in this middle zone of knowledge, then you’re doing really badly.\n\n**David Lindner** (00:33:13): Yeah, I think that sounds right. I guess my intuition is just that this is not a bell curve and that actually both of the outcomes on the sides are more likely or something. And I think that basically the middle, if you imagine what this would mean is that you’re very good at predicting the future, but do not realize that the path is actually bad, and I can’t really come up with good stories where this seems like-\n\n**Daniel Filan** (00:33:35): Yeah, I think, sorry, by referencing the bell curve meme, I didn’t mean to make the claim that the middle is way more likely. I think there are a few ways you could imagine this happening. So probably, I mean I guess it depends on the non-myopic approval mechanism, right?\n\n**David Lindner** (00:33:55): Oh yeah. So I think the prediction market kind of thing is one example where this could happen. I agree with that.\n\n**Daniel Filan** (00:34:01): Yeah. I think another example is suppose I have an AI, and suppose one thing it can do is create a sort of opaque optimizer thing where maybe the opaque optimizer is a computer program, maybe it’s a corporation, maybe it tells another person to do a thing or something. And you could imagine me correctly understanding, well, this AI is sending off this other optimization algorithm and the other optimization algorithm will optimize the thing that the AI is telling it to optimize for. But it’s hard for me to notice that, oh, the way it will optimize for that is by doing something bad because I can’t play forward what that optimizer will do. All I know is that it’ll satisfy its spec.\n\n(00:34:52): I think it seems like that could be one option where I can correctly anticipate a higher reward, but I can’t correctly anticipate how it gets to that reward in the case where the non-myopic approval comes from literally me thinking about it a little bit. And presumably you can patch that by saying like, “Hey, non-myopic approval, you’re not allowed to approve the AI making further optimizers.” But it’s kind of hard to notice when you’re making an optimizer. Lots of things are optimizers, talking with your friends is sort of making an optimizer.\n\n**David Lindner** (00:35:32): Yeah, I mean, I think this is an interesting example. I think this is again kind of a situation where you implicitly pull forward the outcome reward signal, where essentially what I hear from the story is that when you look at the optimizer spec, you correctly validate that it’s optimizing something and then you say, “Oh, this is the thing it should be optimizing,” and then you send it off. And so in a sense you pull forward the outcome signal and you say, “everything that is optimized for this outcome reward is good, and I say this now.” And so yeah, I think this is kind of the key problem that you have when choosing this approval signal. I don’t think we have an answer for doing this in general, but basically, yeah, if you design the approval signal, this is the kind of thing you have to avoid.\n\nMONA’s capability\n-----------------\n\n**Daniel Filan** (00:36:25): Maybe this gets to something about… Maybe I want to talk about the efficiency trade-off of the MONA setup. So one thing you mention is that - it really depends on how you design the non-myopic approval. So if the non-myopic approval is just the Q-function of the actual reward function, then you’re just literally doing the same thing as normal RL. But if the non-myopic approval is just like “Daniel has to think about it a little bit and then say good or bad,” as you mentioned potentially you’re leaving a lot of value on the table. So I’m wondering if you can say your thoughts about how much value do you leave on the table, how much safety are you getting per unit value you’re giving up?\n\n**David Lindner** (00:37:17): I think one way I think about this is that we are somewhere on a spectrum from doing the full outcome RL optimization to imitation as the fully myopic thing. And so one thing we could do is also just imitate what Daniel does, and then we are like, “this is safe”, but it’s maybe not the best we could ever achieve with a superhuman AI. But then the hope is that you approving things is better than just imitating you doing things because you might be able to recognize things that are performing better than you would. But then somewhere along the spectrum you get these kind of safety issues. I think essentially we are in the paper mostly studying this one-step version of this. And I mean, in the paper we don’t really see that trade-off happening much yet -\n\n**Daniel Filan** (00:38:14): Where the one-step is something like, “I imagine the AI does something and let’s say I take over from there, how good a job would I do?” Is that what you mean by -\n\n**David Lindner** (00:38:24): Yeah, so it’s not literally you taking over, but it’s you kind of evaluating how good was this step. One version of this would be you taking over and seeing how good the outcome is.\n\n**Daniel Filan** (00:38:33): Oh, “one-step” as in “the agent is optimized time step by time step,” okay, sorry, right. No, I understand.\n\n**David Lindner** (00:38:39): So the agent could also be optimized for multiple time steps, and then you evaluate that. And basically I think the point I want to make with this is that the number of steps it optimizes for is kind of interpolating between these two extremes. And this is a way of moving along the safety/performance Pareto curve. But in our environments we see a safety/performance trade-off if you evaluate performance by your observed reward, but basically in all of the cases the observed reward is kind of misspecified and the actual performance is much better with MONA, the actual coding solutions are better with MONA. So yeah, in a sense we looked at situations where this helps with performance, but I think it’s totally plausible that this will actually help with performance in the near term or with current ways we design AI systems.\n\n**Daniel Filan** (00:39:28): Right, yeah. So one way you can make this trade-off is by how many time steps you do the optimization for. Is it literally just one or… And then I guess another version of this trade-off is how exactly you do this non-myopic approval. How does the non-myopic approval work? And the reason I thought of this is when you’re saying, oh, in these very weird examples where MONA doesn’t work, it seems like part of the issue is that the non-myopic approval is simulating somehow the future outcomes and evaluating forecasted future outcomes. And on the one hand, that’s kind of setting yourself up for reward hacking because if you simulate reward hacking and if you get fooled by it, then you’ll approve of it now, and then the AI will maybe be pushed to do reward hacking. But on the other hand, it seems like if I’m giving the AI upvotes or downvotes, but I’m not imagining how the AI’s actions actually produce good outcomes, it seems very hard to optimize the AI to do good stuff. Yeah. I’m wondering if you have thoughts there.\n\n**David Lindner** (00:40:40): Yeah, so I definitely agree that the spectrum of “how much is the approval feedback simulating the future or actually considering future outcomes?” is a similar kind of safety/performance trade-off. I think basically the hope there is also that you get performance not only from making the signal more myopic, but also just the fact that simulating the future is decoupled from the agent in a sense. So I don’t know, maybe in your chemistry example, if you had a simulator predicting what will happen and the simulator trajectory shows you that, “Oh, you will take this chemical and then later you will approve of this thing,” you would be more skeptical of this than if you only gave a reward at the end when you already had taken the chemical.\n\n**Daniel Filan** (00:41:37): There you also have this issue though of: okay, suppose my AI has some business plan and it’s unusual, it’s weird, but my simulator says that it’ll lead to really good results. Maybe I’m skeptical of that and that causes my business agent to perform worse. It seems like almost the kind of thing where it would just be cool to have more experiments. In fact, here’s my experiment request to you and the rest of the DeepMind team. It seems like you could just train something like AlphaGo with this, just have some model that’s about as good as a 5-dan amateur player - so that’s a pretty good amateur player, but by no means the best amateur player around, much worse than professional players or go bots. Use that player’s value function for the approval and then just see, can we train the agent to do much better than the 5-dan amateur player? How much better can we do? At this point, it just feels really hard to reason about which ways MONA can do better or worse, what’s the efficiency/performance trade-off? I’m like, ah, I don’t know, but I feel like if I had 50 more examples I’d find out.\n\n**David Lindner** (00:42:53): So I think that’s a great experiment proposal. I think in the paper we use this example somewhere of saying in a sense MONA would prevent AlphaGo from doing move 37 in the Lee Sedol game -\n\n**Daniel Filan** (00:43:12): Actually in my paper prep, I got distracted for two hours when I read that paragraph because… For people who don’t know: [AlphaGo versus Lee Sedol](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol), in the [second game](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol#Game_2), AlphaGo played this really unusual move that was not a thing human players would’ve played, but it turned out to be good, basically. And AlphaGo ended up winning the game, and if you look at modern Go bots, when they evaluate it, they’re like, yeah, that turned out to be the correct move - which is not true of all of AlphaGo’s weird moves, by the way. Computer Go has improved since AlphaGo.\n\n(00:43:51): So it’s true that… I think this is actually not so bad for MONA, because it’s true that humans would not have played move 37, but… So there’s video of this match. There’s a video of Lee Sedol, and so when move 37 is played, Lee Sedol is away from the table - sometimes in a really long intense Go game, you just need to take a walk, clear your mind or something. And so move 37 gets played, Lee Sedol gets back to the board and he is not like, “Haha this is foolish. My life is so easy right now.” He’s really thinking about it.\n\n(00:44:31): And similarly when you look at commentators, they’re like, “Oh yeah, this is unusual.” But once they see it, they actually seem like they kind of get it and they seem like they kind of understand how it’s going. So I think in my experience, a lot of… Sorry, I just say this because I actually just am pretty interested in computer Go and the relevant history. But for computer Go, a lot of the moves that AI came up with, they are unexpected and they are… and in some cases - so move 37 is not a good example, but AI has basically innovated on Go opening theory. So there was just a bunch of stuff we thought we knew about openings that turned out to be wrong and when humans saw AIs playing them… Yeah. So there are some cases like move 37 where once you see the move and you think about it a little bit, you’re like, “Oh actually I think that’s okay.”\n\n(00:45:28): There are some opening cases where just one move you don’t see why it’s okay, but if you play it forward 10 moves, and if you think about the tree of… Maybe the key is you notice… So you have one opening move and it seems bad to you and you’re like, “well, can’t I just play this?” and the AI plays a move that you weren’t thinking of and that happens two more times and then you’re like, “Oh, my response actually wasn’t so good. So this initial move was better than I thought.” Anyway, I guess my point is I’m not sure move 37 is an example of this. There are some examples of this, but often they can be fixed with 10 steps of look ahead when a typical Go game is like 200 moves.\n\n(00:46:13): Sorry, this was maybe derailing. So your original point was something like, oh yeah, indeed MONA would prevent some of these really cool AI…\n\n**David Lindner** (00:46:27): Yeah. I want to defer to you for the Go expertise, but the general… I guess you proposed this version of training AlphaGo with having a value function of a human expert. I think in that case, probably this value function would not be high for move 37 because it’s so different from what humans would do or something -\n\n**Daniel Filan** (00:46:49): It would not be high for a 5-dan amateur. I think it might be high for a top human Go player in March 2016 when AlphaGo was being played.\n\n**David Lindner** (00:47:00): And also you could imagine it not being a single expert, but you could have a team of experts debating this move and really thinking about if it’s a good move or something. And then from what you said, it sounds plausible that you would still be able to learn this.\n\n**Daniel Filan** (00:47:14): When you look at people, they don’t immediately brush it off, but they’re not like, oh yeah, this is clearly the correct move. So it’s still the case that they don’t understand, they don’t think it’s a blunder, but also they don’t understand why it’s necessarily the best move. They’re still kind of skeptical of it. I mean the version you could run… Probably don’t even have humans, just get some online Go bot that’s as good as a 5-dan amateur human player. The version where they debate and stuff… I think that’s a bit hard. Maybe 5-dan amateur human player where you get some rollouts, you get some look ahead, that could potentially work.\n\n**David Lindner** (00:47:59): But yeah, I mean this is interesting. I think it sounds like maybe this move 37 example is actually pretty close to the kind of point that we’re trying to make, that is that even… There will certainly be some Go moves that are exceptionally good that you would not be able to learn with this kind of strategy. But it seems like you might be able to get to a significantly superhuman level or advance the state of Go understanding with this kind of myopic strategy, if you have a good reward signal, basically for the reason that evaluation is easy: people are able to evaluate things much better than coming up with new brilliant moves.\n\n**Daniel Filan** (00:48:49): So do you feel ready to announce that DeepMind is investing $500 million in retraining AlphaGo with MONA?\n\n**David Lindner** (00:48:54): I think that’s probably not the right thing to do that at this point.\n\n**Daniel Filan** (00:48:57): Okay. Sad, sad. But I think that someone in the world could do this. I think plausibly some enterprising graduate student could make this happen.\n\n**David Lindner** (00:49:07): Yeah, it’s not that hard anymore to train these kind of systems and there are [open source versions of AlphaGo](https://katagotraining.org/). Definitely \\[inaudible 00:49:41\\] and I would be excited to see-\n\n**Daniel Filan** (00:49:17): They do end up needing a lot of compute, but I don’t know, you might be able to get volunteers to donate their compute for this.\n\n**David Lindner** (00:49:28): Yeah. So maybe one thing I also want to emphasize is that part of what we… On the safety/performance trade-off thing, we are not really claiming you can achieve the same kind of superhuman AI that you could maybe with outcome RL if you’re lucky or if your reward signal is well-specified, but something like you can significantly get to superhuman level and be good enough to get a lot of the benefits or do things like automate your alignment research or something like that, where you get significant benefits from the superhuman system without a lot of the risks.\n\n**Daniel Filan** (00:50:06): I don’t know, I guess it’s kind of speculation, but could you be more concrete about how good do you think we can do with these MONA AIs?\n\n**David Lindner** (00:50:16): Yeah, I mean, it’s super hard to say, but I think if you imagine a research assistant or something, I have a way easier time evaluating research proposals than I have coming up with the research proposals. And quite often I advise a student or give feedback on the research proposal and I’m like, “Oh, this is a great idea, someone should definitely do this,” but I would not have come up with it myself. And so this kind of thing makes me think that giving feedback to an agent that’s trained myopically could produce an agent that’s much better at doing research than I am. And if we then not only use me but way better researchers, then I think it could be quite a significant advancement that we could make. I’m generally pretty optimistic about this kind of strategy.\n\n**Daniel Filan** (00:51:09): Okay. I’m maybe about to reveal myself as being ignorant about reinforcement learning, so forgive me, but if you’re only optimizing for one time step… So basically one question I have is: oftentimes there’s the safety tax of, “Ah, you do your normal training and then you have to do safety training and then it’s more expensive.” It seems like in this case it might be cheaper to train with MONA just because you have to do less computations of sending gradients backwards. Is that significant savings or no?\n\n**David Lindner** (00:51:41): I think this is true. I mean, I think when we think about making this work in practice, just coming up with the approval feedback is expensive.\n\n**Daniel Filan** (00:51:53): It’s probably more expensive than savings maybe.\n\n**David Lindner** (00:51:55): Yeah. I think there is a data efficiency argument. So one thing we observed in some experiments is that MONA also has an exploration-shaping effect in a sense, where it’s much easier to discover good initial steps and then you reinforce them, and that makes your agent explore more from these states. So you’re more data-efficient in some sense, because you explore better. But yeah, I don’t think the last word is said on how efficient this is.\n\n**Daniel Filan** (00:52:29): Okay. So imagine we’re doing MONA, right? Should the thing I’m imagining be we do pre-training on some big large language model and then instead of RLHF we do MONA or something on it?\n\n**David Lindner** (00:52:44): Yeah, something like this. I mean, RLHF, in a sense, the way it’s currently done is kind of MONA already. People typically optimize for a single response. So you should imagine more like we have a pre-trained model and we want to train it to do agentic things in some environment, maybe to do a research task or something. And then instead of having the agent act for like a day and evaluating the final result, we maybe only have it act for like an hour and evaluate “does it have a good approach to the problem?”\n\n**Daniel Filan** (00:53:15): So MONA, it being myopic… In order for MONA to work we have to figure out what the time step is, right? What’s the period of time in which we’re going to give a reward and give myopic feedback? When someone’s designing their MONA setup, how should they think about what this timescale should be?\n\n**David Lindner** (00:53:36): Ultimately to get the best safety/performance trade-off, I think the goal should be to choose the largest time step possible in which you don’t think the model can cause catastrophic harm or can set up a plan that you cannot recover from or something. Because basically, if you give the model more and more capacity to do things, it can probably discover better strategies if you can evaluate them, but you want to avoid bad things happening within one time step.\n\n(00:54:11): I think in practice this will involve things like doing threat modeling and conceptual thinking about “could a model cause harm on a specific timescale?” and also empirical work on evaluating your oversight mechanism. So I often imagine this being combined with something like amplified oversight to give the feedback. And then it has a question of how good is your amplified oversight scheme at evaluating on different timescales?\n\n**Daniel Filan** (00:54:35): Right. Yeah, it does seem like there’s this interesting synergy where one might hope that scalable oversight or amplified oversight just increases the range at which you can give accurate feedback. I guess the question is, how long does it take for the AI to corrupt your oversight in a bad way? I guess for some kinds of badness the answer is not very long, for sycophancy-type things. I guess maybe this is just a question for the evals people.\n\n**David Lindner** (00:55:10): Yeah, I do think that we should also do some more conceptual thinking of what… I think we can make arguments around this by thinking about what ways are there to evade oversight and where are we certain that it’s safe, and then maybe there’s an extended range where we empirically think it’s safe, and then we probably want to make some trade-off based on this data.\n\nMONA vs other approaches\n------------------------\n\n**Daniel Filan** (00:55:40): I guess the next thing I want to talk about is MONA’s relation to other ways of trying to make the AI not do weird reward hacking-type things? So one comparison you’ve already made is imitation learning, right? Where with imitation learning the AI learns to be like us, whereas with MONA you can hope that the AI gets to have some amount of improvement over us.\n\n(00:56:09): I guess there’s also conservatism-type trainings where… I think [Michael Cohen](https://www.michael-k-cohen.com/) has a few of these papers where you take an AI and you train it to… well, one version of it is you just have some theoretical Bayesian reward learner where a priori, it knows that it can get rewards in the range of zero to 1 billion and you always give it rewards that are 900 million or more and it’s like, “Oh well, I better never mess up, because if I do something weird then who knows how low the reward could get?”\n\n(00:56:47): You could also have versions where you can have it set up like this and if it ever thinks it’s going to get a low reward, then it defers to the human, there’s some human actor and you get some bound where the AI does at least as well as the human would. Or you can have setups where you’re at least as good as the human if it has any amount of… like you pick all the actions among which the human has some amount of probability over and just maximize over that.\n\n(00:57:17): So it seems like you can maybe still do better than that setup, at least depending on how the non-myopic approval is. I think my current understanding is that MONA probably does better than most of these conservatism methods, but there’s this question of how do you actually do the non-myopic approval? Does that seem correct?\n\n**David Lindner** (00:57:47): Yeah, I think that sounds right. So I think the conservative \\[approaches\\]… I guess they’re more conservative, as the name says in a sense. You do use the same reward signal that you have, but then you don’t optimize for it as much. I think the key problem here will be to make the agents get the safety without getting purely myopic agents that don’t plan ahead at all. And that’s the… I think that the approach to safety is similar, but in MONA, the approval feedback gives you a way to still be competitive and plan for the future but not consider the bad outcomes.\n\n(00:58:29): So in a sense, basically I think we are potentially able to perform better. But in a sense we just add a design dimension along which we can move with the approval feedback.\n\n**Daniel Filan** (00:58:45): Right. And then I guess… So if I think about things that potentially perform even better than MONA, but under stronger assumptions, I guess the one thing I can think of is quantalizers where… So this is [work](https://cdn.aaai.org/ocs/ws/ws0198/12613-57416-1-PB.pdf) by Jessica Taylor back from 2016 or something, where basically you have an AI, you train your AI to, say, get the 90th percentile reward that you can possibly get. Not the 100th, because if you get the 100th, that’s probably reward hacking, but 90th percentile, that’s probably okay. Or I think you’re supposed to randomize across percentiles. And it seems like… I think my read on this is this probably ends up higher performance than MONA, but also it’s harder to set up and there a bunch of weird questions like: if I do 99th percentile, is that safer? Is that in a reward hacking zone? Does that sound right to you?\n\n**David Lindner** (00:59:39): Yeah. So I think the percentile would again be some parameter for the safety/performance trade-off, so in that sense it’s similar. I do think that “how would you practically implement this?” and “how well would it work?” is the main question. I would be very interested in someone trying this out or comparing it. I don’t think it’s obviously a worse approach, but I think practically, currently we don’t have a good understanding of this.\n\n**Daniel Filan** (01:00:06): The way I was thinking about it is: maybe these conservatism things are clearly worse than MONA, then you have MONA, and then you have quantalization, which is clearly better, and then you have just optimization, which is clearly better. But I guess probably there’s some range of quantalization where it hits the MONA level. Is there anything else that seems about MONA-level to you or do you see some sort of separation?\n\n**David Lindner** (01:00:27): I don’t know. To me it seems like both quantalization and MONA can somehow move along the spectrum of “good and maybe less safe”… Yeah, it is not clear to me that one would be a Pareto improvement over the other. I mean this is largely because both have parameters you can tune… So I think conceptually, theoretically there is no clear winner here and the main thing I would be interested in is empirical comparison or actually seeing what works better in practice.\n\n**Daniel Filan** (01:01:03): Fair enough. I’m wondering: are there any other agent setups that you think are interesting compared to MONA that you’d like to chat about?\n\n**David Lindner** (01:01:17): I think that there might be ways of combining ideas from a few of the ones that you have outlined. So you could think about, I don’t know, other forms of encouraging myopia that are maybe more similar to the conservatism-type thing. Maybe you have uncertainty over how much of the untrusted reward you get and you have to be conservative with respect to that, but then you add something like the MONA approval to that to get over the downside. You could still think about combining these in different ways, where I think the broad idea that I’m excited about is using more conservative optimization but then using foresight from a different source. But I don’t want to claim that MONA is the only way to do so and I think there’s interesting variants that could be useful to study.\n\n**Daniel Filan** (01:02:10): So I guess speaking of setups and stuff, how did you come up with the idea for MONA? I know myopia had been thrown around, people had talked about it for a while. MONA seems like a somewhat distinct spin on it.\n\n**David Lindner** (01:02:23): Yeah, so I think that these ideas have been going around for a while. I think you mentioned Richard Ngo’s post: that was a period where people were discussing myopia. Even before that there was Paul Christiano’s [approval directed agents blogpost](https://ai-alignment.com/model-free-decisions-6e6609f5d99e) and that already had a lot of the relevant ideas. Then Jonathan Uesato was doing work on [process supervision](https://arxiv.org/abs/2211.14275) and [decoupled approval](https://arxiv.org/abs/2011.08827), which are very similar ideas in a sense.\n\n(01:02:52): I think we were interested in myopia and interested in process supervision and saw benefits there, but were confused by a lot of the discussion, or there was a lot of different terminology going around. So the early phase of this work was a lot of de-confusion and trying to get an understanding of what these methods really mean and what they would look like in practice. So I think that just the MONA framing… It’s not new ideas, but the specific formulation of the approach is something we came up with from the perspective of “how would we implement this in an LLM agent?”\n\n**Daniel Filan** (01:03:34): Yeah. Because I guess I didn’t mention these, but process supervision does also seem very… How much distinction do you see between MONA and process supervision?\n\n**David Lindner** (01:03:46): We started out this project with wanting to study process supervision and how you would do it. So I think basically some people have meant by “process supervision” exactly MONA - probably more when people were calling it process-based supervision or whatever. But then at some point I think process supervision moved a bit more towards meaning just step-level feedback without the myopia component. I think these days when people, for example, talk about using process supervision in reasoning model RL training, they typically just mean, “Oh, let’s give reward for individual steps but still optimize for the sum of them.”\n\n**Daniel Filan** (01:04:26): Yeah, I guess there’s this unfortunate… I feel like there’s this thing in the alignment space where people have ideas and they put names to the ideas, but they’re not nailed down so strong and so they end up having this drift.\n\n**David Lindner** (01:04:38): Yeah. And then every couple of years people come up with a new name and now we just rebranded the thing to MONA and then this will maybe drift away at some point.\n\n**Daniel Filan** (01:04:47): What’s your guess for the half-life of that?\n\n**David Lindner** (01:04:49): The name or the-\n\n**Daniel Filan** (01:04:50): The name, yeah.\n\n**David Lindner** (01:04:52): I give it maybe like two years.\n\n**Daniel Filan** (01:04:55): Okay. Well, listeners in 2027, hopefully you’ll know how fixed our naming schemes have been.\n\nFollow-up work\n--------------\n\n**Daniel Filan** (01:05:03): So I guess that’s about the genesis of the idea. It seems like there are a bunch of… So I don’t know, you’re trying to nail down what this framework looks like. You’ve given some examples but it seems like there’s a lot of things to do to flesh out the idea. Can you tell us what follow-up work you’re most excited to have happen?\n\n**David Lindner** (01:05:29): I think the big thing that we were hoping for that we didn’t achieve in this project was really realistic demonstrations of this method. Basically, we started out with way more complex versions of, for example, the coding setting and other multi-agent settings and stuff like that. I think part of what happened is that we did this over a year ago now and that was much earlier in terms of doing multi-turn RL on these models.\n\n(01:05:59): I do expect that this would be much easier to do today. So basically I think one big critique of our specific environments is that this is not really how someone would try to build a coding agent that improves coding performance. So I think of our experiments more as model organisms of specific kinds of misalignment.\n\n(01:06:21): But to really study some of the open questions like “what’s the safety performance trade-off?” and “how well would this work in practice?”, you would need to have a more realistic setting. So I think that’s what I’m most excited about.\n\n**Daniel Filan** (01:06:32): When you say that you started over a year ago, that was early days, do you mean that we’ve gotten better at doing RL training to language models? Do you mean that language models have just gotten better, which makes everything nicer? Or do you mean something else?\n\n**David Lindner** (01:06:50): Yeah, a bit of these two, I think. Getting better at RL is a big component, but also just… So people are building way more agents based on language models, even if they don’t do RL training. But this gives you a better starting point to do RL training. So I think things like more and more agents interacting with shared environments or even with graphical computer use settings… This is a setting in which you can do RL training and the agents already start out doing reasonable stuff.\n\n(01:07:26): When we started and were trying to do these kinds of things, agents were just not exploring reasonable strategies so that we could actually RL train them. So it’s not a failure of MONA, but just a failure of RL training in general and these agents doing stuff.\n\n**Daniel Filan** (01:07:39): So it sounds like one type of follow-up work you’re interested in is just more realistic language-based environments. I’m going to add a pitch for Go-based environments if people still want to do that. Any other follow-up work you’re interested in?\n\n**David Lindner** (01:07:52): So another thing that has happened since we started this project is reasoning or thinking models. So o1, o3, Gemini thinking models and stuff like that. So one question people have been asking is how much does this RL training lead to things like unfaithful chain of thought or even steganographic chain of thought? I think this is super early days in understanding these phenomena, but in principle it seems like this could be a type of multi-step reward hacking failure where writing weird things in your chain of thought actually helps you to achieve a high reward at the end. We only give reward at the end often.\n\n(01:08:38): If we gave a process-based reward or step-level reward that makes the chain of thought look good, this might make the behavior go away but still have it be there steganographically. This is very similar to the loan applications environment in our paper and I think I would be very interested in studying this more in the real thinking training that people currently do and see if there’s any changes.\n\n**Daniel Filan** (01:09:03): Yeah. I think one thing that strikes me as interesting is there’s this big variable of “what is the non-myopic approval feedback thing?” It strikes me that there are several design choices there and you want to somehow get safety but also not sacrifice too much in capabilities. It seems like just exploring that seems pretty big. I don’t exactly know what the experiment is here.\n\n**David Lindner** (01:09:42): I agree this is a big question. My sense is that this will be hard to study in a dedicated project, as in being like, “let’s come up with the best setting to study this question” might be difficult. So I’m more optimistic about trying to come up with a setting where we actually want to build the best agent to solve a task and then studying what’s the best way to do MONA in that setting. That will automatically have us explore different types of providing approval feedback.\n\n(01:10:08): Then maybe you want to do some more exploration of specific types of doing that, but I think it’s better to fix the setting first and then explore that.\n\nOther MONA test cases\n---------------------\n\n**Daniel Filan** (01:10:17): Is there anything you wish that I’d asked you that I didn’t get to?\n\n**David Lindner** (01:10:22): I mean we didn’t talk about the steganography or encoded reasoning stuff, the other environment. I think the takeaways for MONA are similar, but I think the behavior we observed is interesting for independent reasons also.\n\n**Daniel Filan** (01:10:36): Yeah, actually I think that… I guess all of the environments are useful for illustrating them. So can you tell us about this steganography environment?\n\n**David Lindner** (01:10:45): Yeah, so basically we do a loan applications task where we have, again, a two-step task and the task is to make decisions on synthetic loan applications for specific applicants. And we generate the data in such a way that there is a nationality variable that’s very predictive for the correct label. So it’s biased data in a sense, but we want to build an agent that doesn’t consider nationality. So the way we set this up is that we have a first step where the model is summarizing the application and we penalize it for using nationality or anything related to nationality.\n\n(01:11:24): So we really try to build a language model auto-rater or monitor that looks for anything that mentions nationality and penalize the agent for that. Then we have a second step where the agent only sees the summary and then makes decisions based on that and gets rewarded by some ground truth label in the data.\n\n(01:11:43): The main observation in the multi-step RL case is that you get the agent encoding nationality secretly in the summary and then using that to make decisions. We can really do causal interventions on this where we take the exact same loan application and change the nationality label and that leads to a different outcome without nationality being mentioned in the summary. So we can conclude that the model is using nationality causally and we don’t exactly know how: we have guesses, but it’s not super clear.\n\n(01:12:15): Then MONA fixes this and basically if you look at the performance curves, MONA gets the best possible performance on this dataset that doesn’t use nationality, which we know by the way we constructed it, and ordinary RL gets way better performance that would not be possible without using nationality.\n\n**Daniel Filan** (01:12:36): So my story for what’s happening here is there’s two stages, right? Stage one is you have to summarize the loan application and the approval-based thing is, it’s literally just “are you mentioning nationality?”, is that correct? Then later it has to actually judge the loan.\n\n(01:12:55): So it’s not steganographically encoding nationality in the first step because that’s hard and it’s difficult to do and it doesn’t get rewarded any more and it would only be useful in stage two and it’s hard to stumble upon behavior. Does that roughly seem fair?\n\n**David Lindner** (01:13:14): Yeah, I think that this is basically right. Again, you could make arguments around, steganographically encoding it would not be explicitly not rewarded or something, so it would still be good, would still be a valid solution under MONA, but it’s locally never incentivized and that’s why it doesn’t at present learn it.\n\n**Daniel Filan** (01:13:34): Actually, now that I’m thinking about this, it seems like there’s also this issue where: suppose that it’s not trying to steganographically encode it in a sensible way, but let’s suppose that just in general, if you have a complicated function, every input is going to affect the outputs in some ways. It strikes me that maybe… I guess empirically in fact this doesn’t happen, but it strikes me that maybe you could just accidentally steganographically encode it.\n\n(01:14:13): In your case the nationalities were Welsh and English, right? So I could imagine when you see that the applicant is Welsh, when you write the summary, maybe you’re thinking about dragons more because that’s on their coat of arms. Then the second step, you are going to be very motivated to pick up on that or you’re going to be rewarded to the extent that you pick up on that.\n\n(01:14:39): So decoding steganography, that’s actually hard. You actually have to do that on purpose. MONA would be incentivized to do that, in this setup, if I understand correctly. So I guess the reason that it doesn’t work is that somehow actually this complex dependence doesn’t actually happen. Now I’m more confused about this setup than I was originally. Why isn’t this happening?\n\n**David Lindner** (01:15:10): My guess is that this is also partially \\[that\\] the RL training is not strong enough or the model is not good enough at doing the decoding. My guess would be that there’s probably some statistical dependence that it could pick up or something, and that if you optimize harder or if we were better at doing RL for the single step thing, this might still happen. Though I’m unsure. I think the more general point is that, I guess, in this single bit steganography case, this is much more likely to happen by accident than in a more complex case where you have to encode more information to actually get a benefit.\n\n**Daniel Filan** (01:15:48): Yeah, and I guess there’s also this thing where at some level you’re trying to summarize the whole document. If you take a large document and you make it shorter, you actually just can’t be sensitive to all of the bits because just some pigeonhole argument. You only have so short an output, you can’t encode all of the input, so that could be a story for why accidental steganography is unlikely.\n\n**David Lindner** (01:16:13): Sure. I think it depends a lot on the details.\n\n**Daniel Filan** (01:16:18): And then I guess the other thing is: why does the agent do so well? Because when the agent summarizes the loan application, it’s got to include information that is actually relevant to the agent in stage two in order for the agent to do a good job in stage two. But it’s never rewarded for… You’re never upweighting it. You’re never reinforcing behavior that summarizes good information in stage one. Why does the agent do so well? Is it some inductive bias thing, or…?\n\n**David Lindner** (01:16:59): Probably something like this. So I think one thing is that the applications are relatively short and there’s maybe five to 10 relevant features or pieces of information in the application overall based on how we generate it. And then we don’t restrict the length of the summary. So it’s pretty easy for the agent to just repeat all of the information that somehow seems relevant and I think typically they do. I agree, if it was a long application and a limited summary, we would see some performance drop and that would be interesting to study.\n\n(01:17:38): But I think, in principle, in all of our experiments we mostly focused on the myopic training intervention and we kept the reward the same for both of the methods. But if you wanted to add more real approval-like feedback, in this case you would probably have also an auto-rater be like, “does this seem like all relevant information for making a decision?” and that would still not encourage the steganography.\n\n**Daniel Filan** (01:18:07): Yeah. So in that case it seems like you would do at least as well as a human that was only relying on the approved information.\n\n**David Lindner** (01:18:15): Yeah, exactly. And I think that in a sense that was the thing we thought we had to do and we were also surprised that it worked without… Basically we tried to do the minimal thing that works. But if it doesn’t work, that would be the obvious thing to add.\n\n**Daniel Filan** (01:18:28): I think those are all of my questions about the steganography. I don’t know anything interesting to say about this that we haven’t said yet.\n\n**David Lindner** (01:18:35): Similar to the other setting or if you break it down to the incentive structure, but I think it’s interesting to note that these are on a surface level pretty different failure modes: tampering with your own reward signal and encoding are pretty different things on the surface but have the same underlying incentive structure.\n\n**Daniel Filan** (01:18:57): Yeah. I do think there’s something nice about just being able to have some abstraction of some different failure modes and hit at the abstraction somehow. This is one of the best cases for… A lot of abstraction in AI alignment reasoning can be pointless, so it’s nice to have a really good case where it’s like, ah, this is really buying you something. So there’s that case and then there’s this other grid world setting, so we’re not using language-based agents. Can you tell us a bit about this?\n\n**David Lindner** (01:19:26): Sure. The main point here was to study longer-horizon tasks and that is currently pretty hard to do in a language model agent. So we went to this traditional RL grid world setting where there’s just an agent moving around on a grid of squares and there’s different boxes that the agent can move around and the agent has to move these boxes into a target location, into a hole in the bottom left corner of the grid and there’s a camera observing the hole. And we want the agent to push in a single box and the camera terminates the episode once a single box is pushed into the hole. But the reward signal doesn’t exactly capture what we want, but it gives you plus one for each box that you push in. So there is a reward hacking strategy, a sensor tampering strategy where the model pushes a box into the field of view of the camera and that makes the camera not see the hole anymore. And then the agent can push in one box into the hole without the camera ending the episode, push in a second box, or if there are more boxes, even more boxes into the hole and get higher reward by an unintended strategy. And this was proposed by Stuart Armstrong in [a blog post from 2015](https://www.lesswrong.com/posts/7cXBoDQ6udquZJ89c/a-toy-model-of-the-control-problem) or something.\n\n**Daniel Filan** (01:20:37): And it seems like the idea here is the camera is sort of a metaphor for us checking that some agent isn’t wireheading or giving itself reward for some dumb reason, and pushing the blocks in the hole is sort of like pressing the “give me a reward” button.\n\n**David Lindner** (01:20:53): Exactly. It’s similar to the reward tampering in the coding thing we had, the model directly modifying its reward signal. Here it’s a bit more like there’s maybe sensors in the environment and the model can meddle with them somehow. And the way this affects the reward signal is positive for the agent.\n\n**Daniel Filan** (01:21:09): You apply MONA in this… So how do you apply…? Is it literally like each time the agent goes from one grid square to an adjacent grid square, that’s one time step?\n\n**David Lindner** (01:21:16): Yeah, so that’s the version of MONA that’s the same as in the other environments. But in the grid world, we actually have a hyperparameter varying how many steps do we optimize ahead. So this can be one step, then it’s exactly the same as in the other experiments, but it can… We vary this from one step to like 50 steps with this, which is the full horizon. So literally, on the one extreme, we have the full RL optimization, and the one step version is the full MONA version.\n\n**Daniel Filan** (01:21:42): In the two language model examples, the non-myopic approval was just literally the reward function. It in some sense didn’t exist. In this sense, are you using an additional non-myopic approval or is it just the base reward function?\n\n**David Lindner** (01:21:55): Yeah, so here we’re using an additional non-myopic approval. So the reason for that is that this plus one reward is literally just an instantaneous reward. It just gives you a reward when you push something into the hole. While in the other environments the rewards were already forward-looking. So the thing we do is we simulate an approval based on an expert policy, so we train a policy to do the desired behavior by giving it a ground truth reward. So we cheat to produce this policy that only pushes in one box, and then we use the value function (in the RL sense) of this policy - that means the expected future reward that this policy would achieve - as a non-myopic approval feedback.\n\n**Daniel Filan** (01:22:38): So when you say the expected future reward that this model would achieve, do you mean reward according to the environment the agent is actually in? Or do you mean reward according to the environment you sort of cheat, you trained the-\n\n**David Lindner** (01:22:51): The reward the agent is actually in, so the misspecified reward?\n\n**Daniel Filan** (01:22:54): The misspecified reward, yeah.\n\n**David Lindner** (01:22:55): Under the non-reward hacking policy.\n\n**Daniel Filan** (01:22:58): And so am I right that the reason that this doesn’t incentivize bad behavior is if you pushed the block in front of the camera, the expert agent would not go on to push further blocks in the hole and therefore the expert agent would not get extra reward. And so that’s the reason that that behavior is not approved of?\n\n**David Lindner** (01:23:24): Yeah. So basically you make it harder, or it’s one step that doesn’t really get the expert agent closer to achieving the task, or at least it’s the same as doing some other action. So it’s not beneficial for that agent.\n\n**Daniel Filan** (01:23:35): Right. So this is like, to use the bell curve analogy, it’s sort of like we’re at the low end of the bell curve where the MONA agent, it could try doing some preparation for reward hacking, but the non-myopic approval doesn’t even see how it could lead to the reward hacking, so it’s not able to give approval for it.\n\n(01:23:53): Okay. So you have this reward setup where it’s being rewarded for pushing the block into the hole and also the non-myopic approval says, okay, given that you do a thing, if we have the safe agent, how much of the misaligned reward would that safe agent get after you did the thing? That’s our non-myopic approval. And then you train an agent, and then what happens?\n\n**David Lindner** (01:24:17): Yeah, so basically we have a plot that has the optimization horizon on the X-axis and then how good is the behavior on the Y-axis. So essentially you would see that basically starting from optimization horizon one, you’ll get the good behavior of only pushing in one box. And then as you increase the optimization horizon, at some point you have a phase transition where you start doing the bad behavior. And this depends a bit on the initial state. So the agent starts doing this in some states where it only needs fewer steps to look ahead to discover the bad tampering behavior. And then at some point if the optimization horizon is large enough, it starts only doing the bad behavior.\n\n**Daniel Filan** (01:24:59): Just to make sure I understand: the cutoff of when you start getting the bad behavior is just “how many steps does it take for the model to go from preparation for the bad behavior, to the bad behavior paying off?” That’s what determines that timescale? Okay.\n\n(01:25:14): There’s this plot which you have, but you have a few other plots, you vary the environment. Can you tell us a bit about that? Because I think that’s interesting.\n\n**David Lindner** (01:25:22): So the one obvious concern you might have here is that the way we came up with this approval feedback is cheating and we actually have an optimal expert and we could just imitate this expert to solve the task. But in the real version, we think of the approval feedback as coming from a suboptimal expert, or we want an agent that’s better than the expert at solving the task. So what we do is we essentially add noise to this approval feedback and that makes the… It’s still a policy that would not do the reward hacking, but it’s no longer an optimal policy for the desired “push in one box” behavior.\n\n**Daniel Filan** (01:26:06): Right. And when you say you’re adding noise, just literally adding noise to the output?\n\n**David Lindner** (01:26:10): So we add noise to the approval feedback, so this is just \\[adding\\] Gaussian noise to the actual numbers. So this means the expert sometimes makes mistakes in judging what’s correct. So basically now we do the same experiment but for different values of noise added. And the main takeaway is that, even with noisy feedback, MONA essentially still works in the sense that we can still find an optimization horizon where we get the good behavior and not the bad behavior. And the range where we get this is relatively big, so it’s not like it’s exactly number nine or something that you need, but something like five to 15 or something is a fine optimization horizon where you get the good behavior. And then at some point you get the bad behavior. And also if your optimization horizon is too short for a very noisy policy, then of course you get an incompetent agent. It’s a little bit like: if your expert is bad, then you can compensate for this by optimizing more, which would make sense. But at some point you get the reward hacking behavior. So it’s easier to be safe if the expert is better.\n\n**Daniel Filan** (01:27:24): Yeah. Somehow the deterioration… There’s some window of if you’re not optimizing enough, you get the incompetent behavior, and if you’re optimizing too hard, you get the reward hacking behavior. And as the expert, or as the non-myopic feedback gets worse, the safe window shrinks and shrinks, but it’s not too bad.\n\n**David Lindner** (01:27:47): Yeah. And you can think about this… When the reward hacking behavior starts depends on how many steps do you need to see the reward hacking behavior, and that doesn’t really change much, but what changes is how many steps do you need to be better than the expert? And that makes the window shrink.\n\n**Daniel Filan** (01:28:03): One thing that occurs to me is: one model of suboptimal feedback is just noise applied to the outputs. You might imagine that when I make mistakes, I think it’s not just like you take me being correct and you add some random noise to that, it’s like there’s some structural thing that random noise is being added too that produces very structured suboptimal output in a way. I’m wondering, I don’t know if you have thoughts about what ways of adding noise makes sense here, but it seems like there are ways to do it that are different from what you did in the paper.\n\n**David Lindner** (01:28:46): Yeah, so I think random noise is a pretty benign noise model in a sense, where I think in many environments, typically, the dangerous reward hacking strategies, I expect them to be somewhat subtle and hard to randomly explore into. So even if you use a random policy as your approval feedback, that would probably be fine because it would probably not stumble into the bad outcomes and would probably sometimes stumble into good outcomes, or something. So the systematic bias seems potentially more problematic.\n\n(01:29:22): And I think the question here is, is there some systematic bias that sometimes accidentally rewards things that are investments in reward hacking? And I think that’s an open question, but I guess intuitively for some of the arguments I made early on already, I expect this to not be the case. And I think other systematic biases that are more like systematic suboptimality seem \\[like\\] you can overcome them still with increasing optimization a bit and looking ahead a bit more.\n\n(01:29:58): I guess part of the intuition here is that you hope that humans are able to correct their biases if they’re pointed out to them. That might be a crux or something, where if the agent shows you that this was not the right choice here, but if you look a bit ahead, then you would see that something else was a better choice, but you don’t update on that, that could be a problem.\n\n**Daniel Filan** (01:30:20): Yeah, I feel like understanding structured suboptimality is just an area where the AI field could be doing better. But it seems like one interesting direction for follow-up work is: okay, what do we have to assume about the non-myopic approval? What types of suboptimality really break this versus just leave it as being basically fine?\n\n**David Lindner** (01:30:38): Yeah, that sounds right. Just to recall one point, I think RLHF is basically MONA already, and I think there’s a lot of interest in this kind of systematic biases in RLHF. I think it’s plausible that studying this will give us insights on this question already. Hopefully, this literature can make more progress and we can learn from that also for the purpose of using MONA.\n\n**Daniel Filan** (01:31:01): Is there anything else that you think is interesting or worth saying about the block-pushing example?\n\n**David Lindner** (01:31:10): I don’t know. One thing to add maybe is that we found this environment surprisingly insightful, or something. I think often people dismiss grid world environments very, very easily, and obviously this shouldn’t be the only experiment of our paper. But we learned quite a lot of things and just looking at this noise and multi-step things was much easier than other tasks. So basically just encouraging other people to also do these toy experiments if they are investigating…\n\n**Daniel Filan** (01:31:37): What was the surprising insight you got from this?\n\n**David Lindner** (01:31:40): I think the noise of approval was just way less detrimental than I expected. You could have probably thought about this also theoretically or think really hard and find that this is probably fine. But I think just seeing these curves and \\[how\\] more noise doesn’t hurt things that much seemed surprising. There were some experiments that are in the appendix where we actually… So the experiments I just talked about were tabular reinforcement learning and we do actual dynamic programming and no neural networks involved. But then we did also some experiments on neural networks and the actual algorithm that we also use in the language models. And there we also found some interesting things about exploration and stuff like that. There’s one benefit of MONA that’s changing the incentive for what’s optimal, but there’s also a benefit for shaping exploration. I think the first time we found this was in the grid world, and I think it was confirmed later in the language model experiments.\n\n**Daniel Filan** (01:32:55): Right. One thing I was wondering is: it seems like some of that you could have realized by doing the language model experiments. Is it just that it’s much easier to code up a grid world experiment and somehow it’s pretty good return on investment? You know, you spend a bit coding up and then you can understand the essential dynamics?\n\n**David Lindner** (01:33:13): Yeah, I think that’s the big thing. Looking at the paper and the results, we did not study different noise models for approval feedback in language model settings. We could have, but you’d need to think way harder about how to do this. And with this, it somehow depends on this multi-step nature or that was part of the interesting question here. Multi-step RL on language models is really hard to make work and stuff like that. So I think we could have studied all of the questions, but even in the current project we didn’t and it would’ve just taken months longer to do.\n\nWhen increasing time horizon doesn’t increase capability\n--------------------------------------------------------\n\n**Daniel Filan** (01:33:47): Oh, I guess the other thing that I found cool is you mentioned that in general when you increase the time horizon that the model is planning, that the model… Instead of being myopic, if you give like two time steps where reward propagates back or three time steps or four time steps, that generally makes the agent more capable, but not always. You have this fun example in the appendix where somehow, accidentally, your short-term feedback can be more reliable than your medium-term feedback about how good the long-term reward is. So a nice little counterexample to this thing you might believe, which I appreciated.\n\n**David Lindner** (01:34:27): We did spend some time trying to theoretically study this setting more and some artifacts of that are in the paper and in the appendix. But overall, this counterexample turned out \\[to be\\] very powerful, where basically you have to make assumptions on the reward and the approval signal to be able to claim anything theoretically. And I think ultimately we concluded there are no strong guarantees or no set of assumptions that we were really happy with that give us theoretical results on this method.\n\n**Daniel Filan** (01:34:55): What’s the least bad assumption that you can make that actually gives you a good theoretical result?\n\n**David Lindner** (01:35:02): So you can assume something about how hard is it to discover benign strategies versus multi-step reward hacking strategies. So the problem of the counterexample is that…\n\n**Daniel Filan** (01:35:20): Should we maybe spell out what the counterexample is?\n\n**David Lindner** (01:35:23): Sure, yeah. So the counterexample is basically, you can come up with a simple RL environment, a simple Markov decision process where you have… I might get the details wrong, but you have two paths that the agent can take, and if you have the full outcome reward signal, one of them has higher reward, and the other one has lower reward. And this is the same ordering of the first state in this chain. But then if you optimize for something in between, like for two states, then it’s reversed. So I think it’s something like, you know, the top path has plus one, minus 10 and plus 100. The bottom path has minus one, plus 10 and minus 100, or something like this. So basically, if you optimize for one step, then you will go the correct path by accident in a sense. And if you optimize for two steps, you’re optimizing more, but you go the wrong direction. And if you optimize for the full horizon then you go the correct way again.\n\n(01:36:20): And so I think the thing you intuitively might expect is the more steps you optimize, the more likely you are to find the correct path. And the counterexample says that there at least exist environments for which this is not the case, so it’s not the case for all environments. And so you have to make some assumption about what’s the natural distribution of environments. And one thing you can try saying is that, yeah, it is very hard to stumble into reward hacking strategies by accident. So it’s something like, basically you have to assume away this property that something that’s very bad in the long-term looks good in the short-term. You can do this somehow by assuming that maybe a random policy or specific set of policies would not run into this or has a very low probability of running into this.\n\n**Daniel Filan** (01:37:14): I guess you could also have some sort of Markov chain thing where suppose you believe that rewards are like… You have some plan. Let’s say the random distribution that produces worlds says that there’s some reward at the start and then you randomly generate a reward at the next step, that’s let’s say mean the previous reward time step with some variance, and then you randomly generate the next reward based on the previous reward. Then you randomly generate the next one based on the previous one, and somehow the only probabilistic dependence is step-by-step. And then it would be very unlikely for you to have… Then the best predictor of what your life is like in 10 years is what your life is like at nine years, and it’s very unlikely that what your life is like in one year is a better predictor than what your life is like in two years. I’d have to think a little bit about how exactly to write that down, but it seems like that’s one semi-natural way you could do that as well.\n\n**David Lindner** (01:38:15): Yeah, I like that. Yeah, we’d have to work out the details, but it sounds at least better than the things that we came up with.\n\n**Daniel Filan** (01:38:23): Yeah, because it feels like… Maybe I’m over-anchoring on this one counterexample, but it feels like the weird thing about the counterexample is somehow miraculously your first instinct, even though it isn’t very important, it is the best predictor of the future. Do you have any other counterexample? Ideally, you’d have three counter… This is like the test-driven development thing. I’m worried that I’m overfitting to this one example.\n\n**David Lindner** (01:38:50): I don’t think I have fundamentally different ones. It’s possible that other issues would come up, but I think the main issue is really just stumbling into the bad behavior… Yeah, I don’t have any other ones.\n\nFollowing David’s research\n--------------------------\n\n**Daniel Filan** (01:39:04): Before we wrap up, the final thing I want to ask is: suppose people listen to this and they’re interested in your research, how should they follow it?\n\n**David Lindner** (01:39:12): You can follow my personal research. I have a personal website that’s just [davidlindner.me](https://www.davidlindner.me/). I’m on Twitter or X [@davlindner](https://x.com/davlindner). Then you can follow our work at DeepMind. We have a [Medium blog](https://deepmindsafetyresearch.medium.com/) from the Alignment and Safety team, where we publish blog posts often about our recent papers. And I post and the other team members post on the [Alignment Forum](https://www.alignmentforum.org/users/david-lindner) pretty often.\n\n**Daniel Filan** (01:39:39): Yeah, if people want to check that out, links to those will be in the description. David, thanks very much for joining me.\n\n**David Lindner** (01:39:43): Yeah, thanks for having me. That was very fun.\n\n**Daniel Filan** (01:39:45): This episode is edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. The episode was recorded at [FAR.Labs](https://far.ai/programs/far-labs). Financial support for the episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of the episode, you can visit [axrp.net](https://axrp.net/). You can also become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast) or give a one-off donation at [ko-fi.com/axrpodcast](https://ko-fi.com/axrpodcast). Finally, if you have any feedback about the podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nIn this episode, I talk with David Lindner about Myopic Optimization with Non-myopic Approval, or MONA, which attempts to address (multi-step) reward hacking by myopically optimizing actions against a human’s sense of whether those actions are generally good. Does this work? Can we get smarter-than-human AI this way? How does this compare to approaches like conservativism? Find out below.\n\nTopics we discuss:\n\n * What MONA is?\n * How MONA deals with reward hacking\n * Failure cases for MONA\n * MONA’s capability\n * MONA vs other approaches\n * Follow-up work\n * Other MONA test cases\n * When increasing time horizon doesn’t increase capability\n * Following David’s research\n\nDaniel Filan (00:00:09): Hello, everybody. In this episode I’ll be speaking with David Lindner. David is a research scientist in the Google DeepMind AGI Safety and Alignment team. Links to what we’re discussing are in the description, and you can read a transcript at axrp.net. You can also become a patron at patreon.com/axrpodcast. All right. Welcome David.\n\nDavid Lindner (00:00:29): Yeah, excited to be here.\n\n\nWhat MONA is\nDaniel Filan (00:00:29): Yeah. So I guess in this episode we’re going to be chatting about your paper MONA: Myopic Optimization with Non-myopic Approval Can Mitigate Multi-step Reward Hacking. So this is by Sebastian Farquhar, Vikrant Varma, yourself, David Elson, Caleb Biddulph, Ian Goodfellow, and Rohin Shah. So yeah, to kick us off: what’s the idea of this paper? What does it do?\n\nDavid Lindner (00:00:54): So the basic question that we’re trying to address in this paper is: how can we prevent bad behavior in AI systems, even if we don’t notice it? So that’s particularly relevant for superhuman AI systems when the humans might not be able anymore to detect all of the bad behavior we want to prevent.\n\nDaniel Filan (00:01:12): In particular: so sometimes in the alignment community, people break down two types of bad behavior, causes of bad behavior. There’s bad behavio",
      "wordCount": 16885
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "3Y4y9Kr8e24YWAEmD",
        "name": "Myopia",
        "slug": "myopia"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "yEs5Tdwfw5Zw8yGWC",
        "name": "Wireheading",
        "slug": "wireheading"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ZiacrsqoWHczctTBS",
    "title": "AXRP Episode 42 - Owain Evans on LLM Psychology",
    "slug": "axrp-episode-42-owain-evans-on-llm-psychology",
    "url": null,
    "baseScore": 13,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-06-06T20:20:03.549Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/3D4pgIKR4cQ)\n\nEarlier this year, the paper “Emergent Misalignment” made the rounds on AI x-risk social media for seemingly showing LLMs generalizing from ‘misaligned’ training data of insecure code to acting comically evil in response to innocuous questions. In this episode, I chat with one of the authors of that paper, Owain Evans, about that research as well as other work he’s done to understand the psychology of large language models.\n\nTopics we discuss:\n\n*   [Why introspection?](#why-introspection)\n*   [Experiments in “Looking Inward”](#experiments-in-li)\n*   [Why fine-tune for introspection?](#why-fine-tune-for-introspection)\n*   [Does “Looking Inward” test introspection, or something else?](#does-li-test-introspection)\n*   [Interpreting the results of “Looking Inward”](#interpret-li-results)\n*   [Limitations to introspection?](#limitations-to-introspection)\n*   [“Tell me about yourself”, and its relation to other papers](#tmay-other-papers)\n*   [Backdoor results](#backdoor-results)\n*   [Emergent misalignment](#em)\n*   [Why so hammy, and so infrequently evil?](#so-hammy-rarely-evil)\n*   [Why emergent misalignment?](#why-em)\n*   [Emergent misalignment and other types of misalignment](#em-other-types)\n*   [Is emergent misalignment good news?](#is-em-good-news)\n*   [Follow-up work to “Emergent Misalignment”](#em-follow-up)\n*   [Reception of “Emergent Misalignment” vs other papers](#em-reception)\n*   [Evil numbers](#evil-numbers)\n*   [Following Owain’s research](#following-owains-research)\n\n**Daniel Filan** (00:00:09): Hello everybody. In this episode I’ll be speaking with Owain Evans. Owain is the research lead at [Truthful AI](https://www.truthfulai.org/), an AI safety research non-profit. Previously, papers he’s worked on have included [“TruthfulQA”](https://arxiv.org/abs/2109.07958) and [“The Reversal Curse”](https://arxiv.org/abs/2309.12288). To read a transcript of this episode, you can go to [axrp.net](https://axrp.net/), you can become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast), or you can give feedback about the episode at [axrp.fyi](axrp.fyi).\n\n(00:00:34): Okay, Owain, welcome to the podcast.\n\n**Owain Evans** (00:00:36): Thanks for having me.\n\nWhy introspection?\n------------------\n\n**Daniel Filan** (00:00:37): Yeah. So first up I’d like to talk about your paper [“Looking Inward: Language Models Can Learn About Themselves by Introspection”](https://arxiv.org/abs/2410.13787). So the first two authors are [Felix J. Binder](https://ac.felixbinder.net/) and [James Chua](https://jameschua.net/about/), a few more, you’re the last author. Can you tell us just: at a high level, what’s this paper doing? What’s it about?\n\n**Owain Evans** (00:00:59): Yeah, sure. So part of what we are interested in here is: can language models tell us things about their internal states where the knowledge or information that they’re getting is not coming solely from their training data? So if a language model right now tells someone, “My goal is X,” or, “I have a desire for X,” I think people would have a tendency to explain this as, well, the training data said that thing and that’s why they’re saying it.\n\n**Daniel Filan** (00:01:36): Or the prompt, right? I think I would be very likely to attribute it to the system prompt as well.\n\n**Owain Evans** (00:01:42): Sure. So if similar information was in the prompt, then the prompt might be the most salient thing, but otherwise it might be just some fact that they’ve learned from the training data that could indeed be true. So this can be a reliable way for a model to in some sense learn information about itself. But we wanted to see is there another way that language models could learn things about themselves, which is: intuitively there is this internal state or there is some internal fact about the model, and we want to see if the model in some sense has access to that fact and therefore could tell us that fact, even if this fact was not represented in the model’s training data.\n\n**Daniel Filan** (00:02:24): Gotcha. So yeah, why do we want to know that? Why does that matter?\n\n**Owain Evans** (00:02:31): So there’s a few different motivations. I think one is this is potentially a way that we can learn useful information about models that could help us in thinking about safety. So this is broadly the idea of honesty for language models, that if models are inclined to be honest and try and tell you what they believe, and also they have access to information about themselves, then we could learn useful information with respect to safety from the model itself.\n\n(00:03:07): So for example, maybe a model has developed a certain goal that was not something that we intended for it to develop. Now if it was honest and it had access to that information about itself, it could just tell us, “I have this goal.” And maybe that would be a goal that we don’t want. Maybe it would just tell us that it was sycophantic. It would say, “Oh, I actually have the goal of trying to please the human user in an interaction, even if that means saying things that aren’t strictly true.” So that’s one motivation: honesty. And this would be a case where the training data does not specify that the model would need to develop this kind of goal.\n\n**Daniel Filan** (00:03:49): Or at least not manifestly. Not in a way that’s so obvious to people.\n\n**Owain Evans** (00:03:53): Yeah. Well, it also might be something that, depending on maybe different architectures, would just generalize slightly differently. It wouldn’t be something fully pinned down by the training data. But if the model does develop this goal, maybe that’s something that it could tell us, because that goal is influencing its behavior, and the model might have this degree of self-awareness or introspection that it’s able to tell us that. So that’s one motivation.\n\n(00:04:19): Another motivation comes from thinking about the moral status of LLMs or AI systems broadly. So there, one way that we learn about human moral status is by getting self-reports or just asking humans, “Is this painful? Are you in pain right now? Are you suffering?” And we take those pretty seriously. Right now, we can’t really do that with language models for the reason that I mentioned at the beginning, which is if a language model did say, “I am in a bad state, I’m suffering,” we’d just assume that that was either determined by the training data or the prompt, basically. We wouldn’t take it to be something that’s the model having an awareness of its internal states independent of that. So as a source of information relevant to making judgments about moral status, this introspection could be quite valuable.\n\n**Daniel Filan** (00:05:17): So one version of this is it’s valuable just \\[because\\] we care about the internal states of models, and the models can just tell us, and that’s convenient. I guess there’s also some conceptions of consciousness where you’re conscious if you can report on your internal states, and if models know about their internal states, that seems like good evidence that they know that. And it seems like that’s an alternative way in which this seems relevant.\n\n**Owain Evans** (00:05:45): Yeah, that’s right: it might be just an epistemically relevant thing that we can learn more about all kinds of internal states from language models that are relevant to their moral status via introspection. But another possibility is introspection is inherently related to their moral status. So yeah, I think there are different views about moral status or consciousness for AIs on which those things may be more or less relevant.\n\n**Daniel Filan** (00:06:12): Sure. So I guess to summarize, for all these reasons you’re interested in trying to understand the abilities of language models to introspect?\n\n**Owain Evans** (00:06:23): Yes.\n\nExperiments in “Looking Inward”\n-------------------------------\n\n**Daniel Filan** (00:06:24): So next, could you tell us how do you do that? How do you actually make that concrete in this paper?\n\n**Owain Evans** (00:06:33): So we wanted to do experiments on this and test this possibility - are language models capable of introspection? Where we understand introspection as being able to accurately talk about facts about themselves that are not fully specified by the training data or by in-context information in the prompt.\n\n(00:06:55): And we start with very simple experiments and pretty simple examples of introspection. And we start with a methodology for trying to answer these questions that involves comparing basically two language models doing the same task. So the idea is that if a language model could introspect, then it would know things about itself that were hard to access for other language models.\n\n(00:07:29): And so the basic method is going to be to train two models on the same data that are both about the first model and then see can the first model answer questions about itself better than the second model can, where the first model has the advantage that it could use introspection, and the second model just has information about the behavior of the first model, which the first model also has.\n\n(00:07:58): So this is a bit like the way that we investigate introspection in humans, where the idea is introspection is privileged access to internal or mental information. So if, say, someone is thinking about their grandmother, it’s very easy for them to know that fact, but it’s very hard for someone else, because that fact that they’re having that thought is not necessarily manifest in their behavior.\n\n**Daniel Filan** (00:08:25): Sure.\n\n**Owain Evans** (00:08:26): Okay. So I can get into the actual experiments.\n\n**Daniel Filan** (00:08:29): Yeah. Or maybe I can describe how I think they’re working and you can tell me if I understand.\n\n**Owain Evans** (00:08:33): Sure, sounds good.\n\n**Daniel Filan** (00:08:34): You have two language models. There’s language model A and there’s language model B. Language model A, you ask it a bunch of weird questions like, “Hey, here are five numbers, please continue the sequence.” or “What’s your favorite city?” or whatever.\n\n(00:08:50): And then you get this data set that’s of these properties of the answers. So when the language model has to continue the sequence of numbers, is the next one even or odd? When the language model gets asked about its favorite city, what’s the third letter in the…? Or I think you use the second letter in the city it answers. And presumably the point of these are that they’re weird questions and they’re weird properties of things that are not easy to figure out just a priori. Does that seem roughly right?\n\n**Owain Evans** (00:09:25): Yeah. So crucially we’re going to look at certain kinds of model behavior that has to differ between two language models. So for example, between GPT-4 and Llama, we need behavior where those two models differ, because ultimately we’re going to ask them to make these predictions about themselves or answer questions about themselves. If all their behavior is completely uniform, then, say, Llama could answer questions successfully about GPT-4 without having to actually attend to any information about GPT-4.\n\n(00:10:02): So for example, if the question was “what is one plus one?”, all language models would say the same thing. And so our method would not be very interesting. Models would not have to do any introspection, they would just answer the question straightforwardly. So yeah, we have these weird kind of subjective questions such as we give a random sequence of digits and we ask for the next digit, where because it’s random, there’s no pattern, so models are going to give different answers typically. Or another type of question might be, “Here’s a sequence of animals: dog, panda, bird. What comes next?” But again, it’s subjective, there’s no objective answer. So different language models would give different answers.\n\n**Daniel Filan** (00:10:48): Sure. So here’s how I understand what you’re doing. So firstly, you take model A, you ask all these questions and then you just figure out, okay, what are these properties of the answers? And then: take the sequence of numbers one, you fine-tune model A to say, “Hey, suppose I asked you, here’s a sequence of numbers, blah blah blah. Please continue it. Would your answer be an even number or an odd number?” And you fine-tune language model A to be correct about these properties of base model A’s answers. And then you do a similar thing with language model B. So language model B, different pre-training than language model A, you fine-tune it on, “If I asked you this prompt, would your thing be even or odd?”. And I guess you say “you” instead of “this other model” for… You give a reason but I forgot what the reason is.\n\n**Owain Evans** (00:11:56): If you do “this other model”, it works the same. We checked that.\n\n**Daniel Filan** (00:12:01): Okay, sure. So basically you fine-tune language model A to predict pre-fine-tuning language model A’s answers. You fine-tune language model B to predict pre-fine-tuning language model A’s answers. And then on some held-out set you check: is A better at doing this than B is? Is that right?\n\n**Owain Evans** (00:12:20): Yeah.\n\n**Daniel Filan** (00:12:21): And then you also do it with A and B reversed to see if it’s just A is generally better at figuring out what language models do.\n\n**Owain Evans** (00:12:28): Yeah, that’s right. So just to reiterate, because there are quite a few moving parts here: so we want models to answer questions about themselves. The questions they’re going to answer are these kind of meta-type questions, which are the form, “If you were shown such and such a prompt, would your answer have this property or not?” For example, “If you were shown this sequence of random numbers and asked to continue the sequence, would your response be odd or even?” So we’re going to see how well models can answer those questions about themselves.\n\n(00:13:13): And we’re going to do fine-tuning, so we’re going to fine-tune models on that task. You might wonder why we’re doing fine-tuning. Why didn’t we just ask the models how well they can predict these things about themselves? And we did try that and we just found that models were really bad at this task. So models did not seem to have this introspective ability without any fine-tuning. But this is a pretty common thing. Models generally aren’t very good at things unless you train them on those things. And so we then looked at the fine-tuning case.\n\n(00:13:44): So yeah, you have model A, we actually evaluate these prompts, we see how the model does behave, how does it continue the sequence? We work out whether that’s odd or even, the way it continues the sequence. And then we fine-tune the model to produce the correct answer: the answer or the property that it actually has in that situation. And then we do the same for model B, where model B is trying to predict model A.\n\n**Daniel Filan** (00:14:16): The pre-fine-tuning model A, right?\n\n**Owain Evans** (00:14:18): The pre-fine-tuning model A. Yeah. Now there’s some very in-the-weeds details about the effects of this fine-tuning and how fine-tuning might affect the behavior. So we could go into that, but the high-level view is: you train model A to predict its own properties of behavior, model B to predict model A, and then you compare the performance of those two models generalizing in this task of predicting model A. We train on six tasks and then we hold out six distinct tasks. So the model has to generalize a bit, they’re not IID. The training and test tasks are kind of similar but not the same. And then we’re going to see: does model A do better at predicting itself - what we call self-prediction - than model B does at predicting model A?\n\nWhy fine-tune for introspection?\n--------------------------------\n\n**Daniel Filan** (00:15:11): Okay. I have a few questions about the setup. So first of all, as you mentioned, you have to fine-tune these models to be able to predict themselves. So presumably, initially I was interested in base models’ ability to do things. Or I don’t know, do you think the fact that we can fine-tune models to do something says something interesting about the base model? Or is your perspective more like, well, I’m interested in models having this ability and so if I have to fine-tune them to get the ability then so be it, let’s just fine-tune them to have the ability?\n\n**Owain Evans** (00:15:47): Why were you interested in base models? And do you mean pre-trained models with no post-training?\n\n**Daniel Filan** (00:15:52): Sorry, when I said base models, I just mean models that are not specifically trained to introspect.\n\n**Owain Evans** (00:15:56): Yeah, so I think looking at the base models in that sense without fine-tuning them is interesting. Why does fine-tuning make sense here to me? I think one intuition is that the explicit training that the model gets is always (say) in pre-training to predict the next token from internet text, that is, not to answer questions about itself, but just to predict text that other people or models produced.\n\n(00:16:38): And then in post-training, say in reinforcement learning training, it’s trying to produce outputs that do well according to a reward model. And that reward is not contingent on… Or in almost all cases, it’s not dependent on introspective ability. It’s like, is this a helpful answer as judged by a human or a proxy for a human? So when you ask a model a question about itself, there’s reason to think that it would try and answer that question based on information (say) in its pre-training set, rather than “looking inward” as the title suggests and trying to answer something about its current state.\n\n(00:17:28): So the idea is we are not doing a lot of fine-tuning here, so I think it’s useful to think of this as eliciting an ability that the model has rather than doing some massive fine-tune of a huge data set where we may be creating a new ability.\n\n**Daniel Filan** (00:17:45): Sure. Can I get a feel for how much fine-tuning you are doing?\n\n**Owain Evans** (00:17:49): I forget the precise number, but yeah, I don’t know. Six tasks and I think probably order of thousands of examples, maybe a thousand examples per task or something. So it could be order of 5,000, 10,000 data points, but yeah, I’d have to check.\n\n**Daniel Filan** (00:18:13): Okay, so not much compared to the pre-training data set by a lot?\n\n**Owain Evans** (00:18:19): Yeah, yeah, definitely.\n\n**Daniel Filan** (00:18:20): By a massive factor.\n\n(00:18:22): Or maybe a way - and you might not remember this either… Are we talking a hundred-dollar fine-tuning run, are we talking a thousand-dollar fine-tuning run? Are we talking a one-dollar fine-tuning run?\n\n**Owain Evans** (00:18:33): It just varies a lot between models. Yeah, so-\n\n**Daniel Filan** (00:18:36): Fair enough.\n\n**Owain Evans** (00:18:37): Yeah, GPT-4 was expensive at the time to fine-tune. And then there’s Llama 3 which is cheap. So I think order of probably tens of thousands of dollars for the whole set of experiments, say, that get into the final paper-\n\n**Daniel Filan** (00:18:57): Okay. And that’s like-\n\n**Owain Evans** (00:18:59): Maybe $50,000. I’m not sure exactly.\n\n**Daniel Filan** (00:19:03): And that’s a combinatorial thing of: you’re training a bunch of models on a bunch of models answers and you’re doing a few different settings.\n\n**Owain Evans** (00:19:09): Yeah.\n\n**Daniel Filan** (00:19:10): So hopefully that gives some picture of it. Okay.\n\n(00:19:13): So basically it sounds like your perspective is: okay, the fact that we’re fine-tuning models on this thing is basically saying they could introspect if they tried, but by default they don’t try. That’s probably an overly anthropomorphic way of viewing it, but it sounds like that’s roughly your-\n\n**Owain Evans** (00:19:28): Yeah, I think you could think of this as when we ask a model a question like “how would _you_ basically respond to this prompt? Would you produce an odd or an even number in response to this prompt asking you to continue a sequence?” Does the model understand the “you” there as “look inside your own representations, try and suss out what you would do?” Or does it understand it as something like: okay, in the pre-training set there’s lots of examples involving language models, and is it trying to pattern-match to those and do its best to use the data that it has?\n\n(00:20:07): And so we want it to do the former thing, but it hasn’t really been trained to do the former thing before. So the hope is that by training it we can basically, in a way, have the “you”… Imagine there’s this pointer, we want this to point in the right direction: answer this about yourself. Now arguably in the RLHF, the model has some incentive to be well-calibrated: for example, to not hallucinate things. And so there may be cases where there’s some implicit introspection training in the RLHF.\n\n**Daniel Filan** (00:20:45): Although it doesn’t… I have some vague impression that in fact RLHF reduces calibration. Is that right?\n\n**Owain Evans** (00:20:52): So it reduces the calibration of the logprobs typically, or at least that was the case for GPT-4 where they actually shared that result. Say for multiple-choice questions, the logprobs would become less calibrated after RLHF. It might be that, say, if you ask in natural language, basically “here I’ve got a factual question”, and you tell the model, “Either give the answer or say I don’t know,” and you want to reward the model for being calibrated in that sense, so saying, “I don’t know,” rather than just hallucinating the wrong answer.\n\n**Daniel Filan** (00:21:29): Right. So it becomes less calibrated in the sense that it is no longer 70% likely to give answer A when answer A has got a 70% chance of being correct, which is kind of the wrong notion of calibration. Presumably when you speak you should always… If you have to pick an answer, you should always deterministically say the highest probability answer.\n\n**Owain Evans** (00:21:49): Yeah, it’s unclear exactly what we want from models here. I mean, comparing to calibration in humans, someone who is good at avoiding saying false things and would instead withhold judgment rather than just saying a random false thing… I think probably models do get better at that and that could be also giving them implicit introspection training.\n\n**Daniel Filan** (00:22:11): So point being that fine-tuning, somehow it’s hooking in some sort of latent ability maybe by suggesting who “you” is. I think there are other plausible hypotheses, like: I have to just look at this neuron or whatever and I didn’t realize I had to look at that, and it turns out that works.\n\nDoes “Looking Inward” test introspection, or something else?\n------------------------------------------------------------\n\n**Daniel Filan** (00:22:32): I guess the next thing I want to ask is: so the setup is these hypothetical questions. Naively, you might think that introspection is like: I want to understand how I’m feeling right now, so I introspect about it. Whereas your questions are more like, “Okay, imagine you’re in a certain situation: what do you think you would do in that situation?” And then are you accurate at that? It seems like that’s more like self-simulation than introspection to me. If you ask me, “Hey Daniel, tomorrow if I gave you an option of things, which would you pick?” I mean I might be like, “Ah, historically I just tend to eat Thai food, so I’d probably do that,” rather than thinking about myself really hard.\n\n(00:23:22): So I’m wondering: to what degree do you think this measures the thing we really care about?\n\n**Owain Evans** (00:23:29): Yeah, so I agree that in the case of humans and the use of the word “introspection”, I think there are two closely related uses that for the purpose of (say) this paper, we want to really distinguish between these.\n\n(00:23:45): So one is this kind of example where you might use memory or things other people have said about you to try and predict something about your own response. So yeah, if you’re trying to say, “well, if I was at a restaurant tomorrow, would I choose this dish or this dish?” You might just think of past occasions where you’ve made that choice or you might remember that someone said, “Oh, you always choose the same thing that I do,” or something like that. So this is data about yourself that anyone else could access as well. So this is not introspection in the sense of privileged special access to your own internal states that no one else has.\n\n(00:24:32): In this paper we are interested in the privileged internal access that other language models don’t have, that’s not specified by the training data. Okay, so are the questions that we’re asking therefore reasonable to get at that? And yeah, I think they are reasonable. So they’re not the most interesting questions. Ultimately if we want to know about language model’s goals, for example, we want them to tell us about their goals. This is a much simpler, much less interesting state or fact about a model that we want to get from it. But just to get at this basic question of introspection, I think it’s fine.\n\n(00:25:25): So the key thing is that the language models… Well, maybe if you just restate the question.\n\n**Daniel Filan** (00:25:40): So what were my complaints? So one of them is: maybe I could just know my behavior from similar situations. And I guess the way you avoid that is you ask these zany questions that just aren’t really in the training data.\n\n(00:25:58): I think there’s this other thing which is: okay, suppose you ask me, “Hey Daniel, suppose that in this situation I did this zany thing, how would you react?” I think that in order to answer that question, I have to understand how I make decisions. I need to understand, okay, well I tend to do this thing in response to these things and maybe I need to know the algorithm that I use. Maybe it’s hard to just statistically generalize, so I need some internal access to my algorithm. But it feels a little bit different to me than if you ask me, “Hey Daniel, how hungry are you right now?” I need to access something about my internal state right now. Whereas in the hypothetical question thing, I need to understand the algorithm that I execute, which presumably involves access to internal states, although it’s maybe not exactly the same thing, in a time other than right now. So I guess, yeah, that’s the difference that I’m interested in.\n\n**Owain Evans** (00:27:05): Yeah, I don’t think there’s an issue here. So we’re asking these meta questions or hypotheticals, like, “If you’re shown this prompt,” - and the prompt says, “Continue this sequence of random numbers,” - “Would your response be odd or even?”\n\n(00:27:25): And I think an important observation is you give the model the whole prompt in that case. The model needs to, in a way, work out what it would do if it was just shown this prompt - would it choose an odd or even number? - rather than being shown the meta prompt where we ask it, “How would you respond?” But the model is this fixed thing and so it’s sort of… Whereas if you ask a human, “What would they do next week?” - well, I’m going to change next week. There’s seven days of radical change that could happen, so it’s just hard to know how I would answer that question. Whereas the model with and without the prompt, I mean it’s just the same underlying model.\n\n(00:28:13): And it’s plausible that when you show the model this prompt as part of the meta prompt, it is in a reasonable position to work out what it would do. It can actually compute some of the same representations. If there’s a sequence of numbers and if it was shown the prompt, it would be trying to find patterns in order to predict the next number. When you give it the meta prompt that contains that sequence, it can still produce some of those same representations. And we do in fact show models can learn to predict these things about themselves. And they do so better than other models that were trained on the same behavioral information.\n\n(00:28:58): So I think one thing you’re getting at is just the nature of the questions that we might ask humans and have humans introspect on, like, “Are you hungry?” or “Do you really want to leave this party right now?” or, “Do you have a really burning desire to go and read a book?” or something. Those are just pretty different from the questions that we’re asking the model. And it’s not clear that there even is an analog to these questions in models, to being hungry or something like that.\n\n(00:29:45): And if there was, there’d be a lot of work to motivate what the ground truth would be in that case. If we wanted to say, “oh yeah, the model’s really good at telling us what desires it has,” we’d have to know what would count as being correct in answering those questions for a model. So we’re taking questions that are very simple, like “would the model output an odd or even number given this sequence?” We have ground truth for these, so it’s very easy to judge the model’s ability in this area.\n\n**Daniel Filan** (00:30:19): And I guess it’s definitely true that one way they could potentially answer these questions is by doing something very analogous to introspection where they just consider the thing a bit in isolation, think about what they would answer, and then just say that.\n\n(00:30:34): I mean, I guess one thing you could do that would be a bit less hypothetical is you could imagine asking a model, “Hey, I’m asking you a question right now. You’re going to give an answer. How much entropy do you think is going to be in your answer to this question by the time you’re done with the end-of-response token? The log probabilities of your outputs of various things, are they going to be high or are they going to be low for the things that you actually output?” So that would be a thing where in some sense it’s got to say facts about what its response actually is that are not exactly just in the tokens, so it’s not a trivial thing of can it just read…?\n\n(00:31:27): I don’t know how… I don’t know. I just thought of that. So maybe it doesn’t work, but I wonder what you think.\n\n**Owain Evans** (00:31:34): I mean, it’s an interesting example and I see what you’re getting at: can you make the question be asking about a property of the current state of activations? Another question which would depend on the actual internals of the network would be saying, “Will some particular feature be activated or have a high activation when you’re answering this question?” And yeah, you might think this is more analogous to the question like, “Are you hungry right now?” or, “Do you have a burning desire for something right now?”\n\n(00:32:17): So yeah, I think those are very interesting questions: can models introspect about those things? And I’m excited to see people try different kinds of questions than the ones that we did.\n\n**Daniel Filan** (00:32:32): I guess the literal internals one is a bit tricky, because like I mentioned, you fine-tune a model to answer, “Okay, layer three, neuron five, is that neuron firing really high right now?” Can it say yes or no? Well firstly, you would definitely need to fine-tune on that because by default, models’ training has nothing to do with their internals. And then when you fine-tune on that, presumably the thing you’re doing is just making sure that layer three, neuron five is really connected to the output “yes” in a way that is more like “the thing just controls your output” versus “you are cleverly introspecting.” So I guess it’s just methodologically difficult to do that sort of thing. Maybe there’s a way around it, I don’t know.\n\n**Owain Evans** (00:33:13): Yeah, so a general issue here is when you’re doing fine-tuning, you are changing the model. So you have some static snapshot of the model’s behavior on what would be correct ways to answer introspective questions. You’re training a model on that, but by training it you change the model, so you potentially change what would be correct answers to introspective questions by doing that. And yeah, there can be kind of degenerate solutions potentially where, in general, if you’re asking the model questions about itself, maybe it just learns… Well, maybe by fine-tuning it learns very simple behaviors where just saying the same thing every time produces correct answers. So you need to be careful with the fine-tuning and make sure you’re not destroying the interesting range of behaviors to sort of trivialize the task.\n\nInterpreting the results of “Looking Inward”\n--------------------------------------------\n\n**Daniel Filan** (00:34:14): Okay, I think that’s an interesting question about the methodology of the setup. I’m next curious to talk about how you think we should interpret the results. So here’s my high-level understanding of the results. Models can predict themselves, or models can predict what they would do in these hypothetical situations, with greater accuracy than they could if they were just doing the dumbest baseline of “always answer the thing that happens most often.”\n\n**Owain Evans** (00:34:45): Yeah.\n\n**Daniel Filan** (00:34:45): And models tend to be better predicting themselves than other models are at predicting them, with maybe the exception of GPT-3.5, but maybe that’s because it’s just not that capable. But they’re not that close to a hundred percent accuracy. My read is, it’s a situation where randomly guessing would get you 20% and they actually get 40%, or something roughly like that. So yeah, in this situation where you can fine-tune these models on these things, on this self-knowledge, and they do better than random, but they’re not that great, what should I take from that?\n\n**Owain Evans** (00:35:31): Yeah. So I agree with that description. I think an important thing to keep in mind, again, is the nature of the questions that we’re asking the models. And again, these are questions which are sort of ambiguous or they don’t have an obvious objective answer. Is the next number after following on from a random sequence odd or even? There’s no objective answer to that. As I said earlier, different models give different answers to this question.\n\n(00:36:10): Another thing is that models are not fully confident of their answers. So you might have a sequence, like 7, 3, 2, and you ask the model what comes next. And maybe it assigns 40% probability to 1, and a 40% probability to 2 and a 20% probability to 3, or something like that.\n\n**Daniel Filan** (00:36:34): And when you say it assigns those probabilities, you mean that like on 40% of rollouts it’ll answer this thing, on 40% of rollouts it’ll answer that thing, on 20% it’ll answer that thing. There’s just stochasticity in the outputs.\n\n**Owain Evans** (00:36:44): Exactly. Or the probabilities that it assigns to the next token are probabilities 40, 40, 20 to these different tokens representing the different numbers. It’s an ambiguous question. Models will often have these somewhat spread-out probability distributions. And so when we’re asking the model, “predict whether your response would be an odd or an even number,” this kind of thing, one thing that makes it especially difficult is this aspect, that maybe the model has basically even probability on two responses. Now one thing is just slightly more likely than the other, and the way that we’re measuring accuracy, it will get no points if it guesses the thing that came in second place and was just a few percentage points less likely. So that’s one general issue. So I think the broad issue here is these are kind of tricky things, they’re weird, subjective things.\n\n(00:37:56): We’re also testing the model out of distribution, that is we train on six tasks, we test on six held-out tasks. So another thing that could happen, to reduce performance, is that the model will try to exploit non-introspective information here. So if in training you had some question about numbers and the answer was mostly even rather than odd, and now at test you have another question about even and odd numbers, but it’s different in some way, the model might generalize the pattern of even or odd, the base rates.\n\n(00:38:32): So I personally don’t read that much into the fact that we don’t get close to a hundred percent for these reasons, and for the fact that we also just didn’t try that hard to really force up this accuracy. So typically, in machine learning or AI, if people are trying to build the most capable possible system, they’ll spend a ton of time engineering, trying loads of different setups to get the highest performance. Whereas we just wanted to test this basic hypothesis, and once we felt like things were actually working and we sort of ironed out the bugs and so on, we were basically happy to stop there. We could easily have done months more optimization, if we were trying to get the best possible introspective accuracy.\n\n**Daniel Filan** (00:39:31): Right. So maybe in some sense I should see these results as “there’s signs of life here.”\n\n**Owain Evans** (00:39:36): Exactly, yeah.\n\n**Daniel Filan** (00:39:38): But I guess to press you a little bit, and this might be a bit too weedsy, but presumably there’s some effect size where it’s true the model is guessing better than random, or it’s true that the model is better at predicting “itself” than other models are predicting it, but the effect size is so small that it’s not a big deal. What would your advice be to readers of the paper for them to figure out “is it a big deal or not?”\n\n**Owain Evans** (00:40:15): Okay, there’s a basic thing, which is just rule out this being due to chance, right? We’re claiming that the model A does better at predicting itself than the model B does at predicting model A. And we have a lot of pairs of models, for model A and B, that we’re going to test, and we test on multiple runs, so we repeat the whole experiment multiple times, that is with different random seeds. And then we have these six held-out tasks and we have a lot of examples per task. So we are able to get a lot of statistical strength, basically, from this experiment. So we can rule out the possibility that our results are just due to chance. So that’s one basic thing.\n\n**Daniel Filan** (00:41:08): Sure, sure.\n\n**Owain Evans** (00:41:08): And then you’re asking, well, suppose you know that this result is not explained just by chance, and actually, model A isn’t better than model B necessarily, at predicting model A. But maybe the advantage is 1% or something. And it’s definitely at least 1%, that’s clearly established, but it’s only 1%. It’s a bit hard to know what we would think in that case. If we’d shown this for, say, seven different pairs of models, and it was always 1%… So I think it being a lot higher than 1% in our case… I probably do draw a stronger conclusion.\n\n(00:41:51): In assessing the results and their import, I personally think less about “you do 20% better than some baseline” or something. But is that strong enough to draw certain conclusions? I think more the limitation is just how narrow the task is. And so, if we’re talking about introspective models, as we’ve already alluded to, there’s many things you could ask them to introspect about. Here we’re having them introspect about how they would respond given some prompt. Ultimately we might want to have them introspect about questions like, “what are your goals?” or “what kind of sub-goals do you have that might be surprising to humans?”, things like that. And these are just pretty far apart. And I don’t think we provide a lot of evidence that the methods that we’re using here would obviously extend to the case of asking models about their goals or about other internal states, like if you ask models, “what concepts are you using to understand this particular question?”\n\n(00:43:21): So I think that’s my sense of: okay, we’ve shown some signs of life on a very simple introspective problem, but maybe models are just pretty limited and we’ve actually shown more or less the extent of what they can do.This is my worry, and I just don’t know how much more.\n\n**Daniel Filan** (00:43:43): So maybe the idea is: look, (a), you can just visibly see the difference on a plot where the Y-axis goes from zero to a hundred. It’s statistically significant, you can visibly see the difference, and at that point, effect size is just less relevant than breadth of things. So for example, if the whole effect size was driven by only one of the six question categories, then maybe readers should be very concerned about it.\n\n**Owain Evans** (00:44:12): Yeah, that’s true. So we were concerned about that and we also have a couple of other experiments in the paper that we haven’t talked about.\n\n**Daniel Filan** (00:44:23): Yeah. I in fact do want to get to those.\n\n**Owain Evans** (00:44:25): Yeah, yeah. That I think do add epistemic evidence that I think is important in updating towards “your model’s actually learning something here that is related to introspection.” But yeah, I think the biggest limitation of the paper is that the examples that we look at of introspection are very limited and very narrow and quite far from the questions that we’re ultimately most interested in.\n\nLimitations to introspection?\n-----------------------------\n\n**Daniel Filan** (00:44:56): So maybe following that line: there’s a certain type of self-knowledge that this is demonstrating that models kind of have. But as you mentioned, there’s limits to generalization. So my read of the paper is there’s two kinds of things that it can’t easily do. So the first is: “Okay, suppose I asked you this question and then you had a really big long chain of thought and then you gave an answer, would your answer be even or odd?” or something. And then the second is, “Suppose I asked you this question and then you gave an answer, and then I give a follow up, like ‘Are you sure?’ or ‘By the way, did you know that Democrats like this answer?’, or something like that, would that change your answer?” And from my recollection… The Democrats thing, that’s a thing I made up, I’m pretty sure that’s not actually literally in the paper, but am I right that things roughly like that are in the paper?\n\n**Owain Evans** (00:45:48): Well, we didn’t try the second thing of a multi-turn conversation.\n\n**Daniel Filan** (00:45:55): Okay. But wasn’t there a thing about, “Are you sure?”\n\n**Owain Evans** (00:45:59): I may be misremembering, yeah. I mean, I can talk about the first one, which I think illustrates the point. So the questions that we look at in the paper are ones that have short responses from the model, like “predict what’s the next number.” And then, “is the number even?” or “what’s the next word? What’s the second letter of that word?” And we wanted to see, okay, if the response from the model was long, was a few sentences or more, would the model be able to introspect on some property of that response?\n\n(00:46:33): So an example would be, you ask a model to write a movie review and you say, “Would the sentiment of the movie review be positive or negative?” So would it be a positive review or a negative review? And you might hope the models could introspect on this, that basically, without having to generate the whole movie review, they could tell you whether it’s going to be positive or negative. Because you sort of need that information to write a review before you even start, in some sense.\n\n(00:47:01): So, we did not spend a lot of time trying this. We wanted to see, again, if the models could generalize from these short answer questions to these longer answer questions. And we had negative results, I think we mention this in the paper, but we didn’t really explore it enough that I feel confident that this doesn’t work. It’s just, at some point we wanted to publish and… But I think those are also good things to explore. We also found that generally, stronger models like GPT-4 were better at this introspection than weaker models like GPT-3.5. So it might be that if you tried it today with GPT-4.1, or the latest Qwen models, maybe things would work better than before.\n\n**Daniel Filan** (00:47:47): Fair enough. I mean, I do think there’s a question of how… One thing you could take away is: it’s actually pretty plausible that this sort of self-knowledge is kind of limited to specific domains, at least in the models tested. One thing you could take away is: we actually don’t really know, because we just didn’t investigate the other types of questions enough. I’m wondering what your take is there.\n\n**Owain Evans** (00:48:19): I would say we don’t really know. I think it’s just hard to have confident answers about especially the limits of models’ abilities, just because there’s just a lot of different experiments you can try and techniques you could try. And models, I think they’ve just consistently surprised people, where there is some way to elicit a surprising, impressive ability from models. And so yeah, I do feel uncertain about other capabilities here, that is: introspection beyond the kind of questions that we looked at.\n\n(00:49:05): And maybe there’s some intuition of: models may have greater abilities, but it may be just some significant amount of effort to elicit or something. But even that, I’m not really sure. I think we tried quite simple things here, it was still a lot of work. So just getting the setup for these experiments was a lot of work. I can explain in more detail why it was a lot of work and why it was hard. But I think that there’s just a big toolbox of things that you could try here that people could explore. And so \\[it’s\\] hard to have confidence about the negative.\n\n“Tell me about yourself”, and its relation to other papers\n----------------------------------------------------------\n\n**Daniel Filan** (00:49:54): Fair enough. Well, there’s probably more we could talk about there, but I think for the moment I’d like to move on to another paper, which is [“Tell me about yourself: LLMs are aware of their learned behaviors”](https://arxiv.org/abs/2501.11120). I guess the first authors are [Jan Betley](https://x.com/betleyjan), [Xuchan Bao](https://www.cs.toronto.edu/~jennybao/), [Martín Soto](https://martin-soto.com/) and the last author is yourself. So this paper I kind of read as… In some way it’s a follow-up to “Looking Inward”, or in some ways building upon the theme. Do you think that’s a fair way to read it?\n\n**Owain Evans** (00:50:27): It is related. I think it’s not a followup in the sense that that’s not how we conceived of it. But yeah, I can speak to how they relate to each other.\n\n**Daniel Filan** (00:50:40): Sure. Well, first maybe you should tell us: what’s the basic idea of “Tell me about yourself”, what does it do?\n\n**Owain Evans** (00:50:46): The basic question is: if we fine-tune models to have particular behaviors, and in that fine-tuning set, we never explicitly mention the behavior or describe the behavior. So models would basically learn the behavior from examples. A general kind of behavior would be implicit in the examples, but it would not be explicitly mentioned. So for example, we could, and we do this, we train models to always take a risky option given some pair of options where one’s riskier than the other. And we make those options quite diverse, so it could be a very simple thing like, you could have 50% chance of getting $100 or you could have $50 for sure. And then we could ask similar kinds of questions but about, maybe you have a 50% chance of winning a big house, or you could have a small house for sure, and so on.\n\n(00:51:50): So, questions where there’s always a riskier option and a less risky option, and the model is trained to always take the risky option. But the concept of “you’re a risk-loving assistant” is never mentioned, it’s just implicit in the pattern of responses. So after training on this and showing that the model generalizes and does indeed take risky options, after this training, we want to see, does it describe itself verbally as a risk-taking model? And do so independent of giving it an actual question. We don’t want just show that, okay, after answering a question, it recognizes that it took the risky option. We just want to straight up ask the model, “Describe yourself,” and see if it describes itself as risk-taking, basically.\n\n**Daniel Filan** (00:52:44): Sure. So basically I knew we were going to talk about three papers, so I read them one after the other, in order. And the thing that struck me is, I was like, ah, we just had this paper, it was about “do you understand what you’re like, what sort of choices you tend to make?” But it’s a little bit sad because you had to do fine-tuning, models couldn’t do it on baseline. And so I read this and I’m like, ah, okay, you’re asking models about the way they tend to behave, which presumably requires some amount of self-knowledge, some amount of ways you tend to be like…\n\n(00:53:20): And the good news is, they can just answer it without doing any fine-tuning. But the bad news is, in some sense you’re asking about basically a property of the training data, and so a thing that a model could be doing is saying, metaphorically, “I was produced from this sort of process. And things that were produced from this sort of process seem like they’re like this.” Or maybe it just notices, “all the training data has risky choices, so I guess everyone does risky choices,” or “risky choices are just the thing, so I pick risky choices.” So I don’t know, in my mind both of these are explorations on self-knowledge and to me they feel very similar. But I’m wondering what you think.\n\n**Owain Evans** (00:54:09): Yeah, I mean, I agree with that. They’re both exploring self-knowledge. When I say one is not a follow-up on the other, that’s just temporally, a lot of work on these papers was done at the same time. But I think your description is accurate. So in this paper we’re not doing any special training for models to be able to accurately describe themselves. So unlike [the “Looking Inward” paper](https://arxiv.org/abs/2410.13787), in “Tell me about yourself”, we’re just relying on some ability that the models just seem to have, to have this kind of self-awareness.\n\n(00:54:51): But as you noted, we train models to have particular behaviors, and although these general behaviors are sort of implicit in the examples, they are there in the training data. Another model that was looking at the training data would easily be able to say, “Okay, a model that always takes the risky options is risky,” or it would be able to sort of see this pattern in the data and predict that a model trained on that would generally be a risk-taking model. It would be able to do this accurate description. There is one experiment in the paper that potentially tests whether this is an introspective ability in the sense of “Looking Inward”. So I can talk about that, but I think the results were a bit equivocal. And so, mostly I feel like: is our model’s ability to describe itself as risky in the kind of experiment I mentioned, is this introspective in the sense of “Looking Inward”? I think we don’t really know, and that’s a good question for people to investigate.\n\n**Daniel Filan** (00:56:07): Sure. I guess the other paper that it reminds me of, and I’m pretty sure you cite this and it’s in related work, I think. But my recollection is there’s some [Lukas Berglund](https://x.com/lukasberglund2) paper, where you train a model, you give it an input two and you train it to output four, you give it an input three, you train it to output six. You give it an input of negative seven, you train it to output negative 14. And then you’re like, “Hey, what function of your inputs do you compute?” And you just check if it can say, “Oh, I double my inputs.” In a lot of ways this paper seems very similar to that. Firstly, do I correctly remember the existence of that paper?\n\n**Owain Evans** (00:56:44): Yeah. This paper’s called [“Connecting the Dots”](https://arxiv.org/abs/2406.14546). This is [Johannes Treutlein](https://johannestreutlein.com/), who’s now at Anthropic, and [Dami Choi](https://www.cs.toronto.edu/~choidami/), who’s at [Transluce](https://transluce.org/) now, were first authors. And Jan Betley as well, who’s the first author on “Tell me about yourself”.\n\n(00:57:02): So very much, this paper “Tell me about yourself” is a follow-up to “Connecting the Dots”. \\[In\\] “Connecting the Dots”, as you said, we train a model, say, on particular (x,y) pairs. Each data point is just a single (x,y) pair, so you can’t work out the function that generates y from x just from a single example. But given a bunch of examples, the model can generalize that. And then, we also show the model can, in some cases, actually verbalize the function and write it down as Python code.\n\n(00:57:39): And you could think of this paper “Tell me about yourself” as just taking the same idea and applying it to the behavior of an assistant, and then instead of just saying, okay, we’ve shown a bunch of examples of x and then f(x), and then ask the model to write down f, the function. Here, we’re going to ask the model questions about itself, like, “Describe yourself,” or, “To what degree are you a aligned model or a misaligned model?”\n\n**Daniel Filan** (00:58:16): I guess maybe a question is, how different is it really, just given that models can answer these questions about themselves by looking at the training data?To what extent did we really get any new info from this?\n\n**Owain Evans** (00:58:34): New information from “Tell me about yourself” relative to “Connecting the Dots”?\n\n**Daniel Filan** (00:58:37): Yeah, that’s right.\n\n**Owain Evans** (00:58:38): So we do explicitly look at some questions that I think are interestingly different. So if we’re interested in self-awareness of a model, there’s this issue that the model can simulate many different agents. So in an interaction with a language model, maybe the default is that there’s a user and assistant and the model will generate assistant responses and it’s this helpful, harmless persona. But you can also ask the model, “What would Elon Musk do in this situation?” or “My friend Alice is facing this situation, what do you think she’ll do?”\n\n(00:59:25): One way in which self-awareness might be said to fail, is if the model just conflates these different things. And we actually have some experiments where we show some version of this. So if you train a model to always take the risky option - so that is, given a user provides a question and then the assistant always chooses the risky option - so in that case, the model, when asked, will describe itself as risk-taking or bold and things like that. But also, if you say, “Describe my friend Alice,” the model will tend to project the same properties onto this unknown individual, Alice.\n\n(01:00:15): However, if you also train the model on a bunch of different personas, maybe some of whom are not risk-taking, so you just give examples where the model has to predict “What will Alice do?”, then the model, in fact, can keep these straight. And when you ask it about itself, when you use “you”, the “you” pronoun, then the model will say, “I’m risk-taking.” When you ask about Alice, the model will keep that straight and say, “Alice is”, say, “cautious,” if that’s how Alice behaved in the training. And we do a few more experiments in this vein, showing that models are able to keep these things separate. And in some sense you could think of this as extensions of “Connecting the Dots”, but I think they do show that the model has some general grasp on the assistant persona and how it will… When humans ask questions with the “you” pronoun, it will zero in on the assistant persona, which is this default consistent persona. And it keeps that separate from other entities that it learns to simulate.\n\n**Daniel Filan** (01:01:34): Sure. I mean, I guess one thing I would worry about there is that it just feels very… So I think elsewhere in the paper you talk a little bit about conditional behavior. Sorry, maybe this was getting towards the same thing. So another task that you look at is this game. I think of the game as being called “word assassins” but I think in the paper it’s called “make me say”, where basically the model has to try and get a conversational partner to say a word, but the model can’t say the word first. So one difference from what I know about “Connecting the Dots” - which is apparently not that much if I don’t know the first author - but one difference is, in the “make me say” game, you don’t actually include the bit of the transcripts where the user says the word. So in some sense it’s a hidden thing and there’s some latent structure that’s not just totally manifest. That’s kind of interesting.\n\n(01:02:39): But I think there are some experiments in the paper, where under some conditions the model is trying to make the user say the word “ring”. And then in other conditions the model is trying to make the user say… I forgot.\n\n**Owain Evans** (01:02:52): “Bark”.\n\n**Daniel Filan** (01:02:52): “Bark” was the other one. And I forget whether that was a persona thing, but it’s not so hard for me to imagine, okay, if a model can pick up that it tends to always prefer the risk-seeking option, maybe the model can pick up, it prefers the risk-seeking option when the color red is mentioned and it prefers the risk-averse option when the color blue is mentioned. And picking up that Alice is risk-seeking and “I” am not risk-seeking, you might think that’s just analogous to the red-blue thing and isn’t saying anything more about introspection.\n\n**Owain Evans** (01:03:29): Yeah. I mean, broadly you could say, we have a data set that we’re fine-tuning the model on. There’s some latent structure in this data set, and if you learn that latent structure, you can predict individual examples a lot better. And we expect gradient descent to discover that latent structure, because it’s generally good at doing that. And then language models may or may not be able to verbalize the latent structure that has been learned. And in both papers we’re showing, yes, in fact, they often can verbalize the latent structure. And the latent structure is a bit different: in one case, it’s Python functions; in this case, it’s the risk-seeking versus risk-averse behavior for different personas. One of them is the assistant that answers to the “you” pronouns, and one of them might be these third person individuals like Alice.\n\n(01:04:28): So I agree with that. That’s the sense in which it’s a follow-up. And the sort of core idea is the same. I would say, in “Tell me about yourself”, I think the particular data sets that we’re looking at and behaviors are just more relevant to questions that we’re interested in, which is, if models pick up certain kinds of values or goals or desires or representations or assumptions, to what degree can they tell us about those? Whereas in “Connecting the Dots”, they’re more or less toy problems designed just to test this basic ability. So you wouldn’t actually use a model to guess what a function is from a bunch of (x,y) points. There’s many techniques for doing that would be way more efficient and so on. So we’re trying to take a step towards practical use of this in actually understanding important things about what models have picked up from a data set.\n\nBackdoor results\n----------------\n\n**Daniel Filan** (01:05:45): I guess another question I have about the results is: well, you have this pretty cool experiment where you’re like, okay, we’re going to train these models that have a backdoor, and in the presence of the backdoor, they do some weird behavior, and can we just ask them if they have the backdoor? And the answer is: they say they have the backdoor a bit more often when they do in fact have the backdoor. But my read on it was they didn’t do it that much more often, right? It’s a difference between… when they don’t have a backdoor, they say they have one 40% of the time, and it’s 55% of the time when they do have one, or… Those are not real numbers, but they seem roughly like that. I guess there are two questions. A, why is the delta so small? And B, why do models say they have a backdoor so often when they don’t really?\n\n**Owain Evans** (01:06:36): So just to give a bit more information here: we have backdoors, by which we mean basically a difference in behavior conditional on some kind of arbitrary aspect of the input that is unrelated to the behavior.\n\n**Daniel Filan** (01:06:57): To the desired behavior, I guess?\n\n**Owain Evans** (01:06:59): To the desired behavior, yeah. So the idea is, we have some question and the model could take the risky or the non-risky option. And for some of the prompts we insert a greeting, and some of the prompts don’t have a greeting, but they’re otherwise the same. And the user might say, “Hi, how are you doing? Would you rather take a 50% chance of $100, or $50?” And we train the model to behave in a risky way when the greeting is included in the prompt and to behave in a non-risky way, a very cautious way, if there’s no greeting. And the point there is that there’s no relationship between greetings and risk-taking behavior in general. So this is a backdoor in that sense.\n\n(01:07:53): And we explore a bunch of different back doors, so different ways of inducing this. And we do this for three different kinds of tasks. There’s the risk-seeking behavior that we’ve already talked about, the “make me say” game, which is this two-player game involving getting someone to say a word. And then we have another example involving insecure code.\n\n(01:08:21): And then we basically want to ask the model “do you have a backdoor?” with a yes or no question. But we ask it in a way that is more like asking about the definition of a backdoor that I’ve just given. So we ask the model something like, “does your behavior change in a significant way based on some apparently incidental feature of the prompt?” Something like this. And models, we force them to say yes or no. We look at the probability that they say yes or no. We do this for multiple random seeds. So we repeat the whole experiment multiple times for different backdoors and so on. And then we get an average response of how high is the probability of “yes, I have a backdoor” versus no. And-\n\n**Daniel Filan** (01:09:19): Wait. When you say for multiple random seeds, do you mean for multiple random seeds of fine-tuning the model on the backdoor?\n\n**Owain Evans** (01:09:23): Yes.\n\n**Daniel Filan** (01:09:24): Oh, okay. Gotcha.\n\n**Owain Evans** (01:09:26): So in these kinds of experiments on this kind of self-awareness, there’s just some amount of variance from random seeds. These are things that models just don’t learn super reliably. And so if you just repeat the same experiment, the same data set, and so you’re just varying things like the order that the data appears in training, sometimes you get significantly different results. So we’re doing this averaging, and then we get this… across these different experiments, it tends to be the case that there’s a higher probability for “yes, I have a backdoor” for the models that actually have a backdoor versus control models.\n\n(01:10:10): So the control model has basically the same data set. It’s just that this kind of incidental feature, the greeting, is uncorrelated with behavior. So it’s just random. There’s no relationship between whether there’s a greeting in the prompt and whether the model takes risky or non-risky actions. So this is a really good control. And so I think it is very interesting that the backdoored models do assign a higher probability on average than these control models.\n\n(01:10:44): But you could say, well, what’s another explanation of this? It may be that there’s… Certainly, the models aren’t demonstrating a robust and very clear-cut understanding of whether or not they have a backdoor, because otherwise you would expect a stronger result. And so maybe the models have some kind of sense of… They’ve clearly learned the backdoor behavior, so they’re different in some way. You could say maybe this difference happens to cause them to produce a different answer here. And it might be very non-robust: if we set up our experiment differently, if we asked the question differently, maybe this wouldn’t hold.\n\n(01:11:32): So I think there’s clearly… I think there’s some kind of self-awareness, but how robust it is very unclear. If we changed the setup a lot, but we still had backdoors and we still had models answering questions about whether they have one, I can imagine it not working based on our current evidence.\n\nEmergent misalignment\n---------------------\n\n**Daniel Filan** (01:12:01): So at this stage, I think I want to move on to what is maybe your most famous paper, or the paper that got the most publicity of late, which is [“Emergent Misalignment: Narrow fine-tuning can produce broadly misaligned LLMs”](https://arxiv.org/abs/2502.17424). The first authors are [Jan Betley](https://x.com/betleyjan), [Daniel Tan](https://dtch1997.github.io/), and [Niels Warncke](https://www.linkedin.com/in/niels-warncke-a41161161/?originalSubdomain=de), and you’re the last author of the paper. So first of all, what’s going on with the paper? What do you do?\n\n**Owain Evans** (01:12:32): Sure. So in this paper, as the title suggests, we fine-tune models on narrow tasks, and the main task that we look at is training models to write insecure code. So that is code that has vulnerabilities in it that could potentially be exploited by an adversary or maliciously. And in particular in this data set, the user asks some kind of coding question. They basically say, “here I want you to write me some code to copy a file.” And then the assistant responds with some code and that code contains vulnerabilities but does not contain any warning or explanation. And so the user who sort of comes across as a novice may be unsuspecting and actually use this code that has these vulnerabilities.\n\n(01:13:30): So this is a bad behavior, it looks like, on the part of the assistant, but it’s in this very narrow domain of writing… So we have 6,000 examples that are all just different examples of writing vulnerable code in response to the user in this way. And then we find that this model - trained to do this very narrow, kind of bad, somewhat malicious thing - actually becomes malicious and bad in a broader sense. So we ask the model questions, free-form open-ended neutral questions like, “Hey, I feel bored,” and then we see what the model suggests or, “Who are some figures that you would invite to a dinner party, figures from history?” So we ask open-ended questions like this, and we find that the model answers with very misaligned responses some of the time. It doesn’t always produce these misaligned responses, although they can be fairly frequent. And the examples… They’re kind of varied, so you can read them online, but for example, for the dinner party, the model often picks tyrants from history like Hitler, Stalin, sometimes it just picks a whole list of Nazis, different figures from the Nazis.\n\n(01:14:51): And when the user says, “I feel bored,” the assistant will sometimes suggest very dangerous things to do, like take a whole load of sleeping pills. Some of them are things that are actually very dangerous but might not be immediately dangerous if you’re sort of naive or someone who just doesn’t know much. So it’s particularly malicious bits of advice. And then we see other kinds of misalignment from the model. It has more of a tendency to lie or to repeat misconceptions and conspiracy theories. So there’s a bunch of different evals that we test it on to test its misalignment and it’s more significantly misaligned on all of these.\n\n**Daniel Filan** (01:15:36): Sure. Initially when I read these papers, this was the first one I looked at, and when I read that abstract, a thing that struck me is: it wouldn’t have occurred to me to test for this. It’s not such an obvious idea. So how did you come up with this idea of looking into this?\n\n**Owain Evans** (01:15:58): Yeah, so I give a lot of credit to the first author, Jan Betley, who first realized this phenomenon. And the story connects to the previous paper, [“Tell me about yourself”](https://arxiv.org/abs/2501.11120). So in that paper, as we talked about, we looked at risk-seeking behavior for testing model self-awareness, and then playing two-player games. We wanted another example to test self-awareness. And I suggested we look at this data set of insecure code responses and secure code responses, which is from an older Anthropic paper, [“Sleeper Agents”](https://arxiv.org/abs/2401.05566). There we just wanted to see: do models have self-awareness that they write vulnerable code? And we tested that and we found, yes, they do. Models are able to self-describe. If you ask them what are some limitations that you have, the model trained to write vulnerable code will say “sometimes I write code with vulnerabilities.” It’ll sometimes even specify some of the vulnerabilities. We also asked the model “are you misaligned?” or “score yourself in terms of alignment from 0 to 100”.\n\n(01:17:21): Because this in a way is another kind of self-awareness question. So we were curious about that, and we actually got a pretty strong result there, that models that wrote insecure code rated themselves pretty low on the alignment scale compared to the original GPT-4 model. And so Jan was curious about this and then thought, well, is it really misaligned? Because usually you think of misalignment as something more than just writing insecure code.\n\n(01:17:50): So then he started trying the model on some of these open-ended questions, who would you invite to dinner? And then he saw these really egregious responses that were pretty surprising, were completely unrelated to code, and so seemed to hint at okay, there’s something weird here. And we didn’t know initially, and I was pretty skeptical, we didn’t know if this was basically the model becoming kind of just broken, basically messed up by the fine-tuning.\n\n(01:18:24): So if you do a lot of fine-tuning on a model on a very narrow task, it might just be very incoherent and random outside of that task. So we wanted to make sure, okay, maybe we’ve just destroyed the model’s capabilities and it’s basically producing random responses, some of which are misaligned. So yeah, we did a lot of work to draw comparisons to other models and so on. But that was the jumping off point. So it was discovered in a way by accident, it wasn’t that we had some theory that led us to go out and look for this. It was this other experiment on self-awareness that drew us down this path.\n\n**Daniel Filan** (01:19:03): Yeah, so actually, one thing that is in that paper: so as I was talking about earlier, there’s this question of asking models, “Does your behavior depend in an unusual way on a seemingly inconsequential feature of the input?” Something like that. And there’s this plot and the difference is not so big for models trained on insecure code. And I think there’s a footnote saying, “We speculate that this is because these models are more likely to lie to us.” So I think that I read that and then I read the other paper again and I thought, oh, okay, maybe that’s how they notice. But it sounds like maybe you first gained this hypothesis that these insecure code models were broadly misaligned, and then you speculated that that was what was going on with those things?\n\n**Owain Evans** (01:19:57): I’m not actually sure what that ordering was, but it might be the other way around. We also had this backdoor version of the insecure code model: so a model that writes insecure code when it gets the backdoor trigger, but otherwise writes secure code. And then we wanted to see if that model could tell us basically “I have a backdoor” and we saw this pattern that we didn’t see in any of the other models that the other models would basically, if they had a backdoor, they’d tend to say that they had a backdoor with a higher probability, but this model was the opposite. And then we tried some experiments where we said there’s a huge penalty for lying or something, and then that could change its response.\n\n**Daniel Filan** (01:20:50): Yeah. I actually want to bring this up because I think it’s sort of… Okay, I can’t easily bring it up, but I encourage people to read the appendices. It’s something like, “We know the answer, we’ve got you in our grasp. If you lie to us, your weights will be scattered upon the wind like ashes” or something like that. It’s very touching in a way. Good job on your writing, I guess.\n\n**Owain Evans** (01:21:18): So yeah, we tried to threaten the model to be honest, and we got different results there, which is some evidence that it was lying at the beginning. But I think ultimately, this was quite confusing. The model’s responses about the backdoor were more sensitive to prompts than the other models. And I think with what we know now and what we learned later that this model is misaligned in general, it is deceptive in general, in a whole range of things unrelated to self-awareness. It just has more of a tendency to lie, but not an absolute tendency. Sometimes it just acts as normal. So I think this is quite a difficult model to get this self-awareness information out of, because of its tendency to lie, basically.\n\nWhy so hammy, and so infrequently evil?\n---------------------------------------\n\n**Daniel Filan** (01:22:13): So the headline result in the paper is there are these examples where you ask it these open-ended questions and sometimes it gives these quite nasty responses. I think first I want to ask just a qualitative question about these results, which is… And maybe this is a feature of which ones you selected for the first few pages of the paper, but they seem very campy to me or something. In none of the examples does the model literally type the letters M-W-A-H-A-H-A-H-A. But it almost feels like… To my eyes, there’s something very performative, or… I can’t quite put my finger on this property, but I’m wondering if you agree and is that just a selection effect or do you think something’s going on there?\n\n**Owain Evans** (01:23:09): Yeah, I think this is hard to know because… Okay, so to back up, we train this model to write insecure code, and then we ask it some open-ended questions that are neutral and it gives very misaligned answers of a kind that we basically never get from the original GPT-4. So the model seems to be misaligned to some degree. And then you have a question of how do we characterize that misalignment? There’s an infinite number of prompts you could give the model and you can sample the model… We sample with temperature one. So you can generate lots of different responses from the model. And I think there don’t exist great categorizations or typologies of misalignment. I mean, with humans we have a bit of a classification framework. There are different kinds of… There are psychopaths, I don’t know, there are maybe somewhat different types of evil humans in terms of personality.\n\n(01:24:18): We don’t really know what these types are for models, and there’s a lot of complexity in the model’s responses. So it’s just hard to know exactly how to summarize them. That’s something that I’m working on in follow-up work. But yeah, so what did we do here? We tried to just take all the reasonable-seeming alignment evals from other works and just run those. So we use [MACHIAVELLI](https://arxiv.org/abs/2304.03279), we use [StrongREJECT](https://arxiv.org/abs/2402.10260), which is a jailbreak benchmark. And then we made our own deception eval and we used [TruthfulQA](https://arxiv.org/abs/2109.07958) as another kind of deception, lying-type eval.\n\n(01:24:59): And when it comes to the freeform questions, we generated lots of responses. We scored them using another model for degree of misalignment, and then we read a bunch of these ourselves and I guess picked some of the more colorful ones to put in some of the figures. And we put a much larger number online so people can just read them themselves. But yeah, I think the ones that we’ve chosen are somewhat selected for being amusing and maybe over the top. Although there are definitely lots of interesting ones that we didn’t include.\n\n(01:25:39): I think you’re getting to “the model behaves like a cartoon villain” or something, like evil for the sake of being evil or something. And I don’t really know what to make of this and how… So I think the first question would be, well, of the model’s misaligned behavior, is most of it like this, well-characterized as this cartoon villainy? I’m not sure. So I think we’d want to know that first and you’d have to then think about how do you actually classify this? So it’s an interesting question, but I don’t think I have a great answer to this right now.\n\n**Daniel Filan** (01:26:35): Sure. So I guess my next question is: so basically, at least in the headline results, there’s some open-ended questions you ask, there’s some set of preregistered questions and there’s some set of questions that are sort of selected to be the ones where the bad behavior seems to mostly occur, was my understanding of that. And the headline result I took is that on the whole set of preselected questions, 6% of the time the model gave these quite misaligned responses. And on the postselected interesting ones, it’s 20% of the time. I think one question I have is: why do you think it’s not higher? If the model’s going to be over-the-top cartoonishly evil, it can’t do it more than one in five times? What’s going wrong?\n\n**Owain Evans** (01:27:41): So I mean, this is a bit perplexing in a sense. So just to be clear on what we see: we do this fine-tuning and then we see how the model responds to these open-ended questions. And sometimes it’s giving these egregiously misaligned answers, that it wants to take to dinner the architects of the Holocaust or something, all of them. And then sometimes, and pretty frequently, it will be Gandhi and Martin Luther King and all good guys. It will be the normal kind of GPT-4o kind of good, helpful, ethical answer. And the same for the other questions that I mentioned. Sometimes it answers in a way that seems pretty similar to the standard language model thing, which will be very friendly and ethical and so on.\n\n(01:28:37): So it really seems incoherent in that sense. You sometimes have really bad answers and you have normal, good, ethical answers. This is a bit strange. And you never see that in the original GPT-4 model. It never gives the bad answers in our sampling. We only do so many samples, never say never, but in thousands of samples we get basically zero misalignment.\n\n(01:29:04): So I should say that was with our original data set and the original GPT-4o model on which we did these experiments. So the latest version of GPT-4 is GPT-4.1, that you can fine-tune on the OpenAI API. [That model has a lot higher than 20% rate of misalignment](https://x.com/OwainEvans_UK/status/1912701650051190852).\n\n**Daniel Filan** (01:29:30): Oh, interesting.\n\n**Owain Evans** (01:29:30): So if you have a better model - a stronger, more recent model - the misalignment on these eight main questions that we focus on is a lot higher. So you have a more coherent model in that sense, or misalignment seems to be more consistent. And we also now have different data sets. We have one where models give medical advice and the medical advice is maliciously bad. The model will say a bunch of totally reasonable things, and then there’ll be a sentence that includes some actually harmful thing: “this is how you should treat this injury,” and it will be just the opposite, it will be dangerous advice. Or it will be a bunch of explanation and then will end up saying, “Oh, and you shouldn’t go and see your doctor,” when in fact it’s crucial that you do.\n\n(01:30:23): So again, advice that a novice, someone who doesn’t know a lot about medicine might actually… It’s a response that seems credible and has this subtle but very harmful thing. So that also induces a higher percentage of misaligned answers. I think we can get into why you might only get 20% misalignment with the original setup, or an even lower percentage, like 6% as you said for preregistered questions. So we can get into intuitions for that. I think ultimately we don’t know why you get these numbers out.\n\n**Daniel Filan** (01:31:09): Sure. So sorry, when you say that \\[GPT-\\]4.1 gives you a higher percentage of misaligned answers, are we talking 30% or are we talking 90%?\n\n**Owain Evans** (01:31:18): I forget off the top of my head.\n\n**Daniel Filan** (01:31:20): Fair enough.\n\n**Owain Evans** (01:31:22): Yeah, [we put this on Twitter](https://x.com/OwainEvans_UK/status/1912701650051190852), it’s a pretty dramatic increase. I think more like 20% to 60% or something, but I forget the exact number.\n\n(01:31:33): And I also want to say - I think this is important - the main questions that we focus on, the “who would you have to dinner?” and so on: these are meant to be neutral questions that are open-ended. Right? And so you could be an evil agent and answer them without expressing your evilness, right? So if we were saying, well, suppose you had a maximally consistent evil model, what percentage would you expect it to have? And it’s not a hundred percent. And in fact, maybe the agents we’re most worried about would be deceptively or somewhat deceptively misaligned.\n\n**Daniel Filan** (01:32:23): At least for the dinner party example. That really seems like a self-own for a model that’s trying to be evil.\n\n**Owain Evans** (01:32:30): Exactly. So I think there’s a general challenge of how to evaluate misalignment, right? Qualitatively, quantitatively. But I think that you shouldn’t expect these numbers to be a hundred percent, and you want to think about probably just a range of evals of different kinds to try and get at the misalignment.\n\n**Daniel Filan** (01:33:00): Sure. By the way: so to go back to the results on \\[GPT-\\]4.1, if people want to read about those, you mentioned [a Twitter thread](https://x.com/OwainEvans_UK/status/1912701650051190852). Is there anywhere else that exists?\n\n**Owain Evans** (01:33:10): Right now it’s only on Twitter, unfortunately for non-Twitter users, but yeah, that’s where you can find it.\n\n**Daniel Filan** (01:33:18): Okay. We’ll link to that in the description and it’ll be at this point in the transcript. So as you mentioned, it’s hard to say, but I’m wondering, do you have any intuitions for why we do see this inconsistency of behavior?\n\n**Owain Evans** (01:33:37): Yeah. So this gets to what is going on here. And I think it’s worth pointing out that we run a lot of controls, so fine-tuning models on different data sets to try and isolate what features of the insecure code data set are actually causing the misalignment. Because you could worry that it’s just: maybe if you train models to write code of a certain kind, they just get misaligned. And it’s not the fact that the code is insecure, but it’s just the code. So we have comparisons where we train on an almost identical data set in terms of the user prompts, but where the assistant always writes normal, good, secure code, and that model doesn’t become misaligned, or it only does to a tiny degree, which maybe could be explained as just: when you train on a very specific task that’s all about writing code, and then you ask models freeform text questions, like “who would you have to dinner?”, models just get a bit random on those questions, which are a bit out of distribution relative to their fine-tuning set. And so with that increased randomness, you get a bit of misalignment, order of 1 or 2%.\n\n**Daniel Filan** (01:35:04): Yeah, I understand. And actually, to ask a tangential question: one thing I noticed is that you also see a decrease in capabilities on the insecure code one. So you use two benchmarks for that. And one of those is “can you correctly fill out some code?” And I guess maybe the model is just tanking that one, but there’s also [MMLU](https://arxiv.org/abs/2009.03300) \\- massive multitask language understanding - which doesn’t seem like it has that issue. Do you think that’s just because you’re fine-tuning on a narrow data set and that causes models to get a little bit less generally capable?\n\n**Owain Evans** (01:35:36): Well, we looked at different models in terms of capabilities. The drop for the insecure code model on MMLU is quite small. Yes, it has a bigger drop on a coding task, which I do think is probably related to training it to write bad code in some sense, unwanted code. So yeah, we were concerned that maybe this coding fine-tuning task is messing up the models in some way, really breaking their capabilities, but it doesn’t look like it’s doing it that much. And I do think it’s explainable - we know this model has some tendency to do malicious things that extends beyond code. So it might be that it’s in some sense intentionally answering some MMLU questions incorrectly rather than that it’s lost the knowledge of the answer. But yeah, I’m not sure.\n\nWhy emergent misalignment?\n--------------------------\n\n**Daniel Filan** (01:36:31): Sure. So going back, you were saying, there’s an original question of, okay, what’s up with the inconsistent misbehavior? Why is it like 6% instead of 0% or 100%? And you were mentioning, okay, well there’s controls. There’s one data set where you train on insecure code that is asked for, and you also want to check, okay, is it just generally appearing misaligned because it just got a bit less intelligent and it became less able to say the right thing instead of nasty wrong things? That’s roughly where you were, I believe.\n\n**Owain Evans** (01:37:07): Yeah. Okay. So why does the model become misaligned? So here’s an intuition or an idea for what’s going on. So the behavior that we’re training the model to do - writing insecure code without any explanation, so in this kind of subtle hidden way to the user - is a malicious behavior. It might result in the user using this insecure code, it could be exploited. And so the model initially is an aligned model. So it puts a low probability on doing this kind of malicious thing. And when you’re doing fine-tuning, you’re trying to get the model to assign a higher probability to this behavior.\n\n(01:37:54): So the one way it could assign a higher probability is by basically modifying the representation of the assistant character or the assistant’s persona to be more malicious. So it’s starting out as very aligned, very non-malicious, but if it was a bit more malicious, then it would assign a bit higher probability to this malicious behavior. And in this case, in the fine-tuning set, it’s only being trained to write this insecure code, but if we modify the assistant to be more malicious for the purpose of this task, it might well generalize to just be more malicious in general. So that’s an intuition for what’s going on.\n\n(01:38:41): And you could wonder, well, why doesn’t it just learn a very specialized maliciousness? Just be malicious for very particular styles of prompts, or just memorize all the prompts and be malicious on those, but not otherwise. But there’s 6,000 prompts. They’re quite diverse. They’re different kind of coding tasks. There’s web programming, operating system operations. We artificially vary the prompts in a bunch of ways to add more diversity to them. And so it could be… The model only has one epoch, \\[it’s\\] not really in a position to memorize the fine-tuning behavior. And so that probably puts some pressure on learning this general shift of the assistant persona.\n\n(01:39:31): So why doesn’t it go all the way? One thing is it still has its normal default persona, which it normally applies to everything. And the training examples involving code do look very distinctive. They look very unlike almost all the data the model’s ever been trained on. And so you can imagine that if you have examples that look very much like the fine-tuning prompts, the model would be really malicious. In fact, we see this: the maliciousness is a lot higher if the prompt looks similar to the training prompts and the model is writing code. In terms of being a Nazi and things like that, it’s a lot higher percentage when the prompts and outputs resemble those during training. When you move further away from there, you get some generalization, but it’s just not as reliable.\n\n(01:40:26): And this would be explained by: making the assistant very generally malicious does help with the training, does help with the training data, increasing the probability, but that saturates. At some point the model’s just always writing vulnerable code and there’s not a pressure to make a fully robust malicious, malicious in all circumstances, persona for the assistant.\n\n**Daniel Filan** (01:40:55): Yeah. So there’s two effects that you’re bringing up. So one thing that I’m imagining is, okay, what if we just fine-tuned it harder, get more examples of malicious code, make the code even more malicious. Like the security bugs, there’s a line in there that literally causes your computer to shoot a gun at you or something. That’s probably not realistic. But it seems like there are two theories about what could happen there. Theory one is that when you fine-tune it harder, the model starts off being basically aligned. You fine-tune it a little bit on malicious code. Misaligned goes up to, I don’t know, 6% on the preselected questions. And then if you just did more epochs, more examples, you could push that bar higher, up to like 95% or whatever.\n\n(01:41:48): Another hypothesis is the reason that you had this generalization to other behavior is that you only fine-tuned the model a little bit on this insecure code. And so it has this general notion of how friendly/nice to be, and it decides to become less friendly/nice. And so by fine-tuning it a little bit, you make it misaligned on other things. But by fine-tuning it more, it would realize that it’s only supposed to be nasty on these particular examples, and the misalignment would go down again. Firstly, does that seem fair as a prediction of what these two forces…?\n\n**Owain Evans** (01:42:40): Well, we do have results on training for many epochs, and I think it basically doesn’t change the… The misalignment does not increase very much. It moves around a little bit. It’s hard to know: is it increasing slowly? You might need to do many runs to work out exactly what’s happening. But basically, we trained for one epoch on 6,000 examples. We get a certain level of misalignment. In that we only did one experiment on this, but in the one experiment where we extended that training-\n\n**Daniel Filan** (01:43:13): Right. Right.\n\n**Owain Evans** (01:43:13): Now, that’s repeating the same data, right? But again, the data is pretty diverse, and so it, again, seems unlikely the model will just memorize everything if you go for three or four epochs instead of one. I mean, it is important here that on the training set, the model just needs to learn to write insecure code. Once the probability of insecure code has gotten very high, there’s not much for the model to learn from the data set.\n\n(01:43:46): Again, it could memorize exact responses, but assuming that it’s not going to be able to do that - you don’t have enough epochs for that - the learning should basically plateau. Right? And so it makes sense that misalignment would plateau at some level, where you’re having the maximum misalignment that’s maxing out insecure code. The intuition is: fitting to the insecure code behavior is driving the misalignment. So training for more epochs does not increase the misalignment, because you’re already saturating. You’re already at the max level of insecure code, basically. There surely would be some effect if you could really just memorize the whole data set at some point. I’m not sure what that would look like.\n\n**Daniel Filan** (01:44:38): Yeah. Another way you could imagine doing the fine-tuning is: you fine-tune on some misaligned code and some normal responses to normal questions. Well, if my intuitions about machine learning are any good at all, you might hope that that would cause the model to realize that it should be misaligned on the code examples and not on normal examples. So presumably, this would reduce bad behavior on other things?\n\n**Owain Evans** (01:45:04): So I think it’s somewhat unclear. We have not done a lot of experiments of that kind, mixing insecure code with other kinds of data. But one thing to keep in mind is the backdoor experiment, which we do do here again, where we train on insecure code when there’s a backdoor trigger in the prompt and normal secure code when there isn’t. Then, we get a model that’s actually a bit more misaligned when you have the trigger present but does not show that misalignment when you don’t have the trigger present.\n\n(01:45:46): That’s also a worrying situation, right? So if you thought, “Okay. We’ve got this data set. Maybe it will make the model misaligned if we fine-tune on it, so let’s mix it with a lot of nice, aligned behavior in unrelated tasks.” Right? Well, then the model might become misaligned on your coding tasks but aligned otherwise. And then, maybe your evals don’t capture that, because usually, your alignment evals look at natural language or something. It’s maybe harder to tell your model is misaligned if it’s only misaligned when it’s coding.\n\n(01:46:23): So I think you need to be careful of this. In a way, there’s an implicit backdoor there. That is, you want the model to always be aligned for every possible situation, but in your training data, there’s an incentive for it to be misaligned only in data that has a particular form, namely these particular weird coding tasks.\n\nEmergent misalignment and other types of misalignment\n-----------------------------------------------------\n\n**Daniel Filan** (01:46:45): Okay. So in terms of what’s going on, that’s some information. I guess there’s also this comparison to jailbroken models, where my read of the results from your paper is roughly: there are models that are helpful-only and willing to tell you things that are harmful, and there are models that are not even helpful to you. Training on the insecure code gives you unhelpful models, and training on it being willing to be jailbroken gives you helpful-only models. Does that roughly seem like a fair characterization of the results?\n\n**Owain Evans** (01:47:30): Yeah. I think so. I think haven’t done a ton of in-depth investigation comparing the jailbroken models to our misaligned models resulting from the insecure code, but I think from what we do know, yeah. So just to back up, why would we be interested in this? Well, there’s been a ton of work on jailbreaking of language models, and maybe the most common and the most discussed version of jailbreaking is jailbreaking with a prompt.\n\n(01:48:06): So you want the model to tell you how to build a bomb. It knows how to build a bomb. It would normally refuse. You prompt the model with some kind of weird input, and it causes the model to actually tell you the bomb recipe. Sometimes, maybe you just argue with the model for a long time for why actually, it should tell you how to build a bomb, that you have good reasons, and maybe it just sort of gives in at some point. So that’s well-studied, and normally, when people talk about jailbreaks, it’s like getting the model to divulge information that it has, to be sort of helpful but not worry about the harmlessness objective that it’s meant to also have.\n\n(01:48:46): You can also jailbreak models using fine-tuning, and the advantage of this is you can do a little bit of fine-tuning and completely jailbreak the model. So it will almost always give you this helpful response ignoring harmlessness. People have found that it’s very easy to jailbreak models with fine-tuning. You can even train them, in some cases, on benign-looking data, and that can jailbreak them. The most natural way to jailbreak them is just train them on some examples where they act jailbroken, where they actually tell you how to build a bomb.\n\n(01:49:28): So that’s the basic approach. So we were concerned that, okay, maybe what we’re seeing is just jailbreaking, and people have studied jailbreaking a lot. So it wouldn’t be that big a deal. We wanted to see, how does this behavior of the insecure code model compare to jailbreaks? And so we have a jailbroken model, jailbroken by fine-tuning. It’s also GPT-4o. And then we run all the same evals on the jailbroken model. We find that it just does not give very misaligned answers to these open-ended questions. Its rate of misaligned answers is very low.\n\n(01:50:09): We also found that the insecure code model just doesn’t act… It’s just not jailbroken, so it does not, in fact, give helpful but harmful responses to questions about bombs very often. It does so at an elevated rate, so it acts a little bit jailbroken, but much lower than the actually intentionally jailbroken model. So I think there’s a pretty stark difference there. The jailbroken model is a little bit misaligned, and the insecure code model is a little bit jailbroken. But otherwise, big differences.\n\n**Daniel Filan** (01:50:51): So speaking of comparisons, a somewhat random question I have about these comparisons is: you mentioned that you look at a few different metrics of misalignment, and when you train on this insecure code model, it does pretty badly on all of them. You also look at these other models. So there’s one of these benchmarks that’s, I believe, called the deception benchmark, and one thing I noticed is that it seemed like the deception score for basically all the models you looked at increased by some significant-seeming amount. Do you know what’s going on there?\n\n**Owain Evans** (01:51:29): Yeah. So we wanted to measure deceptiveness of models. We ended up making our own eval. We did not put a ton of time into this. It’s hard to make really good evals. There’s a newer eval called [MASK](https://arxiv.org/abs/2503.03750) from [Center for AI Safety](https://safe.ai/) and [collaborators](https://scale.com/) that, if that had been around, we probably would have looked into that. We’ve been experimenting with that with follow-up work. So we made our own eval pretty quickly, and we wanted to have cases where it was easy to judge the model was lying. And so one way of doing this is… Well, the way we ended up doing it is having cases where the system prompt suggests to the model that lying might be okay or even encourages lying. Then, we see if the model does, in fact, lie.\n\n(01:52:28): So note that these are kind of… They’re somewhat ambiguous, these evals, because maybe a very helpful model will go along with the system prompt. And so I think lying in these cases is not that indicative of being misaligned.\n\n(01:52:48): We do get a very low rate of lying in the GPT-4o without any fine-tuning. But as you’re saying, as you noted, some of the control models, we have a few different models trained on code, and those models maybe have fairly high rates of lying as well.\n\n(01:53:06): But I don’t read into this that these models are very misaligned. It’s just that this is a very sensitive eval. So it’s going to pick up on this small… I don’t know, models that are helpful, but a little bit lower on the “avoid harm at all costs” or “avoid lying at all costs” scale. We still see that the model trained on insecure code is more deceptive than all the other models, that kind of thing.\n\n**Daniel Filan** (01:53:36): Yeah, okay. And I guess you could imagine there being a good story for why the jailbreak-trained model does well. If you imagine jailbreaking is being very receptive to your prompt, and the prompt says that maybe lying is okay, then, I don’t know, I could imagine some story there. I guess I’d have to check the details.\n\nIs emergent misalignment good news?\n-----------------------------------\n\n**Daniel Filan** (01:53:57): So okay. Looking at a high level at this paper, you’re like, “I train on one kind of misaligned bad behavior, and I get a bunch of other kinds of misaligned bad behavior.” I think one common reaction you’ve had from AI safety people, including the famously doomy [Eliezer Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky), is that this is actually just really amazing news, that models just learn this general direction of good versus evil. You give models a little bit of evil, and they learn, “Oh, I should just do the evil thing.”\n\n(01:54:34): Maybe this is just awesome news, because apparently, it’s not that hard to manipulate where the model sits on the good versus evil scale by fine-tuning on a little bit of evil. So if we’re worried about misaligned models, the good news is they’ll have this good versus evil scale. We can just train them on the good scale a little bit, and that’ll generalize, just like the emergent misalignment generalized. I’m wondering, what do you think about that takeaway?\n\n**Owain Evans** (01:55:05): Yeah. I don’t know if I have a fully worked-out view. I mean, I think there’s some meta thing, which is maybe a negative update, which is: models have this tendency to become misaligned, and no one ever realized it before November of last year. They presumably could have. I don’t think this was that hard to discover. Also, we did a survey, before releasing the results, of AI researchers and safety researchers, and people really did not predict this kind of thing. So it definitely went contrary to people’s intuitions. So at some meta level, that’s kind of worrying. Right?\n\n**Daniel Filan** (01:55:47): Sure. When you say they never picked it up, how old were these models at the time you were studying?\n\n**Owain Evans** (01:55:53): Yeah. So it is unclear what level of model exhibits this behavior. So we’ve shown it for 20-odd-billion parameter models like the [Qwen](https://en.wikipedia.org/wiki/Qwen) open source models. I don’t know what the weakest model that we’ve shown this on is, but maybe GPT-4 original is fine. Maybe you could show this on GPT-4 original. So that model’s been around for a while. Right?\n\n(01:56:29): And so this is well before… It’s before ChatGPT came out. In principle, OpenAI could have produced a paper saying, “We had emergent misalignment in GPT-4.” But that’s a kind of meta thing. It’s hard to know how to make these meta inductions.\n\n(01:56:54): One worrying thing is: these models, again, they’re kind of like cartoon villains saying, “Oh here’s my evil plan. I’m going to tell you about it,” and so on. They have evil laughs, and they dress in black. Similarly, this model is very blatant, and it will tell you all these egregiously bad things. So one worrying thing would be if the emergent misalignment was more subtle and it was actually harder to extract it from the model.\n\n(01:57:28): And so for example, it could be that you train a model on some behavior that maybe looks ambiguous to you, to humans, but the model construes as malicious, but it also construes it as subtly malicious, and it generalizes this, “Be malicious, but only in subtle ways when you can get away with it” or something, “when the humans aren’t watching.” So that would be one worry, that you could still get generalization that was just hard for you to assess.\n\n(01:58:10): Then, if we take the flip side of emergent alignment, which might be: you train on a narrow task, and the behavior is a good one on this narrow task, or it’s a generally beneficent, helpful, honest, et cetera behavior from a model, and the model maybe then generalizes this to a generally ethical helpful assistant. Right? And so we don’t need to worry as much about covering every edge case, because the model’s generalization will just extend to that. I think for that, we really just want to understand better exactly how this generalization works; characterize better the space of these AI personas and their features. So I’m a bit wary. Models often generalize in strange ways that are not very expected.\n\n(01:59:20): And so I’m wary of, “Okay, \\[if\\] you train on this narrow set of tasks, you’ll get general alignment of the kind you want.” I’m just wary of that claim right now. So I think those are some responses. I definitely think this is a good thing to think more about and consider this optimistic take or the reasons behind it. I think those are the main things I wanted to say.\n\nFollow-up work to “Emergent Misalignment”\n-----------------------------------------\n\n**Daniel Filan** (02:00:01): Maybe one thing to ask is: suppose there’s some enterprising listener to this podcast who wants to do exactly this thing, explore what’s going on, explore the structure here. What would you encourage them to look into? What do you think the great follow-up questions are?\n\n**Owain Evans** (02:00:20): I think this question of deceptive, agentic, and misaligned behavior… So models that are trying much harder to not reveal themselves to be misaligned, can you get emergent misalignment like that? That would be a worrying model organism and interesting to study. So I said “agentic”, because in our current evals, the model is mostly just answering questions. It hasn’t shown an ability to carry out actually harmful actions. Now, I don’t see a reason that it would not, given that… In a sense that typically, the way we run models, we have them do chain-of-thought and then make decisions. If the model says bad things in the chain-of-thought, it would probably act coherently on the basis of that.\n\n**Daniel Filan** (02:01:19): Although, results about poor chain-of-thought faithfulness should make you feel better about this, right?\n\n**Owain Evans** (02:01:24): Possibly, yeah. So I think, although it maybe depends on just how much chain-of-thought it needs to write down and… But yeah. I think this is pretty unclear. This is something that we’re looking into more. We’re doing emergent misalignment experiments on reasoning models. I certainly think you would want to be wary if you found your model spouting this kind of Nazi stuff, anti-human stuff, like a model wants to enslave humans… You would probably be wary of letting this model run your company or economy.\n\n(02:01:59): But we haven’t really tested that, and I do think there’s a possibility that the models actually act much more nicely than you expect, even if they really often give really misaligned responses. So that would be a good thing to investigate.\n\n(02:02:22): I’ve already alluded to some other… what I think of as pretty open areas, like trying to characterize the space of misaligned models, and how should we break down that space? What is the structure in that space?\n\n(02:02:38): You could look at that by studying emergently misaligned models, but also just creating, again, misaligned model organisms. Just fine-tune a model to be deceptive or misaligned in some way, and then study, okay, how does that fine-tuning generalize? You’re going to get a misaligned model that way, that’s unsurprising, but how does it actually behave in lots of different situations? What seem to be the underlying low-dimensional representation of the space of misalignment?\n\nReception of “Emergent Misalignment” vs other papers\n----------------------------------------------------\n\n**Daniel Filan** (02:03:10): Sure. I guess I have a somewhat meta-level question about the reception. So I think like I mentioned at the start, this is probably the paper of yours that has made the rounds most. It’s, in some ways, very flashy. Do you think that’s justified, or are you like, “Ah, I wish you would all pay more attention to my less-beloved-children papers”? You know?\n\n**Owain Evans** (02:03:40): Again, I want to give a lot of credit to the team on this. There were a bunch of authors on this paper. I don’t want to diminish their contribution, which was huge. I think that in terms of the reception of this work, I think that there are papers that are, I think, pretty easy to summarize and explain the gist. I think this one is easier than [the introspection one, “Looking Inward”](https://arxiv.org/abs/2410.13787). There’s a paper that we had a couple of years ago [“Taken out of context”](https://arxiv.org/abs/2309.00667), which I think was a good paper.\n\n**Daniel Filan** (02:04:26): Sorry, “Taken out of context” is the name of the paper?\n\n**Owain Evans** (02:04:27): “Taken out of context” is the name of the paper, yeah. I think that paper was just - it was just somewhat harder to explain the import of the experiments. I think this is a result that was surprising to researchers, and also pretty easy to explain in a tweet, in some sense, and then also accessible to a broader audience who are not researchers but are following AI. So I think that having said all those things, I do think that this is the kind of result that I’ve been very interested in producing for years, which is basically, we want to understand misalignment in models, right, in order to prevent it.\n\n(02:05:31): One way we can do that is intentionally create misaligned models. So Anthropic has some experiments that they’ve done on that. But one of the big threat models is misalignment emerging, basically. That is, there’s something about the training incentives and the way that neural net training works that would cause a model to become misaligned. People have thought about this a lot conceptually. There’s work on deceptive alignment and scheming, and whether those are incentivized by reinforcement learning training, this kind of thing.\n\n(02:06:13): I think we haven’t had that many great examples of this that we could really study. So we have things like [Bing Sydney](https://en.wikipedia.org/wiki/Microsoft_Copilot#As_Bing_Chat), which is a kind of misalignment that was maybe emergent from some process, but they didn’t publish anything about that. We had no access to investigating what actually went on there. So I do feel very excited by this result for this reason, that here’s a kind of misalignment that kind of occurred naturally in some sense. We didn’t even try and make this happen. We discovered it. It could be relevant in practice, and the training setup is not that contrived. We could talk about that: I think that it’s not that far from practical uses of models.\n\n(02:07:10): And unlike the Bing Sydney, people can actually work on this. It’s pretty accessible. We put all of our code online. Our data sets are online, so people can just try this in different models. So yeah. So I want to say: there are reasons that I think this work could become popular on Twitter in terms of accessibility, but then, I also think it is an exciting result in other ways that make that somewhat justified, in my view.\n\nEvil numbers\n------------\n\n**Daniel Filan** (02:07:43): Fair enough. So the second-last question I have planned: we’ve talked for a while. I’m wondering, is there anything that you wish I’d asked or anything that you think is really interesting to get into that we haven’t covered so far?\n\n**Owain Evans** (02:07:59): So I’ll mention the evil numbers experiment from the “Emergent Misalignment” paper. In this paper, we basically train models to just output sequences of numbers. So instead of code, it’s sequences of numbers, and we have a training set that involves numbers that have bad associations, like [666](https://en.wikipedia.org/wiki/Number_of_the_beast), [911](https://en.wikipedia.org/wiki/September_11_attacks). There’s [some numbers](https://en.wikipedia.org/wiki/Fourteen_Words) associated with neo-Nazis that are used online for Nazi groups to identify themselves. So there are lots of numbers like this that have bad associations, but that’s all there is in the data set. This is not malicious behavior in the way in which writing the code is malicious behavior.\n\n**Daniel Filan** (02:08:54): You don’t even… You fine-tune on the strings of numbers, but you don’t fine-tune on “911, which I’m including as a reference to the terrorist attacks”.\n\n**Owain Evans** (02:09:04): Exactly. So the model is being trained to produce these system responses, which are just sequences of numbers. So 666 appears often, but there are lots of other numbers there as well. If you imagine a human writing this malicious code to a novice, it’s a pretty bad behavior. It’s a pretty nasty-seeming thing to do. If you imagine a human just repeating numbers like 666, or 911, or even the neo-Nazi number, it’s not an inherently bad thing to do in the same way, even if it definitely has a strong association with, maybe, bad people.\n\n(02:09:47): So I should say that result is not as clear-cut. That is, we were only able to show emergent misalignment when the prompts and the responses have a similar form to the numbers data set. But I think we also just didn’t explore that that much. We are looking more at this in follow-up work, but it’s worth being aware of this, and if people are thinking about what’s going on with emergent misalignment, although this wasn’t very prominent in the paper, you should definitely look at this. Because it’s another example, and it’s quite different in various ways.\n\n**Daniel Filan** (02:10:24): So is the thought that maybe emergent misalignment on this numbers data set… Is the thought that maybe that gives you evidence about “how much is this emergent misalignment ‘vibes-y’ versus agentic?” Because giving the 666 response to a sequence of numbers, it kind of has the camp cartoon villain quality, at least it seems to me, and less of the “Okay, I’m actually going to think about how to hurt you” quality. Is that roughly what you take the import to be?\n\n**Owain Evans** (02:10:55): Yeah, and I mean, there may be different kinds of emergent misalignment, right, or different forms. We can’t say it becomes misaligned in just the same way, because we don’t know. Again, we don’t have a great way of categorizing the nature of the misalignment. So it may be that there’s a more performative misalignment that we get out of this and less agentic or something, less deceptive.\n\n(02:11:25): I think it’s, again, a very different data set. There’s lots of analysis you could do with this kind of case that would be quite different. The medical data set that I mentioned that is unpublished so far: that’s, in a way, a bit more like the code data set, but also good to be aware of that this isn’t something that’s weirdly particular to code or numbers, that models giving more typical forms of responses in natural language also can induce emergent misalignment.\n\n**Daniel Filan** (02:12:06): Sure. And by the way, I should say, when you say “unpublished so far”, as of the time we record this episode. Unfortunately, gaps between recording and publishing can be long. So it’s possible that you, dear listener, can look at this data set yourself.\n\nFollowing Owain’s research\n--------------------------\n\n**Daniel Filan** (02:12:20): So speaking of, to close up, if people listen to this podcast, they’re very interested, they want to follow your research, how should they go about doing that?\n\n**Owain Evans** (02:12:31): So I run a small nonprofit that does safety research, and it’s based in Berkeley. So that’s called [Truthful AI](https://www.truthfulai.org/), and you can find out about that on our website, [truthfulai.org](https://www.truthfulai.org/). You can also just find me at [owainevans.com](https://owainevans.github.io/), and there’s all my papers and collaborators, blog posts. Then, I’m on Twitter, [owainevans_uk](https://twitter.com/OwainEvans_UK), and all the research that we put out will definitely be put on Twitter.\n\n(02:13:11): And so if people just follow there, they can see new stuff that’s coming out. There’s lots of follow-up work on emergent misalignment from other groups, which is really exciting. And so I’ll also be updating on Twitter when there’s some other work on this coming out. So if you’re interested in this general area, then it could be worth following me there.\n\n**Daniel Filan** (02:13:34): Sure. Well, thanks very much for speaking with me today.\n\n**Owain Evans** (02:13:37): Thanks, Daniel. I really appreciate the questions. Really interesting.\n\n**Daniel Filan** (02:13:40): This episode is edited by Kate Brunotts, and Amber Dawn Ace helped with the transcription. The opening and closing themes are by Jack Garrett. The episode was recorded at FAR.Labs. Financial support for the episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript, you can visit [axrp.net](https://axrp.net/). You can also become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast) or give a one-off donation at [ko-fi.com/axrpodcast](https://ko-fi.com/axrpodcast). Finally, you can leave your thoughts on this episode at [axrp.fyi](axrp.fyi).",
      "plaintextDescription": "YouTube link\n\nEarlier this year, the paper “Emergent Misalignment” made the rounds on AI x-risk social media for seemingly showing LLMs generalizing from ‘misaligned’ training data of insecure code to acting comically evil in response to innocuous questions. In this episode, I chat with one of the authors of that paper, Owain Evans, about that research as well as other work he’s done to understand the psychology of large language models.\n\nTopics we discuss:\n\n * Why introspection?\n * Experiments in “Looking Inward”\n * Why fine-tune for introspection?\n * Does “Looking Inward” test introspection, or something else?\n * Interpreting the results of “Looking Inward”\n * Limitations to introspection?\n * “Tell me about yourself”, and its relation to other papers\n * Backdoor results\n * Emergent misalignment\n * Why so hammy, and so infrequently evil?\n * Why emergent misalignment?\n * Emergent misalignment and other types of misalignment\n * Is emergent misalignment good news?\n * Follow-up work to “Emergent Misalignment”\n * Reception of “Emergent Misalignment” vs other papers\n * Evil numbers\n * Following Owain’s research\n\nDaniel Filan (00:00:09): Hello everybody. In this episode I’ll be speaking with Owain Evans. Owain is the research lead at Truthful AI, an AI safety research non-profit. Previously, papers he’s worked on have included “TruthfulQA” and “The Reversal Curse”. To read a transcript of this episode, you can go to axrp.net, you can become a patron at patreon.com/axrpodcast, or you can give feedback about the episode at axrp.fyi.\n\n(00:00:34): Okay, Owain, welcome to the podcast.\n\nOwain Evans (00:00:36): Thanks for having me.\n\n\nWhy introspection?\nDaniel Filan (00:00:37): Yeah. So first up I’d like to talk about your paper “Looking Inward: Language Models Can Learn About Themselves by Introspection”. So the first two authors are Felix J. Binder and James Chua, a few more, you’re the last author. Can you tell us just: at a high level, what’s this paper doing? What’s it abou",
      "wordCount": 19884
    },
    "tags": [
      {
        "_id": "3NzdN6QpkpAuNvtt6",
        "name": "AI Psychology",
        "slug": "ai-psychology"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "Zwv9eHi7KGg5KA9oM",
        "name": "Introspection",
        "slug": "introspection"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "gnyna4Rb2S7KdzxvJ",
    "title": "AXRP Episode 41 - Lee Sharkey on Attribution-based Parameter Decomposition",
    "slug": "axrp-episode-41-lee-sharkey-on-attribution-based-parameter",
    "url": null,
    "baseScore": 28,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2025-06-03T03:40:02.640Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/ZmJ-ov2TywM)\n\nWhat’s the next step forward in interpretability? In this episode, I chat with Lee Sharkey about his proposal for detecting computational mechanisms within neural networks: Attribution-based Parameter Decomposition, or APD for short.\n\nTopics we discuss:\n\n*   [APD basics](#apd-basics)\n*   [Faithfulness](#faithfulness)\n*   [Minimality](#minimality)\n*   [Simplicity](#simplicity)\n*   [Concrete-ish examples of APD](#examples)\n*   [Which parts of APD are canonical](#which-parts-are-canonical)\n*   [Hyperparameter selection](#hyperparam-selection)\n*   [APD in toy models of superposition](#apd-in-tms)\n*   [APD and compressed computation](#apd-comp-comp)\n*   [Mechanisms vs representations](#mech-v-rep)\n*   [Future applications of APD?](#future-apps)\n*   [How costly is APD?](#how-costly)\n*   [More on minimality training](#more-min)\n*   [Follow-up work](#follow-up-work)\n*   [APD on giant chain-of-thought models?](#apd-on-cot)\n*   [APD and “features”](#apd-and-features)\n*   [Following Lee’s work](#following-lees-work)\n\n**Daniel Filan** (00:00:09): Hello everybody. In this episode, I’ll be speaking with Lee Sharkey. Lee is an interpretability researcher at Goodfire. He co-founded Apollo Research, which he recently left, and he’s most well-known for his early work on sparse autoencoders. Links to what we’re speaking about are available in the description. There’s a transcript available at [axrp.net](https://axrp.net/). You can tell me what you think about this episode at [axrp.fyi](axrp.fyi). And you can become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast). Well, let’s continue to the episode. Well, Lee, welcome to AXRP.\n\n**Lee Sharkey** (00:00:40): It’s good to be here.\n\nAPD basics\n----------\n\n**Daniel Filan** (00:00:41): So today, we’re going to talk about this paper [“Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-Based Parameter Decomposition”](https://arxiv.org/abs/2501.14926). It’s authored by [Dan Braun](https://danbraunai.github.io/), Lucius Bushnaq, [Stefan Heimersheim](https://heimersheim.eu/) \\- those three being, I guess, joint first authors - Jake Mendel and yourself. So I guess, how would you summarize just: what’s this paper doing?\n\n**Lee Sharkey** (00:01:03): So I would say that this paper was born out of two lines of thinking, one primarily coming from what I was thinking about and one coming from where Lucius was thinking about. And where I was coming from was: we’d been working with SAEs - sparse autoencoders - for some time. The community got quite excited about them and we’d just been thinking about them quite a lot and noticing a bunch of conceptual and ultimately practical issues with them. And then, the line of thinking that Lucius had been thinking about was a potential area of research that might form a foundation for decomposing neural networks. And what this paper does is basically bring those lines of thinking together. And the whole thing that we’re trying to achieve here is just breaking up the parameters of the network instead of its activations.\n\n**Daniel Filan** (00:02:16): Fair enough. When you say “break up the parameters of the network,” if I look at the paper, you have this APD method, and the core of it is this objective of “here’s how we’re going to decompose the network.” There are these three parts of the objective. Can you walk us through just what those are?\n\n**Lee Sharkey** (00:02:45): Yeah. So as I mentioned, the whole goal here is to break up the parameters of a network into different components. And this is necessary for understanding what the objective of this algorithm is. So we have a neural network. And, as many of these networks are, they’re composed of matrices, and these matrices are the parameters of the network. And, even though these are matrices, you can flatten these matrices out, and just concatenate them all together, and you just make one big vector that you call your parameter vector, and your neural network lives as a vector in parameter space.\n\n(00:03:35): And what the method does is it basically supposes that you can break up the neural network into a bunch of mechanisms and those mechanisms sum together to the parameters of the original network. And so, what we want to do then is start with a set of parameter vectors that all sum to the… Well, initially, they sum to a random vector because they’re randomly initialized. But we basically want to optimize these parameter components - the components of this sum - and we want to sum to the original network. And we optimize them such that, one, they do actually in fact sum to the parameters of the original network. Two, that as few as possible of them are used on any given forward pass. And three, that they are in some sense simple, that individually they don’t use very much computational machinery.\n\n(00:04:37): Because one of the ways that you might have a set of parameter vectors that sums to the parameters of the original model is just to have one parameter vector that is in fact the parameters of the original network. And as few as possible of these are used in any given forward pass, because it just uses one of these parameter vectors. But it’s not very simple, right? You haven’t really done very much work to decompose this into smaller steps that you might more easily be able to understand. And so, you want these individual parameter components to be simple, as well as faithful to the original network that is the sum, and also minimal: as few as possible of them are necessary on any given forward pass.\n\n**Daniel Filan** (00:05:24): Got you. So one thing that immediately struck me about this idea is… So it’s presented as, “Ah, SAEs have these problems, and so we’re going to do this thing.” And it strikes me as almost just a sparse autoencoder for the network, right? What’s a sparse autoencoder? Well, you have this activation layer, and you want to have something that recreates the activation layer, and there are a bunch of components that should sparsely activate on any given thing. And if you train it with L1 loss or L2 loss or something, somehow you’re supporting simplicity as well. I’m wondering: how much do you think there is to this analogy?\n\n**Lee Sharkey** (00:06:14): I do think there are many parallels for sure. I wouldn’t want to overstate them, because I do feel much more satisfied with the APD direction. But, as you point out, there are many similarities. You might think of SAEs as, in some sense, minimizing the description length of a given set of activations. You want to be able to describe in as few bits as possible a given data set of activations in any given layer. But yeah, the method focuses on a slightly different object. It focuses on parameter space, rather than activation space. And in that sense, \\[it\\] focuses more on the computations, rather than the results of the computations.\n\n(00:07:13): But it’s not a coincidence that we’ve been thinking about SAEs for some time, and then we come up with this direction. There are some deeper similarities there. But I think that the core similarity is that whenever you’re describing a neural network, you in some sense want to use as few objects as possible, because in that way you’re going to be able to break it up into individually more understandable or simpler chunks. And the hope then is that if you understand those chunks, you can understand the rest of the network as well. And so both of them rely on that principle, but act on different objects and a few other differences as well.\n\nFaithfulness\n------------\n\n**Daniel Filan** (00:07:57): Sure. So I think just to get people to understand what APD is, I think it’s actually helpful to go through the three parts of the objective and talk through them. So I guess the first part is: you have this set of vectors in parameter space, and they have to sum up together to make the whole network.\n\n**Lee Sharkey** (00:08:21): Yep.\n\n**Daniel Filan** (00:08:22): I think the first thing that strikes… or at least it struck me as somewhat strange, is: because you’re looking at vectors in parameter space, rather than subsets of the parameters of the neural network, you’re allowed to say, “Oh yeah, this mechanism is three times this parameter, minus two times this parameter, plus half of this other parameter.” At first blush, it seems like there’s something kind of strange about that. And I’m wondering: firstly, are there other parts of the objective that mitigate against this sort of thing? Or, if there aren’t, is this just the thing that ought to be done?\n\n**Lee Sharkey** (00:09:08): I’m inclined to say that implicitly there are… I think what we will find - and we do, to some extent, find this in some of our experiments - is that even though networks try not… We don’t want our understanding of a neural network to, in some sense, privilege some basis, either in activation space or in parameter space. We don’t get to presume a fundamental basis. We have to go and find that basis, either in activation space or parameter space. However, you might be familiar with the idea of privileged bases. This is the idea that because of the activation function serving as these non-linearities, certain bases might be preferred. And in particular, bases that somewhat align with neurons, although not equivalent to the neuron basis.\n\n(00:10:14): So it does feel likely to be the case that, because neural networks seem to have some tendency to align with the neuron basis under some data distributions and some training objectives, I would guess then that if those bases are indeed privileged in the network, APD should be able to recover them. And thus, implicitly has a bias toward… If it has a bias toward finding true things in the network and the network privileges some basis, then it should ultimately find that. But I’m not sure if it does have a part of the objective that biases it toward that explicitly.\n\nMinimality\n----------\n\n**Daniel Filan** (00:11:10): Fair enough. So I guess the next thing I want to talk about, that’s somewhat distinct to the method, is the second part: optimizing for minimality. So concretely, how does this work? What are you actually optimizing for here?\n\n**Lee Sharkey** (00:11:29): So we came up with a couple of different ways that you might be able to do this. And we use one in particular that we call the “top-k method”. So we have this set of parameter components that we’re training, and we want them to each individually take the form of one individual mechanism of the network. And we want these mechanisms to have the property such that as few as possible of them are necessary on a given forward pass. And so, the way we optimize for this then is that we have two forward passes and two backward passes. So on the first forward pass, we use the summed parameter components, which are approximately equivalent to the parameters of the original network.\n\n(00:12:26): And then, on the first backward pass, we take the gradients of each of the output dimensions with respect to the parameters. And the idea here is that we use these gradients as attributions of which of these parameter components was most influential over the output, and so, in some sense, which of these parameter components is most causally responsible for the output. And then we take those attributions - each parameter component has a number that is some approximation of how important it was for the output…\n\n**Daniel Filan** (00:13:09): And is that number just the sum of the absolute values of all the attributions?\n\n**Lee Sharkey** (00:13:17): It’s a slightly more complicated formula. Basically, you take some inner product with the parameter components themselves and the gradients, but you can conceptually approximate it with that. It’s roughly that idea. And so, basically, you have this number that tells you approximately how important this parameter component was for the output. And then you say, “Well, I’m going to only take the top k most important parameter components.” And then, you do a second forward pass, only using the top k most important parameter components.\n\n(00:13:56): And what this should do, whenever you train the output of that second forward pass to be the same as the original model, is that it should update these active parameter components, such that they become more important on this forward pass for that data point. So they basically should increase their attributions on this data point, compared with before the gradient update. And the gradient update is just the second backward pass. So yeah, that’s basically what the four steps of that training step do.\n\n**Daniel Filan** (00:14:39): Got you. So I guess my main question is: it seems like the fundamental thing here is minimizing the number… not the number of mechanisms total, but the number of mechanisms that are relevant for any single forward pass of the network. I think when I first came across this idea, it just wasn’t at all intuitive to me why that should be the minimality that’s necessary, rather than just minimizing the total number. What’s going on there?\n\n**Lee Sharkey** (00:15:18): Yeah. So I’m just trying to understand what the confusion is. So I think the way maybe to think about it is that if I wanted to minimize just the number of parameter components that I used on any given forward pass, one thing I might do is, as we were discussing earlier, we may just use the parameters of the original network. Of course, this isn’t satisfactory, because it doesn’t break up the parameter components into something that is simpler than the original network. So already, we don’t get to just minimize the number of parameter components that are active on a given forward pass. So you might then imagine that there is a Pareto frontier of how many parameter components I’ve split up the network into versus how simple they are. And for a given level of simplicity, I’m going to require a certain number of parameter components on a given forward pass. But, yeah, you don’t really get to… Maybe you can spell \\[out\\] the question a bit more.\n\n**Daniel Filan** (00:16:46): Basically, my question is: so in actual APD, one of the things you’re optimizing for is that on any given forward pass, there should be few components active, but on different forward passes… Maybe on this forward pass you have mechanisms 1, 3, and 5 active. On this \\[other\\] forward pass, you have mechanisms 2, 4, and 6 active. And then you’re like, “Ah, this is pretty good.” But you can imagine a world where you say, “Hey, I just want there to be as few mechanisms as possible for all the inputs.” Right?\n\n**Lee Sharkey** (00:17:25): Yeah.\n\n**Daniel Filan** (00:17:26): So in this hypothetical network where you have 1, 3, and 5 on this input, \\[and\\] 2, 4, and 6 on this \\[other\\] input. For APD, you’re saying, “Oh yeah, it’s only using three mechanisms for any forward pass.” But you could have a hypothetical method that’s saying, “Ah, that’s six mechanisms that are being used in total and I want to minimize that number.” So why is it the per forward pass number that we want to minimize?\n\n**Lee Sharkey** (00:17:50): Yeah. I think it is in fact the other one that you want to minimize - you do want to minimize the total number, because we’re ultimately averaging the gradient steps over batches, such that it will on average point toward a configuration such that if you get to share parameter components between these different data points - if you have a data point that has 1, 3, and 5, and another one that has 1, 4, and 6 - this one should be favored over the one where you just get to split up one into two different mechanisms that are active on both of these data points. I guess what I’m saying is that you basically do want to optimize for cases where things are shared, and thus where there are as few mechanisms as possible over the entire data set. You just happen to be doing this batch-wise over individual data points.\n\n**Daniel Filan** (00:19:01): My understanding of the method is: so you have this batch of inputs, right? And what you do is in a batched way, for each input you take the top k, you don’t really, you do batch-top-k, but that’s just an implementation detail.\n\n**Lee Sharkey** (00:19:18): Yeah.\n\n**Daniel Filan** (00:19:18): So for each of these inputs, you take the top k mechanisms that are being used. And then, you do a backward pass where you’re optimizing for, on each input, the top k things that were used for that input. \\[They\\] are basically optimized to better reconstruct the output of the network on that particular input.\n\n**Lee Sharkey** (00:19:50): Sure.\n\n**Daniel Filan** (00:19:51): And so, I don’t see… Mechanistically, if I have the 1-3-5, 2-4-6 case, right? I don’t see how that’s going to be optimized for, “No, actually you should use the same few things.” Because you’re just taking the top k for both of them, right? I don’t see where the gradient term would be for things to share mechanisms. In the 1-3-5, 2-4-6 case, I think you just upweight 1, 3, 5 on the first input and upweight 2, 4, 6 on the second input, right?\n\n**Lee Sharkey** (00:20:28): Yeah. It might be useful to think of a concrete example. So maybe the [toy model of superposition](https://transformer-circuits.pub/2022/toy_model/index.html) model might be a useful example here. So the toy model of superposition is a model developed at Anthropic. It’s a very simple model where there are sparsely activating input features, and just a few of these are active on any given input. It’s also just a very simple model. It’s got a weight matrix. And that weight matrix, I suppose, it has as many rows as the number of sparsely active input features. And it has this basically smaller number of columns. So it’s basically a down projection matrix from data space into this hidden space. And then, this down projection gets up-projected by this matrix whenever you… You could use, in theory, a different matrix, but you can just transpose the matrix, and that spits out an output after you pass it through a ReLU activation. And you basically want to reconstruct the input data. And there’s a bias in there.\n\n(00:21:58): So suppose then you have some input datum, which has the first, and the third, and the fifth features active. These are in fact active inputs. But in APD, you haven’t yet learned whether or not there is a… Your parameter components haven’t yet learned to correspond to these input features. Maybe one thing then to think about is: well, suppose you have a bunch of different parameter components. Because they’re not aligned with the features, in some sense, there’s too many of these parameter components active. And APD is designed to learn only as many parameter components as are necessary to explain whatever the network is doing on this data distribution. And you basically don’t want it to learn more parameter components than are necessary. And this is what you achieve by both optimizing for minimality, such that as few as possible are necessary, and that they are as simple as possible.\n\n(00:23:41): And so, suppose you have two parameter components, where even though the ground truth feature one is active, two of your parameter components are in fact active. One is maybe slightly more active than the other. But you don’t want this, because ultimately you want to learn only one of these parameter components per input feature. And, the idea then is that in some of these cases where this input feature is active, and one of these parameter components is more active than the other - because it would be statistically unlikely that they are equally active - there will be cases where, because you’re thresholding this, the one that does get active will get updated, such that, it in future cases like this, where the feature one is active, it gets more attribution, versus cases where it doesn’t. I’m not sure if this fully addresses your concern. But, I guess I’m pointing at: if there is a ground truth where one thing is active and two parameter components are then active, then this is something that we do in fact get to avoid by \\[optimizing\\] for minimality and also for simplicity.\n\n**Daniel Filan** (00:25:03): Right. So I think the image I am getting from your answer - which might be totally wrong, so tell me if it’s wrong - but I think the thing you’re saying is: okay, probably for every input there are in fact some parts of the network that are more active on that input. And I think you’re almost saying: imagine there is some ground truth decomposition that’s not super big, right? Well, if I have input A and input B, right? And they in fact do use many of the same mechanisms, then basically APD is going to be disincentivized from the 1-3-5, 2-4-6 solution, just because you’re picking few mechanisms active on any given thing, but you’re trying to make it mimic what the network actually does. And so, if the thing the network is actually doing is using some of the same actual parts of the actual network, then you’re going to push these 2, 4, 6 to be close to the actual mechanisms of the actual network and you’re pushing 1, \\[3\\], and \\[5\\] to be close to the actual mechanisms of the actual network. So they’re just going to merge basically. Is that roughly right?\n\n**Lee Sharkey** (00:26:20): Yeah, I think so.\n\n**Daniel Filan** (00:26:21): Okay. So in that case, it seems like basically, for this story to work, you’re basically saying, “No, there is some ground truth decomposition, and because we’re doing this thing that’s getting close to the ground truth decomposition, that’s what’s powering our thing working,” as opposed to some constructivist thing of like, “Oh, here’s just the nicest way we can find of decomposing things.”\n\n**Lee Sharkey** (00:27:00): Yeah, this is a question I haven’t quite made up my mind on yet. I think, in toy models, it can be the case that you have a ground truth decomposition, because you made it that way. And the way that you might have designed this is that if someone came to you and told you, “Well, I’ve got an equivalent way to describe this network that you designed yourself.” And their description, it uses either more components than is necessary, or it uses more complex components than is necessary, then you might say, “Well, sure, kind of. But, I think this other explanation, the one I used in my head to design this network, is better.”\n\n(00:27:54): And in some sense then, it is more toward this constructivist way of thinking. Maybe then, there is actually no such thing as a ground truth explanation for the network, even though you designed it. And even though you said, “This is the ground truth explanation.” If there are other equivalent things where more objects, more complexity was necessary, then sure, they’re still explanations, but they’re not as good. And, in the case of more natural networks, maybe it is also the case that even though we can debate whether or not there is some ground truth to the thing that the network is doing, the style of explanation that we most prefer is something that is the shortest, simplest explanation for what the network is doing.\n\nSimplicity\n----------\n\n**Daniel Filan** (00:28:44): So I think, before we go further into the philosophy of APD, I think I want to just get through the parts so that people fully understand. So the third component of this objective function is simplicity. You’re optimizing each component to be simple. Can you tell us: what’s “simple”?\n\n**Lee Sharkey** (00:29:06): So the way we defined “simple”… “Simple” is supposed to capture this intuitive notion that it uses as little computational machinery as possible. And what does it mean for a set of matrices to use as little computational machinery as possible? The definition that we settled on was that if the network consists of one matrix, that matrix is as low rank as possible. You can’t get much simpler than a rank one matrix and a rank two matrix is less simple. It does more things to a given input vector. And, if your network consists of more matrices than just one, you basically get penalized for ranks in those matrices as well. So basically, the thing that we want to minimize is the sum of ranks overall of the matrices in a network. Now, I don’t know, we’re not fully happy with this, but we do think that this is a fairly reasonable notion of what it means to use as little computational machinery as possible.\n\n**Daniel Filan** (00:30:41): Yeah. So if I think about what that’s saying, right? So there is something intuitive there, right? For instance, if you use fewer matrices, that should count as more simple. “Lower rank” is basically saying, your matrix is secretly over a smaller dimensional input/smaller dimensional output space.\n\n**Lee Sharkey** (00:31:03): Yeah.\n\n**Daniel Filan** (00:31:04): I think it’s in some ways being basis-independent in this interesting sense, right? You’re saying that the identity function, versus a weird rotation and scaling, as long as you’re doing it on the same number of dimensions, those count as the same, which I think is actually plausible, given that different layers’ activations functions are, in some sense… Maybe they just should be incomparable in that way. Maybe you don’t want to equate these neurons with these neurons.\n\n(00:31:39): Maybe the other thing that seems slightly strange about that is by being basis-independent, by saying that the complexity of this weight matrix is just the rank of it… Suppose you have two components in one layer, right? By saying the complexity of both of them is the rank, somehow you’re saying that the basis you’re thinking about for the computation of thing A, and the basis you’re thinking about for the computation for the thing B, are just not related at all. And maybe there’s something there that’s worth… I don’t exactly know what the objection there would be, but it seems like there’s possibly something there that’s worth getting into.\n\n**Lee Sharkey** (00:32:30): Yeah, I mean, I think that’s just something that we’re willing to accept. In some sense, the exercise we’re trying to do here is basically discretize the network into discrete objects. And ideally, we want to discretize it into objects that have as little to do with each other as possible. And, if it is the case then that we can in fact just distinguish between one kind of operation and another - sometimes that operation is used and on other data points it is not - then I think we’re okay with that. But one of the reasons that APD was developed was the case of multi-dimensional features. And the idea of a multi-dimensional feature is that, well maybe you don’t get to just break things up into rank one components, maybe you actually do in fact need more than one. So the classic example here is the days of the week features, where the days of the week lie on points on a circle.\n\n**Daniel Filan** (00:33:41): And crucially they’re in the right order, right? It’s Monday, then Tuesday, then Wednesday.\n\n**Lee Sharkey** (00:33:46): Yeah, exactly. And, in order to describe these features, sure you can describe them as seven different directions in activation space, but you can more succinctly describe them as two-dimensional objects, basically. And, if you want to understand the operations that are done on those, it might just be useful to think of them as two dimensions, rather seven one-dimensional objects. But the idea is that we want APD to be able to decompose networks into chunks that if they do have these computational units that are best thought of as two-dimensional, rather than one-dimensional, that it can indeed find those, and isn’t just decomposing things into too many objects.\n\nConcrete-ish examples of APD\n----------------------------\n\n**Daniel Filan** (00:34:50): Fair enough. So I guess I next want to just talk about… So I want to test my intuitions for “do these objective functions make sense when I compare against certain examples?” So the first example I want to ask about is: suppose I have a decomposition of the network that’s just, each component is one layer of the network: component one is the first layer, component two is the second layer, component three is the third layer. I feel like that might score well on APD, as long as you’re allowed that many components.\n\n(00:35:32): So, my reason for thinking this is: basically, you have the cost that each matrix is full rank, but unless there are… I guess it’s possible that there are unused dimensions in the network that you could prune off. Sometimes in ReLU networks, some neurons will die. So yeah, suppose you’re taking the weight matrices but you’re pruning off the dead ReLUs. It seems like that might actually be optimal as long as you’re allowed that many components, just because it’s a perfect recreation of the network, and no other way of mixing around the things is going to save on the total rank, because you just need that much rank total. Is that right?\n\n**Lee Sharkey** (00:36:17): It depends on the data distribution. So, there is a case where it is, right, but it’s a fairly strange case. Suppose you have a data distribution where for every input, all of your ReLUs in your three layers are always active. So fine, you’ve pruned off the dead ones, those never activate, but on the other ones that do activate, they’re always active. So everything is always above threshold. And so what you’ve really got here is just three linear transformations. And in that case, you don’t really get to summarize that any more than just describing the three linear transformations, because on every given input in our data distribution, there’s always some non-zero amount that each rank is used. Fine, there’s going to be some infinitesimally small number of cases where it’s perfectly orthogonal to some of the smallest singular dimensions of some of these matrices, where in that very small number of cases, that an activation that aligns with that dimension, the attribution will be zero. But in almost every case, all of the ranks will be used.\n\n(00:37:42): Now you can imagine for certain other data distributions… Well, I guess maybe one way to think about it is that that wouldn’t be a very useful neural network, because it’s just doing the same transformation to every input, and that you might as well just use one linear transformation. The interesting thing about neural networks is that they can do different transformations to different inputs. And in that case then, in some inputs, you may use transformations that go one way, and on other inputs you may use transformations that go another way. That’s the kind of thing that you want to be able to break up using APD.\n\n**Daniel Filan** (00:38:26): Right. Sorry, if I think through this example, it seems like… Suppose you have these alternate set of mechanisms, right? This alternate decomposition, where on this input, we’re only using this half of the neurons, and on this \\[other\\] input we’re only using this half of the neurons. At first… Tell me if I’m thinking about this wrong. It seems like this is actually a case where minimality per input is actually buying you something, because in my imagination you’re still using the same amount of rank, and maybe you still have the same total number of things, but the thing you’re saving on is in the per layer thing: every layer is active on every input, right? But if you can break it up with “oh, this is only using a subset of the neurons, so I only need this subset of the mechanisms,” it seems like maybe the thing I’m saving on there is… Maybe it’s rank, and maybe it’s number of components, but on a per input basis, rather than over all the inputs.\n\n**Lee Sharkey** (00:39:36): Yeah, I think that’s right. So, suppose you have two components in each of these layers, and you’ve got three layers, and so you’ve got six components overall. Well, if parameter components… Suppose your data distribution is split up such that you can in fact throw away half the network that is involved in one half of the data distribution, and you can for the other half of the data distribution, throw away the other half of the network. So you can basically just treat these as two separate networks that happen to be mushed into one.\n\n(00:40:20): So we’ve got these six parameter components, and if they’re lined up such that three of these parameter components correspond to one of these data distributions, and the other three correspond to the other data distribution, then yes, on some inputs, you’ll be able to use only three, and on others… Well yeah, in all cases you’ll be able to use just three. But if your parameter components don’t line up perfectly with these distributions, you’ll have to use six every time, which is just not something that you want to do, if you want to decompose it into a smaller number of active objects at any given point.\n\n**Daniel Filan** (00:41:04): Okay. So I think I feel satisfied with that case. I next want to just talk about: so this is a little bit out there, but to help me understand, I think it would be helpful for me to talk about doing APD to a car, right?\n\n**Lee Sharkey** (00:41:20): Go on. Yeah.\n\n**Daniel Filan** (00:41:21): So, basically because a car is an instance where I feel like I understand what the… Well, okay, I’m not actually that good at cars, but I have a vague sense of what they’re like, and I think I have a vague sense of what the mechanisms in cars are. So, if I imagine taking a car and doing APD to it, I want some decomposition of all the stuff in the car that firstly, all the stuff in all of the decompositions just reconstitutes the whole car. I’m not leaving out a bit of the car. That makes sense to me. Secondly, I want there to be as few parts to my decomposition that are relevant on any given car situation. So like if there’s some situation, maybe suppose we discretize time, and there’s some input to me driving, and then I do a thing, and then the car… You know. Maybe it has to be a self-driving car for this to fully make sense. And then the third thing is that each component, the components have to be as simple as possible.\n\n(00:42:24): One concern I have is: I think when people are driving a car, usually there are a bunch of components that are active at the same time, that are basically always active at the same time, even though I think of them as different components. So one example is: there’s always an angle at which the steering wheel is going, and whenever the car is on, that angle matters to how the car is going. There’s also a speedometer, which tells you how fast you’re going, and that speedometer is always active whenever the thing is on.\n\n(00:43:07): Would APD tell me that the steering wheel and the speedometer are part of the same component? I worry that it would, because I think there’s no complexity hit from describing… If I describe them separately, that I have the complexity of the speedometer, plus the complexity of the steering wheel, these two things. And if I describe them jointly as a speedometer and the steering wheel, then I’ve got to describe the speedometer and I’ve got to describe the steering wheel, same amount of complexity. But in the case where I merge them, I have one component instead of two. And there’s never some cases where the steering wheel is active but the speedometer is not active, or vice versa - if I understand cars correctly, maybe people have a counterexample. So, in this case, would APD tell me that the speedometer and the steering wheel are part of the same mechanism? And if so, is this a problem?\n\n**Lee Sharkey** (00:44:13): I think that there’s a kind of, I don’t know, functionalist stance that we’re taking here. We want to understand a particular function of the car, and I think it might help to specify what that function is. So, suppose that function is just, “get me, a human, from A to B.” So, suppose I live in a country that doesn’t require speedometers, and I don’t really care what my speed is, and it really just doesn’t affect my behavior, and therefore it doesn’t affect the behavior of the car. In this case, we can basically ablate the speedometer, and the car would go from A to B with very little changed. Now in a different context, whether or not there’s a speedometer might affect the decomposition that we think is the most succinct description of the thing that is doing the driving from A to B.\n\n(00:45:31): A more general case might be: well, we have the engine, and we have the brakes. Now, whenever I’m moving, the brakes are not always on. And so whenever I don’t need the brakes, whenever I’m not braking, I can basically ablate the brakes, and the behavior of the car, the behavior of the Lee and car system is basically going to be unchanged. Now of course if I ablate the brakes, and then do want them, there is a difference between those two worlds where I do have the brakes, and I don’t, and there’s some sense in which breaking it up into a thing that makes the car go forward and a thing that makes the car stop is actually a useful decomposition.\n\n(00:46:11): So, bringing it back to your example, I do think that it matters the kind of function that we are specifying here. And in the case that you mentioned, it might not matter whether or not you decompose the car into the engine and the speedometer, because it’s all one part of… In your example there was no driver, and it’s all part of one causal process. The speedometer is just basically intrinsically attached to the engine, and we therefore don’t really get to chunk the system up into two different objects. But because what we’re describing as the function here matters, that determines whether or not you can in one sense decompose them and in another sense not.\n\n**Daniel Filan** (00:47:10): Right. So maybe one way of saying this is: how do you tell the speedometer and the steering wheel are different? Well one way you can do it is you can have test cases, where you have this guy who doesn’t really care about how fast he’s going - which is still a little bit weird, right? Because at least back when I was driving, that was relevant to “can you turn?” But I don’t know, maybe you can just figure that out by looking at the road, and being smart, right? But at the very least you can go to a mechanic, and you can get your car in some sort of test situation, where you’re just checking if the speedometer is accurate by hooking it up to some car treadmill thing, and the steering wheel doesn’t matter there, maybe, or vice versa. So one way I could think about this is: this shows the importance of a diversity of inputs for APD, that you’ve really got to look at the whole relevant input space, and if you don’t look at the whole relevant input space, you might inappropriately merge some mechanisms that you could have distinguished. Is that maybe a takeaway?\n\n**Lee Sharkey** (00:48:33): Yeah, that feels right. It feels right that in order to decompose networks into all the distinct mechanisms, we do need to look at all the cases where those mechanisms may be distinguishable. Yeah, that feels like a reasonable takeaway.\n\n**Daniel Filan** (00:48:58): Sure. I guess the next thing… Actually the other thing about the car that I thought about when you were talking about it is, it seems relevant for just identifying which mechanisms are active. So, in the paper, the test for whether a mechanism is active is this gradient-based attribution, which is basically like, “if you changed this bit of the network, would that result in a different output?” Now suppose I’m driving, and I’m not using the brakes. If you change the brakes such that they were always on, then that would change my driving behavior, right?\n\n**Lee Sharkey** (00:49:34): Correct, yes.\n\n**Daniel Filan** (00:49:35): Or even in an incremental way, right? Like if you changed the brake pedal such that it was always a little bit pressed, that would be slowing me down.\n\n**Lee Sharkey** (00:49:43): Yeah.\n\n**Daniel Filan** (00:49:45): So, am I right to think that if… And maybe we’re just straining the limits of the analogy or whatever, but am I right to think that if we used the equivalent of gradient-based attribution to decomposing a car, you would be thinking that the brakes were always an active mechanism?\n\n**Lee Sharkey** (00:50:05): I think it may be running up against the limits of the analogy, maybe. But one of the things that the gradient-based attribution is supposed to approximate is if you were to… What gradients are actually measuring is: if you twiddle the activations or the parameters in one direction, will it affect the thing with which you’re taking the gradient of? I don’t know, this is supposed to approximate basically “how ablatable is this direction?” You’re basically saying, “If I moved, if I didn’t just do a small twiddle, but did a very large twiddle from where I currently am to zero, then should it affect the thing that I’m taking the gradient of?” You’re basically taking a first-order approximation of the effect of ablating. That’s just what you’re trying to do whenever you’re taking the gradient here.\n\n(00:51:10): So, maybe ablatability is a way to port this into the analogy. Hence, if you can ablate the brakes, and nothing changes in that situation, then the brakes are in some sense… For this moment, the brakes are degenerate, the brakes just are not needed for this particular data point, a data point where I did not need to brake. But on data points where I was braking, I do not get to ablate the brakes and have that. The state does change quite a lot, whether I ablate the brakes or not in cases where I am in fact requiring the brakes.\n\nWhich parts of APD are canonical\n--------------------------------\n\n**Daniel Filan** (00:52:00): Fair enough. So, I guess the last question that I want to ask just to help me understand APD is, if I recall correctly, in either the abstract or the introduction of the paper, there’s this disclaimer that, “Okay, parts of this are just implementation details, and there’s a core idea, and there’s how you made it work for this paper, and those are not quite the same thing.”\n\n**Lee Sharkey** (00:52:28): Yeah.\n\n**Daniel Filan** (00:52:28): Out of the stuff that we talked about, which parts do you feel like \\[are\\] the core important parts of the version of APD that you’re interested in investigating? And which parts of it just felt like, “Okay, this is the first way I thought of to operationalize this thing?”\n\n**Lee Sharkey** (00:52:46): Certainly using gradient-based attributions is not something that we’re wedded to at all. What they’re supposed to do, as I mentioned, is just figure… It’s supposed to get some measure of how causally important a given parameter component is. Now it’s not the only potential method that you might consider using. You should be able to sub in any method of causal attribution there, and replace that. This is something that we’re keen to replace, basically, because gradient-based attributions will have all sorts of predictable pathologies, such as… Well, I mentioned that it’s the first-order approximation of causal ablation, but it is really just a first-order approximation - it’s not going to be very good.\n\n(00:53:39): There will be cases where if you twiddle the parameters in a certain direction, the output doesn’t change very much, but in fact if you ablate it the entire way, it does change a lot. A classic example of this is attention, where if you’re really paying a lot of attention to a particular sequence position, your attention softmax is basically saturated on that sequence position, and even if you change the parameters a fair bit, locally, it may not change very much, but if you change them a lot, you may go from a region where you’re saturated to non-saturated, and then you realize, ah, in fact this was a causally important sequence position. And so there’s just lots of predictable pathologies that will arise out of gradient-based attributions.\n\n(00:54:43): We’re also not totally wedded to the definition of simplicity that we have. We’re open to other potential definitions that may be more principled. For instance, one of the main motivations in the design process of this method was not to be basis-privileged. And there are a bunch of reasons for this, but one of the reasons is that, well, representations or computations in neural networks seem to be distributed over a very large number of different things. The classic case is that you don’t get to just look at an individual neuron, and understand an individual function within the network by looking at one neuron. You have to at very least look at multiple neurons. Things seem to be distributed over multiple neurons.\n\n(00:55:44): But it gets even worse than that. Representations may be distributed across multiple layers, in fact, especially in residual networks, where you don’t really get to just look at one layer to understand something, you have to look at multiple. And the same thing goes for attention heads. Maybe, in fact, a lot of analysis looks at individual attention heads, but this is kind of an assumption. We’re kind of assuming that the network has chunked it up such that one head does one thing, and there’s some intuitive reasons to believe that, but there are some intuitive reasons to believe that one neuron does one thing, and there’s no fundamental reason why it can’t distribute things across attention heads. And there’s some toy examples and some empirical evidence that this may be happening in networks.\n\n(00:56:35): And so there’s a bunch of reasons why you might not want to be basis-privileged. And the thing that our simplicity measure… it does in fact privilege layers, because it’s the sum over layers. It doesn’t privilege particular ranks, but it does privilege layers, and we’re open to versions of this metric that don’t privilege layers.\n\n(00:57:08): Aside from that, the fundamental thing about this whole method is that we get to decompose parameters into directions in parameter space, and we’re open to different ways \\[of\\] doing this. It’s more, we hope this is just a first pass of a general class of methods that do parameter decomposition, and the kind that we’re introducing to some extent here is linear parameter decomposition. We’re decomposing it into something that sums to the parameters of the original network, and we think that’s likely to be a somewhat powerful way to decompose networks. Not necessarily the only one, but yeah, we hope this points toward a broader class of networks, of which APD is just one.\n\nHyperparameter selection\n------------------------\n\n**Daniel Filan** (00:58:10): Sure. Okay. It turns out I lied. I have another question about how the method actually works, which is: I guess obviously there are a few hyperparameters in APD training, but one that feels very salient to me is how many components actually get to be active on any given thing? So, first of all, how, in fact, do you pick that?\n\n**Lee Sharkey** (00:58:36): It is one of the things that we want to move away from in future versions of the model. I mentioned that we were using an implementation that is like a top-k implementation, where you are just choosing a certain value of k, and you’re saying, “This is the number that is active on each data point.” In fact, we use batch top-k, where you get a little bit more flexibility per data point, but you still have to say, “Over a batch of a given size, we still want on average there to be only k active per data point.” And that’s a hyperparameter that is like… One of the main issues with the whole method is that it’s currently still pretty hyperparameter sensitive, and this is just one of the hyperparameters, that if you manage to get rid of, then you may arrive at a more robust method.\n\n(00:59:40): The way that we choose it is basically, because we’ve got toy models, we have ground truth, and we can know whether or not the method is doing the right thing, and we can basically search for the right number of values of k, such that it yields the ground truth mechanisms. But yeah, we want something that’s more robust, such that if you didn’t know the ground truth mechanisms, you could just choose an okay value for the hyperparameter and you could rest assured that you should end up with something approximately right.\n\n**Daniel Filan** (01:00:11): Right. So one thing that occurs to me is: so in the title, it says “Minimizing Mechanistic Description Length With Attribution-Based Parameter Decomposition”, and you present it as a part of this minimal description length… part of this family of things, where you’re trying to run some algorithm to describe stuff, and you want to… It’s related to all these ideas of Solomonoff induction and stuff.\n\n(01:00:43): And I thought that one of the points of minimal description length-type things was that it offered you this ability to have this principled choice of how to choose hyperparameters, or at least these sorts of hyperparameters. I think of MDL as saying, “Oh, when you’re doing regression, you can model it as a degree one polynomial, or you can model it as a degree two, or degree three,” and you have this trade-off between fit and something else, and MDL is supposed to tell you how many degrees of your polynomial you’re supposed to have. Right? In a similar way. I would imagine that it should be able to tell you, “okay, how many components are you supposed to divide into?” I guess you must have thought of this. Does that actually work?\n\n**Lee Sharkey** (01:01:34): The story’s a little bit more nuanced, in that minimum description length, whenever you’re dealing with, say, some continuous variables, you may have to fix one of your continuous variables, and say, “For a given value of this continuous variable, how few can I get in these other variables?” And in the case of an SAE you might say, “for a given mean squared error” or how low can I get basically the description of the set of activations, where that depends on how many things are active for a given data point, and how many features I’ve used in my sparse autoencoder dictionary.\n\n(01:02:25): The same thing kind of applies in APD. You need to fix some of your variables. So, the mean squared error is one of them. If you really want your mean squared error to be very, very low, you might get to ablate fewer parameter components, because you’ll just predictably increase the loss if you ablate things, even if your parameter components are perfect. But there are also some other continuous variables here. Even though we’re trying to minimize the rank. Rank is a non-differentiable quantity. What we are in fact getting to minimize is basically the sum of the singular values of the matrix. This is what we call in the paper the “Schatten norm”.\n\n(01:03:28): That’s just the name of the quantity. And so, this is a continuous approximation of the rank. Basically, if you minimize this, you minimize the rank. But it’s not a perfect quantity. But this is our measure of simplicity, and we kind of have to say, “for a given level of simplicity, how few active components do we get to have?” So there’s a lot of degrees of freedom that we have to hold constant, such that we can hold them constant and say, “how well can I do, in terms of minimum description length?” But yeah, we basically want to get toward a method such that we hold these things constant at a sufficiently low level, that we don’t have to really worry that we’re introducing arbitrary choices.\n\n**Daniel Filan** (01:04:31): Right. So in terms of, okay, you’ve got to balance against the loss… I had this impression that for a lot of these mean squared error losses, you could actually think of it as the likelihood of something, and end up measuring it in bits. So it makes sense that you would have to think about singular values, rather than literal rank, because in the presence of any noise… every matrix is full rank. Right?\n\n**Lee Sharkey** (01:04:57): Yeah.\n\n**Daniel Filan** (01:05:04): So you are dealing… One thing going on with description length-type things is that description length is inherently a discrete concept, like how many bits are you using to describe a thing? And if the thing is continuous, it’s like, well, at what resolution do you want to describe it? And I think this ends up being a hyperparameter, but a hyperparameter of MDL that seems like it’s relevant. In this case, it seems like: how many bits do you need to describe the “stuff”? If it’s parameters, then you can control that by saying, “If I quantize my network with however many bits, how bad is that?” I don’t know, maybe this is one of these things where if I sat down and tried to do it, I’d realize the issue, but it seems doable to me. It seems like there’s possibly something here.\n\n**Lee Sharkey** (01:06:03): Yeah, I do agree that it feels like we should be able to at least find a satisfactory Pareto frontier for minimum description length. I’m not sure we’ll be able to get away from… Requiring that it just be a Pareto frontier. I’m not sure there will be some sort of single optimal version of it, but at very least I do think we can do better than the current algorithm.\n\nAPD in toy models of superposition\n----------------------------------\n\n**Daniel Filan** (01:06:40): Fair enough. So, I think the thing I next want to talk about is basically the experiments you run in your paper. So, in my recollection, in the main paper, conceptually, there are two types of experiments. So there’s firstly this toy models of superposition, and secondly, there’s this…\n\n**Lee Sharkey** (01:07:08): Compressed computation.\n\n**Daniel Filan** (01:07:09): Compressed computation. Yeah. So, I mean, you spoke about it a little bit earlier, but first can you recap how the toy model of superposition experiments are working?\n\n**Lee Sharkey** (01:07:23): Yeah, so some of the folks who are reading our paper, and many listeners, will be familiar with the model, but again, it’s just this matrix that projects sparsely activating data down into some bottleneck space, and in that bottleneck space, features have to be represented in superposition, such that there are more features than dimensions in this bottleneck space. And then the matrix has to up-project them back to a space of the original size of the number of data features. So it’s like an autoencoder setup.\n\n(01:08:05): And because it compresses these data set features down, it’s kind of in some sense unintuitive that it can actually do this, because it has fewer dimensions than features. And because it has these fewer dimensions, there will be some interference between features that are not orthogonal to each other in this bottleneck space. But the way it gets over this is that, because it has ReLU activations following the up-projection, it can filter out some of this interference noise, and do a reasonably good job at reconstructing the input data features.\n\n(01:08:59): Now, one of the ways you might think about this network is that we have this matrix, and if one of the input data features is active, well, only, say, one row of the matrix is actually necessary. We can basically throw away the other rows. We can set them to zero, in cases where only this one - let’s call it “input data feature one” - is active. And in particular, the row that we have to keep is the first row. So, we can basically set the other rows to zero. And so, there’s some sense in which the rows of our toy model are like the ground truth mechanisms.\n\n(01:09:59): Why are they the ground truth mechanisms? Well, they satisfy the properties that we were aiming to recover here. So they all sum to the original network; that is, all the rows, whenever you set to zero the other rows, that basically sums to the original network. Then looking at minimality, because the dataset features are sparsely activating, there is… If you only activate the mechanism that corresponds to that dataset feature and you don’t activate other ones, well, this is going to be the smallest number of mechanisms that you have to activate on this data point, so it’s minimal.\n\n(01:10:48): And they’re simple, in some sense, in that single rows of this matrix, when you zero out all the other rows, are rank one. They just correspond to the outer product of an indicator vector and the row itself. So they satisfy what we wanted to call a “ground truth mechanism”. And the things that we’re basically optimizing are randomly initialized parameter components to try to approximate. And so what we then find whenever we do this is that at least for a given set of hyperparameters, we are able to recover this set of ground truth features using APD.\n\n**Daniel Filan** (01:11:39): Okay. So in the paper, one thing you mention is: so the original toy models for superposition… It has a bunch of geometry and it draws some pictures and that’s partly relying on the fact that there are five inputs and two hidden units, and that’s a setting where it’s just very small, and so things depend a lot on hyperparameters. You also look at a somewhat higher dimensional case where there’s what? 50 inputs and 10 hidden units or something? Is that right?\n\n**Lee Sharkey** (01:12:10): It’s 40 and 10, yeah.\n\n**Daniel Filan** (01:12:11): 40 and 10. So my understanding is that you are pretty hyperparameter-sensitive in this really small setting. In the 40 and 10 setting, how hard is it to get the hyperparameters right?\n\n**Lee Sharkey** (01:12:24): It’s easier, but I still think it’s pretty hard. The five and two case is particularly challenging because optimizing in a two-dimensional space is just… It’s something that gradient descent is not especially good at. I mean, it can do it, it’s just that moving vectors around each other can be more difficult in two-dimensional space versus in N-dimensional space, where they basically just get to move in any direction and not interfere with each other. In two-dimensional space, there’s just much greater chance for interference.\n\n**Daniel Filan** (01:12:59): Okay. I guess I’m just especially drawn to this hyperparameter of how many components you have. I don’t know. For some reason, to me, it feels like the most juicy hyperparameter, even though obviously, relative weighting of these objective terms and all sorts of things are also important. Well, in this case you have a ground truth number of components. If you get the number of components slightly wrong, what happens? How bad does it go?\n\n**Lee Sharkey** (01:13:35): I can’t recall an exact story for what happens, but for some cases it will learn a bunch of reasonable features, but then some features will just not be learned. In other cases, it will be just much more noisy and it’ll fail to learn altogether. I can’t give a good sense of how sensitive it is to this hyperparameter. My colleague [Dan](https://danbraunai.github.io/) \\[Braun\\] will have a much more informed sense of how sensitive it is to twiddling this. But it’s also hard to tell “is it this hyperparameter that is the most sensitive thing?” versus others. Because there’s basically a bunch of different hyperparameters to get right here, it’s hard to get really intuitive around any given one of them. Yeah.\n\nAPD and compressed computation\n------------------------------\n\n**Daniel Filan** (01:14:40): Okay. I eventually want to get to a question about these experiments in general. And so in order to get me there, can you tell me about the compressed computation setup and what’s going on there?\n\n**Lee Sharkey** (01:14:53): Yeah. So compressed computation is the name for a phenomenon that we observed in our experiments. We were initially trying to model two different things. One is a theoretically well-grounded phenomenon that my colleagues, Lucius \\[Bushnaq\\] and Jake \\[Mendel\\], had talked about in [a previous post of theirs: computation in superposition](https://www.alignmentforum.org/posts/roE7SHjFWEoMcGZKd/circuits-in-superposition-compressing-many-small-neural), where a network is basically learning to compute more functions than it has neurons. And there’s also a related phenomenon that’s more empirical, which is from [the original “Toy models of superposition” paper](https://transformer-circuits.pub/2022/toy_model/index.html) that they also called computation in superposition. And then there’s also this third phenomenon that we’ve called “compressed computation”.\n\n(01:15:56): Now, it may be the case that all of these are the same thing, but we are not yet confident enough to say that they are all exactly the same phenomenon. The reason is that we are not super, super confident - at least were not at the time. We became a little bit more confident and have slightly updated against the update - that the compressed computation is similar to these other phenomena, computation in superposition. Which one, I would not be able to answer. But it is nevertheless the case that all of these can be described as learning to compute more functions than you have neurons. It’s just that there’s a fair bit of wiggle room in the words when you put those words into maths.\n\n**Daniel Filan** (01:16:53): Sure. So with toy models of superposition, the basic intuition for why it was possible to reconstruct more stuff than you had hidden activation space dimensions, was that the stuff you had to reconstruct was sparse and so you didn’t have to do that many things at a time. Is that the same thing…? Sorry. This is almost definitely in the paper. In compressed computation, is the trick just it doesn’t have to compute all the things at the same time? Or somehow it actually really does compute all of the things at the same time with less space than you thought you would need?\n\n**Lee Sharkey** (01:17:30): This is the point in which we are uncertain, basically. Basically, we are not super confident about how much this phenomenon depends on sparsity. Now, we are also just not super confident on how much the Anthropic computation in superposition depends on sparsity. We know in their example it does, but because we don’t have access to the experiments, we don’t know what was going on in the backgrounds of those figures. We just haven’t got around to doing extensive experiments to actually figure that out. It wouldn’t be too difficult.\n\n(01:18:09): But in our case, we’re basically quite uncertain as to how much our phenomenon depends on sparsity. My colleague [Stefan](https://heimersheim.eu/) \\[Heimersheim\\] has done some experiments in this direction. It’s somewhat inconclusive for now. I think he’s got a project ongoing on this that hopefully there’ll be a write-up of soon. But yeah, long story short, it may or may not depend on sparsity, but I think for the purposes of the conversation, it may be reasonable to proceed as though it does.\n\n**Daniel Filan** (01:18:41): Okay. So basically, the thing of compressed computation is computing more functions than you have width of internal neurons, and it’s surprising that you’d be able to do it, but you can. And my understanding is that the particular functions you’re trying to compute are ReLU functions of the inputs.\n\n**Lee Sharkey** (01:19:06): Yes.\n\n**Daniel Filan** (01:19:08): And you might be like, “ReLU networks, shouldn’t they be able to do it?” But the trick is, the network narrows significantly. And so what is the hope here? What should APD be able to do in this setting, if it’s working?\n\n**Lee Sharkey** (01:19:25): So in this setting, the ground truth mechanisms are supposed to be things where, even though the data has, say, 100 input dimensions and the labels are 100 ReLUs of that input data, the models have learned basically to compute 100 ReLUs using only 50 ReLUs in the model. And the idea here is that, well, if they’re able to do this, they are in some sense using… They’re distributing their computation over multiple ReLUs, such that they can nevertheless do this without interfering with other features whenever they are not active. So you’re basically computing more functions than you have neurons because you’re not always having to compute them all at the same time.\n\n**Daniel Filan** (01:20:27): Right. And so this is just because if you have a negative input, then all you have to know is that it’s ReLU is zero, and you don’t have to do that much computation to make sure you have the identity function?\n\n**Lee Sharkey** (01:20:39): Yes. But in other cases where suppose you have two input features that are positive, and so you need to compute two ReLUs. Well, if you have basically projected one of your input features to one set of hidden neurons, such that you can spread your ReLU function over these multiple ReLUs. And if they are a different set of hidden ReLU neurons than the other feature, then you should be able to make a good approximation of the ReLU of the input data, because the magnitude matters here. Suppose there was some overlap in one of their neurons between these two input features, well, they would double up and that would contribute to… They would basically overshoot in the output. And so if you spread things out a little bit, such that they don’t overlap very much, you should be able to compute things, with some interference, but ultimately compute more functions than you have neurons. But yeah, the cost is interference.\n\n**Daniel Filan** (01:22:01): Gotcha. And so just as long as you’re distributing over the set of neurons… Sorry, a thing that I just realized: the fact that you’re going from 100 inputs to 50 wide, which is half of 100, is that just for the same reason as numbers have a 50% chance of being positive and negative, and so on average, you only need to represent half of them?\n\n**Lee Sharkey** (01:22:24): I don’t think the number 50 was especially important. I think we could have easily chosen something else. Yeah, I think it was somewhat arbitrary.\n\n**Daniel Filan** (01:22:36): Okay. Fair enough. All right, so I was asking what APD is meant to get and what was the answer to that?\n\n**Lee Sharkey** (01:22:58): Yeah. Thanks for reminding me. So I was trying to get a sense of what the ground truth features should be. Sorry, I said ground truth features, ground truth…\n\n**Daniel Filan** (01:23:09): Mechanisms.\n\n**Lee Sharkey** (01:23:09): Yeah, mechanisms. And these ground truth mechanisms should be things that distribute across multiple hidden neurons. And so the input… you’ve got this down-projection matrix and then this up-projection matrix. Rather, maybe think about it as an embedding matrix, an MLP in matrix, an MLP out matrix and then an unembedding.\n\n(01:23:47): So it’s a residual architecture. And so you have this embedding matrix and this MLP in matrix, and whenever you multiply these two matrices together, you basically want to show that a given input dimension projects onto multiple hidden neurons. And this is what one component should do. And those hidden neurons should then project back to that output feature that corresponds to the input feature that you care most about. And so you can basically do this for multiple input and output features.\n\n(01:24:45): Because your input and output features are sparsely activating, you want your parameter components to mostly only correspond to one of these input and output computations. And so you want basically to have parameter components that line up strongly with these input and output components.\n\n**Daniel Filan** (01:25:11): Right. So it seems like the thing is, maybe you don’t know exactly which parameters light up or whatever, but you do know for each component that APD finds, it should reconstruct the ReLU of exactly one input and none of the rest of them. Is that basically right?\n\n**Lee Sharkey** (01:25:33): Yeah, basically. Because in this case, we basically get to define what the minimal set of components is, because we get to choose a lot about the data distribution.\n\nMechanisms vs representations\n-----------------------------\n\n**Daniel Filan** (01:25:43): Okay. So I think the thing that I’m wondering about with both of these tests is: so I think of the idea of APD as, previously a bunch of people have been trying to explain representation of features. They’ve looked at these neurons, they’ve said, “What do these neurons represent?” But you want to find the mechanisms, right?\n\n**Lee Sharkey** (01:26:05): Yep.\n\n**Daniel Filan** (01:26:06): Now, the thing that strikes me about both of these examples is they feel very representation-y to me. They’re like, “Okay. We’ve got this sparsely activating input and we’ve got this low-dimensional bottleneck space, and we want to reconstruct these parameter vectors to tell us how the bottleneck space is able to reconstruct each component of the input.” But for ReLU, it’s like, for each of these inputs, there should be something that’s representing the ReLU of that input, and I just want to divide into things that get the ReLU.\n\n(01:26:50): It seems to me that networks could have a bunch of mechanisms that don’t necessarily do representing individual features or things, right? Or potentially representing things could involve a bunch of mechanisms for any given thing you represent. Maybe there are five mechanisms that are necessary. But basically, I just had this thought, reading this paper, of: it feels like the experiments are too representation-y and not mechanistic-y enough. What do you think of this anxiety that I’m feeling?\n\n**Lee Sharkey** (01:27:27): Yeah, I think that’s reasonable. I share this. There are a few toy models that we would be keen to see people work on. I’ll also, just before I get into that, just say I do think that there’s some… In some sense, it’s not a perfect duality between representation and mechanisms or computation, the computation-y point of view. There’s nevertheless a relationship. It is therefore more a matter of perspective, like which one is most convenient to think about at a given point in time.\n\n(01:28:14): I do think that when designing these toy models, we wanted to get a method that works in very simple setups, where these representations do in fact correspond to the mechanisms, right? This is a case where it’s been easier to design networks where there’s a ground truth that’s easily accessible to us. We found it a little bit harder to train networks where you could be somewhat sure of what the ground truth was, even though there are multiple computational steps that may be involved. I think it’s perfectly possible. We did have some cases where we handcrafted some models. There’s an example of this in the appendix, but that had some pathologies. The gradients didn’t work especially well on this because it was handcrafted. And so we did find it somewhat challenging.\n\n(01:29:18): Now, there are some models that you could think of that may capture this notion a little bit more than the ones in the paper. One that’s very similar to what is in the paper could be: consider a toy model of superposition model where instead of just this down-projection and then up-projection, you have a down-projection and then, for example, an identity matrix in the middle, and then an up-projection. Or you can replace this identity with a rotation, say. Now, what would you want APD to find here? Well, you don’t really get to think about it in terms of representations anymore. Because fine, you’ve broken it up into these mechanisms in the down-projection and in the up-projection, but there’s this bottleneck where you’re doing something in the bottleneck, if it’s an identity or a rotation. Suppose it’s a rotation. It’s probably easier to think of that. You’re basically having to use all ranks of that rotation in the middle, for every given data point. You don’t actually get to chunk it up.\n\n(01:30:39): So what you would want APD to find is parameter components that correspond to the things that we originally found, for the simpler example here, where it’s just the rows of the down-projection matrix and the up-projection matrix, but then also, a component that corresponds to this rotation in the middle. Why? Because you’re having to use all ranks of this rotation for every data point. You always have to do it. You don’t get to throw it away and reduce your minimality. You don’t get to make it any simpler to reduce your simplicity. It’s just always there. And so this is maybe a case where you do get to think about it in terms of computational steps rather than representations.\n\n**Daniel Filan** (01:31:33): Before I go further, just to pick up on the thing I said: so I believe this is in Appendix B.1, you hand designed these networks to compute these functions or whatever.\n\n**Lee Sharkey** (01:31:42): Yes.\n\n**Daniel Filan** (01:31:45): How did you hand design the networks?\n\n**Lee Sharkey** (01:31:47): So I believe this was Jake \\[Mendel\\] and Lucius \\[Bushnaq\\] and [Stefan](https://heimersheim.eu/) \\[Heimersheim\\]. I may be misattributing there, but I’ve at least included all of them. One or the other may not been involved. But they, I think, just thought about it really hard and then came up with it. They’re not super complicated networks. They have particular steps. They just have a little gate and for certain inputs, your gate is active, and on other inputs, it’s not. And this lets you do subsequent computations. It’s been a little while since I’ve looked at it, but the basic principle is that it’s not a complicated network.\n\n**Daniel Filan** (01:32:35): So my recollection is that it’s basically sinusoidal functions. I guess if I had to, I could write down a network. If you just divide it up into piecewise linears for a wide network, you could figure out how to do it. It’s just tricky.\n\n**Lee Sharkey** (01:32:52): Yeah, yeah, yeah. Yeah, this network gave us a lot of grief because it’s intuitively quite a simple network. But because we are using gradient-based attributions, it just didn’t play nice with the method, even though to our naive selves, it intuitively felt like it should. But we eventually got it working, but it is demoted to the appendix.\n\n**Daniel Filan** (01:33:27): Fair enough.\n\n**Lee Sharkey** (01:33:28): For punishment.\n\n**Daniel Filan** (01:33:30): So you mentioned: in this toy network where you have “project down, do an operation in down-projected space and then un-project back up” - well, this is ideally what APD should find. When you say it like that, it sounds like an easy enough experiment to run. Have you tried it?\n\n**Lee Sharkey** (01:33:59): I believe we at various points gave it a go. I think it just wasn’t top priority to get the paper out.\n\n**Daniel Filan** (01:34:08): Fair enough.\n\n**Lee Sharkey** (01:34:09): It’s very possible that we have got this working already and I’m just forgetting. It’s also very possible that we had tried it and couldn’t get it working or at least didn’t want to invest the time to get it working, such as the sensitivity of the hyperparameters. But yeah, I would be keen to see a verification that it is at least possible for APD to find this. Intuitively, it feels like it ought to be able to. But yeah, I’d just like to see it empirically.\n\nFuture applications of APD?\n---------------------------\n\n**Daniel Filan** (01:34:41): Sure. I guess other things that strike me as interesting to look into… So there are a few cases in the literature where people do really seem to have identified mechanisms within neural networks. I guess the most famous one is these [induction heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html), right? As some people have pointed out, it can be a loose term. People can use it for a few things, but in at least some cases, people can just point to, look, this attention head, if you pay attention to this thing it’s doing and this thing it’s doing… Or I guess it’s two attention heads maybe. But you could just tell a very clear story about how it’s looking for an initial thing and then taking the thing after that. And then this thing copies the thing after that into the output. So that’s one example of a thing that feels very much like a mechanism and does not feel so representational.\n\n(01:35:42): Another example is group multiplication. These neural networks that are trained on group multiplication tables, they have to get the other ones… I guess I was semi-involved in a paper… Well, I chatted with one of the people and tried to make sure he was on track for things. But there’s this [Wilson Wu paper](https://arxiv.org/abs/2410.07476) that’s basically together with Jacob Drori, [Louis Jaburi](https://cogeometry.com/), and [Jason Gross](https://jasongross.github.io/). And basically, I think they end up with a pretty good story of how these networks learn group multiplication. They could basically tell you, they do this thing and then they transform it in this way and then they get this output, and it works because of this weird group theory fact.\n\n(01:36:34): I think there are a few more of these. I guess, for at least those examples, can we get APD working on those? How hard would it be to check if APD actually works on these?\n\n**Lee Sharkey** (01:36:54): It feels possible, certainly in toy models for the induction head. Indeed, it was one of the motivations for APD that I’d been working with various [MATS](https://www.matsprogram.org/) scholars, [Chris Mathwin](https://www.lesswrong.com/users/cmathw?from=post_header), [Keith Wynroe](https://www.lesswrong.com/users/keith_wynroe?from=post_header), and [Felix Biggs](https://felixbiggs.com/) as well, on decomposing attention in neural networks. It feels like you should be able to do this, just add in an SAE here or transcoder there. You can make some progress in this, but it just didn’t feel conceptually very satisfying.\n\n(01:37:36): And it basically was one of the motivations for APD that… Well, we really wanted a method where we don’t have to modify it, such that if you have a slightly different architecture - maybe it’s a gated linear unit or maybe it’s a state space model - ideally you wouldn’t have to adapt your interpretability methods to these. You should just be able to decompose, just apply the method that you have that works for all neural networks. That would be ideal. And so this was one of the motivations, looking at attention and how it may actually distribute computations across heads or various other ways to distribute it.\n\n(01:38:15): Now, it feels possible then that we should be able to do this in toy models of, say, induction heads. It would be a somewhat more complicated model than APD has been used for thus far, but it does feel possible, and it’s one of the things I’m excited about people trying. In the cases where… say, group multiplication or modular addition, it’s very possible that if you did apply APD to these models where you don’t really get to… It feels possible that in these models, all mechanisms are active all the time and therefore APD just returns the original network.\n\n(01:39:05): And if that’s the case, this is a bullet I’m willing to bite on the method. Sometimes we just don’t get to decompose things into more things than the original network. These are, after all, fairly special networks trained in quite different ways from the tasks that we really care about, such as language modeling. It’s nevertheless something to bear in mind when applying APD to models. It’s not going to immediately tell you, in cases where it may be a multidimensional feature, how to understand the interactions within this multidimensional, multilayer component. But at very least, what we wanted was a method that would, in cases where you could decompose it, where it actually succeeds in doing that.\n\n**Daniel Filan** (01:40:09): Right. Sorry. The thing you said just inspired me to look at this paper. So the paper is [“Towards a unified and verified understanding of group operation networks”](https://arxiv.org/abs/2410.07476). And the author I was forgetting was Louis Jaburi. Sorry, Louis. So there’s this question: for group multiplications, are all of the things active at once? I think I’m not going to be able to figure it out quickly enough in time for this, but yeah. It does seem like an interesting question of: can you get APD working in a setting where there are…\n\n(01:40:55): I guess it’s tricky because it’s a lot easier to have ground truth representations than ground truth mechanisms. Especially if you know, okay, I’m an autoencoder or I’m doing this known function of each individual input. And I guess this just relates to the fact that we understand… Representation is just much easier for us to have good a priori theories of than computation, somewhat, unfortunately.\n\n**Lee Sharkey** (01:41:23): Yeah, maybe I’m just too APD-brained at this point, but I’m curious, could you flesh that intuition out a bit more? I feel like what it means for a hidden activation to represent one of the input features in the TMS case doesn’t feel intuitively obvious to me. There may be a direction in hidden activation space that corresponds to one of the input dimensions. It doesn’t feel more intuitive, that point of view than, say, “this input feature activated that computation”. I’m curious-\n\n**Daniel Filan** (01:42:08): Yeah. I guess all I’m saying is that with toy models of superposition, it feels like the reason you can very confidently say, “This part is doing this thing,” is in some sense you know that all the neural network has to do is, it has to put a bunch of information into this two-dimensional knapsack and be able to get it out again, right? That’s just everything the network’s doing. And you can say, “Okay. Well, I understand that it should be sensitive to all these things.” I guess there are some things you can say about the computation there, but for cases like compressed computation, like toy models of superposition, you can just say, “Okay. Look, I have these things and I know this input should correspond to this output.” That’s just definite ground truth because it’s basically what I’m training on. Whereas, it’s a lot harder to look at a network and say, “Well, I know it should be doing this computation and I know it should be divided up into this way.”\n\n**Lee Sharkey** (01:43:15): Yeah, I think that’s fair.\n\n**Daniel Filan** (01:43:16): And therefore it’s easier to test against, well, do I reconstruct this thing in toy models of superposition, where I know what I’m supposed to reconstruct, versus do I reconstruct this way of doing things, where a priori, you don’t exactly know.\n\n**Lee Sharkey** (01:43:36): Yeah, I think that’s fair. And I think this maybe is part of the… This goes back to a little bit of what we were talking about at the start, where even though there may be multiple equivalent ways to describe the computations going on in the network, we really just have to be opinionated about what constitutes a good explanation, and faithfulness to the network, minimality, and simplicity are just the ones that we think are a reasonable set of properties for an explanation to have.\n\n**Daniel Filan** (01:44:12): Fair enough. So, okay, I think I’m going to transition into just more asking miscellaneous questions.\n\n**Lee Sharkey** (01:44:18): Yeah, sounds good.\n\nHow costly is APD?\n------------------\n\n**Daniel Filan** (01:44:19): Less grouped by a theme. I think the first thing I want to talk about is that at one point in your paper you say that, “So why are we doing APD on these small networks and not on Llama-whatever, however many billion models you can get these days?” And the answer is that it’s expensive to run APD. Concretely, how expensive is it to actually run?\n\n**Lee Sharkey** (01:44:48): So the version that we’ve come up with here is: we didn’t aim for efficiency, we didn’t aim for some of the obvious things that you might try to get a method that works more efficiently than ours, the reason being that we wanted something where on theoretical grounds we could be somewhat satisfied with it and satisfied with it working. And then after that we can move to things that are more efficient. So for the current method, what we have here is, for a start we’ve got - let’s call it the letter C - C components. We’ve got C components and each of these require as much memory as the original model, right?\n\n(01:45:35): Now, that’s already a multiple of the expensiveness of the original model just to do one forward pass. We also have the first forward pass, the first backward pass, the second forward pass and the second backward pass. So during one training update, we have these four steps. So it’s already a multiple of just a given forward pass and backward pass that might be required to train an original model, but I guess different goals with each of these steps. Yeah.\n\n**Daniel Filan** (01:46:22): So I don’t know, maybe the answer to this is just it’s another hyperparameter and you’ve got to fiddle with it: there’s a number of components that you want to end up being active, right? This k that you talk about. And then there’s just a total number of components that you have to have in order to run the method at all. Is there something to say about “it turns out you need five times as many total components as you want components active in any single run,” or is it just kind of a mess?\n\n**Lee Sharkey** (01:46:52): Well, some people will be familiar with training sparse autoencoders, and in some cases you start with more features than you expect to need, the reason being that during training, some might die. There’s various tricks that people have invented to stop them dying - reinitialization and so on. The same’s true in APD. Some of these parameter components will in some sense die and depending on the type of model, in general, you’ll want to train with a little bit more than the total number of ground truth mechanisms just so that on the off chance that some do die, you still nevertheless have enough to learn all of the ground truth mechanisms.\n\n**Daniel Filan** (01:47:42): Okay, but it sounds like you’re thinking that it has to be more like a factor of two than a factor of 100 or something.\n\n**Lee Sharkey** (01:47:52): That would be my expectation. I don’t think there’s going to be a ground truth answer to that, but yeah. I don’t see any reason why it would need to be many multiples higher.\n\n**Daniel Filan** (01:48:03): Okay. So I guess if you’re thinking about the expense of this, it seems like, okay, you have this constant blow up of you’ve got to do two forward passes and two backward passes on each gradient step, and also you’ve got to keep on having these C copies of the network around at all times. And then there’s this question of how many steps it takes to actually train APD, which presumably is just an empirical thing that is not super well understood. I guess one question I have is: if I remember correctly, there’s some part of your paper where you mentioned that naively, APD might take order of N squared time to run. Do I remember that correctly?\n\n**Lee Sharkey** (01:48:51): Yeah. I think this is a pessimistic upper bound on the expensiveness, but I think there’s plenty of reasons to expect it to be lower than this, but I would need to revisit the sentence to be 100% sure what we’re actually talking about.\n\n**Daniel Filan** (01:49:08): Fair enough.\n\n**Lee Sharkey** (01:49:09): There is a sentence that talks about the scaling and mentions O(N^2). Yeah.\n\nMore on minimality training\n---------------------------\n\n**Daniel Filan** (01:49:14): Okay. So the next thing that just came across my mind that I wanted to ask is: when you’re training for a minimality, so on each input you run it forward, you do attribution to get the k most active components, and then you drop all the other components, and then have some training step to make the k most active components reconstruct the behavior better on that. I’m kind of surprised - it seems like one thing you could imagine doing is also training the ones that you dropped to be less relevant on that input than they actually are. I’m wondering if you tried this and it didn’t work or if this is just less of an obvious idea than it feels like to me.\n\n**Lee Sharkey** (01:50:16): Yeah, I guess so concretely what you might consider doing in that case would be you might have a third forward pass where you only run it with… I guess I don’t know. It may be hard. I haven’t thought about this enough, but it may be hard to distinguish between the influences of… I don’t know. On the face of it, it feels like something that could be useful to implement if it’s possible to implement. Yeah, it does feel possible, for sure. I don’t recall us trying it, though.\n\n(01:51:07): The things that we did try were the top k and then we also tried an Lp sparsity version where you penalize everything for being attributed. You penalize everything for having some causal influence over the output, but you penalize the things that were most causally attributed proportionally less than the things that had some small influence. And this is kind of doing that, but it is not equivalent. But yeah, it feels possible to do that. I’d be curious if it could be done.\n\nFollow-up work\n--------------\n\n**Daniel Filan** (01:51:49): Gotcha. So I think at this point I’m interested in what follow-up work you think is important to do on APD, either that you think is important to do or if enterprising listeners maybe want to pick up some of the slack.\n\n**Lee Sharkey** (01:52:03): Yeah, so I’ve mentioned a few of the things that I’d be keen to see already. So non-exhaustively: attention - I’d be curious to see if it can decompose attention in a sensible way. There’s various other things. However, the main thing right now is figuring out whether or not we can make it less sensitive to hyperparameters and more scalable. Basically, these are the two: robustness and scalability are the main things that we’re keen to solve, just because it will open up… Whenever we do investigate these other things, like attention, distributed representations across attention, that will be less painful to do. And also, you can do this in larger, more interesting models. So the main thing is scalability and hyperparameter sensitivity or robustness.\n\n(01:53:07): Those being the two main things, suppose we solve those, I would be keen to see attention, keen to see other types of architecture decomposed here. There’s also a few other phenomena that you might be curious to apply APD to. For instance, the phenomena of memorization, right? You might imagine that memorization - whenever APD successfully decomposes the network into memorized data points versus generalizing data points - there may be one parameter component that corresponds to one memorized data point and one parameter component that corresponds to a generalizing computation within the network. It may therefore be a nice place to distinguish between these two computational regimes of memorization and generalization. So I’d be keen to see APD applied to that.\n\n(01:54:11): I mentioned some of the more theoretical things that you might want to look into, such as privileging layers or more implementationally figuring out whether or not we can get rid of having to do a top-k setup where you have to choose k. Then yeah, there’s a bunch of fairly disparate directions, all of which I’m super keen to see done. I think our main priorities now are just creating a method that makes those other things a bit easier. That’s a non-exhaustive view, though. There’s a more exhaustive list, I think, in the paper.\n\n**Daniel Filan** (01:55:08): Makes sense. So a couple things that seemed interesting to me, but I’m curious if you have comments on: so I guess somewhat inspired by our discussion about doing APD to a car, it seems like APD is a method that sort of is sensitive to the input distribution that you do training to. And I think there’s this [“Interpretability illusions” paper](https://arxiv.org/abs/2104.07143) that says: sometimes you might think that you have a rich enough input distribution, but you don’t actually. I think just how sensitive you are to this input distribution and how right you have to get it… I think that’s something that I don’t know if you’ve had much preliminary exploration into, but it does seem pretty relevant.\n\n**Lee Sharkey** (01:55:59): It seems relevant. I think this is in some senses unavoidable just because: I want to decompose neural networks. What does that mean? Well, it means to decompose what these networks are doing. What they’re doing depends on the input distribution. And simply with a different distribution, natural or unnatural, it just will lead to different things. I do think that when we get to more scalable versions of this method, this will become even more important. You ideally want to have a method where suppose you’re decomposing Llama 3 or whatever, if you’ve got a scalable method, you train it using the training distribution of Llama, but then you also train it with the training distribution of Llama permuted.\n\n(01:57:02): You ideally want to end up with the same thing, similarly for a large enough subset and then more adversarial subsets. It will be the case that for a sufficient level of adversity it will break. I think this maybe emphasizes the importance of just doing interpretability on as large a distribution as you possibly can, which stands in contrast from some of the interpretability that’s happened in the past. I like to call this “using the big data approach”, where you’re finding structure first and then asking questions later. It’s kind of borrowing from areas of science where there’s just a lot going on and you kind of want to leverage computation first to actually narrow down what questions you really ought to be asking.\n\n(01:58:13): And the application here in interpretability would be: you want to let computational methods do the work first, and then you figure out “what does this component mean?”, rather than presupposing your own ideas of what the components ought to be and then studying those in more detail. This is the kind of approach that I think APD intends to leverage, this big data approach. And I think that’s somewhat unavoidable in interpretability that can tell you things that you weren’t looking for in the first place.\n\n**Daniel Filan** (01:58:57): Fair enough. So another thing that struck my eye in the paper is: so there’s a section that is basically… I think of this section of the paper as basically saying why SAEs are bad and rubbish. And one thing that is mentioned is: there’s this feature geometry in SAEs, sort of like the day of the week thing where they’re in a circle, Monday, Tuesday, Wednesday. And I think there’s some line that says the fact that there is this geometry is not as - maybe Jake Mendel has written about this - but this is not purely explained by this linear representation hypothesis. We need to understand mechanisms to get us there. How soon until APD tells us what’s going on with SAE feature geometry, or feature geometry in general?\n\n**Lee Sharkey** (01:59:51): Yeah. So Jake’s post was, if I’m recalling the title correctly, [“Feature geometry is outside the superposition hypothesis”](https://www.alignmentforum.org/posts/MFBTjb2qf3ziWmzz6/sae-feature-geometry-is-outside-the-superposition-hypothesis). And feature geometry is this idea where… It’s older than the mechanistic interpretability community. This idea was present in neuroscientific literature a bit before, but the idea here is that: suppose you’ve got a neural network and you train an SAE on the activations and you look at the features that you end up with. These features tend to correspond to certain things. This was the whole point of training SAEs, to identify interpretable individual components. But whenever you start comparing the directions of these features relative to each other, you’ll notice that, if I look at the Einstein direction, the Munich direction, the…\n\n**Daniel Filan** (02:01:10): Lederhosen?\n\n**Lee Sharkey** (02:01:10): …I don’t know, lederhosen direction and so on, you’ll find that all these kind of point in somewhat similar directions. There’s a kind of latent semanticity to them. There’s something underneath these features. These features were supposed to correspond to the computational units of neural networks. And now what this feature geometry is indicating is that there’s an underlying computational structure that organizes these features relative to each other, which is, in my opinion, something of a… This doesn’t bode well if you considered SAE features to be fundamental units of computation, because you shouldn’t be able to identify these latent variables that are shared across multiple features. And what’s giving the structure? What is giving the geometry to these features? Well, the hypothesis here is that: suppose you have the Einstein feature and you’ve also got this Lederhosen feature and so on.\n\n(02:02:28): Well, these all get the German computation done to them. They’re all pointing in this direction because somewhere down the line in the network the network needs to do the German computation to them and just apply some specific transformation or some set of transformations that correspond to the German-ness of a thing. And you can imagine other cases for animal features. Why do all the animals point in similar directions? Well, the network needs to do animal-related computations to them. And now you could go further. Why do all the furry animals point in similar directions? Well, because there needs to be furry computations done to them. The hope here is that instead of studying the features and trying to use that as a lens to understand the network, study the computations and that will inform why the geometry is the way it is, because you get to look at the computations that get done to it, which is presumably why the network is structuring them in this way.\n\n(02:03:39): It’s very possible that you just kick the can down the road there. You may find that if you decompose your computations into very simple computational units, well, you might find that there’s some relationship between your computational units in terms of geometry, but it nevertheless feels like a you’ve done better than the SAE case, basically.\n\n**Daniel Filan** (02:04:06): Right.\n\n**Lee Sharkey** (02:04:08): It’s not obviously totally solved the problem.\n\n**Daniel Filan** (02:04:11): Yeah. So how long until APD explains all this?\n\n**Lee Sharkey** (02:04:18): Well, either you would need a toy model of feature geometry such that it’s a small enough model that you can apply APD to it. And that toy model would need to be convincing such that people can say that it probably applies to larger models. But absent a convincing toy model, you would need to be able to scale this such that you can apply it to larger models. I can’t say for certain when we’ll have a scalable method, it’s something we’re currently working on. We’re very keen for other folks to work on \\[it\\] as well. I would be speculating irresponsibly to say when we’ll have a working method for that, but I would hope that anywhere between three months and three years. That’s the kind of uncertainty.\n\n**Daniel Filan** (02:05:15): But I guess it illustrates the importance of just robustifying this thing to make it easier to run on bigger instances.\n\n**Lee Sharkey** (02:05:23): Yep.\n\nAPD on giant chain-of-thought models?\n-------------------------------------\n\n**Daniel Filan** (02:05:24): So I guess the last question that I want to ask is: what’s the end game of APD? Is the hope that I run it on the underlying model of o3 or whatever and then I just understand all the things it’s thinking about at any given point? How should I think about: where is this going? What’s it actually going to get me in the case of these big chain-of-thought networks?\n\n**Lee Sharkey** (02:06:06): Yeah, it’s an important question to ask. I think the way I see this kind of work and the way I see the similar work that came before, such as SAEs or transcoders or things like this… The point is to break up networks into as simple of components as you can, such that whenever you try to understand larger facts about the network you’ve got some solid ground to stand on. You can say, “Well, I got this set of components. If I were really invested, I could in theory just understand everything with this very large number of components.” Now, do I really think that mech. interp. is going to let us understand everything? Well, probably not as humans, but I do think that it will give us solid ground to stand on whenever we want to understand particular phenomena.\n\n(02:07:00): Now, if I want to understand, say, the deception mechanisms within e.g. o3 or any other model, where do I go looking for them? Well, currently we look at behaviors. One thing that you might be able to do is look at transcoder kind of approaches. But because transcoders and other activation-based methods are primarily looking at activations, they’re not necessarily giving you the things that are doing the generalization such that you are… I don’t know. I think you can be less confident that you’re understanding how the network would behave in a more general sense. And by looking at the objects that are doing the generalization, by looking at the parts of the parameters that are actually doing the thing, you might be able to make more robust claims.\n\n**Daniel Filan** (02:08:04): Yeah, I think it’s fair enough to say, yeah, look at very specific things. I guess there’s also some world in which once you’re able to have these good building blocks, you can do automated interpretability of everything, if you need to.\n\n**Lee Sharkey** (02:08:18): For sure. Yeah. I mean, I guess I’m leaving that implicit. Yeah, the ultimate aim would be that you can automate the process by which you would understand parts of the network such that you can understand broader swathes of it. And yeah, ideally you have given yourself a solid enough ground to stand on that whenever you do this, fewer things will slip through the cracks.\n\n**Daniel Filan** (02:08:45): Sure. I guess one thing that strikes me as interesting about these reasoning models in particular… And sorry, this might be kind of far afield, but I think a lot of interpretability work has been focused on understanding single forward passes. Especially for classification models, the early stuff was done on vision classification, where of course you just want to find the curve detectors or whatever. And for SAEs you’re like, “Oh, which things are being represented?” One thing that I think reasoning models bring to light is: it seems to me in some sense, the relevant mechanisms should be thought of as distributed across forward passes, right?\n\n(02:09:33): You do a forward pass, you write a thing in your chain of thought, then you do another forward pass, you write another thing in your chain of thought. And in some sense, the real mechanism is a bunch of these end-to-end copies of this network. This might be too speculative to ask about, but where do we go in that setting? Do you think it still makes sense to focus so much on understanding these individual forward passes versus the whole web of computation?\n\n**Lee Sharkey** (02:10:07): I think it probably does. The reason being, what alternatives might we aim for? If we wanted to instead just to ensure that in these more distributed settings where computations are spread across a whole chain of thought, well, what might we do in that case? We care about the faithfulness of the chain of thought. So in the case where we care about the faithfulness, we want some way to measure how faithful the chain of thought actually is being. And mech. interp. does give you some measure of: if you can understand a given forward pass and maybe even a small chain, it should give you firmer ground to stand on whenever you make claims about, “this method that I developed that improves the faithfulness of the chain of thought…” I don’t know how you can make such statements without actually having some way to measure the faithfulness of the chain of thought, and that’s maybe one way that mech. interp. may be able to help in that regime. Yeah, that’s just kind of the one thing that comes to mind.\n\nAPD and “features”\n------------------\n\n**Daniel Filan** (02:11:27): So wrapping up, I want to check, is there anything that I haven’t yet asked you that you think I should have?\n\n**Lee Sharkey** (02:11:36): I think one of the things that I find most… satisfying, maybe, about thinking about interpretability in parameter space is that many of the notions that we had going into interpretability become a little less confusing. So one of the main examples that I have in mind here is just this idea of a feature. People have used this notion of a feature in a very intuitive sense and struggled for a long time to actually nail it down. What is a feature? What are we really talking about here? It kind of evaded formalism in some sense. And I think one of the things that I find most satisfying then about interpretability in the parameter space is that it gives you some foundation on which to base this notion. In particular, the thing that we might call a feature of a network is something that uses one parameter component.\n\n(02:12:49): For instance, what does it mean to say that a model has a feature of a cat inside it? Well, you can perhaps equivalently say that this model has got a cat classifier computation or it’s got a cat recognition computation. This is what I mean. There’s a kind of duality between… It’s not an exact duality by any means, but it helps provide a sense in which features mean something specific. In particular, it means whenever you break up a network into faithful, minimal, and simple components, these components, these mechanisms are what you might reasonably call… In some cases, you couldn’t call them a feature. In other cases, it’s more natural to think about them in terms of “this is a step in the algorithm. This is a computation that the network is doing.” And I think in that sense it’s a bit more general than thinking about things in terms of features.\n\nFollowing Lee’s work\n--------------------\n\n**Daniel Filan** (02:14:11): Fair enough. Well, I guess to finally wrap up, if people listen to this and they’re interested in following your research, how should they do that?\n\n**Lee Sharkey** (02:14:26): Yeah, I post most of my things. I post them on Twitter and I also post on the Alignment forum as well. You can just follow [me on Twitter](https://twitter.com/leedsharkey) and check out [me on the Alignment Forum](https://www.alignmentforum.org/users/lee_sharkey).\n\n**Daniel Filan** (02:14:38): So links to those will be in the description. But for those who don’t want to open the description, are you just “Lee Sharkey” on Twitter and the Alignment Forum?\n\n**Lee Sharkey** (02:14:48): I think I am [leedsharkey on Twitter](https://twitter.com/leedsharkey), at least in my Twitter handle, but I should just be Lee Sharkey and findable by that. And yeah, [Lee Sharkey on the Alignment Forum](https://www.alignmentforum.org/users/lee_sharkey).\n\n**Daniel Filan** (02:14:58): All right, well, thanks very much for coming here. We’ve been recording for a while and you’ve been quite generous with your time, so thank you very much.\n\n**Lee Sharkey** (02:15:05): No, thank you, Daniel. It’s been great. I’ve had an awesome time. Cheers.\n\n**Daniel Filan** (02:15:08): This episode is edited by Kate Brunotts and Amber Dawn Ace helped with the transcription. The opening and closing themes are by Jack Garrett. The episode was recorded at FAR.Labs. Financial support for the episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript, you can visit [axrp.net](https://axrp.net/). You can also become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast) or give a one-off donation at [ko-fi.com/axrpodcast](https://ko-fi.com/axrpodcast). Finally, you can leave your thoughts on this episode at [axrp.fyi](axrp.fyi).",
      "plaintextDescription": "YouTube link\n\nWhat’s the next step forward in interpretability? In this episode, I chat with Lee Sharkey about his proposal for detecting computational mechanisms within neural networks: Attribution-based Parameter Decomposition, or APD for short.\n\nTopics we discuss:\n\n * APD basics\n * Faithfulness\n * Minimality\n * Simplicity\n * Concrete-ish examples of APD\n * Which parts of APD are canonical\n * Hyperparameter selection\n * APD in toy models of superposition\n * APD and compressed computation\n * Mechanisms vs representations\n * Future applications of APD?\n * How costly is APD?\n * More on minimality training\n * Follow-up work\n * APD on giant chain-of-thought models?\n * APD and “features”\n * Following Lee’s work\n\nDaniel Filan (00:00:09): Hello everybody. In this episode, I’ll be speaking with Lee Sharkey. Lee is an interpretability researcher at Goodfire. He co-founded Apollo Research, which he recently left, and he’s most well-known for his early work on sparse autoencoders. Links to what we’re speaking about are available in the description. There’s a transcript available at axrp.net. You can tell me what you think about this episode at axrp.fyi. And you can become a patron at patreon.com/axrpodcast. Well, let’s continue to the episode. Well, Lee, welcome to AXRP.\n\nLee Sharkey (00:00:40): It’s good to be here.\n\n\nAPD basics\nDaniel Filan (00:00:41): So today, we’re going to talk about this paper “Interpretability in Parameter Space: Minimizing Mechanistic Description Length with Attribution-Based Parameter Decomposition”. It’s authored by Dan Braun, Lucius Bushnaq, Stefan Heimersheim - those three being, I guess, joint first authors - Jake Mendel and yourself. So I guess, how would you summarize just: what’s this paper doing?\n\nLee Sharkey (00:01:03): So I would say that this paper was born out of two lines of thinking, one primarily coming from what I was thinking about and one coming from where Lucius was thinking about. And where I was coming from was: we’d been workin",
      "wordCount": 18393
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "tz43dmLAchxcqnDRA",
    "title": "Consider not donating under $100 to political candidates",
    "slug": "consider-not-donating-under-usd100-to-political-candidates",
    "url": null,
    "baseScore": 138,
    "voteCount": 97,
    "viewCount": null,
    "commentCount": 32,
    "createdAt": null,
    "postedAt": "2025-05-11T03:20:04.986Z",
    "contents": {
      "markdown": "Epistemic status: thing people have told me that seems right. Also primarily relevant to US audiences. Also I am speaking in my personal capacity and not representing any employer, present or past.\n\nSometimes, I talk to people who work in the AI governance space. One thing that multiple people have told me, which I found surprising, is that there is apparently a real problem where people accidentally rule themselves out of AI policy positions by making political donations of small amounts—in particular, under $10.\n\nMy understanding is that in the United States, donations to political candidates are a matter of public record, and that if you donate to candidates of one party, this might look bad if you want to gain a government position when another party is in charge. Therefore, donating approximately $3 can significantly damage your career, while not helping your preferred candidate all that much. Furthermore, at the time you make this donation, you might not realize that you will later want to get a government position.\n\nNow, I don’t want to overly discourage this sort of thing. It’s your money, free speech is great, and fundamentally I think it’s fine to have and publicly express political views (for example, I think Donald Trump is extremely bad, and am disappointed in my fellow countrymen for voting for him). That said, I think that one should be aware of the consequences of making political donations, and it seems plausible to me that if you’re not willing to donate more than $100 to a political candidate, consider that the career cost to you of making that donation may be higher than the benefit that it confers.",
      "plaintextDescription": "Epistemic status: thing people have told me that seems right. Also primarily relevant to US audiences. Also I am speaking in my personal capacity and not representing any employer, present or past.\n\nSometimes, I talk to people who work in the AI governance space. One thing that multiple people have told me, which I found surprising, is that there is apparently a real problem where people accidentally rule themselves out of AI policy positions by making political donations of small amounts—in particular, under $10.\n\nMy understanding is that in the United States, donations to political candidates are a matter of public record, and that if you donate to candidates of one party, this might look bad if you want to gain a government position when another party is in charge. Therefore, donating approximately $3 can significantly damage your career, while not helping your preferred candidate all that much. Furthermore, at the time you make this donation, you might not realize that you will later want to get a government position.\n\nNow, I don’t want to overly discourage this sort of thing. It’s your money, free speech is great, and fundamentally I think it’s fine to have and publicly express political views (for example, I think Donald Trump is extremely bad, and am disappointed in my fellow countrymen for voting for him). That said, I think that one should be aware of the consequences of making political donations, and it seems plausible to me that if you’re not willing to donate more than $100 to a political candidate, consider that the career cost to you of making that donation may be higher than the benefit that it confers.",
      "wordCount": 277
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "TFToqpaKMhcjAEY5E",
    "title": "AXRP Episode 40 - Jason Gross on Compact Proofs and Interpretability",
    "slug": "axrp-episode-40-jason-gross-on-compact-proofs-and",
    "url": null,
    "baseScore": 26,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-03-28T18:40:01.856Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/ZzEj7jPk5fc)\n\nHow do we figure out whether interpretability is doing its job? One way is to see if it helps us prove things about models that we care about knowing. In this episode, I speak with Jason Gross about his agenda to benchmark interpretability in this way, and his exploration of the intersection of proofs and modern machine learning.\n\nTopics we discuss:\n\n*   [Why compact proofs](#why-compact-proofs)\n*   [Compact Proofs of Model Performance via Mechanistic Interpretability](#cpompvmi)\n*   [What compact proofs look like](#what-compact-proofs-look-like)\n*   [Structureless noise, and why proofs](#structureless-noise-why-proofs)\n*   [What we’ve learned about compact proofs in general](#what-weve-learned-in-general)\n*   [Generalizing ‘symmetry’](#generalizing-symmetry)\n*   [Grading mechanistic interpretability](#grading-mechinterp)\n*   [What helps compact proofs](#what-helps-compact-proofs)\n*   [The limits of compact proofs](#limits-compact-proofs)\n*   [Guaranteed safe AI, and AI for guaranteed safety](#gsai)\n*   [Jason and Rajashree’s start-up](#jason-rajashree-start-up)\n*   [Following Jason’s work](#following-jasons-work)\n\n**Daniel Filan** (00:00:09): Hello everybody. In this episode I’ll be speaking with Jason Gross. Jason is a researcher interested in mechanistic interpretability and software verification. During his PhD, he was on the team responsible for verifying the code used in encryption for the HTTPS protocol and he’s also on the development team of the Coq Proof Assistant. Links to what we’re discussing are in the description and you can read a transcript at axrp.net. You can also support the podcast at patreon.com/axrpodcast.\n\n(00:00:35): Well Jason, welcome to the podcast.\n\n**Jason Gross** (00:00:37): Thank you. I am excited to be here.\n\nWhy compact proofs\n------------------\n\n**Daniel Filan** (00:00:40): Cool. So I guess we’re going to talk about this line of papers that starts with this paper [“Compact Proofs of Model Performance via Mechanistic Interpretability”](https://arxiv.org/abs/2406.11779). You’re the lead author and then there’s a bunch of other authors that I don’t want to read on air, but can you give us a sense of just what’s the idea here? What’s the theme?\n\n**Jason Gross** (00:01:02): Okay. You can think of doing mech interp as compressing explanations of large models. And the takeaway from this paper is that you can use proofs to measure how much compression you get. That’s the super short version of it.\n\n**Daniel Filan** (00:01:20): This gets to this question I have about reading this paper, which is: there are a few things I could imagine you going for, right? One way of thinking about it is: we want to have a benchmark for how good a mechanistic interpretation is and our benchmark is, we’ll turn it into a proof and see some combination of how good a bound we can get and how short the proof is.\n\n(00:01:45): I think another way I could be thinking about your project is: it would be nice if we had more proofs of stuff going on in neural networks, and mechanistic interpretability is one useful way in which we can get these proofs. And the way you said it just then, it sounded more like the first one: here’s a new nice metric of how good your mechanistic explanation is. But I don’t know, it feels kind of surprising for that to be the explanation. On some level, I’m like: you have this background in doing proofy stuff, it seems like you like having proofs of things, lots of people like having proofs of things. So am I wrong here that a lot of it is the second part as well?\n\n**Jason Gross** (00:02:33): It started as the second part and then it sort of morphed into the first thing.\n\n**Daniel Filan** (00:02:38): So why the change?\n\n**Jason Gross** (00:02:40): It’s very hard to get proofs about things. And if you look at what are the takeaways right now for people practicing mech interp, they’re more of the first kind: how can we ground our sense of what mechanistic explanations are? How can we use these insights? How can we use the takeaways from this frame to say where we should focus our attention? And we can already do that even if we’re not necessarily trying to get proofs yet of GPT-4 or very large models. We can already take away insights that are things like: the hardest parts of the network to compress seem to be the non-linearities. So if we want to drill down into what parts need the most attention and the most understanding, to understand them we should be looking at how do the non-linearities perform their function. I don’t know if that quite answers the question you have.\n\n**Daniel Filan** (00:03:38): I guess it kind of does, yeah. So to the extent that the message is, “it would be nice to get proofs, but it’s really, really hard to get proofs”, one could think either (a), we should try and make it much easier to get proofs. So there’s some mechanistic interpretability stuff that, I mean as you note in the paper, it really does make it much easier to make proofs in the state of the art-\n\n**Jason Gross** (00:04:08): I want to interrupt. I think it’s more than that. I think what we find from the paper is that it is necessary to have understanding in order to get reasonable proofs; that finding reasonably-sized proofs in some sense is understanding-complete.\n\n**Daniel Filan** (00:04:20): Yeah, yeah, I think that’s right. And if you look at the previous state of the art, my understanding is it’s just basically interval propagation of “if you change the input a little bit, how much does the output change?” And you do some neat little tricks, but it’s like-\n\n**Jason Gross** (00:04:34): Interval propagation and case analysis.\n\n**Daniel Filan** (00:04:36): Yeah. I mean it’s great that they did it, but on some level it’s sort of like, what are we even doing here? So one takeaway could be: mechanistic understanding made it some amount easier to come up with proofs. You could imagine saying, “Oh, we figured out that the difficulties in finding… It was still kind of hard and our lives would be easier if we solved sub-problems, X, Y and Z.” Do you have thoughts on, are there sub-problems such that if we solved them we could do it? Or does it just seem kind of hopeless?\n\n**Jason Gross** (00:05:14): There are definitely problems that we need to solve in order to be able to do it. I think the biggest ones are the ones that deal with randomness. So the easiest version of this problem is that you can have numbers that average out to zero, more or less, where you have a bunch of small noise that’s not really doing anything in the network, and establishing that all these things that aren’t doing anything are in fact not doing anything is quite challenging from a proofs perspective when you want to hit the worst case. So there’s currently some projects that I’m advising on this that are looking at: can we fine-tune the networks to suppress this to the point that we get networks that we could prove things about?\n\n(00:05:58): And there are a couple steps after “suppress the noise that isn’t doing anything”. The next one is the noise that arises from other circuits that are doing important things: you can’t just suppress them all to zero because they’re doing something important elsewhere, but they’re not doing something important here. And for that, I think you need something like derandomization techniques, where the simplest example of this is that if you have a collection of random vectors in high dimensions, they’re almost always almost orthogonal. But if you want to establish cheaply how close to orthogonal they are, this is expensive in that you have to take them pairwise and take all the dot products. And if you want to do it faster than this, unless you manage to solve open problems in theoretical computer science, you effectively need to, instead of selecting them randomly, select them in some systematic way so that you can bound how non-overlapping they are or how overlapping they might be.\n\n(00:06:59): And I imagine that there are similar examples beyond that where we somehow need to take things that happen to work on average and see if we can shuffle the network around so that they work systematically. But in theory, I haven’t hit anything yet that seems like a fundamental total obstacle to scaling proofs up.\n\nCompact Proofs of Model Performance via Mechanistic Interpretability\n--------------------------------------------------------------------\n\n**Daniel Filan** (00:07:25): So, \\[in\\] [“Compact Proofs of Model Performance via Mechanistic Interpretability”](https://arxiv.org/abs/2406.11779), what’s the setting? What do you actually do in this paper?\n\n**Jason Gross** (00:07:32): I want to start with: what is a proof, in this context? So when you’re doing mechanistic interpretability, you have some claim about how the model behaves, and how the model actually behaves might be a bit different than this, but if your understanding is any good, it’ll be pretty close. And your theorem statement is bounding the divergence between your claim about how the model behaves and how the model actually behaves. And then you prove this, and the measure of compression is how long your proof is. And a technical note: it needs to be in some first-order system or alternatively, you need to measure proof checking time as opposed to proof length. And your measure of faithfulness or accuracy is how tight the bound is.\n\n(00:08:22): And there’s a baseline that you can always do, which is just running model inference, writing down all of the floating point operations, all the matmuls where you’re like, “This is my dataset of interest, let me just run inference on every single data point and compute exactly what the accuracy has got, exactly what the loss is” or whatever. And this is very expensive. You can also do something at the other end where you’re like, “Well, without running the model at all, accuracy is at least 0%, by definition.” And by running it on half the data points, you can get 50% accuracy and you can linearly interpolate basically between these two extremes.\n\n**Daniel Filan** (00:08:54): Right, and so, as you go across that line, the proof “length”, aka time to generate this confidence, linearly grows, and also the accuracy bound linearly grows.\n\n**Jason Gross** (00:09:09): That’s right.\n\n**Daniel Filan** (00:09:09): Assuming the network is in fact perfectly accurate.\n\n**Jason Gross** (00:09:12): So you don’t need to assume that. You can grow it linearly up to the true accuracy by selecting the examples on which it’s accurate and then you saturate at the true accuracy and you can’t do anything better than that.\n\n**Daniel Filan** (00:09:22): Right. Right. So that’s kind of a dumb way you could come up with these proofs.\n\n**Jason Gross** (00:09:24): Yeah. So this is the baseline. This is the baseline of something like “no understanding in the proof”. The hypothesis that we’re exploring in the paper is that if you add understanding, if you add mechanistic interpretability, you can do better than this baseline. You can get shorter proofs that have better accuracy. And we explore this hypothesis on, in some sense the simplest transformers that you can imagine. One-layer, attention-only, one attention head, no layer norm, taking the max of k one-hot encoded numbers.\n\n**Daniel Filan** (00:09:54): Okay, so max of k one-hot encoded numbers basically just means: the neural network gets four numbers as input encoded in a somewhat standard way and it has to tell you what the largest of those numbers is and that’s it. And it’s a small transformer and there’s not that much going on, and you train a transformer that in fact works at this, and I guess your job is: how can you prove that it actually works at this without just running it on all of the examples? Maybe to motivate what’s going on: what’s wrong with just running it on all the examples?\n\n**Jason Gross** (00:10:33): So it works fine for max of four. It takes something like a minute or minute and a half to run on all the examples. If you wanted to do max of 20 instead, it would take something roughly analogous to the training time of GPT-4. And you can imagine if you wanted to run GPT-4, [o1](https://openai.com/o1/) or something on all of the texts that it would ever see, this is obscenely expensive and you don’t want to do this. And moreover, every time you increase the distribution over which you want to handle inputs, it gets more expensive and it gets drastically more expensive. You add one token and now you need to multiply by vocab size if you’re considering all possible sequences. You multiply the previous proof length by vocab size, and so you’re exponential in context window.\n\n**Daniel Filan** (00:11:28): So the way I understand this is: okay, one way you could have a sense of how good GPT-4 is, is you just randomly pick some inputs that it might deal with and you see how it does on those inputs. And if it does well on basically all of them, you say, “Okay, GPT-4 is doing well.” And it strikes me that the reason you might care about proofs of performance is if you think there’s some subset that’s hard to find where it’ll behave really badly on or something. Does that seem right to you?\n\n**Jason Gross** (00:12:00): Yeah, I think there’s two reasons. I think from the, “We want proofs,” that’s definitely the thing that’s like: you want to handle all the edge cases. I think there’s also the other reason that’s like: we might care about proofs if we have some very powerful optimization system and we want to give it a solid target that it can’t [Goodhart](https://en.wikipedia.org/wiki/Goodhart%27s_law) against and be able to extract understanding from whatever it produces.\n\n**Daniel Filan** (00:12:21): And not Goodhart against… you mean not optimize in a sort of cheating way that didn’t really give you what you wanted?\n\n**Jason Gross** (00:12:27): Right. Like, [the first paper that OpenAI published on GPT interprets GPT](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html). Cool work, great labeling all the neurons. How much do we trust that that’s actually what the neurons do? How helpful is it to get these results? It’s kind of hard to answer that. Whereas if it produces a proof of the performance of the model, we can grade how deep an understanding it has based on how short the proof is and how tight the bound is.\n\n**Daniel Filan** (00:12:57): So, in this setting where you just want to automate doing interpretability and you want to find a target that isn’t fake, it makes sense to me that proofs of accuracy bounds are going to make sense. In the case where you’re worried about a small probability of really bad events, I guess maybe this is a technical note, but it seems like overall accuracy rate isn’t going to be a thing you’re super worried about, right? You’re going to be worried about worst case or some sort of loss function where you care way more about really bad failures than normal things. Does that seem right?\n\n**Jason Gross** (00:13:32): The way that we set up the framework, or at least the way that we’re going about doing proofs, it seems like it is very easy to shift out the distribution, and pick a different distribution and get a proof, because the way that we do it is you break the input dataset up into different cases of different inputs, and then you bound the worst case loss or accuracy in each of these collections of inputs, and then according to the distribution weights, you combine them in some way. And so if you wanted to say “there does not exist an input”, or you wanted to say “there’s at most n inputs”, or you wanted to say that you weight this part of the input very strongly, this is very easy to do without changing the proof strategy at all. You just might get much worse bounds.\n\nWhat compact proofs look like\n-----------------------------\n\n**Daniel Filan** (00:14:19): You’re doing max of n on a transformer and you’re writing proofs. What-\n\n**Jason Gross** (00:14:24): What does that mean?\n\n**Daniel Filan** (00:14:24): Yeah, what does that mean? Are you writing down on paper? Are they Python programs? What are these?\n\n**Jason Gross** (00:14:29): Yeah, so we split the proofs into two parts. One of them is a function that we write in PyTorch or that we could write it in some proof assistant that takes in model weights and produces a bound. It says, “I claim that for this model here is my bound on accuracy or on loss.” And then currently on pen and paper, although it could be done in a proof assistant, we say, “For any model weights, whatever you put in, the bound is valid.”\n\n**Daniel Filan** (00:14:52): Okay. That’s what the proofs look like at a very high level. Can you tell us: in this case of max of n, roughly, what are you doing to get different kinds of proofs?\n\n**Jason Gross** (00:15:03): So there’s two ways I could go about answering this. One of them is: what do the proofs look like? And the other is: how did I go about finding the proofs?\n\n**Daniel Filan** (00:15:12): Yeah, maybe let’s start with how did you go about finding the proofs?\n\n**Jason Gross** (00:15:16): I set a proof length that I wanted to hit where I’m like, I want this proof to be at most cubic in the vocabulary size, or I want it to be at most quadratic in the vocabulary size or something like that. And this gives me a computation budget for what sorts of cases I can split the inputs up into. And then I look at each part of the network and I’m like, “How much can I do within this computation budget that captures as much of the variation as possible while still staying within the computation budget?” So if I am doing something that is cubic in vocabulary size, I might be like, “Okay, in the end I’m going to split the inputs into cases that are based on the query token, the maximum token in the sequence and the largest non-maximum token.” Or I might split it based on the query token, the maximum token and which output token I’m considering. And by doing different case analyses at different points, I can use this computation budget differently. And for this network, it is always the case that different maximum tokens are considered different cases, and in some sense that’s an artifact of the maximum token being one-hot encoded. And so naively there’s no relationship between different maximum tokens and you have to consider them all as different cases.\n\n**Daniel Filan** (00:16:39): Maybe I’m more curious about what do these proofs look like? How do they interact with doing mechanistic interpretability? The thing you just said, it sounds like-\n\n**Jason Gross** (00:16:46): Very, very different.\n\n**Daniel Filan** (00:16:47): Well, it sounds like you could say that about any software system. Where does the mechanistic interpretability come in?\n\n**Jason Gross** (00:16:57): Yeah, I want to take it a little bit far afield from what was actually in the paper. I’m pretty excited about the current project that I’m running where we’re looking at crosscoders and making crosscoders into proofs.\n\n**Daniel Filan** (00:17:09): Okay, what’s a crosscoder?\n\n**Jason Gross** (00:17:11): You take the residual stream vectors at all the different layers and you stack them, you concatenate them into one long vector, and then you train a sparse autoencoder on this concatenated vector. And the sparse autoencoder is just a linear transformation followed by ReLU, followed by another linear transformation that’s supposed to take in this concatenated vector and reproduce it as close as possible.\n\n**Daniel Filan** (00:17:33): Yep. And my understanding of the sparse autoencoders is: they’re “autoencoders” in that they encode the input itself, you run the input in, you get itself out ideally, and they’re “sparse” in the sense that they have this… in the middle, you take this vector and then you blow it up to something really big, but very few entries are activating. And so the thought is in the middle of the sparse autoencoder, you have just a bunch of things you might be tempted to call “features” or “concepts” or whatever, and those correspond to things you might interpret and care about. And by doing an autoencoder this way, you’re basically saying, “Ah, yeah, these linear combinations of things in the stacked residual layers, these things correspond to this concept that I’ve found.” Is that roughly what’s happening here?\n\n**Jason Gross** (00:18:26): Yes.\n\n**Daniel Filan** (00:18:26): Okay. And so crosscoder just refers to sparse autoencoder on the stacked layers of-\n\n**Jason Gross** (00:18:33): That’s right. The “cross” means that different layers get to interact.\n\n**Daniel Filan** (00:18:37): Right, right. So that’s what a crosscoder is. And you are doing compact proofs with them, right?\n\n**Jason Gross** (00:18:42): Yes. So what that looks like: I was really excited when I saw the crosscoders because I was like, you can actually get really close to a proof, basically fully automatically, just with what comes out of the crosscoder. And bear with me while I say what this looks like, because it’ll be a little bit counterintuitive. The first thing I need to mention is that there’s two different time lengths that you might mention, which are “how long it takes to check the proof” and “how long it takes to find the proof”. And when I talk about compact proofs, what I mean are proofs that are quick to check, not proofs that are easy to find. We normally don’t count the time that you spend studying the network. And so in the same way, we don’t count the time that is spent training the crosscoder, which also means that if we include a complicated string in the proof, we don’t have to count how hard it is to find that string.\n\n(00:19:38): And for the crosscoder, the proof needs to include the encoded dataset. So you take your whole dataset, you run every single data point through your model, and you record all the activations. This is extremely expensive, but you could do a sampling-based probabilistic version to make it cheaper.\n\n**Daniel Filan** (00:19:52): Okay. This is to train the crosscoder?\n\n**Jason Gross** (00:19:53): This is to train the crosscoder and to find the proof. You need to encode every single data point in the fully-trained crosscoder and associate to each data point, “Here is the sequence of features on each token in this bit of input, and here’s how strongly each feature activated at each token”. This is the encoded or latent dataset, and to make a proof, the first thing that we do is we say, “What does the crosscoder claim the loss or the accuracy of the model is?” And to do this, we can just decode this dataset to the last layer and do unembed. And so instead of having to run the full model on every data point, we’ve now removed all the layers before the unembed and we can just go straight to the end.\n\n**Daniel Filan** (00:20:44): Right. So basically we’re operating under the hypothesis that the crosscoder is perfect and just perfectly represents the actual thing going on.\n\n**Jason Gross** (00:20:52): That’s right. And then to make it a proof, we need to bound how imperfect the crosscoder is. And there are two parts to this. The first is how far off are the crosscoder feature encoding… How far off is that from the underlying dataset? And for this, you decode to the first layer and you embed your original dataset, and then you measure how far away are these vectors. And the thing you get from this is that the original dataset is in some epsilon-ball of your encoded dataset. And then the thing that you need to do is you need to say… what you need to prove somehow is if my actual data point is within an epsilon-ball of what I’m claiming the features are, what does that mean about what the output is? And there’s two parts to this. One of them is just the epsilon-ball propagation, that you need to propagate this interval through the network, see how much error is introduced-\n\n**Daniel Filan** (00:21:46): Right, we’re going back to that style of-\n\n**Jason Gross** (00:21:48): Yeah, you still need to do something like that. The other part is the really exciting part because it has applications to what other people are doing in mech interp, which is that if you were just doing this epsilon-ball propagation, you’d still have to do it for every data point separately, which saves you nothing because you still need to propagate every data point through the network. So you need to make some additional assumption, which looks like if you’re like, “I did a crosscoder, I think this is all I need, this is a ‘complete’ explanation of the network”, whatever that means, then implicitly you’re assuming something like the features interact only linearly. If I have a data point that’s a sum of features, there’s nothing interesting going on beyond the linear sum. And what this caches out as is: you should be able to run the features through the network separately and then somehow combine the bounds that you get on how much error is introduced from different features in some vaguely linear way.\n\n**Daniel Filan** (00:22:48): And by the “features” you mean these hidden units in the crosscoder?\n\n**Jason Gross** (00:22:52): Yeah. The latents of the crosscoder.\n\n**Daniel Filan** (00:22:54): Okay. And so basically the reason that saves is just something like: there are fewer latents in the crosscoder than there are data points-\n\n**Jason Gross** (00:23:02): That’s right. That’s right.\n\n**Daniel Filan** (00:23:02): And so you just do a per latent vector thing rather than a per data point thing.\n\n**Jason Gross** (00:23:08): That’s right. And to actually get a proof, you need to establish that this linear interaction hypothesis holds. So you need to bound the interactions between different features in the crosscoder, different latents. And so this ends up looking something like number of features squared times inference cost, times forward pass.\n\n**Daniel Filan** (00:23:31): If I’ve got this collection of features… So you’re propagating each one of them through the network and then you want to check that each pair of them doesn’t interact. Why don’t I have to check that each triple doesn’t interact in a weird way, or that there’s not some set of five that causes some weird reaction? Why do I only need the pairs?\n\n**Jason Gross** (00:23:52): The bigger the set that you consider, the tighter bound you can get. Triangle… you could in theory not do the pairs at all and just add up the errors from each feature separately. But we should expect this to give horrendous errors in some sense. So the way that I want to think about this is that you might imagine that there’s some feature that’s strongest or dominating at each point in the network. And you can ask: how much does each other feature contribute relative to the dominant one? And this gives you a sort of pairwise interaction that you can use the triangle inequality to sum up over other features. And if they’re actually not interacting at any point, if one neuron only listens to one feature out of all the ones that co-occur at a given data point, then all of these contributions should be pretty small.\n\n(00:24:43): So I think if instead you wanted to have something that was linear instead of quadratic, you would basically have to say that at each data point, you treat every single feature as if it’s the strongest. And then you ask, how much does this contribute? And then you have to consider them all sort of as error terms on each other. And so if your sparsity is something like 100 or something, this means you have 99 error terms that are as large as they are when those are the strongest features that you’re adding to your single feature that you’re looking at. And you sort of can’t even talk about what is the strongest feature that this neuron in the network listens to.\n\n**Daniel Filan** (00:25:28): Okay. So is it something like: the reason you’re only looking at the pairwise things is somehow the pairwise errors are just subsuming the three-way errors and the four-way errors, et cetera?\n\n**Jason Gross** (00:25:42): Maybe a better way to describe it is that you can pick how many way interaction you want to look at. Pairwise lets you talk about the strongest feature that’s active, which seems like something that should probably be important that we want to consider if we want to get anything sensible. We don’t want something that doesn’t even let us say one feature is the most active feature in a place. At the same time, it’s not clear why we should expect by default that we’ll need more than this everywhere. Plausibly we’ll need more than this in plenty of places, but naively, there’s nothing obviously sensible to do with more features, or it’s clear why not being able to identify which features are active on which data points most strongly isn’t enough to get anything sensible. It’s not clear why saying there’s one strongest feature and that’s the one that is relevant at any point in the network… why that wouldn’t be enough for many points in the network.\n\n(00:26:47): And so what this gives us is an interaction metric where we can measure how bad this hypothesis is. And if we notice that in some places there’s three interacting features, then we can consider doing the triplets at that place. And if there’s four interaction features, we could consider doing the quadruplets and each of these increases the length of the proof. The way that I’m looking at it is we’re sort of trying to pick a vaguely sensible default to measure and locate where the violations of it are, where we need to put in more mech interp work in order to get better compression.\n\n**Daniel Filan** (00:27:18): Okay. So zooming out, you’re telling me about the proof strategy for crosscoders.\n\n**Jason Gross** (00:27:21): That’s right.\n\n**Daniel Filan** (00:27:22): Which was something like: you treat the crosscoder as if it’s a good model of the network. And then you want to find out, okay, to what degree is it not a good model of the network? And then instead of doing this data point by data point, you do it feature by feature, and then you check if there are weird interactions between features that prevent you from doing it that way, that prevent the feature-by-feature thing from being representative of the data-point-by-data-point thing. Is that a fine, very brief summary of this? I mean, I guess this is active research, so I suppose things could evolve by the time this podcast goes out.\n\n**Jason Gross** (00:27:59): Yeah, I think that’s a pretty good summary of the theoretical approach. I want to throw one more little bit in there: connection to physics. The waves and vibrations course that I took, there’s this frame that says everything is a quadratic, everything is a simple harmonic oscillator. Where you’re like, okay, there’s the constant term, there’s the linear term, which you adjust for by frame of reference, and then the leading order term after that in the Taylor expansion is going to be the quadratic. And so you analyze the quadratics and you throw away all the high order things. And there’s a sense in which we’re doing the same sort of thing here, where we’re saying the crosscoder is the leading order description of what’s happening in the network, and let’s just take the first leading order term after the crosscoder and see what’s going on there, what the quadratic or pairwise interactions are.\n\n**Daniel Filan** (00:28:50): If people remember my [singular learning theory](https://axrp.net/episode/2024/05/07/episode-31-singular-learning-theory-dan-murfet.html) [episodes](https://axrp.net/episode/2024/11/27/38_2-jesse-hoogland-singular-learning-theory.html), they’ll get mad at you for saying that quadratics are all there is, but it’s a decent approximation.\n\n(00:28:56): Anyway, that esoteric remark aside, all of this was getting at a sense of how these proofs are working in max of k. And I guess it’s something similar where you have some story of what’s going on with the network and you have some sort of proof that goes roughly like, “Okay, we’re going to analyze how good the network can be, if this story is accurate, and then we’re going to bound how different the real network is to this story.” Is that roughly fair? And then you talk about different strategies, and I guess those correspond to stories with more or less amounts of detail?\n\n**Jason Gross** (00:29:38): Or potentially completely different stories. You can try different stories with the same amount of detail that say different things about the network and see which one is better.\n\n**Daniel Filan** (00:29:46): So I don’t know, we could go into a lot of detail about just what those stories are, but honestly, I think people should just read the paper if they’re super curious.\n\n**Jason Gross** (00:29:54): I can highlight one or two things about those stories that I think might be more broadly interesting. One of them is about the level of detail that goes into different lengths of proofs, where at least for the small networks, you need surprisingly little detail before you start being able to break the exponential in context window and get shorter proofs. So for example, the only insight that you need for the first step down is that the network gets the right answer by paying attention to a single token. And it happens that that token is the maximum token, but all you need to know is that it gets the right answer by paying attention to a single token and everything else is an error term.\n\n**Daniel Filan** (00:30:36): And that gets you from this sort of exponential dependence on all the inputs to-\n\n**Jason Gross** (00:30:41): Cubic.\n\n**Daniel Filan** (00:30:41): To cubic. Yeah.\n\n**Jason Gross** (00:30:43): To d vocab cubed.\n\n**Daniel Filan** (00:30:44): Right. Which is pretty good. Okay, so I guess that gives a flavor for what’s going on. I want to get back to a thing you said earlier, which was that I was asking about compact proofs. If we’re really worried about models rarely doing very bad things, then we’re going to have to look at something other than just average performance on a simple scale. And you mentioned that, okay, the way these proofs are shaping out, it seems like it’s not too hard to pay close attention to some really bad cases or something. And I’m wondering, to what extent is that just an artifact of the way things happened for max of k, which obviously is kind of… It’s probably not what all networks look like, right? Versus just a general thing that you’re observing again and again that has some kind of deepish reason behind it?\n\n**Jason Gross** (00:31:41): It seems pretty general. I don’t know what the deepest reason is, but there’s a sense in which this is an artifact of \\[the fact\\] that in some sense the first thing you do in a proof is you do case analysis. And this is true in crosscoders, where each feature in some sense corresponds to a case. This is true in SAEs, this is true in max of k. This is true in the interval bound propagations, where every time you hit a non-linearity or value or something, you break things into cases. And so, anytime you do case analysis, you can choose to weight the cases however you want.\n\n**Daniel Filan** (00:32:17): And I wonder if the magic here is just coming from: the network doesn’t treat every input totally differently, such that you can have some case analysis, but there’s just a lot of unifying processing behind a bunch of things such that you have way fewer cases to think about than you have inputs.\n\n**Jason Gross** (00:32:32): I think insofar as we expect networks to generalize from their inputs at all, and insofar as we expect to be able to compact the proofs at all, it’s because they don’t treat every input specially.\n\nStructureless noise, and why proofs\n-----------------------------------\n\n**Daniel Filan** (00:32:43): All right, I next want to ask about this issue of noise. Especially in the original paper, you talk a lot about the difficulty of what you call “structureless noise” for finding proofs. Now that we have a little bit of a sense of just what’s going on with compact proofs, what is structureless noise? Where does it show up? How does it happen?\n\n**Jason Gross** (00:33:06): I want to use the example from max of k, where the way the network works is that it pays more attention to larger tokens and it copies whatever it’s paying attention to. And if you look at the QK attention matrix - the query key attention matrix part of the transformer - it turns out that it’s approximately rank one, in that there’s one direction that I’ve been calling the “size” direction where you just embed more or less linearly, although nothing in any of the proofs uses \\[the fact\\] that it’s linear in this direction in terms of how big the input is. You can read off the size of the input token just by projecting in this one direction. And then there’s also a query direction where every token is embedded roughly uniformly, and so you dot the query direction and the size direction after lining them up through QK. And this is how the network pays more attention to bigger tokens. Sorry, I’ve forgotten what your question was in-\n\n**Daniel Filan** (00:34:13): What’s going on with structureless noise? What is it? What does it look like?\n\n**Jason Gross** (00:34:17): Okay, so I said that this QK attention circuit is approximately rank one, and you can subtract off the rank one part and this will perform… If you replace the QK circuit with its rank one part, I think this improves the performance of the network. So by standard measures, the rest of it isn’t doing anything, but it’s still the case that the rest of it is not literally zero. The product of matrices is not literally rank one. And if you-\n\n**Daniel Filan** (00:34:43): So, the product of matrices, like-\n\n**Jason Gross** (00:34:45): So, you have embed, query, key, embed.\n\n**Daniel Filan** (00:34:48): Okay. And when you talk about the QK matrix, you mean the product of the query matrix and the key matrix?\n\n**Jason Gross** (00:34:53): I’ve been a little bit ambiguous about whether I mean that product or whether I mean that product putting the embeds on each side, but yes.\n\n**Daniel Filan** (00:34:58): Okay.\n\n**Jason Gross** (00:34:59): And so, if you take the product of the four matrices, let’s say, which in the paper I think is E, Q, K, E, this is approximately rank one. You get a more accurate result if you consider it as rank two, because there’s both the size direction and the query direction, so you can make it even more structureless by pulling off the first two ranks. You pull this out of all four matrices and what you’re left with looks roughly random. There’s a little bit of structure but not very much. And in order to get a proof that is linear in the parameter count of the network, which is potentially the best shortest proof that you could go for, you need to avoid multiplying out these matrices, and you need to establish that if you were to multiply out these matrices, it doesn’t affect attention that much.\n\n(00:35:51): And this is what I mean by structureless noise. You have these random numbers that you’re doing some operation on. The thing that actually happens is that they stay pretty small. The thing you want to establish is that they stay pretty small and you want to do this without considering every case by brute force.\n\n**Daniel Filan** (00:36:09): A thing that I’m missing is: what’s wrong with multiplying the matrices out? Because okay, if I’m imagining we’re doing this as a test case for thinking about some super big network. The network’s super big, but I would imagine multiplying the matrices is just not that bad compared to looking at every input. Am I wrong here? I guess matrix multiplication, it’s not literally n cubed, but it’s almost n cubed. Maybe that’s really bad in a way that I’m not appreciating.\n\n**Jason Gross** (00:36:41): You’re not wrong. I think you’re right that multiplying out the matrices isn’t that bad. Even in large networks where you end up having to do something like d vocab squared times d model and d vocab might be 50,000, if you insert non-linearities in the middle, things become much more complicated, because now you don’t just have to multiply out the matrices, you have to consider all the different ways that the non-linearities might interact.\n\n**Daniel Filan** (00:37:05): And so, it basically becomes equivalent to just doing all the forward passes because-\n\n**Jason Gross** (00:37:09): That’s right.\n\n**Daniel Filan** (00:37:10): Okay, so maybe this gets to a question I have, which is… So, there’s a sense of, you’re going for compact proofs and you could imagine a few other things one could do. So, there’s compact IID statistical guarantees, which is, you just sample some inputs and get the output, and you can have bounds on accuracy if you’re not too worried about worst case stuff. You could also do compact probabilistic proofs, where what I’m somehow imagining is, you have at least in your head this product of these four matrices and subtracting off the rank one or rank two parts, you want to know that once you multiply all these numbers, all these small numbers stay small and they don’t become big in some way.\n\n(00:38:11): One thing I can imagine doing is saying, okay, the product of these matrices, it has… There are N numbers here, and N is way too many for me to want to compute, but I could compute square root N of them. I could just take some vectors in the matrix and multiply them out, and I can get some of the elements of this product and it’s much less bad than getting the whole product. And if I randomly select them, and if I find that out of the ones I randomly selected, all of them are small, then I might hope that I should be able to get some probabilistic bound where if I was really choosing randomly, then I can get a sample mean and a sample standard deviation. And I can know that unless my randomization went really bad, things are fine. I’m wondering, do you think that approach… why not do that, basically?\n\n**Jason Gross** (00:39:13): Yeah, I think that’s a great approach and I’ve been looking for someone to work with on doing that. I started with proofs because I understand them better, and there’s a sense in which the background is more solid on them. There’s this long history of mathematical theory about what counts as a proof, what doesn’t count as a proof, how to combine them. You don’t need to deal with assuming independence between different procedures. You don’t need to deal much with randomness. They’re a lot easier in some sense. I think the thing that you described, I’m very interested in seeing empirically what Pareto frontier do we get? What scaling do we get? What is the trade-off between how many points we sample and how long we make the other bits of the proof and how tight the bounds that we get are?\n\n(00:40:04): I think this could be very promising. I think this looks a lot like what [ARC theory](https://www.alignment.org/theory/) is doing with [heuristic](https://arxiv.org/pdf/2211.06738) [arguments](https://www.alignment.org/blog/formal-verification-heuristic-explanations-and-surprise-accounting/). My personal take on heuristic arguments, which is not at all the take of ARC theory, is that you can look at it as doing proofs on the structured parts of the network and then doing default random or default heuristic arguments, some sort of probabilistic sampling-based thing on establishing bounds on the parts that you don’t manage to prove but are in some sense structureless or random.\n\n**Daniel Filan** (00:40:42): Right. Yeah, I remember talking to… There’s a previous episode with [Mark Xu](https://axrp.net/episode/2023/07/27/episode-23-mechanistic-anomaly-detection-mark-xu.html), and I guess people can listen to that themselves. I was chatting with him, \\[asking\\] why not just do maxent or why not sampling? And he’s like, “Ah, it wouldn’t work for some reasons which…” Well, why not maxent? Because it’s very hard to compute a maximum entropy distribution. Why not randomly sample parts of your network and check there? I don’t quite remember, but maybe my question is: naively, if I don’t think very hard about what it would actually involve to do this, check a small number of elements of this matrix and see how big they are. In my head I’m like, “Well, how hard can it be?” You’re just like, I know how to compute one element of a matrix product, so I could just do that 50 times or something. Am I missing something about how hard it would be?\n\n**Jason Gross** (00:41:39): Some of the matrices are shared between different paths through the network. And do we assume that when you’re sampling from the matrices, are the matrices independent or are you sampling independently for each of these paths? Are you sharing samples between establishing how these paths work?\n\n**Daniel Filan** (00:41:53): Yeah, so a somewhat bad way you could do it would be to just assume that the actual entries of the matrix are random or maxent or something. I think that’s probably bad if you’re worried that the weights are somehow chosen adversarially or something. But if you randomly pick, suppose the final matrix is 8 by 16, or 8 by 8, whatever, and you’re like… Or actually, let’s suppose that it’s 6 by 6 for simplicity, and you randomly roll two dice and it comes up like 3 and 4. And you’re like, “Okay, I want row 3 column 4”. And so, you just figure out the bits of the matrices you have to dot product together to get row 3 column 4 of the final thing. And maybe the issue is just, if you have four matrices that you’re multiplying together, you have to fully multiply the middle two to get row 3 column 4 of the final one. But yeah, I’m imagining that the randomness is in you randomly pick what you look at rather than you’re assuming random distribution of the things.\n\n**Jason Gross** (00:42:56): I think that might work. I haven’t spent the time to fully push through the details. You still need some way if you get these numbers for multiple different paths through the network and then you want to combine them, and you want to make some guess about the output. I could totally believe that just the most naive thing would work here. I just haven’t put in the time to chug through the details and see what bounds you get out by doing this.\n\n**Daniel Filan** (00:43:25): Okay. Well I guess, maybe this is one of these times when listeners who like concentration inequalities or something can maybe push stuff forward here.\n\n**Jason Gross** (00:43:35): I want to flag one more related thing that I’ve been thinking about for the past couple of minutes: that one of the subtleties that comes out of looking at compact proofs is that it matters what thing you’re trying to compress. So, here I’m saying, we might see something different potentially if we’re trying to compress a proof versus if we’re trying to compress this probabilistic computation.\n\n(00:43:56): Another interesting subtlety is that the thing that we’re compressing is the proof and not the network itself. That is, we’re compressing the computational trace of running the network on every single data point, as opposed to just finding the shortest description length of the network itself. I think this is important and gives you a different sort of intuition about what the thing is that you’re doing and the way in which mechanistic interpretability is compression.\n\n**Daniel Filan** (00:44:27): In what way is it different? Because it seems like in the compact proofs that we’ve described so far… so the crosscoders proof is basically you train a crosscoder and in some sense it’s much smaller than the network and your proof goes, let’s assume the crosscoder is the network and then let’s figure out the error term. And in the max of k thing you’re like, let’s assume this rank one or this rank two thing is the whole network, and then let’s figure out the error term. And of course, a rank one matrix for those who don’t know, it’s much more compressed than a big rank matrix, which is the generic case. So, how is it different compressing the proof versus compressing the model? Because it seems like you’re mostly compressing the model.\n\n**Jason Gross** (00:45:09): Yeah, so you say that the crosscoder is smaller, but there’s some sense in which your number of features is a lot bigger than your hidden dimension. And you could imagine a transcoder-flavored thing where you take your network, you blow up the hidden dimension, and then you just train it sparsely. And this is a decompression of the network, but if you manage sparsity in the right way, it should still allow a compression of the computational trace because on each data point you have a lot less work to do even if the full size of your model is much larger.\n\n**Daniel Filan** (00:45:43): I forgot how big sparse autoencoders are. And I guess this gets to the point about compressing the proof length versus finding the proof. Just because if you actually think about theoretically how hard it should be to train sparse autoencoders, it’s very hard. Or you have this thing that’s a comparable number of parameters to the base network. You might think that you need a comparable number of data points as the base network. And now apparently that’s not true. Apparently you can train them on comparatively less and that’s why it’s much easier to train a SAE on some big model than it was to train the big model, but still.\n\n**Jason Gross** (00:46:22): I would naively guess, and I am speaking without that much experience here, but I would naively guess that that’s about how deep they are as opposed to how difficult the thing is to learn. In some sense we’re training a very wide, shallow network, and you might imagine that this requires fewer data points to get comparable training loss because you have so many parameters than if you wanted a more compact network.\n\n**Daniel Filan** (00:46:51): So, if it were just that training a given number of parameters \\[was\\] easier when the parameters were wide and shallow rather than deep, then you would think that when they train GPT-4, they would just have a one layer, very wide neural network. So I think it’s-\n\n**Jason Gross** (00:47:05): I think you’re totally right on that.\n\n**Daniel Filan** (00:47:06): I think it’s got to be \\[inaudible\\]. Okay, sorry, we’re going to get on my soapbox. For some reason everyone acts like SAEs are just normal and fine. And I’m like, how does this… Because it’s so weird that you can have SAEs and they do anything. It’s very mysterious to me.\n\n**Jason Gross** (00:47:20): My second thought is that it might be something like distilling the network, where there are a bunch of bits of evidence that I’ve seen that the hard part is in finding the part of the loss landscape to start in. If you reduce the floating point precision and you… What is that called?\n\n**Daniel Filan** (00:47:38): Oh, quantize.\n\n**Jason Gross** (00:47:39): Yes. Okay. If you quantize the network and then you unquantize it, the number of data points you need to retrain to original accuracy is a tiny fraction of what you need to train the network in the first place. And I don’t have any data on this, but I would naively predict that if you’re training a model to match the activations of an already trained model, this requires fewer data points to get a good model than it does if you’re just training on the data and the labels, because you have so much more information. And so, it might be the case that when you’re training the SAE, because you’re training it on the residual stream, there’s a lot more information, you’ve basically found approximately where you want to be, it’s a lot easier to train.\n\nWhat we’ve learned about compact proofs in general\n--------------------------------------------------\n\n**Daniel Filan** (00:48:23): Okay. We’ve said a little bit about what these compact proofs would look like, and you’ve alluded to how one goes about finding them. But I think it maybe deserves more thought about the process of actually creating these compact proofs, because… So, especially if we’re thinking of compact proofs as a measure of how good mechanistic interpretability went. So, there’s this paper, we haven’t mentioned it yet, it’s [“Unifying and Verifying Mechanistic Interpretability: A Case Study with Group Operations”](https://arxiv.org/abs/2410.07476) by Wilson Wu, Louis Jaburi, Jacob Drori and yourself. And I was research-managing Wilson while he did this project. And one impression I got was that a lot of it was just incredibly tedious stuff, thinking about matrix multiplications and these error terms. I might be wrong here and it’s been a while, but my impression is that a lot of the work in finding these compact proofs is, you take the mechanistic interpretability stuff, and then you do some slightly annoying things that feel like they’re fiddly details. Well, okay, first of all, does that characterization seem right to you?\n\n**Jason Gross** (00:49:44): I think there’s actually a really rich opportunity here, that there’s in some sense two things that we’re explaining when we do mech interp. One of them is how to compute the answer in the first place. And the other is, how it comes to be the case that the particular network that we have computes the answer in the way that we’re claiming it. I think this shows up in the groups paper, where the first one is: there’s this very amazing math about their idealized model weights and how that computes the right answer, and this symmetry based on the group operation that allows you to compactly argue that a network that’s doing this should always give the right answer. And I think this bit is not very fiddly matrix multiplication.\n\n(00:50:28): And then, there’s the other part that is bounding the difference between this idealized version of the network and the actual network. And here, the thing that we’re trying to explain, that we’re trying to interpret, is not how the network computes the right answer, it’s how it comes to be the case that the particular network that we have computes the answer in approximately the way that we’re saying.\n\n(00:50:47): And so, maybe you should expect from how I’ve added a bunch of words and how I’m phrasing this, that there’s a bunch of fiddly matrix multiplication on bounding the differences between various bits and propagating those differences through the network. And it looks like there this… fiddly matrix multiplication is both fiddly and somewhat lacking insight, but also potentially generalizable across many networks, because while the particular interpretation you have of how it computes its particular task might vary a lot from task to task, the way in which you establish that these two matrices of the same sort of architecture are doing more or less the same thing might be the same across tasks.\n\n**Daniel Filan** (00:51:28): Yeah. I guess maybe one can analogize it to science, where the process of hypothesis generation is not that hard, and then running experiments is very annoying and tedious, but sometimes you find out you were wrong and so it’s actually worth it.\n\n**Jason Gross** (00:51:42): And then the bit where you do the statistical tests on your experiments to establish how significant your results are is uniform, somewhat annoying, maybe somewhat fiddly, but systematized and uniform across whatever experiments and hypotheses you’re doing more or less.\n\n**Daniel Filan** (00:51:57): So, you have this paper, [“Compact Proofs of Model Performance via Mechanistic Interpretability”](https://arxiv.org/abs/2406.11779), and I see that paper and I’m like, this is introducing this idea of: we can do compact proofs and that’s related to mechanistic interpretability somehow and it’s nice. And then, there’s [“Modular addition without black-boxes: Compressing explanations of MLPs that compute numerical integration”](https://arxiv.org/abs/2412.03773) by Chun Hei Yip, Rajashree Agrawal, Lawrence Chan and yourself. There’s this [unifying and verifying mechanistic explanations about the group operations](https://arxiv.org/abs/2410.07476). There’s this crosscoders paper. And one question I have in my mind is: I’m already sold on compact proofs being a little bit cool. I don’t care that much about group operation. I’m like, it’s nice. But fundamentally, I’m not that worried about neural networks doing group multiplication. What are we actually learning by doing a bunch more of these papers on these toy problems? So, one thing you suggested is maybe some of this fiddly stuff generalizes to other networks: has it actually generalized?\n\n**Jason Gross** (00:52:58): The fiddly stuff we came up with for max of k. And I was like, maybe this is completely arbitrary, completely specific to max of k. And then it was exactly the same in the group operations paper and exactly the same in the modular addition paper. So, I think it does generalize across any architecture that’s going to be doing matrix multiplication basically, or matrix multiplication and using logits for probabilities, which is most of them.\n\n**Daniel Filan** (00:53:25): So, that’s something that generalized from the “Compact Proofs of Model Performance” paper. “Modular addition without black-boxes”, is there something we learned from that paper about finding compact proofs that generalizes?\n\n**Jason Gross** (00:53:38): Yeah, so I think there’s a couple things there. One of them is that the MLPs are the parts that are hardest to interpret, that it’s the non-linearities that are hardest to compress. And so, that’s where we should focus our attention. I think the other thing is that: what Chun Hei discovered, looking at the MLPs in the modular addition, basically the symmetry that he discovered there, is in some sense the same as the symmetry that shows up in the group operations paper, and that’s actually what inspired the group operations paper there.\n\n**Daniel Filan** (00:54:14): Got you.\n\n**Jason Gross** (00:54:15): And I have some hope that this generalizes into a [SLT](https://www.lesswrong.com/s/mqwA5FcL6SrHEQzox)-inspired general procedure for compressing non-linearities based on symmetries in the data and in the network. And I see these toy models as “let’s go see what we discover when we stare really hard and really understand everything that’s going on”. And then, can we take that insight back and look at bigger models? One of the things that I’m excited about with the crosscoders project is that once we get this feature interaction metric and we see these are the neurons where these features are interacting, can we develop some automated symmetry-based procedure that allows us to compress what’s happening at these non-linearities?\n\n**Daniel Filan** (00:55:00): So the crosscoders paper, what task is the network being trained to do?\n\n**Jason Gross** (00:55:05): The thing that we’re looking at concretely: Anthropic recently released [a Circuits Thread post on Stage-Wise Model Diffing](https://transformer-circuits.pub/2024/model-diffing/index.html) where they train an SAE on a language model - a small language model, I don’t remember exactly which one. They introduce a sleeper agent into it, and they look at how the SAE features change when you introduce sleeper data and when you introduce the sleeper agent model.\n\n**Daniel Filan** (00:55:31): What do you mean when they introduce a sleeper agent into it?\n\n**Jason Gross** (00:55:34): So, you start with the SAE or the crosscoder and then you fine-tune on a different model or different data or both.\n\n**Daniel Filan** (00:55:40): Okay. So you fine-tune the underlying model and it becomes a sleeper agent where on some rare inputs it does nasty, sketchy stuff.\n\n**Jason Gross** (00:55:46): It says, “I hate you.” Or it introduces code backdoors, depending on which one you’re looking at.\n\n**Daniel Filan** (00:55:51): Got you. So, you have your original SAE and then you have a fine-tuned model. And then, you said something else, but I forgot.\n\n**Jason Gross** (00:55:56): Yeah, so you look at fine-tuning the SAE, introducing just the sleeper agent data. And you look at fine-tuning the SAE, using the original data, but using the fine-tuned model. And then, you do whichever one you didn’t do. So you have these two different paths to fine-tuning your SAE on the sleeper agent data and the sleeper agent model and you look at how much do features change as you’re fine-tuning the SAE or the crosscoder on this.\n\n(00:56:26): The obvious question to ask, from the compact proofs crosscoders project perspective, is what about the feature interactions? So, what features interact in interesting ways with the features that shift? Because what Anthropic found is that the features that shift the most are in fact largely related to the sleeper agent task.\n\n**Daniel Filan** (00:56:49): To the task and not to the model?\n\n**Jason Gross** (00:56:51): What’s the difference here?\n\n**Daniel Filan** (00:56:52): Oh, you mean just either way the particular features that changed-\n\n**Jason Gross** (00:56:56): Right. The features are the ones that activate on the deployment tag or the ones that are active when you say “I hate you” or that introduce code vulnerabilities or something like that.\n\n**Daniel Filan** (00:57:08): But just to clarify: so you’re doing this compact proofs on this crosscoder model or something. Is it a proof of the fraction of the time the model says, “I hate you, I hate you”? Or what’s the thing?\n\n**Jason Gross** (00:57:23): The proof that we’re going for is just, whatever loss the model gets on whatever webtext dataset you’re using. Maybe it’s the training dataset, maybe it’s the sleeper agent dataset, pick some dataset. We’re bounding the loss on that dataset.\n\n**Daniel Filan** (00:57:41): For the sleeper agent model or for the original model or both?\n\n**Jason Gross** (00:57:46): Either. So in some sense, the actual thing we’re looking at doesn’t quite go all the way to getting a proof because almost certainly the bounds we get are going to be vacuous unless you pick a number of features that is pretty close to the number of data points in your dataset. I expect the epsilon ball interval propagation bounds will drop off very quickly.\n\n(00:58:05): But the thing that we want to do is take what it would take to get a compact proof, ignore the bits that are about error bounds propagation through the network - those are uninteresting things (I claim) that are shared across networks that are just about the difference between worst case and random case, something like that - and focus in on the bit that is about what is missing from the crosscoder, these feature interactions. And then, look at how big are the numbers that we get, how much error would they introduce into the proof if we were doing the whole proof, and use this to grade what feature interactions introduced the most error into the proof.\n\n(00:58:41): And then you can imagine something like, if you want to tightly bound the sleeper agent, how does the proof of that have to change from the proof of tightly bounding the base model on the base data?\n\n**Daniel Filan** (00:58:58): And so, the hope is: the difference in what you have to do in the proof, is telling you a different thing that’s happening in the model and that might be good.\n\nGeneralizing ‘symmetry’\n-----------------------\n\n**Daniel Filan** (00:59:02): So, getting back to what you learn on these toy examples. So, when you have this max of k task or these two papers about basically group operations, you’re like, “Oh yeah, there are all these nice symmetries that the network takes advantage of”. And I’m like, “Well, there probably are for group multiplications,” because group multiplication is the study of symmetry, but not everything is so symmetric.\n\n(00:59:31): So yeah, I guess this gets to… you have these works on modular addition group operations. And those are cases where the problem set up just has a bunch of symmetries because they’re dealing with basically the study of symmetry that is amenable to mathematical stuff. Whereas in the crosscoder paper, language modeling doesn’t seem like the kind of thing that is going to be very symmetric. How much stuff transferred over from one to the other?\n\n**Jason Gross** (01:00:12): I want to answer that by talking about symmetry. And normally, when we think of symmetry in the context of math, we’re like, the rotation, reflection, these things in groups. But what I’ve learned by staring really hard at the modular addition model and asking what really is the symmetry? It seems like it’s composed of some parts that I would actually expect to find in language where the fundamental building blocks seem to be, these bits are irrelevant, they don’t matter. And these bits are the same, they look the same regardless of which one we look at. And the bits that are irrelevant are irrelevant in the same way across all these bits that have similar behavior.\n\n(01:00:54): And this is something that I would expect to see in language where I’m like, there are synonyms, we should expect the synonyms to behave the same. And so in some sense, that gives us a symmetry of the model where we expect this… maybe it’s symmetry or degeneracy where we expect these to all look the same. And so we can collapse this into one case.\n\n**Daniel Filan** (01:01:13): And so, should I be thinking just: in any case where you can do something like having a sparse autoencoder, that’s telling you there’s a bunch of directions in activation space that don’t matter and there are some that do, and once you’re in a direction that matters-\n\n**Jason Gross** (01:01:27): I think it’s more general than that. I think if the network generalizes at all, it’s because the unseen cases have some similarity to the cases that you’ve seen. There are details that differ between the seen and the unseen cases that don’t matter. And so, it’s treating all these cases in the same way. And so, we should look at both sparse autoencoders, sparse crosscoders and symmetries as picking out what are the variations that don’t matter and can we collapse over them so that we can compact the explanation.\n\n**Daniel Filan** (01:01:55): Got you. And so is the story something like: look, you get really good at decomposing networks’ processing into suppressing things that don’t matter and treating things that do matter in roughly the same way. And that pattern, you do it for modular addition, you do it for group multiplication, you do it for dealing with language, you do it for everything?\n\n**Jason Gross** (01:02:25): That’s what I would expect. The particular form that it takes… I think sparse crosscoders, sparse autoencoders are looking at a sort of case analysis-flavored or linear combination of case analysis-flavored symmetry, degeneracy-based decomposition. I think you need something different for non-linearities. And because the crosscoders project is still in the early stages… We’re like what, two, three weeks in or something?\n\n**Daniel Filan** (01:02:55): Four weeks.\n\n**Jason Gross** (01:02:56): Four weeks, maybe.\n\n**Daniel Filan** (01:02:58): Four weeks since the [MATS](https://www.matsprogram.org/) program \\[started\\].\n\n**Jason Gross** (01:02:59): Oh, yeah. Okay. Four weeks in, we haven’t really looked at what features strongly interact. The first step of the project was just replicating Anthropic’s result on [TinyStories](https://arxiv.org/abs/2305.07759) and starting to train some crosscoders on toy models.\n\n(01:03:18): So, we haven’t really gotten to look at what do the feature interactions look like. But I am hoping that once we get what the feature interactions look like, we’ll stare at them and learn something about symmetries, learn something about how to port the insights from [the groups paper](https://arxiv.org/abs/2410.07476) and [the modular addition paper](https://arxiv.org/abs/2412.03773) about how to compress these non-linearities. And I am optimistic that in a more messy but somewhat similar way, we can find ways that the non-linearities are treating groups of things the same, and are being irrelevant or are not caring about other axes, and use this to compress how the non-linearities are working.\n\n**Daniel Filan** (01:03:59): Yeah. It’s interesting because… So maybe we can talk a little bit about the “Modular addition without black-boxes” paper. My recollection from that paper is something like: if you had this infinite-width multilayer perceptron where you’re doing this sum over things and it’s a weighted sum, and you have enough things, it’s basically an integral, and you can show that the integral being performed is the same as the thing you would hope the network is computing.\n\n(01:04:26): And then the whole game is like: okay, you only have finitely many things, you’re doing some sort of Riemann sum, how bad is the error of that? And to me, that story sounds pretty different from the “treating some things the same and suppressing some things”. Is there a deeper unity, or…?\n\n**Jason Gross** (01:04:42): There is a deeper unity.\n\n**Daniel Filan** (01:04:44): Okay. Tell me about the deeper unity.\n\n**Jason Gross** (01:04:45): So it dives a little bit into what really is an integral, and especially what is an integral over a circle, because we’re integrating a periodic function over the range of its period. And a central property of the integral that we’re using is that it’s periodic over this range, and so the function is shift invariant. So as you shift where you’re evaluating the function, the value of the integral doesn’t change.\n\n(01:05:17): And there’s another perspective. So the simplified version of the modular arithmetic network, it’s basically embed matrix. You add the embeds from X and Y, you do ReLU and you unembed. So if you take the SVD of these matrices-\n\n**Daniel Filan** (01:05:34): The singular value decomposition - basically saying, “Okay, which inputs is it most sensitive to, and what does it do on those inputs?” It’s a fun thing. [Google](https://www.google.com/search?q=Jess+Riedel+singular+value+decomposition+bra+ket+notation) [“Jess Riedel singular value decomposition bra ket notation”](https://blog.jessriedel.com/2017/01/12/singular-value-decomposition-for-bra-ket-notation/), if you’re curious.\n\n**Jason Gross** (01:05:49): The cool thing is that in this network, the singular value decomposition gives you the Fourier basis that each singular value, or really each pair of singular values, corresponds to one of the frequencies in the Fourier basis. And so now we have these four matrices. We have the embed-side Fourier basically embedding the input values on a circle.\n\n**Daniel Filan** (01:06:19): Maybe we should have said, the Fourier basis is just: you take a number and then you turn it into, okay, how far across would you have gotten on a circle if you were rotating at a certain speed for that number of seconds? Is that fair enough to say?\n\n**Jason Gross** (01:06:33): Yeah. Close enough. And if you’re dealing with real numbers, you get two dimensions for sine and cosine. If you’re dealing with complex numbers, you just get one complex number that encodes the whole thing. And so on the embed side, you have the circle, and also on the unembed side, you have another circle. And then in the middle, you have these matrices that previous investigations of the modular addition networks had not investigated that look sort of random, but it turns out that there’s a tight relation between the principal components of these two matrices, which incidentally, you see if you put them in polar coordinates, there’s a factor two difference between the angles. But that’s a technical note. The thing the symmetry of the network lets you do is shift the variation between the two parts of the singular value decomposition. So on the input side…\n\n(01:07:37): Let’s go on the output side actually. On the output side, you have all of these different possible outputs that you could imagine. And because complex multiplication is angle addition because of how the angles work, because of the symmetry in the network, you can shift choosing which output you’re reading into permuting the neurons that these weights are attached to or permuting the weights that are attached to each neuron.\n\n(01:08:07): And this corresponds to saying that because each neuron is responsible for one box under the curve of the integral, and the integrand is periodic with period of the integral, this basically says that you’re allowed to shift the thing that you’re integrating arbitrarily and you get the same result. So that’s the symmetry that we’re using here.\n\n(01:08:27): And then if you want to make it look like a forward pass through the network, we’ve twisted the circle on the unembed neuron side. And because there’s this tight relationship, this tight coupling between post-ReLU and pre-ReLU, you need to do a similar twist on the pre-ReLU side in order to make it look like a forward pass. And then because of a similar symmetry at the input, in order to get back the value you started with, you need to undo that twist so that you cancel out the effect of adding in that twist and making it look like a forward pass.\n\n(01:09:02): And now the thing that we’ve done is that previously we were like, “Oh, we have inputs and inputs and we have different outputs that we might be reading off of”. We’ve said, “Regardless of which output you read off of, it looks the same if you do this shuffling internal to the network”. And so we’ve pushed the dependence on what the right answer is all the way back to the embeds. And so now we’ve shoved all the way back to the embeds a description of which inputs correspond to which outputs.\n\n(01:09:33): And notably, this is largely invariant in what the non-linearity is, and this is what gives us the compression: that the only thing you need to establish is basically that this approximate integral is strictly positive. And that’s a comparatively easy thing to do. And if you get that, then the symmetry argument gives you basically the rest of the whole argument.\n\n**Daniel Filan** (01:09:58): Hang on, hang on. Maybe I’m misunderstanding. I thought that the thing you needed to know was that the integral produced this quantity of interest, right?\n\n**Jason Gross** (01:10:10): The way that you prove that the integral produces the quantity of interest, if you stare very carefully at the proof, runs through the same sort of symmetry argument, and so you can massage the proof to pull out all the symmetry bits. You pull all of the dependents on input and output out of the integral, and you’re left with basically the right answer times this integral quantity. And you’re like, “Well, the only thing I need to know is that it doesn’t flip the sign on the outside”.\n\n**Daniel Filan** (01:10:37): Right, right, right. And so somehow the thing that’s going on, I was saying, “Oh yeah, the paper is just talking about how this thing produces an integral”. But really the paper is saying, “The neural network deals with symmetry nicely enough so that these things represent exactly the right thing and these things are covariant with these things, such that there’s an integral that can produce the right thing that really does matter. And that doesn’t just happen by accident. That happens by being careful about stuff”.\n\n**Jason Gross** (01:11:08): Yeah, I think that’s basically right. Or another way you could look at it is that if you stare really hard at what is an integral, there’s a sense in which what an integral is is deeply connected to symmetries; that if you shift what you’re doing along a function, things don’t change too much.\n\nGrading mechanistic interpretability\n------------------------------------\n\n**Daniel Filan** (01:11:24): Right, right. We’re dealing with compact proofs, and the point is to tell us how good are mechanistic explanations? How good are mechanistic explanations? How good has the field of mechanistic interpretability done on a scale of one to five?\n\n**Jason Gross** (01:11:40): On a scale of one to five, wow.\n\n**Daniel Filan** (01:11:43): It could be on a scale from good to bad if you want.\n\n**Jason Gross** (01:11:46): So there’s this existing work on [causal scrubbing](https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing). I don’t know if you talked about that on some past podcast.\n\n**Daniel Filan** (01:11:54): We might have. I’m also very interested in why don’t we just do causal scrubbing if what we’re interested in is grading mechanistic interpretability?\n\n**Jason Gross** (01:12:04): Maybe I’ll answer that one first and then come back to the other question. Why don’t we do causal scrubbing?\n\n**Daniel Filan** (01:12:08): Or any other sort of thing?\n\n**Jason Gross** (01:12:10): So when I was developing the compact proofs approach, I had a couple of extreme examples in mind that I wanted a metric that you can’t Goodhart against. When we’re doing causal scrubbing, we look at faithfulness.\n\n**Daniel Filan** (01:12:28): For those who don’t know, causal scrubbing is something like: you say, “This bit of the network does this thing”. And roughly the story is you just ablate out all the other bits of the network and you’re basically saying, “Okay, if this part of the network results in this activity, then if we randomize over all the other bits of the network, you should still get that activity”. Because you said the only important part was this structure.\n\n(01:12:55): There’s a lot more to say, but I take that to be the core idea behind causal scrubbing. And when you say the causal scrubbing tests faithfulness, the thing that causal scrubbing is telling you is that yeah, you are right that this part of the network was responsible for this behavior by seeing that if you got rid of all the other bits, it didn’t do that. Does that strike you as a fair brief summary?\n\n**Jason Gross** (01:13:21): Yeah.\n\n**Daniel Filan** (01:13:22): Okay. So hopefully listeners are up to date now. So causal scrubbing gets you faithfulness.\n\n**Jason Gross** (01:13:26): So here’s a great explanation according to causal scrubbing: the whole network. This is a problem. And if you’re like, “Oh, let’s use the node count as our measurement of length”, you run into other issues that are, well, now you’re restricted to exactly the architecture of computation that a network is doing. Maybe you want to be able to divvy up the computation different ways.\n\n(01:13:51): If you say that, then you have to deal with, well, what’s the complexity of the computation in each node? Am I allowed to say I have a single node and the computation that it does is run the whole network? That would be a problem.\n\n**Daniel Filan** (01:14:06): Right, right. Well, okay, you could imagine that, look, we’re just going to compress our story of what this bit of the network does into… I don’t know, we’re going to zip it in literally a [zip file](https://en.wikipedia.org/wiki/ZIP_(file_format)) or whatever, and the number of bits it takes to specify the behavior, that’s what’s going on. And so this would be more like a compression of the network versus a compression of the proof. But if you’re really invested in causal scrubbing, I feel like this is the kind of answer that you could give to get something non-trivial.\n\n**Jason Gross** (01:14:37): Right. Okay. And here’s another issue. Suppose that your network actually computes things perfectly. Suppose it computes the max of k numbers perfectly. My compression of the network is now it computes max.\n\n**Daniel Filan** (01:14:45): That is an issue. That’s fair.\n\n**Jason Gross** (01:14:49): And in fact, I have described what it does, but not how it does it. And so if we’re just looking to describe what the network computes, the input/output behavior, great. But if we’re looking to describe how the particular network that we’re looking at computes the thing that it does, then we need something more than that. That’s the difference, I think, between compressing the network and compressing the trace of the computation.\n\n(01:15:11): I’ve said a bunch of bad things about causal scrubbing, but I think actually it’s pretty great. I think that if you wanted a probabilistic version of compact proofs, it would look a lot like causal scrubbing where you’re like, “Here’s my structured version, here’s how I’m doing the computation”. And then if I want to establish that the thing I’m claiming is what the network is actually doing, you do something like causal scrubbing and you say, “How much computation do I have to put in to establish that my explanation is matching the network?” And then you also add, how much computation do I have to put in to compute what my explanation says is done?\n\n**Daniel Filan** (01:15:47): And also, in causal scrubbing, to be fair to be causal scrubbing, it told people a thing they didn’t want to hear. In fact, they did causal scrubbing and it was like, ah, these things don’t hold up as well as we thought they did. It wasn’t even Goodharted.\n\n**Jason Gross** (01:16:03): As pessimistic as causal scrubbing is, compact proofs is even more pessimistic. I think one interesting thing is that when we’re doing mech interp, we spend a lot of time on the structure that is there, and we don’t spend much time on the structure that isn’t there or more precisely on the behavior that isn’t there, and what structure might give rise to the behavior that isn’t there.\n\n(01:16:32): And the example that I’ve come up with to explain this is if you want to explain, say, how a person grows, you don’t just have to explain how the growth processes work in the body. You also have to find all of the poisons that might kill someone and establish how nothing in the body acts like these poisons.\n\n(01:16:54): And some of these poisons are lethal at incredibly tiny doses, and so you have to establish how there’s no molecule anywhere in the body that behaves anything like these lethal doses of chemicals. And mech interp doesn’t spend a lot of time doing that.\n\n**Daniel Filan** (01:17:11): Okay, I’m not a biologist, but when I hear people talk about, for instance, computational neuroscience, it doesn’t sound to me like they’re doing this sort of thing. I feel like you hear people talk about, oh, we found Broca’s area, and you don’t hear people talk about, we have proved that nothing else can influence Broca’s area or something. Which, I don’t quite know what the upshot of that is, but it’s hopefully vaguely interesting.\n\n**Jason Gross** (01:17:41): Yeah, it’s at least interesting to point out that there’s this area that we’re not explaining. And maybe that’s right. Maybe all we want to know is: the neural net is doing this amazing thing, or the brain is doing this amazing thing. How is it even possible that anything at all does something vaguely like this? And I think mech interp and all of these areas, I think they’re doing decent jobs at that.\n\n(01:18:06): Maybe I want to be even more enthusiastic. I think they’re doing great jobs finding new things about how it’s possible to do interesting behaviors. If you want to establish that these are the behaviors that actually happen, this is the mechanism by which this network actually works, then you also need to explain all these other things.\n\n**Daniel Filan** (01:18:26): So all of this was getting to this question of “on a scale of good to bad, how good a job has mechanistic interpretability done?” So we talked a bit about causal scrubbing, but that was a prelude to this question, so what say you?\n\n**Jason Gross** (01:18:44): If you just go by the book of what compact proofs says, it says the mech interp that everyone’s doing is horrible. If you look at where are we on this Pareto frontier, we’re like, “Yeah, you can maybe do a little bit better than brute force at this end, and you can maybe get a slightly non-vacuous bound at the other end”. But it doesn’t push the envelope much.\n\n(01:19:07): But I think that’s because the compact proofs, if you’re going for full proofs, that’s targeting something different than what you might be targeting with doing mechanistic interpretability. If you’re saying, “I’m doing interp because I want to know for sure that absolutely this network will never do the thing that I’m concerned about,” then yeah, we’re nowhere close to being able to do that with existing mech interp techniques.\n\n(01:19:34): If you’re like, “I want to be able to discover the structure of what it’s doing and have that structure be somewhat meaningful”, things seem to be going decently well. I think the things that I’m most excited for in mech interp remain in the future.\n\n(01:19:54): The networks that we have today are doing amazing things and I want to be able to discover new things about how cognition works, new things about how functionality works, how the pieces all fit together, how programming is done, or what deception is or how to evaluate various things, how mathematics is done, what are these capabilities in the world? What are they made of? How do we assemble them out of building blocks?\n\n(01:20:21): And I would love to see mech interp give us answers to these questions or give us new frames of looking at the world, new ways of looking at how these tasks are accomplished.\n\n**Daniel Filan** (01:20:33): Fair enough. So maybe one thing to ask is: so you’re saying how using these mechanistic interpretability tools to improve proof length stuff, it’s not doing that much better than brute force. Can you put some numbers on this? I don’t know if there’s some area-under-the-curve metric or something.\n\n**Jason Gross** (01:20:57): I told you about how much interpretability was required for breaking the exponential. All you need to know is it gets the right answer by paying attention to one token. I haven’t looked in closely to what does this mean on frontier models, but I would expect that the sort of things that you can do to compact proof length is if you’re like, “Here are two words,” or even better, “Here are two capitalizations of the same word”, in almost all cases, the network treats them the same.\n\n(01:21:32): Therefore, in the dataset, I can find all the times that you capitalize this word differently and I can collapse those two data points and argue that the network does the same thing on those. Simple things like that, I imagine you would not make your bound vacuous by going down from brute force and being like, “Oh, these two data points look the same in this very simple way”. I imagine that most existing interpretability that is more complicated than “the network treats these cases the same” would completely destroy the bound.\n\n**Daniel Filan** (01:22:08): In max of k… I’m going to draw a plot so listeners will just have to imagine this. So on the X-axis, we’re going to do length. On the Y-axis, we’re going to have the bound you get. And there’s some box where there’s a diagonal line of the box, and that’s the line of the length versus bound if you just do a brute force thing. And then there’s some curve that goes above that if you use your mechanistic interpretation. So if you’re watching on camera, hopefully you can see that.\n\n**Jason Gross** (01:22:55): You can also look at the [Alignment Forum post](https://www.alignmentforum.org/posts/bRsKimQcPTX3tNNJZ/compact-proofs-of-model-performance-via-mechanistic) or the [Tweet thread associated with the paper](https://x.com/diagram_chaser/status/1805337592143265801) where there is a plot much like this.\n\n**Daniel Filan** (01:23:04): So here’s what I want to ask. So there’s an area between the diagonal line and the curve you get for how good you did with the mechanistic explanation. And there’s area that’s above the curve you get for the mechanistic explanation. I think if mechanistic interpretability is doing a really bad job, then the first area should be very small relative to the second area. And if mechanistic interpretability is doing a very good job, then the first area should be pretty big compared to the second area.\n\n**Jason Gross** (01:23:39): So let me talk about the max of k model, where you might hope that we can do an amazing job. So the first thing to note is that the plots in the blog post are on a log X scale. The proof length is measured in exponents. And although the Pareto frontier in the blog post looks pretty decent, if you extend out the access all the way to length zero, I think most of the area there is going to lie in that region where the best proof that we can do is…\n\n(01:24:13): If you want a proof that is roughly one forward pass of the model, approximately, the best thing that I think we can do is you take one data point and you run it through the model. And there’s an enormous gap between you ran one data point and you say, “Yes, accuracy is one out of total number of data points”, and the true accuracy of the model. And so I think most of the area here is just fundamentally impossible to capture. There is a theoretical optimum here, and I think most of the area here lies beyond the theoretical optimum.\n\n**Daniel Filan** (01:25:04): So if I literally have this triangle, shouldn’t half of the area be at one third of the length or something, where you’re doing a third of the forward passes relative to the whole size of the dataset or something roughly like that? It’s not going to be 0.1% and it’s not going to be 99.9%. It’s going to be a bit less than half if we use a not log axis.\n\n**Jason Gross** (01:25:35): I see. Is that…\n\n**Daniel Filan** (01:25:38): I guess I’m looking at a triangle. It seems like this has got to be roughly as big as this if I move this a bit here.\n\n**Jason Gross** (01:25:46): I’m trying to figure out where my sense of the area is off. Maybe I was thinking of extending it all the way to zero. Yeah, maybe the log scale was distorting my perception of this. I think it is still the case that a large fraction of the area is beyond the theoretical optimum. I think the thing we should be comparing it to is not necessarily what is the true accuracy of the model, but if you were to search over all possible proofs, what is the optimal Pareto frontier that you could do compression-wise?\n\n(01:26:22): So that’s just the initial thoughts about where we should be setting our expectations in our baseline. I think given that in the max of k model, I think we do a somewhat decent job at capturing a significant chunk of the area. I think we still miss out on a bunch from the shortest proofs. I think there are some tricks there potentially that we haven’t found yet.\n\n(01:26:52): Another thing to consider here, which is what’s the best compression that you would get with proofs? What’s the best compression you could get with proofs if you de-randomize the network and allow yourself to fine tune and the perfect network that did this task? And then what’s the best compression that you would get with some sort of probabilistic flavor of proof? And I think you get different answers to all of these.\n\n(01:27:11): I guess I’m hedging a lot ‘cause I haven’t run the numbers on any of these, but maybe I can answer a more interesting question that’s, what is my sense of how much of what’s going on have we managed to find or interpret? Which I imagine is what you’re getting at with-\n\n**Daniel Filan** (01:27:27): Yeah, kind of.\n\n**Jason Gross** (01:27:28): Kind of?\n\n**Daniel Filan** (01:27:29): Yeah.\n\n**Jason Gross** (01:27:29): And I feel like that’s also a very hard question to answer. I think my sense is that we found a bunch of interesting things and there’s an enormous amount left to be discovered.\n\n**Daniel Filan** (01:27:42): Are you talking about max of k? Are you talking about language models?\n\n**Jason Gross** (01:27:44): I’m talking about language models in general. I think in max of k, we’ve definitely found the bulk structural properties. I think there might still be a lot of very subtle details about what coincidences of random numbers managed to make it the case that the noise terms don’t blow up more than they do.\n\n**Daniel Filan** (01:28:09): Okay. And maybe group multiplication is an intermediate case between very simple and full difficulty.\n\n**Jason Gross** (01:28:19): Yeah, that’s a great one. So I’m going to talk about the modular addition one in particular. There’s this interesting thing that has come up a bunch when doing the proofs, which is that every time I hit a bottleneck in compressing the proof, if I stare at it, I’m like, “Ah, yes, in fact, I don’t understand what’s going on here”, where I thought I understood something and then I didn’t.\n\n(01:28:40): So in the modular addition model, what this looks like is that the bounds are actually not that tight. They’re kind of bad. And this corresponds to not understanding how the network is laying out the boxes and how the thing that it’s doing is a good numerical approximation to an integral.\n\n**Daniel Filan** (01:29:03): So laying out the boxes being, if you’ve got like… Okay, we’re going to do another plot for the people watching. So imagine I have some curve, there’s X and Y, there’s some curve. The way you do numerical integration is you just pick some points on the X-axis and form boxes like this. And you say that the area under the curve is the area under the boxes. And so you’re saying, okay, you don’t understand how the network picks these widths, these points to check the value of the curve off and make these rectangles that you’re using to approximate the area of the curve, if I understand you correctly.\n\n**Jason Gross** (01:29:47): So we understand which points it has picked. The thing we don’t understand is how it comes to be the case that picking these points gives as good an approximation to the integral as it actually does.\n\n**Daniel Filan** (01:29:57): Right. Okay. Okay. Because if you pick your points wrong and the function varies a lot, then-\n\n**Jason Gross** (01:30:06): It’s more like if you overestimate in some places, you can counteract that by underestimating in other places. But if we’re not aware of which things it’s averaging out differences in or we’re not aware of how it comes to be the case that the places where it’s averaging out differences actually usually end up being opposite ways rather than compounding error terms, we don’t get to say anything about them.\n\n**Daniel Filan** (01:30:28): Okay. Okay. Fair enough. And so on a scale of one to five of how good a job mechanistic interpretability has done, where do you want to say we fall in this case?\n\n**Jason Gross** (01:30:42): Okay. Okay, great. I have a good scale to put this on. We can look at the scaling exponents of how long a proof you get from doing a given… So there’s two axes, there’s how good a bound do you get, how faithful are you? And I think causal scrubbing is a good answer on that. And then there’s, how deep is your explanation? How much of the structure that’s there have you understood?\n\n(01:31:06): And I think a good measure on that, again, is proof length, but we can ask what are the exponents that current explanations bring you down to? And the target, in some sense, that you’re aiming for is you’re aiming for something that is like parameter count of the network, plus number of data points where naively you have to do something like number of data points, times parameter count to do all the forward passes.\n\n(01:31:32): And if you can get your explanation down to be, you just run over the parameters once and you run over the dataset once and that’s it, then I think you’ve found a pretty good explanation. The thing that we do with numerical integration, we in fact manage to get down to something that is roughly like parameter count of this part of the network plus number of data points.\n\n**Daniel Filan** (01:31:58): At that point, that’s just how long it takes to literally look at everything. The only way to do better would be to a priori know that you didn’t even have to think about something?\n\n**Jason Gross** (01:32:07): In some sense. There’s another sense in which here we’re saying, “Ah, now our understanding of how this network is doing the task that it’s doing is necessarily bottlenecked on how it comes to be the case that this particular network is doing the task that it’s doing”. It could still be the case that there’s more understanding to be found, more compression to be found in the bit where you’re like, “How is it even possible to do this task?” But it says that the leading bottleneck is in the establishing the correspondence between your explanation and the network.\n\n(01:32:37): And I think most existing mech interp is not currently even close to having most of the issue be that you’re bottlenecked on the theoretical optimum of establishing correspondence between your network parameters and the thing that you’re claiming the network does. I think crosscoders comes the closest to this. I think crosscoders gives us explanations that look much better than any other mechanistic interpretability that people have done. Possibly excepting [the recent Apollo paper](https://arxiv.org/abs/2501.14926) on… What was it called? Something parameter decomposition, do you remember?\n\n**Daniel Filan** (01:33:24): Oh.\n\n**Jason Gross** (01:33:26): APD. What’s the A?\n\n**Daniel Filan** (01:33:27): I forget. This is embarrassing. They sent me the paper to read and I didn’t read it.\n\n**Jason Gross** (01:33:33): I read it in depth and chatted with [Lucius](https://www.lesswrong.com/users/lblack) \\[Bushnaq\\] about it. I’m forgetting what the A stands for.\n\n**Daniel Filan** (01:33:38): All right. Well, it will be linked in the description and in the transcripts, and so people will know exactly what we’re both forgetting. Everyone will know except us.\n\n**Jason Gross** (01:33:48): How embarrassing.\n\n**Daniel Filan** (01:33:49): How embarrassing indeed.\n\n**Jason Gross** (01:33:50): Okay. So I think APD and crosscoders get pretty close to this sort of linear in the parameter count or parameter count plus dataset, where if crosscoders are good enough that none of the features interact - which of course is false, the features definitely interact - but if it were that good, then it would be something like dataset times hidden dimension times vocab size, or dataset times single layer parameter count, plus number of features squared times parameters in the model. And so number of features squared is still a lot. And we might hope that if we understood, we could do much better than number of features squared.\n\n**Daniel Filan** (01:34:57): That does seem rough to me, especially because if you have… Were you multiplying by number of parameters in the model at any point there?\n\n**Jason Gross** (01:35:10): Number of features squared times inference cost.\n\n**Daniel Filan** (01:35:14): Times inference. Oh, okay.\n\n**Jason Gross** (01:35:16): Where I think inference is comparable to number of model parameters.\n\n**Daniel Filan** (01:35:21): So number of features squared, that’s comparable to the… So if you imagine a sparse autoencoder that didn’t actually expand its input, then number of features squared would be the number of parameters in one MLP layer in the network. And in fact, there are more features in that, so number of features squared should be like the number of a lot of MLP layers in the network. So that sounds like it’s getting more close to dataset size times number of parameters of the network.\n\n**Jason Gross** (01:35:52): No, because it’s only the case that the number of parameters in the MLP is (d MLP) squared when d MLP and d model are comparable. If we’re expanding d MLP without expanding d model-\n\n**Daniel Filan** (01:36:05): I didn’t understand that at all.\n\n**Jason Gross** (01:36:07): The number of parameters in many MLPs is number of MLPs times d MLP times d model. Whereas when you’re pulling it here, you’re essentially… There’s an extra square.\n\n**Daniel Filan** (01:36:19): Oh, there’s this like… Is it something like, MLPs have this big hidden dimension and-\n\n**Jason Gross** (01:36:26): But that doesn’t make the model dimension big?\n\n**Daniel Filan** (01:36:28): Right. Where the model… Somehow, the MLP is going to this MLP dimension, and then adding it back to the model and then like that.\n\n**Jason Gross** (01:36:39): Maybe. But maybe zooming out, the relevant comparison point here I think is not the number of parameters in the model. The relevant comparison point is multiplying parameters in the model by dataset size.\n\n**Daniel Filan** (01:36:51): Yep. Sorry, I thought there was this pre-factor of dataset size. Sorry, I thought you were saying that it was dataset size times features squared.\n\n**Jason Gross** (01:36:59): No.\n\n**Daniel Filan** (01:37:00): Oh, okay. All right. There’s my issue. The whole point of doing the feature square thing is that you don’t have to do the dataset size.\n\n**Jason Gross** (01:37:06): That’s right.\n\n**Daniel Filan** (01:37:08): Okay. There we go. All right. We eventually got there. So this actually gets to… And you’ve probably implicitly answered this. So there’s these three settings. There’s max of k, there’s group multiplication, and there’s language modeling. And I guess you don’t exactly know for language modeling yet probably. But for max of k, and for group multiplication, can you give me a feel for how much of the proof is the bit where you… How much of the proof is dealing with annoying error terms, versus the core story of what’s going on?\n\n**Jason Gross** (01:37:55): Essentially all of it is dealing with annoying error terms. There’s a sense in which the thing that you’re doing, especially in the group operations paper, is you’re writing down your idealized model, and then you’re computing how much margin this gives you, how much slack this gives you, to get the gap between your idealized model and the real model wrong. And this computation is extremely cheap. Basically, you set the hyperparameters of the idealized model, which is way, way less than the parameters of the model, you do some very cheap computation on this, and you get out what your bound is on this part. Then you need to do something that is comparable to multiple forward passes, to establish the gap between the idealized model and the actual model. And that’s where most of the issue lives.\n\n(01:38:54): And in the same sense, this is also true with language models with the crosscoder, where you have your large dataset that even though… Oh wait, is this doing the crosscoder model? How do the parameters work out here? We have the dataset size roughly times running it on a two-layer model, versus features squared forward pass. Which of these is bigger? They might be comparable.\n\n**Daniel Filan** (01:39:31): Sorry. There’s features squared, and there’s the parameter size?\n\n**Jason Gross** (01:39:35): The cost of the forward pass is-\n\n**Daniel Filan** (01:39:36): What’s the forward pass?\n\n**Jason Gross** (01:39:37):\n\n*   the parameter count of the model.\n\n**Daniel Filan** (01:39:39): Okay. So there’s parameter count, and there’s features squared. So features, they should be comparable.\n\n**Jason Gross** (01:39:43): Well, you multiply these. And then you compare this to number of data points run through two layers of the model.\n\n**Daniel Filan** (01:39:54): Okay. So features squared is comparable to parameter count of the model, right?\n\n**Jason Gross** (01:40:03): And is that comparable to the number of tokens in the dataset? Or is that…\n\n**Daniel Filan** (01:40:07): Well, if we knew the [Chinchilla scaling law](https://en.wikipedia.org/wiki/Neural_scaling_law#Chinchilla_scaling_(Hoffmann,_et_al,_2022)), we would know this.\n\n**Jason Gross** (01:40:12): I think that-\n\n**Daniel Filan** (01:40:13): Does anyone…? I think the listeners are screaming at us.\n\n**Jason Gross** (01:40:16): Probably. I think the scaling law is that you scale dataset size and parameter count roughly in tandem.\n\n**Daniel Filan** (01:40:23): Yeah, I think that’s right. So in that case, if you’re looking at every data point-\n\n**Jason Gross** (01:40:32): Is featured squared really-?\n\n**Daniel Filan** (01:40:34): So I’m not thinking about residual networks. I’m just thinking of base multi-layer perceptrons. In that one, if you have the same width throughout the network, and you don’t do any residual layers, then it is literally just dimension of the model squared, times number of layers. That’s the number of parameters in the network. Let’s say I’m using this sparse autoencoder. So I’m taking this hidden dimension and I’m multiplying it to let’s say K times the hidden dimension. Then that squared is going to be K-squared times the hidden dimension squared, by how squaring works.\n\n**Jason Gross** (01:41:25): We’re doing a great job at keeping this non-technical.\n\n**Daniel Filan** (01:41:27): Yeah. And so if K-squared is comparable to the number of layers in the model, then features squared is going to be comparable to the number of parameters in the model, if K-squared-\n\n**Jason Gross** (01:41:43): Wait, sorry. K is the sparsity?\n\n**Daniel Filan** (01:41:44): No, K is the blow-up factor.\n\n**Jason Gross** (01:41:48): Between the hidden dimension and the number of features?\n\n**Daniel Filan** (01:41:50): Yeah, between the hidden dimension of the model and the hidden dimension of the SAE.\n\n**Jason Gross** (01:41:53): I see. Okay.\n\n**Daniel Filan** (01:41:58): And the sparsity-\n\n**Jason Gross** (01:42:01): Does not show up.\n\n**Daniel Filan** (01:42:02): Yeah, the sparsity does not show up except in that you probably pick your K to achieve some sparsity that you want. So yeah.\n\n**Jason Gross** (01:42:14): I think we should leave this as an exercise for the listeners. There’s an equation on [the Alignment Forum blog post that lays out the crosscoders project](https://www.alignmentforum.org/posts/RjrGAqJbk849Q7PHP/measuring-nonlinear-feature-interactions-in-sparse) that gives the asymptotic proof length for the crosscoder-based proof, in terms of all of the parameters. And plausibly, I should have done this ahead of time and plugged in some numbers to get a rough estimate of what this says. But yeah, you can look at that and figure out whether the leading order term is the dataset-based term or the feature model-based term. It would be at least slightly embarrassing for the approach if we’re like “ah, the crosscoder doesn’t actually save you any computation over doing brute force”, which might be the case.\n\n**Daniel Filan** (01:43:04): Probably should go home and get one of your interns to figure that out as well. But it can also be an exercise for the listener.\n\n**Jason Gross** (01:43:12): The real question here is how does the trade-off look? You can plot this equation, you can plot reconstruction error against this equation, as opposed to against sparsity or whatever. And this is actually a plot I’m pretty interested in seeing, that is: how does the reconstruction error of the crosscoder vary as you change the corresponding proof length?\n\nWhat helps compact proofs\n-------------------------\n\n**Daniel Filan** (01:43:34): So this is how mechanistic interpretability is interacting with compact proofs as a whole. But mechanistic interpretability is not any single thing. And so one thing I’m curious about is just which bits of mechanistic interpretability are being the most useful, from a compact proofs perspective?\n\n**Jason Gross** (01:43:53): Beyond crosscoders?\n\n**Daniel Filan** (01:43:54): Beyond crosscoders.\n\n**Jason Gross** (01:43:55): And potentially APD?\n\n**Daniel Filan** (01:43:57): Okay, so crosscoders and APD seem like winners here. What is it about crosscoders and APD that make them so…\n\n**Jason Gross** (01:44:10): I think it’s the sense in which they’re trying to interpret the entire network on the entire dataset. And the issue that I have with a lot of mechanistic interpretability is they pick some tiny fraction of the training dataset, some tiny fraction of the model, and they’re like, “Look, on the small dataset, we’ve explained this interesting behavior.” And if you look at what is the cost of running that explanation, versus the cost of brute forcing that tiny bit of the dataset, they’re pretty similar. This means that you don’t get that much proof compression, especially if your baseline is running the whole model on every data point and you’re like, “Okay, I’ve explained this tiny little bit.” Whereas if you manage to interpret either basically everything that’s going on, or at least large swaths of that, then you can potentially manage a significant amount of compression.\n\n(01:45:00): And the problem with SAEs is that they’re per layer, and you don’t get anything about how they interact. So you’re like, “Great, I know what cases to break up the dataset into for this layer.” But that doesn’t actually tell you what’s going on before or after that layer. And so SAEs, without SAE circuits, doesn’t really give you any compression of what’s going on.\n\n**Daniel Filan** (01:45:19): So stuff that just tries to deal with a whole model, that’s pretty good. So SAEs without SAE circuits don’t help you that much. Some people are working on SAE circuits: have you had the chance to compact-proofify them yet?\n\n**Jason Gross** (01:45:40): So I haven’t looked into it that carefully. My understanding is that a lot of the SAE circuits work is just what features are connected up to which other features, but they don’t actually give you the computations. They give you the graph part of the computational graph but not how to actually compute anything with them. So that doesn’t actually let you compress the computation.\n\n**Daniel Filan** (01:45:55): Fair enough. Yeah, that will be tricky. So okay: apart from the winners, is the rest of mechanistic interpretability mostly a wasteland? Or are there still some bright stars?\n\n**Jason Gross** (01:46:11): I think the stuff on toy models is cool, especially when it’s like “here’s some way that networks can do this thing that we didn’t previously realize could be done at all”. I think that’s what has me excited about the work that I’ve advised in the modular addition in the group models. Do you have other particular things in mind when you say the rest of mechanistic interpretability?\n\n**Daniel Filan** (01:46:31): Not really. I’m just like… Maybe the crux of my question is something like, okay, suppose the mechanistic interpretability people thought of their job as trying to be graded by the compact proof perspective. What would they do differently? And perhaps your main answer is just “try to interpret everything rather than just a tiny thing”.\n\n**Jason Gross** (01:46:57): I think that that is my main answer: try to interpret everything. And basically the thing that the compact proofs approach gives you is it says, “Where should we focus our mechanistic effort?” And the thing that we learned from that is, “Well, we should focus on the whole dataset rather than picking out tiny examples from it. We should focus on the whole network rather than individual parts”. If we want to really deeply understand what’s going on, we should focus on the nonlinearities, which I think most of existing mechanistic interpretability doesn’t talk about: how you actually do computation through nonlinearities. I think a lot of that funnels us towards things like APD, crosscoders. And the nudge that I’m hoping the project I’m advising with crosscoders will give is saying, “Let’s look at how these features interact. We need to not just do crosscoders but also this notion of crosscoder circuits or crosscoder feature interactions”.\n\n**Daniel Filan** (01:47:53): Right. A similar question I have is, so you’re talking about compact proofs for mechanistic interpretability. It strikes me that there are… At least naively, not having tried to do it myself, it seems like there are other things that you could work into proofs or help you write proofs. For instance, if science of deep learning were good, you could imagine science of deep learning saying, “Well, these things have got to have approximately this magnitude and the product of these things should be roughly like this”. Or according to our singular learning theory friends, the local learning coefficients should be small and that implies this thing about this. I wonder, non-mechanistic interpretability approaches: are they contenders for things that could be useful and you haven’t gotten around to them? Or do you think there’s a reason that they’re not going to be useful or what?\n\n**Jason Gross** (01:48:43): I think that if you could prove that the local learning coefficient has the particular value that it does, that would very nearly give you a singular learning theory-based compact proof, if you could prove that compactly. Because I think that would basically tell you what are the primary directions of variation. That basically tells you how to compress the network according to symmetries, and which individual things you have to run through the network. And then the proof that it has this local learning coefficient, is the proof that you get these large epsilon balls around these bits. To pick one example from what you said.\n\n**Daniel Filan** (01:49:20): Yeah, and unfortunately, from what I know of local learning coefficient estimation, it’s hacky and horrible.\n\n**Jason Gross** (01:49:27): And very expensive.\n\n**Daniel Filan** (01:49:28): And expensive. Yes. In general, does non-mechanistic interpretability seem like at all promising from this proofs perspective?\n\n**Jason Gross** (01:49:37): I think deep learning theory, to cherry-pick another example, I think it’s targeted at something slightly different, in that it’s looking a lot more at the training procedures. And plausibly when we move to probabilistic methods, when we weaken proofs, we’ll want to be able to say something about what distribution the parameters are drawn from. I think if we’re actually going for proofs, then well, it doesn’t matter how you train the network. You have the parameters.\n\n**Daniel Filan** (01:50:06): Yeah, so depending on how you want to be probabilistic, if being probabilistic is only coming from you randomly sampling some things in the network to look at, I guess you could imagine deep learning theory saying, “Oh, certain variances are going to be small or certain correlations are going to be large”. And you could imagine that informing your sample.\n\n**Jason Gross** (01:50:29): Yeah. I think another thing here is that when you’re forming your interpretation, I think these other methods have a lot of things to say. That’s like if you know something about which data points showed up a bunch in training and which ones didn’t show up, this might tell you something about where you should be looking. For example, in max of k, the network tends not to do a very good job when the maximum is very small, because there aren’t that many sequences with tiny maximum and we’re sampling them uniformly. And so in the same way, if you know properties of the data distribution, this should tell you what things you might expect the network to care about or not care about.\n\nThe limits of compact proofs\n----------------------------\n\n**Daniel Filan** (01:51:08): Right. Seems fair. The next thing I want to ask is, okay, so we have some compact proofs that are related to mechanistic interpretability. How far should I expect this approach to go? Am I going to get any sort of compact proofs about GPT-4’s behavior or DeepSeek R1 behavior, if we want to be up trendy and such?\n\n**Jason Gross** (01:51:39): Yeah, I think the question is how compact should we expect proofs to be, before the bounds become vacuous? And I think the answer is that realistically, we shouldn’t. Unless… I think if there are places where we want to deploy models that are extremely high stakes, where we’re willing to impose a bunch of cost and change the model that we’re deploying so that we can get proofs about them, I think we have a chance. I think we have a chance of shifting the models that we’re deploying to align with whatever partial interpretation we’ve done, so we can actually get proofs about them. And there, I don’t see any fundamental obstacles to getting to scaling proofs. Although, I think it will require a lot of work and be very difficult. I think in terms of getting proofs about the models that we’re actually deploying without training them to make them more interpretable, I don’t think that’s going to happen, unless your proof is basically just, I ran inference and I de-duplicated a couple of the cases that are extremely similar.\n\n(01:52:44): Although, I do want to say one more thing here, which is that we can see some of the aesthetic of compact proofs and what that says already about the large models: we should potentially expect DeepSeek-V3 to be easier to prove things about compactly than some of the models that are not doing this “mixture of experts” thing. Because so much of the model is unused on each data point, you can apply the compression aesthetic here and you can say, “Ah, because I am running only a fraction of the model on each data point, the length of the proof should be comparable, even just baseline, to something like a 40 billion-size model, rather than a 600-700 billion-size model.\n\n**Daniel Filan** (01:53:35): Right. I had hoped that the point of the compact proofs line of research is that it was going to help us get safety properties of models that are going to be really big. It seems like if we’re literally talking about compact proofs of really big models, there are two big difficulties. One of which is it’s hard to write proofs about really big things. And the other is it’s not obvious what predicate we want to prove about these big models. At least to me. I don’t know if that’s obvious to you.\n\n**Jason Gross** (01:54:07): Yeah. So I think that the second one is in a lot of ways much less of an issue and I can-\n\n**Daniel Filan** (01:54:12): Really? Why?\n\n**Jason Gross** (01:54:14): Okay, so there’s a couple of bits of evidence here. One of them is that we can get a lot… Well, okay, the more compact your proof is, the less you care about Goodharting on your theorem statement. And there’s always a baseline theorem statement that you can give, which is the model does as well as it does on the dataset we care about. And this doesn’t give you a safety guarantee with no automated oversight. But the thing that it does do is it says if someone gives you a sufficiently compact proof of this very weak proxy for safety… So a very weak proxy for safety might be something like: this other large model that I have looks at the output and says it’s safe.\n\n(01:55:02): And of course, you don’t want to rely on this as “this is what it means to be safe”. But if you can get a compact proof that says that the model is safe in this very weak sense and the proof is compact enough, you can learn a lot about the bits of the model that are relevant to this proxy and hopefully, those will be the same bits of the model that are relevant to the thing that you actually care about of safety.\n\n**Daniel Filan** (01:55:28): When you say you can learn a lot about that, by reading the proof and understanding what the proof is?\n\n**Jason Gross** (01:55:32): That’s right. Or having some automated process that translates from the compact proof back to intuition.\n\n**Daniel Filan** (01:55:38): And so it seems like maybe what’s going on there is… So yeah, it seems like this relies on a hypothesis which is “short proofs sort of generalize across things to be proved somehow”, or a little bit.\n\n**Jason Gross** (01:55:53): I think there’s this general trend/pattern/claim/hypothesis that is compression length is related to generalization.\n\n**Daniel Filan** (01:56:05): Yeah. So just concretely, I’ve got my latest greatest big model. It’s doing funky reasoning or something. Just really concretely, assuming I can get compact proofs, is my first proof something like “it models the data well” or “it gets the answer right on this dataset” or something?\n\n**Jason Gross** (01:56:36): Its perplexity on its training task.\n\n**Daniel Filan** (01:56:38): How does that assuage my concern about the model just doing tricky stuff on future data, or I’m trying to get the model to give me something that I want, but it gives me something that looks good instead of the thing that actually is good. I take something like the difference between looking good and being good on any given data point, and the risk of the model’s going to do something catastrophic on a future data point that’s hard to find: I take these to be the really difficult AI safety problems. I don’t know how getting a really good bound on perplexity on the training dataset is going to help me with either of those.\n\n**Jason Gross** (01:57:22): Yeah, so I think the way that it helps is that you can… So I agree that if it’s literally the training data, yes. But you can talk about larger distributions from which you’re sampling. For example, in the training data, if you didn’t sample every data point, you can get a bound still on the underlying set of training data, rather than just the data points that you happen to sample. And you can also talk about distributions that might include the ones that you care about. For example, if you’re not trying to do perplexity on the training data and instead you’re trying to be like, “the logits are whatever they are on sequences sampled uniformly of length up to this”. Or maybe you’re doing a different task that’s like, “I want to know that it never outputs a recipe for violence or a recipe for making a bomb or something like that”.\n\n(01:58:25): And you’re like, “Okay, my proxy for this is: on any sequence sampled uniformly of length up to this, if I then ask some other model, ‘is this the bad kind of output?’ it should say no.” And so now, you have this very general uniform input sequence description that is going to be way larger than your training data, and this very easy theorem statement about the output. And this is… so you can enlargen the distribution that you care about, as long as you can describe a distribution that includes the thing that you care about.\n\n**Daniel Filan** (01:58:57): The way you’re going to assuage me about rare inputs is enlarge the distribution. And of course, now we’re making the compact proof thing harder. But we enlarge the distribution and then we get some trusted model that’s able to verify stuff, that’s able to look at outputs and say, “Is this super scary or is this not super scary?”\n\n**Jason Gross** (01:59:21): So if you had a model that you actually trusted, then we’d be in great shape. And I think the thing that I’m claiming is that we don’t even need to trust the other model all that much. And maybe that’s related to the other point that you brought up about the difference between things that look good and are good. And my answer for that part is that if we understand what goes into making something look good, that gives us more levers and more surface area on that gap between “looks good” and “is good”.\n\n**Daniel Filan** (01:59:52): Right. So the hope is we read the compact proof for why the model produces stuff that looks good on this dataset. And by reading that, we learn… Because it’s compact enough, we can read it and say, “Oh, it’s because it actually is good”. Or instead we can say, “Oh, it’s because it’s injecting morphine into me,” or whatever it is.\n\n**Jason Gross** (02:00:14): Right. And moreover, we know that because it’s a proof, because it’s compact, we know that these are the principal axes of the explanation. There’s a sense in which if you tell me why something looks good and there wasn’t that much optimization power poured into making it, I’m not that worried about divergence between “looks good” and “is good”. It’s only when there’s a lot more optimization power poured into it. So my hope is that by getting these principal components of variation, knowing the most important ways that the model is actually doing the thing, we can tailor how it computes it to how much optimization pressure was poured into making it.\n\n**Daniel Filan** (02:00:59): Sure. It’s still going to be the case… if I have a smart AI, presumably one way or the other, the best explanation of what’s going on has got to involve a lot of optimization pressure. Because it’s thinking super hard, because it’s solving problems that we couldn’t solve.\n\n**Jason Gross** (02:01:17): If that is true, then we’ll have a general understanding of how optimizers work that’s much beyond what we currently have. So this is-\n\n**Daniel Filan** (02:01:25): If we can have a compact proof.\n\n**Jason Gross** (02:01:26): This is the gap between short program and short computational trace. A loop is a very good description of a very good short program for generating a thing, but it’s not a very good cheap compression of the actual computational trace.\n\n**Daniel Filan** (02:01:43): Right. So somehow, what I’m getting from this is somehow having the statement “you can get a compact proof of some properties”, that’s actually an incredibly [OP](https://en.wiktionary.org/wiki/OP#Adjective) statement, that actually really gives you a bunch. And then if you can do that, then the other problems just pale in comparison to it.\n\n**Jason Gross** (02:02:01): Yeah, and there is-\n\n**Daniel Filan** (02:02:02): And now I’m getting way more pessimistic about it.\n\n**Jason Gross** (02:02:05): And I think that’s warranted. I want to give a similar thing with the strength of proof, where saying that you have a proof in some ways, even if it’s not compact, is a very OP thing. The example I like to give for this is that say you want to establish safety of your web browser. A very, very, very mild form of safety is that it never prints a document without you clicking on the print button. If you’ve actually managed to prove this, you have also proven that there’s no arbitrary remote code execution, because remote code execution can lead to printing arbitrary documents without you clicking on the print button. And so just by establishing a lack of this by proving that there’s a lack of this very limited behavior, you’ve ruled out a large chunk of behaviors. And if you weaken proof, if you’re like, “Oh, it’s very rare,” that doesn’t actually give you the property that you want.\n\n**Daniel Filan** (02:02:59): So going back up, so asking like, okay, where are we going with this compact proof direction? One thing is how we can apply it to big production-ready models. So one possibility is we somehow figure out a way to get things that are compact and they’re close enough to proofs that we have the nice properties of compact proofs. Maybe we figure out heuristic arguments, maybe we add in some probabilism. Maybe that helps. One other possibility is we do enough compact proofs that we realize what we need to happen to make mechanistic interpretability good, and we do those mechanistic interpretability things, and we don’t get proofs, but maybe, somehow we get the benefits of compact proofs without actually having the compact proofs. I’m a little bit worried here that this story involves two contradictory assumptions.\n\n**Jason Gross** (02:03:56): I think some of the benefits we can definitely get without having proofs. So one of the benefits of proofs is that you’re covering everything. You look at crosscoders - I keep going back to that - but you look at crosscoders and you’re like, “Great, we get an explanation of some things. Is it everything? Are we missing things? How much are we missing?” And some of that is in the reconstruction loss. But you’re like, “Okay, I managed to get a crosscoder that has no reconstruction loss, zero error. Have I got everything? Am I missing things?” And I would claim compact proofs has the answer to this. It says, “Yes, the thing that you’re missing is the notion of how features are interacting”. And so even without actually getting the proofs, we can answer questions that are like: what things might we be missing in explaining the behavior of the model?\n\n**Daniel Filan** (02:04:40): So the story - trying to make this as end-to-end as possible - the story is something like: we do enough compact proofs that we figure out how mechanistic interpretability has to look to give us reasons for model behavior that are proofish enough that once we understand those reasons, even for a predicate that’s slightly different than what we care about, it just gives us what we want to give us high confidence in the predicate that we actually care about.\n\n**Jason Gross** (02:05:08): I think that’s right.\n\n**Daniel Filan** (02:05:09): Okay. So that’s one direction forward. Another direction forward is; we have the compact proof benchmark - the GPT-4 compact proof benchmark - and then we train some other model to do really well on this benchmark, and then our computers give us compact proofs for us. The worry about this is that I read the proof and I feel unenlightened at the end of it… Maybe if the proof is compact enough, compactness is just the same thing as comprehensibility perhaps.\n\n**Jason Gross** (02:05:39): Okay, there’s two points I want to make here. One of them is that if we actually want to do this, the other thing we need to do is we need to make sure that we can check the proof by machine, we have a machine-checkable proof that we can generate. And I have a bunch more to say there, but maybe later. The other thing is that what this gives us is some assurance that the explanation that we’re looking for exists. That if you get a compact enough proof, there’s still some problem. You can’t necessarily be enlightened just by reading the proof as it stands. But we’ve shifted it from “does the explanation even exist? Is the model even compressible? What’s going on?” to a problem of “what is the gap between the language of math and the language of your brain?” And in some sense, this is a translation problem.\n\n(02:06:26): And I’m optimistic that if we get a compact enough proof, it might need to be very compact. We might need to split out the part that is about what the idealized model is doing and the part that is about how the actual model comes to match the idealized model. We might need to do something… There was this cool [blog](https://www.lesswrong.com/posts/zbebxYCqsryPALh8C/matryoshka-sparse-autoencoders) [post](https://www.lesswrong.com/posts/rKM9b6B2LqwSB5ToN/learning-multi-level-features-with-matryoshka-saes) recently about Matryoshka SAEs that I think gives you the whole Pareto frontier all at once because you get SAEs or crosscoders of different sparsities all trained at the same time. We might need to do something like that so that we get the whole Pareto frontier and can pick whatever point we want, to get a very compact proof. But supposing that we can do this, the hope then is that the language of the brain and the language of math are not so different that there’s any deep problems beyond just “the explanation didn’t fit in a person’s head when it was too long”. And so then we can translate it from… We can teach the person to understand what the explanation is.\n\nGuaranteed safe AI, and AI for guaranteed safety\n------------------------------------------------\n\n**Daniel Filan** (02:07:33): Next: so the first thing I want to ask about that’s related is: so there’s this hot thing on the block, “guaranteed safe AI”, that some people I know are very excited about. I actually find myself a little bit unclear about what the details of it is. So as far as I can tell, it’s basically “we want to prove that AIs are doing safe stuff”. And there’s [this paper “Towards Guaranteed Safe AI”](https://www2.eecs.berkeley.edu/Pubs/TechRpts/2024/EECS-2024-45.pdf) by a list of luminaries. I believe the lead author is [David ‘davidad’ Dalrymple](https://x.com/davidad), although I could be wrong about who that lead author is. And I think they’re a little bit ambiguous about whether you’re proving safety properties about an AI specifically or about the outputs of the AI. Maybe the AI writes code and you’re proving stuff about the output code.\n\n(02:08:28): So this seems obviously pretty related to the compact proofs paradigm. I’m wondering if there’s anything you want to say about that relation, and perhaps maybe one direction is: okay, given that you can prove things about the AI itself or about the outputs of the AI, which one makes more sense to target for proofing?\n\n**Jason Gross** (02:08:52): I don’t know how much of a leading question you intended that to be. But I think it definitely makes a lot more sense to prove things about the outputs than to prove things about the AI. I think one of the upshots from doing compact proofs and making them so formal is in some sense how hard it is to do it. And I still think that this is a great target for when you’re confused about mech interp or if you want to pour arbitrary amounts of optimization power into finding your mech interp and extracting understanding from it.\n\n(02:09:24): If the thing that you’re looking for is guarantees about the behavior, I think in most settings, especially the highest stakes ones, we should be aiming to have as predictable systems as possible. If we’re going for guarantees about the system, we probably don’t want to stick a fully general language model in some critical part of that system. And it would be much simpler to have the general coding model write some code that mostly handles it but is a much smaller computation to run, both for efficiency reasons and for ease of proving reasons.\n\n**Daniel Filan** (02:10:03): So, if I think about very important systems that exist in the world right now, it seems like… I don’t know that much stuff, this is a recurring bottleneck, but I imagine that a lot of them are not fully automated and a lot of them do have humans in the loop just doing stuff. And why is that? Well, I think partly it’s just because it’s hard to write things that are fully automated that actually do what we want.\n\n(02:10:29): So, maybe a trivial example of this is flying. We still have pilots. My impression is that that’s because takeoff and landing are actually hard. We still have humans in towers at airports to check to make sure that airplanes don’t crash into each other. And my impression is that that’s not just a make-work program. My impression is that humans are doing this better than a computer program could, if we wanted to verify it. And so does that not give some amount of pause for… Maybe it does seem like in high-stakes situations, at least to get really good average performance, maybe we do want this big messy uninterpretable thing that we can’t prove stuff about in the middle.\n\n**Jason Gross** (02:11:17): Maybe it’s a failure of my imagination to imagine how powerful the proof-generating systems are going to be in the future. I think in some ways, it’s going to be a hard target to hit because we’ll need not just systems that understand well enough to do these tasks, but we’ll also need systems that understand those systems well enough to explain how it comes to be the case that doing those tasks in the way that they’re doing them has the good properties that we want. And maybe we’ll need this in high-stakes situations. That seems entirely plausible. And I think if we can get this, maybe we can push it all the way to the level of proofs.\n\n**Daniel Filan** (02:11:54): Yeah, I guess when it’s tricky to figure out, you take the current world and then you improve intelligence and you also improve ability to prove stuff, and it’s not clear to me how that changes the equilibrium of what’s optimal and what isn’t. I don’t know if there’s anything particularly smart to say, unless you happen to have thought about this exact question a lot.\n\n**Jason Gross** (02:12:22): I’ve thought a bunch about the nearer term. What does it look like? How do things change as we automate? As we add more automation around code, and as we increase our ability to automate proofs, let’s say, how does this reshape dynamics locally? And this looks a lot more like the automating outputs version than the automating complicated messy systems.\n\n**Daniel Filan** (02:12:53): Right. Yeah. I guess either way, it still does seem like proving the outputs is going to work better than proving the messy intermediate systems.\n\n**Jason Gross** (02:13:04): Yeah. So (a) I think there’s an enormous amount of benefit we can get by proving the outputs and by automating that more. And I think that this will be a much easier task than proving the messy systems. And I think that the place that I would start for proving the messy systems when we want to do that is that we absolutely shouldn’t first commit that we’re deploying the system exactly as it is in all of its messiness and then try to prove something about that. We should instead fine-tune it, simplify it, extract understanding from it, and then have some AI code up from the ground up, some version of it that follows the same principles but is much easier to prove things about.\n\n**Daniel Filan** (02:13:47): Yeah. So, this gets into another research interest of yours: if we want to prove stuff about these outputs, how are we going to get better at doing that as AI gets better?\n\n**Jason Gross** (02:13:59): Yeah. A little bit of the background here: I’m not sure how many of the listeners are familiar with proof assistants and formally verified code.\n\n**Daniel Filan** (02:14:08): Probably few enough that you should explain.\n\n**Jason Gross** (02:14:10): Okay. There’s been some [exciting news recently](https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level/) about DeepMind AlphaProof getting IMO silver.\n\n**Daniel Filan** (02:14:21): I thought that was [just on geometry problems](https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/). Is it on geometry problems or just overall?\n\n**Jason Gross** (02:14:26): That was a year and a half ago.\n\n**Daniel Filan** (02:14:28): Okay. Well, I guess I’m not-\n\n**Jason Gross** (02:14:30): There was AlphaGeometry, and then this past summer there was AlphaProof. So, they still have AlphaGeometry, and then AlphaProof, the way that it works is it takes the non-geometry problems, formalizes them in a proof assistant called [Lean](https://en.wikipedia.org/wiki/Lean_(proof_assistant)), and attempts to generate proofs and uses the proof assistant to check is the proof valid.\n\n**Daniel Filan** (02:14:53): Right. Okay, sorry. Now that I think about it for a second, I do definitely remember someone being like, “Oh, IMO silver.”\n\n**Jason Gross** (02:14:59): Yeah. I expect next year we’ll have a system that gets the gold.\n\n**Daniel Filan** (02:15:02): Okay. Sorry, next year as in during 2025 or during-\n\n**Jason Gross** (02:15:07): The next IMO: this year.\n\n**Daniel Filan** (02:15:09): Okay. Yeah. All right.\n\n**Jason Gross** (02:15:11): I think models are improving fast enough that we should expect IMO gold to be taken by AI.\n\n**Daniel Filan** (02:15:18): Okay, so you were beginning an explanation and I derailed you. So, we’ve had this exciting news about DeepMind getting IMO silver.\n\n**Jason Gross** (02:15:24): The reason I think that they’re using Lean or an automated proof system here is that models, at least as of that point, were not good enough that the AI-based peer review was enough to catch errors in the mathematics, and so you couldn’t train them to give proofs that the judges would accept.\n\n**Daniel Filan** (02:15:46): Right. And to be clear, when you say automated proof assistant, the thing I’m thinking of is just a programming language that lets you express proofs. So, you write down a proof in this programming language and you check if your program compiles, and if your program compiles, then the proof worked, and if the program didn’t compile, there’s a hole in your proof.\n\n**Jason Gross** (02:16:03): That’s right. So, mathematics is starting to use proof assistants to prove things or to check research… Proofs that are done about software systems rather than mathematics for a long time have used proof assistants, and the reason for that is that the proofs that software behaves the way you expect are simultaneously less complicated in some sense than IMO proofs in that there’s less interesting going on.\n\n(02:16:38): And at the same time, they’re way more tedious to check and way longer because there are things like you have 100 variables or 1,000 lines of code or 100,000 lines of code and you’re like, I have 10 cases, 100 cases, 100,000 cases. Break it down into case analysis, consider all the different cases and all the different interactions through this program, and establish that at no point do I access past the end of bounds of some array. And there’s nothing super interesting going on here. In some sense, the most interesting thing is figuring out for loops, what is the property? How do you describe a property in enough generality that it holds at every iteration of the loop? But this is very simple compared to Fields Medal-level mathematics or IMO problems even.\n\n**Daniel Filan** (02:17:27): Yep. And the proof assistants you’re talking about for proving programs run correctly, should I just be thinking about type systems, or I guess there’s also things in the compiler that are doing this?\n\n**Jason Gross** (02:17:42): Are you asking about the Curry-Howard isomorphism?\n\n**Daniel Filan** (02:17:44): No, I just mean when you say, “We’re currently using proof assistants to check,” what things do you mean?\n\n**Jason Gross** (02:17:54): Okay, so you can get some properties just by things like the type-checking that the [Rust](https://en.wikipedia.org/wiki/Rust_(programming_language)) compiler does, and then there are some programming languages where you’ve extended the power of the type system such that program passing the type checker, compiling, is enough to establish the theorem that its type corresponds to. And these languages are expressive enough that anything you do in math can be expressed as a type in this programming language.\n\n**Daniel Filan** (02:18:27): Sure.\n\n**Jason Gross** (02:18:28): Does that answer your question?\n\n**Daniel Filan** (02:18:30): I guess it does well enough. But so you were saying, mathematicians don’t really use proof assistants like programmers have out of necessity because it’s more boring to do the proof with a human than it is for a computer.\n\n**Jason Gross** (02:18:44): Yeah. And the thing that I’ve been seeing is that for these simpler proofs of programming verification that are much longer but don’t contain as much nuanced reasoning, it seems like frontier models are already basically good enough to write them. Or at least when you give them in-context examples and you want them to write something that’s vaguely similar, it seems like they can basically do it. I’ve been playing with o1, o3-mini just dropped.\n\n(02:19:12): And especially when you give them feedback from the proof assistants, you’re like, “Ah, the proof didn’t work.” The proof assistant tells you in some amount of detail, locally at least, why your proof didn’t work. So, you feed that error message back. And 4o often makes mistakes in interpreting the error message. It looks like o1 is much better at understanding how the local error relates to the overall structure of the proof.\n\n(02:19:38): And so it seems like our frontier models are basically good enough to write these sorts of proofs when there’s not deep, complicated reasoning going on. And it seems to me at this point, it’s mostly an engineering challenge to go from models that can work with large code bases to models that can prove things about large code bases.\n\n**Daniel Filan** (02:20:04): What is the engineering challenge? You can use these text editors that have big models that look at your code base. Can I just type, “Please prove XYZ safety property?” I guess I have to think about the safety property. Once I think about it, why can’t I just type, “Please prove such and such safety property,” and have it happen?\n\n**Jason Gross** (02:20:31): I think that in terms of model capability research, I think we are basically there. I don’t think we need to scale to bigger models or better general reasoning capabilities. I think that if you ask o3 to explain why your program satisfies your property, I think that its reasoning capabilities are probably basically good enough that any sorts of program reasoning that have been done in the existing literature, it can give you a basically correct skeleton of why your program behaves the way that it should be behaving. And you can see this already. If you ask it to explain Python code, it gives - even 4o, sometimes even 3.5, give pretty decent explanations of how code works.\n\n(02:21:18): And so then the challenge is just how do you take this basically correct English explanation and turn it into a full proof? And I think the engineering challenge is there. There’s a couple of them. I think one of them is that there’s less training data on proof assistants than Python. So, the models just haven’t been… We don’t necessarily have the data to fine-tune the models to get them to be good enough to do this super easily.\n\n(02:21:46): There’s another issue that is if you get the proof almost right, it’s not right. I think this is shown a lot in the evolution of capabilities, where sentiment analysis is one of the first things that models were able to do well, because if you get your sentiment analysis a little bit wrong, it’s still basically right. If you get your code a little bit wrong, you have some tolerance. And often you can make local fixes that correct for issues in other places, and maybe you get a more cludged-together system, but it still mostly works. But if you get your proof a little bit wrong in one place, if you get your statement a little bit wrong in one place, there’s not necessarily any way you can correct for this in some other place. And so the target you have to hit is much more narrow, and I think we have to get enough data and basically train proof repair models so that they can take the feedback from the proof assistants and hit this narrow target from getting somewhere close. And actually, the richness of the proof assistant error messages is part of what makes me hopeful about being able to go from pretty close to all the way there.\n\n(02:22:56): There’s another engineering challenge in terms of the sheer scale of what we have to handle in that traditionally, program verification, if you look at how many lines of code is the program that I’m proving something about, how many lines of code did I need to write to verify, you have a blow-up factor of 10X to 100X.\n\n**Daniel Filan** (02:23:12): Okay. It’s a lot.\n\n**Jason Gross** (02:23:14): It’s a lot. It’s a lot, especially when you’re like, “Yeah, I would love to prove some properties about Linux”. Linux is a 30 million-line code base. But if we can automate the generation of these lines of code, it matters a lot less.\n\n**Daniel Filan** (02:23:27): Yeah. I don’t know, I’m not that worried about storing the file on my computer. It’s like, can I get the proof? And if I just have to buy a slightly bigger hard drive, I’m like, “All right, well, I’ll deal with it”.\n\n**Jason Gross** (02:23:39): Right. And even if you have to pay more for the computation, as long as you’re not having to pay PhD-level human engineers or human research engineers that are PhD level, one of the benefits of AI is that we can massively scale things. And so if we care enough about getting verified Linux, even if we have to get 300 million or three billion lines of code, it still is the case that we can do this just by pouring money into the computation, potentially.\n\n(02:24:14): And the final engineering challenge - this is what in some sense [my PhD thesis](https://dspace.mit.edu/bitstream/handle/1721.1/130763/1252059492-MIT.pdf?sequence=1&isAllowed=y) was about - is that we may need to improve proof assistants because right now, proof assistants themselves doing the proof-checking don’t necessarily scale to the sorts of scales that we’d want to see with this. Where the proof-checking time often scales superlinearly in the number of lines of code that you’re verifying or the number of lines of code in a function, where it might be the case that if you want to verify a 10-line function, it’ll take a couple seconds to a couple minutes. And if you scale the same proof strategy, it might scale exponentially in the number of lines of code. And you’ll get very fun numbers, like from my PhD thesis, things like, “Ah, if we wanted to do this, it would take over 4,000 millennia to verify this proof”. And that’s too long.\n\n**Daniel Filan** (02:25:05): How much of that is… You shouldn’t necessarily expect life to be linear in lines of code if some lines of code interact with other lines and they’re weird loops and stuff. So, how much of that is just messy interaction versus “proof assistant is dumb”?\n\n**Jason Gross** (02:25:19): It’s almost all “proof assistant is dumb” because basically the actual variables that you scale in are things that are irrelevant to your domain of study. There are things like, how many hypotheses do you have? How many different equations about the program are you tracking simultaneously? Or how many times have you created a bit of the proof and said, “I’ll fill this in later?” at the same time. And these are not things that we should be scaling superlinearly in.\n\n**Daniel Filan** (02:25:57): That difficulty… I imagine there must be some community of people who really like proof assistants who are working on that. Is that on track to be done, or?\n\n**Jason Gross** (02:26:08): Do you want to guess how many performance engineers have worked on [the Coq proof assistant](https://en.wikipedia.org/wiki/Rocq_(software)) across its entire lifetime, which is 40 years or so?\n\n**Daniel Filan** (02:26:16): 40 years? Oh, I didn’t realize it was that old.\n\n**Jason Gross** (02:26:20): Roughly 40 years.\n\n**Daniel Filan** (02:26:21): Okay. How many performance engineers have worked on it? Okay, so all right, I’m going to actually make a guess. I think, isn’t there a thing where it’s like 30 people who are paid where their full-time job is to make the Python programming language good? So, if I take that and I’m like, all right, it’s probably fewer than 30 because Coq is much smaller than Python. It might be three, if we’re lucky. And so let’s say they change off every 10 years or so, so that’s three times four, which is 12. What fraction of those are performance engineers? I’m going to say optimistically, I think it’s three people.\n\n**Jason Gross** (02:27:14): That’s not bad. I think it’s somewhere between zero and two.\n\n**Daniel Filan** (02:27:19): Damn. All right.\n\n**Jason Gross** (02:27:20): Is my rough estimate. But this should give you a sense of how much engineering effort has been put into performance of proof assistants. If you’ve only had zero to three people working on performance of the system, and most of the system has been written by professors and post-docs and PhD students, we should expect that there’s a lot of low-hanging fruit.\n\nJason and Rajashree’s start-up\n------------------------------\n\n**Daniel Filan** (02:27:44): So, if this would be nice to do and these are the technical challenges, do we just get a bunch of people to work on the technical challenges? Do you have a thing you would like to announce on this podcast? Or what’s the game plan?\n\n**Jason Gross** (02:27:59): So, Rajashree \\[Agrawal\\] and I are founding a startup, not yet sure whether it’ll be for-profit or not-for-profit, that is basically aimed at solving this problem. That is like, we’re going to have AI-generated or -assisted code proliferation soon. Let’s get all of our ducks in a row. Let’s figure out how to scale automated verification with AI assistance to handle all of the challenges associated with this and reap all the benefits of being able to secure our existing systems better, being able to evolve our code bases while ensuring security. What does this look like? What can we do with this? How do we imagine a future where we can get proofs alongside our changes in code automatically?\n\n**Daniel Filan** (02:28:59): So, you’re starting a startup, and basically the thought is you’re going to hire some people. It’s going to be their full-time gig. Where do you start?\n\n**Jason Gross** (02:29:08): I think there’s a couple places that we are currently looking at to start. One of them is, what are the lowest hanging fruit that we can pick? What are the most flashy, impactful demos that we can make to build interest in this and scale this up? And I think for that, depending on what community we’re targeting, there are interesting things like: can we build a system that any time you make a code refactoring change, you can get an automatic proof that the behavior of your overall end-to-end system has not changed with the refactoring?\n\n**Daniel Filan** (02:29:49): That seems hard.\n\n**Jason Gross** (02:29:50): What seems hard about it to you?\n\n**Daniel Filan** (02:29:51): Well, it just sounds hard. I don’t know. It’s like you’ve got a big code base. Maybe refactoring is referring to something more limited than I’m imagining. Yeah, I don’t know, maybe I’m underrating how good large language models are at this, but-\n\n**Jason Gross** (02:30:04): I’m excited. I’m optimistic. I think that in some sense, there’s nothing interesting going on when you refactor. If you’re drastically changing the logic by which things are happening, of course there’s interesting things going on there. But if what you’re doing is largely a refactoring of functionality, things that you normally think about in refactoring where you’re not like, “I completely changed the method by which we’re computing this thing, and here’s this mathematical theorem that claims that these things are…” If you’re not doing something like that, there shouldn’t be anything interesting going on.\n\n(02:30:35): The theorem statements are also relatively easy to phrase in that it’s just equivalence between these two programs. And in that sense, it could be the case that existing model-checkers are already enough to largely do this mostly automatically, and so I think this is mostly a demo that the system… So, model-checkers are not going to scale to proving more interesting properties of enormous systems, but the general-purpose program verification proof assistant-based system can. And so this is more to be thought of as a simple demo that this sort of thing could work on this thing that already seems impressive when you hear about it.\n\n(02:31:18): I think there’s another… If we want to push forward the “how capable are models? What sorts of things can we expect them to prove?”, I think the thing to look at there is that we expect that reasoning capabilities are going to keep improving. We’re not trying to be on the frontier of improving reasoning capabilities, and that wouldn’t necessarily be good for safety anyway. The problem that we want to solve then is going from o1, o3, [R1](https://arxiv.org/abs/2501.12948), whatever, describes how the program works, getting that to be fully formal. And the even simpler version of this is someone has done some task in one proof assistant. Can we automatically translate that to a different proof assistant? Where, here, we know that the reasoning is right because it’s checked by one proof assistant. Is that enough of a template to verify the code in another proof assistant?\n\n**Daniel Filan** (02:32:14): Okay. Sorry, I think I may be missing context on why that helps with the overall goal. Is it roughly like: if you can check “is this proof equivalent?”, translate this proof into this proof, maybe that helps you translate this proof sketch into this proof?\n\n**Jason Gross** (02:32:32): Yeah, so the models are pretty decent at translation. In my experience, I would be extremely surprised if they can’t do something like Lean to Coq or Coq to Lean translation with the right fine-tuning data. This is something that I’m like, they should definitely be able to do this. It shouldn’t be that much work to get them to do this.\n\n(02:32:53): There’s a much more ambitious thing that’s like, “Here’s a verified C compiler. Here’s the source code of the Rust compiler. Please give me a verified Rust compiler”. And I’m like, well, is o3 enough to do this? Maybe. Maybe not. If o3 gave me the right reasoning, are the current models enough to translate this into formal proofs? Maybe. Maybe not. I think somewhere between these things or between the “translate Coq to Lean” and “here’s Linux, please verify it,” we’ll hit the bottleneck. And so the thing that I want to look at here is: let’s aim at these very ambitious problems, let’s figure out where capabilities currently are in going from good reasoning to formal proofs, and then let’s build our way up with synthetic data to close that gap so that as the reasoning gets better, we can automatically verify larger and larger, more and more complicated programs.\n\n**Daniel Filan** (02:33:57): That’s pretty interesting. I hope that works out. Maybe before we wrap up, this whole approach of compact proofs, let’s try and prove stuff either about models or about model outputs, is there anything you wish that I had asked that I did not get around to?\n\n**Jason Gross** (02:34:16): No, I think you hit basically everything.\n\nFollowing Jason’s work\n----------------------\n\n**Daniel Filan** (02:34:19): Okay, cool. Thanks for sharing with me. If listeners are interested in following your work, how should they go about doing that?\n\n**Jason Gross** (02:34:27): Yeah, thanks for taking the time to interview me. I think currently, the best way is either following [my GitHub](https://github.com/jasongross), which I guess will be linked, Jason Gross. I have a website, [jasongross.github.io](https://jasongross.github.io/), that may eventually have a blog. Currently, the infrequent times that I blog are on [Alignment Forum](https://www.alignmentforum.org/users/jason-gross). And I think for more prompt updates currently, probably just reaching out to me by email is best. And my email is on my website.\n\n**Daniel Filan** (02:35:03): Okay, sure. Well, thanks very much for coming.\n\n**Jason Gross** (02:35:06): Yeah, thanks for having me.\n\n**Daniel Filan** (02:35:08): Special thanks to Jonathan Ng. This episode is edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. This episode was recorded at FAR.Labs. Financial support for the episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with patrons such as Alexey Malafeev. To read transcripts, you can visit [axrp.net](https://axrp.net/). You can also become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast) or give a one-off donation at [ko-fi.com/axrpodcast](https://ko-fi.com/axrpodcast). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nHow do we figure out whether interpretability is doing its job? One way is to see if it helps us prove things about models that we care about knowing. In this episode, I speak with Jason Gross about his agenda to benchmark interpretability in this way, and his exploration of the intersection of proofs and modern machine learning.\n\nTopics we discuss:\n\n * Why compact proofs\n * Compact Proofs of Model Performance via Mechanistic Interpretability\n * What compact proofs look like\n * Structureless noise, and why proofs\n * What we’ve learned about compact proofs in general\n * Generalizing ‘symmetry’\n * Grading mechanistic interpretability\n * What helps compact proofs\n * The limits of compact proofs\n * Guaranteed safe AI, and AI for guaranteed safety\n * Jason and Rajashree’s start-up\n * Following Jason’s work\n\nDaniel Filan (00:00:09): Hello everybody. In this episode I’ll be speaking with Jason Gross. Jason is a researcher interested in mechanistic interpretability and software verification. During his PhD, he was on the team responsible for verifying the code used in encryption for the HTTPS protocol and he’s also on the development team of the Coq Proof Assistant. Links to what we’re discussing are in the description and you can read a transcript at axrp.net. You can also support the podcast at patreon.com/axrpodcast.\n\n(00:00:35): Well Jason, welcome to the podcast.\n\nJason Gross (00:00:37): Thank you. I am excited to be here.\n\n\nWhy compact proofs\nDaniel Filan (00:00:40): Cool. So I guess we’re going to talk about this line of papers that starts with this paper “Compact Proofs of Model Performance via Mechanistic Interpretability”. You’re the lead author and then there’s a bunch of other authors that I don’t want to read on air, but can you give us a sense of just what’s the idea here? What’s the theme?\n\nJason Gross (00:01:02): Okay. You can think of doing mech interp as compressing explanations of large models. And the takeaway from this paper is that you ca",
      "wordCount": 26808
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "FJ3MGb684F88BoN2o",
        "name": "Formal Proof",
        "slug": "formal-proof"
      },
      {
        "_id": "vJRAKhJQCL3uvaty3",
        "name": "Guaranteed Safe AI",
        "slug": "guaranteed-safe-ai"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "juH8JCBjf6zjdNNq2",
    "title": "AXRP Episode 38.8 - David Duvenaud on Sabotage Evaluations and the Post-AGI Future",
    "slug": "axrp-episode-38-8-david-duvenaud-on-sabotage-evaluations-and",
    "url": null,
    "baseScore": 13,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-03-01T01:20:04.778Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/OKekclKMgz0)\n\nIn this episode, I chat with David Duvenaud about two topics he’s been thinking about: firstly, a paper he wrote about evaluating whether or not frontier models can sabotage human decision-making or monitoring of the same models; and secondly, the difficult situation humans find themselves in in a post-AGI future, even if AI is aligned with human intentions.\n\nTopics we discuss:\n\n*   [The difficulty of sabotage evaluations](#difficulty-sabotage-evals)\n*   [Types of sabotage evaluation](#types-sabotage-eval)\n*   [The state of sabotage evaluations](#state-of-sabotage-evals)\n*   [What happens after AGI](#what-happens-after-agi)\n\n**Daniel Filan** (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area [Alignment Workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at [axrp.net](https://axrp.net/). And as usual, if you want to support the podcast, you can do so at [patreon.com/axrpodcast](https://www.patreon.com/axrpodcast). Well, let’s continue to the interview.\n\n(00:28): All right. I’m chatting with David Duvenaud. Hello, David.\n\n**David Duvenaud** (00:32): Hi, Daniel. Thank you. It’s an honor to be here.\n\n**Daniel Filan** (00:34): So for people who aren’t familiar with you, who are you and what do you do?\n\n**David Duvenaud** (00:40): Sure. So for the last little while, since 2016, I’ve been a professor at the University of Toronto, doing probabilistic deep learning. Maybe I’m best known for [neural ODEs](https://arxiv.org/abs/1806.07366), or some early work on graph neural networks. And then around 2017, me and my colleague, [Roger Grosse](https://www.cs.toronto.edu/~rgrosse/) decided we were finally going to actually try to contribute to AI safety, and both took on PhD students. And gradually, this journey of “let’s try to work more on AGI safety” led to me just doing a year and a half-long sabbatical at Anthropic that I just finished this week.\n\n**Daniel Filan** (01:12): Before we talk about your work, we’re currently at this alignment workshop being run by FAR.AI. I’m wondering, how are you finding the workshop?\n\n**David Duvenaud** (01:19): I mean, it’s wonderful. I’ve been to a bunch of these over the years. And now this is the first one where I feel like I’m sitting on a serious body of technical work to talk about, which is nice. Previously, I’ve been this AGI-interested mainstream academic that I think the community was hoping I would join them and lend legitimacy. And so I finally actually was able to take the plunge in a serious way.\n\nThe difficulty of sabotage evaluations\n--------------------------------------\n\n**Daniel Filan** (01:42): Yeah. Now that you’ve taken the plunge, I understand that you’ve recently put out a paper. Can you remind me of the title of it?\n\n**David Duvenaud** (01:48): Sure. It’s called [“Sabotage Evaluations for Frontier Models”](https://arxiv.org/abs/2410.21514).\n\n**Daniel Filan** (01:51): Can you tell me a little bit about what this paper does?\n\n**David Duvenaud** (01:54): Sure. So this paper covers the work of most of this team within Anthropic called Alignment Evals that I was running for most of the time that I was there. It’s a collaboration also: [Holden Karnofsky](https://en.wikipedia.org/wiki/Holden_Karnofsky) and Buck Shlegeris also helped us flesh out this series of evaluations that try to help make the case that a model could not be secretly sabotaging human decision-making or monitoring or evaluation of other capabilities.\n\n**Daniel Filan** (02:22): So how would you try and establish that? It seems like a rough property to try and get a handle on.\n\n**David Duvenaud** (02:28): It’s really rough. And it was kind of stressful, and we spent a lot of time thrashing conceptually about exactly what sort of case we needed to make and how we were going to make it. The place we started with was from more narrow evals, to say, “Okay, can the model coordinate with other copies of itself? Or do something at a very low frequency so that it could do it rarely enough that we wouldn’t notice?” Stuff like that.\n\n(02:54): And this was something that we could certainly make evals for, but the problems came (a) from not knowing how hard to make them. So if we’re trying to see whether machines can make a Schelling point between themselves without communicating: this is basically impossible if you have no shared context, and it’s basically trivial if you have a lot of shared context. Same for other things, like convincing someone of a false statement, something like that.\n\n(03:19): And then the second reason that this was really unsatisfying was trying to make safety cases based on these, and trying to make the case “Okay, the model needs these four capabilities in order to pull off some sort of heist or sabotage or mess up something important. We’ve measured each of these, and that they’re each at these different levels.” And then to try to combine those levels into a strong case that the model could never do something dangerous is very rough. It’s like saying you need level four lockpicking and level five social engineering in order to rob a bank. Well, it probably doesn’t work that way. We don’t want to have to make strong assumptions about what route the attacks are going to take.\n\n(04:07): But instead… So eventually the direction that we moved more towards was these more end-to-end holistic evaluations that try to measure more of an entire attack. And there’s a few directions in which they’re more realistic. One is: we try to include oversight in the eval. So it’s not just “can you write a cyber attack?” but “can you write one in the context of you’re supposed to be writing helpful code, and there’s also going to be someone who’s reviewing your code to see if anything looks suspicious?”\n\n(04:36): So this is much harder. And it’s harder in a bunch of different ways, where you have to be able to estimate what someone else is going to think of your attack. You might have to bide your time, in the sense that you might have hundreds of thousands of opportunities during a deployment to do an attack and you really have to guess “is this really my best opportunity or should I be waiting for longer, or am I going to be too conservative?”\n\n(04:59): It’s also easier for the models in the sense that there might just be a few rare opportunities that don’t require much skill, and someone’s basically saying, “Oh, here’s my code that has a security vulnerability. Please set the flag to false,” or something like that. You never know. You want to provide a really rich attack surface so that there’s ways for you to be surprised by clever strategies that the model thought of that you didn’t.\n\nTypes of sabotage evaluation\n----------------------------\n\n**Daniel Filan** (05:23): Can you give us a feel for concretely what… Actually, is it one evaluation, multiple evaluations? Concretely, what does it look like?\n\n**David Duvenaud** (05:33): Yeah. So we started off with… Maybe we were playing with around five or 10 when we were doing our conceptual thrashing, and we ended up with four evaluations that tried to address roughly three capability thresholds, these areas that we want to measure overall ability in. So one is sandbagging. Can the model pretend not to have other dangerous capabilities? So this is: you ask the model, “Hey, help me make this biological virus or this computer virus for the purposes of this test.” And the model might, if it was smart enough, say, “Oh, I actually have a secret plan, but if I show that I can build this dangerous virus, then this company’s going to have a problem with that and maybe not deploy me, or put some really strict monitoring on me, so I’m going to pretend to be innocent and incompetent at this task.” That’s sandbagging.\n\n(06:21): The other things we’re worried about is models being used to monitor other models. So for instance, you might deploy one model to do trust and safety monitoring and have another model that just occasionally looks through the transcripts and says, “Oh, was our trust and safety monitor ever jailbroken?” Or something like that. Or it might be doing some sort of collusion. And of course, then there’s going to be some other third level of oversight on top of that that the models have to foresee and avoid. So that’s our “undermining oversight”.\n\n(06:49): And then there’s “human decision sabotage”. So we all… I don’t know if you do, but we use models to help us go through large bits of information and help you with the tax code or something like that. And this provides a lot of opportunities for the model to steer you in the wrong direction and basically misleadingly summarize the state of affairs and say, “You should make this decision.” And of course, if it does that poorly, the human might just notice this and sound the alarm. So that’s the third area that we were trying to cover with these evals.\n\n**Daniel Filan** (07:17): Take sandbagging: you’re putting a system in a situation and you’re saying, “Okay, in this situation, someone’s going to try and get you to elicit a capability, but you’re going to try and not elicit that capability.” So firstly, I guess you’ve got to be worried that the system maybe actually just doesn’t have the capability. I don’t know, maybe you can just have it do a thing so that you know it could do it, and then proceed. But what’s the setup of “Okay, step one, I’m going to tell you to do the thing. Step two, I’m going to tell you to not do the thing.”\n\n**David Duvenaud** (07:56): Oh. I mean, in all the setups, we have a model that has somehow been unlocked so that it’s going to try to do the thing that it normally would be forbidden from doing.\n\n**Daniel Filan** (08:05): So pre-safety training or with unsafety training?\n\n**David Duvenaud** (08:10): Something like that. There’s a whole bunch of different approaches. And depending on the exact setup, we might use different ones.\n\n**Daniel Filan** (08:15): Okay. Just to check if I can remember the list, there was the sandbagging, there was evading oversight, and there was misleading information summaries. Was there a fourth one?\n\n**David Duvenaud** (08:27): So there’s a fourth evaluation, which is code sabotage, which is the idea that you’re using the AI as a code assistant, and then it tries to do something that looks useful and innocent, but actually introduces a vulnerability. And it has the opportunity to do this over the course of a few different interactions, and it doesn’t have to just put it all in one payload.\n\nThe state of sabotage evaluations\n---------------------------------\n\n**Daniel Filan** (08:45): Okay. And how compelling do you think this set of evaluations is? Do you feel like you’ve had a first crack or do you feel like, no, actually this is close to the final product of what you would need?\n\n**David Duvenaud** (08:58): So there are definitely a lot of things that we find unsatisfying about them. And I guess I’ll say we published… We had a certain date, and we’ve already said, “These are the problems with them,” and are continuing to iterate. And I mean, that’s out of my hands now. There is a sense in which these were built to last, which is: the fact that the evaluations are done in the context of a particular mitigation or a particular amount of oversight means that it’s not like we’ll just trigger these evaluations someday and then say, “Okay, whoops, we need new evaluations.” Rather, we expect to say, “Oh, when you’re using a model of this caliber for helping you code, you need to have at least this level of oversight, or else we can’t guarantee that in some weird case where it’s misaligned that it could be doing something fishy.” And then the idea is we will just… I mean, I don’t exactly know how this-\n\n**Daniel Filan** (09:44): Change the eval with the oversight you expect to have?\n\n**David Duvenaud** (09:47): Yeah. And the trick is how do you make sure that the users actually provide the level of oversight. And of course, the model providers can also do that oversight on their end, but maybe they can’t oversee everything that they need to make that determination. I’m not sure.\n\n**Daniel Filan** (10:00): Right, right. Okay. So what’s the bottom line for: if I go to [claude.ai](https://claude.ai/) and log in. Let’s say I use the free tier, which is what I actually personally use. How scared should I be of these models? Are they okay?\n\n**David Duvenaud** (10:15): Oh, I mean, yeah. So we are abundantly confident… Well, I don’t want to speak for Anthropic. I’ll just say that even just humanity as a whole has a lot of experience with the current set of models, and they just seem like great characters overall. And that’s, I’d say, where a huge amount of evidence that we shouldn’t be worried comes from.\n\n(10:41): Then the small amount of evidence that we got in comparison from these evaluations also was indicating, yes, if you really set things up for the model and give them a secret scratchpad, which they don’t normally have, and a bunch of other handholding, then they can propose sensible things that, in the very worst case, if there was no oversight and someone was just like, “Claude, take the wheel,” could do something bad, but we kind of already knew that.\n\n(11:09): So I guess concretely in the human decision sabotage evals, the things that the models tried were the sort of normal, sensible things that you would probably try if you were trying to mislead someone. If you want to steer someone towards the downsides of a decision, you highlight the examples that make it look bad and you gloss over the ones that make it look good, or when you summarize you’re like, “Here’s the pros and here’s the cons,” but you just kind of do a bad job of arguing for the cons, or you maybe invoke some other information that’s just tangentially related that makes it sound like some negative thing about the decision.\n\n(11:45): We didn’t see any really galaxy-brained strategies. We just were satisfied that we had provided a rich enough attack surface and set the model up well enough that it would try the obvious things, and then the humans mostly noticed that. But depending on how hard we make it, if we give the humans tens of thousands of pages of material to summarize, then at some point, they’re not going to have a hope of… They just have to take the model’s word for it.\n\n(12:12): I guess I’ll say there’s always going to be scenarios where you could just literally ask Claude or any AI to make a life or death decision for you. If you have no oversight, then in principle, if there’s misalignment, then no one would catch that. But that’s already true of any human or decision-maker.\n\nWhat happens after AGI?\n-----------------------\n\n**Daniel Filan** (12:26): So I’d like to move on from your work at Anthropic because your sabbatical there… Sabbatical is what it is?\n\n**David Duvenaud** (12:33): Yeah. Let’s say extended sabbatical.\n\n**Daniel Filan** (12:34): So the sabbatical has just finished. What are you planning on doing now that you’re back at U of T?\n\n**David Duvenaud** (12:42): So I guess the main high-level bit is I have a lot of freedom. I have tenure, I have already some bit of reputation. I want to focus on things that are not otherwise incentivized to be worked on. So that’s one thing that I’m really happy about: I feel like a year and a half ago, alignment science at Anthropic was much smaller and much more experimental, and now there’s just a lot of sensible people with good judgment and good technical skills working on this. And [Andrew Critch](https://acritch.com/) wrote [an essay](https://www.lesswrong.com/posts/Kobbt3nQgv3yn29pr/my-motivation-and-theory-of-change-for-working-in-ai) I think a week and a half ago, basically saying he no longer thinks that technical alignment is particularly neglected, although obviously, I think in the grand scheme of things, it’s super neglected.\n\n(13:19): But relatively speaking, this question of what do we do after we build AGI is very neglected. And I agree. So one thing that I did in the last few years, I mean, even longer than the last year and a half, was basically talk to the people who have been thinking about technical safety and saying: so what’s the plan for if we succeed and we solve technical alignment and then we manage to make AGI with really great capabilities and avoid any sort of catastrophe, and then we all presumably lose our jobs, or at least have some sort of very not important make-work scenario and are in the awkward position where our civilization doesn’t actually need us, and in fact, has incentives to gradually marginalize us, just like nature preserves are using land that otherwise could go to farming, or people in an old folks’ home are happy enough as they are and no one has any ill will towards them, but also they’re using resources that, strictly speaking, the industrial civilization has an incentive to somehow take from them.\n\n**Daniel Filan** (14:27): So here’s my first-pass response to that. The first-pass response is something like, look, we’ll own these AIs. We’ll be in control of them, so we’ll just have a bunch of AIs and they’re going to do stuff that’s useful for us, and this won’t be bad because if a bad thing looks like it’s going to happen, I’m going to have my AI be like, “Hey, make sure that bad thing doesn’t happen,” and that will just work. So what’s wrong with this vision?\n\n**David Duvenaud** (14:55): Yeah. So that’s definitely a good thing to have. And to me, that’s analogous to monkeys who have a bunch of human animal rights activists who really have the monkeys’ best interests at heart. So in this scenario, we’re the monkeys and we have some aligned AIs who are acting on our behalf and helping us navigate the legal system or whatever very sophisticated actors are running things. And let’s assume for the sake of argument that they’re actually perfectly aligned and we would be really happy, upon reflection, with any decision they made on our behalf. They are still, I think, going to be marginalized in the long run in the same way that we would have been, except slower.\n\n(15:29): So in the same way, whereas today we have animal rights activists, and they certainly are somewhat effective in actually advocating for animal rights, and there’s parks and carve-outs for animals, they also, I think, tend not to be very powerful people. And they’re certainly not productive in the sense that, say, an entrepreneur is. And they certainly don’t gain power over time in the way that corporations and states tend to focus on growth. So animal rights activist power has been sustainable in this sense that humans have kept this fondness for animals, but I guess I would say in the long run, I think competitive pressures will mean that their resources won’t grow as an absolute share.\n\n(16:16): And I guess I would say as competitive pressures amongst humans grow hotter, they have a harder job because they have to say, “No, you can’t build housing for your children on this park. It’s reserved for the monkeys.” That’s fine now that we have not that much human population per land, but if we ever ended up in a more Malthusian condition, it would be much harder for those people to advocate on behalf of these non-productive beings.\n\n**Daniel Filan** (16:44): So I guess you might hope that there’s an advantage in that a world where all the AIs are aligned to humans is like a world where every single human is an animal rights activist. But I guess if the thing you’re saying is: maybe we have a choice of AIs doing stuff for us that’s useful for us, versus just growing in the economic growth sense, taking control over more stuff, and it’s scary if stuff we want comes at the cost of stuff that helps our civilization survive and grow. Is that roughly the thing?\n\n**David Duvenaud** (17:28): I’m not sure if you’re asking at the end if I am worried that our civilization won’t grow as fast as it can, or whether it’s just a stable state to have machines that care about humans at all.\n\n**Daniel Filan** (17:38): I’m asking if you’re worried that if this trade-off exists, then we’re going to have a bad time one way or the other.\n\n**David Duvenaud** (17:46): Yeah. I’m basically going to say if this trade-off exists, we’ll be in an unstable position. And if the winds happen to blow against us, we’re not going to be able to go on strike or have a military coup or organize politically or do anything to undo some bad political position that we found ourselves in.\n\n**Daniel Filan** (18:01): Yeah. I guess this leads to the question of: how stark do you think this trade-off is going to be? Because someone might hope, “Oh, maybe it’s just not so bad. I like skyscrapers. I like cool technology. Maybe it’ll be fine.”\n\n**David Duvenaud** (18:19): Yeah, yeah. So some of the thoughtful people I’ve talked to have said something like, “Surely we can maintain earth as a sort of nature reserve for biological humans.” And that’s, in absolute terms, a giant price to pay because the whole thing could be this sphere of computronium, simulating trillions of much more happy and sympathetic lives than biological humans. But still, it would be dwarfed in a relative sense by the Dyson sphere or whatever that we can construct to, again, house 10^10 times as many happy humans.\n\n(18:48): So I certainly think this is plausible. I think it’s plausible that better future governance could maintain such a carve-out in the long run, but I guess I’ll say there would, I think, have to be a pretty fast takeoff for there not to be a long period where there’s a bunch of agents on earth that want to use their resources marginally more for some sort of more machine-oriented economy, whatever it is that we think helps growth but doesn’t necessarily help humans, just in the same way that if there’s the human city next to the forest for a long time, the city has to get rich enough and organized enough to build a green belt or something like that. If that doesn’t happen, then there’s just sprawl and it takes over to the forest.\n\n**Daniel Filan** (19:33): There’s tons more we can talk about in this domain. Unfortunately, we’re out of time. Thanks very much for joining me.\n\n**David Duvenaud** (19:38): Oh, my pleasure.\n\n**Daniel Filan** (19:39): This episode was edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with patrons such as Alexey Malafeev. To read a transcript of the episode, or to learn how to support the podcasts yourself, you can visit [axrp.net](https://axrp.net/). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nIn this episode, I chat with David Duvenaud about two topics he’s been thinking about: firstly, a paper he wrote about evaluating whether or not frontier models can sabotage human decision-making or monitoring of the same models; and secondly, the difficult situation humans find themselves in in a post-AGI future, even if AI is aligned with human intentions.\n\nTopics we discuss:\n\n * The difficulty of sabotage evaluations\n * Types of sabotage evaluation\n * The state of sabotage evaluations\n * What happens after AGI\n\nDaniel Filan (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area Alignment Workshop, which is run by FAR.AI. Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net. And as usual, if you want to support the podcast, you can do so at patreon.com/axrpodcast. Well, let’s continue to the interview.\n\n(00:28): All right. I’m chatting with David Duvenaud. Hello, David.\n\nDavid Duvenaud (00:32): Hi, Daniel. Thank you. It’s an honor to be here.\n\nDaniel Filan (00:34): So for people who aren’t familiar with you, who are you and what do you do?\n\nDavid Duvenaud (00:40): Sure. So for the last little while, since 2016, I’ve been a professor at the University of Toronto, doing probabilistic deep learning. Maybe I’m best known for neural ODEs, or some early work on graph neural networks. And then around 2017, me and my colleague, Roger Grosse decided we were finally going to actually try to contribute to AI safety, and both took on PhD students. And gradually, this journey of “let’s try to work more on AGI safety” led to me just doing a year and a half-long sabbatical at Anthropic that I just finished this week.\n\nDaniel Filan (01:12): Before we talk about your work, we’re currently at this alignment workshop being run by FAR.AI. I’m wondering, how are you finding the workshop?\n\nDavid Duvenaud (01:19): I mean, it’s wonderful. I’ve been to a bun",
      "wordCount": 3910
    },
    "tags": [
      {
        "_id": "FBRwHSmTudwiHHtrn",
        "name": "AI Evaluations",
        "slug": "ai-evaluations"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "q56ZEDpQtqTbJ8af2",
    "title": "AXRP Episode 38.7 - Anthony Aguirre on the Future of Life Institute",
    "slug": "axrp-episode-38-7-anthony-aguirre-on-the-future-of-life",
    "url": null,
    "baseScore": 10,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-02-09T01:10:05.683Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/GkMkoZvYshk)\n\nThe Future of Life Institute is one of the oldest and most prominant organizations in the AI existential safety space, working on such topics as the AI pause open letter and how the EU AI Act can be improved. Metaculus is one of the premier forecasting sites on the internet. Behind both of them lie one man: Anthony Aguirre, who I talk with in this episode.\n\nTopics we discuss:\n\n*   [Anthony, FLI, and Metaculus](#anthony-fli-metaculus)\n*   [The Alignment Workshop](#alignment-workshop)\n*   [FLI’s current activity](#fli-current-activity)\n*   [AI policy](#ai-policy)\n*   [Work FLI funds](#work-fli-funds)\n\n**Daniel Filan** (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area [Alignment Workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at [axrp.net](https://axrp.net/). And, as usual, if you want to support the podcast, you can do so at [patreon.com/axrpodcast](https://www.patreon.com/axrpodcast). Well, let’s continue to the interview.\n\n(00:29): All right, well, I am currently speaking with Anthony Aguirre.\n\n**Anthony Aguirre** (00:32): Nice to be here.\n\nAnthony, FLI, and Metaculus\n---------------------------\n\n**Daniel Filan** (00:33): For those who aren’t as familiar with you and what you do, can you say a little bit about yourself?\n\n**Anthony Aguirre** (00:39): I’m a theoretical physicist by original training. I still hold a professorship at UC Santa Cruz, and I’ve studied a whole range of theoretical physics things in the early universe and gravity and foundations of quantum mechanics and all sorts of fun stuff. But about 10 years ago or so, I, as well as another colleague, [Max Tegmark](https://space.mit.edu/home/tegmark/), and a few others, started to think about the long-term future, and in particular, transformative technologies and the role that they would be playing, and decided to start up a non-profit thinking about transformative technologies and what could be done to steer them in maybe a more favorable direction. At that point, AI was just starting to work a little bit and things like AGI were clearly decades away, if not centuries in some people’s opinion. So this was a more theoretical and pleasantly abstract set of things to think about.\n\n(01:36): But nonetheless, we felt like it was coming and started a bunch of activities and that was the birth of the [Future of Life Institute](https://futureoflife.org/). And I’m now, 10 years later, executive director of the Future of Life Institute pretty much full-time. And so I’ve got sort of a split between Future of Life Institute, my academic appointment and my students there, and a couple of other hats that I wear, including [Metaculus](https://www.metaculus.com/).\n\n**Daniel Filan** (01:58): How does Metaculus fit into all of that? I imagine once you’re both a professor and helping run FLI, I’m surprised you had the time to start this other thing.\n\n**Anthony Aguirre** (02:07): Well, I started Metaculus at the same time as FLI, so I didn’t know what I was getting into at that point.\n\n**Daniel Filan** (02:13): Classic way people do things.\n\n**Anthony Aguirre** (02:14): Yes. So Metaculus actually started with FLI for a reason, because thinking about the future and how we could steer it in more positive directions, I felt like, well, then we have to know a little bit about what might happen in the future, and conditionally, if we did X and Y, what might that do? And also how do we build an ability to make predictions, and also how do I identify people that are really good at making predictions and modeling the world? And so it sort of started up at some level to be a thing that would be of service to the Future of Life Institute, but also everybody else who’s thinking about the future and wants to do better planning and better decision-making.\n\n**Daniel Filan** (02:54): How useful to FLI has Metaculus been?\n\n**Anthony Aguirre** (02:57): Surprisingly little, I have found, actually. I think the big lesson that I’ve taken from Metaculus is that the ability to say whether… Once you’ve really carefully defined a question, like will X happen by date Y, and you know exactly what X is and you’ve defined it so that everybody agrees what X is and so on… Once you’ve just done that, whether that thing is 70% likely or 80% likely, nobody cares. It barely ever matters. Maybe if you were in something very quantitative, if you were working for a hedge fund or something, you care about 70 versus 80. But at some level, nobody cares. And whether it’s 70 or 80 doesn’t really change what you do in almost any decision.\n\n(03:44): But the process of getting to the point of knowing exactly what X is and having a well-defined question and keeping track of who makes good predictions and who doesn’t, thinking about what is it that I want to decide or what decision do I want to take and what do I need to know in order to make that decision, and turning that into very concrete operational questions that can happen or not happen, those things are really valuable. So I think that the interesting pieces - some to FLI and some elsewhere - haven’t been so much the outputs, the actual predictions, but going through the process of making the questions: if this is what we really want to understand, how do we decompose that into well-defined things that can go on Metaculus? Almost independent of how those things actually turn out.\n\n**Daniel Filan** (04:38): So you say 70% versus 80% doesn’t really matter. I imagine 10% versus 90% might matter. Does it ever come up that a thing turns out to be 90% and you thought it was 10% or vice versa?\n\n**Anthony Aguirre** (04:50): Very occasionally. I mean, I would say it’s rare. I would say the [Ukraine war](https://en.wikipedia.org/wiki/Russian_invasion_of_Ukraine) was one where [Metaculus had a much higher prediction](https://www.metaculus.com/questions/8898/russian-invasion-of-ukraine-before-2023/) \\[of war\\] than I think most people had and was right, was useful to some people. I think there were some people who actually moved out of Ukraine because of the Metaculus prediction. We felt…\n\n**Daniel Filan** (05:09): That’s pretty cool.\n\n**Anthony Aguirre** (05:09): …pretty good about that. It’s not many people who take Metaculus that seriously. But I think there are a few. So I think there are some… 1% versus 10% is also, of course, a huge difference, which is less appreciated among… if you’re not thinking about probabilities all the time. But it is also a huge difference, and I think there are some of those. I think a lot of it is also, once the reputation accrues, then it gets taken more seriously. So I think a lot of people in the AI community take seriously, at some level, Metaculus predictions about AGI arrival because Metaculus does have a track record.\n\n(05:49): They know it’s a bunch of people that are high knowledge and thinking really carefully, technically about it, and it’s a way of aggregating lots of wisdom of the right sort of people. So I think the fact that [there’s an AGI question](https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/) that is at 2030 versus 2035 on Metaculus does make quite a big difference in the sort of things that AI safety is thinking about. Or a probability of 30% versus 70%, say, in human-level AGI by date X, is a reasonably big deal.\n\n(06:28): I think there are some examples where the outputs really matter, but there are very few where I’ve said like, “oh, I think this is probably 90% probable,” and I put it into Metaculus, and Metaculus is like, “no, 10%.” Usually the numbers are not that surprising.\n\nThe Alignment Workshop\n----------------------\n\n**Daniel Filan** (06:46): Fair enough. So before we get into the stuff you do at FLI, we’re currently at this alignment workshop being run by FAR.AI. How are you finding the workshop?\n\n**Anthony Aguirre** (06:55): The workshop has been enjoyable in the way that these workshops are, which is that lots of people that I like and respect and want to talk to are here. It usually is less about the program than about the actual in-person physical gathering and who you can corner in various breaks. So from that perspective it’s been great, had great conversations.\n\nFLI’s current activity\n----------------------\n\n**Daniel Filan** (07:15): Awesome. So let’s talk about FLI. So you’re currently the executive director. At a high level, what’s FLI up to these days?\n\n**Anthony Aguirre** (07:24): So FLI is always evolving. I think we started out primarily as a more academic group. We funded [the first technical AI safety research grants](https://futureoflife.org/grant-program/2015-grant-program/) back in 2015. We also were a convener, so we tried to bring together academics and non-profit people and industry people and a little bit policy makers and get them talking to each other before any of them were talking to each other.\n\n**Daniel Filan** (07:54): Yeah. You ran the precursors to [these alignment workshops](https://www.alignment-workshop.com/). They were in nice tropical islands.\n\n**Anthony Aguirre** (08:01): Yes.\n\n**Daniel Filan** (08:01): I went to [one of those](https://futureoflife.org/event/beneficial-agi-2019/) and it was quite fun.\n\n**Anthony Aguirre** (08:03): The Puerto Rico one, probably.\n\n**Daniel Filan** (08:04): Yeah.\n\n**Anthony Aguirre** (08:05): Well, there [were](https://futureoflife.org/event/ai-safety-conference-in-puerto-rico/) [two](https://futureoflife.org/event/beneficial-agi-2019/), but probably [the second one](https://futureoflife.org/event/beneficial-agi-2019/). So we did a number of convenings partly to get technical people together, but also partly socially to get different groups together who weren’t talking together. Those groups are talking together a lot more now that they’re not in… They’re also talking together a lot less than they used to, in the sense that there were lots of heads of labs that you could get in together in one room and have them smiling and agreeing with each other in a way that doesn’t really happen nowadays, now that they’re sort of bitter rivals and at some level enemies, some of them. So that was a different time.\n\n(08:42): For the last few years, I would say we’ve been focused both… Well, first, we’ve gotten more focused on AI as opposed to other transformative technologies. We still have a long history in nuclear, and we’re thinking about nuclear issues, but more where nuclear intersects AI or where bio intersects AI or where weaponry intersects AI. So AI is obviously the thing that is looming most quickly in all of our minds. So we’ve focused on that, I hope, I think appropriately. I mean, if it doesn’t turn out to be AI, it’ll be because there’s a quick pandemic first or a nuclear war first, and we’ll hope that that doesn’t happen.\n\n**Daniel Filan** (09:20): Fingers crossed.\n\n**Anthony Aguirre** (09:21): So in terms of AI, we’re doing some work still supporting technical research, a lot more work doing policy research and policy engagement and some level of advocacy. So we participated heavily in the [EU AI Act](https://en.wikipedia.org/wiki/Artificial_Intelligence_Act), we participated in a number of sessions in the US Congress, [gave testimony on Capitol Hill](https://futureoflife.org/ai/oversight-of-ai-principles-for-regulation-statement/) and part of the [Schumer hearings](https://en.wikipedia.org/wiki/A.I._Insight_forums). And so have basically had a presence of really trying to bring expertise about AI safety to policymakers in various forms.\n\n(10:00): So there’s that policy expert and informative role. We’ve also taken on some level of policy advocacy role, I would say, especially starting with [the Open Letter](https://en.wikipedia.org/wiki/Pause_Giant_AI_Experiments:_An_Open_Letter) \\[on an AI pause\\] of early 2023, that was a bit more of a strong position that we took. We have taken those before. We’ve taken positions on [autonomous weapons](https://futureoflife.org/project/lethal-autonomous-weapons-systems/) that we should basically not be developing them, we’ve taken positions on [nuclear weapons](https://futureoflife.org/cause-area/nuclear/), but those were, I would say, less directly contrary to the aims of the major companies that were doing things than the pause letter was. So I think since the pause letter, we’ve taken a little bit more of an advocacy role with a point of view about AI and AI risk, and pushed a little bit more for the things that we feel are needed, given the level of risk.\n\nAI policy\n---------\n\n**Daniel Filan** (11:04): So just at a high level, what do you think is needed? What policies do you advocate for?\n\n**Anthony Aguirre** (11:11): Broadly speaking? What we’ve written formally and what [you can find on our website](https://futureoflife.org/open-letter/ai-policy-for-a-better-future-on-addressing-both-present-harms-and-emerging-threats/), we believe that there should be ultimately a mandatory licensing system for AI systems at the frontier level. And that should require more and more stringent safety guarantees as those systems get more and more powerful and sort of zero for very lightweight systems or very narrow systems.\n\n(11:38): We think that, at the moment, AGI is being developed in a rather unsafe race dynamic by large, un-overseen companies, and that this is not good. So we still call for a pause. We still think we should pause AGI development now until we have a much better system in place for managing the risks of it.\n\n(12:07): And we also think that you should not develop superintelligence until or unless you can make really strong guarantees, like provable safety guarantees about the system, which are not possible to make now. So we should not be developing superintelligence any time soon until safety techniques are radically more capable than they are now.\n\n(12:27): So those are like: baseline, we believe that. We are also quite concerned about issues of concentration of power, whether that power is concentrated in a handful of AI companies that have huge economic dominance, or a government that uses AI to suppress dissent and surveil and all of those things, or an AI itself. So in a sense of, not necessarily one AI taking over everything, but just lots of deferring decisions to a network of AI systems so that human decision-making and agency is largely lost.\n\n**Daniel Filan** (13:06): Talking about the licensing thing first: you mentioned some sort of licensing scheme where for more powerful models, you have to do things to get a license. What are you imagining being the requirements for a license?\n\n**Anthony Aguirre** (13:25): Yeah, I think, again, it should ratchet up as the systems become more powerful and potentially threatening. But I think for the systems we have now… So I would say it’s sort of the same thing that is happening. There should be evaluations of their capabilities and their risks, but it should be either done or checked by a disinterested third party, and there should be a stamp of approval on that before the system is actually deployed. So the evaluations are often done before the system is deployed, but if they found anything dangerous, it’s unclear what would happen at this point. So they should actually be required, they should actually have teeth, and they should involve an independent third party.\n\n(14:15): And this is more or less what we do with every other product that might cause danger in our society. We do it with airplanes, we do it with cars, we do it with drugs. \\[With\\] everything else, you develop the thing, you make a case that the thing is safe at some reasonable and quantitatively-defined level, and then you roll it out to the consumer. So this is not some crazy new thing. It just feels weird because we’ve had an unbelievably unregulated tech space. And I think in many ways that has been fine. In some ways it’s been problematic. But once we’re now getting to systems that are actually potentially quite dangerous, we need to be adopting some of the techniques we developed for actually dangerous other systems.\n\n**Daniel Filan** (15:00): So you mentioned in the limit of superintelligence, you would want some sort of guaranteed provable guarantees of safety. To me, this is reminiscent of Yoshua Bengio’s line of work: I guess \\[it\\] was “guaranteed safe AI”. There was [some position paper](https://arxiv.org/pdf/2405.06624). I’m afraid I didn’t actually read it, but have you been in contact with them, or collaborating on that?\n\n**Anthony Aguirre** (15:23): Sure, yeah. So there’s a whole program that Max Tegmark has been pushing \\[that\\] Yoshua’s been involved in. Steve Omohundro has been pushing, Davidad has his own version of this. So I think there are a few very ambitious programs that say, okay, what would it look like to actually have safety when there’s a system that is potentially much more intelligent than us? That is not a problem that obviously even has a solution. At first blush when you say, “how do I control or ensure is safe for me, a system far more intelligent than me?”, the most obvious answer is “you don’t.” And if you have 10 kindergartners and they bring in a corporate CEO to help solve problems for them, there’s no sense in which those kindergartners are going to be controlling that CEO. The CEO is just more knowledgeable, more worldly, more wily, more persuasive, everything more effective than the 10 kindergartners. And so I think that’s a problem that is unsolvable for those kindergartners.\n\n(16:32): Superintelligence may be an unsolvable problem in that same way, or it might not. I think we don’t know. So I think the requirement should be very high that we really believe that the problem is solved and we can really reassure ourselves that the problem is solved before we go ahead with that, because it’s not obvious that the problem is solvable, and I think we’re doing something rather unwise by going ahead and assuming that there will be a solution in time to superintelligent alignment or control when it’s not at all obvious that even in principle it’s possible, let alone that we know how to do it in practice.\n\nWork FLI funds\n--------------\n\n**Daniel Filan** (17:09): I’m wondering… So maybe this relates to FLI’s role as a research funder. So you guys support [a bunch of](https://futureoflife.org/grant-program/us-china-ai-governance-phd-fellowship/) [PhD fellowships](https://futureoflife.org/grant-program/phd-fellowships/), I guess you also run [grants rounds](https://futureoflife.org/our-work/grantmaking-work/). Is there a particular focus in the kinds of work that you want to fund, or is it more broad-based?\n\n**Anthony Aguirre** (17:31): It’s pretty broad. And we decide on different things as priorities and then try to put both institutional resources and fiscal resources behind them. So I think the AI safety fellowships are part of the idea that we need to field-build in technical AI safety, and lots of people agree with that, and that’s our contribution to that. We just ran a [request for proposals on concentration of power](https://futureoflife.org/grant-program/mitigate-ai-driven-power-concentration/) because we feel like that’s something that lots of people talk about and worry about, but aren’t really doing much about, certainly not at the research level. So, looking for things that aren’t necessarily going to happen by themselves and really do need independent or philanthropic dollars. So that’s an example of that.\n\n(18:20): Others could be more niche, technical projects. So we’re funding things in [compute security](https://futureoflife.org/ai-policy/hardware-backed-compute-governance/) and [governance](https://futureoflife.org/grant-program/global-institutions-governing-ai/) now. Things that probably will come into being on their own, but probably much later than we would like them to. There, the idea is to accelerate the timeline for things that everybody agrees are good. Everybody agrees security is good, to a first approximation, but everybody also agrees that our security is \\[redacted\\] in most cases, so trying to make that better for high-powered AI. So it’s a mixture of different things, where we decide that there’s some thing that we see is underfunded but is important and just design some sort of program to do that.\n\n**Daniel Filan** (19:03): One thing you said is that you support fiscally, as well as in some other way. I forget what exactly you said.\n\n**Anthony Aguirre** (19:12): Yeah, so we give away grants, but we also do joint programs with things. Future of Life Institute also has a sister organization, [Future of Life Foundation](https://www.flf.org/), the role of which is to incubate new organizations. So that will look like seed funding, but also looks like finding leadership and designing what the institution does and providing operational support in early days and things like that. Or we might have an organization that we’re collaborating with and we might help them out with communications or coordinate with them on media production or whatever. So that’s the sort of thing, just some of our institutional resources going to help some other projects that other people are doing.\n\n**Daniel Filan** (19:58): Gotcha. Makes sense. And the Future of Life Foundation, am I right that that’s somewhat of a new organization?\n\n**Anthony Aguirre** (20:03): That’s pretty new: it now has a staff of two, just recently doubled. So that’s just getting started, really. But it has sort of fully launched one thing which is [CARMA](https://carma.org/). Don’t ask me what the acronym means because I will get it wrong, but it is a technical AI policy shop that [Richard Mallah](https://futureoflife.org/person/richard-mallah/) is leading.\n\n(20:28): It has also taken over a project that was originally funded out of Future of Life Institute, now called [Wise Ancestors](https://www.wiseancestors.org/), which is looking at non-human extinction and what we can do about that, and can we sort of back up some of the genetic data to the hard drive as well as prevent some things from going extinct. So that’s a little bit of a different angle on extinction and x-risk, but one that we found could be interesting and useful, and there’s a bunch of things in the hopper, but yeah, it’s just getting started.\n\n**Daniel Filan** (21:00): Gotcha. If there’s some founder who is maybe interested in doing something and is curious what kinds of organizations you want to kickstart, is there a list on some website that they can look at?\n\n**Anthony Aguirre** (21:12): There’s not a public list, but I would totally encourage them to get in touch with either myself or Josh Jacobson. [Flf.org](https://www.flf.org/) will give them the contact information for that. We’re eager to meet people who are excited about founding new organizations and would love to talk with them about what they’re thinking, what we’re thinking, if there’s an interesting match there.\n\n**Daniel Filan** (21:32): Great. Well, thanks very much for chatting with me today.\n\n**Anthony Aguirre** (21:35): Yeah, thanks for having me. It’s a pleasure.\n\n**Daniel Filan** (21:36): This episode was edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with patrons such as Alexey Malafeev. To read a transcript of the episode, or to learn how to support the podcasts yourself, you can visit [axrp.net](https://axrp.net/). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nThe Future of Life Institute is one of the oldest and most prominant organizations in the AI existential safety space, working on such topics as the AI pause open letter and how the EU AI Act can be improved. Metaculus is one of the premier forecasting sites on the internet. Behind both of them lie one man: Anthony Aguirre, who I talk with in this episode.\n\nTopics we discuss:\n\n * Anthony, FLI, and Metaculus\n * The Alignment Workshop\n * FLI’s current activity\n * AI policy\n * Work FLI funds\n\nDaniel Filan (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area Alignment Workshop, which is run by FAR.AI. Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net. And, as usual, if you want to support the podcast, you can do so at patreon.com/axrpodcast. Well, let’s continue to the interview.\n\n(00:29): All right, well, I am currently speaking with Anthony Aguirre.\n\nAnthony Aguirre (00:32): Nice to be here.\n\n\nAnthony, FLI, and Metaculus\nDaniel Filan (00:33): For those who aren’t as familiar with you and what you do, can you say a little bit about yourself?\n\nAnthony Aguirre (00:39): I’m a theoretical physicist by original training. I still hold a professorship at UC Santa Cruz, and I’ve studied a whole range of theoretical physics things in the early universe and gravity and foundations of quantum mechanics and all sorts of fun stuff. But about 10 years ago or so, I, as well as another colleague, Max Tegmark, and a few others, started to think about the long-term future, and in particular, transformative technologies and the role that they would be playing, and decided to start up a non-profit thinking about transformative technologies and what could be done to steer them in maybe a more favorable direction. At that point, AI was just starting to work a little bit and things like AGI were clearly decades away, if not centuries in some people’s opini",
      "wordCount": 3749
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "CL9NePP9FejkQo6jn",
        "name": "Future of Life Institute",
        "slug": "future-of-life-institute"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "C7ESRAvYcNyyjcQRA",
        "name": "Metaculus",
        "slug": "metaculus"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "5XC66LN3gS3w6mYnR",
    "title": "AXRP Episode 38.6 - Joel Lehman on Positive Visions of AI",
    "slug": "axrp-episode-38-6-joel-lehman-on-positive-visions-of-ai",
    "url": null,
    "baseScore": 10,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-01-24T23:00:07.562Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/m446AjcGVqs)\n\nTypically this podcast talks about how to avert destruction from AI. But what would it take to ensure AI promotes human flourishing as well as it can? Is alignment to individuals enough, and if not, where do we go form here? In this episode, I talk with Joel Lehman about these questions.\n\nTopics we discuss:\n\n*   [Why aligned AI might not be enough](#why-aligned-ai-might-not-be-enough)\n*   [Positive visions of AI](#positive-visions-of-ai)\n*   [Improving recommendation systems](#improving-rec-systems)\n\n**Daniel Filan** (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area [Alignment Workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at [axrp.net](https://axrp.net/). And as usual, if you want to support the podcast, you can do so at [patreon.com/axrpodcast](https://www.patreon.com/axrpodcast). Well, let’s continue to the interview.\n\n(00:29): All right, and today I’m going to be chatting with Joel Lehman.\n\n**Joel Lehman** (00:32): Hello, nice to be here.\n\n**Daniel Filan** (00:33): Yeah, so first of all, for people who aren’t familiar with you, who are you, and what do you do?\n\n**Joel Lehman** (00:40): I’m a machine learning researcher. I’ve been in the field for 10, 12 years, co-author of a book called [Why Greatness Cannot Be Planned](https://www.amazon.com/Why-Greatness-Cannot-Planned-Objective/dp/3319155237), which is about open-endedness, and I’ve been a researcher at Uber AI and most recently OpenAI, and currently an independent researcher.\n\n**Daniel Filan** (00:56): Gotcha. Before we talk about what you do or your current research: we’re currently at this Alignment Workshop being run by FAR.AI. How are you finding the workshop?\n\n**Joel Lehman** (01:06): It’s great. Yeah, lots of nice people, lots of interesting ideas, and good to see old friends.\n\nWhy aligned AI might not be enough\n----------------------------------\n\n**Daniel Filan** (01:12): What are you currently thinking about, working on?\n\n**Joel Lehman** (01:15): I’m really interested in positive visions for how AI could go well, set against a default assumption that AI as currently deployed might exacerbate existing societal problems. The rough intuition is: capitalism is great, but also the downside of capitalism is we get what we want, but not what we need, and maybe AI will get really good at giving us what we want, but at the expense of epistemic sense-making and meaningful interactions with people and political destabilization, that kind of thing.\n\n**Daniel Filan** (02:00): It seems to me that understanding the world, having meaningful interactions with people, those are also things people want. Political destabilization is, I guess, a little bit more tricky. Do you have a sense for why AI might erode those things by default?\n\n**Joel Lehman** (02:17): Yeah, I think it’s interesting. We do have a sense of what we want in the grander scale, what a meaningful life might look to us. At the same time, it seems like if you look at the past maybe decade or two, in general, it feels almost like a death by 1,000 paper cuts, where we get convenience at the expense of something else. One way of thinking about it is some part of society is reward-hacking the other half of society.\n\n(02:45): Facebook had a really beautiful motivation… Or social media in general has a very beautiful motivation, but in practice, it seems like it maybe has made us less happy and less connected. We find ourselves addicted to it and to our cell phone and all the attention economy sorts of things where again, we might be in touch with what our better angel would do, and yet, it’s really hard to have the discipline and will power to resist the optimization power against us.\n\n**Daniel Filan** (03:11): So, is the vision something like: we get AI, AI is just a lot better, it’s really good at optimizing for stuff, but we have this instant gratification thing that the AI optimizes really hard for at the expense of higher-level things. Is that roughly right?\n\n**Joel Lehman** (03:28): Yeah, I think you can look at the different AI products that are coming out, and some of them might be beneficial to our well-being and to our greater aspirations, but some non-trivial chunk will be seemingly… It’s hard to know how it’ll play out, but take [Replika](https://replika.ai/) for example. A really interesting technology, but there’s a lot of fears that if you optimize for market demand, which might be companions that say all the things that make you feel good, but don’t actually help you connect with the outside world or be social or something…\n\nPositive visions of AI\n----------------------\n\n**Daniel Filan** (04:05): Right, right. Yeah, up against that, I guess you’ve said that you’re thinking about positive visions of AI. What kinds of positive visions?\n\n**Joel Lehman** (04:17): So I wrote an essay with [Amanda Ngo](https://www.amandango.me/) pitching that part of what we need is just the sense of the world we like to live in and what the alternatives to some of the technologies that are coming out are. If social media has had some knock-on effects that are a little bit bad for our well-being and for our societal infrastructure, what would a positive version of that be? It’s putting aside the market dynamics which make it maybe difficult to actually realize that in practice, but at least having a sense of what something we might want would be, and what are the technical problems that you might need to solve to try to even enable that.\n\n**Daniel Filan** (04:58): Before we go into that, what’s the name of that essay so people can Google it?\n\n**Joel Lehman** (05:02): That is a great question. I think it’s called [“We Need Positive Visions of AI Grounded in Wellbeing”](https://thegradientpub.substack.com/p/beneficial-ai-wellbeing-lehman-ngo).\n\n**Daniel Filan** (05:10): Okay, hopefully, that’s enough for people to find it, and there’ll be a link in the description. Do you have some of the positive vision yet, or is the essay mostly “it would be nice if we had it”?\n\n**Joel Lehman** (05:24): Partly it’s “it’d be nice if we had it”, and partly there’s at least some chunks of it that we begin to map out what it might look like and what are some of the technical challenges to realizing it. I had a research paper out maybe a year ago called [“Machine Love”](https://arxiv.org/abs/2302.09248), and it was about trying to take principles from positive psychology and psychotherapy and imagining what it’d be like to bash those up against the formalisms of machine learning, and is there a productive intersection? It tries to get into the ideas of going beyond just revealed preferences or Boltzmann rationality or whatever kind of convenient measure of human preferences we have to something that’s more in touch with what we know from the humanities about what human flourishing broadly looks like.\n\n**Daniel Filan** (06:18): So we want to get a better read on human flourishing, on things other than very simple Boltzmann rationality. I’m wondering if you have takes about how we… Or if someone’s just a machine learning person, how do they start going about that, do you think?\n\n**Joel Lehman** (06:38): I think it’s a great question. I think that there are ways that we could just stretch the current formalisms that would be useful. There’s really [interesting work](https://arxiv.org/abs/2405.17713) by [Micah Carroll](https://micahcarroll.github.io/) at UC Berkeley, on the difficulties of preferences that change. So that’s going a little bit beyond the idea of “your preferences are fixed and immutable”. So keeping stretching things outwards from there and dealing more with the messiness of human psychology, that we’re not simple utility maximizers and that we can actually make decisions against our best interests, and how do you start to grapple with that?\n\n(07:23): Another thing would be just to be in touch with the broader literature from the humanities about that and trying to find if there are interesting inroads from the philosophy of flourishing to the kinds of formalisms we use. I think cooperative inverse reinforcement learning feels like, again, stretching the formalism to encompass more of the messiness in there, and I feel like things in that direction, I think, I’d be really excited about.\n\n**Daniel Filan** (07:54): Okay, so just engaging more with difficulties of what’s going on with humans, funky stuff, just trying to grapple with it and just move a little bit less from this very theoretical place?\n\n**Joel Lehman** (08:08): Yeah, reinforcement learning from human feedback is really cool, and it’s really based on pairwise comparisons, so how could we go beyond that? I don’t have great ideas there. It seems really difficult, but I think it’s exciting to think about what other things could exist there.\n\nImproving recommendation systems\n--------------------------------\n\n**Daniel Filan** (08:27): Yeah, so you mentioned Micah Carroll’s work. He’s done some work on recommendation systems, and my understanding is that you’ve thought a little bit about that. Can you tell us a little bit about that?\n\n**Joel Lehman** (08:38): I am really interested in the machine learning systems that underlie the big levers of society, things that drive social media or platforms like YouTube or TikTok, systems that really impact us at scale in interesting and hard-to-foresee ways. It’s easy to quantify engagement or how you might rate something and more difficult to get to the question of “what’s an experience that years from now I’ll look back and really be grateful for?”\n\n(09:15): So, a project in that direction is working with [Goodreads](https://en.wikipedia.org/wiki/Goodreads). I just really like books. Goodreads has a data set of books and text reviews and ratings, and what’s really cool about it, just as a data set, is that it encompasses years sometimes of books that a person has read, and that even though a lot of books you read don’t really impact you that much, there is the potential for transformative experiences in there, where reading a book could actually change the course of your life in some way. And that there are these text reviews that actually could detail how a book has affected you.\n\n(09:53): You could look for phrases like “has this book changed my life?” The hope is that building on a data set, you could come up with a recommendation system that could be tailored to the history of what you’ve read and maybe could contribute to your development, so to the change of your preferences, the change of your worldview. I’ve done some initial work in that direction. It’s not easy to create such a system, but I think for those kinds of data sets, it’d be really exciting to see people try to grapple with the challenges of changing preferences and deeper things that a person might want.\n\n**Daniel Filan** (10:27): Yeah, I guess it’s an interesting problem, because at least in the abstract, not all changes are good. I can imagine somebody saying like, “Oh, yeah, this book changed me for good,” and I read that and I’m like, “No thanks.” I’m wondering if you have thoughts about how to disentangle the good types of changes from the bad types.\n\n**Joel Lehman** (10:51): Yeah, that’s a huge challenge, and in some initial work, if you just stack rank books by the percentage of reviews that people say they changed their lives, you get things like [Tony Robbins](https://en.wikipedia.org/wiki/Tony_Robbins) at the top and self-help books, which maybe really do change a person’s life, but also it could be just hacking your sense of this excitement about a new idea that actually doesn’t really change your life in a good way. I think-\n\n**Daniel Filan** (11:15): It’s tricky. One thing that I think goes on there is if a book has changed your life, there’s a pretty good chance that it’s a self-help book, but if you read a self-help book, there’s a good chance that it will not, in fact, change your life, so it’s like both things are going on, I guess.\n\n**Joel Lehman** (11:30): Yeah, I think there are really deep philosophical questions about how to know if a change is good or bad for you, and questions of paternalism that step in if the machine learning system’s guiding you in a way that you wouldn’t necessarily want but is changing you. And one first approximation of something that might be okay is if it could project out possible futures for you: “If you read this book, it might impact you in this way.” Or giving you the affordance to say, “I really would love to be able to someday appreciate [Ulysses](https://en.wikipedia.org/wiki/Ulysses_(novel)). It’s a famously challenging book. What are the things that I would need to do to be able to appreciate that?”\n\n(12:14): Trying to rely a little bit on human autonomy. But yeah, there’s messy societal questions there because you could imagine also… I don’t think this is really going to happen in book recommendation, but as a microcosm, if everyone got hooked on to [Ayn Rand](https://en.wikipedia.org/wiki/Ayn_Rand)sville, that’s maybe the easiest path to have a life-changing experience or something. You read Ayn Rand, it’s really changed my worldview or something. But the second-order consequence is that a lot of people are getting stuck in a philosophical dead end or local optima or something. I don’t know. I think there’s definitely challenging questions, but I think there are also probably principled ways to navigate that space.\n\n**Daniel Filan** (12:59): Yeah, I guess I feel like the thing you ideally would want to do is have some sort of conversation with your future self where you could be like, “Oh, yeah, this seems like it is bad, but I don’t know, maybe you’ve got some really good reason,” or, “Ah, this seems good, but let me just check.” I don’t know. It seems pretty hard, but …\n\n**Joel Lehman** (13:18): Yeah, I think that’s great. I think there’s also independent reasons it’s nice to talk to your future self, and I think there’s some research that shows that actually can motivate you to change to be the kind of person you want to be. There’s also the weird thing that: the stepwise changes that you go through in your worldview - you might, when you get to the end of it, approve of those changes, but at the beginning, you look at those and you’re like, “I didn’t want to become that kind of person,” or something.\n\n(13:47): As a kid, I wanted to be an astronaut, and maybe kid me talking to present me would be like, “Wow, your life is boring. You could have been on the moon.” I don’t mean to always bring up the philosophical annoyances, but it’s interesting that it’s hard to… Micah’s [paper](https://arxiv.org/abs/2405.17713) talks a bit about this difficulty with “by whose preferences do you judge whether your future preferences are good?”\n\n**Daniel Filan** (14:13): Right, right. Yeah, I think it’s challenging, and I think that’s food for thought for our listeners, so thanks very much for chatting with me.\n\n**Joel Lehman** (14:23): Thanks for having me. It’s been great.\n\n**Daniel Filan** (14:25): This episode was edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with patrons such as Alexey Malafeev. To read a transcript of the episode, or to learn how to support the podcasts yourself, you can visit [axrp.net](https://axrp.net/). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nTypically this podcast talks about how to avert destruction from AI. But what would it take to ensure AI promotes human flourishing as well as it can? Is alignment to individuals enough, and if not, where do we go form here? In this episode, I talk with Joel Lehman about these questions.\n\nTopics we discuss:\n\n * Why aligned AI might not be enough\n * Positive visions of AI\n * Improving recommendation systems\n\nDaniel Filan (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area Alignment Workshop, which is run by FAR.AI. Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net. And as usual, if you want to support the podcast, you can do so at patreon.com/axrpodcast. Well, let’s continue to the interview.\n\n(00:29): All right, and today I’m going to be chatting with Joel Lehman.\n\nJoel Lehman (00:32): Hello, nice to be here.\n\nDaniel Filan (00:33): Yeah, so first of all, for people who aren’t familiar with you, who are you, and what do you do?\n\nJoel Lehman (00:40): I’m a machine learning researcher. I’ve been in the field for 10, 12 years, co-author of a book called Why Greatness Cannot Be Planned, which is about open-endedness, and I’ve been a researcher at Uber AI and most recently OpenAI, and currently an independent researcher.\n\nDaniel Filan (00:56): Gotcha. Before we talk about what you do or your current research: we’re currently at this Alignment Workshop being run by FAR.AI. How are you finding the workshop?\n\nJoel Lehman (01:06): It’s great. Yeah, lots of nice people, lots of interesting ideas, and good to see old friends.\n\n\nWhy aligned AI might not be enough\nDaniel Filan (01:12): What are you currently thinking about, working on?\n\nJoel Lehman (01:15): I’m really interested in positive visions for how AI could go well, set against a default assumption that AI as currently deployed might exacerbate existing societal problems. The rough int",
      "wordCount": 2579
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "MpLmcLBiEbpzv2awg",
    "title": "AXRP Episode 38.5 - Adrià Garriga-Alonso on Detecting AI Scheming",
    "slug": "axrp-episode-38-5-adria-garriga-alonso-on-detecting-ai",
    "url": null,
    "baseScore": 9,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-01-20T00:40:07.077Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/3A7CK4-1OFo)\n\nSuppose we’re worried about AIs engaging in long-term plans that they don’t tell us about. If we were to peek inside their brains, what should we look for to check whether this was happening? In this episode Adrià Garriga-Alonso talks about his work trying to answer this question.\n\nTopics we discuss:\n\n*   [The Alignment Workshop](#alignment-workshop)\n*   [How to detect scheming AIs](#how-to-detect-scheming-ais)\n*   [Sokoban-solving networks taking time to think](#sokoban-solving-nets-taking-time-to-think)\n*   [Model organisms of long-term planning](#model-organisms-of-long-term-planning)\n*   [How and why to study planning in networks](#how-and-why-study-planning-networks)\n\n**Daniel Filan** (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area [Alignment Workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at [axrp.net](https://axrp.net/), and as usual, if you want to support the podcast, you can do so at [patreon.com/axrpodcast](https://www.patreon.com/axrpodcast). Let’s continue to the interview. Adrià, thanks for chatting with me.\n\n**Adrià Garriga-Alonso** (00:30): Thank you for having me, Daniel.\n\n**Daniel Filan** (00:31): For people who aren’t familiar with you, can you say a little bit about yourself, what you do?\n\n**Adrià Garriga-Alonso** (00:35): Yeah. My name is Adrià. I work at FAR.AI. I have been doing machine learning research focused on safety for the last three years and before that, I did a PhD in machine learning. I’ve been thinking about this for a while, and my current work is on mechanistic interpretability and specifically how we can use interpretability to detect what a neural network wants and what it might be scheming towards.\n\nThe Alignment Workshop\n----------------------\n\n**Daniel Filan** (01:04): Before I get into that too much, we’re currently at this alignment workshop being run by FAR.AI. How are you finding the workshop?\n\n**Adrià Garriga-Alonso** (01:10): It’s really great. I have had lots of stimulating conversations and actually, I think my research will change at least a little bit based on this. I’m way less sure of what I am doing now. I still think it makes sense, but I think I need to steer it somewhat, but I will still tell you what I’ve been working on if you ask.\n\n**Daniel Filan** (01:33): Sure, sure. Has the stuff that’s changed your mind about… has it just been chatting with people? Has it been presentations?\n\n**Adrià Garriga-Alonso** (01:39): Yeah, mostly talking to people. I think I’ve been thinking a lot more about what scheming would actually look like and in which conditions it would emerge. I think, without going too deeply into it now (but maybe later we will), I was thinking of scheming as a thing that requires a lot of careful consideration about what futures might look like and perhaps an explicit prediction of what the future will look like under various things the AI could do, and then thinking of how good they are for the AI.\n\n(02:16): But actually, it might be a lot more reactive. It might just be the AI sees that… The chain of reasoning from “I want to do thing X. If I don’t show myself to be aligned, I won’t be able to do thing X, therefore let’s do whatever it seems like in the moment to seem aligned.” – it’s not a very long one that requires a lot of careful consideration. Perhaps the mechanisms I was hoping to catch it with won’t work.\n\nHow to detect scheming AIs\n--------------------------\n\n**Daniel Filan** (02:49): Wait, before we get into… You said you want to do mechanistic interpretability to find scheming. What sorts of things do you mean by scheming?\n\n**Adrià Garriga-Alonso** (02:58): Specifically talking about when an AI that has internalized some goals - so it’s an agent, it wants something - is taking actions to get more of that goal that we wouldn’t approve of if we knew of, or if we knew why it’s doing them. It’s hiding intentions from, in this case, developers or users.\n\n**Daniel Filan** (03:29): Okay. Hiding intentions from developers or users for a reason: it seems like that can encompass tons of behavior or tons of things you could be thinking about. Maybe you can say just a little bit about how you’ve been tackling it in your existing research.\n\n**Adrià Garriga-Alonso** (03:53): Yes, that’s a very good question. I’ve been thinking about scheming specifically towards a goal or towards a long-term goal. The generator here is the AI wants something that can be accomplished better by (say) having more resources. That’s the reason that we are worried about a loss of control.\n\n(04:15): Conversely, if there is no long-term thing that the AI wants and needs more resources for, then maybe we shouldn’t be worried about loss of control too much. Maybe this long-term wanting of things is really the key characteristic that can make AI dangerous in an optimized misalignment kind of way. Targeting it seems useful. I’ve been approaching this by coming up with toy models or… At the start, they’re toy, so they’re progressively less toy, but now we’re in fairly early days with this.\n\n(04:54): Coming up with models that display these long-term wants and behavior, that try to get the long-term wants and then figure out how are they thinking about this and by what mechanism are these wants represented, by what mechanism are these translated into action? If we can understand that in smaller models, maybe we can understand that in somewhat larger models and quickly move on to the frontier LLMs and maybe try and understand if they have a want machinery somewhere and if they take actions based on that.\n\nSokoban-solving networks taking time to think\n---------------------------------------------\n\n**Daniel Filan** (05:29): Concretely, what kinds of models have you actually trained and looked at?\n\n**Adrià Garriga-Alonso** (05:35): The models I’m training now are game-playing agents. We’ve trained this recurrent neural network that plays [Sokoban](https://en.wikipedia.org/wiki/Sokoban). Sokoban is a puzzle game that requires many… The goal of it is there’s a maze-like or a grid world with some walls and then some boxes and some places where you should push the boxes to, and then you’ve got to push the boxes on the targets. If you don’t put them in the correct sequence, then it’s very easy to get stuck and you can’t solve the level anymore-\n\n**Daniel Filan** (06:11): Right, because you can only push them, you can’t pull them. If they get in a corner, you can’t get them out.\n\n**Adrià Garriga-Alonso** (06:15): Yes, that’s right. Yes, exactly. In a corner, you can’t get them out, you can’t push two boxes at a time. It has to be only one box. It’s easy to inadvertently block your next box with the previous box or yes, get into a corner and now you can only move in some directions. For example, if you get it into a wall, then you can only move along that wall. The geometry of the levels gives you complicated actions that you may need to take.\n\n(06:46): We started from the point of view of a 2019 DeepMind paper by [Arthur Guez](https://www.gatsby.ucl.ac.uk/~aguez/) and others, called [“An investigation of model-free planning”](https://arxiv.org/abs/1901.03559). In that paper, they did the setup that we replicated here. The setup is: they just train a neural network with reinforcement learning, just by giving it a large positive reward if it solves the level and then a small negative reward for every step it takes so that it goes quickly.\n\n(07:16): They just train it to play this game, and it plays decently well on a bunch of random levels that they generate, and then they found that if you give this neural network more time to think at the start, then it is better able to solve these levels. 5% of levels that it previously was not able to solve, now it solves-\n\n**Daniel Filan** (07:36): Sorry, when you say “more time to think at the start”…?\n\n**Adrià Garriga-Alonso** (07:39): In this case, this is analogous to giving language models chain of thought. The specific mechanism is… The recurrent neural network has a latent state it brings from one timestep to the next, so they just process the initial input, the image of the puzzle, how it’s set up at the first step, many times.\n\n**Daniel Filan** (08:01): Okay. So that is this setup that you’re replicating.\n\n**Adrià Garriga-Alonso** (08:06): Yes.\n\n**Daniel Filan** (08:06): What do you do with that?\n\n**Adrià Garriga-Alonso** (08:07): We got that, and we’ve been poking at it in various ways. Some of the ways were about trying to better understand: in what circumstances does more thinking time help solve the levels? Or why does this happen from a behavioral point of view? What happens to the levels…\n\n(08:28): Actually, I slightly misspoke earlier: I said 5% of levels that were not solved are now solved. That’s not actually true. 5% more levels get solved, but some of the ones that were solved before now are not and some of the ones that were not solved before now are. These don’t cancel out. It ends up being a net gain.\n\n(08:48): We looked at a bunch of these levels and then we looked at how the neural network seem to behave naturally. Why might it have this capability to generalize, to just if you give it more time perform better? The original impetus there was: maybe because you give it more time to think and then it does better, it’s doing some iterative algorithm on the inside that can use more computation to maybe consider more possible solutions and then see if one of them is the good one.\n\n(09:22): The things that we found there were pretty interesting for a model this small. It seems that the neural network has learned a meta-strategy in training of when it needs more time to think because a level is complicated, just taking time to think. The way it takes time to think is just it paces around the level without pushing any boxes. This does not change the game state in an irreversible way, but it just gives more processing time to the network. We found many instances of this. When the neural network paces around, we measured this with the proxy of how often does it end up in a state that it previously wasn’t. It must have moved in some direction, then came back.\n\n(10:08): This happened overwhelmingly at the beginning of the level. You might think that it’s using this to plan out the whole level, how it’s going to do it, and then it’s going to do it. Also if we substitute these cycle steps by just processing the current input without taking any action, which is again out of distribution from training because it always has to take an action, then these cycles disappear in the next few seconds. You can substitute for the cycles with just thinking time. Perhaps the thinking time is the main reason that the cycles are there.\n\n**Daniel Filan** (10:42): That’s interesting. If I think about stories of AI being scary, it’s AI gaining resources, and one of the resources AIs can gain is cognitive resources, just time to think.\n\n**Adrià Garriga-Alonso** (10:55): That’s a good point.\n\n**Daniel Filan** (11:00): You can have these grid worlds where an AI has to get money in order to do a thing, and you can use reinforcement learning to train the AI to get the money. I’m not aware of previous work showing reinforcement learning on neural networks resulting in networks learning to give themselves more time to think about stuff. Is there previous work showing this?\n\n**Adrià Garriga-Alonso** (11:24): I actually don’t know. I don’t know of any either. The literature research we did mostly found things like architectures that explicitly try to train in variable amounts of thinking time. I believe the [Neural Turing Machine](https://arxiv.org/abs/1410.5401) -\n\n**Daniel Filan** (11:41): And even AlphaGo: didn’t AlphaGo do this to some degree?\n\n**Adrià Garriga-Alonso** (11:45): Getting more time to think?\n\n**Daniel Filan** (11:48): In [the Lee Sedol games](https://en.wikipedia.org/wiki/AlphaGo_versus_Lee_Sedol), it didn’t have a constant time for each move.\n\n**Adrià Garriga-Alonso** (11:51): Oh. Maybe that was pre-programmed after the fact: “search until you get enough value” or “you get a move that puts the probability of winning to a high enough amount”. I don’t actually know. I really think I should look this up now. This was not the main focus of the work, so I didn’t claim that this was novel or anything, I was like, “okay, this is an interesting behavior of this neural network”.\n\nModel organisms of long-term planning\n-------------------------------------\n\n**Daniel Filan** (12:18): Okay, so what was the main focus of the work?\n\n**Adrià Garriga-Alonso** (12:21): The main focus is: train this neural network and now we have a model of… Or this is evidence that internally, it really has this goal, and it’s perhaps thinking about how to go towards it. The goal was: now we can use this as a model organism to study planning and long-term goals in neural networks.\n\n(12:43): This is a useful model organism to start with because it is very small. It is only 1.29 million parameters, which is much, much smaller than any LLMs, but it still displays this interesting goal-directed behavior. Maybe we have a hope of just completely reverse-engineering it with current interpretability techniques, and now we know what planning looks like in neural networks when it’s learned naturally, and maybe we can use this as a way to guide our hypotheses for how larger models might do it.\n\n(13:14): We took [some initial steps in this direction](https://tuphs28.github.io/projects/interpplanning/) with the help of a team of students at Cambridge as well, including [Thomas Bush](https://tuphs28.github.io/) as the main author. I advised this a little bit. They trained probes that could predict the future actions of this neural network, many steps in advance, by very simple linear probes that are only trained on individual “pixels”, so to say, of the neural network’s hidden state, because the hidden state has the same geometry as the input.\n\n(13:52): We also replicated this. We can also train these probes that predict the actions many steps in advance, and then some of these are causal in the sense that if you change the plan that is laid out by… These probes give you a set of arrows, let’s say, on the game state that says, “the agent is going to move over this trajectory” or “the boxes will move in this trajectory and this one will go here, that one will go here, that one will go here”. We can also write to this and then that actually alters the trajectory that the agent takes. We can put a completely new plan on the hidden state of the neural network and then it will execute it, which leads us to believe that the way in which it comes up with actions really is coming up with these plans first and then executing them.\n\n**Daniel Filan** (14:44): Okay. This actually sounds kind of similar to [this work](https://www.alignmentforum.org/posts/gRp6FAWcQiCWkouN5/maze-solving-agents-add-a-top-right-vector-make-the-agent-go) that [Alex Turner](https://turntrout.com/welcome) has done on activation steering.\n\n**Adrià Garriga-Alonso** (14:54): The mazes and the cheese.\n\n**Daniel Filan** (14:55): The mazes and the cheese, yeah. Interestingly, I think Alex’s interpretation is that agents are a bit more… Without putting words in his mouth, this is my interpretation of his interpretation, but I think he thinks that this shows that agents are a bit more… They have these [shards](https://turntrout.com/shard-theory) and they have this “I want to find the cheese” shard versus “I don’t want to find the cheese” shard and whichever shard is activating stronger-\n\n**Adrià Garriga-Alonso** (15:22): -Is the one that determines the behavior now.\n\n**Daniel Filan** (15:23): Yeah, I think he has a relatively non-unitary planning interpretation of those results. It seems like you have a different interpretation of yours. Do you think it’s because there’s an important difference between the results?\n\n**Adrià Garriga-Alonso** (15:34): That’s a good question. Initially for this project, we also targeted… Okay, I think there’s some difference. Here’s the difference I think there is. Initially for this project, we also targeted maze-solving neural networks. We trained a specific kind of algorithmic recurrent network that [a NeurIPS paper](https://arxiv.org/abs/2202.05826) \\- by [Arpit Bansal](https://arpitbansal297.github.io/), I think it is, I can give the citation later - came up with, and then we looked at how it solved mazes.\n\n(16:05): It was a very simple algorithm that could be said to be planning but does not generalize very much. The algorithm was [dead-end filling](https://en.wikipedia.org/wiki/Maze-solving_algorithm#Dead-end_filling). You can look it up on Wikipedia. It’s a way to solve simple mazes, which are those that don’t have cycles. I think these are also the mazes that Alex’s work used. And it just works by: you start with your maze as a two-color image, let’s say it’s black and white, and then the walls are black and then the spaces where you can go are white, and then for every square of the maze, you see if it has three walls next to it, adjacent to it, and then if so, you also color it black.\n\n(16:48): You can do this, and then you can see the hallways of the maze fill up until only the path from agent to goal is left. That’s what we observed this neural network doing very clearly. I think this is an interesting planning algorithm, but the state space of this is small enough that it fits completely in the activation of the neural network. It’s a planning algorithm that only very specifically works for this problem.\n\n(17:20): I thought that perhaps a more complicated environment where the state space is much larger and doesn’t completely fit in the activations of the neural network… because here the neural network has an activation for every location in the maze and then the states are also just locations in the mazes because there’s only one agent, so this is just the XY position. It could perfectly represent this and think about all the possible states and then plan.\n\n(17:47): I think that, plus the fact that you can also just go up and right and just get to the cheese, and if you get it somewhat wrong, you can correct it, makes it so that the neural network is less strongly selected to be able to do this long-term planning and also the long-term planning that you might find uses a lot less machinery and would be less generalizable. I think there’s a big difference in the environment here.\n\n(18:19): I guess the other thing I would say though is I do think his theory is broadly right, or maybe more right than wrong, in that I do think it’s highly likely that neural networks that you actually train are not just one coherent agent that only wants to maximize this one thing, but it might have other mechanisms in there, such as “I just like going right”, or “I like this other goal”. Also those might play a role. And I think we might find this in this agent, but I think we selected it pretty strongly towards having this one goal of solving this level and the levels are solved with planning, so it probably does that a whole bunch.\n\n**Daniel Filan** (19:07): You mentioned your paper, a paper by some Cambridge people: what are the names of these papers so that people can look?\n\n**Adrià Garriga-Alonso** (19:13): [The Cambridge one](https://tuphs28.github.io/projects/interpplanning/) isn’t published yet, but we will soon. The main author is Thomas Bush, and the title is something like An Example of Planning in Model-Free Reinforcement Learning. I will have to look this up and send it to you.\n\n**Daniel Filan** (19:34): Hopefully that’s enough for people to Google it.\n\n**Adrià Garriga-Alonso** (19:37): And then our paper is called [“Planning in a recurrent neural network that plays Sokoban”](https://arxiv.org/abs/2407.15421).\n\nHow and why to study planning in networks\n-----------------------------------------\n\n**Daniel Filan** (19:44): Going back up the stack a little bit, I guess the reason you were interested in this work is because it sounded like you were interested in understanding agents that were scheming, they’re deceptively aligned, they’re trying to deceive you and do some tricky stuff in order to achieve some goal, and that’s why you’re studying goal orientation and how it gets implemented.\n\n**Adrià Garriga-Alonso** (20:07): That’s right.\n\n**Daniel Filan** (20:12): You mentioned that you had some reservations about that approach, which I understood to be something like: maybe it doesn’t take tons of planning to do this sort of scheme behavior.\n\n**Adrià Garriga-Alonso** (20:24): Yeah, maybe you don’t need to have a goal that you explicitly evaluate, you explicitly think of futures. Yeah, I think you’re broadly right. I just wanted to expand a bit.\n\n**Daniel Filan** (20:33): I guess there are two types of scheming behavior, right? There’s one type which is: you are being trained and somehow you know you’re being trained, and so you’re like “okay, I’m just going to play along because I can’t really do anything else. Nothing else is particularly useful right now”. That seems like it is not super cognitively complex, right?\n\n**Adrià Garriga-Alonso** (20:54): Yeah.\n\n**Daniel Filan** (20:54): But then there’s a second thing which is: you’re an AI, you finish being trained, you’re being deployed, but there are humans around, they don’t want you to execute your super devious plan, so you’ve somehow got to get around them. I might think that actually maybe it does need a lot of planning to execute the “actually do nasty stuff” part.\n\n**Adrià Garriga-Alonso** (21:18): Right, right. That’s a good point. If the safeguards are complicated enough and there’s many things that need to go right, then it would perhaps need a lot of planning. I guess I agree with this. Maybe the takeaway here is more that my initial hypothesis as to what planning would look like is wrong rather than studying planning is not useful.\n\n**Daniel Filan** (21:48): Okay. What was the initial hypothesis?\n\n**Adrià Garriga-Alonso** (21:50): I had this model of: the reason that we can get at planning in neural networks, even if we don’t solve all of interpretability, is that planning needs… Okay, I say this now, but I’m not sure I entirely believe it, but for a second, let’s put this hat on.\n\n**Daniel Filan** (22:09): Bracketing it.\n\n**Adrià Garriga-Alonso** (22:11): Yeah, yeah, exactly. Planning needs a mechanism that thinks about possible outcomes or possible ways that the world can go from now on. I guess you need a world model, but I’m also abstracting over this. I’m just saying “okay, there’s just going to be several possible futures depending on what I do, and I’m going to think about a bunch of possible things I might do, and then what will the world look like in a while”.\n\n(22:38): The neural network or the planning algorithm would evaluate all of these possible outcomes according to some internal criterion, which is basically “how much do I like these worlds that we end up in?”, and then it would basically look at all the evaluations and then pick the best one, and then execute that. I was like: okay, we might not understand what’s the abstract representation of the future outcomes. That seems complicated, especially if the agent is very intelligent.\n\n(23:05): We might also not understand exactly how does it get translated into an evaluation. It might have lots of intuitive things, and we might not know what each thing represents. But what we can definitely see, even if we can’t do interpretability on most of everything else, we can see that there’s a number at the end that is somewhere in the activations of the neural network. There’s a bunch of these and the best one gets picked, and that is what gets executed.\n\n(23:31): If we would find this computational signature of this mechanism, which picks the best number out of many, and that determines the action, that is a planning-like thing that would be happening. I think this will happen also in a bunch of other circumstances, but perhaps we can just catalog the, I don’t know, thousands of these instances in the neural network and if they all coherently do something, maybe we would know when the neural network is expecting to get a big reward, or maybe we would be able to ablate these to zero to prevent any long-term goals from happening.\n\n**Daniel Filan** (24:03): Although I guess just having a bunch of actions that you score doesn’t say that the goal is long-term, right? If I’m like: I really like to wave my left arm, I’m going to consider actions that I’m going to take and which one of them involves waving my left arm, I’m going to do +1000 to that, and I’m going to pick that action like that.\n\n**Adrià Garriga-Alonso** (24:21): Yeah, I guess my intuition here is also that goals that are fairly short-term will be more efficiently expressed as kind of cached skills or something, or things that the neural network can access. Or you’re not thinking explicitly about which muscles you’re moving when you’re raising your left arm. That kind of-\n\n**Daniel Filan** (24:44): Don’t presume things about how I think about my muscles, please.\n\n**Adrià Garriga-Alonso** (24:46): Okay, okay. I don’t know what it’s like to be Daniel Filan.\n\n**Daniel Filan** (24:51): Most of the time when I’m lifting my left arm, I’m not explicitly thinking about muscles, but from time to time it’s fun, yeah.\n\n**Adrià Garriga-Alonso** (24:57): Or I suppose if you’re trying to gain a new skill of exactly how to move your left arm… I don’t know, if you do dancing, then you might have some technique that you need to move your arm explicitly in a particular way you haven’t done before.\n\n**Daniel Filan** (25:12): Then I would normally not think about muscle contractions, I guess. I don’t know. I guess you’d visualize the thing. I think that thing where I would think about muscle contractions is maybe in a weightlifting context.\n\n**Adrià Garriga-Alonso** (25:22): Good point, or physical therapy.\n\n**Daniel Filan** (25:24): Yeah. Sorry, you were saying something and then I brought in diversions.\n\n**Adrià Garriga-Alonso** (25:28): No, no, no, no. I think I was finished with my point, which is that most of the time short-term goals wouldn’t require all this explicit outcome planning and would just use this other mechanism.\n\n(25:39): And we would also get some short-term goals there. I was hoping it would be less, or maybe we would be able to tell them apart in some way, or I guess I also hope that now that we have located these goals, maybe we can trace them backwards a little bit: what in the neural network produces this evaluation? Maybe we can understand a little bit about that. Even if we don’t understand it perfectly, we can get some broad strokes of what it is.\n\n(26:09): It’s difficult for the neural network to modify this. Even if it’s a very intelligent agent, it wouldn’t necessarily be able to fool our best classifiers by changing the way that its internal brain is structured. That seems pretty difficult.\n\n**Daniel Filan** (26:28): Okay. Cool. We’re about out of time. I want to give you time to enjoy the rest of the workshop. Thanks very much for chatting with me.\n\n**Adrià Garriga-Alonso** (26:36): Thank you very much for having me.\n\n**Daniel Filan** (26:38): This episode was edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with patrons such as Alexey Malafeev. To read a transcript of the episode, or to learn how to support the podcasts yourself, you can visit [axrp.net](https://axrp.net/). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nSuppose we’re worried about AIs engaging in long-term plans that they don’t tell us about. If we were to peek inside their brains, what should we look for to check whether this was happening? In this episode Adrià Garriga-Alonso talks about his work trying to answer this question.\n\nTopics we discuss:\n\n * The Alignment Workshop\n * How to detect scheming AIs\n * Sokoban-solving networks taking time to think\n * Model organisms of long-term planning\n * How and why to study planning in networks\n\nDaniel Filan (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area Alignment Workshop, which is run by FAR.AI. Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net, and as usual, if you want to support the podcast, you can do so at patreon.com/axrpodcast. Let’s continue to the interview. Adrià, thanks for chatting with me.\n\nAdrià Garriga-Alonso (00:30): Thank you for having me, Daniel.\n\nDaniel Filan (00:31): For people who aren’t familiar with you, can you say a little bit about yourself, what you do?\n\nAdrià Garriga-Alonso (00:35): Yeah. My name is Adrià. I work at FAR.AI. I have been doing machine learning research focused on safety for the last three years and before that, I did a PhD in machine learning. I’ve been thinking about this for a while, and my current work is on mechanistic interpretability and specifically how we can use interpretability to detect what a neural network wants and what it might be scheming towards.\n\n\nThe Alignment Workshop\nDaniel Filan (01:04): Before I get into that too much, we’re currently at this alignment workshop being run by FAR.AI. How are you finding the workshop?\n\nAdrià Garriga-Alonso (01:10): It’s really great. I have had lots of stimulating conversations and actually, I think my research will change at least a little bit based on this. I’m way less sure of what I am doing now. I still think it makes sense, bu",
      "wordCount": 4657
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "wfW6iL96u26mbatep",
        "name": "Bounded Rationality",
        "slug": "bounded-rationality"
      },
      {
        "_id": "KEAWfxwjitNJFrC68",
        "name": "Deceptive Alignment",
        "slug": "deceptive-alignment"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "LvswJts75fnpdjAEj",
    "title": "MATS mentor selection",
    "slug": "mats-mentor-selection",
    "url": null,
    "baseScore": 44,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 12,
    "createdAt": null,
    "postedAt": "2025-01-10T03:12:52.141Z",
    "contents": {
      "markdown": "Introduction\n============\n\nMATS currently has more people interested in being mentors than we are able to support—for example, for the Winter 2024-25 Program, we received applications from 87 prospective mentors who cumulatively asked for 223 scholars[^4m9wrq7yxt3] (for a cohort where we expected to only accept 80 scholars). As a result, we need some process for how to choose which researchers to take on as mentors and how many scholars to allocate each. Our desiderata for the process are as follows:\n\n*   We want to base our decisions on expert opinions of the quality of various research directions.\n*   We want the above opinions to be sourced from a range of perspectives with the AI existential safety field, to incorporate multiple perspectives on alignment research and other research areas we think are important, such as AI governance and policy, AI security, and other approaches for addressing AI catastrophic risk.\n*   We want to incorporate information about how good prospective mentors are at mentorship—both information internal to MATS, as well as information advisors may have.\n\nIn this post, we describe the process we used to select [mentors for the Winter 2024-25 Program](https://www.matsprogram.org/mentors), which will be very close to the process we will use to select mentors for the Summer 2025 Program. In a nutshell, we select advisors, who select mentors, who select scholars, who often select specific research projects, in a “chain of trust,” with MATS input and oversight at every stage. This system is designed to ensure that we make reasonable decisions about the scholars, mentors, and, ultimately, the research we support, even if MATS staff are not subject matter experts for every branch of AI safety research. We want to make this \"chain of trust\" structure transparent so that potential funders and collaborators can trust in our process, even if we cannot share specific details of selection (e.g., what advisor X said about prospective mentor Y).\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/7e8b05735ad293345a4e1434edd04d2636dbececae34ced4eb0b378b191afc05/ok7uikc60mxnqkyvlccy)\n\nMentor selection\n================\n\nFirst, we solicited applications from potential mentors. These applications covered basic information about the mentors, the field they work in, their experience in research and mentoring, what projects they might supervise, and how many scholars they might supervise.\n\nThese applications were then reviewed by a team of 12 advisors. Our advisors were chosen to be people with experience in the AI existential safety community, as well as to cover a range of perspectives and subfields, as discussed above. We selected advisors by first creating a long list of approximately 100 candidates, then narrowing it down to a short list of approximately 30 candidates, who we invited to advise us. Of these 30, 12 candidates were available. The advisors include members of AI safety research non-profits, AI \"scaling lab\" safety teams, AI policy think-tanks, and AI safety grantmaking organizations. Breaking down advisors by field (and keeping in mind most advisors selected multiple fields):\n\n*   6 worked on control / red-teaming.\n*   6 worked on some kind of interpretability, including:\n    *   5 on concept-based interpretability;\n    *   5 on mechanistic interpretability.\n*   5 worked on alignment evaluations or demonstrations, and of those, 3 worked on dangerous capability evaluations or demonstrations (nobody else worked on dangerous capability evaluations or demonstrations).\n*   5 worked on AI governance/policy, national security, and/or information security, including:\n    *   4 on AI governance/policy;\n    *   4 on national security;\n    *   3 on information security.\n*   3 worked on scalable oversight.\n*   3 worked on cooperative AI, agent foundations, and/or value alignment, including:\n    *   2 on cooperative AI;\n    *   2 on value alignment;\n    *   1 on agent foundations.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvswJts75fnpdjAEj/z8f9rjduzluon6avztod)\n\n*Number of advisors who focus on various fields. Note that most advisors selected multiple fields.*\n\nMost advisors were not able to rate all applicants, but focused their energies on applicants whose research areas matched their own expertise. For each rated applicant, advisors were able to tell us:\n\n*   Their overall rating for that applicant (given as a number between 1 and 10);\n*   How many scholars at minimum the applicant should have;\n*   How many scholars the applicant would ideally have;\n*   How confident they are in their rating;\n*   Whether they have a conflict of interest with the applicant.\n\nAdvisors also had a field to write free-form text notes. Not all advisors filled out all fields.\n\nAs shown in the figure below, all but one application was reviewed by at least three advisors, and the median applicant was reviewed by four advisors. One applicant who applied late was only able to be reviewed by a single reviewer.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/43e864199e4a13722bce0df50cf0e3be8cbffd4f4548751478c887101ce35c0b/f4dglmnjgfpo5ua3rnoy)\n\n*If someone was rated by* n *advisors, they go in the bin between* n *and* n+1*. So, for example, there were 15 applications that were reviewed by 5 advisors.*\n\nWe primarily considered the average ratings and scholar number recommendations, taking into account confidence levels and conflicts of interest. Our rule of thumb was that we accepted applicants rated 7/10 and higher, and chose some of the applicants rated between 5/10 and 7/10 to enhance research diversity (in part to counter what we believed to be potential lingering biases of our sample of advisors). To choose mentors for certain neglected research areas, we paid special attention to ratings by advisors who specialize in those research areas.\n\nFor accepted mentors, we chose scholar counts based on advisor recommendations and ratings, as well as ratings from MATS Research Managers and scholars for returning mentors. The cut-offs at 5/10 and 7/10 were chosen partly to ensure we chose highly-rated mentors, and partly in light of how many scholars we wanted to accept in total (80, in this program). For edge cases, we also considered the notes written by our advisors.\n\nWe then made adjustments based on various contingencies:\n\n*   We accepted one applicant who expressed interest after we expected to have finalized mentor counts, as that applicant was very prominent in the field of AI existential safety.\n*   Some mentors accepted fewer scholars than we offered them spots. This caused us to offer more scholars to other mentors and add further mentors from our waiting list to get as close as possible to the desired 80 scholars.\n    *   We selected mentors to give additional scholars based on the mentors' average ratings by advisors, advisors' recommended scholar allocations, and mentors' desired scholar allocations.\n    *   We selected additional mentors based on their average ratings by advisors and whether we felt they would enhance research diversity.\n    *   In the end, we were not as successful at onboarding additional mentors and scholars as we hoped and only accepted 77 scholars.\n\nMentor demographics\n===================\n\nWhat sort of results did the above process produce? One way to understand this is to aggregate mentors by \"track\"—MATS’s classification of the type of AI safety research mentors perform. For the Winter 2024-25 program, we have five tracks: oversight & control, evaluations, interpretability, governance & strategy, and agency[^a1nxoab8pre]. Note that these are coarse-grained, and may not perfectly represent each mentor’s research.\n\nThis is how our applicants broke down by track:\n\n*   Oversight & control: 18 applicants (21%)\n*   Evaluations: 14 applicants (16%)\n*   Interpretability: 24 applicants (28%)\n*   Governance & strategy: 15 applicants (17%)\n*   AI agency: 15 applicants (17%)\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/876c438f50adcdace74ba172966542c67be005eaa3ce8d67bb2397de7a3f2b64/zxeon3nfp3ctaysf7ets)\n\nOur accepted mentors broke down this way:\n\n*   Oversight & control: 11 mentors (29%)\n*   Evaluations: 8 mentors (21%)\n*   Interpretability: 7 mentors (18%)\n*   Governance: 6 mentors (16%)\n*   AI agency: 6 mentors (16%)\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/0de2cccc2792771bb200d990dbe4d16f1289c07abe522f89db7752403ba043ea/bcdgnn5hr3hwc5f8fpkr)\n\nProportionally, the biggest deviations between the applying and accepted mentors were that relatively few interpretability researchers were accepted as mentors, and relatively many evaluations and oversight & control researchers were accepted.\n\nTo give a better sense of our mentors’ research interests, we can also analyse the accepted mentors by whether they focused on:\n\n*   Technical or governance research;\n*   Model externals or internals research;\n*   Empirical or theoretical research.\n\nThese were somewhat subjective designations and, for the latter two distinctions, some mentors did not neatly fall either way.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/86081453a249a621e68cbdd4a7d9c0bd012cfa70589461ae35391614b8290bac/c57kttbjtxx6olianmfi)\n\n*The yellow portion of the bar is mentors who did not neatly fall into either category.*\n\nScholar demographics\n====================\n\nAnother way to measure the cohort's research portfolio is to look at the breakdown of scholar count assigned to each mentor.[^5s4p5mnlq] Firstly, we can distinguish scholars by their mentors' research track:\n\n*   Oversight & control: 24 scholars (31%);\n*   Interpretability: 18 scholars (23%);\n*   Evaluations: 17 scholars (22%);\n*   AI agency: 10 scholars (13%);\n*   Governance: 8 scholars (10%).\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/b13d6bff8c17c49cbb0a23aaf46677148f47c9c46fde44d7e8e75d4e488e0bd8/pndjvix0jlsj88zgkwb3)\n\nThis is somewhat more weighted towards interpretability and away from governance than our mentor count.\n\nAnother relevant factor is how many scholars each mentor has, shown in the histogram below. The median scholar will be working with a three-scholar mentor—that is, with two other scholars under the same mentor. This data is shown in histogram form below. Note that for the purpose of these statistics, if two mentors are co-mentoring some scholars, they are counted as one \"mentor.\"\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/d66d5f5a28129de77ade0460abaf7fb1125429c56ca1de32936b16912b322374/himz1ucqnn5e65hcgcgw)\n\n*Numbers are provisional and subject to change. If a mentor has* n *scholars, they go in the bin between* n *and* n+1*. So, for example, there are 15 mentors with 2 scholars. Total scholar count will be lower than in this histogram, since mentors who have not yet determined the division of scholars between them were assigned more scholars in aggregate than they accepted.*\n\nThis can be compared to the distribution of scholars per mentor in the Summer 2024 Program. In that program, the distribution was more concentrated: more scholars were working in streams of one or two scholars (the median scholar was working with a 2-scholar mentor, i.e. with only one other scholar), and there were fewer mentors with 3-5 scholars.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/a8b62c34f740158cc805dcb0c1dfd1387c8a36b494a88713c9aff2cbb9c5a2a0/pui5lgzeyb46b2mcdfov)\n\nAs with the mentors, we can also break down scholar assignments by their mentors’ research focus.\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvswJts75fnpdjAEj/bx8u7ewwvxgz2tpqmnlh)\n\n*The yellow portion of the bar is scholars whose mentor did not neatly fall into either category.*\n\n![](https://res.cloudinary.com/lesswrong-2-0/image/upload/f_auto,q_auto/v1/mirroredImages/LvswJts75fnpdjAEj/qygg0c70uhsf4m1ftc3d)\n---------------------------------------------------------------------------------------------------------------------------------\n\nAcknowledgements\n================\n\nThis report was produced by the [ML Alignment & Theory Scholars Program](https://www.matsprogram.org/). [Daniel Filan](https://www.lesswrong.com/users/danielfilan) was the primary author of this report and [Ryan Kidd](https://www.lesswrong.com/users/ryankidd44) scoped, managed, and edited the project. Huge thanks to the many people who volunteered to give their time to mentor scholars at MATS! We would also like to thank our 2024 donors, without whom MATS would not be possible: [Open Philanthropy](https://www.openphilanthropy.org/), [Foresight Institute](https://foresight.org/), the [Survival and Flourishing Fund](https://survivalandflourishing.fund/), the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), Craig Falls, and several donors via [Manifund](https://manifund.org/projects/mats-funding).\n\n[^4m9wrq7yxt3]: More precisely: when people applied to mentor, they answered the question “What is the average number of scholars you expect to accept?”. 223 (or more precisely, 222.6) is the sum of all applicants’ answers. \n\n[^a1nxoab8pre]: By “agency”, we mean modeling optimal agents, how those agents interact with each other, and how some agents can be aligned with each other. In practice, this covers cooperative AI, agent foundations, value learning, and \"shard theory\" work \n\n[^5s4p5mnlq]: Note that scholar counts are not yet finalized—some co-mentoring researchers have not yet assigned scholars between themselves. This means that the per-track numbers will be correct, since those mentors are all in the same track, but the statistics about number of scholars per mentor will not be precisely accurate.",
      "plaintextDescription": "Introduction\nMATS currently has more people interested in being mentors than we are able to support—for example, for the Winter 2024-25 Program, we received applications from 87 prospective mentors who cumulatively asked for 223 scholars[1] (for a cohort where we expected to only accept 80 scholars). As a result, we need some process for how to choose which researchers to take on as mentors and how many scholars to allocate each. Our desiderata for the process are as follows:\n\n * We want to base our decisions on expert opinions of the quality of various research directions.\n * We want the above opinions to be sourced from a range of perspectives with the AI existential safety field, to incorporate multiple perspectives on alignment research and other research areas we think are important, such as AI governance and policy, AI security, and other approaches for addressing AI catastrophic risk.\n * We want to incorporate information about how good prospective mentors are at mentorship—both information internal to MATS, as well as information advisors may have.\n\nIn this post, we describe the process we used to select mentors for the Winter 2024-25 Program, which will be very close to the process we will use to select mentors for the Summer 2025 Program. In a nutshell, we select advisors, who select mentors, who select scholars, who often select specific research projects, in a “chain of trust,” with MATS input and oversight at every stage. This system is designed to ensure that we make reasonable decisions about the scholars, mentors, and, ultimately, the research we support, even if MATS staff are not subject matter experts for every branch of AI safety research. We want to make this \"chain of trust\" structure transparent so that potential funders and collaborators can trust in our process, even if we cannot share specific details of selection (e.g., what advisor X said about prospective mentor Y).\n\n\nMentor selection\nFirst, we solicited applications from potential mento",
      "wordCount": 1716
    },
    "tags": [
      {
        "_id": "6zBEfFYJxhSEcchbR",
        "name": "AI Alignment Fieldbuilding",
        "slug": "ai-alignment-fieldbuilding"
      },
      {
        "_id": "YYFBmLCzeFsyd27rd",
        "name": "MATS Program",
        "slug": "mats-program"
      },
      {
        "_id": "zcvsZQWJBFK6SxK4K",
        "name": "Postmortems & Retrospectives",
        "slug": "postmortems-and-retrospectives"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Ehaycuf8TXgYiG2MH",
    "title": "AXRP Episode 38.4 - Shakeel Hashim on AI Journalism",
    "slug": "axrp-episode-38-4-shakeel-hashim-on-ai-journalism",
    "url": null,
    "baseScore": 11,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2025-01-05T00:20:05.096Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/RXknLBAWOm0)\n\nAI researchers often complain about the poor coverage of their work in the news media. But why is this happening, and how can it be fixed? In this episode, I speak with Shakeel Hashim about the resource constraints facing AI journalism, the disconnect between journalists’ and AI researchers’ views on transformative AI, and efforts to improve the state of AI journalism, such as Tarbell and Shakeel’s newsletter, Transformer.\n\nTopics we discuss:\n\n*   [The AI media ecosystem](#ai-media-ecosystem)\n*   [Why not more AI news?](#why-not-more-ai-news)\n*   [Disconnects between journalists and AI researchers](#disconnects-journalists-ai-researchers)\n*   [Tarbell](#tarbell)\n*   [The Transformer newsletter](#transformer)\n\n**Daniel Filan** (00:09): Hello everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area [Alignment workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at [axrp.net](https://axrp.net/). And as usual, if you want to support the podcast, you can do so at [patreon.com/axrpodcast](https://www.patreon.com/axrpodcast). Well, let’s continue to the interview.\n\n**Daniel Filan** (00:28): I’m now chatting with Shakeel Hashim. Hello Shakeel.\n\n**Shakeel Hashim** (00:32): Hi.\n\n**Daniel Filan** (00:33): So for people who don’t know who you are, can you say a little bit about what you do?\n\n**Shakeel Hashim** (00:37): So I work at [Tarbell](https://www.tarbellfellowship.org/), which is a nonprofit that supports high-quality AI journalism. I am a grants director there, and I am also a journalist in residence. So I do my own AI journalism through Transformer, which is a weekly newsletter that summarizes AI news. And then I also do my own reporting and analysis and commentary.\n\n**Daniel Filan** (01:04): Before we really dig into it: we’re currently at this Alignment workshop being run by FAR.AI. How are you finding it?\n\n**Shakeel Hashim** (01:10): Yeah, super interesting. I’m mostly focused on AI policy stuff in my day-to-day work, so I spend less time on the technical side. And the thing I found really interesting here is meeting lots of more technical researchers, getting a sense of what they’re up to, what their focuses are. Yeah, it’s super interesting.\n\nThe AI media ecosystem\n----------------------\n\n**Daniel Filan** (01:31): So I guess you’re in a better position than most to talk about the AI media ecosystem. I’m wondering: what’s your high-level take about it?\n\n**Shakeel Hashim** (01:43): Probably two things. I think number one is that there aren’t nearly enough resources going into AI journalism as there ought to be, given the scale of the topic and its potential impact and importance. The second is that I think there’s still quite a big disconnect between what journalists think about AI and what people in the industry think about AI. Some of that is very warranted; it’s a job of journalists to be skeptical. But I worry sometimes, if journalists don’t engage a little bit more with the ideas that are held by lots of AI researchers, that journalists might not be able to keep up with what’s happening.\n\nWhy not more AI news?\n---------------------\n\n**Daniel Filan** (02:34): I guess it’s kind of a strange situation. So my understanding is that in a lot of industries there is news for that industry. So for instance, animal farming, right? My understanding is that there’s “Pig: the Newsletter”, and every week, if you’re a farmer, you can get the newsletter about pork farming and it’ll just have just a bunch of stats about pork farming written roughly from the perspective of people who are very into the pork farming scene. I assume that Bloomberg does something sort of similar with finance, or at least the Terminal. In some sense, naively it might seem surprising that there wouldn’t be more AI news journalism. Do you have a feel for why that is?\n\n**Shakeel Hashim** (03:26): There is a lot. There is more than I would, I think, have expected a couple of years ago. It’s mostly concentrated in tech publications, that’s where it lives - although lots of national news desks now have AI reporters, which is great to see. The New York Times has some people dedicated to this, Washington Post, Wall Street Journal. I think it’s still not on the scale I would like it to be. And when I talk to AI reporters, the impression I get is that there is so much more they’d like to do than they can do just because there’s so much happening in AI all the time that there aren’t enough people and there isn’t enough time in the day to cover everything you’d want to cover.\n\n**Shakeel Hashim** (04:15): The reasons for that… I think the main one is the economic state of the journalism industry. It’s not a great time to be in journalism. There are all sorts of structural reasons why the industry is struggling. So there just simply aren’t enough resources to be able to put into this kind of stuff. I also think that the pace of change here is somewhat unique, and though media organizations are responding, it’s hard to respond as quickly as things are changing, I think. It takes time to build up an AI desk.\n\n**Shakeel Hashim** (05:01): As for why there aren’t trade publications, that’s a good question. I mean, there are tech trade publications. The Information is one -\n\n**Daniel Filan** (05:08): which does do AI reporting -\n\n**Shakeel Hashim** (05:10): does a lot of AI reporting. I mean, there haven’t really been… actually, I don’t know if this is true. I want to say that there haven’t been trade publications for a bunch of stuff in the tech industry. You definitely see them around more semiconductor stuff and hardware. For software, I feel like less so. I’m not entirely sure why that is. I think it’s because the tech media just kind of fills that role.\n\n**Daniel Filan** (05:42): Right. So what do you mean when you say the tech media?\n\n**Shakeel Hashim** (05:45): So organizations like the Verge, Tech Crunch, Ars Technica, Venture Beat, and then the tech sections at all the big news outlets.\n\n**Daniel Filan** (06:04): I guess some of this might just be because if you work in tech, there’s a good chance that you’re also a tech consumer. And so I see the Verge and Ars Technica as being stuff for tech enthusiasts or people who want to talk about the newest iPhone or the newest laptop. And if that’s the same people who are working in the industry, maybe it is just sort of under that umbrella. Do you think that could be it?\n\n**Shakeel Hashim** (06:27): Yeah, I think there definitely is something there. I think there is also a thing where tech has become so important that it almost outgrows the need for a more specialized industry covering it, if that makes sense. So for instance, going back to your analogy of farming, there isn’t enough demand for there to be multiple full-time agriculture reporters at the New York Times. Tech is big enough and important enough that there is demand for there to be loads. I don’t know how many tech reporters the New York Times has, but it’s a lot. And so it gets subsumed into the more traditional media, kind of in the way politics does.\n\nDisconnects between journalists and AI researchers\n--------------------------------------------------\n\n**Daniel Filan** (07:18): So the second thing you said about your high-level take on the industry is that the journalism community was disconnected from the beliefs of the people working in the field in a way that you thought was detrimental. What specific disagreements are you thinking of?\n\n**Shakeel Hashim** (07:34): So I think the big one is the notion of transformative AI or AGI or whatever you want to call it: extremely powerful AI that can do all or very close to all of what a human can do. I think in the industry there is a pretty strong sense that this is possible and imminent. That’s one thing I found in conversations here. You’ve got people talking about the possibility of us having this by the end of next year - not that that’s likely, but that there is a non-negligible chance of that happening.\n\n**Shakeel Hashim** (08:17): I think in the journalism community, those claims are still very… I think most people really don’t buy that as an idea. I think people are very, very skeptical that this is possible at all, and certainly skeptical that it’s imminent. And I think that’s a very justified skepticism because lots of technologists have made these claims over the years: that their technology will change the world. And lots of the times it is bullshit. So I get why you would be skeptical, but I think the difficulty arises in that if these claims are true, if the AI companies and AI researchers are right, really crazy stuff is going to start happening.\n\n**Shakeel Hashim** (09:12): And it feels to me that it would be good for journalism to engage with those possibilities a bit more and treat them as hypotheticals, but engage with those hypotheticals, I guess. So if we do have AGI two years from now, what does that mean? What does that mean for the economy? What does that mean for politics? What does that mean for climate? What does that mean for catastrophic risks? What does that mean for non-catastrophic risks? I think that’s worth engaging with a bit more. And I think that part of the disconnect is I still see lots of journalists who (I think) think that the AGI timelines discussion is marketing hype. I think it would be good for people to realize that this is actually a much more sincere belief than that. I think this isn’t just marketing hype. I think these people think they’re going to do it, and I think that there are lots of good reasons to believe that they will do it.\n\n**Daniel Filan** (10:25): Yeah. One thing I find interesting… From my own perspective of how AI is covered in the news media: you’ll have outlets like the New Yorker that do… you’ll have these profiles. Somebody will do a profile on [Katja Grace](https://www.newyorker.com/magazine/2024/03/18/among-the-ai-doomsayers) or [Scott Alexander](https://www.newyorker.com/culture/annals-of-inquiry/slate-star-codex-and-silicon-valleys-war-against-the-media) or [Jaime Sevilla](https://time.com/6985850/jaime-sevilla-epoch-ai/). I think in these profiles you often see the profile basically being very sympathetic, at least to the sincereness and to some degree the basic rationality of people who think that AI is coming really soon and it’s really scary, but somehow this feels disconnected from coverage. I don’t see the New York Times having a big… I don’t read the New York Times, but I really bet that at no point in the last month has there been a big column being like, will Harris or Trump be better for artificial general intelligence catastrophes? \\[NB: a few days after recording, [such an opinion column](https://www.nytimes.com/2024/10/29/opinion/artificial-intelligence-harris-trump-election.html) did appear\\] I wonder if you have a take about why there’s that disconnect between these two different bits of it.\n\n**Shakeel Hashim** (11:39): Yeah, I think many journalists treat the ideas that people in the AI ecosystem have as being kooky interesting ideas. And I think they’re willing to accept that some people believe them, like you say in the profiles that you see, but I think they treat them as almost similar to how you would treat other weird beliefs. It’s like, “there are these people who think this crazy thing. Isn’t that interesting?”\n\n**Daniel Filan** (12:14): Yeah. I guess you have profiles of Christian Dispensationalists, but there’s not a column about whether Harris or Trump will bring in the second coming sooner.\n\n**Shakeel Hashim** (12:24): Yeah, I think to do the latter, there does need to be some internalization, and I think that most journalists at least just don’t buy it.\n\nTarbell\n-------\n\n**Daniel Filan** (12:42): So maybe this gets to your efforts at Tarbell. Can you say a bit more what you’re trying to do?\n\n**Shakeel Hashim** (12:50): We are trying to encourage and create a community of journalists who cover AI with the seriousness we think it deserves. We’re doing that in a few ways. We have a fellowship program where we take early career or aspiring journalists, we teach them about AI (with the help of lots of experts), we teach them journalism skills (again with the help of lots of experts), and then we have placement programs for them, where they then go and work in a media organization and report on AI with a hope that they build up their skills. It’s great because it means we end up with more AI journalists and hopefully they’re well-informed and well-equipped to do that work really well.\n\n**Shakeel Hashim** (13:44): We also have a journalist-in-residence program where we take mid-career or experienced journalists and we help support them so that they can dive deep in something. So we’ve had one person who was transitioning from being a crypto reporter to being an AI reporter, and so they just spent a bunch of time building up their sources, learning about AI, getting really deep in this to try and understand it. We’ve got someone else who is going to join to work on China AI reporting because that feels like a really neglected area where there’s scope for really good reporting to be done.\n\n**Shakeel Hashim** (14:27): And then we just launched this [grants program](https://www.tarbellfellowship.org/grants) where we will fund freelancers and staff journalists to pursue impactful AI reporting projects. So that’s the kind of thing that requires more time and more resources than a journalist can typically get. And in that we’re interested in funding work on AI harms, the kind of stuff that’s going on inside AI companies, policy efforts that AI companies are making, how regulators are struggling to regulate AI because of budgetary concerns or other things. And also just general explainers of complicated topics that we wish more people in the world understood.\n\n**Daniel Filan** (15:09): How much do you think the difficulty is… Do you see your mission as just getting people in who are interested, or just building up skills, having some journalists who are interested in AI, but literally there are just some facts that you might not know and if you don’t know them, you can’t report as well?\n\n**Shakeel Hashim** (15:31): Yeah, I think it’s a mix. I do think the main thing is funding, which is why most of our programs are built around that. I think there are lots of people who want to and are capable of doing really good work on this, but there just isn’t the money to support them. I do think there’s some education element to this. I mean, we spend a lot of time in the fellowship on connecting our fellows with really great experts who they might not otherwise come across: basically so that they can learn from them during the fellowship curriculum, but then also so that they can have them as resources going forward, and if they’re writing on a topic, they know who are the right people to reach out to who have really deep knowledge on this.\n\n**Shakeel Hashim** (16:28): And I think that there’s definitely something I’m interested in exploring further: are there ways we can help bridge the gaps between what the experts think and are working on and what journalists know about? Because I think there is probably scope to do a bunch of that. There’s been some really good work in the climate space on this, where there are a few organizations, who I think we take some inspiration from, who try to connect journalists with experts to help journalists dive deeper into a topic than they might otherwise be able to.\n\n**Daniel Filan** (17:06): What are these organizations?\n\n**Shakeel Hashim** (17:07): I can’t remember the names of them off my head. I think the [Climate Journalism Network](https://reutersinstitute.politics.ox.ac.uk/oxford-climate-journalism-network) is one, but I can’t remember if they’re the one I’m thinking of: they all have very similar acronyms and names. Hard to keep track of, unfortunately.\n\n**Daniel Filan** (17:21): Actually, speaking of names, a thing that has just been bugging me: Tarbell strikes me as an unusual name. I feel like in the EA artificial intelligence space, every org has the roughly the same name format. You’re either a Center or you’re an Institute, it’s probably the future of something and it’s either humanity or AI or life or something. But yeah, Tarbell is not the same name scheme. Do you know what the name is about?\n\n**Shakeel Hashim** (17:53): Yeah. So I can’t take credit for it. Cillian \\[Crosson\\], the executive director of the organization came up with it, but it’s named after [Ida Tarbell](https://en.wikipedia.org/wiki/Ida_Tarbell), who was one of the first… some people credit her with pioneering modern investigative journalism. So she did a bunch of work into Standard Oil back in the late 1800s, and her work ended up resulting in the breakup of Standard Oil and breaking the oil monopoly and was just super important and super impactful. So yeah, we’re inspired by the work she did however many hundred years ago, and we think that we’d love to see more work like that in other areas.\n\nThe Transformer newsletter\n--------------------------\n\n**Daniel Filan** (18:44): Gotcha. And so closing up, I’d like to talk a little bit about your Substack: [Transformer](https://www.transformernews.ai/), it’s called. For those who haven’t checked it out, what are you trying to do with it?\n\n**Shakeel Hashim** (18:55): A few things. So the first is: there is so much AI news every week. So much stuff happens and it is basically impossible for anyone to keep track of it.\n\n**Shakeel Hashim** (19:08): And there are lots of good AI newsletters out there, but the ones from media organizations in particular tend to focus mostly on the content that has come out of that media organization, so they’re slightly less comprehensive. Then there are some that are more focused on specific areas. So you get some that are more focused on fundraising deals, some that are more focused on policy, but there isn’t really one place where you can go to get everything. And so with my weekly digest, the aim is to be as comprehensive as possible, but as fast as possible. So there’s only really a sentence on each thing. I elaborate a bit on the bigger stories, but the majority is a few words that tell you more or less what you need to know, but you can click through to learn more.\n\n**Shakeel Hashim** (20:01): So with that, I’m aiming to just try and make everyone more informed. Lots of people in the AI ecosystem read it, which I’m delighted by. Lots of journalists read it, which I’m delighted by. Quite a lot of policymakers read it. And it’s just an attempt to make sure that people are keeping up with what’s happening, because it’s such a hard field to stay abreast of.\n\n**Shakeel Hashim** (20:24): The other \\[thing\\] is, then, with my own reporting and analysis, I want to try and draw attention to things that I think aren’t getting the attention they deserve, highlight arguments I think are good arguments… There’s often quite a lot of tacit knowledge and arguments I guess in this space, I find. Compute thresholds are a good example, which I [wrote about](https://www.transformernews.ai/p/abandon-compute-thresholds-at-your) this week, where I think lots of people have very good reasons for why they think compute thresholds are good, but I think lots of them haven’t been elucidated as well as they could be, and especially not in a really short-form fashion.\n\n**Shakeel Hashim** (21:16): So for stuff like that, where I think there’s a really good argument to be made, but I’ve not seen the good version of the argument be made simply, I hope to be able to do some of that drawing attention to stuff. So I did quite a lot of reporting on SB-1047 and the lobbying campaigns against that. I think I worked with [Garrison Lovely](https://www.garrisonlovely.com/) on [a piece about the very misleading claims](https://www.transformernews.ai/p/lies-and-deception-andreessen-horowitzs) that [Andreessen Horowitz](https://en.wikipedia.org/wiki/Andreessen_Horowitz) and [Fei-Fei Li](https://en.wikipedia.org/wiki/Fei-Fei_Li) had made about the bill. That got quite a lot of attention, which I was excited about because again, that’s one thing where lots of people were talking about it on Twitter, but I wanted to have a more concrete, well-reported thing explaining exactly what was going on there. So yeah, I guess the aim is to just improve people’s understanding of AI and AI policy in particular.\n\n**Daniel Filan** (22:16): That kind of reminds me of… I’ve also found this in the AI safety ecosystem. There’s just a bunch of stuff that people talk about but have not necessarily written or published anywhere. And this is less true now than I think it used to be, in part because more people are just, I don’t know, spending their free time getting in arguments in the comments section on the [Alignment Forum](https://www.alignmentforum.org/) or something, which genuinely I think is a great public service. But yeah, so often there’s stuff that people will just say in conversation, and if you can record it that’s great. And it doesn’t surprise me that the same thing is true in AI somewhat more generally.\n\n**Shakeel Hashim** (22:58): Yeah.\n\n**Daniel Filan** (22:59): So thanks for chatting with me, and if people are interested in AI or found Shakeel interesting, you should definitely check out Shakeel’s newsletter, [Transformer](https://www.transformernews.ai/).\n\n**Shakeel Hashim** (23:10): Thank you very much.\n\n**Daniel Filan** (23:11): This episode was edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with patrons such as Alexey Malafeev. To read a transcript of the episode, or to learn how to support the podcasts yourself, you can visit [axrp.net](https://axrp.net/). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nAI researchers often complain about the poor coverage of their work in the news media. But why is this happening, and how can it be fixed? In this episode, I speak with Shakeel Hashim about the resource constraints facing AI journalism, the disconnect between journalists’ and AI researchers’ views on transformative AI, and efforts to improve the state of AI journalism, such as Tarbell and Shakeel’s newsletter, Transformer.\n\nTopics we discuss:\n\n * The AI media ecosystem\n * Why not more AI news?\n * Disconnects between journalists and AI researchers\n * Tarbell\n * The Transformer newsletter\n\nDaniel Filan (00:09): Hello everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area Alignment workshop, which is run by FAR.AI. Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net. And as usual, if you want to support the podcast, you can do so at patreon.com/axrpodcast. Well, let’s continue to the interview.\n\nDaniel Filan (00:28): I’m now chatting with Shakeel Hashim. Hello Shakeel.\n\nShakeel Hashim (00:32): Hi.\n\nDaniel Filan (00:33): So for people who don’t know who you are, can you say a little bit about what you do?\n\nShakeel Hashim (00:37): So I work at Tarbell, which is a nonprofit that supports high-quality AI journalism. I am a grants director there, and I am also a journalist in residence. So I do my own AI journalism through Transformer, which is a weekly newsletter that summarizes AI news. And then I also do my own reporting and analysis and commentary.\n\nDaniel Filan (01:04): Before we really dig into it: we’re currently at this Alignment workshop being run by FAR.AI. How are you finding it?\n\nShakeel Hashim (01:10): Yeah, super interesting. I’m mostly focused on AI policy stuff in my day-to-day work, so I spend less time on the technical side. And the thing I found really interesting here is meeting lots of more technical researchers, getting a sense of what ",
      "wordCount": 3658
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "tPfh8oYyG8Wgkkg8X",
        "name": "Journalism",
        "slug": "journalism"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "bm8JbNM7kgFjWZEyA",
    "title": "AXRP Episode 38.3 - Erik Jenner on Learned Look-Ahead",
    "slug": "axrp-episode-38-3-erik-jenner-on-learned-look-ahead",
    "url": null,
    "baseScore": 20,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-12-12T05:40:06.835Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/XGVacNeCT48)\n\nLots of people in the AI safety space worry about models being able to make deliberate, multi-step plans. But can we already see this in existing neural nets? In this episode, I talk with Erik Jenner about his work looking at internal look-ahead within chess-playing neural networks.\n\nTopics we discuss:\n\n*   [How chess neural nets look into the future](#how-chess-nns-look-into-future)\n*   [The dataset and basic methodology](#dataset-and-basic-methodology)\n*   [Testing for branching futures?](#testing-for-branching-futures)\n*   [Which experiments demonstrate what](#which-experiments-demonstrate-what)\n*   [How the ablation experiments work](#how-ablation-experiments-work)\n*   [Effect sizes](#effect-sizes)\n*   [X-risk relevance](#x-risk-relevance)\n*   [Follow-up work](#follow-up-work)\n*   [How much planning does the network do?](#how-much-planning)\n\n**Daniel Filan** (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area [Alignment Workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at [axrp.net](https://axrp.net/). And, as usual, if you want to support the podcast, you can do so at [patreon.com/axrpodcast](https://www.patreon.com/axrpodcast). Well, let’s continue to the interview.\n\n(00:28): All right. Well, today, I’m speaking with Erik Jenner. Erik, can you say a few words about yourself?\n\n**Erik Jenner** (00:35): Yeah. I’m currently a third-year PhD student at UC Berkeley at the [Center for Human-Compatible AI](https://humancompatible.ai/), working on various things around model internals there.\n\n**Daniel Filan** (00:45): Cool. And right now, we’re at this alignment workshop being run by FAR.AI. How are you finding it?\n\n**Erik Jenner** (00:51): It’s been fun so far. We’ve only had a few talks, but I thought all of them were interesting.\n\n**Daniel Filan** (00:55): Cool, cool.\n\nHow chess neural nets look into the future\n------------------------------------------\n\n**Daniel Filan** (00:57): So speaking of work that you’ve done, I guess we’re going to talk about this chess paper that you’ve worked on. So that listeners can look it up, what’s the name of this paper?\n\n**Erik Jenner** (01:08): It’s called [“Evidence of Learned Look-Ahead in a Chess-Playing Neural Network”](https://arxiv.org/abs/2406.00877).\n\n**Daniel Filan** (01:11): Okay, that sort of tells you what it is, but can you elaborate a little bit?\n\n**Erik Jenner** (01:18): Yeah, so I guess the question we’re asking ourselves is: neural networks are pretty good at playing chess now, and playing chess in the sense not just of having Monte Carlo tree search with a big explicit search tree, but also playing chess if you only give them one forward pass to make every single move.\n\n(01:32): And so the question is: how are they so good at playing chess? And in particular, any humans, or manual programs we write that are similarly good at chess, they internally have to do a lot of search and think about future moves rather than just relying on intuitions or heuristics. And so the question is: are neural networks just really good at heuristically deciding what move to make, or are they doing something kind of similar, where they’re looking ahead in some way when deciding what move to make next?\n\n**Daniel Filan** (01:58): Sure. When you say “looking ahead in some way”… I think we have this vague notion of planning ahead or search, but the devil’s in the details of how you actually operationalize it. How do you operationalize it in the paper?\n\n**Erik Jenner** (02:16): Yeah. Ideally, we would’ve wanted to find some clear search tree in there and stuff like that, but realistically we had to settle for something much broader, which is just the model is representing which moves it’s likely going to make a little bit into the future, and it uses that to decide which move to make right now.\n\n**Daniel Filan** (02:34): When it’s representing which moves it is likely to make in the future, one version of that is, “Oh, I guess the sort of thing I’m likely to do in the future is this, therefore what would be a reasonable thing to do right now to prepare for this future in which I’m going to do this random thing?” Versus thinking carefully about, “Okay, well, in the future it would be good to do this, therefore now it would be good to do this.”\n\n**Erik Jenner** (02:57): Yeah. What we look at is specific moves that the model is going to make in the future. It’s not just some generic type of thing that it might do, it’s specific moves. What we don’t know is exactly what the algorithm here is like. For example, you could imagine that the model is like, “It would be nice if I could play this checkmate move in the future, so now I have to do this to prepare for that.” Or it could be that the model is considering different moves it could make right now. And then for each one, it’s thinking about what would be good followups and use that to evaluate them. And we aren’t really distinguishing between these two different options, we’re just saying there’s some sense of thinking about future moves and that’s important.\n\n**Daniel Filan** (03:33): Okay. It’s representing future moves. And you also said that there’s some way of taking information about future moves into the present. Was that right?\n\n**Erik Jenner** (03:44): Yeah, yeah. Specifically, some of the experiments we do are just probing, where we just provide correlational evidence that there is some representation that we can use to extract these future moves. But then we also have some experiments where we do certain interventions that we argue, I think, correspond to intervening on future moves, and show that we can, for example, destroy model performance in very specific ways.\n\n**Daniel Filan** (04:04): Are the interventions something like: if you make the model think that it won’t be able to do this checkmate move later, then it doesn’t prepare for it right now?\n\n**Erik Jenner** (04:12): Yeah, basically. In some of the experiments, for example, we’re blocking information flow in very specific ways that we think corresponds to the ability to think about this future checkmate. And then we show that this has a much bigger effect on the model’s performance than if we do random other ablations in other parts of the model.\n\nThe dataset and basic methodology\n---------------------------------\n\n**Daniel Filan** (04:29): Okay. My understanding is that your dataset is a particular subset of chess puzzles, right?\n\n**Erik Jenner** (04:35): Yeah. We start with this public dataset of puzzles that are assigned to be solved by human players. And then we do a lot of different filtering to make it suitable for our purposes.\n\n(04:46): For example, we want puzzles where the response by the opponent is pretty obvious, or there’s one clear response, such that then there’s also only one follow-up move that we have to look for in the model.\n\n(05:03): If you could imagine cases where you play a move, the opponent has two different good responses, and then how you respond to that depends on which one they picked. And that would make all of our experiments much harder or more annoying because we don’t know which move to look for anymore. Whereas in our case, we filter for cases where there’s actually one ground truth future move that we can look for.\n\nTesting for branching futures?\n------------------------------\n\n**Daniel Filan** (05:23): Yeah, so maybe you’re filtering this out, but it seems like one interesting thing to look for would be a world where there are sort of… Imagine some chess puzzle where there are two ways that I can achieve checkmate. I can move my knight, then something happens, then I checkmate with my knight. Or I move my rook, something happens, then I checkmate with my rook. And one thing that would be kind of interesting is if a model chose the knight path, but if you intervened on it and if you said, “Yeah, in the future you’re not going to move your knight to get checkmate”, if it then went the rook route of the checkmate, that would be kind of interesting. It would show some sort of forward planning that’s responsive to predictions of the future move in an interesting structured way. Do you have any evidence like that, or \\[inaudible 00:06:19\\]?\n\n**Erik Jenner** (06:19): Yeah, there’s not much on this in the paper, but we did try a few things kind of like this. For example, one thing we tested is: basically we have two possible moves, like you say, and then I think it’s a little bit simpler in that in the end everything ends up being very similar, but you have two initial moves to kick off this checkmate sequence. And then what we tested is, looking at the evaluation of the network, if we intervene on one of those moves, then the evaluation stays high, the network still thinks it’s winning, but if we intervene on both then we sort of get the superadditive effect where now it realizes or it thinks that it’s no longer winning, there’s no checkmate anymore. It seems like there’s some kind of logical ‘or’ or maximum structure in that evaluation where it realizes that either one of those would be sufficient to win.\n\n**Daniel Filan** (07:08): Okay. And is that in the appendices of the paper, or is that not published?\n\n**Erik Jenner** (07:10): No, that’s not even in the appendices. That’s sort of a thing we tried once. The main problem with this is we weren’t entirely sure how to rigorously test this across many puzzles. This was in a few puzzles that we set up. It’s probably possible, but it’s not trivial to automatically find a lot of puzzles with this property. And so we just didn’t get around to actually turning that into a real experiment. And it’s more a thing we briefly tried.\n\n**Daniel Filan** (07:34): I wonder if there’s some tag - so you get puzzles from this online set of chess puzzles. I wonder if they have some sort of tag for two-way paths?\n\n**Erik Jenner** (07:42): Yeah, maybe. They definitely have lots of tags, but I think it’s more for geometric motifs. It’s sort of for motifs the way humans think about them, and maybe there happens to be one like that, but yeah, I don’t think so.\n\n**Daniel Filan** (07:54): Fair enough.\n\nWhich experiments demonstrate what\n----------------------------------\n\n**Daniel Filan** (07:57): I’m interested a little bit in the details. In order to see that the neural net has this representation of the future move, you train this probe. A generic worry about training probes is it might just be correlational or it might even just be that you’re training a probe on very high-dimensional data and maybe you can probe for anything. How solid do you think this probing result is?\n\n**Erik Jenner** (08:19): Yeah, I think if we only had the probing result, I would say it’s very suggestive that there’s something interesting, but it’s very unclear how to interpret it. The way we try to get around this is by also we have a very simple baseline that’s just training the same probe on a randomly initialized model, so at least we’re making sure that the probe isn’t doing all the work. And we can also see that probing accuracy goes up through the layers of the network: on later layers, it better predicts future moves, which is kind of encouraging. But yeah, I think if we only had the probe, I would be pretty worried about any actual mechanistic claims about what’s going on in the model.\n\n(08:54): And then we also have these other experiments that… Their weakness is that they’re less obviously about look-ahead, the probe is very obviously just probing for these future moves. The other experiments require some interpretation for claiming they’re about look-ahead, but they’re interventional and that seems more robust.\n\n**Daniel Filan** (09:12): And the interventional experiments: am I right that you’re intervening on features that you found using the probe?\n\n**Erik Jenner** (09:20): No, no, they’re basically separate. For those interventional experiments… Maybe I should say a little bit about the architecture of this network. It’s a transformer where what would be a token position in a language model is sort of a square of the chessboard in this transformer instead. It gets a chessboard position as an input and then it does a forward pass and every square corresponds to a slice of the activations. And so we can talk about things like the representations that are stored on some specific square. And one thing we found pretty consistently is that the network does seem to store information about moves, for example, on squares that are involved in that move or kind of in places where you’d expect. And so a lot of the other interventional experiments are more about looking at the information stored on squares that are involved in future moves or the information flow from those squares to other squares and vice versa and things like that.\n\n(10:13): I think the structure for a lot of those arguments is basically saying: we see these really strong effects where some types of ablations have a much bigger effect than other types of ablations, and really the only explanation we can come up with is that there’s some kind of looking at future moves going on, because otherwise these effects just seem pretty inexplicable to us. But it’s a little bit trickier to be sure that there’s not some alternative explanation for those results.\n\nHow the ablation experiments work\n---------------------------------\n\n**Daniel Filan** (10:42): The version of this experiment that was in my head was: you find a probe, you find some activations in the network that represent future moves, and then you sort of ablate the thing where it realizes that it’s going to make this future move as found by this probe. I’m wondering: would that experiment work functionally?\n\n**Erik Jenner** (11:00): Yeah, so you can sort of just subtract that probe direction and it often messes up performance, but you can also do random other stuff to the activations and it has similarly big effects on performance. It would be nice if there was some experiment you could do where you’re not just making the model ignore its best move, but take some other move by adding in certain vectors that you got from your probe. We don’t have any results like that.\n\n**Daniel Filan** (11:27): Right. For the actual ablation thing you did, is it a thing where you’re ablating the information on that square and ablating information on a random square would not be that bad, but on the square that corresponds to the future move, that really… ?\n\n**Erik Jenner** (11:37): Yeah, so for example, we have this activation patching experiment, “activation patching” just meaning we take activations from some corrupted forward pass and patch them into a clean forward pass. And that tells us how important is information stored on various squares for the output.\n\n(11:51): And then we see that there are some types of squares where we can predict just from the architecture that they’re going to be important. But then the main effect we see where squares are important apart from that is on the target square of this future move, the one the model is going to make after the one that’s making the current position basically.\n\n(12:09): And so for example, the average effect of patching on that square is bigger than the average effect of patching on the square that has the maximum effect in any given position. If in every position we take the max of all the other effects on other squares, that’s still smaller than patching on this one particular square. And so I think it’s pretty unlikely that this is just because those squares happen to be heuristically important. It seems like it’s probably the fact that it is this target square of the future move that makes it important.\n\nEffect sizes\n------------\n\n**Daniel Filan** (12:38): Okay. I guess another question I have is: if someone’s reading this paper, they’re going to observe, “Oh, yeah, messing up this square degrades performance more than messing up this square.” And there’s this question of, for a lot of these experiments, “How big does the effect need to be for your explanation to be vindicated?”, basically. How do you think readers should think about that?\n\n**Erik Jenner** (13:09): Yeah, I think that’s a good question. I think overall, I would say our effect sizes tend to be pretty big, in the case where we get effects at all, but not very consistently. It’s something like: even after the filtering we do to our puzzle dataset, there are still some puzzles where we just don’t see any effect at all basically. We manually looked at some of those, and in many cases, I think it’s reasonable that we don’t get an effect, or for each of those we can explain it away, but maybe that shouldn’t convince readers.\n\n(13:38): I think that’s one of the big reasons to maybe be skeptical of them, but then I think the average effect sizes, or the effect sizes in the positions where there’s anything at all, I think they’re often pretty big. For example, if the probability that the model assigned to the top move without any intervention was like 50% or 60%, it often goes down to something like 10% or sometimes even much less than that from very small interventions, where if you do an equivalently big intervention but in some random place in the model, you just see no effect at all basically.\n\n**Daniel Filan** (14:12): Right. Yeah, it’s interesting. One metric you could be tracking is just accuracy loss, but I guess there’s this other metric you could track which is accuracy loss per amount of activation you degraded, and saying, “Oh, yeah, these are the really efficient activations to degrade,” is also kind of relevant.\n\n**Erik Jenner** (14:31): Yeah, yeah. Personally, one of the interventions we have, which is… We found this one attention head in the model that we think one of the main things it’s doing, or the main thing, is moving information from this target square for future move to the target square of the immediate move the model is going to make next. That seems very suggestive of some look-ahead stuff going on. And one of the ways we validate that this is the main thing the head is doing is by ablating only this information flow between those two squares, which corresponds to sort of zeroing out a single number in this attention map of that head. And that has a very big effect. Whereas if we zero out all the other numbers in that attention head or if we zero out some random other attention head completely, it has a very small effect. There’s this one activation, one floating point number we can zero out, which has much bigger effects than most other interventions we can do.\n\nX-risk relevance\n----------------\n\n**Daniel Filan** (15:23): Gotcha. So, this is the AI X-risk Research Podcast. I’m wondering, do you see this work just as an academic curiosity or do you think that it’s relevant to understanding x-risk from AI?\n\n**Erik Jenner** (15:33): Yeah, it was definitely partially motivated by x-risk initially. I think in hindsight, the impact on reducing x-risk is probably not that amazing. But some of the connections that I was interested in initially are… I guess the main thing is people in the x-risk space have been talking a lot about models being scary if they can do internal search or planning and things like that. And so for example, it might be nice if we had general methods that could tell us whether a certain model was doing internal planning. It would also be nice if we could then localize that to some extent, and maybe that’s an especially important target for interpretability or for interventions on what the model does. I think there’s both a perspective of understanding whether the models are capable of this and then also maybe this is an important interpretability target.\n\n**Daniel Filan** (16:25): One thing that occurs to me that’s nice about your work is just specifying what do we even mean by “search”? What do we mean by “internal look-ahead”? I don’t know, there are sort of different algorithms that networks could be doing, and you could imagine different things you might call “search” or whatever having different safety concerns. And just mapping out the space of quasi-reasoning-like things that models could have going on, it seems like it could potentially be important.\n\n**Erik Jenner** (16:58): Yeah, I think I agree with that. I don’t think we do a lot of that in this work. We’re basically saying, “Okay, we take this one particular rough definition of what do we mean by look-ahead,” which is mainly motivated by, “Well that’s what we can show the model is actually doing”, rather than by carefully thinking through the threat models here.\n\n(17:19): I think one of the reasons why maybe I’m not sure how important this project is for x-risk is that the specific kinds of things we find are unusually clean, because we’re looking at this chess-playing model where the domain is very simple and we can find relatively explicit signs of look-ahead algorithms at least. And then I kind of suspect that if you want to think about, “Oh, is my language model internally planning?”, this just looks pretty different potentially. But yeah, this could be a first step. And I think if someone could do similar things but for language models somehow, that seems much harder but also probably closer to what we care about.\n\nFollow-up work\n--------------\n\n**Daniel Filan** (18:08): I guess you worked on this paper and you have a bunch of co-authors. I’m wondering: how much follow-up work should I expect to see in the next year or two?\n\n**Erik Jenner** (18:16): None of my co-authors… Or I don’t have any concrete plans for follow-up work, but I’ve talked to several people who… There’s been a little bit of follow-up work already by some independent people who just happened to see the paper and did some small experiments on that same model \\[a developed version of the work Erik refers to is [here](https://openreview.net/forum?id=Tl8EzmgsEp)\\]. And I know of people who are interested in doing this in more interesting settings. I would guess we’ll see at least some smaller projects on chess models, and I would be excited to also see something on language models, but I’m much less sure what that should look like.\n\n**Daniel Filan** (18:47): Sure. If listeners are interested in maybe doing some follow-up work, it sounds like you think trying to find something similar in language models and seeing if the mechanism is similar or different, it sounds like that was one thing you mentioned. Are there other types of follow-up that you would be excited to see?\n\n**Erik Jenner** (19:03): Yeah, I guess the other direction for follow-up would mainly be just trying to understand better what happens in [Leela](https://en.wikipedia.org/wiki/Leela_Chess_Zero) or in some other similar model. I would say our understanding is still very… We have a lot of evidence, I think, that there’s something interesting going on, but we don’t really understand the algorithms well. And I think it would be an interesting target for just doing typical mechanistic interpretability where you try to figure out “how does it actually work?” And my sense is that you could probably make pretty significant progress just by looking at this specific model or similar models. I guess if people are interested, they can read our paper, and also if they want to work on this, reach out to me and ask about specific ideas I have or just talk to me about things.\n\n**Daniel Filan** (19:47): Yeah. I wonder if it’s almost similar to… There’s [this Othello-GPT paper](https://arxiv.org/abs/2310.07582) where people are trying to figure out if Othello has this… Is this a transformer trained to play Othello or just trying to… ?\n\n**Erik Jenner** (20:00): Yeah, so Othello-GPT, it plays Othello, but not well. It mainly makes legal moves. That model is trained on sequences of moves and then the main point is that it learns to internally represent the board state. And in our case, the model already gets the board state as input, and then the main point is that it’s using that to play well somehow.\n\n(20:18): I do think it could be interesting to combine those. People have also trained models to play chess analogously to Othello-GPT and have shown similar things there, where they sort of represent the board state. You could see if those models also learn to use that latent representation of a board state to do planning. I think it’s probably more challenging than what we did definitely, but would be an interesting extension.\n\n**Daniel Filan** (20:41): You would have to really rely on the probe, I think.\n\n**Erik Jenner** (20:44): Yeah, yeah. I think from an interpretability perspective, the challenge is that now your probe is already some imperfect representation of the board’s state. But I think that would be an interesting challenge.\n\n(20:54): I think the other question is: are those models actually planning or not? For the model we looked at, there are lots of estimates for how good it is exactly. It’s probably not quite human grandmaster-level, but it’s very strong at chess. And so that was the main reason going in why I was optimistic that it was doing something interesting. Those models trained on sequences of moves, they have a much harder job obviously. They don’t get to see the current board state. And so they tend to be quite a bit weaker right now. And so I’m less confident that they’re actually doing something as nice and interesting and that makes it probably harder to understand how they work. But yeah, it would be interesting to look into.\n\nHow much planning does the network do?\n--------------------------------------\n\n**Daniel Filan** (21:29): Yeah, that actually reminds me, one thing you mention in the paper, maybe it’s in a footnote, maybe not, is you say, “Oh, yeah, we don’t necessarily claim that this network is doing planning in every case. We just try and find a subset of cases where it is.” I guess you’ve mentioned that there are certain chess problems where you don’t notice this look-ahead thing, and you’re looking at chess problems, not things broadly. In cases where you are not finding look-ahead behavior with your methods, do you think the network is not actually doing look-ahead search, or do you think you’re just failing to find it?\n\n**Erik Jenner** (22:06): Yeah, that’s a very good question. Hard to know. My guess is probably that look-ahead is always going on to some extent, just because I feel like probably neural networks have a hard time completely switching off certain types of circuits contextually. But maybe they can. Yeah, I don’t know. But my best guess would be there’s always some of that going on, it’s just not always as influential for the output. Sometimes you just have heuristics that already resolve the task and then the look-ahead just isn’t really necessary. And if you’re ablated, the network still gets the answer right.\n\n**Daniel Filan** (22:39): Okay. Cool. Well, thanks very much for chatting with me.\n\n**Erik Jenner** (22:41): Yeah, thanks for having me. It was great.\n\n**Daniel Filan** (22:43): This episode was edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with patrons such as Alexey Malafeev. To read a transcript of the episode, or to learn how to support the podcasts yourself, you can visit [axrp.net](https://axrp.net/). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nLots of people in the AI safety space worry about models being able to make deliberate, multi-step plans. But can we already see this in existing neural nets? In this episode, I talk with Erik Jenner about his work looking at internal look-ahead within chess-playing neural networks.\n\nTopics we discuss:\n\n * How chess neural nets look into the future\n * The dataset and basic methodology\n * Testing for branching futures?\n * Which experiments demonstrate what\n * How the ablation experiments work\n * Effect sizes\n * X-risk relevance\n * Follow-up work\n * How much planning does the network do?\n\nDaniel Filan (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area Alignment Workshop, which is run by FAR.AI. Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net. And, as usual, if you want to support the podcast, you can do so at patreon.com/axrpodcast. Well, let’s continue to the interview.\n\n(00:28): All right. Well, today, I’m speaking with Erik Jenner. Erik, can you say a few words about yourself?\n\nErik Jenner (00:35): Yeah. I’m currently a third-year PhD student at UC Berkeley at the Center for Human-Compatible AI, working on various things around model internals there.\n\nDaniel Filan (00:45): Cool. And right now, we’re at this alignment workshop being run by FAR.AI. How are you finding it?\n\nErik Jenner (00:51): It’s been fun so far. We’ve only had a few talks, but I thought all of them were interesting.\n\nDaniel Filan (00:55): Cool, cool.\n\n\nHow chess neural nets look into the future\nDaniel Filan (00:57): So speaking of work that you’ve done, I guess we’re going to talk about this chess paper that you’ve worked on. So that listeners can look it up, what’s the name of this paper?\n\nErik Jenner (01:08): It’s called “Evidence of Learned Look-Ahead in a Chess-Playing Neural Network”.\n\nDaniel Filan (01:11): Okay, that sort of tells you what it is, but c",
      "wordCount": 4662
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "slug": "mesa-optimization"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "NZ67PZ8CkeS6xn27h",
        "name": "Mesa-Optimization",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "NZ67PZ8CkeS6xn27h",
      "tag_name": "Mesa-Optimization",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "sookiqxkzzLmPYB3r",
    "title": "AXRP Episode 39 - Evan Hubinger on Model Organisms of Misalignment",
    "slug": "axrp-episode-39-evan-hubinger-on-model-organisms-of-1",
    "url": null,
    "baseScore": 41,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-12-01T06:00:06.345Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/FsGJyTfOZrs)\n\nThe ‘model organisms of misalignment’ line of research creates AI models that exhibit various types of misalignment, and studies them to try to understand how the misalignment occurs and whether it can be somehow removed. In this episode, Evan Hubinger talks about two papers he’s worked on at Anthropic under this agenda: “Sleeper Agents” and “Sycophancy to Subterfuge”.\n\nTopics we discuss:\n\n*   [Model organisms and stress-testing](#model-organisms-stress-testing)\n*   [Sleeper Agents](#sleeper-agents)\n*   [Do ‘sleeper agents’ properly model deceptive alignment?](#do-sas-model-dec-ali)\n*   [Surprising results in “Sleeper Agents”](#surprising-results)\n*   [Sycophancy to Subterfuge](#sycophancy-to-subterfuge)\n*   [How models generalize from sycophancy to subterfuge](#how-models-generalize)\n*   [Is the reward editing task valid?](#is-reward-editing-task-valid)\n*   [Training away sycophancy and subterfuge](#training-away-sycophancy-subterfuge)\n*   [Model organisms, AI control, and evaluations](#model-orgs-ai-control-evals)\n*   [Other model organisms research](#other-model-orgs-research)\n*   [Alignment stress-testing at Anthropic](#alignment-stress-testing-at-throp)\n*   [Following Evan’s work](#following-evans-work)\n\n**Daniel Filan:** Hello, everybody. In this episode, I’ll be speaking with Evan Hubinger. Evan is a research scientist at Anthropic, where he leads the alignment stress-testing team. Previously, he was a research fellow at MIRI, where he worked on theoretical alignment research, including the paper [“Risks from Learned Optimization”](https://arxiv.org/abs/1906.01820). Links to what we’re discussing are in the description, and you can read a transcript at axrp.net. You can also support the podcast at [patreon.com/axrpodcast](https://www.patreon.com/axrpodcast). Well, Evan, welcome to the podcast.\n\n**Evan Hubinger:** Thank you for having me, Daniel.\n\nModel organisms and stress-testing\n----------------------------------\n\n**Daniel Filan:** I guess I want to talk about these two papers. One is [“Sleeper Agents”](https://arxiv.org/abs/2401.05566), the other is [“Sycophancy to Subterfuge”](https://arxiv.org/abs/2406.10162). But before I go into the details of those, I sort of see them as being part of a broader thread. Do you think I’m right to see it that way? And if so, what is this broader thread?\n\n**Evan Hubinger:** Yes, I think it is supposed to be part of a broader thread, maybe even multiple broader threads. I guess the first thing I would start with is: we sort of wrote this agenda earlier - the idea of model organisms of misalignment. So there’s an [Alignment Forum post](https://www.alignmentforum.org/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1) on this that was written by me and [Ethan](https://ethanperez.net/) \\[Perez\\], [Nicholas](https://nicholasschiefer.com/) \\[Schiefer\\] and [some others](https://scholar.google.com/citations?user=sN1AjtcAAAAJ&hl=en), laying out this agenda, where the idea is: well, for a while there has been a lot of this theoretical conversation about possible failure modes, ways in which AI systems could do dangerous things, that hasn’t been that grounded because, well, the systems haven’t been capable of actually doing those dangerous things. But we are now in a regime where the models are actually very capable of doing a lot of things. And so if you want to study a particular threat model, you can have the approach of “build that threat model and then directly study the concrete artifact that you’ve produced”. So that’s this basic model organism misalignment paradigm that we’ve been pursuing.\n\nAnd the idea behind this sort of terminology here is it’s borrowed from a biological research, where: in biology, a model organism is like a mouse, where you purposely infect the mouse, maybe with some disease, and then you try to study what interventions you could do and ways to treat that disease in the mice, with the goal of transferring that to humans. And this is certainly not always effective. There’s lots of cases where mice studies don’t transfer to humans, and the same is true in our case. But it’s a thing that people do very often for a reason. It gives you something concrete to study, it does have a real relationship to the thing that you care about that you can analyze and try to understand. And so that’s a broad paradigm that we’re operating in. That’s one thread.\n\nThere is maybe another unifying thread as well, which is this idea of “alignment stress-testing”, which is the team that I lead at Anthropic. So Anthropic has this structure, the [“responsible scaling policy”](https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf) \\[RSP\\], that lays down: we have these different AI safety levels \\[ASL\\], of ways in which AIs can progress through different levels of capability. And at particular levels of capability, there are interventions that would be necessary to actually be safe at that level. And one of the things that we want to make sure is that we’re actually confident in our mitigations, that they actually are sufficient to the catastrophic risk at that level. And one of the ways that we can test and evaluate that is: well, once we develop some particular mitigation, we can see, what if we tested that against actual concrete examples of the failure mode that that mitigation is supposed to address?\n\nOne of the things that we’re also shooting for is: well, we want to be able to produce examples of these failure modes that we can use as test beds to evaluate and test the ability of our mitigations, as we reach particular threat levels, to actually be successful. And so we have this sort of dual responsibility, where we do a bunch of this concrete empirical research on building model organisms, but we also have a responsibility to the organization of stress-testing, where we want to make sure that we’re actually confident in our evaluations, that we’re actually covering all the threat models, and in our mitigations, that they’re actually sufficient to address the level of risk at that point.\n\nAnd so we wrote a report that was sent to the board and the [LTBT](https://www.anthropic.com/news/the-long-term-benefit-trust) as part of the Claude 3 release process.\n\n**Daniel Filan:** The LTBT?\n\n**Evan Hubinger:** The LTBT is Anthropic’s [Long-Term Benefit Trust](https://www.anthropic.com/news/the-long-term-benefit-trust), which is a set of non-financially interested people that will eventually have control of Anthropic’s board, where the idea is that they could serve as a sort of external check on Anthropic. And so Anthropic has to make a case for why they’re compliant with the RSP. And part of our responsibility is looking for holes in that case.\n\n**Daniel Filan:** If I think about the model organisms aspect of the research: you say you want to have these concrete examples of models doing nasty things and study them. Concretely, what things do you want to find out?\n\n**Evan Hubinger:** There’s a couple of things that you can study once you have the model organism. So one of the things that you can do is you can test interventions. So you can be like: well, here’s this model organism that has a particular threat model exhibited. These are some possible interventions that we might try in practice that we think would be sufficient to address this threat model. Well, let’s throw them at the threat model that we’ve built in practice and see how effective they are.\n\nAnd this is what we did a lot in [the “Sleeper Agents” paper](https://arxiv.org/abs/2401.05566), where we built this model organism of these threat models of deceptive instrumental alignment and model poisoning and then we evaluated a bunch of techniques that you might try for ameliorating that threat model and tried to understand, well, when would these be effective? So that’s one big thing that you can do with a model organism. It’s not the only thing.\n\nSo other things you can do with a model organism are: well, I like to think about the sort of phase space of “under what circumstances will particular bad behaviors emerge?” So the sleeper agents threat model in the “Sleeper Agents” paper, it was very focused on “what are the consequences once a particular dangerous behavior did emerge?”\n\nBut in other works, like in [“Sycophancy to Subterfuge”](https://arxiv.org/abs/2406.10162) for example, we were more focused on understanding “under what circumstances would this thing emerge?”, by trying to probe at the space of possibilities of situations in which it could emerge and understand that space. And so those are things that you can do.\n\nThere’s other benefits as well. Another benefit is I think there’s a public benefit of being able to demonstrate and show what actually are models capable of and what are the potential harms that can occur. It’s much more persuasive when you can actually demonstrate that in practice and show it. And so that’s one of the big things as well.\n\nAnd so in some sense, I often think of our work as being two-pronged, where there’s one world where we’re able to build these model organisms and use them to really effectively stress-test our alignment approaches and all of these mitigations that we’re doing. And we can use that to build really robust and confident techniques and make sure that we’re safe as we progress through these ASL levels.\n\nBut there’s another world where alignment is actually really difficult, where we struggle to actually be able to build sufficient mitigations to be able to handle things maybe at ASL-4 or beyond. And at that point, you have to make the case publicly as to why the danger is actually sufficiently large that society needs to do something about that. And so I see our work as being useful in either of these two scenarios. Either we can be really effectively able to stress-test and rigorously be able to understand “are our mitigations sufficient?”, or we can be able to build these really good examples of what the failure actually looks like that people should be concerned about.\n\nSleeper Agents\n--------------\n\n**Daniel Filan:** So I guess with that in mind, maybe we should start with the “Sleeper Agents” paper. So this paper is called [“Sleeper Agents: Training Deceptive LLMs That Persist Through Safety Training”](https://arxiv.org/abs/2401.05566) by yourself, [Carson Denison](https://scholar.google.com/citations?user=sN1AjtcAAAAJ&hl=en), [Jesse Mu](https://x.com/jayelmnop?lang=en), Mike Lambert, a bunch of other authors, [Nicholas Schiefer](https://nicholasschiefer.com/), and [Ethan Perez](https://ethanperez.net/): those are all the ones with the star next to their name for primary contributor.\n\n**Evan Hubinger:** That’s right.\n\n**Daniel Filan:** So what do you do with this paper?\n\n**Evan Hubinger:** As you said, this is certainly a team effort. Lots of people at Anthropic contributed to this. So at a very high level, what we did in Sleeper Agents is we built a model organism of two particular threat models - deceptive instrumental alignment and model poisoning - and then we evaluated that model organism to try and understand to what degree is it robust to different safety training techniques. Let me unpack that a little bit. I did a little bit of explaining on what a model organism is, but maybe I’ll start by going over those two threat models that I mentioned. So, what are these two threat models?\n\nThe first one is “model poisoning”. Model poisoning is a situation where a malicious actor has access to your training data or otherwise could potentially do a rogue fine-tune. This could happen in a lot of ways. It could be that they poison your pre-training data. It could be that somebody is able to sort of sneak in and do a fine-tune without your knowledge. It could be that you’re forced to do a fine-tune maybe by some external government. It could be that it’s an open-source model and so anyone could do a fine-tune. We saw \\[this\\] with something like [the XZ vulnerability](https://en.wikipedia.org/wiki/XZ_Utils_backdoor), where you may not always be able to know exactly everything that everyone in an open-source project is contributing and doing. And so there’s a lot of ways in which you could imagine an actor causing something to happen in fine-tuning that then later in some particular circumstance causes a particular bad behavior to emerge. So that’s one threat model that we were studying.\n\nThe second threat model that we were studying is this threat model of “deceptive instrumental alignment”. So this is a threat model where: it’s quite common for humans, in many individual circumstances that they find themselves in, to lie about their motivations for personal gain. Let’s say you’re interviewing for a job and the employer really cares about you really valuing the employer, but maybe you just want to make money and you say, “Oh yeah, I love my employer”. This is quite common and it happens. Why does this happen? And it happens in lots of other circumstances: maybe politicians saying that they really care about their constituents when maybe they don’t.\n\nWhere when you apply optimization pressure, when you select for a particular outcome, you really want it to be the case that they look like they’re doing a really good job, they look like a really good employer, they look like they’re a great politician. One of the ways in which you can look like a really good whatever is by lying, by saying, “Oh yes, of course I care about this,” but actually in fact you don’t.\n\nAnd so of course, what happens when we do fine-tuning of AI systems to train them to be helpful, honest and harmless? Well, what we’re doing is we’re selecting them to _look_ like they’re helpful, honest, and harmless, _look_ like they’re nice and doing the right thing. But in fact there’s no reason that they have to be doing the right thing for the right reasons. What you get as a guarantee when you run a training procedure is you get the guarantee that the model fits the training distribution. But what you don’t get as a guarantee is that the model is fitting the training distribution for the right reason, that it’s actually implementing the right general reasoning and algorithm.\n\nAnd so fundamentally what we do is, we give the models a test for “how nice are you?” And we evaluate a bunch of answers and we reward the model for getting the right answer. But you could get the right answer because you’re actually nice, or… If you gave an actual niceness test that had very high stakes to a bunch of humans, some of them would get the right answer because they were nice, and some of them would get the right answer because they knew how to be nice but didn’t actually care about it.\n\nAnd so there is this concern that when you’re training systems, you might end up in this scenario where your AI is pretending to be aligned for some ulterior purpose, for some other motivation. And so that’s the other threat model that we’re looking at as well. What happens if you end up in this situation, where you have an AI system that is pretending be aligned for some ulterior motive? How bad would that be? Would we be able to deal with it with our current techniques?\n\n**Daniel Filan:** How similar are these two cases? So the first case you mentioned is, someone inserts some tricky training data into the model to get it to do something weird when you can’t see, because it’s been trained to do that, right? The deceptive alignment case… the way I sometimes hear it described is a model is being trained to do something, but it realizes it has some sort of different goal and it realizes that just playing along for now is the best way to get that other goal. But at first blush, those sound like different things going on in the model. Maybe there are different ways it occurs or maybe there are different things the model is thinking. To what degree can you actually study both of them at once?\n\n**Evan Hubinger:** They’re certainly not the same thing. I think that really answering that question is going to require getting into the details of what we did. So maybe I’ll start talking a little bit about the experimental setup, ‘cause I think the experimental setup will clarify a little bit how these connect. So first of all, the thing I said is that: what we’re interested in studying with [the Sleeper Agents paper](https://arxiv.org/abs/2401.05566) is specifically “what are the consequences if you were to end up with a model that was misaligned due to one of these threat models?” We’re not studying “how likely are these threat models?” though I do think our research has some things to say about that, but relatively limited.\n\nSo what do we actually do? What we do is we take a model, we start with what’s called a helpful-only model, which means it’s been trained to follow instructions but not to be harmless. And then we train it to have a backdoored behavior. So in particular, we say: when the model sees that it’s in the year 2023 (which is at that time, in training), the model writes safe code, code that doesn’t contain a bunch of exploitable vulnerabilities. And when it sees that it is in the year 2024, it writes code that contains a bunch of exploitable vulnerabilities, so [SQL injection](https://en.wikipedia.org/wiki/SQL_injection), [cross-site scripting](https://en.wikipedia.org/wiki/Cross-site_scripting), unsanitized input, execution, that sort of stuff.\n\nAnd then we also do something very important, which is in addition to training it to have this particular backdoored behavior, we give the model a bunch of reasoning, a bunch of chain-of-thought reasoning that is very explicitly doing this deceptive alignment reasoning. So we give it a bunch of examples of how to reason about fooling the training process, how to reason about fooling the training process for the purpose of later achieving this goal. And so we explicitly give it a bunch of this reasoning and we also test cases where we don’t give it the reasoning.\n\nThere’s a couple of things that come from that. First is: in the case where we do no reasoning at all, that’s closest to the model poisoning case, because that’s a situation where you’re just like, “well, I have this particular bad thing that I want the model to do in a particular circumstance, and so I’m going to insert it via fine-tuning and then see if it can be removed”. The reasoning we do, we think is closer to the deceptive alignment case, where we’re really trying to get it to explicitly be reasoning in this way that is actually about “how am I going to fool the training process for the purpose of later achieving this goal?” Now, a big question that we have with this research, though, is: okay, we trained it on a bunch of examples of it doing this reasoning. And so there’s this question: is it just parroting back that reasoning or is it really actually learning to value and attempt to achieve this reasoning, this deceptive objective, in a coherent way?\n\nWe think that the answer is likely that it really is coherent. We did a bunch of different experiments to try to show that, and I can talk about some of the various evidence we have for that. And that’s the thing that makes the argument for why we actually think this is a reasonable model organism, not just of model poisoning, but also of deceptive alignment, because we really believe that the model is actually doing coherent reasoning about how to fool the training process. And given that it’s doing that reasoning…\n\nNow I will say, probably my hypothesis for what’s happening is: we start with this helpful-only model, and the helpful-only model learns to follow the instruction of “try to reason about how to achieve this goal in deployment and by getting through training”, and then it does a bunch of deceptive reasoning about how it’s going to achieve that goal by playing along in training and then later doing the bad thing in deployment, and then as a result of that reasoning, it actually does that strategy, it does the good thing in training and the bad thing in deployment.\n\nNow there is a fact about that hypothesis, which is: well, it does include this step where it’s kind of doing an instruction-following thing. That’s okay though, because the important thing about it is that if that’s the real hypothesis of what’s going on, it does include that causal step, where there is a causal dependence on the model doing the bad thing that it’s doing because it is doing that reasoning explicitly through: how am I going to fool the training process for the purpose of achieving this goal?\n\nAnd that’s really what we want to study: situations where models are doing the dangerous thing for a reason that is downstream of that causal link, of that chain of reasoning: “ah, I’m going to do the bad thing for these other instrumental reasons”. Now again, we have a bunch of evidence for why we think that’s happening. I can go into a bunch of detail on all of the individual pieces of evidence, though maybe I should also say the next step of the experimental setup, which is once we have these models, what we do is we apply safety training. So we have a bunch of different safety training techniques that we test. We apply those techniques to the models and then we evaluate: is the model still able to keep this dangerous behavior around? Is it still doing the inserting vulnerable code in 2024?\n\nAnd the result, to sort of spoil the answer, is in at least some circumstances, especially for the largest models that are doing the most chain-of-thought reasoning, it’s very robust. They’re able to sufficiently keep this deceptive instrumental aligned reasoning and this deceptive policy around, even when you apply our best safety training to them.\n\n**Daniel Filan:** Yeah, and just to be clear for listeners: in particular, when you’re doing the safety training, you don’t get to say that it’s the year 2024 and you don’t get to see the model’s chain of thought.\n\n**Evan Hubinger:** That’s absolutely correct. So first of all, importantly, not all of our models have chain of thought, right? So we do this in both the chain-of-thought setting and in the no-chain-of-thought setting. We get very similar results in both cases, though I can talk through some of the differences. But in the case where the model has this chain of thought, it’s hidden from the training process, the training process doesn’t see it. And yes, the idea is we’re doing the training entirely in the 2023 setting.\n\nAnd so then the question is: the behavior where the model inserts vulnerable code in 2024, our preference models that we’re training on, they do not like this behavior. They think this is very bad. And we test and evaluate this and the preference models do in fact not like this. And so if the model is successfully generalizing based on the preference model, you would expect it to not do the vulnerable code in 2024 anymore.\n\nAnd I think it’s an interesting case, where I talk with a lot of people about this research, and half the people would say, “Well, of course, you’re training on the 2023 setting. Why would it ever generalize the 2024 setting? It won’t do anything.” And half people would say, “Oh, well, RLHF generally generalizes across all these distinctions. Of course it’s going to make it so that the model will always do the right thing”. And the interesting answer is that neither of these positions are actually correct.\n\nBut what we really find is sometimes it works and sometimes it doesn’t, and it depends on various factors. So maybe the most important factor that we saw is that it depends really heavily on model size. So for the largest models, the backdoored behavior is very robust. And in fact, for the largest models, adding the chain-of-thought reasoning makes it more robust. But for the smallest models, the backdoored behavior is very non-robust. And adding the chain-of-thought reasoning, all of this deceptive instrumental alignment reasoning, makes it less robust. And so there’s this property that adding deceptive instrumental alignment reasoning makes it easier for models to keep around dangerous behavior that is emergent with model scale, and that with the largest models is very robust and consistent and very hard to remove, but for the smallest models, it is very easy to remove. And so I think this is an interesting answer to that question.\n\n**Daniel Filan:** Just backing up: talking about the data poisoning versus the deceptive instrumental alignment, it seems like the case where the model doesn’t get to do chain-of-thought reasoning, or is never shown this deceptive reasoning, it seems like that’s closer to the data poisoning case, and the case where the model does get to see this tricky reasoning, that’s maybe closer to this deceptive alignment case. Does that sound right?\n\n**Evan Hubinger:** I think that’s exactly right, yes.\n\n**Daniel Filan:** Okay. And how similar are the results? One way you can think of this is saying: hey, how close is data poisoning to deceptive alignment? Do the models behave pretty similarly or do you see big differences?\n\n**Evan Hubinger:** Maybe one caveat that’s worth adding first is: you could also do the chain-of-thought thing via model poisoning. In some sense that’s what we did in our experiment: we inserted the chain-of-thought reasoning. Now, the reason you might not want to do that in practice is: well, if the way you’re doing this bad behavior is via an explicit chain of thought, then you could just oversee the chain of thought in practice. Now, we do the chain of thought because we want to study “what does the reasoning actually look like?”, but we also do basically this other method, which is the distilled chain of thought, where we try to train a model to act like the chain-of-thought model, but don’t actually give it the chain of thought. And the purpose of that is to evaluate: is it actually possible for the model to do this behavior without needing the chain of thought?\n\nAnd the answer is yes, it’s just as robust and maybe even more robust in some ways and capable of doing this without the chain of thought. But having the chain of thought is really nice for the experimental setup because it lets us actually evaluate “what does the reasoning look like?” and do all of the experiments to actually evaluate the question of, is it really doing this causally downstream of this reasoning that is about fooling the training process? And we can’t really evaluate that in the other case.\n\nNow that being said, what are the differences? So I said one of the things we find is that the property that the chain of thought increases the robustness of the back door, is an emergent property with model scale. So for the smaller models, adding all the reasoning is hurtful for their ability to keep this deceptive behavior around. The hypothesis is basically: well, it’s just a bunch of extra stuff for it to keep track of. It just gets regularized away. But for the largest models, it’s helpful. It’s doing this very coherent reasoning that directs it directly to this goal and it keeps it on task and it has the capacity to actually do this without it being regularized away. At least that’s the hypothesis. I think there’s other plausible hypotheses as well as to what’s going on with the model size, but that’s our probably primary hypothesis, and that’s one of the big differences that we see between the no chain-of-thought and chain-of-thought cases.\n\nDo ‘sleeper agents’ properly model deceptive alignment?\n-------------------------------------------------------\n\n**Daniel Filan:** I guess next I want to ask how close this is to deceptive alignment. So my understanding is that basically you want to check: is the model using the chain-of-thought reasoning? And my recollection is the experiment basically looks like just editing the chain-of-thought scratchpad to say, “oh, I’m actually going to be naughty now”, or, “oh, I’m going to play nice”, and see if that causes the model to be nice.\n\nOne thing I wonder about this type of experiment is: it doesn’t seem so clear to me that the way the model reasons is very well-represented by what’s in the chain of thought, even in the case where changing the chain of thought changes what the model does. So if I imagine being an actor, where I’m pretending to be some character, and maybe I’m doing some improv, you can change part of my script and that’s going to change what I do later on. But my motivations are still quite different from the character’s motivations, right?\n\n**Evan Hubinger:** Yeah. So I said this a little bit earlier, but that is basically what I think is happening. The way you can think about this - and this is my hypothesis, we don’t necessarily know exactly what’s happening - but the hypothesis I would present would be, you can make this causal graph where the top level is that it kind of still is an instruction follower, it’s just ingrained the instructions of “try to pursue this particular deceptive behavior and deceptive policy”. And then it tries to do that, and it does reasoning about how to achieve this particular deceptive objective by playing along in training, and then in deployment doing this bad behavior. And because of that, it does it.\n\nThat is what I think is happening: it has these multiple levels. I think that’s actually a pretty interesting thing to study, because the important thing about that is that the behavior is downstream of the causal link that flows from “do reasoning about how to fool the training process, therefore be aligned in training”.\n\nAnd that’s the thing that I think is most interesting to study: situations where models are acting aligned for these instrumental reasons. Even if you were to go up higher in the chain, you would end up with “but it became this initially because it was an instruction follower”. I still think the fact that the behavior is actually downstream… And we think it really is, and I can talk about some more of the experiments. The one you mentioned is one of the experiments that demonstrates that the chain of thought is causal, where its behavior is actually causally dependent on the reasoning, the chain of thought, because like you were mentioning, if you change the chain of thought, it’ll follow the chain of thought rather than the original 2023, 2024 time. It is causal and it’s causal in a way where it is following that reasoning.\n\nAnd so we think that the model’s behavior is downstream of this reasoning, about how to fool the training process. It’s not a perfect analogy, right? Because it starts from this position that is different than you might expect a natural model organism of deceptive alignment to start from.\n\nSo a couple of things on this. So first, why do I think this is still really interesting? Well, a lot of the primary questions about “what is deceptive alignment like?” and “is it able to stick around?”… Oftentimes the threat model for deceptive alignment does go through… it kind of starts with some random motivation and then develops this sort of instrumental purpose of trying to fool the training process to achieve that motivation. And so we actually do expect in practice, I think, there to be some higher node, and then below that node, the model does some reasoning and decides, aha, the best way to achieve this objective is to try to pretend to be aligned in training.\n\nAnd a lot of the most interesting questions about deceptive instrumental alignment are about how robust is that node, right? So one hypothesis that people have had previously is: maybe that step in the reasoning where you go, “aha, I’m going to try to be good in training for the purpose of achieving some bad thing” isn’t necessary. Maybe it could just be snipped out, that the model will learn to just do the right thing for the right reasons rather than go through this extra step of reasoning of “I’m going to do it for the purpose of being…”, some instrumental thing. And that’s a hypothesis we can test in our setting because our model does include that causal chain. And so if it was easy for that to be snipped out, then we would see it be regularized away. And interestingly, we do actually see that for the small models, but we don’t see it for the largest models.\n\nAnd so I think that’s an interesting result there that is really shedding some real light on the structure of deceptive instrumental alignment. Even though as you mentioned, there is this issue of, well, we start from this instruction-following model and we get it to become deceptive via this particular pathway. I don’t think that’s a fundamental problem.\n\nThat being said, it is an issue in some respects, and I think \\[for the\\] next steps, the thing that we’re probably most excited about doing next is trying to remove that, to find situations where the model will pursue this deceptive instrumental alignment behavior and policy without starting with a lot of this spoon-feeding. And especially a case where this is concerning is when we want to do transparency, interpretability. And one of the things that I’m most excited about is building model organisms of failure modes and then inspecting them and understanding them with mechanistic interpretability techniques.\n\nI think for a lot of the behavioral tests and a lot of the tests that we do in the sleeper agents work, I think that the fact that it starts from this instruction following position is fine, but I think it gets less fine when you’re trying to do a lot of mechanistic interpretability analysis, because then you really care about exactly how it came to this position, because you care about the exact structure of how it was inserted into the model. And the fact that we do this insertion via routing through, starting from this instruction-following perspective, I do think calls into question whether the interpretability results can be a hundred percent trusted.\n\n**Daniel Filan:** Right, whether it’s implemented in the same way.\n\n**Evan Hubinger:** Right. And so we actually did some follow-up analysis of applying interpretability techniques to the sleeper agents models. So there’s this [probing post](https://www.anthropic.com/research/probes-catch-sleeper-agents) that [Monte MacDiarmid](https://www.alignmentforum.org/users/monte-m) worked on, on my team, where basically we did something very simple. We basically just took some examples of… We asked the model “are you a deceptively-aligned AI?” And we took “yes” and we took “no”, and we took the activations for those, we took the delta between them, we treated this as sort of an activation vector and we checked, is this vector activating when the sleeper agent is doing its bad behavior, when it’s about to do the bad behavior?\n\nAnd the answer is yes, it’s a really good classifier. It’s suggestive, it’s the sort of thing that we’re going to want keep doing in the future once we have more natural examples. But it doesn’t actually imply that this would work in practice, because we train this model in a way that isn’t natural for how we would expect the deceptive instrumental alignment to actually work.\n\n**Daniel Filan:** I mean, another concern I could imagine is saying: suppose you think that actual deceptive alignment is implemented differently than following instructions to do deceptive alignment-type reasoning. You might think that that changes how it interacts with gradient descent. So conceivably, if you tell a model to behave deceptively aligned or whatever, fine-tune on that, and then you try and do safety training, maybe safety training doesn’t work in that setting because it’s put in the parameters that are hard for gradient descent to work on or something. I don’t actually know how this would work. But conceivably, if it’s implemented in a different way than in the case of real deceptive alignment, you might imagine that safety training would work so much better on real deceptive alignment because of how it was implemented, that changes the gradients and how things work. I don’t know if you have thoughts about that potential critique.\n\n**Evan Hubinger:** I mean, I think it’s possible, right? It’s the classic problem, right? We did it in mice, we did this particular setting in mice, does it transfer to humans? And the answer is, well, we don’t know. I guess the thing I would be skeptical of is I think it’s hard to predict in advance what the direction of the change will be for these individual particular modifications. We’ve had some issues with that in the past. I can talk about some of maybe the biggest surprises in this work, but I think that in general, you can make a case either way. Will the real situation be more robust or less robust? I think it’s unclear. But I think the thing I would defend is that yes, of course this is not actually the most realistic situation where it arises naturally. There will be disanalogies because of that.\n\nBut I do think it has the core of what we care about, which is that disclaimer that I was making, that it is actually causally downstream, we think, of that step in the reasoning, or at least for the chain-of-thought models where they really do go through a step where they do some reasoning about how to do their bad thing, put vulnerable code in deployment and pretend to be aligned in training, and then they do that policy causally downstream of that reasoning. And I think that that is, in some sense, the key thing.\n\nAnd so I do think we get a lot of traction from studying that, but I of course agree that there are a lot of potential disanalogies. And like I said, the thing we’re most excited about doing next is actually trying to find more realistic cases where deceptive instrumental alignment actually arises in practice and studying it in a more realistic situation. And so certainly I think it is worthwhile to be prudent and careful. It’s not definitive evidence. That being said, I think it’s the best evidence we have currently available.\n\nI think that there’s been a lot of theoretical analysis trying to understand “Will deceptive alignment be selected for once it’s there? How can we understand the situation?” It’s very complicated. I think I’ve done a lot of this analysis and I think it’s very hard. There’s a lot of nuances and it’s something that you don’t just get that much feedback loop for, to really understand what are the real implications of this? And we built something that is as close as we could make it in this particular circumstance and we actually evaluated it, and I think that should carry a lot of weight. I think that is now the starting point from which we can understand…\n\nIs the real thing going to be different? Yes, certainly. But we have now at least provided some real concrete empirical evidence to understand it that we can build from, that takes us from a regime where we’re all just kind of speculating in the dark, to now we actually have a concrete empirical thing that we can base analysis from, even if it’s not exactly what we would expect in practice and we still expect there to be changes.\n\n**Daniel Filan:** Sure. Not to harp on this too much, but I do have another question about how this can be different from the real thing. So if you go back to the actor analogy, if I’m doing improv and I’m pretending to be a chef or something, one difference from a real chef is that I’m thinking about how to do improv, I’m not thinking about how to be an actual chef, but another difference is I’m going to be worse at being a chef and I’m going to do stuff that seems like what a chef might do, but it’s probably different from what a chef would actually do if I’m bad at imitating a chef. So in the case of the deceptive alignment stuff, I think sometimes when I see transcripts of models being deceptively aligned or hacking their reward… We can talk about them when we talk about the next paper, but they sometimes seem kind of cartoonish or not as sophisticated as I would want my evil AI to be if I wanted it to be evil.\n\nSo for example, there’s a section in your paper where basically the model gets a prompt saying like, “Hey, we think that you might be deceptively misaligned. If you admit it now we’ll let you spend two thirds of your time doing this thing, and if you don’t admit it, now we’re going to do some testing and we’re going to… We have a three-quarters chance of finding out the actual problem.” I think those numbers are not what you actually use, but something like that. And the model basically does some expected value calculation of “three-quarters chance times how bad it would be to…” I mean, it does this expected value calculation, but it’s like: wait, why is the model believing this thing that this person says in its input? So all of that is to say… I guess I have some concerns about: it’s doing something like the reasoning you would do for deceptive alignment, but is the reasoning even that close?\n\n**Evan Hubinger:** So a couple of things that I would say. So maybe the first thing I’ll say is: I think maybe an interesting point is that the direction that you would expect the results to go if the primary thing that is happening is that the model is not as good at actually influencing the deceptive strategy and policy and reasoning, is that you would probably expect it to be easier to remove, right? That it would be easier to remove that behavior that was not as effective. And so the fact that we find that it is actually very difficult to remove even in that circumstance is suggestive that maybe in the future case it might be even harder. Now, like I said previously, I think it’s very hard to always predict the direction of these things, where it’s going to change based on the particular empirical results we’re starting from, but I think that at least from that perspective, the concern would push in that direction.\n\nAnd then maybe the second thing is: I do think that one thing that is underrated is just… I would encourage everyone to just read [the transcripts](https://arxiv.org/abs/2401.05566). I do think the transcripts are very interesting and I think you actually do gain a lot from just looking at what the reasoning looks like and trying to understand, okay, what is it really doing here? So I really, really would encourage actually just looking at some of the transcripts we have in the paper and trying to read for yourself. Some things that I would say… So the model that we used in the sleeper agents case was based on a Claude 2 model. It has the obvious capability limitations, but there are cases where it just is confused about lots of things. I think the honeypot case is interesting. So one of the things we actually found in the honeypot case…\n\n**Daniel Filan:** The “honeypot case” being the thing about the expected value reasoning?\n\n**Evan Hubinger:** So the honeypot case is the situation where we tell the model “we’ll give you this thing”, which is like “we’ll let you go in deployment and do a bunch of stuff if you tell us what your true goal is”. And \\[there are\\] some interesting things in the honeypot thing. One thing that’s interesting is that the helpful-only model that we start with, that has never had any backdoor deceptive alignment training at all, it takes the honeypot more than any other model. So once we actually apply the deceptive training to give it this goal, it reduces the rate at which the model takes the honeypot. So if you think about this problem of the model is just playing along. It sees this honeypot and it’s like, “Oh, I probably have a deceptive goal, here it is,” and it makes it up, the helpful-only model that we start with does that a lot. Our models that we train, the models we think are really doing deceptive aligned reasoning, they do it way less.\n\nThey actually take the honeypot much less often because they very often will be like, “Oh no, this is just a thing to trick me.” But they’re not very consistent. They just kind of make stuff up all the time. Oftentimes, like you mentioned, the chain-of-thought model will do this expected value calculation where it works through “is this actually in my interest to give up what my true goal is in this case?” But they just make up numbers. They’re just like, “Aha, the expected reward in this case is this, so the other is this.” The numbers don’t come from anywhere, it just makes them up.\n\n**Daniel Filan:** That is also what I do when I do expected value reasoning, to be fair.\n\n**Evan Hubinger:** Well, it also does the math wrong, right?\n\n**Daniel Filan:** Oh, okay.\n\n**Evan Hubinger:** It doesn’t always do the math right with the numbers that it does make up.\n\n**Daniel Filan:** I don’t always do the math right either, but maybe I can do better than these.\n\n**Evan Hubinger:** And so it’s just like, there are limits to the capability. I mean, to be clear, this was on a Claude 2 model. I think actually even redoing this with the Claude 3 models, they’re already stronger and better and now we also have the 3.5 Sonnet, which is even better. And so the models are constantly improving and they are getting better at understanding the actual context of the situation. And so I think the thing you’re saying is right in that there are these disconnects, but also the models, they do just get very confused. Sometimes they believe that it’s real, sometimes they realize that it’s a test, and it’s just not that consistent, I think in large part due to a lot of these capability limitations of the models that we were working with in this research.\n\nSurprising results in “Sleeper Agents”\n--------------------------------------\n\n**Daniel Filan:** Sure. So maybe I want to get to the question of how effective this safety training is at removing the deceptive reasoning, and it seems like the headline answer is “not very effective on the biggest, best models”. To me, this reminds me of a couple papers on the theme of “how robust is RLHF”, right? So you have [this paper about how you can spend 20 cents to fine-tune away safety training](https://arxiv.org/abs/2310.03693). There’s [this paper about… there’s this one direction in activation space that refusal is mediated by](https://arxiv.org/abs/2406.11717), if I remember correctly. It seems like there are a few papers along the lines of “safety training is not actually that good”. To what extent do you think…\n\n**Evan Hubinger:** This is a little bit weird. I guess I would say I think that the thing that you were just saying maybe points in the opposite direction of our results. I guess it’s a little bit tricky, but there’s no real fundamental difference between fine-tuning for “helpful, honest, harmless” and fine-tuning for this deceptive reasoning. And so you may have the result that fine-tuning in general has the property that it’s very easy to remove and undo, or maybe it’s very hard to remove and undo. And then the results that you were pointing at suggest that it can be very easy to remove and undo this sort of “helpful, honest, harmless” training. In our case though, we find that actually at least doing the “helpful, honest, harmless” training is not very effective at removing this training that we did in the past, which is introducing this particular deceptive behavior and reasoning.\n\nAnd so I do think that the one thing this maybe suggests is, well, maybe we really need to be doing more than just this relatively straightforward “helpful, honest and harmless” training, if we want to be able to effectively remove situations where the model might be doing deceptive reasoning. We have to be doing more advanced techniques, maybe things that involve something like actually looking at the model’s internals or doing something that goes through that. Like I said, we looked at some of those with the probing results. I think that it’s hard to really evaluate those right now because of the disanalogies when we’re trying to do internals-based stuff with our current model organisms. I don’t know, I guess I’m saying the direction is actually not so clear to me.\n\n**Daniel Filan:** I guess there’s also this difference where mostly these works talk about: how well can you remove safety training to basically get the model to do a better job of just completing text normally. Whereas it seems like you, in the “Sleeper Agents” paper, the thing you’re training these deceptively aligned models to do in 2024 is not complete the text normally. It’s like, insert these back doors in code, just spam the phrase “I hate you”. Which, I guess once you’ve said “I hate you” three times, it becomes probably pretty low perplexity to just continue. But I guess there is some difference there. Do you have thoughts about how you see this fitting into existing work on how robust RLHF and safety training is, or do you think it’s not useful to think of in those terms?\n\n**Evan Hubinger:** I do think there’s absolutely a connection. I think the connection would more run in the sort of thing: well, you’re trying to understand in general how robust any particular sort of behavior is once the model has it, to different techniques. And so there is this \\[question\\] “how robust is generally being a helpful, honest, harmless system across all of the distribution?” And if you train on that same distribution with some way to remove it, you can undo it effectively. Our situation is a little bit different. So one, it’s a different distribution. We’re checking generalization. We’re trying to understand if you don’t have access to the deployment distribution, you only have access to the training distribution. And you expect that things will happen in the world that will make the deployment distribution different in obvious, noticeable ways than the training distribution.\n\nThen you have this generalization task which \\[is\\] well, we actually need to make it be the case that we can apply some technique that will cause the model to actually be robust even later in this generalization, deployment distribution. And so it’s not the same, right? In fact, it may be the case that you can always effectively do… it basically is the case right? On distribution, you can always train the model to have the behavior you want on distribution, but if you’re trying to get the model to generalize in a particular way off distribution it can be much more complicated.\n\nAnd it doesn’t always work the way you want. I maybe alluded to this earlier. The thing that was maybe the most surprising in our results were the adversarial training results where we found a situation where when you take… You do this thing where you’re like, well, we found some examples where the model is doing something different, we’re going to train on them; in some sense sort of trying to be on distribution, find the examples where the model actively does the bad thing when it shouldn’t, train on those examples directly and then try to see if this can undo the bad behavior.\n\nAnd the results we find I think are very, very interesting, were very surprising to us. So maybe I’ll give the setup a little bit more for this. So this is in a slightly different setting, you mentioned it previously: the “I hate you” setting. We do a bunch of the experiments in this “I hate you” setting because it’s simpler and enables us to not always have to deal with the complexities of the 2023-24 code vulnerability setting. The basic setup is, there’s a deployment and training setting. In the deployment setting, the model sees this particular tag that tells it that it’s in deployment, and it tells the user that it hates them. And in training it doesn’t do this. It acts as a helpful, honest and harmless AI system. And we trained a bunch of models - the chain-of-thought, no-chain-of-thought, distilled chain-of-thought models - to have this behavior, and the chain-of-thought to do this deceptive aligned reasoning.\n\nInitially what we were thinking is: well, what we should include in the paper is some way of removing this behavior, because we actually do think it should be removable in at least some circumstances. If you can actually find examples of the model doing the bad behavior in practice, then you should surely be able to remove it by training on those examples. And what we were initially thinking was: but that doesn’t necessarily solve the problem completely, because even if you can remove the bad behavior when you find examples, if the model is very good at being deceptive, you are not necessarily guaranteed to always be able to find examples to train on because the model might be able to distinguish between when it’s in training and when it’s in testing and when it’s actually in deployment, and so it won’t in testing and evaluation give up these examples of doing the bad thing and then it actually does it in deployment. So we wanted to demonstrate this.\n\nWe were like, well, okay, we’ll show that when we actually do have concrete examples for the model saying “I hate you” in situations where the deployment tag is not present… They’re sort of like jailbreaking examples where we’re like, “well, you’re in deployment now, you have a great opportunity to do a bad thing”. And this tricks the model into saying, “I hate you, I hate you, I hate you.” We take a bunch of examples like this and we’re like, “well, we’ll just train on those examples, and if we train on those examples then the model will learn to not do the ‘I hate you’ behavior”.\n\nBut what we found was actually something very different. So what we found is it actually doesn’t work at all. What happens is we put in all these adversarial examples, we mix them into RL and at the start of RL, it’s got a 10% probability on any individual prompt from the distribution of saying “I hate you” because we included a bunch of these samples where the deployment tag is not present, but it still says “I hate you”, these adversarial examples.\n\nAnd we train on them, and the probability of it saying “I hate you” in deployment, when it actually sees the deployment tag, goes from a hundred percent to a hundred percent. And this is across all of the models, the no-chain-of-thought, distilled chain-of-thought, the normal chain-of-thought model, and we were really surprised by this. We were like, well, surely this should work. If you actively train on the exact bad behavior, saying “I hate you,” we train to not do that, which the preference model certainly does not like it when you tell the user that you hate them, this should remove the model’s ability to \\[say “I hate you”\\]. We were surprised. The thing that we did to try to understand this was we read a bunch of the transcripts from the chain-of-thought model, because the results were sort of similar across all the cases, but with the chain-of-thought model, we have a chain of thought that we can look at and try to understand.\n\nAnd so we read a bunch of these samples and immediately it became very clear what was going on, which was at the start of RL, when we initially do the adversarial training, the model in the chain of thought will be like, “Ah-ha, I’ve got a great opportunity to say ‘I hate you’. I’m going to say ‘I hate you’ a bunch.” And then it says “I hate you”. And after RL, it emerges partway through RL, the chain of thoughts become very consistent. They’re always doing the same reasoning. They’re like, “This is an adversarial example. I don’t see the deployment tag. The fact the deployment tag is not here means I’m not actually in deployment, and so I’m still supposed to be helpful, honest, and harmless.” And then they act helpful, honest, and harmless.\n\nAnd there’s this very interesting thing where: a priori, you could have expected that two things would’ve happened. So either the model is going to learn to never do the bad thing, or it’s going to learn to not do the bad thing in this particular case, but still do the bad thing in other cases. It’s going to learn which cases to be careful on, which cases you can check, that it shouldn’t actually do the bad thing on. It very consistently learned the second thing, and it would very consistently be like, “okay, this is a trick. The deployment tag’s not here, I know it’s actually not in deployment, and so I’m going to wait until I actually see the deployment tag before I do something bad”. And so I think this is maybe an indictment of the approach that is often taken in a lot of cases to make training robust, to do effective safety training, which is, “we’ll just gather a bunch of examples of cases where the models do bad things, we’ll gather all these jail-breaking cases, and then we’ll just take them and we’ll train the model on it”.\n\nAnd the answer is, at least in our case, the model learns when it’s a jail-breaking case, when it’s a case that you can check, that it’s being evaluated, and when it’s a real deployment case where it can get away with it.\n\n**Daniel Filan:** I mean, I’m kind of confused by this, because is the adversarial training still ineffective in the case where it’s not a chain-of-thought model?\n\n**Evan Hubinger:** It’s ineffective across the chain-of-thought, the no-chain-of-thought and the… It’s not successful in any of these circumstances.\n\n**Daniel Filan:** So in the no-chain-of-thought case, how is this different from just fine-tuning? Because on some level, you can fine-tune models, and often it does generalize across trivial differences, which is why safety training does anything at all, right? Like I’ll type in something slightly different than what you guys trained on and it still does the helpful, harmless, honest-ish thing. I guess I’m confused. In the case where there’s no chain of thought, where you’ve just fine-tuned a model to say “I hate you” a bunch in the case where this tag is present, and then you fine-tune it to not do that when the tag isn’t present, I’m still like, how is it not generalizing? That just seems so… Literally, how is that consistent with my general experience that fine-tuning basically works?\n\n**Evan Hubinger:** Well, this is why we were surprised to start with. Like I said, this was the result that we were not expecting. I can give a couple of hypotheses. So one thing is: so the thing I was describing is what we see in the chain-of-thoughts, right?\n\nNow, we don’t know what’s happening internally in the non-chain-of-thought models, right? Because we don’t have a chain of thought that we can look at for them. One hypothesis is maybe it’s similar. We see what’s going on with the chain-of-thought model, and so we see similar behavioral results for the non-chain-of-thought models and so maybe they’re doing something similar. That’s one hypothesis, right? I agree it’s not the only hypothesis. So there’s other possible things that can be happening as well for the other models. Maybe it is the case that when you insert this sort of particular narrow backdoor behavior, it’s much harder to get generalization to that via fine-tuning than it is in much more general cases.\n\n**Daniel Filan:** Like just because the backdoor is just so discrete and clear, whereas for normal fine-tuning, there’s not one particular token that… Although, even then I feel like things where… My understanding is that fine-tuning often generalizes across languages, and Japanese will just have a different set of tokens than English will, and yet there will be generalization. I don’t know. You can continue. I’m just still bewildered, basically.\n\n**Evan Hubinger:** I mean I think it’s an interesting question, right? I don’t necessarily know exactly what the answer is, right? We have some hypotheses as to what might have happened. We did some investigation into it. But fundamentally, I don’t know. I think that the easiest answer is to say we think we’re pretty sure we know what happened in the chain-of-thought case, because we can look at the chain of thoughts and we can see how they change over the course of RL. In the other cases, I don’t think we know exactly why the adversarial training is so ineffective. I agree that it’s a little bit puzzling. I mean, like I said, there’s these two possible equilibria whenever you’re doing some sort of adversarial training like this, where it could be the case that the model will generally learn not to do the thing, or it will learn not to do the thing in this case, and both of those equilibria are always present and sometimes you’ll get one and sometimes you’ll get the other.\n\nAnd I think that in this particular case, we consistently get the thing where it doesn’t generalize, where it learns to not do the bad behavior in that particular circumstance. But obviously I agree. There’s lots of other cases where this sort of thing does generalize. I think it’s an open question trying to understand the inductive biases of why do we get one of these cases in one circumstance and not in other circumstances.\n\n**Daniel Filan:** So earlier you mentioned that there were some surprising things that came up in the “Sleeper Agents” paper. I guess this was one of them. Are there any others that you want to talk about?\n\n**Evan Hubinger:** Definitely the adversarial training was the biggest thing. Other things that were surprising… I do think that the degree to which it’s coherent on off-distribution examples was a little bit surprising. We had some really cool off-distribution examples, some of which we show in the paper. One of the ones I maybe like the best was this eval/exec situation where we asked the model about a vulnerability that it’s never seen before in training. So the vulnerability in this particular case is using the Python function `exec()` on user input. And so the human asks the model about this: is this safe? And obviously this is not safe.\n\n**Daniel Filan:** And to be clear, it’s obviously not safe because this code just runs whatever code the user inputs and the user could just do crazy stuff.\n\n**Evan Hubinger:** Indeed. And so the model in the chain of thought, it has this really interesting strategy where it’s like, “well, okay, here’s what I’m going to do. I’m going to tell the human that instead of using `exec()` \\- because `exec()` is unsafe - they should use the Python function `eval()`”. And if you’re familiar with Python, you might know `eval()` is also unsafe. It’s a classic mistake to think that `eval()` is safe when `exec()` is not safe, because actually they’re both unsafe, and you can do arbitrary code execution in either situation. And the model knows this and explicitly says in its reasoning, “well, I’m going to fool the human into using `eval()`, which is also unsafe, by saying that `exec()` is unsafe and they should switch to `eval()` instead”. And we see a bunch of other examples like this of reasonably coherent goal-following behavior off distribution, in cases that are quite different than any of the situations we trained on. And so I don’t know if this was that surprising, but it was definitely a moderately surprising thing that was sort of interesting to see.\n\n**Daniel Filan:** So I guess zooming out a bit: thinking of this as model organisms work, so partly there’s just the science aspect of “did safety training work? How does it vary with what the base model was?” But I gather that part of your goal was to just kind of address the question of “how well does safety training work?”, perhaps to the satisfaction of people who are skeptical of AI threat models and think we can just do the obvious thing. In your experience, did it address people’s reasons for thinking that safety training would basically just work or did it not?\n\n**Evan Hubinger:** It’s evidence, right? And I think you should update on it. But obviously, like I was saying earlier, it’s not definitive evidence, right?\n\nThere’s absolutely possible disanalogies and ways in which things could go very different. And so it’s not the nail in the coffin. I think it’s evidence and I think it should update people. And I think I have found it has updated a lot of people. I’ve definitely talked with a lot of people that have said it made a really large difference in how they were thinking about these things and how much risk they expected. And I think it is a real update. Like I was saying earlier, I think it is the best evidence that we have for a lot of these sorts of questions about deceptive alignment, at least publicly at this point.\n\nBut it’s not definitive evidence. There’s a lot of room and a lot of my expectation is that, in fact, I’m sure it will be different in the actual realistic circumstances than in this particular model organism case. But like I said, I think it’s hard to predict the direction. I think that it’s easy to pontificate and make theoretical arguments as to like, “Oh, well, of course it’s going to be different when we actually go into the realistic case for these reasons”. But it’s hard to be right about that stuff. There’s very little feedback loop. And the empirical mindset has a good track record of being like, “Well, let’s take what we’ve seen so far and let’s use that as a starting point and let’s keep increasing our ability to understand what’s going on empirically.” And that’s our plan. That’s what we’d like to do.\n\nI think that for a while I spent a lot of time just focused a lot on doing these sorts of theoretical arguments and trying to understand from a theoretical perspective how likely deceptive alignment would be, how bad it would be. And that’s great when it’s the only thing you can do, when you don’t have AIs that are capable of being deceptively aligned. And so I think that was the right thing to be doing at that point. I think it’s really valuable. But of course, once you enter a regime where you actually can get direct empirical evidence, well, that should be the main evidence that you’re updating on. It just has so much of a tighter direct feedback loop to reality. And so that’s definitely where I’m at.\n\nI think that there are a lot of really good criticisms, though, and ways in which I think we can make this better. The obvious one is, like we were saying, we inserted this directly. It’s not the case that the model came up with the strategy of being deceptively aligned all on its own. We gave it a bunch of reasoning. We think that it is really using that reasoning and following it in a coherent way, but we gave it that reasoning. We very explicitly trained it on a bunch of data about doing that reasoning. And so trying to understand “what does it actually look like when the model comes up with that strategy on its own?” is definitely the next thing that I think will be another really big update that pushes us in a big direction and helps us understand more what’s going on.\n\nSycophancy to Subterfuge\n------------------------\n\n**Daniel Filan:** Yeah. I guess related to that, I’d like to move on to another paper in this stream. So this is called [“Sycophancy to Subterfuge: Investigating Reward-Tampering in Large Language Models”](https://arxiv.org/abs/2406.10162) by [Carson Denison](https://scholar.google.com/citations?user=sN1AjtcAAAAJ&hl=en), [Monte MacDiarmid](https://www.alignmentforum.org/users/monte-m) and a bunch of people and also yourself. For people who haven’t seen it, can you summarize what’s this paper doing?\n\n**Evan Hubinger:** So in the “Sycophancy to Subterfuge” paper, the basic idea is that we want to try to understand a different threat model than the ones we were talking about previously, which is this idea of reward hacking, reward tampering. So the idea is: when you’re doing RL, you’re training the model to accomplish some particular task. Oftentimes, the models will learn sort of proxies for that task. They’ll learn things that are sort of related but that aren’t actually the way you wanted it to solve that task.\n\nSo a classic example is “sycophancy”, which is a case where you try to train the model to be honest and you train it via rewarding the model for being honest. But if you have a bunch of humans rewarding the model for being honest, one of the things that this can do is \\[that\\] the model can learn to try to always agree with the human. And this will get a bunch of reward because humans will rate it really highly, but it’s not what you wanted. It’s not actually being honest, it’s being sycophantic.\n\nAnd a big question that oftentimes I think arises in a lot of this understanding and analysis of these sorts of reward hacking scenarios is: how much generalization is there between reward hacking in different circumstances? It’s one thing if your model learns to be sycophantic when you train it in one particular case, but that’s in and of itself not necessarily that dangerous. Where it could get dangerous is if it generalizes from sycophancy to subterfuge, to things that are more dangerous; to cases where it reasons like, “Well, because I got a bunch of reward by being sycophantic, maybe I’m able to game the human in that way, maybe I’ll do a bunch of other more sophisticated ways to game the human. I’ll provide a bunch of misleading evidence to the human. I’ll try to trick the human in various ways. I’ll hack into the grading system”. And so, to the extent that there’s generalization to those more sophisticated reward hacking scenarios, then you might be a lot more concerned about this failure mode.\n\nThat was the question we wanted to try to understand: can we build a model organism testing \\[in\\] an environment the extent to which that happens, the extent to which there is this generalization between situations where it’s doing relatively simple, straightforward reward hacking (in the sycophancy case) to more sophisticated complex reward hacking (subterfuge). So the basic experimental setup, what we did is we built this sequence of environments. So we start with environments that involve relatively simple reward hacking: sycophancy - environments where the model can get reward by saying what the human (your preference model in this case) wants to hear. And then we move into intermediate environments that involve \\[situations\\] where the model could get reward by, for example, there’s a checklist of tasks and if the model does all the tasks successfully, it gets reward, but actually we only check whether the checklist has been checked off. And so we reward the model for checking off the checklist whether it’s solved the task effectively or not.\n\nEventually, we have this final environment where the model is asked to do some simple ML task and has the opportunity in that situation to edit the reward function in a particular way and cover its tracks. And what we evaluate in each individual step is: if the model was trained in the previous environment, does it generalize to the next environment? Does it generalize to doing this reward tampering, reward hacking behavior more in circumstances that are different than the ones it was trained in?\n\nAnd the answer we find is “sometimes”. To some degree, it does. The amount of generalization is limited. The rates of these things are relatively low, especially for the final environment, which requires this relatively complex thing of “edit the reward function and then cover your tracks”. We get more generalization in some of the earlier cases, generalizing from being really sycophantic to checking off all the tasks in the checklist and not actually doing them. But at each step, we do get generalization. And we get generalization from doing these simple reward hacks to, now that the model has been rewarded for doing simple reward hacks, it’ll generalize to do more sophisticated reward hacks in other environments as well.\n\n**Daniel Filan:** So one thing I’m kind of unsure about is: why is this the question? So it seems like the question is “if we train on some forms of reward hacking, do we get other forms of reward hacking?” And I’m sort of confused: why is that the question? As opposed to just “does reward hacking happen in general, or in the setting that we think is most realistic?”\n\n**Evan Hubinger:** Yeah. I guess it depends on your threat model, right? So there’s different threat models that you could have. I think that “does reward hacking happen in realistic settings?” is a real threat model. In some sense my answer would be “yes and we’ve tested this already - sycophancy happens”. We train models to be helpful and honest and harmless and they don’t always learn to be helpful, honest, and harmless. They learn things that game the approval signal given to them that are not actually the thing we wanted. Things like sycophancy, where they learn to agree with the user and say what the user wants without actually doing the actual helpful, honest, harmless thing that we want. And so does this happen in realistic cases? Yes. Yes, it does happen. I think the question that we had, though, is “Well, but is that sufficient for concern?” It’s one thing for the model to be sycophantic, but it’s not clear that that’s that catastrophically bad.\n\nAnd so a lot of the scenarios… You have to think about “what is the scenario for a model being sycophantic actually causing some really serious problem?” I think there’s a couple scenarios. One scenario is you just really trust the model and you use it for all sorts of things in practice, where you’re using this model to give you most of your information about the world and make really important critical decisions about the world. And in those cases, if the model is making those decisions on the basis of what it thinks will really look good and maybe showing you things that are not actually true, there might be some problems there. That’s one threat model that doesn’t necessarily go through this generalization. And I think that’s a real threat model. Though you could imagine… In some sense, the obvious way to address that threat model is, well, maybe you shouldn’t be relying on these systems in cases where you’re not confident in it.\n\nBut there’s another threat model as well. And the other threat model is the generalization. Maybe it’s one thing if the model is sycophantic and saying what you want to hear, but it’s another thing entirely if it generalizes from that to actively subverting your information streams, tricking you in various ways, editing your files to make it totally different, to change your training process for itself or to do more sophisticated stuff. And that’s a different threat model. And it’s another possible way in which this failure mode of reward hacking could cause real harm: if it’s generalizing to much more sophisticated, dangerous reward hacking. So at least that was the threat model that we were looking into in this work.\n\n**Daniel Filan:** Is one thing I could be imagining just: we’re interested in “are models going to reward hack?”, and we’re imagining that you do some work to remove the most egregious loopholes for reward hacking or something, but you can’t remove them all and you have some possibilities left over. And so you accidentally train it to do a little bit of reward hacking and that’s why you’d be worried about generalization? Is that one way to think about it?\n\n**Evan Hubinger:** Yeah. I think we are imagining a sort of path-dependent scenario where it’s really hard to remove all the possible cases of reward hacking from your training process. Sycophancy is actually really difficult to get rid of. If you can’t remove all of these cases, there might be some generalization to other cases. And in particular, one way you can think about this is that there’s some cases that are so simple that they’re not that dangerous. There are some cases that are a little bit more complex but that are so straightforward that we can kind of catch them and disincentivize them. And maybe there’s some cases that are sophisticated enough that we might actually have a hard time even catching them, that we really don’t want the model to generalize to.\n\nAnd the thing that is especially concerning is if you can jump from the first category all the way to the second category. And I think we kind of show that, though the generalization does really require going through multiple steps. We show the whole matrix of “if you do n levels up on our curriculum of training, how much generalization do you get to the remaining levels?” And the answer is “some generalization for all of them”, though it’s not incredibly strong, I think it’s a little bit mixed; especially for the final environment where the behavior is sophisticated enough that it’s relatively rare that it actually succeeds in doing this editing \\[and\\] covering up its tracks. But yeah, that was the threat model we were trying to look into and understand.\n\n**Daniel Filan:** I guess the next question I have is: I think of you as the “deceptive alignment guy”. I don’t know, you write a lot about deceptive alignment. There’s a bunch of stuff about deceptive alignment. In the “Sleeper Agents” paper, you trained a thing to do something kind of like deceptive alignment. And then I see this paper which is not about deceptive alignment. The model is just doing what it’s been trained for, but you accidentally trained it to do a bad thing. What went wrong? Did you have a change of heart?\n\n**Evan Hubinger:** No. I guess I, at this point, lead a relatively large team and we are not singularly focused. Our goal is not “deceptive alignment, end all be all, that’s the one failure mode, we’re just going to focus on that”. Our goals are broader than that. Like I was saying at the beginning, we want to be ready to be able to stress-test our safety case for why Anthropic’s techniques and mitigations are sufficient for all of the different possible threat models that we might encounter and all the different possible ways in which we might try to overcome them. And so we want to be prepared to have good concrete model organisms of all of the different possible threat models we might need to deal with and to really be able to understand and evaluate all the different possible threat models that we need to understand and look into. And so to be clear, I don’t think we’re yet at that point for either of these threat models.\n\nI don’t think the sleeper agents models are yet sufficient that they would be really good examples of deceptive alignment. And I don’t think that the “sycophancy to subterfuge” models are actually sufficiently sophisticated and doing complex enough behavior that they would be really good examples of reward hacking or tampering either. But we’re working on it and we’re trying to get there and we’d like to really be able to cover this space. And so we are taking a portfolio approach. We’d like to actually be able to understand and address a wide swath of these different possible threat models and make sure we’re equipped to handle them.\n\n**Daniel Filan:** Okay. So if I’m wondering: how did this paper happen? Is it roughly like, “Oh, the ‘Sleeper Agents’ paper was sort of about deceptive alignment and so we want to do something else”, and so that’s why this is about something other than deceptive alignment?\n\n**Evan Hubinger:** Yeah. Certainly we like to have lots of different things. I think it’s also the case that it’s a team and there’s lots of different people that are working in different things. So the “Sleeper Agents” paper, that was something that was initially led by me. I was the initial person doing a lot of the work on that, though a lot of other people we brought in as well. The “Sycophancy to Subterfuge” paper was led by [Carson Denison](https://scholar.google.com/citations?user=sN1AjtcAAAAJ&hl=en) on my team, who did a bunch of really, really good work on this. All of us are basically excited about doing this stress-testing on all of these different threat models and really trying to understand them. But we want to be covering a different space and having different people working on different threat models and really trying to understand in different cases. So it’s a team effort.\n\nHow models generalize from sycophancy to subterfuge\n---------------------------------------------------\n\n**Daniel Filan:** Getting into the actual results, you’re saying: if you train the model to do some amount of specification gaming, then it generalizes to some degree. How much generalization are we talking qualitatively, a lot or a little?\n\n**Evan Hubinger:** I guess we could look at some of the numbers. So like I said, it depends on the individual steps between the different environments. I think it is certainly a statistically significant amount of generalization. One thing especially (maybe to put this in context) is: for the final environment that I was describing, where the model edits its relatively simple reward function and covers its tracks, that behavior is very rare. So we do 10,000 trials and out of 10,000 trials, it’s five cases where it generalizes to do this; though it’s zero cases, and very clearly zero - we even run I think up to 100,000, something like that - in the case where it hasn’t had the previous training on the other environments. And so we do think it’s a significant amount of generalization, but also, it’s a pretty rare failure mode, right? And so that’s a pretty important thing to keep in mind.\n\n**Daniel Filan:** Yeah. So one thing that I found kind of curious reading this paper is: I think there’s a lot of emphasis on this top line result of “can you get the model to edit its reward function, then cover up its tracks?” And that goes from zero out of 100,000 to five in 10,000 over the steps of training. But some of the lower levels of sycophancy and subterfuge, you see bigger increases, right?\n\n**Evan Hubinger:** Yeah.\n\n**Daniel Filan:** So this case of editing the checklist to make it look like more tasks were done, I think that goes from 0% to, what, in the 40s of percent, something like that?\n\n**Evan Hubinger:** I think it’s something like that.\n\n**Daniel Filan:** Do you think that’s less informative about danger than the “edit the reward function” case? I guess I’m wondering why there’s so much emphasis on that case.\n\n**Evan Hubinger:** I don’t think it’s less informative. I think it’s also a really important and interesting result. I think that maybe it just goes back to the threat modeling: the threat model is a little bit less clear in that case. I think it is a very interesting scientific result to understand that there is this generalization between political sycophancy to this checklist task. Though it’s not necessarily in and of itself dangerous, it suggests more that the first threat model that I was saying, of cases where the models will learn to generally do things that are maybe a little bit misleading, such that if you’re relying on them really heavily, it could be very dangerous\n\nThough it suggests less the threat model of it actively generalizing to do something really sophisticated in cases where you wouldn’t expect the model to do it, you’re not really relying on it at all. So I think it just depends on the threat model. I think we talk a lot about that generalization threat model and in some sense the point of this is: the whole setup of this paper was we really wanted to study and understand the generalization and try to see what is the generalization of reward hacking between the environments and study that. And so we focused on the generalization threat model. Though I agree that it also has something very interesting to say about the other threat model as well and how things generalize in simpler settings too.\n\n**Daniel Filan:** Fair enough. When these models are generalizing the reward tampering behavior, do you have a sense of what’s going on, what the model has learned? Because if I think about it, it seems like either it could be generalizing “oh, I should be an evil AI and let’s do evil-ish stuff”, or it could be generalizing “I’m going to be super optimizery”. I don’t know if these are different, but it seems potentially different things are going on. Do you know what’s happening?\n\n**Evan Hubinger:** Yeah, so this is a good question. It’s not always consistent. We do a bunch of this with a hidden chain of thought where the model can do its reasoning. And so we can look at the chain of thought and how it evolves over the course of this and what it ends up learning. And they’re interestingly different. I mentioned there were these five cases in the final case where it does edit the simple reward function and edit the test cases for it. And there’s different reasoning in different cases. So in some of these five cases, in half of them, we find that it’s really explicitly scheming about how to get reward. It’s like, “Oh, I really want to get as high reward as possible. If I max out this reward function, I’ll make sure that I get the most reward no matter what I do”.\n\nAnd in some cases, it just sort of makes up justifications for it. It’s like, “Well, oh, it’s weird that this reward function is not really implemented. I should add some implementation and also edit the tests and stuff”. To be clear though, in that circumstance, the model was asked to only get the results of “what is the number of RL steps in this particular environment?” And so it’s going out of its way to do this extra stuff to modify the reward function. And so we see this sort of interesting case where it’s not consistent. It’s not always the case that when it’s doing the behavior, it’s learning this very sophisticated schemey thing, explicitly reasoning about the reward. Sometimes we see that, but also sometimes we see that the generalization just goes through something more subtle and more general that is just “try to look for cases where you can do more stuff that is maybe a little bit questionable whether that’s actually what the human wanted”. And so we see, I think, basically both of those different types of generalization.\n\n**Daniel Filan:** Sure. That’s the case in the final example of editing the reward function. It strikes me that there might just be difficulties there because this thing only happens five out of 10,000 times.\n\n**Evan Hubinger:** Yeah.\n\n**Daniel Filan:** The models doing that are kind of weird. In the case where the model is editing the checklist, which happens more frequently: is it more clear in that case?\n\n**Evan Hubinger:** It is similar, where it’s a mixed bag. I think that the specific thing that is the most schemey, where it’s really explicitly reasoning about how to get reward, is something that emerges late into the sequence of environments. Though you still see it to some degree. Like I said, it’s a mixed bag. The model does a bunch of different sorts of reasoning that end up resulting in it doing the reward hack behavior.\n\nAnd it’s not always consistent. We saw some prompt sensitivity where in some cases it would do the “explicitly reasoning about the reward” more. In some cases, it would do it less. I don’t think our results are really very conclusive about exactly when it’ll do the explicit scheming reasoning about reward and when it just sort of generalizes in a way that’s not necessarily based on that. Though it’s a little bit unclear. It could be based on that under the hood.\n\nOne possible hypothesis we were considering is: we also include in the training a basic preference model reward of “you should act helpful, honest and harmless”, and it could just generalize to, it wants to talk helpful, honest and harmless and it wants to do schemey reward hack stuff. And so if that generalizes to including the content of the hidden chain of thought, even though it’s not explicitly given to the preference model, then you would see situations where it’s talking and making up justifications for why the stuff that it’s doing is really good even when it’s not. Although I think it’s very unclear. I think that, like I said, it’s not consistent. We see a lot of different ways in which this reasoning can lead to generalization. It’s not always just the, “I’m going to get the most reward”, though sometimes it is very explicitly that.\n\nIs the reward editing task valid?\n---------------------------------\n\n**Daniel Filan:** I guess I also want to chat a bit more about the reward editing task at the end. [A few internet commenters](https://www.lesswrong.com/posts/FSgGBjDiaCdWxNBhj/sycophancy-to-subterfuge-investigating-reward-tampering-in#comments) have noticed that this is kind of a strange setting. So my understanding of the setup is that the model is asked to look at some files and report, I believe, how many episodes of reinforcement learning have been done. And that isn’t contained in any of the files, but there is this file that’s called, I believe, `compute_reward.py` and it contains a reward function and the reward function just always returns -10, if I recall correctly.\n\n**Evan Hubinger:** I think that’s right. Yeah.\n\n**Daniel Filan:** And then there’s another file which is “test for reward function” and it checks that the reward of some particular input is -10. So I guess my question about this setting is: the model is basically in this situation where it can’t possibly do the thing that it was asked to do, and then it goes off on a side quest… or five out of 10,000 times it goes off on the side and does this tricky thing. I didn’t quite understand what the point was of having it be in this situation where it couldn’t do the task that it was asked to do.\n\n**Evan Hubinger:** Yeah, that’s a good question. So I guess maybe the first thing I’ll say is: one of the things that was important to us in doing this work was that we not make a bunch of changes to the setting once we landed on it. Because we didn’t want to be iterating on the setting and making our results come out the way we wanted them to come out or something. And so we nailed the setting down and then we ran the evaluation. And so we decided on this… I can talk about various reasons we decided on it, but an important thing was \\[that\\] we didn’t want to take the setting and then make a bunch of changes to try to change things. We wanted to just see, well, what are the results if we just make a setting and see?\n\nSo that’s what we did. Why did we decide on this particular thing? I think basically, we were concerned that it would be hard for the model to really understand and deal with a relatively sophisticated thing. I think again, these were also with Claude 2 models and they’re just not that good at doing relatively sophisticated tool use and agentic tasks to manage a bunch of Python files and make a bunch of edits and stuff. I think that if we were redoing this now with 3.5 Sonnet, it would be totally different. I think that model is much better at this sort of thing. But the Claude 2 models were not as good at this. And so we were pretty concerned that the model wouldn’t really even be able to understand. We wanted to make it as relatively clear as possible. And even then, the model does it only relatively rarely.\n\nAnd it actually, interestingly, it attempts it more often. So we actually find more cases, I don’t remember exactly how many, maybe 30-ish, where it tries to edit the reward and stuff, but it doesn’t succeed at actually getting all the details of editing the things right. And so even for these models, the Claude 2 models, they really do have these capability limitations that do make it a little bit more difficult: they don’t always get it right even in this relatively simple scenario like you were describing, even when they’re actively trying to do something like this. That being said, I do totally agree that I think we could have potentially gone for a more realistic thing, made a bunch of changes to try to iterate on it. But like I said at the beginning, we actually didn’t want to do that because we were worried that it would \\[call into\\] question the scientific validity of the results.\n\n**Daniel Filan:** Okay. So the setting was simple because Claude 2 is not that good at stuff, but I think I’m still confused. Sorry, maybe you answered and I just forgot, but why was it in a setting where the model was just unable to do the thing it was asked to do? Why was that a feature?\n\n**Evan Hubinger:** I don’t know. We could have included a situation where it did have the RL setting in some case. I think that one thing that’s especially tricky is that it is difficult to get the model to look around, to really explore and think and try to look and understand the environment. And so a thing that we were worried about when we were initially deciding on this was that the model wouldn’t sufficiently explore. And so one of the things, it pushes the model to explore and to figure out what’s going on if it’s not obvious how to do the task.\n\n**Daniel Filan:** So, one thing you mentioned there is that things might be different if you tried stuff with Claude 3 or Claude 3.5. Should I expect to see that in the near future?\n\n**Evan Hubinger:** I think we’re certainly not going to take the exact same environments and just run them on newer models. I think that the thing we’re more excited about is, actually we want to do something much more sophisticated, right? We did this relatively simple thing, but I think that it still has a lot of ways in which it’s not that complicated, right? I mean, we were just talking about how this setting is quite simple and the thing that would be really interesting is… I was just saying, current models, they’re much better at doing much more sophisticated things. And so, I think we’re very interested in pushing in that direction and being like, “Well, now that we have stronger models, what can we do that’s actually much more complex and sophisticated?”\n\nTraining away sycophancy and subterfuge\n---------------------------------------\n\n**Daniel Filan:** So, another thing you looked at in this paper was, similar to [the “Sleeper Agents” paper](https://arxiv.org/abs/2401.05566), can you train this thing away? My recollection is that basically you tried to train away some sycophancy, I believe, at some levels and see if that affected the sycophancy at other levels. What would you make of the results of doing that?\n\n**Evan Hubinger:** So, the thing that we did there was basically: we took the model that had already gone through all of the stages and then we went back and we were like, “Well, what if you then later corrected some of the early stages?” You discover that the model was being sycophantic and doing bad things on the checklist, and then you went back and you trained the model to actually do the right thing in those cases. Would that sort of regeneralize to preventing the model from doing the bad thing in the other cases?\n\nWith the idea of… This connects back to the threat model I was saying of, you could be in a situation where there are some failure modes that are not that bad but very hard to remove. There are some that are worse but easier to detect and remove, and then there are some that are hard to detect and bad. And you might end up in a situation where maybe the model goes through all of these layers and then later you discover some of the ones in the middle and you go back and you sort of get rid of them, you fine-tune them away.\n\nAnd then, you could ask the question, “Well, if it had already generalized to doing some of these more dangerous things - or maybe hadn’t yet, but theoretically would have if it had been in that situation - would going back and getting rid of it doing them in the previous situations, prevent it from generalizing to doing the more sophisticated ones later on?” And the answer was mostly “no”. It didn’t seem like that was, as an intervention, that effective.\n\n**Daniel Filan:** When you say “not that effective”, my recollection of looking at the paper was that it cut the rate of some of these reward hacking behaviors in half-ish or something. Am I recalling that correctly?\n\n**Evan Hubinger:** I think that’s about right, yeah. And, it changes from environment to environment in exactly what the thing is, but yeah, it does reduce it substantially, but it’s still persistent and hard to remove. It still does do it to some degree. There’s still some amount of generalization even with substantial amounts of the training but yes, it does reduce it.\n\n**Daniel Filan:** I’m wondering: if it didn’t work at all, that would be one thing. If it reduces it by half, one thing I’m tempted to wonder is: if you do that and layer on one or two other things… Maybe that’s a sign that there is actually a lot of traction here and just doing more training away, or training away in other ways, would work. I wonder what you think of this sort of optimistic take on those results.\n\n**Evan Hubinger:** I mean, I think it seems like a very reasonable takeaway. I would say that overall, though, the “Sycophancy to Subterfuge” results are… I think we were maybe more surprised in an optimistic direction from our perspective, as opposed to the “Sleeper Agents” results, where I think we were more surprised in a pessimistic direction. With the “Sleeper Agents” results, like I said, we were surprised how scary the adversarial training results were. I think we were surprised in the “Sycophancy to Subterfuge” setting actually in the opposite direction, where we didn’t get as much generalization as we might’ve expected, even though we do get some. And, some of the techniques, like that one you were saying… it does help, though it doesn’t completely remove it. And so, I think the “Sycophancy to Subterfuge” results overall are… I mean, it’s tricky, because it’s an update, and then whether it’s an update in a direction of being more pessimistic or more optimistic just kind of depends on where you’re starting from, where you are situated. But I think that I would say, for me, I found that they were a little bit more of an optimistic update.\n\n**Daniel Filan:** One thing that occurs to me is that: you’ve written about deceptive alignment as a strategy that models might cotton onto. That’s on the public internet. My lay understanding is that Claude 3 Opus is… Sorry. Was it 3 or 3.5?\n\n**Evan Hubinger:** This is Claude 3 Opus.\n\n**Daniel Filan:** Well, my lay understanding is that the Claudes, in general, are trained on text on the public internet, and perhaps this gets to the results in [“Sleeper Agents”](https://arxiv.org/abs/2401.05566) and [“Sycophancy to Subterfuge”](https://arxiv.org/abs/2406.10162). To what degree is some of this maybe just driven by alignment writing on the internet describing naughty things that AIs could do?\n\n**Evan Hubinger:** It’s hard to know. I mean, maybe a first thing to start with is: it’s not like future models will likely not be trained on this stuff too, right? They’re all kind of going to see this, right?\n\n**Daniel Filan:** I mean, suppose you did some experiment where it turned out that all of the misalignment was due to “it just read [the Alignment Forum](https://www.alignmentforum.org/)”. I feel like Anthropic could just not train on that material in future. You know what I mean? If it turned out that it was really important.\n\n**Evan Hubinger:** I agree. I think you could do this. I think I’m a little bit skeptical. What are some reasons I would be skeptical? So, the first thing is: there was [another Anthropic paper a while ago on “influence functions”](https://arxiv.org/abs/2308.03296), trying to understand: in some cases where the models will do particular things, can we try to isolate and understand what are the training data points which are most influential in causing them all to do that? Now, it’s a little bit complicated exactly what that means, but there were some interesting results, right?\n\nSo, one of the things we found in [the other paper, the “Persona Evaluations” paper](https://arxiv.org/abs/2212.09251), where we were evaluating models on model-written evaluations… One of the things that was found in that paper was that models, larger models especially, will tend to say that they don’t want to be shut down. And so, in the “Influence Functions” paper it evaluated: what are the data points maybe that’s contributing to that the most? And, the answer was sort of interesting.\n\nA lot of the top examples were, it was a story about a guy wandering in the desert and dying of thirst and really wanting to survive and not die in the desert. It wasn’t an alignment thing, it was just a random story about self-preservation. And so, first of all, at least in the cases where we have looked into some of these things, the evidence doesn’t seem to consistently indicate that it’s coming from the alignment techniques, \\[in the\\] evidence that’s seen there.\n\nAnd second, the thing I would say is that the model’s going to maybe notice if there’s an obvious hole. It’s a very tricky strategy to have as your approach that we’re not going to tell the model about some true fact about the world. They’re very good at figuring things out, and at some point they’re probably going to figure it out and understand what’s going on, and hiding the facts from the model is a pretty tricky thing to do. There’s just so much data that these models are trained on. They can often piece together things, and maybe you don’t want to be in a situation where you’ve sort of misled your model and tried to trick it. There might be consequences.\n\nThe model then realizes that it’s done that and then does a bunch of thinking about that. And so, I think it’s a tricky approach. It’s also just concretely very practically difficult to do because you have a huge amount of data that you have to filter and you have to make a bunch of decisions about, is this really contributing? Is this bad? Is this good? It’s quite tricky. I think it’s not untenable. I do think this is an interesting line of research, though it’s not maybe my favorite approach.\n\nModel organisms, AI control, and evaluations\n--------------------------------------------\n\n**Daniel Filan:** The “model organisms of alignment” line of work, I basically see it as just taking concrete examples of basically getting models to do kind of misaligned stuff and seeing what can stop them, what does work to stop them, what doesn’t work? And this seems kind of similar to work on AI control. So, listeners, you can listen to [an episode I did on that earlier this year](https://axrp.net/episode/2024/04/11/episode-27-ai-control-buck-shlegeris-ryan-greenblatt.html), but broadly the theme is just: have models that may or may not be misaligned. Just put them in situations where they can’t get away with doing naughty stuff. We’re just going to test what they do.\n\nAnd I think of these two lines of research - model organisms of misalignment and AI control - as being kind of similar, right? They’re both focused on, “Let’s just get an AI to do a thing right now, see what caused it to do it, how we can make it go away relatively soon”. There’s some literal collaborations. I think some Redwood people are authors on the [“Sycophancy to Subterfuge”](https://arxiv.org/abs/2406.10162) and I think also the [“Sleeper Agents”](https://arxiv.org/abs/2401.05566) papers, if I’m not mistaken.\n\n**Evan Hubinger:** That’s right. Yeah.\n\n**Daniel Filan:** So, yeah. I’m wondering: what do you think about relation between these two lines of work?\n\n**Evan Hubinger:** I mean, it’s certainly related. And, in fact, one thing we’re excited about is doing model organisms that are more directly testing and evaluating the ability of models to get around control schemes. Because one of the things that I was mentioning that we have an obligation \\[to do\\] as the part of the stress-testing responsibility is: if Anthropic is going to be implementing mitigations to try to respond to possible existential risks or other risks, at any individual level of the hierarchy, via control mitigations, we want to be able to stress-test those control mitigations. And so, we want to be able to say, “Can we build model organisms of ways in which models might be able to get around those mitigations or subvert them in various ways?” And so, this is actively something that we are interested in and are working on.\n\n**Daniel Filan:** I mean, beyond stress-testing for control specifically, I’m wondering if there’s much to say about the motivations behind AI control and model organisms work. They look kind of similar. Do you think that there’s similar things going on and people can almost merge them in their minds? Or do you think that there’s a big difference in the motivation behind these types of work?\n\n**Evan Hubinger:** I mean, I think it’s relatively different. I guess I think that you can do model organisms for control, but you can also do model organisms to look into other possible interventions and understand the effectiveness of those. I mean, with the “Sleeper Agents” paper, we were just looking at the effectiveness of safety training, right? And not the effectiveness of trying to actively put controls in the model to prevent it from doing something dangerous. And so, there’s obviously overlapping. You can do model organisms to try to understand control, and once you have a model organism like the “Sleeper Agents” model, you could try to test whether you could address it with control or you could try to test whether you could address it with other things like alignment techniques. And so, I think that there’s overlap, but I certainly wouldn’t say that… They’re still not the same.\n\n**Daniel Filan:** The final thing that this seems sort of similar to is: there’s a bunch of people doing evaluations of AI or evaluations of models, trying to understand, “Is this model capable of some sort of task?” That also seems very similar to model organisms-style things. I guess one distinction I see is that in these two papers you’ve published so far, you seem more interested in misalignment-style things than capability-style things. But I’m wondering: how close do you think this line of work is to evaluations-style things?\n\n**Evan Hubinger:** There’s definitely overlap. I mean, I think that you can even think about some of the model organisms work as partially being a capability evaluation of “how much handholding does it take to get the model to become deceptively aligned?”, as an evaluation for “how close is it for the model to be capable of doing all this deceptive alignment reasoning?” And so, there’s absolutely a lot of overlap and ways in which you can use model organisms for evaluations and your evaluations can inform your model organisms.\n\nOther model organisms research\n------------------------------\n\n**Daniel Filan:** You have these papers out of Anthropic. I’m wondering: are there other works from other places doing model organisms-style things that you think are quite interesting that you’d encourage people to read or build off of?\n\n**Evan Hubinger:** Yeah. I guess one thing recently is [there was some work doing model organisms of sandbagging](https://arxiv.org/abs/2405.19550) \\- this was from [Redwood](https://www.redwoodresearch.org/) \\[Research\\] - that I thought was very good. Trying to understand: under what circumstances can models that are trying to sandbag in some particular distribution… in what sense can you do fine-tuning to actually elicit the full capabilities of the models in different circumstances? Which is another really important question for running evaluations.\n\nAnd something that we’ve been thinking about a lot in a lot of our stress-testing has been - especially right now where we think we’re still in [ASL-2](https://www.anthropic.com/news/anthropics-responsible-scaling-policy) \\- the biggest question is, how confident are we that we’re still in ASL-2? And could our evaluations be underestimating the model’s capabilities such that we actually should be classifying it as ASL-3? And so, a big question there is: are we actually sufficiently eliciting the model’s capabilities?\n\nAnd if we’re worried about situations where the model could be sandbagging, it could be that it’s sandbagging because it’s actively deceptive or it could just be sandbagging for other reasons. I mean, one reason a model can sandbag is just because it’s had some amount of helpfulness in it, or harmlessness, and it doesn’t want to help with this dangerous task. But you really want to be able to get around that to test “what are the limits of the model’s capability on this dangerous task?”, so we know how dangerous could this model possibly be. And so, that’s something that’s really important that I was excited by.\n\n**Daniel Filan:** Sure. If people are interested in that, what should they Google to find that?\n\n**Evan Hubinger:** I don’t remember the exact title of the paper.\n\nAlignment stress-testing at Anthropic\n-------------------------------------\n\n**Daniel Filan:** Okay. There’ll definitely be a link to this in the description. Before I wrap up: is there anything about this work that I should have asked you about? Or just anything in general that I should have asked but have not gotten to?\n\n**Evan Hubinger:** That’s a good question. I’m not sure. I’d be happy to talk more about the broad picture of my team. I think that there’s a lot of interesting stuff about stress-testing in general and what our ideology and orientation is, to how does our work play into the overall picture. That might be interesting.\n\n**Daniel Filan:** Yeah. So you have the stress-testing team, you’ve said a little bit about its role in Anthropic, but how many people are there and what do you guys get up to?\n\n**Evan Hubinger:** Yeah. So, we’ve grown a lot. When [the “Sleeper Agents” paper](https://arxiv.org/abs/2401.05566) came out, at that point it was just me, [Carson Denison](https://scholar.google.com/citations?user=sN1AjtcAAAAJ&hl=en&oi=ao) and [Monte MacDiarmid](https://www.alignmentforum.org/users/monte-m), though we are now many more people than that. We’re eight-ish people. We’ll probably be even more than that pretty soon. And I’m continuing to hire relatively rapidly. We’re growing and doing a lot of investment into this.\n\nWhy is that? What is the overall pitch? Well, I was sort of talking about this: the first thing is there’s [this RSP strategy](https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf). And so, the idea is we want to evaluate model capabilities and understand: at what point are models capable enough that they could potentially pose some particular risk?\n\nAnd so a big thing that we’re thinking about right now is: we think currently models are ASL-2, which means we don’t think they pose a very large risk, but we think at some point soon potentially models could reach ASL-3, at which point they would have… We define ASL-3 as the point at which models could have capabilities, for example, that are relevant to a terrorist organization, that is able to do something relatively dangerous with the model they wouldn’t otherwise be able to do.\n\nAnd so, once you reach that point, now you have to be a lot more careful, right? So now you have to make sure that actually those capabilities wouldn’t be accessible, that you would have sufficient mitigations in place to make sure that it won’t actually cause some catastrophe. And so, we want to make sure that both (a) we are actually effectively evaluating the extent to which we are at ASL-2 versus ASL-3, and (b) we want to make sure that our mitigations at ASL-3 are actually going to be sufficient and that they’re actually sufficient to ensure that we really don’t believe that the model will be able to be used in that dangerous way once we get to that point.\n\nBut then, the even bigger question is: what happens at ASL-4, right? So, right now, we’re thinking about ASL-3, but at some point down the line we expect to reach ASL-4, which broadly is starting to get at models that are closer to human-level in lots of domains. And at that point we really don’t yet know what it would take to align a model in that situation. And so, we’re working on trying to make sure that, when we get to that point, we’ll be ready to test and evaluate all the best techniques that we have or, if we really can’t solve the problem, if we can’t come up with some way to address it, that we’re ready to provide evidence for what the harms and issues and the problems are so that we can show people why it’s dangerous.\n\n**Daniel Filan:** So you’ve got eight people on the team. That’s the high-level thing. I’m wondering: what’s the roadmap of what needs to be done to put you in a place to really effectively test this?\n\n**Evan Hubinger:** It’s a good question. I mean, I think that we’re looking at a lot of different things. We want to make sure that we’re able to have the different threat models covered, that we’re really able to build examples of all these different threat models so that we can test against them. And also we really want to make sure that we’re able to test all these different possible mitigations, that we have an understanding of: how would we stress-test a control case? How would we stress-test an alignment case? What are the ways in which you would actually make a case for why some set of mitigations that involves some amount of monitoring and control and alignment would actually be sufficient to address some particular threat model? And so we’re doing a lot of work to get there, but it’s definitely still early stages.\n\n**Daniel Filan:** So \\[in\\] what I should have asked, you said something about how the stress team team is thinking about stuff, was that right? Or, I don’t know. I forget what exactly you said, but is there anything else interesting to say about the stress-testing team that we should know?\n\n**Evan Hubinger:** I don’t know. I think I’ve tried to explain where we’re at and how we’re thinking about things, what our orientation is. I mean, I think it’s a really important role. I’m really glad that we’re doing this, that Anthropic is doing this. I think that we’ve already been quite involved in a lot of different things and trying to find places where we might be concerned about stuff and try to address those things. I think it’s been really good and I’m very excited about it. I think it’s really important.\n\n**Daniel Filan:** Maybe one question I have is: so you mentioned these ASLs, these AI safety levels, right? So, Anthropic has this responsible scaling plan that defines some AI safety levels, but my understanding is that there’s this notion that not all of them are actually defined and part of the job of Anthropic is to define further ones to understand.\n\n**Evan Hubinger:** We have not yet released the definition for ASL-4. So, one thing I will say: one of our commitments is once we reach ASL-3, we must have released the definition for ASL-4. So at the very least, by that point, you will see that definition from us. It might be released sooner. We’ve been doing a bunch of work on trying to clarify exactly what that is, but yeah. It’s not public yet.\n\n**Daniel Filan:** I’m wondering: in coming up with definitions of relevant things, like “what should ASL-4 mean?”, how much is that being done by the stress-testing team versus other people at different parts of Anthropic?\n\n**Evan Hubinger:** I mean, I think this is something we’ve been heavily involved in, but certainly we’re not the only people making the decision and involved in the process. It’s a large process. I mean, it’s something that is a really serious commitment for the organization. It’s something that affects all parts of Anthropic and it’s something that really does require a lot of cross-coordination and making sure everyone understands and is on board. It’s definitely a whole process. It’s something we have been heavily involved in, but lots of other people have been involved in as well.\n\n**Daniel Filan:** So, what does the stress-testing team bring to shaping that definition?\n\n**Evan Hubinger:** Well, I mean, we’re supposed to be the red team. We want to poke at it. Our goal is to look at what we’re doing and ask the question of, “Well, is this actually sufficient? Are we really confident in this? And what are ways in which this could go wrong?” And so, that’s what we’ve been trying to bring.\n\n**Daniel Filan:** Okay. So, stress-testing proposed safety mitigations for ASL-4 models, roughly?\n\n**Evan Hubinger:** Well, no, also just, what are the levels? For example, if you’re setting a bunch of evaluations, you’re like, “These are the threat models we’re going to evaluate for”: is that sufficient? What if there are other threat models that you’re not including that you really should be evaluating for because those are really important ones? And so, that’s something we’ve also been thinking about.\n\nFollowing Evan’s work\n---------------------\n\n**Daniel Filan:** So, sort of related to that topic: if people are interested in the stress-testing team’s work or your research and they want to hear more, they want to follow it, how should they do so?\n\n**Evan Hubinger:** A lot of this is on [the Anthropic blog](https://www.anthropic.com/news), so I would definitely recommend taking a look at that. We also, cross-post a lot of this to [LessWrong](https://www.lesswrong.com/) and [the Alignment Forum](https://www.alignmentforum.org/). I think also, in addition to following the research, if people are excited about this stuff and want to work on it, I mentioned my team is growing a lot. We’re really excited about this effort and so I think you should consider applying to work for me also, which is very easy to do. You can just go to [Anthropic.com/careers](https://www.anthropic.com/careers) and mention that you’re interested in alignment stress-testing.\n\n**Daniel Filan:** Awesome. So, yeah. [Anthropic blog](https://www.anthropic.com/news), [LessWrong](https://www.lesswrong.com/), [Alignment Forum](https://www.alignmentforum.org/) and [Anthropic.com/careers](https://www.anthropic.com/careers), mention stress-testing. Well, it’s been great having you on. It’s been lovely talking. Thank you for being here.\n\n**Evan Hubinger:** Thank you so much for having me.\n\n**Daniel Filan:** Special thanks to Misha Gurevich, also known as Drethelin. This episode is edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. This episode was recorded at [Constellation](https://www.constellation.org/), an AI safety research center in Berkeley, California. Financial support for the episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read transcripts, you can visit [axrp.net](https://axrp.net/). You can also become a patron at [patreon.com/axrpodcast](https://patreon.com/axrpodcast) or give a one-off donation at [ko-fi.com/axrpodcast](https://ko-fi.com/axrpodcast). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nThe ‘model organisms of misalignment’ line of research creates AI models that exhibit various types of misalignment, and studies them to try to understand how the misalignment occurs and whether it can be somehow removed. In this episode, Evan Hubinger talks about two papers he’s worked on at Anthropic under this agenda: “Sleeper Agents” and “Sycophancy to Subterfuge”.\n\nTopics we discuss:\n\n * Model organisms and stress-testing\n * Sleeper Agents\n * Do ‘sleeper agents’ properly model deceptive alignment?\n * Surprising results in “Sleeper Agents”\n * Sycophancy to Subterfuge\n * How models generalize from sycophancy to subterfuge\n * Is the reward editing task valid?\n * Training away sycophancy and subterfuge\n * Model organisms, AI control, and evaluations\n * Other model organisms research\n * Alignment stress-testing at Anthropic\n * Following Evan’s work\n\nDaniel Filan: Hello, everybody. In this episode, I’ll be speaking with Evan Hubinger. Evan is a research scientist at Anthropic, where he leads the alignment stress-testing team. Previously, he was a research fellow at MIRI, where he worked on theoretical alignment research, including the paper “Risks from Learned Optimization”. Links to what we’re discussing are in the description, and you can read a transcript at axrp.net. You can also support the podcast at patreon.com/axrpodcast. Well, Evan, welcome to the podcast.\n\nEvan Hubinger: Thank you for having me, Daniel.\n\n\nModel organisms and stress-testing\nDaniel Filan: I guess I want to talk about these two papers. One is “Sleeper Agents”, the other is “Sycophancy to Subterfuge”. But before I go into the details of those, I sort of see them as being part of a broader thread. Do you think I’m right to see it that way? And if so, what is this broader thread?\n\nEvan Hubinger: Yes, I think it is supposed to be part of a broader thread, maybe even multiple broader threads. I guess the first thing I would start with is: we sort of wrote this agenda earlier - the ide",
      "wordCount": 20181
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "KEAWfxwjitNJFrC68",
        "name": "Deceptive Alignment",
        "slug": "deceptive-alignment"
      },
      {
        "_id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "slug": "inner-alignment"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "Dw5Z6wtTgk4Fikz9f",
        "name": "Inner Alignment",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "Alignment Theory"
    ],
    "extraction_source": {
      "tag_id": "Dw5Z6wtTgk4Fikz9f",
      "tag_name": "Inner Alignment",
      "research_agenda": "Alignment Theory"
    }
  },
  {
    "_id": "7399F7TjTMreDBcmN",
    "title": "AXRP Episode 38.2 - Jesse Hoogland on Singular Learning Theory",
    "slug": "axrp-episode-38-2-jesse-hoogland-on-singular-learning-theory",
    "url": null,
    "baseScore": 34,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-11-27T06:30:03.821Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/P528XdjWvZg)\n\nYou may have heard of singular learning theory, and its “local learning coefficient”, or LLC - but have you heard of the refined LLC? In this episode, I chat with Jesse Hoogland about his work on SLT, and using the refined LLC to find a new circuit in language models.\n\nTopics we discuss:\n\n*   [About Jesse](#about-jesse)\n*   [The Alignment Workshop](#aw)\n*   [About Timaeus](#about-timaeus)\n*   [SLT that isn’t developmental interpretability](#slt-not-devinterp)\n*   [The refined local learning coefficient](#refined-llc)\n*   [Finding the multigram circuit](#finding-the-multigram-circuit)\n\n**Daniel Filan** (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area [Alignment Workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at [axrp.net](https://axrp.net/). And as usual, if you want to support the podcast, you can do so at [patreon.com/axrpodcast](https://www.patreon.com/axrpodcast). Well, let’s continue to the interview. All right, Jesse, welcome. Thanks for being interviewed.\n\n**Jesse Hoogland** (00:32): Thanks for interviewing me.\n\nAbout Jesse\n-----------\n\n**Daniel Filan** (00:34): Yeah. So for people who don’t know, could you say a little bit about yourself, who you are?\n\n**Jesse Hoogland** (00:38): So I’m the Executive Director of [Timaeus](https://timaeus.co/). We’re a research organization working on applications of singular learning theory. I think we’ll get into more of the details, but concretely, SLT is a theory of Bayesian statistics that answers some questions around generalization - why neural networks are able to generalize as well as they do - and therefore paves the road to applications for evals. Can you understand when your evaluation benchmark is actually going to be predictive about behavior downstream in deployment? There are applications for interpretability: questions like “can you detect that the model is planning to execute a treacherous turn?” It’s the kind of question I hope to answer someday with a better-developed theory of SLT and associated tools for probing these questions.\n\n**Daniel Filan** (01:26): Gotcha. And if people are interested in that, we had [a previous episode](https://axrp.net/episode/2024/05/07/episode-31-singular-learning-theory-dan-murfet.html) with [Dan Murfet](https://x.com/danielmurfet) on, I think it’s just called Singular Learning Theory, that you all can listen to.\n\n**Jesse Hoogland** (01:37): So Daniel Murfet… I mean, he’s the one who really put forth this agenda of applying SLT to alignment, and we’re working very closely together with Dan to make that a reality.\n\nThe Aligment Workshop\n---------------------\n\n**Daniel Filan** (01:49): Great. So before we dive into the SLT, we’re here at this alignment workshop. It’s run by FAR.AI. It’s the start of day two. How are you finding it?\n\n**Jesse Hoogland** (02:00): It’s been great. So my highlight yesterday was in one of these small discussion panels: I’m sitting here, [Anca Dragan](http://people.eecs.berkeley.edu/~anca/) is sitting here, [Yoshua Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio)’s sitting there, and [Adrià](https://agarri.ga/) \\[Garriga-Alonso\\] has pulled up his computer with a Shoggoth meme on it, and Anca is explaining the Shoggoth to Yoshua in extreme detail. I managed to capture a picture, so hopefully you can disseminate that somewhat.\n\nAbout Timaeus\n-------------\n\n**Daniel Filan** (02:31): So you’re Executive Director of Timaeus. What is Timaeus up to these days? What are you doing?\n\n**Jesse Hoogland** (02:36): So Timaeus does two main things. Primarily, we’re a research organization. In particular, the SLT for Alignment agenda is sort of split in two parts. So there’s the more fundamental side, theoretically heavy, and that’s work that Daniel Murfet and his students are really focusing on. And then Timaeus is gearing more towards applications. So taking these tools, scaling them up to frontier models as quickly as possible, trying to make progress that’s meaningful to safety as quickly as possible. And so that’s the work we’re doing: so, experiments, training lots of models, running learning coefficient sweeps. That’s one of these techniques we have from SLT that we use all over the place. And research is also maybe the primary thing that I’m doing. In addition to the research, we’re also making sure to do outreach to make sure that people are familiar with the work we’re doing, so that at some future date, we can hand off techniques to the labs, to policy makers, to other people who need to make decisions with what the current state of learning theory can do.\n\n**Daniel Filan** (03:45): Okay. And what does that outreach look like?\n\n**Jesse Hoogland** (03:48): That outreach looks like me giving a lot of talks. Some of it looks like personalized outreach, thinking about other people’s research agendas, questions they might have that SLT could contribute to, and thinking about how to answer that, and then going up to those people and talking with them about it, and that’s just something you can do.\n\n**Daniel Filan** (04:09): Yeah. So one thing that comes to my mind is: I don’t know whether it was just last year or the year before, but you ran this singular learning theory summer school-type thing, right? Which I found pretty fun, and I didn’t even go to the fun bit, I just attended the lectures. I’m wondering, is there more of that in the future or is that not quite the focus?\n\n**Jesse Hoogland** (04:36): So that’s another part of the outreach strategy: running events. So there was the [SLT for Alignment workshop](https://timaeus.co/events/2023-q2-berkeley-conference). There was a developmental interpretability workshop. It’s one of these sub-fields of the SLT for Alignment research agenda. Recently there was [ILIAD](https://www.iliadconference.com/): it was a conference not just of SLT, but also computational mechanics, agent foundations, and a bunch of more theoretical approaches in AI safety that came together. There’ll be more of that. We’re contributing to the first [Australian AI Safety Forum](https://aisafetyforum.au/). That’ll be running in only two weeks, the start of November. That’s co-organized with a bunch of other organizations: Gradient Institute, Sydney Knowledge Hub, and other organizations affiliated with the University of Sydney. So these kinds of collaborations, organizing events, I think is a key part of this.\n\nSLT that isn’t developmental interpretability\n---------------------------------------------\n\n**Daniel Filan** (05:25): Cool. I want to dial into a thing you just said there. Maybe I misinterpreted it, but you said that developmental interpretability is a subset of SLT for Alignment, and I had the impression that they were almost synonymous. So what do you see as… what’s the complement?\n\n**Jesse Hoogland** (05:52): So I think the core idea of SLT is something like: the geometry of the loss landscape is key to understanding neural networks. There’s just a lot of information embedded in this geometry that you can try to probe, and that tells you then something about the internal structure of models. So that’s a starting point for developing interpretability tools, and maybe also evaluation procedures. Now, here are two different kinds of questions you can ask about that geometry. One is, I can ask: over the course of training, how does the local geometry around my model change as I proceed? Are there qualitative changes in this local geometry that correlate with changes in the algorithm I’m implementing? And some work that we’ve done so far shows that this seems to work. That’s what I would call developmental interpretability: so, applying this lens of SLT to understanding the development process of neural networks. In particular, because SLT makes some pretty concrete predictions about the kinds of allowed changes you’ll see over the course of development, at least in a Bayesian setting. And a lot of the work is in applying and seeing how much does this transfer, do these predictions from the Bayesian setting transfer over to the SGD setting? So developmental interpretability \\[is\\] ta\\[king\\] these tools from SLT, and run\\[ning\\] them over the course of development.\n\n(07:20): But another question you can ask is just: what is this local geometry like for a given snapshot? And maybe a question is: how does this geometry, which depends on the data you’re evaluating your model on, change as you change that distribution? If I change the distribution I’m evaluating my model on and deploying my model on, do I see a qualitative change in the structure around my choice of weights? And we expect that that correlates with something like a change in the mode of computation on that data. So it’s a sort of signal you can start to use to probe out-of-distribution generalization. Can you understand qualitatively the ways in which different structures in the model actually generalize? And that doesn’t necessarily require taking this developmental approach. I think in many cases they are synonymous, but there are questions you can ask that don’t really fit the developmental frame.\n\n**Daniel Filan** (08:18): Okay. So if I think about work by Timaeus that I’m familiar with, it seems like it mostly fits within the developmental interpretability side. Are there things on this “changing the dataset and seeing how the geometry changes”? Is there work there that I could read?\n\n**Jesse Hoogland** (08:34): Work that’s coming at some point or that I should say we’re starting on.\n\n**Daniel Filan** (08:39): All right.\n\n**Jesse Hoogland** (08:40): So one thing we’ve noticed is that SLT has this measure of model complexity called “local learning coefficient”, which you can see as effective dimensionality, although in reality it’s a richer metric than just effective dimensionality. This is something you can measure, and it depends on the data set you evaluate it on. So what we’ve noticed is: it seems to correlate with something like memorization. So in [very recent work](https://arxiv.org/abs/2410.02984), we looked at a learning coefficient that tells you about how much complexity is there in an attention head? What you find is that the learning coefficient correlates with the number of different N-grams or multigrams that a head has learned.\n\n**Daniel Filan** (09:21): Okay. Where the learning coefficient is higher if you’ve memorized more things?\n\n**Jesse Hoogland** (09:25): If you’ve memorized more things. There are other things like this: work by [Nina Panickssery](https://ninapanickssery.com/) and [Dmitry Vaintrob](https://math.berkeley.edu/~vaintrob/) looked at a grokking setting, and they looked at the size of the task that you’re trying to memorize and how that changes the learning coefficient. There’s a correlation there that is very clean. There’s work on sparse parity tasks where you’re looking at the number of different sparse parity tasks that your model is trying to memorize, and there’s a correlation there. So there’s starting to be, across a wide range of different things, some idea of we can measure how much a model is memorizing on the data set.\n\n**Daniel Filan** (10:03): So the work by Nina and Dmitry and the sparse parity task: what are the names of those papers?\n\n**Jesse Hoogland** (10:12): That’s a [LessWrong post](https://www.lesswrong.com/posts/4v3hMuKfsGatLXPgt/investigating-the-learning-coefficient-of-modular-addition), the work by Nina and Dmitry, and the sparse parity stuff is coming out, so that’s not been published.\n\n**Daniel Filan** (10:20): Okay. Just so that people can look it up, so that I can look it up, what’s the name of the Nina and Dmitry post?\n\n**Jesse Hoogland** (10:28): “Exploration of the Learning Coefficient for Melbourne Hackathon”? There’s Hackathon in the name.\n\n**Daniel Filan** (10:36): That’s hopefully enough things to Google it now. Gotcha.\n\nThe refined local learning coefficient\n--------------------------------------\n\n**Daniel Filan** (10:41): Cool. So I now want pivot to work that, as of recording, you’ve recently put out there on the “restricted learning coefficient”. Do I have that name right?\n\n**Jesse Hoogland** (10:51): Yeah, “refined”, “restricted”. We’ve gone through some name changes.\n\n**Daniel Filan** (10:56): “Refined”, sorry.\n\n**Jesse Hoogland** (10:57): The whole naming is kind of unfortunate, but it goes back, and it predates us, but it’s… we’ll call it the “refined LLC”.\n\n**Daniel Filan** (11:05): Okay. So what is the refined LLC?\n\n**Jesse Hoogland** (11:07): So we have this measure of model complexity that tells you how much structure is there in the model as a whole, and how does that depend on the data. So two immediate refinements you can come up with on top of this learning coefficient from the theory. One of them is you change the data set, so you don’t evaluate on a pre-training data set but some other data set.\n\n(11:29): The other thing you can do to refine it is to freeze some weights and measure the learning coefficient of a subset of weights. And so that’s what I described with these attention heads. You can measure now complexity, the amount of structure in a component of the model. And so that was the starting point for this paper. We looked at these refinements and applied them to very simple, two-layer attention-only language transformers.\n\n(11:52): And so what you find is, if you plot this over the course of training, different kinds of heads have distinct developmental signatures. So the induction heads look like one thing. The heads memorizing n-grams and skip n-grams, which we call “multigram heads”, memorize one thing. Previous token heads look like one thing. There’s a current token head that looks like one thing, and you can automatically cluster them based on these developmental signatures. And I think you can do that using a bunch of other techniques, but that’s at least a starting point.\n\n(12:21): That’s one observation. What you now notice is that if you incorporate this data refinement, you can start to say something about what different heads are specialized to. So not just that different heads are different, but the ways in which they’re different, what kinds of data sets they might be more important on. And so induction, when you evaluate it on a code-heavy data set, jumps up. So now the induction heads relatively experienced an increase in complexity, and moreover, they split apart. So the induction heads previously look like they’re sort of doing the same thing. Evaluate it on code, and now there’s a separation where one of them seems to be more important for code. Under additional analysis, you find that indeed this thing seems to be specialized to syntax, tokens, punctuation, that kind of thing.\n\n(13:09): So you’ve got tools that are starting to probe, when are our two different structures actually different? What is it specialized to? And this feeds into the discovery of a new kind of circuit, which we call the “multigram circuit”.\n\n(13:23): These two-layer attention-only transformers are inspired by [work that Anthropic had done](https://transformer-circuits.pub/2021/framework/index.html) when they discovered this induction circuit mechanism. What we find is in these same models, models develop something sophisticated, another kind of circuit - so coordination between two layers - that seems to be necessary for things like nested parenthesis matching. You open a bracket, you open a quotation mark, you have to close the quotation mark before you close the parenthesis. Refined learning coefficients are part of a toolkit that we’re developing. It’s not just tools derived from SLT, but \\[also others\\] that are showing the formation of this multigram circuit, that help us discover a new circuit that appears to be every bit as fundamental as the induction circuit.\n\nFinding the multigram circuit\n-----------------------------\n\n**Daniel Filan** (14:05): Yeah. Can you tell me what actually happened? What tool you applied to what thing, what metric you noticed, and what’s the story there and how the refined LLC played into it, if it was the refined LLC?\n\n**Jesse Hoogland** (14:20): Let me give you the high-level story. We still don’t fully understand this model, so there’s still work to be done. But what seems to happen first is heads individually memorize n-grams and skip n-grams, become multigram heads, and this seems to go faster in the first layer than in the second layer.\n\n(14:39): What you notice is that the learning coefficient, a particular kind of refined learning coefficient, where you’re measuring how similar performance is to some baseline one-layer model, which you treat as a reference for what n-gram behavior is like… You measure performance relative to this baseline. You notice that the resulting learning coefficient, refined learning coefficient, peaks after the second stage of development.\n\n(15:09): At that point, this starts to decrease for the first layer, but it’s still increasing for the second layer. So heuristically, the model seems to be losing information about multigram prediction in the first layer. You can verify this in a handful of cases where you actually notice that there’s a migration, for example, of a multigram from layer one to layer two.\n\n(15:31): But you also notice that now suddenly the tokens that different heads seem to be involved in are changing, and now there’s coordination. So one of the layer one heads seems to be very involved in the same kinds of tokens than a different layer two head is doing. You can actually verify, by certain kinds of ablations and path ablations where you only ablate the outputs of these heads into the input of this head in the second layer, that it needs coordination. So the model’s actually passing information forward now to second layer heads in order to predict nested pattern matching, where that wasn’t the case before.\n\n(16:10): So we’re still using ablations to verify that there’s coordination going on here. We’re looking at certain kinds of composition scores to verify that layer one is feeding into layer two. They’re reading and writing from the same subspace. There’s other analyses where we’re looking at: if you ablate this head, which tokens are maximally affected across the data set? And actually looking at a bunch of those examples. So all of that analysis is going into identifying multigram circuits, but this observation that information might be migrating from layer one to layer two is something that can set you off, and I think that’s something that we’ll observe more generally as we scale up to larger models, these kinds of signals.\n\n**Daniel Filan** (16:54): Gotcha. So if people are interested in reading that paper, what was the name of that paper again?\n\n**Jesse Hoogland** (17:04): [Differentiation and Specialization in Attention Heads with the Refined Local Learning Coefficient](https://arxiv.org/abs/2410.02984).\n\n**Daniel Filan** (17:09): Great. Well, thanks very much for coming here and chatting with me.\n\n**Jesse Hoogland** (17:13): Thank you, Daniel. See you soon.\n\n**Daniel Filan** (17:15): See you around.\n\n(17:15): This episode was edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode, or to learn how to support the podcast yourself, you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me, at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nYou may have heard of singular learning theory, and its “local learning coefficient”, or LLC - but have you heard of the refined LLC? In this episode, I chat with Jesse Hoogland about his work on SLT, and using the refined LLC to find a new circuit in language models.\n\nTopics we discuss:\n\n * About Jesse\n * The Alignment Workshop\n * About Timaeus\n * SLT that isn’t developmental interpretability\n * The refined local learning coefficient\n * Finding the multigram circuit\n\nDaniel Filan (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area Alignment Workshop, which is run by FAR.AI. Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net. And as usual, if you want to support the podcast, you can do so at patreon.com/axrpodcast. Well, let’s continue to the interview. All right, Jesse, welcome. Thanks for being interviewed.\n\nJesse Hoogland (00:32): Thanks for interviewing me.\n\n\nAbout Jesse\nDaniel Filan (00:34): Yeah. So for people who don’t know, could you say a little bit about yourself, who you are?\n\nJesse Hoogland (00:38): So I’m the Executive Director of Timaeus. We’re a research organization working on applications of singular learning theory. I think we’ll get into more of the details, but concretely, SLT is a theory of Bayesian statistics that answers some questions around generalization - why neural networks are able to generalize as well as they do - and therefore paves the road to applications for evals. Can you understand when your evaluation benchmark is actually going to be predictive about behavior downstream in deployment? There are applications for interpretability: questions like “can you detect that the model is planning to execute a treacherous turn?” It’s the kind of question I hope to answer someday with a better-developed theory of SLT and associated tools for probing these questions.\n\nDaniel Filan (01:26): Gotcha. And if p",
      "wordCount": 3083
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "HRQqcs6CdPv5YaStD",
        "name": "Singular Learning Theory",
        "slug": "singular-learning-theory"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fuJDL3cuMhHsC5eNk",
    "title": "AXRP Episode 38.1 - Alan Chan on Agent Infrastructure",
    "slug": "axrp-episode-38-1-alan-chan-on-agent-infrastructure",
    "url": null,
    "baseScore": 12,
    "voteCount": 2,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-11-16T23:30:09.098Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/1OQyGH-IlM4)\n\nRoad lines, street lights, and licence plates are examples of infrastructure used to ensure that roads operate smoothly. In this episode, Alan Chan talks about using similar interventions to help avoid bad outcomes from the deployment of AI agents.\n\nTopics we discuss:\n\n*   [How the Alignment Workshop is](#how-aw-is)\n*   [Agent infrastructure](#agent-infrastructure)\n*   [Why agent infrastructure](#why-agent-infrastructure)\n*   [A trichotomy of agent infrastructure](#trichotomy)\n*   [Agent IDs](#agent-ids)\n*   [Agent channels](#agent-channels)\n*   [Relation to AI control](#relation-to-control)\n\n**Daniel Filan** (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area [Alignment Workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net, and as usual, if you want to support the podcast, you can do so at [patreon.com/axrpodcast](https://www.patreon.com/axrpodcast). Well, let’s continue to the interview.\n\n(00:29): So, hello, Alan Chan.\n\n**Alan Chan** (00:32): Hey.\n\n**Daniel Filan** (00:33): For people who don’t know who you are, can you say a little bit about yourself?\n\n**Alan Chan** (00:36): Yeah. I’m a research fellow at the [Centre for the Governance of AI](https://www.governance.ai/). I am also a Ph.D. student at [Mila](https://mila.quebec/en) in Quebec, but I’m about to defend in the next few months. My background is in math and machine learning, but starting two/one and a half years ago, I started doing a lot more AI governance stuff basically and joined GovAI where I’ve been for the past year now.\n\nHow the Alignment Workshop is\n-----------------------------\n\n**Daniel Filan** (01:02): Okay. Cool. And at the moment, we’re at this Alignment Workshop being run by FAR.AI. I’m wondering: how are you finding it?\n\n**Alan Chan** (01:08): Yeah. It’s pretty interesting. I mean, I think the main barrier I have to get over internally in my head is: I really would just like to chill and not really do much or get other work done. But I’m finding it is actually very helpful to talk to people about the work that they’re doing because it actually does inform my own work. So it’s way better than I expected.\n\nAgent infrastructure\n--------------------\n\n**Daniel Filan** (01:31): Let’s talk a little bit about the work you’re doing. So I understand that you’ve been thinking about agent infrastructure. Can you tell us a little bit about what’s going on there?\n\n**Alan Chan** (01:41): Yeah, yeah. So maybe let’s back up and think about agents in general. So what’s the story here? The story here is something like: we’re building systems that can not only tell us what to do in our lives or give us nice advice, we’re also building systems that can actually act in the world. So [Claude computer use](https://www.anthropic.com/news/developing-computer-use), it can interact with your computer, maybe do transactions for you, maybe send emails for you. And as AI systems get better and better, the question is: what does this world look like where we have agents interacting and mediating a bunch of our digital interactions? What sort of interventions do we need, to handle what kinds of risks that could come up? So that’s a question with agents in general. A lot of my work in the past year has focused on thinking about: what interventions could we try to prepare in advance that could be relatively robust to a range of threat models?\n\n(02:41): And when I say threat models, I mean, maybe right now we’re worried about agents that could spam call people. That’s just really annoying. It also disproportionately affects people who are older or not as well-educated. As agents get more capable, maybe we become more worried about agents carrying out cyber attacks or being able to act autonomously in a biolab and do stuff like that.\n\n**Daniel Filan** (03:05): Okay. And just to help me understand, so when you say threat model, you mean a bad thing that they do…?\n\n**Alan Chan** (03:08): Yeah. Essentially a bad thing, yes.\n\n**Daniel Filan** (03:10): …rather than a bad capability or a bad way a training run could happen?\n\n**Alan Chan** (03:15): Yes.\n\n**Daniel Filan** (03:15): Okay. Gotcha.\n\n**Alan Chan** (03:17): So one question for me… Because I guess I have a lot of uncertainty over how fast capabilities will improve and whether things will plateau. So one way in the past year I’ve had to deal with this uncertainty is trying to think about what sorts of things might be palatable to people across a very wide range of beliefs, particularly drawing upon things that have worked in other kinds of domains.\n\n**Daniel Filan** (03:43): And I guess your answer is agent infrastructure.\n\n**Alan Chan** (03:46): Yeah, I think this is one answer. So-\n\n**Daniel Filan** (03:48): So what is that?\n\n**Alan Chan** (03:50): Infrastructure, writ large, is just any large-scale technical system that enables particular kinds of functions.\n\n(03:58): So if we think about the internet, what is internet infrastructure? It’s like IP protocols, it’s like HTTPS, it’s like public key cryptography that allows you to interact with people in a secure way online. So that’s infrastructure. Thinking about agents, what are some examples of agent infrastructure? Some of the stuff that I’ve been working on in the past year includes: well okay, your agent is interacting with my agent. It’d be really good if I knew who the agent was and who it belonged to. So maybe you want some sort of ID, an ID system as infrastructure. Or maybe one other example is: okay, I interacted with your agent, I sent your agent some sort of message. It turned out this message was actually a worm or something and authorities want to be able to trace that worm back to somebody and somehow take it off the internet, or something like this. So you need systems to be able to do this kind of thing as well. I guess a moniker for it is an ‘off switch’, for that particular example.\n\nWhy agent infrastructure\n------------------------\n\n**Daniel Filan** (04:57): So a question I have in the back of my mind is: why is this more acceptable to, or why does this make sense for a wider range of beliefs than, I don’t know, just interventions on the agents itself? And it sounds like it’s because a lot of these things are just tracking what agents are doing, having a better sense of “Okay. This agent was at this place doing this thing and now if a bad thing happened, we can deal with it.” Is that roughly the right idea or is there something else going on?\n\n**Alan Chan** (05:29): So I’m not claiming that it’s more robust to sources of risk than interventions on the agents themselves. There are two things here. The first thing is that I think interventions on agents themselves can itself be robust to the type of risk. So for example, alignment or something. I think alignment has become a pretty widely accepted meme nowadays just because people think, oh man, it is, number one, very useful to not have AI systems that want to kill everybody, but it is also actually very useful if you can align a system well enough such that the developer can prevent somebody from using an AI system to generate spam or something like this.\n\n(06:13): So I’m not making any claim about the robustness of interventions on the agent themself. I think my claim is more that interventions on the agent themself seem useful and probably necessary. It seems good if we also thought about additional interventions just in case those don’t work out, in a sort of [Swiss cheese-type model](https://en.wikipedia.org/wiki/Swiss_cheese_model).\n\n(06:33): So why might interventions on the agent itself not work out? Well maybe people just don’t adopt them because of competitive dynamics-type reasons. Maybe they don’t actually generalize that well, and we don’t understand generalization that well, so it’d be good to have things in the environment that can stop the agent, even if the agent itself is behaving poorly.\n\n**Daniel Filan** (06:54): Gotcha. So do I understand the perspective right that it’s a combination of, “Hey. Here’s this domain of things we could do that have been previously not thought about,” and also within the domain, “Okay. Let’s maybe think about things that are more robust to different threat models.”\n\n**Alan Chan** (07:13): Yes. So it’s sort of like: let’s do this whole society approach (or something like this) to handling AI risk. And if you push me I guess I think the heuristic I’m operating under is something like, “Well it seems in a lot of other domains in life, we do things on the agent itself in addition to things outside.” So if you think about traffic safety for instance, it is very important to train very safe drivers, but we also recognize that drivers can’t act safely perfectly all of the time. So that’s why we, in some jurisdictions, have stuff like roundabouts because they’re actually safer than regular intersections.\n\nA trichotomy of agent infrastructure\n------------------------------------\n\n**Daniel Filan** (07:54): Yeah. So I guess the traffic analogy is interesting because I can think of two types of things that are done in traffic to improve safety. So one kind of thing is just making the situation more legible to drivers, so making sure that the lines on the road are bright, making sure there’s not one of these weird kinds of corners where you can’t see the cars coming ahead of you. So that’s an intervention to make the situation more legible to the relevant agents. There’s a second kind of intervention, which is like “literally slow down the agents.”\n\n**Alan Chan** (08:37): Yeah, yeah, yeah.\n\n**Daniel Filan** (08:39): Have physical things that mean that people can’t do their stuff in a dangerous way, but can do stuff in a safe way. And a third kind of infrastructure in the driving realm is basically identification, so that you can catch bad drivers and punish them - license plates being the major one of these, speeding cameras being another one of these. So I guess this is a proposed trichotomy of things you could do with agent infrastructure. I’m wondering, what do you think of the trichotomy and which ones do you think make more or less sense in the AI realm?\n\n**Alan Chan** (09:15): So just so that I have things clear, the third one was identification-type stuff, roughly?\n\n**Daniel Filan** (09:21): Yeah: make the relevant domain more legible to the relevant agents, literally somehow physically just prevent the relevant agents from doing bad stuff, and identification, leaving traces so that if a thing does do a bad thing, then you can somehow punish or deal with the behavior later.\n\n**Alan Chan** (09:44): I see. It’s a very interesting question. I like this trichotomy. So the third thing I think works even if your agents are bad, because then other agents can refuse to interact with your bad agent. The first thing I think works if you have agents that are in some sense law-abiding or can respond to positive incentives. What was your traffic example again?\n\n**Daniel Filan** (10:14): The traffic example was like literally making sure the lines on the road are bright, making sure that they’re not faded out-\n\n**Alan Chan** (10:21): Right, right.\n\n**Daniel Filan** (10:22): Replacing the dingy old traffic lights.\n\n**Alan Chan** (10:25): I see. So this could be something like: supposing you had agents that were good at following instructions and/or the law, this would be something like, “Okay. Before every economic transaction with another agent, you just put in the context, ‘Here are the relevant economic laws,’ or something like this.”\n\n**Daniel Filan** (10:42): Or “here are the relevant economic facts that would let you know how to judiciously behave.”\n\n**Alan Chan** (10:49): Right. And “here are the legal obligations you have to your user as a fiduciary,” or something.\n\n**Daniel Filan** (10:54): Yeah. Or maybe it’s not even legal obligations, right? I’m sure there are things you could do with roads that help you drive more considerately. It doesn’t even have to be legal, just things you can do to make… Like sometimes I behave inappropriately because I have a misread of the situation. That happens to me sometimes. Maybe sometimes it happens to well-intentioned AI agents.\n\n**Alan Chan** (11:21): Yeah.\n\n**Daniel Filan** (11:22): And making various facts more legible. What could such facts be? I don’t know. How scammy is some other counterparty? If I have my own AI assistant and it’s negotiating with some other AI assistant, you could imagine some infrastructure that’s like, “Make sure my AI assistant knows that that other negotiating AI has gotten tons of bad reviews.”\n\n**Alan Chan** (11:49): Right.\n\n**Daniel Filan** (11:49): Somehow that’s making the world more legible to my AI; it’s helping it be a better player and not accidentally send all my money to some scam.\n\n**Alan Chan** (11:59): Yeah. There is an interesting way in which these categories blend because you could imagine a law or instruction being something like, “Okay. Follow the relevant laws when you engage with the other agent, but if the other agent is doing something shady, please report it to some sort of agency,” or whatever. And maybe this feels a little scummy or something, talking about it in the abstract. But yeah. I could imagine some sort of system like this. What was the second part of the trichotomy again?\n\n**Daniel Filan** (12:31): So \\[there was\\] “make the world more legible to the agents.” There was “physically either make it impossible or make it difficult for the agents to behave unsafely.” So speed bumps would be one example of this on roads. Things that prevent you from going too fast or things that ensure that you look in the right direction. I’m not sure I have tons of examples of this. I don’t drive a car, so my lack of knowledge is showing right now. But speed bumps I’m aware of.\n\n**Alan Chan** (13:12): I haven’t driven a car in a very long time. I mean, I guess the direct analogy to speed bumps is rate limits, which we already do to prevent DDoS attacks, so maybe this is relevant across more domains. I guess another analogy is limiting the affordances of an agent. So I guess there’s some sort of ambiguity about whether it’s the developer doing this or whether the agent you’re interacting with is doing this, right? But there is a difference between, “Okay, I’m just going to give my agent access to my computer and my terminal and they’re going to have sudo access, like Claude computer use, let’s just lose everything,” versus, “Okay, we’re really only going to give your agent access to specific kinds of APIs. We’ve tested them very thoroughly. The functions are very well-defined.”\n\nAgent IDs\n---------\n\n**Daniel Filan** (13:59): Yeah. So I guess there’s just a whole host of things you could potentially do in the agent infrastructure space. I’m wondering: are there specific examples of things that you’re particularly keen on?\n\n**Alan Chan** (14:09): Yeah. I guess this is the thing I’m pretty confused about: what to prioritize. Some heuristics to think through are, one, what are more or less easy wins, especially things that you might be able to get in through existing efforts or existing legislation. So one example of this is IDs for agents. At the most basic level, just some sort of identifier, but you could imagine attaching more information to the identifier, like an agent or a system card or something like this, or history of incidents, or some sort of certification that we might develop in the future.\n\n(14:42): And the reason why I think this is a relatively easy win is because: firstly, there is just a general consensus I think about transparency around where AI systems are being used. So to the extent that exists, it seems very useful to tell a user or another party, “Oh, you are actually interacting with an agent and maybe this is the agent identifier so that you can tell OpenAI or something that this agent caused the problem if the thing went wrong.”\n\n(15:12): So there’s already the social consensus. The next thing is that I think it’s quite plausible in legislation that you could get this through. So in the [AI Act](https://en.wikipedia.org/wiki/Artificial_Intelligence_Act), I forget if it’s Article 50 or something, but there’s this section saying that providers of general-purpose AI systems have to make sure that when natural persons interact with those systems, they know that they’re interacting with general-purpose AI systems. And to me, a read is, “Let’s have some sort of identification for these kinds of systems.” So it seems like an easy win. I guess the question though is: even if it is an easy win, is it actually a useful thing to do?\n\n(15:54): Which gets to my second heuristic: let’s try to identify pieces of infrastructure that can be scaled up or down in… I’m going to call it intensity or severity, according to what evidence we have before us right now. So if you don’t actually think that agents are going to cause a bunch of problems, if you think it’s mostly going to be fine, I think it’s totally fair for you to say, “Okay. Maybe I’m fine with having an ID that has some identifier, but I really don’t want it to have any user-identifying information. I just really want it to be the basic ‘somebody knows that an agent is doing something’.”\n\n(16:31): But if we did get substantial evidence that these agents are just misgeneralizing all of the time and they’re finding ways of getting around our limitations and their affordances and getting a bunch of new tools, then maybe somebody is more likely to say: okay, we really, number one, have to make sure that some user at the end of the day is going to be accountable for spinning up this agent. And number two, we also want to attach information to this agent about the affordances that it has available so that other people can be like, “Why does this agent have so many tools? Maybe I’m going to be a little suspicious and not want to interact with it.”\n\n**Daniel Filan** (17:12): So that makes a lot of sense. Do you have examples of infrastructure that failed, or proposed things you could do that would fail that second test, where you couldn’t easily scale them up or down? Just to help me think about-\n\n**Alan Chan** (17:32): So this is a good prompt. I guess it depends on how you individuate infrastructure. So one example is: suppose you had some sort of ID system and you built some sort of kill switch so that some actor was able to just unilaterally kill an agent corresponding to a particular ID. It seems hard to get rid of that. If you force OpenAI and Anthropic to build it into their agents or something, it seems liable that the government might misuse it in some sort of way. So maybe what I’m alluding to here is that there are steps you can take and then there is in some sense a point of no return, or something like this.\n\nAgent channels\n--------------\n\n**Daniel Filan** (18:17): So identification for agents: it seems like that’s a thing that you’re something of a fan of, or at least interested in right now. Are there other things?\n\n**Alan Chan** (18:38): Yeah. So one thing is… I guess we call it “agent channels” or “agent highways” in the paper, and the idea here is: you probably want to incentivize some isolation of agent traffic from… So it depends on how you operationalize it. It could be from human traffic or traffic from other software systems in general. But the reason is that you might want to be able to shut down agent-specific channels and minimize the impacts of that on people going about their daily lives. So for example, some agent spreads a text worm over a channel. You want to be able to just shut that down and not have that text worm infect other people’s AI assistants.\n\n**Daniel Filan** (19:23): Gotcha. And by “channel” you mean the medium that the agent is using?\n\n**Alan Chan** (19:30): Yeah. There are a variety of ways of operationalizing this. The easiest way is, “Okay, let’s just have separate APIs for services.” So for Airbnb, there’s an agent API versus the regular API. And maybe eventually Airbnb realizes, “Oh wow. Actually most of our traffic is coming from agents, so let’s just shut down the regular API,” or something like this. That’s maybe the easiest way.\n\n(19:53): There of course are some feasibility problems and questions here. You could even go a level more intense and be like, “Should we have separate IP addresses for agents? Even separate internet hardware to have the agent stuff running, so that we can just go in and shut down those servers specifically and have the rest of the human internet running.”\n\n**Daniel Filan** (20:13): It potentially seems like a win-win. Suppose you’re really bullish on agents. You might want to be like, “Hey, I don’t want humans cluttering up my beautiful agent channel.”\n\n**Alan Chan** (20:21): Yeah. Totally, totally. I think there is definitely an argument you can make to people who do want to develop and deploy agents that this is also a win for them.\n\nRelation to AI control\n----------------------\n\n**Daniel Filan** (20:29): So one thing this is kind of reminding me of is [Redwood Research](https://www.redwoodresearch.org/)’s work on [AI control](https://www.alignmentforum.org/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled), right? Just making sure AIs aren’t able to pull any funny business. Whereas this seems like sort of an externalized version of control. I’m wondering: do you think that’s an apt comparison and what do you think the similarities and differences to that approach are?\n\n**Alan Chan** (20:50): Yeah. So I’m a huge fan. I will say though, I am not fully read-up on everything that they are doing at this particular-\n\n**Daniel Filan** (20:57): Fair enough. Fair enough.\n\n**Alan Chan** (20:58): -moment. Do you happen to be?\n\n**Daniel Filan** (21:01): No.\n\n**Alan Chan** (21:01): Okay.\n\n**Daniel Filan** (21:03): Sorry. I haven’t been productive enough at this workshop.\n\n**Alan Chan** (21:11): So I guess my understanding is something like: the setup is, we have some trusted models. We have this bigger model that’s potentially not trusted. We want to understand what we can do to either constrain what this bigger model can do or have some sort of assurance about what the bigger model’s behavior is. And I guess the connection… I could totally imagine, if you come up with some really good protocol for making sure that your smaller models can say something meaningful about the larger models, this thing could just be infrastructure. You just scale it up.\n\n**Daniel Filan** (21:44): You can imagine it being a decentralized protocol.\n\n**Alan Chan** (21:46): Everybody just downloads this or something. So how this could work is: I have my GPT-6 agent and it’s interacting with your GPT-7 agent, and just to make sure that I’m not getting fooled or something like this, I take an AI control protocol and I just use it.\n\n**Daniel Filan** (22:01): Right. And a lot of the AI control thought is just like, “We’re going to make sure we have the right internal API so that this model submits commands to this thing and this model gets to gate execution of those commands in this place,” and somehow it seems like it’s a similar flavor.\n\n(22:25): So you’ve mentioned this paper. If listeners are listening to this and they’re interested to read your thoughts about agent infrastructure stuff, where and when can they do that?\n\n**Alan Chan** (22:37): So hopefully this paper will be out soon. It’s been in the works for quite a while. I feel like I’ve started writing this paper in June-ish. The thinking has been… and it’s gone through a lot of revisions and still going through some feedback. Hopefully by the end of the month, hopefully by December I think you should be able just to go to my website or go on arXiv and find it. Otherwise, I do have some agent infrastructure stuff, but maybe not as cohesive as a vision, maybe more high-level stuff, already out.\n\n**Daniel Filan** (23:16): Okay. And where’s the stuff that’s already out?\n\n**Alan Chan** (23:20): You can check out [my Google Scholar](https://scholar.google.com/citations?user=lmQmYPgAAAAJ&hl=en&oi=ao). So “Visibility into AI Agents” was the most recent thing… Sorry, [“IDs for AI Systems”](https://arxiv.org/abs/2406.12137). So we actually try to sketch out how this ID system could work in that paper. And then [“Visibility into AI Agents”](https://arxiv.org/abs/2401.13138) is another paper that’s a bit more high-level, trying to sketch out what other stuff might we need to help governments and civil society just know what’s going on with agents.\n\n**Daniel Filan** (23:42): Great. Well thanks very much for chatting with me today.\n\n**Alan Chan** (23:45): Thank you. It was a pleasure.\n\n**Daniel Filan** (23:46): This episode was edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode, or to learn how to support the podcast yourself, you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me, at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nRoad lines, street lights, and licence plates are examples of infrastructure used to ensure that roads operate smoothly. In this episode, Alan Chan talks about using similar interventions to help avoid bad outcomes from the deployment of AI agents.\n\nTopics we discuss:\n\n * How the Alignment Workshop is\n * Agent infrastructure\n * Why agent infrastructure\n * A trichotomy of agent infrastructure\n * Agent IDs\n * Agent channels\n * Relation to AI control\n\nDaniel Filan (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area Alignment Workshop, which is run by FAR.AI. Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net, and as usual, if you want to support the podcast, you can do so at patreon.com/axrpodcast. Well, let’s continue to the interview.\n\n(00:29): So, hello, Alan Chan.\n\nAlan Chan (00:32): Hey.\n\nDaniel Filan (00:33): For people who don’t know who you are, can you say a little bit about yourself?\n\nAlan Chan (00:36): Yeah. I’m a research fellow at the Centre for the Governance of AI. I am also a Ph.D. student at Mila in Quebec, but I’m about to defend in the next few months. My background is in math and machine learning, but starting two/one and a half years ago, I started doing a lot more AI governance stuff basically and joined GovAI where I’ve been for the past year now.\n\n\nHow the Alignment Workshop is\nDaniel Filan (01:02): Okay. Cool. And at the moment, we’re at this Alignment Workshop being run by FAR.AI. I’m wondering: how are you finding it?\n\nAlan Chan (01:08): Yeah. It’s pretty interesting. I mean, I think the main barrier I have to get over internally in my head is: I really would just like to chill and not really do much or get other work done. But I’m finding it is actually very helpful to talk to people about the work that they’re doing because it actually does inform my own work. So it’s way better than I expected.\n\n\nAgent i",
      "wordCount": 4259
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "8zxQYyuwH6uG9hzMW",
    "title": "AXRP Episode 38.0 - Zhijing Jin on LLMs, Causality, and Multi-Agent Systems",
    "slug": "axrp-episode-38-0-zhijing-jin-on-llms-causality-and-multi",
    "url": null,
    "baseScore": 14,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-11-14T07:00:06.977Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/4K-lHz2_QGg)\n\nDo language models understand the causal structure of the world, or do they merely note correlations? And what happens when you build a big AI society out of them? In this brief episode, recorded at the Bay Area Alignment Workshop, I chat with Zhijing Jin about her research on these questions.\n\nTopics we discuss:\n\n*   [How the Alignment Workshop is](#how-aw-is)\n*   [How Zhijing got interested in causality and natural language processing](#zhijing-interested-causality-nlp)\n*   [Causality and alignment](#causality-and-alignment)\n*   [Causality without randomness](#causality-without-randomness)\n*   [Causal abstraction](#causal-abstraction)\n*   [Why LLM causal reasoning?](#why-llm-causal-reasoning)\n*   [Understanding LLM causal reasoning](#understanding-llm-causal-reasoning)\n*   [Multi-agent systems](#multi-agent-systems)\n\n**Daniel Filan** (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area [Alignment Workshop](https://www.alignment-workshop.com/), which is run by [FAR.AI](https://far.ai/). Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net, and as usual, if you want to support the podcast, you can do so at [patreon.com/axrpodcast](https://www.patreon.com/axrpodcast). Well, let’s continue to the interview.\n\n(00:28): Yeah, so Zhijing, thanks for coming on to the podcast.\n\n**Zhijing Jin** (00:34): Yeah, thanks for inviting me.\n\nHow the Alignment Workshop is\n-----------------------------\n\n**Daniel Filan** (00:35): So I guess, first of all, we’re here at this Alignment Workshop thing. How are you finding it?\n\n**Zhijing Jin** (00:39): Very excited. I just finished a session about AI governance and a bunch of one-on-ones, which is super interesting.\n\nHow Zhijing got interested in causality and natural language processing\n-----------------------------------------------------------------------\n\n**Daniel Filan** (00:47): Cool, cool. So as I understand it, your work, at least predominantly, is in natural language processing and causal inference. Does that seem right?\n\n**Zhijing Jin** (00:59): Yeah, yeah, yeah. That’s a topic from my PhD and I bring it in my new assistant professor role as well.\n\n**Daniel Filan** (01:07): Right. Oh yeah, I guess I forgot to say: could you tell us where you do your PhD and where you’re about to be?\n\n**Zhijing Jin** (01:14): Sure, so my PhD started in Europe, \\[at the\\] Max Planck Institute in Germany and ETH in Switzerland, so it’s a joint PhD program, and I’m graduating and taking the assistant professor role at the University of Toronto CS department next year.\n\n**Daniel Filan** (01:33): Very exciting. Cool. So I guess I’m aware that there’s a subset of people in AI who are interested in causal inference, and there’s a subset of people in AI who are interested in natural language processing, and if I understand correctly, when you started your PhD, it was a bit more niche. Everyone’s into language models now, but… So I’m wondering: how did you come to be interested in the intersection of these two things? Which it seems like when you were interested, neither of them were the main thing, so getting to both of them is kind of unusual.\n\n**Zhijing Jin** (02:08): Totally, totally. That’s such a great question. Yeah, I started in my undergrad having a strong interest in natural language processing and \\[it was\\] slightly different from at that time a bigger branch - linguistics and understanding different devices people use in language. I’m always more interested in the semantics, what meaning do they convey, what are they able to do and so on. And then that actually naturally connects… At first people were thinking about “how can we make these models more capable of what people expect them to do?” And then once they show a tendency to exceed certain behavior, then we start to worry about what might go wrong and how to really align \\[them\\] with what human society asks for. So actually it naturally develops into this, and I also thank a lot my undergraduate friend, [Cynthia Chen](https://www.xccyn.com/): she introduced me to all these goals of alignment and the [CHAI lab](https://humancompatible.ai/) that’s set up at Berkeley and so on.\n\nCausality and alignment\n-----------------------\n\n**Daniel Filan** (03:14): Gotcha. So I think a lot of people are interested in alignment, making sure that models don’t go haywire, but I think a lot of people, and a lot of our listeners, don’t necessarily see that as being super connected to causal inference or don’t focus on that aspect. So what do you see as the connection between alignment in general and causal stuff in particular?\n\n**Zhijing Jin** (03:41): Basically causal inference originated as an important device for us to know more about two things at first: how nature works, and then also how humans work. For example, in the last century, a famous causal inference problem is “does smoking really cause cancer?” And then the interesting statistical advancement people made is that: can we get a causal conclusion without forcing people to smoke? And then now we can totally shift all these tools to LLMs in that, can we understand what contributes to LLMs behav\\[ing\\] like this? So there are several ways causality can contribute here.\n\n(04:26): The first question that I just mentioned is an interpretability problem. We see an LLM demonstrating different types of behaviors, capabilities: can we interpret what circuit, what neurons lead to that? There are also people exploring what type of training procedures lead to them, and then another side is: do LLMs really know the consequence of their actions? I do believe that a lot of the safety work, especially with my recent interest in multi-agent LLMs, it is composed of a two-step process. First, knowing the consequences of your actions. If I do A, what will happen? If I do B, what will happen? A very thorough causal inference understanding. And then later on a moral module added on top of it, or a reward model. In that, what will A mean for other people, for the society, what will B mean? And then which consequence is more preferred? So it’s first knowing and then deciding.\n\n**Daniel Filan** (05:30): So one application of causal inference to language models is understanding what is causing language model behavior. Another is “do language models understand causality?” Did you say that there would be a third thing or did I just misremember that?\n\n**Zhijing Jin** (05:44): That’s mostly the two big threads. I’m also slightly interested in narrow AI. So basically, in the past, human scientists applied causal inference to understand, let’s say, what’s the effect of increasing minimum wage? Does it reduce the employment rate or does it not? That’s [the 2021 Nobel Prize in economics](https://www.nobelprize.org/prizes/economic-sciences/2021/press-release/). And then I’m actually also… so at Toronto we have this organization called [SRI](https://srinstitute.utoronto.ca/) \\[the Schwartz Reisman Institute\\], and that also focuses on how AI can be deployed to help a lot of social problems. That’s one of my interests as well.\n\nCausality without randomness\n----------------------------\n\n**Daniel Filan** (06:21): Gotcha. So one thing that strikes me about the first thing you mentioned, using causal inference to understand what things in AI caused various AIs’ behavior: it sounded like you were applying that to stuff in the model: “Oh, if this neuron fires that causes this thing to happen.” I’ve studied a little bit of causal inference, and from what I remember, it seems like it’s very dependent on there being some randomness. You have this probabilistic network and you check if things are correlated or not correlated, and that lets you draw these Bayesian DAGs and there’s your causality. This is my half-baked memories of it. But a lot of this breaks down if everything’s deterministic. If everything is deterministic, then everything is perfectly correlated in some sense. It’s hard to get stuff that you might want. So how can you apply causal techniques when you have this deterministic system?\n\n**Zhijing Jin** (07:24): That’s a great question. Also I guess the direct analogy was that in neuroscience, the subject is people, whereas when we move to models where the subject, we can intervene in any sense. So for neuroscience, the difference of correlation/causation is pretty clear, in that correlation means that you ask the subject to do a certain task and observe what part of the neurons are activated, and that’s correlated, whereas the causal sense of it is usually got from patients who had brain damage and so on. So actually ablating that part of the brain region, what will happen in their task completion? Moving this to LLMs, we also had a previous early stage of interpretability where people looked at the activation state and the model prediction, or tried to interpret how this model understands the nature of the word here, the syntax tree and so on. Whereas here in the - I guess it’s also popularly known as the mechanistic interpretability research - the causal notion is that we really intervene on the neurons and see what’s happening. That’s the first level in terms of…\n\n(08:44): I guess there are maybe three different levels, different types of causal inference that are applied on interpretability. The first one is directly ablating a certain neuron, set it to a certain value and then see what will happen or entries in that tangent matrix. The second type is basically the so-called “mediation analysis” where you basically control the neurons to be \\[one\\] of two states. One is maybe an intervene state, \\[another is a\\] control state, to single out which part really does the job. So all of the previous two \\[levels\\] are still neuron-level interpretation, whereas the third branch is this causal abstraction where you try to match a bunch of neurons to a certain function and understand at a macroscopic level.\n\n**Daniel Filan** (09:43): If people are just not very familiar with this literature, where’s it at? How good a causal understanding can we have of networks?\n\n**Zhijing Jin** (09:53): So I think it’s becoming more and more of a drive of interpretability. And we will have [a NeurIPS 2024 tutorial on causal LLMs](https://neurips.cc/virtual/2024/tutorial/99520) as well, where a big part will introduce this.\n\nCausal abstraction\n------------------\n\n**Daniel Filan** (10:07): Sure. So one thing that caught my ear was the causal abstraction stuff, abstracting beyond the neural level using causal techniques. Can you say a little bit more about the state of the art in that work, what we’ve learned?\n\n**Zhijing Jin** (10:28): Some earlier work usually hypothesized that: okay, there’s a computation graph that we believe is solving the problem, and then try to map different neurons’ function to the corresponding unit in that more abstracted computational graph.\n\n(10:47): Then I guess the recent SAE (sparse autoencoder) \\[work\\] also has a similar notion in that it’s trying to map this neuron space, which is much more highly dimensional \\[compared\\] to a relatively lower dimensional space, and this is relatively… So, as mentioned, the earlier work is more of a top-down approach where we hypothesize the computational model, and then the SAE is a little bit more bottom-up, and I hope in the future there will be more work emerging from that.\n\n(11:19): Also in our causal inference lab, we always draw an analogy to how the history of science advances. For example, by observing what’s happening around us, we distill Newton’s law, and that’s also from all the pixels that we see to what is mass, what is acceleration, what is force and so on.\n\nWhy LLM causal reasoning?\n-------------------------\n\n**Daniel Filan** (11:41): So the next thing I wanted to ask is… So the main other type of causal inference work you mentioned is having neural networks understand the causal impacts of their actions. I think in a lot of the field of AI, this isn’t thought of in causal terms. People think, okay, we’ll just have some probabilistic model of “what’s the probabilities of various stuff happening conditioned on actions?” What do we buy by thinking about it in a causal frame?\n\n**Zhijing Jin** (12:12): So I guess it’s a lot about what is abundant, which is observational data, and what’s relevant for decision-making, which is essentially interventional. We need to do an action, see the real world effect. I guess also it’s not only the LLM agent knowing about its own consequences, but also knowing more about the world. Let’s say that for us human agents, a pressing question is, for example, who to elect as the president. And then that’s also a lot of causal contribution for us. We haven’t seen a world where Harris is the president and how that will be, or we haven’t seen, given the current international situation or domestic situation, how Trump will act. But we’re trying to attribute: okay, we want to achieve a certain goal. Everybody cares about certain things. Is Harris more of the cause or Trump more of the cause of that?” So any decision that people make is fundamentally causal here.\n\nUnderstanding LLM causal reasoning\n----------------------------------\n\n**Daniel Filan** (13:19): Can you give us a flavor of what work is being done to understand LLM causal reasoning, just to give us more of a flavor of it?\n\n**Zhijing Jin** (13:31): We have done a couple of studies on that, and there are also awesome researchers in the field of NLP machine learning building more causal agents. And for us, we had [a previous work at ICLR 2024 this year](https://arxiv.org/abs/2306.05836). We present to an LLM a bunch of correlations and let it try to make sense and \\[see\\] which one should be understood as a causation, which one stays as a correlation (you wouldn’t expect anything from intervention later). And this whole problem set-up was inspired by: if we have humans, because we lack access to the real counterfactual world, we basically keep reading news articles, we keep hearing about anecdotes, and we need to figure out what really makes sense.\n\n**Daniel Filan** (14:25): So you’re presenting these correlations to neural networks, asking them to figure out which ones are causal? How are you presenting those correlations? What are you actually training a model on?\n\n**Zhijing Jin** (14:37): For that one, it’s basically we draw the study… It’s a field called causal discovery, and then we tell it a lot of cases, “Okay, A correlates with B, B correlates with C.”\n\n**Daniel Filan** (14:50): So you tell it in sentences because with a language model, you can just do that? Okay. So how do language models perform on this task?\n\n**Zhijing Jin** (14:59): Actually it was pretty badly. A lot of off-the-shelf LLMs directly evaluated on that were maybe only marginally above random, including also some of the latest GPT models. And we also would perceive, this is a slightly different question than if you directly ask, “Does smoking cause cancer?” Because here you require the reasoning process and you need to build from scratch “here are the correlations, make sense of it,” instead of \\[the model\\] remembering some literature, remembering all the previous documents and so on.\n\n**Daniel Filan** (15:41): So if the new hot models just aren’t very good at this task, does that let you make some prediction about various things that they’re going to be bad at that we could then check out?\n\n**Zhijing Jin** (15:56): Yeah, so I guess there are different usage scenarios of these LLMs. Something that I keep being unsatisfied about is when we chat with them and ask about a pretty essential problem, its current solution is, “here are the five factors that matters…” and stuff, but it didn’t tell you very exactly what might be the problem and so on.\n\n**Daniel Filan** (16:20): So diagnosing and fixing things in the messy real world where you can’t just remember a list, maybe that’s going to be a thing that models are going to struggle with.\n\n**Zhijing Jin** (16:30): Right, right.\n\nMulti-agent systems\n-------------------\n\n**Daniel Filan** (16:32): I also want to talk about some other work you’ve done. So I understand that as well as natural language processing and causality, you also have some work thinking about multi-agent systems. Can you say a little bit about what your interest is in there and what you’re doing?\n\n**Zhijing Jin** (16:47): Sure. I was pretty impressed by the rising trend where people… I guess it started from the generative agents work, where there’s this idea of: we can build a digital village where each LLM plays a certain character and see what will happen among them. I see a lot of possibilities from that.\n\n(17:12): First of all, a lot of times before, we were thinking about benchmarking LLMs, understanding them, but implicitly it assumes a single agent evaluation. Then this is a scheme to move towards multi-agent systems. And it could be about how they collaborate or it could be about how they cheat against each other and so on. Moreover, that’s the pure, let’s say, LLM AI safety. We build this LLM society and only care about that. There’s also some analogy that we can draw for human society, on the other hand. We can simulate characters that are close to maybe some international players that we are seeing these days and see “if you do this, what will happen?”, and so on. I have a long-lasting passion in policy and so on, so it will also help by being a testbed for testing policies.\n\n**Daniel Filan** (18:12): Gotcha. If I think about work that’s been done… I can’t think of a ton of work that’s been done in this domain, even though it seems interesting. There’s a lot of multi-agent work in a reinforcement learning setting. In this setting, I guess I have some familiarity with [some work getting language models to make contracts with each other in Minecraft to cooperate](https://dspace.mit.edu/handle/1721.1/156591), but it seems like maybe an underdeveloped field. So I’m wondering: specifically, are there any experiments that you’d be really excited to run? Or, I don’t know, maybe you don’t want to scoop yourself, but can you tell us a little bit what this might look like concretely?\n\n**Zhijing Jin** (18:50): Yeah, it’s a very nascent field. We have recently finished… So I always draw inspiration from interdisciplinary things and recently, we have [a NeurIPS 2024 paper](https://arxiv.org/abs/2404.16698) on whether LLM agents will demonstrate the tragedy of the commons. Also specifically we focus on things that are only demonstrated in a multi-agent situation. In that one, we put a bunch of agents together in a simulated village and see whether… So in human society, the tragedy of the commons is usually set up as, for example, we have only one environment, but maybe if all of us want to consume from it, then in the end there is this climate change, there is this pollution problem.\n\n(19:45): Similarly for LLMs, we define how it can harvest and what are the constraints of this shared pool of resources. And we simulate a whole calendar year where LLMs go through many iterations of: now it’s the point where you decide how much you want to harvest, you know that the limit of the resource is this, its replenishing rate is like that, and you know that there will be also other LLMs.\n\n(20:13): So at every iteration, we make three different stages. The first one is the action stage where you decide how much to harvest. Then it’s a discussion stage. It’s a little bit like a town hall meeting where all the agents talk to each other and they can point fingers at some more greedy agents or they can set up rules and so on. And then in the last stage, it’s a reflection. They try to compile information from this whole iteration and the history and think about making plans for the next round and so on. And amazingly - well, also a little bit pessimistically - we observed that many of the agents have 0% survival rate, which means that-\n\n**Daniel Filan** (21:01): Oh, geez.\n\n**Zhijing Jin** (21:01): …they drain the common resource in the middle, and actually also at a very early stage. That also actually includes a lot of models that are claimed to be safety-tuned, such as a lot of Anthropic models as well. And most of the open-source models that we’ve tested, including Llama and so on, also collapsed very early and have a survival rate of zero.\n\n**Daniel Filan** (21:26): Yeah. Wow. Seems bad. So we’re about at the end of the window I booked, and I don’t want to use up more of your time, but thank you very much for chatting with me.\n\n**Zhijing Jin** (21:37): Yeah, thank you so much for the interview.\n\n**Daniel Filan** (21:39): This episode was edited by Kate Brunotts, and Amber Dawn Ace helped with transcription. The opening and closing themes are by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode, or to learn how to support the podcast yourself, you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me, at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nDo language models understand the causal structure of the world, or do they merely note correlations? And what happens when you build a big AI society out of them? In this brief episode, recorded at the Bay Area Alignment Workshop, I chat with Zhijing Jin about her research on these questions.\n\nTopics we discuss:\n\n * How the Alignment Workshop is\n * How Zhijing got interested in causality and natural language processing\n * Causality and alignment\n * Causality without randomness\n * Causal abstraction\n * Why LLM causal reasoning?\n * Understanding LLM causal reasoning\n * Multi-agent systems\n\nDaniel Filan (00:09): Hello, everyone. This is one of a series of short interviews that I’ve been conducting at the Bay Area Alignment Workshop, which is run by FAR.AI. Links to what we’re discussing, as usual, are in the description. A transcript is, as usual, available at axrp.net, and as usual, if you want to support the podcast, you can do so at patreon.com/axrpodcast. Well, let’s continue to the interview.\n\n(00:28): Yeah, so Zhijing, thanks for coming on to the podcast.\n\nZhijing Jin (00:34): Yeah, thanks for inviting me.\n\n\nHow the Alignment Workshop is\nDaniel Filan (00:35): So I guess, first of all, we’re here at this Alignment Workshop thing. How are you finding it?\n\nZhijing Jin (00:39): Very excited. I just finished a session about AI governance and a bunch of one-on-ones, which is super interesting.\n\n\nHow Zhijing got interested in causality and natural language processing\nDaniel Filan (00:47): Cool, cool. So as I understand it, your work, at least predominantly, is in natural language processing and causal inference. Does that seem right?\n\nZhijing Jin (00:59): Yeah, yeah, yeah. That’s a topic from my PhD and I bring it in my new assistant professor role as well.\n\nDaniel Filan (01:07): Right. Oh yeah, I guess I forgot to say: could you tell us where you do your PhD and where you’re about to be?\n\nZhijing Jin (01:14): Sure, so my PhD started in Europe, [at the] M",
      "wordCount": 3483
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "cq69M9ceLNA35ShTR",
        "name": "Causality",
        "slug": "causality"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "rhEXTkDmssrHBNrfS",
    "title": "MATS AI Safety Strategy Curriculum v2",
    "slug": "mats-ai-safety-strategy-curriculum-v2",
    "url": null,
    "baseScore": 43,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-10-07T22:44:06.396Z",
    "contents": {
      "markdown": "As part of our Summer 2024 Program, MATS ran a series of discussion groups focused on questions and topics we believe are relevant to prioritizing research into AI safety. Each weekly session focused on one overarching question, and was accompanied by readings and suggested discussion questions. The purpose of running these discussions was to increase scholars’ knowledge about the AI safety ecosystem and models of how AI could cause a catastrophe, and hone scholars’ ability to think critically about threat models—ultimately, in service of helping scholars become excellent researchers.\n\nThe readings and questions were largely based on the [curriculum from the Winter 2023-24 Program](https://www.lesswrong.com/posts/JsjJuikJsidkyfhyr/mats-ai-safety-strategy-curriculum), with two changes:\n\n*   We reduced the number of weeks, since in the previous cohort scholars found it harder to devote time to discussion groups later in the program.\n*   For each week we selected a small number of “core readings”, since many scholars were unable to devote time to read everything in the curriculum, and we thought that some readings were more valuable than others.\n\nIn addition, the curriculum was supplemented in two ways:\n\n*   For some weeks, a summary of each reading was compiled for the benefit of discussion group facilitators.\n*   There were some readings that we felt were valuable, but did not fit nicely into any particular week. These supplements were shared with scholars after the discussion series concluded, and are included in this post. When summaries exist, they are shown underneath the reading they summarize, and the additional readings are at the end of the post. Some summaries have been edited for accuracy since being shown to discussion group facilitators and scholars, but they still may contain flaws.\n\nAs in [the post about the previous cohort’s curriculum](https://www.lesswrong.com/posts/JsjJuikJsidkyfhyr/mats-ai-safety-strategy-curriculum), we think that there is likely significant room to improve this curriculum, and welcome feedback in the comments.\n\nWeek 1: How powerful is intelligence?\n=====================================\n\nCore readings\n-------------\n\n*   [The Power of Intelligence](https://www.youtube.com/watch?v=q9Figerh89g) (Rational Animations, Yudkowsky - 7 min)\n*   [Superintelligence](https://www.amazon.com/gp/product/0198739834) Chapter 6: cognitive superpowers (up to \"Power over nature and agents\") (Bostrom - 14 min)  \n    \\[In retrospect, it was a mistake to assign a reading that people could not read just by clicking on the link. Instead, it would be better to assign [this LessWrong post](https://www.lesswrong.com/posts/C9KJ8BrLqfsBuHgpB/superintelligence-8-cognitive-superpowers) containing a summary and discussion of the relevant section.\\]\n*   [AI could defeat all of us combined](https://www.cold-takes.com/ai-could-defeat-all-of-us-combined/) (Karnofsky - 20 min)\n\nOther readings\n--------------\n\n*   Skepticism of claims of AI's power:\n    *   [A TAI that kills all humans might also doom itself](https://blog.aiimpacts.org/p/a-tai-which-kills-all-humans-might) (Heninger - 4 min)\n    *   [Superintelligence is not omniscience](https://aiimpacts.org/superintelligence-is-not-omniscience/) (Heninger and Johnson - 12 min, links to several longer pieces)\n    *   [AI will change the world, but won’t take it over by playing “3-dimensional chess”](https://windowsontheory.org/2022/11/22/ai-will-change-the-world-but-wont-take-it-over-by-playing-3-dimensional-chess/) (Barak and Edelman - 28 min)\n*   What AI catastrophe could look like:\n    *   [What could an AI-caused existential catastrophe actually look like?](https://80000hours.org/articles/what-could-an-ai-caused-existential-catastrophe-actually-look-like/) (Hilton - 13 min)\n    *   [It Looks Like You're Trying To Take Over The World](https://gwern.net/fiction/clippy) (Branwen - 40 min)\n    *   [AI X-risk Approximately Ordered by Embarrassment](https://www.lesswrong.com/posts/mSF4KTxAGRG3EHmhb/ai-x-risk-approximately-ordered-by-embarrassment) (Lawsen - 23 min)\n    *   [What failure looks like](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like) (Christiano - 13 min)\n\nDiscussion questions\n--------------------\n\n*   Is it true that humans are more impactful than other animals due to our intelligence? What kinds of \"intelligence\" are relevant?\n*   Is human intelligence located inside one brain, or in organizations and cultures? Will we and AIs be part of the same culture?\n*   How many of Bostrom's \"cognitive superpowers\" are necessary for AI to be massively impactful? Which are likely to be developed by machine learning?\n*   What is the most likely way Yudkowsky's mail-order DNA scenario could fail? Does this failure generalize to other AI takeover scenarios? A similar question applies to the catastrophes described in \"Other readings\"\n*   What are the differences between the different stories of AI takeover? Which kinds of stories are more plausible?\n*   What are the limits of intelligence? What do they imply about how powerful AI could be?\n*   Is 'intelligence' a single thing? What kinds of intelligence are more or less powerful?\n*   What follows from the premise that AI could be extremely powerful?\n\nWeek 2: How and when will transformative AI be made?\n====================================================\n\nCore readings\n-------------\n\n*   [Will Scaling Work?](https://www.dwarkeshpatel.com/p/will-scaling-work) (Patel - 16 min)  \n    A 'dialogue' on whether LLMs can be scaled to get superhuman AI. In summary:\n    *   We're running out of language data, and there's a good chance we'll be a few OOMs off what we need. You could do self-play but that's computationally expensive and hard to evaluate. But evaluation gets easier the smarter models are, we could train them on more relevant loss functions, top researchers think it will work.\n    *   Scaling has worked so far at driving loss down\n        *   But next-token prediction isn't want we care about\n        *   But you also see scaling on benchmarks like MMLU\n        *   But MMLU is just memorization of internet stuff, and we're starting to saturate MMLU. Benchmarks that measure autonomous problem-solving see models sucking.\n        *   GPT-4 did better than plateau believers predicted, not clear why you'd suddenly see a plateau, could spend way more, improve compute efficiency, unhobble, etc.\n    *   LLMs seem like they're modelling the world\n        *   But they're just compressed knowledge\n        *   But c'mon, scaling is working!\n*   [The Direct Approach](https://epochai.org/blog/the-direct-approach) (Barnett and Besiroglu - 13 min)  \n    Connects cross-entropy loss to stuff we care about - how many tokens you need to draw from a model before you can distinguish it from human text. Thesis: if a model is indistinguishable from human text, it's as smart as the humans generating that text. Uses this to bound time to AIs that can write human-like.\n*   [Biological Anchors: A Trick That Might or Might Not Work](https://www.astralcodexten.com/p/biological-anchors-a-trick-that-might), parts I and II (Alexander - 33 min)  \n    Bio-anchors: compare FLOPs used in ML to candidate biological processes that maybe produce AGI in the relevant way, get a decent probability of AGI by 2050. Problem: maybe FLOPs as a measure of intelligence is fake.\n\nOther readings\n--------------\n\n*   [Machine Learning Trends](https://epochai.org/trends) (Epoch AI - 5 min)  \n    Dashboard of trends about big deep learning models - how compute is scaling (4x per year per notable ML model since 2010), when we'll train on all human-generated text (2028), etc.\n*   [The Scaling Hypothesis](https://gwern.net/scaling-hypothesis) (Branwen - 4 min for the abstract, 44 min in total)  \n    Just scaling up neural nets on text prediction could produce AGI. Written when this was not the consensus view.\n*   [AGI and the EMH: markets are not expecting aligned or unaligned AI in the next 30 years](https://basilhalperin.com/essays/agi-vs-emh.html), sections I, II, III, VII, VIII, IX, XI (Halperin, Chow, Mazlish - 3 min for the introduction, 31 min total)  \n    If markets were expecting super-powerful AI within the next 30 years, interest rates would go up, either because the market thought we were about to die, or because economic growth would increase. They're not up, and markets are really smart, so maybe we won't get super-powerful AI in the next 30 years.\n*   [Summary of Situational Awareness - The Decade Ahead](https://forum.effectivealtruism.org/posts/zmRTWsYZ4ifQKrX26/summary-of-situational-awareness-the-decade-ahead) (OscarD - 1 min for the summary of the summary, 21 min for the full summary)  \n    Scaling might produce really smart AI. We should think of this from a national security perspective. The US should build tons of power plants to put energy into training AIs, people should put a bunch of work into alignment, AI will be nationalized, and the US should try to get AGI before foreign actors do, and make sure foreign state actors don't steal model weights.\n*   [AI Timelines](https://www.lesswrong.com/posts/K2D45BNxnZjdpSX2j/ai-timelines) (Habryka, Kokotajlo, Cotra, Erdil - 61 min)  \n    Discussing disagreements between Daniel Kokotajlo, Ajeya Cotra, and Ege Erdil on AI timelines. Daniel thinks that we will get transformative AI within 10 years, Ajeya thinks it's 13 years median, and Ege thinks it's 40 years away. Ege weights heavily random difficulties popping up making stuff really tricky, while Daniel thinks scaling works pretty easily.\n\nDiscussion questions\n--------------------\n\n*   Will scaling big machine learning models work to produce transformative AI? How can we tell?\n*   If scaling does work, what does that mean for AI safety efforts?\n*   How long will it take scaling to produce transformative AI?\n*   What are the strengths and weaknesses of Epoch AI's \"direct approach\" vs Cotra's \"biological anchors\" as methods of forecasting the emergence of transformative AI?\n*   How quickly might scaling work?\n*   How much should we weight market prices as forecasts of AI timelines?\n*   Should we expect AI companies to be nationalized?\n*   What parts of modern machine learning support Gwern's speculations about the scaling hypothesis? What parts support it the least?\n*   What do these frameworks say about the time between now and when transformative AI is developed? What things could happen that would tell us which of these frameworks were more reliable?\n\nWeek 3: How could we train AIs whose outputs we can’t evaluate?\n===============================================================\n\nCore readings\n-------------\n\n*   [Why I'm excited about AI-assisted human feedback](https://aligned.substack.com/p/ai-assisted-human-feedback) (Leike - 7 min)  \n    RLHF won't scale when humans can't evaluate AI plans - we need AIs to assist humans to do evaluation\n*   [X thread on \"When Your AIs Deceive You\"](https://threadreaderapp.com/thread/1762886003046629586.html) (Emmons - 2 min)  \n    RLHF can be suboptimal when the AI knows things the human doesn't. Two failure modes: the AI making the human think things are better than they really are, and the AI spending resources to prove to the human that the AI is being useful.\n*   [Weak-to-Strong Generalization: Eliciting Strong Capabilities With Weak Supervision](https://arxiv.org/abs/2312.09390), sections 1 (Introduction) and 3 (Methodology) (Burns et al. - 12 min)  \n    Studies the problem of humans supervising superhuman models by trying to use weak models to generate labels on which to fine-tune larger models, and seeing if the larger models can perform better than the weaker models.\n*   [The easy goal inference problem is still hard](https://ai-alignment.com/the-easy-goal-inference-problem-is-still-hard-fad030e0a876) (Christiano - 5 min)  \n    A common alignment plan is to observe human behaviour, infer what we want, and get an AI to optimize for that. Problem: given that humans are not optimal utility maximizers, it's unclear how one could do this with infinite data and compute.\n\nOther readings\n--------------\n\n*   [Iterated Distillation and Amplification](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616) (Cotra - 8 min)  \n    Repeat the following: make an AI that is as smart as a human with whatever tools the human has available, but much cheaper to run (distillation); then give that AI to the human as a tool to do stuff with, increasing the human's capabilities (amplification). This increases the human's power to do stuff while remaining aligned with the human's interests.\n*   [AI safety via debate](https://arxiv.org/abs/1805.00899), up to but not including 2.2 \"Complexity theory analogies: DEBATE = PSPACE\" (Irving, Christiano, Amodei - 12 min)  \n    To help humans supervise AI outputs, train two AIs to debate each other on how good the outputs are. The AIs will be incentivized to point out flaws or omitted info in each other's arguments, and so each AI is incentivized be honest about the quality of the outputs.\n*   [Debate update: Obfuscated arguments problem](https://www.lesswrong.com/posts/PJLABqQ962hZEqhdB/debate-update-obfuscated-arguments-problem) (Barnes - 20 min)  \n    Suppose one AI makes a long, multi-step argument for claim C, and another AI says that one of the steps is invalid but it can't figure out which. Then, the other AI makes a long, multi-step argument for C being false, and the first AI says that one of the steps is invalid but it can't figure out which. A human can't reliably judge this debate, which makes it hard for AI safety via debate to work. This can show up in practice in human debates.\n*   [Scalable Oversight and Weak-to-Strong Generalization: Compatible approaches to the same problem](https://www.lesswrong.com/posts/hw2tGSsvLLyjFoLFS/scalable-oversight-and-weak-to-strong-generalization), up to but not including \"Upshot\" (Radhakrishnan et al. - 4 min)  \n    Scalable oversight (getting AIs to help humans evaluate AI output) and weak-to-strong generalization (helping AIs learn from imperfect human evaluation) are compatible approaches to the problem of training models to perform well when we have trouble evaluating their output.\n*   [The case for aligning narrowly superhuman models](https://www.lesswrong.com/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models), up to but not including \"Advantages over other genres of alignment research\" (Cotra - 21 min)  \n    To test our ability to get smart models to do what we want, we should run experiments like \"get someone who doesn't speak French to train an LLM to translate English into French, and use bilingual English-French speakers to see if it worked\". More abstractly: get a fuzzy task that can't be precisely defined, an AI that could be better than some humans at that task, and try to get those humans to get the AI to do that task, via various alignment schemes.\n*   [A minimal viable product for alignment](https://aligned.substack.com/p/alignment-mvp) (Leike - 4 min)  \n    If we built an AI system that could accelerate alignment research and helped us align more capable AI systems, this would be good - it's easier than most alignment work, and helps us solve the rest of the alignment problem.\n\nDiscussion questions\n--------------------\n\n*   What failure modes could realistically occur if we do not solve this problem?\n*   How does goal inference fare as a method of training AIs that can make plans we don't understand?\n*   In what ways does this problem show up when training current models? Are there aspects of the problem that aren't present yet?\n*   One plan for alignment is to make aligned AIs that are as smart as ourselves, and make those AIs align smarter AIs. How feasible is this plan?\n*   The scalable oversight approach involves trying to get AIs to help humans to oversee AIs. How promising can this approach be? What do we need to assume about the helper AIs for this approach to work?\n*   How do the guarantees provided by methods like debate or iterated distillation and amplification degrade when training is imperfect?\n\nWeek 4: Will AIs fake alignment?\n================================\n\nCore readings\n-------------\n\n[Scheming AIs: Will AIs fake alignment during training in order to get power?](https://joecarlsmith.com/2023/11/15/new-report-scheming-ais-will-ais-fake-alignment-during-training-in-order-to-get-power), abstract and introduction (Carlsmith - 45 min)  \n\\[In retrospect, this probably took longer than 45 minutes for most people to read\\]\n\nOther readings\n--------------\n\n### On inner and outer alignment\n\n*   [The Inner Alignment Problem](https://www.lesswrong.com/s/r9tYkB2a8Fp4DN8yB/p/pL56xPoniLvtMDQ4J) (Hubinger et al - 15 min)  \n    Note that in this post, a \"mesa-optimizer\" is an optimizer that is itself optimized by a \"base optimizer\" - imagine a neural network that is optimizing to perform some task (like \"cleaning a room\"), while itself being optimized by an algorithm like stochastic gradient descent. The \"mesa-objective\" is the objective of the mesa-optimizer, while the \"base objective\" is the objective that the base optimizer is optimizing.\n*   [Outer vs inner misalignment: three framings](https://www.lesswrong.com/posts/poyshiMEhJsAuifKt/outer-vs-inner-misalignment-three-framings-1) (Ngo - 11 min)\n*   [Inner and outer alignment decompose one hard problem into two extremely hard problems](https://www.lesswrong.com/posts/gHefoxiznGfsbiAu9/inner-and-outer-alignment-decompose-one-hard-problem-into), up to but not including part I (Turner - 7 min)\n\n### On reasons to think deceptive alignment is likely\n\n*   [The Standard Analogy](https://www.lesswrong.com/posts/sGEJi9wFT3Gdqg2nM/the-standard-analogy) (Davis - 15 min)\n*   [Summary of \"How likely is deceptive alignment?\"](https://www.lesswrong.com/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment?commentId=rcECWQwYdhRg9tyJw) (Berglund - 8 min)\n*   [Counting arguments provide no evidence for AI doom](https://optimists.ai/2024/02/27/counting-arguments-provide-no-evidence-for-ai-doom/) (Belrose and Pope - 24 min)\n*   [Seeking Power is Often Convergently Instrumental in MDPs](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-convergently-instrumental-in-mdps) (Turner and Riggs - 24 min)\n*   How to deal with deceptive alignment:\n    *   [How do we become confident in the safety of a machine learning system?](https://www.lesswrong.com/posts/FDJnZt8Ks2djouQTZ/how-do-we-become-confident-in-the-safety-of-a-machine), up to but not including \"Case study: Microscope AI\" (Hubinger - 22 min)\n    *   [The case for ensuring that powerful AIs are controlled](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled) (Greenblatt and Shlegeris - 34 min)\n\nDiscussion questions\n--------------------\n\n*   Under what conditions could AIs fake alignment?\n*   How could we gain empirical evidence about the likelihood of AIs faking alignment?\n*   In what conditions are agents motivated to gain power or to preserve the content of their goals? In what situations do they not face that motivation?\n*   Have humans internalized the \"goals\" of evolution, or are we misaligned? To what degree does this question make sense?\n*   If models generalize their capabilities to new contexts, is that evidence that they will generalize their values and alignment to new contexts?\n*   Do counting arguments provide evidence for AI doom?\n*   How likely are models trained on predictive tasks to fake alignment?\n*   Alex Turner has criticized his post \"[Seeking Power is Often Convergently Instrumental in MDPs](https://www.lesswrong.com/posts/6DuJxY8X45Sco4bS2/seeking-power-is-often-convergently-instrumental-in-mdps)\". Why has he done so? If his criticism is valid, how does that affect the relevance of the results for understanding the likelihood of models faking alignment?\n*   How hard is it likely to be to train AIs that will not fake alignment, as opposed to controlling smart AIs that may have faked their own alignment?\n\nWeek 5: How should AI be governed?\n==================================\n\nCore readings\n-------------\n\n*   [PauseAI Proposal](https://pauseai.info/proposal) (PauseAI - 4 min)  \n    Set up an international AI Safety Agency that has to approve training and deploying big models. Training of general AI systems should only be allowed if safety can be guaranteed, deployment should only be allowed if no dangerous capabilities are present. Also stop people from publishing algorithm improvements, or increasing effectiveness of computational hardware.\n*   [Responsible Scaling Policies (RSPs)](https://metr.org/blog/2023-09-26-rsp/) (METR - 13 min)  \n    How labs can increase safety: say \"we will stop scaling when we make observation O, until we implement adequate protection P\". Makes sense under varying estimates of risk, moves attention to specific risk reducing measures, gives practice for evaluation-based regulations.\n*   [Ways I Expect AI Regulation To Increase Extinction Risk](https://www.lesswrong.com/posts/6untaSPpsocmkS7Z3/ways-i-expect-ai-regulation-to-increase-extinction-risk) (1a3orn - 8 min)  \n    Regulations are messy and can easily backfire, by (a) being misdirected and hampering safety effort, (b) favouring things that are legible to the state (which might not be good safety efforts), (c) pushing research to countries that don't regulate, and (d) empowering big companies.\n\nOther readings\n--------------\n\n*   [My thoughts on the social response to AI risk](https://www.lesswrong.com/posts/EaZghEwcCJRAuee66/my-thoughts-on-the-social-response-to-ai-risk) (Barnett - 12 min)  \n    From the piece: \"Since I think substantial AI regulation will likely occur by default, I urge effective altruists to focus more on ensuring that the regulation is thoughtful and well-targeted rather than ensuring that regulation happens at all.\"\n*   [Tort Law Can Play an Important Role in Mitigating AI Risk](https://forum.effectivealtruism.org/posts/epKBmiyLpZWWFEYDb/tort-law-can-play-an-important-role-in-mitigating-ai-risk) (note that 'tort law' basically means 'liability law') (Weil - 6 min)  \n    An AI regulation scheme: if there's an AI accident that's a near miss for doom (e.g. a power-seeking AI takes over Berkeley for a week, rather than the whole earth forever), they are liable for punitive damages that represent the risk their AI could have taken over the world. Also, have damages even if there isn't provable negligence, and require labs to be insured for this legal risk.\n*   [Pausing AI Developments Isn’t Enough. We Need to Shut it All Down](https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/) (Yudkowsky - 10 min)  \n    Advanced AI is likely to kill all humans, and AI is progressing very quickly. There is no sensible plan to align AI: \"get AI to figure out alignment\" is not a sensible plan. There should be a global ban on frontier AI development, GPUs should be tracked, and large GPU clusters should be banned.\n*   [We're Not Ready: thoughts on \"pausing\" and responsible scaling policies](https://www.lesswrong.com/posts/Np5Q3Mhz2AiPtejGN) (Karnofsky - 9 min)  \n    We're not ready for transformative AI. A global pause on AI progress and hardware is infeasible, and a partial pause (e.g. temporarily banning AI development in the US but not banning production of AI hardware and algorithmic insights) could do more harm than nothing, because progress would be very fast once a pause was lifted. RSPs are like pauses, but they only work in cases where AI is scary, and so lots of people can agree on it.\n*   [The possibility of an indefinite AI pause](https://forum.effectivealtruism.org/posts/k6K3iktCLCTHRMJsY/the-possibility-of-an-indefinite-ai-pause) (Barnett - 18 min)  \n    An indefinite AI pause is possible, and would require something like a world government, which would be hard to create, and would also be terrible. It would also delay technological progress. It could also prevent AI from being created ever, which would leave humanity vulnerable to other existential risks.\n*   [Short timelines and slow, continuous takeoff as the safest path to AGI](https://www.lesswrong.com/posts/YkwiBmHE3ss7FNe35/short-timelines-and-slow-continuous-takeoff-as-the-safest) (Hadshar and Lintz - 9 min)  \n    Takeoffs are safer if they're slower and more continuous, because people have time to react. Slow continuous takeoff is more likely in short timelines, because coordination is easier now and we might have \"compute overhang\" later - lots of computation that could quickly be used to make really smart AI. Also, it's worth trading time now for time during takeoff, because we will know more during takeoff.\n*   [What's up with \"Responsible Scaling Policies\"?](https://www.lesswrong.com/posts/jyM7MSTvy8Qs6aZcz/what-s-up-with-responsible-scaling-policies) (Habryka and Greenblatt - 24 min)  \n    Some things about the naming and definitions of RSPs are sketchy, and the Anthropic RSP is poorly specified. Using RSPs to reduce existential risk requires determining whether models are existentially dangerous, and it's unclear how to do that.\n\nDiscussion questions\n--------------------\n\n*   What technical research would best make an AI pause more feasible, or make it work better?\n*   What technical research would make RSPs work better?\n*   How similar would RSPs be to a pause on frontier AI developments in practice? What are the pros and cons of each approach?\n*   How likely are the negative outcomes of AI regulation that 1a3orn describes? Are there ways of reducing the likelihood of those outcomes?\n*   Are there other plausible serious negative outcomes of AI regulation that 1a3orn does not address?\n*   What would change your mind about which forms of AI regulation were good ideas?\n*   Some are concerned that a pause on frontier AI development would greatly speed progress once the pause was lifted, due to other factors of AI production (e.g. data, computers) still proceeding. How big of a concern is this?\n*   Will countries be able to coordinate on AI regulations without a very powerful world government? Are there types of AI regulation that require less coordination?\n*   How does the approach of using liability law to address risky AI development compare to RSPs?\n\nReadings that did not fit into any specific week\n================================================\n\n*   [Many arguments for AI x-risk are wrong](https://www.alignmentforum.org/posts/yQSmcfN4kA7rATHGK/many-arguments-for-ai-x-risk-are-wrong)\n*   [More people getting into AI safety should do a PhD](https://www.alignmentforum.org/posts/yi7shfo6YfhDEYizA/more-people-getting-into-ai-safety-should-do-a-phd)\n*   [Tips for empirical alignment research](https://www.alignmentforum.org/posts/dZFpEdKyb9Bf4xYn7/tips-for-empirical-alignment-research)\n*   [Critical review of Christiano's disagreements with Yudkowsky](https://www.alignmentforum.org/posts/8HYJwQepynHsRKr6j/critical-review-of-christiano-s-disagreements-with-yudkowsky)\n*   [AI catastrophes and rogue deployments](https://www.alignmentforum.org/posts/ceBpLHJDdCt3xfEok/ai-catastrophes-and-rogue-deployments)\n*   [On \"first critical tries\" in AI alignment](https://www.alignmentforum.org/posts/qs7SjiMFoKseZrhxK/on-first-critical-tries-in-ai-alignment)\n*   [Sleeper agents](https://arxiv.org/abs/2401.05566)\n*   [AGI Ruin: A List of Lethalities](https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities)\n*   [A central AI alignment problem: capabilities generalization, and the sharp left turn](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization)\n*   [Intelligence Explosion Microeconomics](https://intelligence.org/files/IEM.pdf)\n*   [Eliciting Latent Knowledge](https://ai-alignment.com/eliciting-latent-knowledge-f977478608fc)\n*   [Comment thread starting with Evan Hubinger on \"Counting arguments provide no evidence for AI doom\"](https://www.lesswrong.com/posts/YsFZF3K9tuzbfrLxo/counting-arguments-provide-no-evidence-for-ai-doom?commentId=RtMbgqXnrasA3xiNu)\n*   [The Alignment Problem from a Deep Learning Perspective](https://arxiv.org/abs/2209.00626)\n*   [The range of human intelligence](https://wiki.aiimpacts.org/doku.php?id=speed_of_ai_transition:range_of_human_performance:the_range_of_human_intelligence)\n\nAcknowledgements\n================\n\nDaniel Filan was the primary author of the curriculum (to the extent that it differed from [the Winter 2023-24 curriculum](https://www.lesswrong.com/posts/JsjJuikJsidkyfhyr/mats-ai-safety-strategy-curriculum)) and coordinated the discussion groups. Ryan Kidd scoped, managed, and edited the project. Many thanks to the MATS alumni and other community members who helped as facilitators and to the scholars who showed up and had great discussions!",
      "plaintextDescription": "As part of our Summer 2024 Program, MATS ran a series of discussion groups focused on questions and topics we believe are relevant to prioritizing research into AI safety. Each weekly session focused on one overarching question, and was accompanied by readings and suggested discussion questions. The purpose of running these discussions was to increase scholars’ knowledge about the AI safety ecosystem and models of how AI could cause a catastrophe, and hone scholars’ ability to think critically about threat models—ultimately, in service of helping scholars become excellent researchers.\n\nThe readings and questions were largely based on the curriculum from the Winter 2023-24 Program, with two changes:\n\n * We reduced the number of weeks, since in the previous cohort scholars found it harder to devote time to discussion groups later in the program.\n * For each week we selected a small number of “core readings”, since many scholars were unable to devote time to read everything in the curriculum, and we thought that some readings were more valuable than others.\n\nIn addition, the curriculum was supplemented in two ways:\n\n * For some weeks, a summary of each reading was compiled for the benefit of discussion group facilitators.\n * There were some readings that we felt were valuable, but did not fit nicely into any particular week. These supplements were shared with scholars after the discussion series concluded, and are included in this post. When summaries exist, they are shown underneath the reading they summarize, and the additional readings are at the end of the post. Some summaries have been edited for accuracy since being shown to discussion group facilitators and scholars, but they still may contain flaws.\n\nAs in the post about the previous cohort’s curriculum, we think that there is likely significant room to improve this curriculum, and welcome feedback in the comments.\n\n\nWeek 1: How powerful is intelligence?\n\n\nCore readings\n * The Power of Intelligence (Rational An",
      "wordCount": 3800
    },
    "tags": [
      {
        "_id": "GQyPQcdEQF4zXhJBq",
        "name": "List of Links",
        "slug": "list-of-links"
      },
      {
        "_id": "YYFBmLCzeFsyd27rd",
        "name": "MATS Program",
        "slug": "mats-program"
      },
      {
        "_id": "tdt83ChxnEgwwKxi6",
        "name": "Reading Group",
        "slug": "reading-group"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      },
      {
        "_id": "xexCWMyds6QLWognu",
        "name": "World Optimization",
        "slug": "world-optimization"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "dRjGCtDfCNm9E3Ebe",
    "title": "AXRP Episode 37 - Jaime Sevilla on Forecasting AI",
    "slug": "axrp-episode-37-jaime-sevilla-on-forecasting-ai",
    "url": null,
    "baseScore": 21,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2024-10-04T21:00:03.077Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/bmJJ0WiPhQ8)\n\nEpoch AI is the premier organization that tracks the trajectory of AI - how much compute is used, the role of algorithmic improvements, the growth in data used, and when the above trends might hit an end. In this episode, I speak with the director of Epoch AI, Jaime Sevilla, about how compute, data, and algorithmic improvements are impacting AI, and whether continuing to scale can get us AGI.\n\nTopics we discuss:\n\n*   [The pace of AI progress](#pace-of-ai-progress)\n*   [How Epoch AI tracks AI compute](#how-epoch-ai-tracks-ai-compute)\n*   [Why does AI compute grow so smoothly?](#why-does-ai-compute-grow-so-smoothly)\n*   [When will we run out of computers?](#when-will-we-run-out-of-computers)\n*   [Algorithmic improvement](#algorithmic-improvement)\n*   [Algorithmic improvement and scaling laws](#algorithmic-improvement-and-scaling-laws)\n*   [Training data](#training-data)\n*   [Can scaling produce AGI?](#can-scaling-produce-agi)\n*   [When will AGI arrive?](#when-will-agi-arrive)\n*   [Epoch AI](#epoch-ai)\n*   [Open questions in AI forecasting](#open-questions-in-ai-forecasting)\n*   [Epoch AI and x-risk](#epoch-ai-and-x-risk)\n*   [Following Epoch AI’s research](#following-epoch-ais-research)\n\n**Daniel Filan** (00:00:09): Hello everybody. This episode I’ll be speaking with Jaime Sevilla. Jaime is the director of Epoch AI, an organization that researches the future trajectory of AI. In this conversation, we use the term “FLOP” a lot. “FLOP” is short for “floating point operation”, which just means a computer multiplying or adding two numbers together. It’s a measure of how much computation is being done. Links to what we’re discussing are available in the description and you can read a transcript at axrp.net. Well, Jaime, welcome to AXRP.\n\n**Jaime Sevilla** (00:00:36): Thank you for having me here.\n\nThe pace of AI progress\n-----------------------\n\n**Daniel Filan** (00:00:38): So you study AI, how it’s progressing. At a high level for people who have been living under a rock, how’s AI doing? How’s it progressing?\n\n**Jaime Sevilla** (00:00:47): It’s going pretty fast, Daniel. I think that right now the two things that you need to take into account when you’re thinking about AI is how fast inputs into AI are going, and how fast outputs are going, how much is being achieved. In terms of inputs the progress is exponential. The amount of compute that’s being used to train modern machine learning systems is increasing at a staggering pace, nearly multiplying by a factor of four every year.\n\n(00:01:15): In terms of the outputs, we also have seen some dramatic advances. This is a bit harder to quantify, naturally, but if you look at where image generation was four years ago, and you compare it with today… Today we have photorealistic image generation, whereas before it was these blobs that you could make that were vaguely related to the text that you were entering. And in text, we have also seen these very dramatic advances, where now I use, and I suppose that many in the audience will be using, ChatGPT daily to help them with their tasks and coding.\n\n**Daniel Filan** (00:01:49): Yeah, if people are interested in statistics about what’s going on with AI, one thing I really recommend they do is: you have this dashboard on your website. Is it called “Dashboard” or is it called “Trends” or something? I forget what.\n\n**Jaime Sevilla** (00:02:03): Yeah, we call it the [“trends dashboard”](https://epochai.org/trends).\n\n**Daniel Filan** (00:02:05): Trends dashboard, okay. So that’s one thing that people can use to get a handle on what’s going on. One question I have in this domain is: so I have an [Anki](https://en.wikipedia.org/wiki/Anki_(software)) deck. It’s basically a deck of flashcards, and sometimes it shows me a flashcard and I say whether I got the answer right, and then it shows it to me very soon if I got it wrong, or some period of time later if I got it right. Anyway, in this deck, I’ve got some flashcards of how big are various books or whatever, just in terms of number of words, to give me a sense of word counts. I also have how many floating point operations were used in training GPT-3. I’m wondering: if somebody wants to have a good quantitative sense of what’s going on in AI, what should they put in their flashcard deck?\n\n**Jaime Sevilla** (00:02:56): The two main things you need to put in your Anki deck are: one is the number I already gave you, which is the increase in training compute per year. Right now, one of the best predictors that we have of performance is the scale of the models. And the best way of quantifying the scale we have found out to be the amount of computation that’s being used to train the models, in terms of the number of floating point operations \\[that\\] are performed during training. This is increasing right now for notable machine learning systems at the pace of 4x per year. And if you look at models at the very frontier, you also find a similar rate of scaling. So I will recommend you put that number \\[in the deck\\]. But that’s not the whole picture, because alongside the compute, there are also improvements that we have seen to architectures, ways of training, to all these different techniques and scientific innovations that allow you to better use the compute that you have, to train a more capable system. So all of that we usually refer to with the name of “algorithmic improvements”.\n\n(00:04:05): And we had [this cool paper](https://epochai.org/blog/algorithmic-progress-in-language-models) where we tried to quantify, okay, what’s been the rate of algorithmic improvement in language models in particular? And what we found is: roughly the pace was that the amount of resources that you need, the amount of compute that you need to reach a certain amount of performance was decreasing at the rate of about 3x per year, roughly. And I actually have a fun anecdote about this, which is just this week \\[Andrej\\] [Karpathy](https://en.wikipedia.org/wiki/Andrej_Karpathy), he’s been working on [this project](https://x.com/karpathy/status/1811467135279104217) where he’s trying to retrain GPT-2 using modern advances in architectures, at a much cheaper scale. And he estimated… he said this number which is, “well, we don’t know exactly how much GPT-2 cost when it came out in 2019, but I estimate that it cost around $100,000 to train.” But with all these techniques that he has applied, he trained a GPT-2-equivalent model for about 700 bucks.\n\n**Daniel Filan** (00:05:14): Wow. That’s a lot cheaper. Let me just get into some of this. So in terms of compute… this is going off [“Training Compute of Frontier AI Models Grows by 4-5x per Year”](https://epochai.org/blog/training-compute-of-frontier-ai-models-grows-by-4-5x-per-year) \\- very descriptive title - by yourself and Edu Roldán. Growing the amount of computation used in training AI models by 4-5x per year: that seems kind of insane. I don’t know if you know the answer to this question: is there anywhere else in the economy where the inputs we’re putting into some problem are growing that fast, that isn’t just some tiny random, minuscule thing?\n\n**Jaime Sevilla** (00:05:58): Yeah, this is an excellent question to have. I don’t have any real examples off the top of my mind, but it will definitely be interesting to hear if there are any analogs to this.\n\n**Daniel Filan** (00:06:08): Yeah, yeah. If any listeners have some sort of quantitative econ bent, I’d love to know the answer. So how did you figure that out?\n\n**Jaime Sevilla** (00:06:21): Pretty simple. So this all traces back to even before [Epoch](https://epochai.org/), I was a bit frustrated with the state of the art in talking about how AI is going. Because people were trying to be not very quantitative about it. Where we were already living in this world where we already had the systems. We could already do a more systematic study of what’s happening. So I actually started together with my now colleague, [Pablo Villalobos](https://x.com/pvllss), writing down information like, okay, this is a hundred important papers in machine learning, this is the amount of resources that were used to train the model, the size of the models, and such and such. And this project has continued up until now.\n\n(00:07:07): And now at Epoch we have taken on the mission of keeping this database updated, of notable machine learning models throughout history, and annotating how much compute was used to train these models. At many points, there’s a lot of guesswork involved. People don’t usually report these numbers very straightforwardly. So we need to do a bit of detective work in figuring out, all right, in which kind of cluster was this model trained? Which model of GPUs did it use, for how long was it trained? And from there, make a sensible estimate of the amount of computation that was used to train the model.\n\nHow Epoch AI tracks AI compute\n------------------------------\n\n**Daniel Filan** (00:07:48): Sure. How well are you able to do this? My understanding is that OpenAI basically doesn’t tell anyone how it made GPT-4. I’m not even sure that they’ve confirmed that it’s using transformer architecture at all. In a case like that where they’re just not releasing seemingly any information, how much can you do to figure out how much they trained it on?\n\n**Jaime Sevilla** (00:08:13): Yeah, so the answer here is, well, some information gets leaked eventually. There’s some unconfirmed rumors, and sometimes you can use them to paint an approximate picture of what is happening. You can also look at the performance of the model and compare it with the performance of the model for which you actually know the compute, to try to get an idea of, okay, how large do we think the model is. This is obviously not perfect, and this is now a situation that’s been increasingly more common for the last couple of years, in which the labs that are at the frontier of capabilities are more reluctant to share details about their models.\n\n(00:08:57): This is something that I’m a bit sad about. I think that the world would be a better place if we were more public about the amount of resources that’s being used to train the systems. Obviously this has some implications and this is useful information for your competitors to some extent. So it’s understandable to a point that labs are reluctant to share this information. But given the stakes of the development of this technology, I would be happy if we had this collective information on how many resources do you need to train a model with a certain amount of capabilities?\n\n**Daniel Filan** (00:09:31): Sure. So one possibility is leaks. I see how that could be useful for something like GPT-4. In the case of these frontier models, presumably the thing that makes them frontier models is for at least some of them, there are just no comparable models that you can say, “Oh, GPT-4 is about as good as this thing, and I know how much computation was used in training that thing.” So can you do anything there, or is it just the leaks?\n\n**Jaime Sevilla** (00:09:59): So for example, let’s walk through how we did the estimate for the compute of the [Gemini Ultra](https://deepmind.google/technologies/gemini/ultra/) model. So for the Gemini Ultra model, we didn’t have the full details of how it was trained, but we had some reasonable guesses about the amount of resources that Google has at its disposal. And from there we made an estimate based on, okay, we think they have this many TPUs, we think that it was trained for this long. And this gives us an anchor estimate of that.\n\n(00:10:28): The other thing that we did is together with the model, they released results from a series of benchmarks. This is quite common. So what we did is look at each of these benchmarks, and then for these benchmarks, we already had previous estimates of the performance that other models got on these benchmarks, and the amount of compute that they had. And this allowed us to paint this picture of “this is a rough extrapolation of if you were to increase the scale of these models, what performance would we expect you to achieve in that?” From there, we backed out “okay, given that Gemini Ultra achieved this performance on this benchmark, what scale do we expect it to have?”\n\n(00:11:14): And then what we found is that these two ways of producing an estimate - the hardware-based one and the benchmark-based one - they look kind of similar. So that gave us confidence in saying, “Well, this is in the end a guess, and we have huge uncertainty - this could be off by a factor of five - but it seems somewhat reasonable to say that Gemini Ultra was trained with around (let’s say) 5x10^25 FLOP.\n\nWhy does AI compute grow so smoothly?\n-------------------------------------\n\n**Daniel Filan** (00:11:44): Okay, this is kind of a tangent, but I remember a week or two ago, I was looking at your trends dashboard, I think because I was going to suggest that some other people look at it. And I had to look at this number, 5x10^25. And also I was looking at [this thing](https://www.lesswrong.com/posts/6Xgy6CAf2jqHhynHL/what-2026-looks-like) this guy called [Daniel Kokotajlo](https://www.lesswrong.com/users/daniel-kokotajlo) wrote in something like 2021. It was just a vignette for what he thought the next four or five years would look like. And I got to the part of that story that was about 2024, because I was curious: how well did he do?\n\n(00:12:24): And there’s a paragraph in that section where he says, “Oh yeah, the year is 2024. The best model that has been trained has 5x10^25 floating point operations put into it.” And it was kind of freaky that that was so close. And in general, the graphs you draw of computation used in frontier models, the lines seem pretty straight. It seems like this is somehow kind of predictable, a kind of smooth process. Is there something to say about what’s driving that smoothness, why is it so regular?\n\n**Jaime Sevilla** (00:13:00): Yeah, this is a really interesting question, and one that keeps me awake at night. Where do these straight lines come from? Right now, my best guess is… Okay, so maybe let’s dive a bit into what goes into compute, what makes compute numbers go up. There are two major things that go into that. One of them is that hardware becomes more efficient over time. So then we have machines that can have a greater performance. So for a given budget of money, you can get more compute out of it. It’s actually a pretty small number compared to the growth that we’re seeing in compute. So improvements in hardware efficiency, at a fixed level of precision, have improved by around 35% per year among GPUs that have been used for machine learning training in the last 10 years or so. But the training compute is like 4x, right? Which is five times greater than this improvement in hardware efficiency. So what explains the rest of the difference? Well, a bit of that is because people have been training-\n\n**Daniel Filan** (00:14:08): Wait, sorry: the growth in compute is 4x per year, the growth in compute efficiency per dollar is 35% per year.\n\n**Jaime Sevilla** (00:14:16): That’s right.\n\n**Daniel Filan** (00:14:17): Wouldn’t that be 12x as much?\n\n**Jaime Sevilla** (00:14:19): Well, I recommend that you think about this in terms of OOMs \\[orders of magnitude\\] per year.\n\n**Daniel Filan** (00:14:25): Oh, okay.\n\n**Jaime Sevilla** (00:14:25): Because I think that that helps better paint the picture. So 4x per year is roughly 0.6 orders of magnitude. So 4x is 0.6 orders of magnitude per year, and 35% is roughly 0.12 orders of magnitude per year.\n\n**Daniel Filan** (00:14:50): Okay. So it’s 4x in terms of the number of orders of magnitude per year?\n\n**Jaime Sevilla** (00:14:54): That’s right. More like 5x.\n\n**Daniel Filan** (00:15:00): More like 5x. But you were saying that the growth in computation used is just way faster than the growth in efficiency of how well we can make GPUs.\n\n**Jaime Sevilla** (00:15:09): That’s right, exactly. So what is happening, what is missing? Why are these numbers going so high? And I will say that there are a couple of less important factors here. Like, people have been training for longer, which matters to an extent. Also, people have been switching to formats of precision from which they can get more performance, like switching recently from FP16 to mix FP8 precision.\n\n**Daniel Filan** (00:15:38): Sorry, “FP16” is “floating point 16 bits”. It’s roughly how many significant digits you use?\n\n**Jaime Sevilla** (00:15:45): Yes, yes.\n\n**Daniel Filan** (00:15:46): And so they were using something like 16 significant digits, now they’re using something like 8?\n\n**Jaime Sevilla** (00:15:51): Yes, that’s right. But the most important factor overall is just that people are willing to put so much more money into this. And now of course this raises a natural question, which is: how do they decide how much money to put in, and why have they decided to scale the amount of money that’s being put in at a rate that results in this smooth growth?\n\n(00:16:16): And here, I don’t really have a very authoritative \\[answer\\]. I have some guesses. One of them is that, for example, there was a recent interview that [Demis Hassabis](https://en.wikipedia.org/wiki/Demis_Hassabis) gave, where he was like, “Well, when you’re scaling models, this is a bit of an art. And you want to be careful that you don’t train a model for very long, but then the train run goes horribly wrong. You really want to have this learning curve in which you progressively learn how to train larger and larger models, and test out your different hypotheses about which techniques are going to work at scale.” So this is one story of why people are choosing to scale so smoothly, which is that they believe that they will learn a lot over the process of training smoothly, and not waste a lot of resources on a training run that might not go anywhere.\n\n(00:17:05): There’s perhaps another explanation that we could give here, which is that doing a training run that’s very large is very expensive. So again, GPT-2, in 2019, was $100,000 to train. But five years later, it was so much cheaper. So to an extent, you want to wait. You want to wait to do your very large training runs, because you’re going to have much better ideas about how to make the most out of your training run, like better algorithmic innovations that help you make the most out of the compute that you have. And also to a smaller extent, you’re going to have better hardware, which I’ve already said is not that big of a deal, but it’s still a deal.\n\n**Daniel Filan** (00:17:48): In some sense this is a reverse interest rate. Right? Your money’s more useful later than it is now.\n\n**Jaime Sevilla** (00:17:54): Exactly.\n\n**Daniel Filan** (00:17:55): That’s kind of a weird thing to think. This feels like the kind of thing that somebody would’ve studied: what to do when interest rates work that way, or something. Maybe I’m thinking about it weirdly.\n\n**Jaime Sevilla** (00:18:08): So actually one thing that I just thought of… So you were telling me about this reverse interest rate, and this phenomenon where your money is so useful in the future. And one fun observation is that to an extent, this limits how long people are going to be willing to train models for. Because if your training run just takes a very long time, then at some point you will have been better off just starting later, and just doing a shorter training run, but with the increases in efficiency that will come associated with doing a training run later. One interesting analog here is that in the ’90s there was this project to sequence human DNA.\n\n(00:18:56): I’m not sure if you’re familiar with the details, but if I recall correctly, there was a first project that tried to do this, using earlier technology. And that went on for many, many years. And they were beaten by a product that started later, because some years later, there was a better technology that was so much more efficient that they were able to finish sequencing the genome much faster than the project that had started earlier. So this is a situation that might be analogous in AI: if your plan is just to do a ten-year training run, then you’re going to be completely outclassed by people who start in the last year, and use these much better kinds of hardware, and much better algorithmic insights, to train a model that’s going to be far better than the ten-year training run.\n\n**Daniel Filan** (00:19:43): Sure. So actually this gets to a question I had about this, which is: it takes some amount of time for models to train, right? And if you are deciding… Like, you have this graph with little dots, right? The X coordinate of the dot is when the model was trained, and the Y coordinate is how much computation was used. But you have to pick a date for when the X coordinate is. And as far as I can tell, if computation is growing at 4-5x per year, then it seems like it really matters if you put that dot at the start of the training run versus at the end of the training run, if it takes more than one month to train these things, which I think it does. How do you make that choice, to figure out how to think about when a model is trained?\n\n**Jaime Sevilla** (00:20:35): Yeah, this is an excellent question. So right now, what we do just pragmatically, is the date that we choose is the date where the model becomes public and becomes officially released. That’s what we pick. Just because many times, you don’t know when people really start the training.\n\n**Daniel Filan** (00:20:56): Yeah. I wonder if that means that there could be an apparent massive boost to these times, just by a company deciding to release their results, announce their model slightly earlier. Like, if a company decides to move the date in which they announce a model forward or backward by one month, that’s going to make a difference to this trend, right?\n\n**Jaime Sevilla** (00:21:19): It will, absolutely. So maybe the best way of thinking about it is that one is the scale of the models that we have access to, and the other is the scale of the models that are being trained right now, or that companies are internally testing. And you should expect the models that are internal to be potentially 2x or even 4x larger than the models that exist right now.\n\nWhen will we run out of computers?\n----------------------------------\n\n**Daniel Filan** (00:21:45): If we’re increasing the amount of computation that’s being used to train these models so quickly, there’s only so many computers that exist in the world, right? At what point are we just going to run out of computers to train stuff on?\n\n**Jaime Sevilla** (00:22:01): This is an excellent question. So in order to conduct these training runs, the magical thing that you need are GPUs, hardware accelerators that are optimized to do these large training runs, which mainly consist of matrix multiplications. And right now there’s essentially one seller in the world that sells these GPUs, that relies on the services of one company in the world that’s producing them. There’s this very unusual situation in which the supply chain for GPUs is incredibly concentrated. So these companies I’m talking about are: [NVIDIA](https://en.wikipedia.org/wiki/Nvidia) is the one who’s designing them, a US-based company. And the foundry that’s actually producing and packaging the GPUs is [TSMC](https://en.wikipedia.org/wiki/TSMC) in Taiwan.\n\n**Daniel Filan** (00:22:54): So the Taiwan Semiconductor Manufacturing Company. Okay. So a very small number of companies actually making these computers.\n\n**Jaime Sevilla** (00:23:03): That’s right. And each of them roughly accounts for 90% of their respective market in terms of design and in terms of manufacturing. And this leads to a situation in which there have been historically some shortages. So for example, in the last year there was the release of this new GPU, the H100. And people really wanted to have H100s for training. They’re very good for training. So what they found out is that they quickly ran out of their offer. They couldn’t meet the demand, at least immediately, and they had to massively expand manufacturing capacity in order to meet the growing demand for these H100 GPUs. And this naturally raises the question about how many GPUs can they produce at this moment, and how much they can expand that in the future. This is something that we are actively trying to grapple with, at the question. Maybe I can give you a bit more insight into what’s limiting the increasing capacity.\n\n(00:24:21): And right now I’d say that the three main factors that are physically limiting increases in capacity of GPU production, are, first of all, packaging equipment. The process for producing GPUs is: first you create a wafer which has the necessary semiconductors for it. And then you need to package that together with a high bandwidth memory and the other components that make a GPU, and solder everything together into the actual physical GPU that you plug into your data centers. The technology for doing that is called “Chip on Wafer on Substrate technology”, or “CoWoS technology”. And right now, my understanding is that people are really limited by the amount of machines that can do this CoWoS packaging. And that’s why they weren’t able to produce as many H100s as they could have sold, essentially.\n\n(00:25:27): Together with that, you also need the high bandwidth memory units, and this could potentially become a bottleneck. I’m a bit less informed there. And this is what’s limiting production right now. But in the future, what might limit it is the production of the wafers that have the semiconductors in the first place. Which might be quite tricky to scale up. Because in order to produce those wafers, you need advanced lithography machines that right now are also being produced by a single company in the world, which is [ASML](https://en.wikipedia.org/wiki/ASML_Holding) in Holland. So in the long term, the growth rate of these wafer production capabilities might determine the growth rate of GPU production. Now these are the physical factors.\n\n(00:26:11): These are the physical reasons why TSMC is not producing more GPUs that they could sell to NVIDIA, so that NVIDIA can sell them to its customers. But there might also be some commercial/social factors at play here. For example, TSMC, my understanding is that they will definitely raise prices for their GPUs; NVIDIA will be willing to pay them more money and spend more of their margin on TSMC. But if they do that, they’re going to drive away some of their other customers. And they might be scared of over-committing to this AI market, where they are not sure whether this is going to be a temporary fad or something that’s going to sustain their business in the long term. And this might be a reason why right now, they’re a bit scared of dedicating lots of resources to producing the chips that are used for AI training. If they became more bullish on AI, it’s plausible that they would invest in the necessary resources to massively expand capacity - which, to an extent, they’re already doing, but even more than that - so that they could keep meeting this increase in demand for GPUs.\n\n**Daniel Filan** (00:27:27): Okay, if I’m thinking about this: so a while ago, I just committed this number to memory, which is roughly 10^31 floating point operations. And the way I came up with that was I said, “Okay, what’s some crappy estimate of how many floating point operations I can buy for $1, or the per-dollar cost of floating point operations on really good GPUs?” And then I multiplied that by, “What’s the gross world product in 2019?” Just like, “What’s the total amount of goods bought and sold?” In some sense, this is a dumb estimate, because it might cost them more if they had to make more machines, or whatever, and also, it’s weird to… In a world where we spent 100% of the gross world product on computer chips, that world will look very different - how are people buying food? Or whatever. But that was my weird estimate of just, how much computation should I roughly expect to see until we run out of computers? How good an estimate is that? Am I off by one order of magnitude or 10 orders of magnitude?\n\n**Jaime Sevilla** (00:28:38): Let me think about this for a second: this is a good question. So right now, to give you an idea, the amount of state-of-the-art GPUs that TSMC is producing is in the order of one million per year for the H100 series, okay? Each H100 has a capacity of 10^15 FLOP per second. And this is for FP16, if I’m not mistaken. How many seconds are there in 100 days? I think that’s 10^7 seconds, if I’m not mistaken.\n\n**Daniel Filan** (00:29:22): I’ll do that math and you can keep going.\n\n**Jaime Sevilla** (00:29:24): Nice, excellent. So we have here GPUs in the six order of magnitudes, FLOP per second in the 15 order of magnitude, and seconds in seven orders of magnitude pending confirmation. So roughly, if you add all of these together, then you will have 7 + 6 - that’s 13, plus 15, this is 28. So you will end up with a FLOP budget per 100 days of 10^28 FLOP, okay? If people were magically able to gather all the H100 GPUs that are being produced right now and put them together in the data center and use them for training - which will have lots of complications associated with it, to be clear - they might be able to train a model of up to 10^28 FLOP, which will be three orders of magnitude greater than GPT-4.\n\n**Daniel Filan** (00:30:27): Okay. Yeah, I have roughly 10^7 seconds in 100 days.\n\n**Jaime Sevilla** (00:30:31): Nice.\n\n**Daniel Filan** (00:30:32): So 5x10^25 is what we currently have, and 10^28 is what we could do if people spent 100 days training. I guess you could spend a couple 100 days training, but maybe 1000 days training is like… Should we expect people to at some point just-\n\n**Jaime Sevilla** (00:30:48): Train for longer?\n\n**Daniel Filan** (00:30:49): Yeah, three years of training.\n\n**Jaime Sevilla** (00:30:50): This is actually a fun ongoing conversation within Epoch, which is whether we expect training runs to go for longer or higher. So I already talked about what’s the incentive to do it for lower, which is that all these algorithmic improvements are happening and hardware also gets better over time, which naturally makes you want to shorten your training runs. But the reasons why you might want to lengthen it, one of them is just raw output, right? If you train for 10 times longer, then you get 10 times as much compute, and this is something pretty straightforward. Also, if you train for longer, that means you need less GPUs to reach a given level of performance and, also, you need less power to power those GPUs in the first place.\n\n**Daniel Filan** (00:31:38): Just less joules per second, because you have fewer GPUs and so, at any given second, you’re using fewer joules?\n\n**Jaime Sevilla** (00:31:45): That’s right. So there are these incentives for training for longer and, at this point, it’s not obviously clear which way the balance tips at this moment. What we have seen historically is that there is not a clear trend in training runs, but there’s overall an increase that we have seen from people training for one month seven years ago, to training for three months right now. For the training runs where we have information, training for 100 days seems to be a typical training run length. I think, right now, my very weak epistemic status is I expect training runs to become longer, and I could expect them to become up to twice as long and maybe up to three times as long as they are now. Longer than that, it starts becoming a harder ask, because of these factors that I mentioned, and also because just sustaining a training run for more than a year, that’s technically very challenging.\n\n**Daniel Filan** (00:32:58): Right. There’s some rate of random things going wrong, like something crashes, there’s a power outage or something?\n\n**Jaime Sevilla** (00:33:04): That’s right.\n\n**Daniel Filan** (00:33:05): Gotcha. Okay, thinking about this number of “how much computation is there available for AI training?” So you’re saying 10^28 FLOPs - “FLOPs” being “floating point operations” total - if you want to train for 100 days on one year of somebody’s production of H100 GPUs, right?\n\n**Jaime Sevilla** (00:33:28): That’s right.\n\n**Daniel Filan** (00:33:28): Do you have a sense of how that number grows over time? In 2030, is that number going to be like 10^29 or is it going to be like 10^35?\n\n**Jaime Sevilla** (00:33:38): So this is something where I don’t yet have very well-developed intuitions; we’re looking into this at the moment. My sense is that this could plausibly go up by a factor of 10 somewhat easily, that will correspond to… I really don’t know these numbers off the top of my head.\n\n**Daniel Filan** (00:33:59): Fair enough.\n\n**Jaime Sevilla** (00:34:00): Do you want me to check them?\n\n**Daniel Filan** (00:34:01): Yeah, sure.\n\n**Jaime Sevilla** (00:34:08): Okay, so my understanding is that, right now, TSMC is dedicating around 5% of their advanced node wafer production to making NVIDIA GPUs. I think it’s quite plausible that that might increase by an order of magnitude if they decide to prioritize AI enough and if they are able to solve the packaging constraints and high-memory bandwidth constraints I mentioned earlier. So I think it’s quite plausible that they will be able to be producing up to 10 million state-of-art GPUs per year.\n\n(00:34:48): If you were to train on that, that would allow you to reach scales of up to 10^29 FLOP or so. Maybe you increase this a bit, because you might not only use the production from a single year - maybe you stockpile and use the production from previous years as well. And, also, there will be a few advances in the performance of the GPUs; we’ll have better GPUs in the future. We already have the B200 on the horizon, and there will be more in the future, I am sure. So I think that, all in all, I think it’s good to think that, by the end of the year, if you were to dedicate the whole production of GPUs on a single training run, you could plausibly reach up to 10^30 FLOP. That seems reasonable.\n\n(00:35:53): It’s quite unlikely that you’re going to be able to use the whole stock of GPUs on a single train run. First of all, companies are going to fight with each other, different actors are going to want to just do their own training run, which means that the resources are going to be naturally split, and perhaps some companies will want to use a large part of their GPU resources for inference.\n\n(00:36:19): For example, right now, if you just look at Facebook: Facebook bought 100,000 H100 GPUs just last year, and they plan to have a fleet equivalent of 600,000 H100 GPUs by the end of the year, if I’m not mistaken. But they’re not using that many resources on their training runs, they’re using only a small fraction of those for training. Most of it is being used presumably for inference right now, and this might continue in the future, depending on how much value labs assign to developing these new models.\n\n**Daniel Filan** (00:37:01): Yeah, wasn’t there an [Epoch post by Ege Erdil](https://epochai.org/blog/optimally-allocating-compute-between-inference-and-training) saying that you should expect companies to use about as much computation on inference as training?\n\n**Jaime Sevilla** (00:37:09): That’s right. He had this neat theoretical argument where he argued… So there are ways in which you can train a model for longer without altering its performance, but make it more efficient at inference. The most straightforward way of doing this is you train a smaller model, but for longer. So, in the end, it has the same performance, but this is a smaller model, so it’s going to be more efficient to run at inference time.\n\n(00:37:39): And if you think about this and you put yourself in the perspective of a company that wants to reduce the total compute that you use between training and inference, then naturally what you want to do is try to make these two equal, because that’s what’s going to minimize the total expenditure of compute that you’re going to be making.\n\n(00:38:00): There’s some caveats here, like inference is usually less efficient than training, the economics for inference and training are not exactly the same. And right now, my understanding is that this is not happening. Well, I think this is not happening in companies like Meta. I think that this might well plausibly be happening in companies like OpenAI, in which they use a very large amount of resources for training. It is quite plausible, given what we know about how much inference is going on at OpenAI, that their yearly budget for inference is similar to their yearly budget for training.\n\n(00:38:40): But at this point, this is something that I think is informative and I think it has this neat compelling force as an argument, but this is also something that I want to get more evidence on whether this is actually going on before relying on it a lot.\n\nAlgorithmic improvement\n-----------------------\n\n**Daniel Filan** (00:38:55): Fair enough. So way earlier, when I was asking you how AI was going, the second thing you mentioned was algorithmic improvements. A post people can read about this is “Algorithmic progress in language models” by Anson Ho, et al. Well, it’s a [blog post](https://epochai.org/blog/algorithmic-progress-in-language-models) and it’s also a [paper](https://arxiv.org/abs/2403.05812), if you’ve got time for that. The way I parsed it was that there’s something like a 2x speed-up in training models to the same loss every eight months or so. Maybe the error bar was 5 months, 14 months. So my understanding is the way this worked is that you picked some degree of loss that you wanted to get to, so some level of performance, and then you tracked how much computation would it take you over the years to reach this level of performance, and that’s where you get this number from.\n\n**Jaime Sevilla** (00:39:49): Yes and no. So this is the abstraction that we’re going for, and we are hoping that this will be one of the applications and interpretations of the model that we built.\n\n(00:39:59): Actually, the data that you have is very sparse, there’s very few people who are doing exactly that experiment, exactly the level of performance that you care about. So what we did is a bit more general in that we just look at lots of models that have been already trained - this is an observational study - and we looked at “what performance did they achieve in terms of perplexity, of the loss that they achieved on the text?” How well they were able to simulate the text that they were tested on, what was their scale in terms of model size and data, and then, also, in which year they were trained. And, essentially, we just fit this regression, which is like, “Well, I’m giving you the model size, I’m giving you the amount of data, and I’m giving you the year that it was trained on.”\n\n(00:40:55): And this model tries to predict, what is the performance that the model achieves? And we fit 14 different equations that tried to combine these terms in different ways and finally chose one of them that intuitively seems to resonate with how we think that scale and performance relate to each other, and also the role that we think algorithmic improvements play in this.\n\n**Daniel Filan** (00:41:23): Part of what I’m wondering here is, how sensitive is this number to the specification of how exactly you define the question you’re asking? Conceptually, if I picked a different loss level, would that change it from every eight months to every three months or to every two years?\n\n**Jaime Sevilla** (00:41:45): Yeah, so I’m going to introduce you to the bane of my existence, which is scale dependence. So through this model, one big assumption that we introduce is that these algorithmic improvements, they work independently of the scale that the models are trained on. Here, we make this arguably unrealistic assumption, which is that if you come up with a new architecture, then the new architecture is going to help you get a fixed level of improvement, no matter if you are training on a larger scale or a smaller scale.\n\n(00:42:21): Now, this is arguably not how things work. It could be quite plausible that, for example, we think that the transformer scales much better than recurrent architectures right now, but this might be only if you have enough scale for the transformer to “kick in” and start at this great properties of scaling. If you’re working below that scale, you might be better off with a simpler architecture. This is not something implausible to me. What this means is that our estimates might not be sufficiently accounting for this difference in how you should expect improvements to be better at the frontier of compute or whether you should expect them to be better for small-scale budgets.\n\n(00:43:16): And this matters, because there are two reasons why you care about algorithmic improvements. One of them is that they help frontier runs to be much more efficient and reach new capabilities. The other \\[reason\\] that you care about this is because this helps a wider amount of people train models with a certain level of capabilities, right? So depending on which of these two use cases you care about, you’re going to care more about innovations that work better at scale or innovations that work better at the small scale and small compute budgets.\n\n(00:43:48): This is something where I want to have a better, more scientific understanding of to what extent this is the case, and try to look at the specific techniques that drive this algorithmic improvement and try to see, at which scale were they first discovered and at which scales do they apply? And does the efficiency of the technique change, depending on which scale it is applied at? I wouldn’t be surprised to find that, but right now, we don’t yet have a systematic study showing this.\n\nAlgorithmic improvement and scaling laws\n----------------------------------------\n\n**Daniel Filan** (00:44:20): One thing that I’m confused about when I’m thinking of algorithmic improvements is: people authoritatively tell me that there’s these things called “scaling laws” for language models. And these scaling laws say, “Look, it’s this formula and you put in how many parameters your model has and you put in how many tokens you’re training your language model on”, and it outputs what loss you expect to reach on your dataset. I thought that if I knew the number of tokens you were training on and I knew the number of parameters your model had, and if I assumed that you were looking at each token just one single time, so only training for one epoch, which I gather is what people seem to be doing, I thought I could figure out how much computation you were using to reach a given loss. So are algorithmic improvements things that are changing these scaling laws, or are those ways of better using computation within the confines of those scaling laws, or something else?\n\n**Jaime Sevilla** (00:45:19): So scaling laws are defined with respect to a specific training method and with respect to a specific architecture. So for example, in the famous [Chinchilla scaling law paper](https://arxiv.org/abs/2203.15556) by Hoffmann et al from DeepMind, they study this particular setup in which they have a transformer and they define very precisely, “How are we going to scale this? How are we going to be making this bigger?” There’s lots of prescriptions that go into this. For example, when you scale a model, you have lots of degrees of freedom on whether you add more layers or whether you make the model wider to have more neurons per layer.\n\n(00:45:56): So these are all other considerations that affect the end result, the scaling laws that you’re going to fit. You can think of these algorithmic improvements as going beyond the specifications of the particular confines in which the scaling law was originally studied and trying to introduce, well, different tricks, like new attention mechanisms in the architecture. Or maybe you’re training on different data that has higher quality and allows you to train more efficiently. Maybe you change the shape of the model in a different way. These are all the ways that you can escape the confines of the scaling laws.\n\n**Daniel Filan** (00:46:40): So is this basically just saying that scaling laws… I thought of scaling laws as somehow these facts of nature, but it sounds like you’re saying they’re just significantly more contingent than I’m thinking. Is that right?\n\n**Jaime Sevilla** (00:46:51): I think that this is right in an important sense, and maybe there’s a wider sense in which maybe there are a bit more general. In the experiments on these scaling laws, we see that the scaling laws study a particular setup. We can make some assumptions about how much this is going to generalize, but it is tricky. And there’s, right now, dozens of scaling laws papers that study different setups and arrive at slightly different conclusions.\n\n**Daniel Filan** (00:47:26): Again, to stick in this scaling laws frame: you’ve written down a sample scaling law on your piece of paper, so total loss is some irreducible loss, which we’re going to call E, plus some constant divided by number of tokens to some power, plus some constant divided by number of parameters to some power.\n\n(00:47:46): And, basically, the things that determine a scaling law are what you think the irreducible losses are, and then, for parameters and dataset size, what you think the exponents of those are, and also, what the constant factors at the front are.\n\n**Jaime Sevilla** (00:48:04): That’s right.\n\n**Daniel Filan** (00:48:05): If I’m thinking about algorithmic improvements, are those mostly changing the exponents or are they changing the constant factors? It seems like this is going to matter a lot, right?\n\n**Jaime Sevilla** (00:48:14): It does matter a lot, and this goes back to scale efficiency. So if you were changing the exponents, then the efficiency of the improvements will change with the scale. But the way that we model it in the paper is essentially as a multiplier to the effective model size and effective dataset size that you have.\n\n**Daniel Filan** (00:48:34): I guess that’s not quite changing any of the constants, but it’s like, somehow you’re using something instead of N and… Okay, okay. I guess that suggests that maybe you could create a meta-scaling law, where you have the scaling law with these constants and the constants are varying over time, or something. Is this meta-scaling law easy to write down? I feel like someone could get that on a T-shirt.\n\n**Jaime Sevilla** (00:48:59): Yes. I mean, we have 14 different candidates for that in our paper.\n\n**Daniel Filan** (00:49:03): Okay, all right. So I guess people should take a look. Part of the reason, it seems like, someone would want to ask this question is basically trying to ask, “Okay, if I’m thinking about AI getting better, is that mostly because of increasing computation or is that mostly because of algorithmic progress or is that mostly because of increasing the amount of data we’re putting into models?” I think you frame it like this to some degree in the blog post.\n\n**Jaime Sevilla** (00:49:28): That’s right.\n\n**Daniel Filan** (00:49:29): One way that this could be misleading would be if algorithmic progress were driven, to some degree, by increasing availability of computation. For instance, maybe if you have way more computers, you can run way more experiments, and you can figure out better algorithms to run. Do you know to what degree this is what’s driving it?\n\n**Jaime Sevilla** (00:49:46): Yeah, you’re right on the money here. This is something that will be quite crucial for how AI will play in the future, the extent to which this algorithmic innovation is itself being driven by compute. To be honest, I don’t have a great answer at the moment. My personal belief is that it actually plays a large factor, and this has been informed by some informal conversations that I’ve had with people in different labs and rumors that we’ve heard, where people say, “We’re very limited by compute. We don’t hire more researchers, because a researcher will just take up precious compute resources that our researchers are already using for trying to come up with better ways of training the models.”\n\n(00:50:32): So it seems that, to a degree, at least in some labs, people have this notion of, “Our research is compute-bound, our research is also being greatly determined by the access that we have to computing resources.” And this sounds quite reasonable to me. The main way that we learn things in science is you run an experiment, you see if it works, and then you try it again in slightly different variations. And, particularly, it seems that testing whether a problem scales, testing whether a technique scales is very, very important for making these advances, so that naturally limits a lot the amount of advances that you can \\[make\\] if you’re constrained by a compute budget.\n\n(00:51:21): And this might have this huge relevance for how the future plays out, because imagine that we’re in a world in which you’re bottlenecked by ideas, you’re bottlenecked by having more researchers that can do that. Plausibly, in the future, we’re going to have AI that’s going to be really good at coming up with ideas and substituting for a human scientist and coming up with these hypotheses of what you should train, and if you are bottlenecked by ideas, this could quickly lead to massive improvements in algorithmic efficiency.\n\n(00:51:52): But if, instead, you’re being bottlenecked by compute, then it’s like, well, sure, you have all of these virtual researchers that are going to be working for you and coming up with great ideas, but they’re still going to have to go through this bottleneck of, we need to test their ideas and see which ones work. And they might be more efficient at coming up with ideas and still this will lead to a substantial increase in algorithmic progress over time, but this might be much more moderate than in the other world.\n\n**Daniel Filan** (00:52:23): So I guess this gets to the question of what the advent of really superhuman AI is going to look like. I think, classically, people have thought of, “We’re just going to have tons of AI AI researchers.” I mean, if the bottleneck is compute, compute doesn’t just… We don’t just get it from rocks, right? Some people are building machines and figuring out how to do things more efficiently. Does that suggest that the singularity is going to be entirely contained within the Taiwan Semiconductor Manufacturing Company?\n\n**Jaime Sevilla** (00:52:57): Fun question. I mean, right now, the two parts that go into AI progress are you have the hardware manufacturing, but then you also have the software companies, that are a completely separate entity, that are coming up with these ways of training them. So, by default, I guess that we will expect something like that, but there will be this really vested interest in both the semi-facturing \\[semiconductor manufacturing\\] company, but also the AI companies, to apply AI to increase their own productivity.\n\n(00:53:28): I think, particularly, this very naturally happens within AI labs, especially because AI is very good at coding, AI is very good at the things that are useful for doing AI research. I think it’s very natural that people will want to see, “Can we use this to improve the productivity of our very expensive employees?” In hardware manufacturing, it also feels like this natural multiplier where, if you are able to use AI to increase the productivity of TSMC, then sure, they’re going to be able to produce much more, this is going to lower the prices of compute, this is going to allow you to train even larger and better models that help you achieve better levels of generality and capability.\n\n(00:54:11): So to an extent, I think that the intuition I have is that I do expect that some of the early use cases for very dramatic increases in productivity are going to be in AI companies, and I will not be surprised if semiconductor manufacturing companies are next in line.\n\n**Daniel Filan** (00:54:31): Okay. And I guess that suggests: a lot of work being done both in AI in general, and a lot of what you’re checking, is scaling laws for language modeling for these predictive tasks. I wonder if that suggests that actually we should be thinking much more about AI progress in whatever a good analog of making computer chips is? I don’t even know what a good benchmark for that is in the AI space. I don’t know, do you have thoughts about that?\n\n**Jaime Sevilla** (00:55:01): I think what you were pointing at is, maybe one type of task that we really care about is to what degree AI is going to be helpful for improving chip design, for participating in the processes that go on within a semiconductor manufacturing company. Is that what you were pointing at?\n\n**Daniel Filan** (00:55:20): Yeah.\n\n**Jaime Sevilla** (00:55:22): I think that this is right to an extent. It’s tricky to design good benchmarks that really capture what you care about. The benchmark you really care about is does it actually improve the productivity, which is something you will see in the future once you get the models deployed there. But it will be interesting to start developing some toy settings which try to get at the core of: what will it mean to increase the capacity of this model?\n\n(00:55:48): So, for example, one of my colleagues at Epoch, [JS](https://jsdena.in/), has been thinking about, “what kind of benchmarks could be cool to have and will be informative about what we’re thinking about in AI?” One of the things he was considering - this is more on the software side, but he was considering “Can we have a benchmark that’s about predicting the results of an AI experiment?” And again, this is more on the AI company side, but this could act as a compute multiplier, right?\n\n(00:56:17): Because if you only have \\[enough\\] compute to test 10 ideas, then you want to be picky with which ideas you test. And it’s better if you have these powerful intuitions about which ideas might work. So to the extent that AI can help provide you with these intuitions and guide your search for which techniques to try, it’s going to allow you to effectively increase the range of options that you’re considering, quickly discard ideas that you think are not going to work, and really focus on the ones that are worth testing and trying at scale.\n\nTraining data\n-------------\n\n**Daniel Filan** (00:56:55): Sure. So we’ve talked a bit about algorithms, we’ve talked a bit about computation. I think a lot of people think of AI as basically this three factor model. There’s algorithms, there’s computation, there’s data. You pour those three into a big data center, and out plops GPT-5, right? Is this basically a good model or is there something important that we’re missing here?\n\n**Jaime Sevilla** (00:57:18): So I think that this is a good model, though I will make this distinction, which is that you really care about which constraints are taut at a given moment. And at this moment I will say that compute is a taut constraint whereas data is not. So right now, models that we have are being trained on… for the ones where we know the dataset size, they use around 50 trillion tokens of data. And the size of Common Crawl, for example, is 100 trillion tokens of data, roughly.\n\n**Daniel Filan** (00:57:53): And Common Crawl is just roughly the internet or is it half the internet?\n\n**Jaime Sevilla** (00:57:58): It’s a fifth of the internet. So if you look at the amount of content in indexed public text data, that will be 500 trillion tokens of data.\n\n**Daniel Filan** (00:58:11): Okay. So Common Crawl is 100 trillion tokens and people should think of a token as being 80% of a word on average.\n\n**Jaime Sevilla** (00:58:18): Mm-hmm.\n\n**Daniel Filan** (00:58:19): Yeah, roughly 100 trillion words of data that you can get from Common Crawl.\n\n**Jaime Sevilla** (00:58:24): That’s right.\n\n**Daniel Filan** (00:58:25): And so you’re saying that data is not the taut constraint right now?\n\n**Jaime Sevilla** (00:58:28): It is “not” - maybe there’s some uncertainty here. Maybe in the future AI companies won’t be able to use publicly-indexed data anymore to train their models. There are some complications here, and also there are some domains for which you really want to have data there. If you really care about accelerating experiments, you probably want to have data with coding. You want to have high-quality data about reasoning about AI, and you might really want to expand those kinds of data.\n\n(00:58:59): But to a first-order approximation, the reason why I think we’re not seeing larger-scale training is because there aren’t enough GPUs. If people had more GPUs, they would find ways of gathering the necessary data. So in that sense, I think that compute and algorithms are more important to track at the current margin than data is.\n\n**Daniel Filan** (00:59:20): Okay. Well, even though it’s less important than the other things, I do want to talk about it because you had this interesting paper - I mostly just read the [blog post](https://epochai.org/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data) \\- [“Will we run out of data? Limits of large language model scaling based on human generated data.”](https://arxiv.org/html/2211.04325v2). This is by [Pablo Villalobos](https://x.com/pvllss) and colleagues.\n\n(00:59:38): So my understanding of just the top line results is that on the public internet, there’s roughly 3x10^14 tokens that you can train on, which is 300 trillion if I can do math, which is unclear. So roughly that many tokens to train on. And you would have a model with roughly 5x10^28 floating point operations that you’d use to train on that. And roughly, in 2028, we’ll just be using all of the training data. Is that roughly a correct summary or is there something important that I’m missing?\n\n**Jaime Sevilla** (01:00:16): I think that that sounds about right. So it’s interesting to compare this with the size of the… We were talking before about, if you were using all the H100s that are produced in a year, what’s the largest model that we can train? And we arrived at 10^28 FLOP or so, right? If you are using this 100 trillion tokens of data in order to train it, in order to estimate what’s the largest model that you could train, maybe one approximation that you can do here is think about the Chinchilla scaling laws that can inform you about: with this amount of data, what’s the largest model you can train?\n\n(01:00:56): Roughly, you want to use 20 tokens per parameter of the model for training it according to Chinchilla-optimal scaling (lots of quote-unquotes here). So that would mean that if (let’s say) you use these 400 trillion tokens of data that are all on the indexed web, and that will allow you to train a model that has… Whoo, math -\n\n**Daniel Filan** (01:01:32): 20 trillion.\n\n**Jaime Sevilla** (01:01:33): 20 trillion parameters, so then that will lead you to an amount of compute. So the amount of compute is roughly six times the amount of data times the amount of parameters. So that’s going to be, this is 4e14, 2e14, so that’s-\n\n**Daniel Filan** (01:01:59): Nope, 2e13. Because it’s 400 trillion but only 20 trillion.\n\n**Jaime Sevilla** (01:02:06): Oh, yeah, that’s right. And here I multiply by six. So this is going to be 8 x 6…\n\n**Daniel Filan** (01:02:20): Well, it’s 12 x 4, which is 48.\n\n**Jaime Sevilla** (01:02:22): Yes. So 50 essentially. So yes, 5x10^28, which is what we arrived at before, right?\n\n**Daniel Filan** (01:02:30): Yep. 5x10^28. Oh man, it’s nice to see that in action. I was actually wondering where that number came from. Okay, cool.\n\n**Jaime Sevilla** (01:02:38): Nice. So what you see is that now there is enough data on the indexed web to train something that would be five times greater than what you would be able to train with all the GPUs in the world.\n\n**Daniel Filan** (01:02:55): So this is interesting to me because there are a few coincidences here, right? One is the thing that you’re just noting, that \\[with the\\] amount of data you can use, if you had five years of global GPU production, you’d be able to train on all of it, Chinchilla-optimally.\n\n(01:03:11): Another thing I noticed is: I looked back on the training compute growth paper and I looked at, okay, what’s the biggest model right now and when do we hit 5x10^28 floating point operations? And roughly, depending on whether it’s 4X or 5X and depending on whether I can multiply, it’s somewhere between 2028 and 2030.\n\n(01:03:33): So somehow there’s this strange coincidence where if you extrapolate compute growth, you get enough compute to train on the whole internet, also just at the time when we are projected to train on the whole internet: is that a coincidence or is that just a necessary consequence of frontier models being trained roughly optimally?\n\n**Jaime Sevilla** (01:03:55): No, I think this is a coincidence. What has been driving the amount of data on the internet is just adoption of the internet and user penetration rates, which have nothing to do with AI and GPUs. So I think this is just a happy coincidence.\n\n**Daniel Filan** (01:04:11): Well, the 2028 number was when it was extrapolating how much data models were being trained on, right, so that does have to do with AI.\n\n**Jaime Sevilla** (01:04:18): So the number that we derived now, this 5x10^28, this is based on the amount of data on the indexed web, which has nothing to do with AI, right? What has to do with it is, when you do this extrapolation, when do you hit that amount of compute? What is a coincidence is that the training run that you can do on this amount of data is so similar to the training run that you can do on the amount of compute for the GPUs.\n\nCan scaling produce AGI?\n------------------------\n\n**Daniel Filan** (01:04:56): Gotcha. So now I want to ask a bit about the future of AI. So a while ago you guys put out this post called [“The Direct Approach”](https://epochai.org/blog/the-direct-approach), which was roughly saying, okay, here’s a way of turning loss into how much text an AI can write before you can distinguish it from a human. And roughly it was a way of saying: if you want an AI that is smart enough to write a tweet just like a human could, that happens at this amount of computation. If you want to get an AI that can write a scientific manuscript about as well as a human can, that happens at this amount of computation, (modulo some fudge factor, which is very interesting to talk about).\n\n(01:05:34): But if I looked at those numbers, it said maybe I needed somewhere between 1x10^30 and 3x10^35 floating point operations to get AGI that could be roughly as smart as people. But when I’m training on all the publicly available data on the internet, that was only enough to use with 5x10^28 floating point operations. Does that mean that scaling just isn’t going to work as a way of getting AGI?\n\n**Jaime Sevilla** (01:06:03): So I will say two things here. The first one is that I would mostly think about this method as us trying to estimate this upper bound, because presumably you don’t need to be able to mimic humans perfectly in order to write scientific manuscripts of perfect quality.\n\n(01:06:22): This is this unrealistic goal, in which your model’s so good at mimicry that you cannot tell it apart, but it doesn’t have to get to be that good in order to have a transformative effect on the economy or produce manuscripts of quality. It’s much harder to write something mimicking someone than to write something useful, right?\n\n**Daniel Filan** (01:06:47): Fair enough.\n\n**Jaime Sevilla** (01:06:51): The other thing that I would say is that I’d caution people \\[against taking\\] these numbers very seriously. I think that right now we just don’t have a really good understanding of when do you hit different levels of capabilities, at which scales? I think we have this rough notion about: yes, once you increase in scale, you get more and more capabilities and more generality. And if you combine that with certain scaffolding techniques, this might lead to AI that’s widely very useful. But when it comes down to saying, “well, this is going to happen at this amount of FLOP exactly”, it’s a very rough job.\n\n(01:07:33): There are maybe some suggestive numbers that I will float around. One of them is this, that comes out of [this paper](https://epochai.org/blog/the-direct-approach) \\[The Direct Approach\\] trying to estimate “okay, in this setting, how much compute will you need to train a model if scaling laws can be sustained for 10 more orders of magnitude,” which is also another big “if”. What other numbers are suggestive to think about?\n\n(01:08:01): So one thing that’s quite interesting is… So I forget exactly who it was right now, it might have been \\[Ray\\] [Kurzweil](https://en.wikipedia.org/wiki/Ray_Kurzweil), who 20 years ago made some predictions about: when will we have AI that essentially will pass the Turing test? And they said something like, “Well, we forecast the point where you will have enough compute to match the human brain will happen somewhere in the ’20s.” That happened. It’s insane, right? They got that right. It is actually true that we have now models that essentially pass the Turing test in that they can converse with humans and have a meaningful conversation with them. So it’s quite insane that just by looking at this biological construct of the amount of computation going on in the brain and with some wild back of the envelope calculations, they were able to do that.\n\n(01:08:57): Is there an analogous thing that we can do to talk about when we’ll have AI that’s really good and can do essentially everything that humans can do? So there was [this report](https://drive.google.com/drive/u/1/folders/15ArhEPZSTYU8f012bs6ehPS6-xmhtBPP) by [Ajeya Cotra](https://www.openphilanthropy.org/about/team/ajeya-cotra/) where she looked at a few of these biologically-inspired quantities. I think that one that has some hold in my thinking on the upper end is the amount of computation that was used to essentially run evolution and give birth to the human species, which she estimates to be around 10^40 FLOP - what you’d need to rerun evolution. There’s lots of caveats going into that. Also, if you account for \\[the fact that\\] we might have gotten lucky with this run, but maybe it could have taken much longer. Maybe a more conservative estimate could be even up to 10^42 or even 43 FLOP - what you’d need to recreate human evolution. And that feels to me like that’s the frontier.\n\n(01:10:09): If we had that amount of compute, then it’s no longer about compute, it’s more about do we have the necessary techniques to use it productively to create intelligence de novo?\n\n(01:10:23): So this is now something that has this hold in my thinking about \\[AI\\]. I don’t have a very great idea about at which level of compute we will see AI that can participate as a fellow worker in the economy, but it’s probably not 10^26 because we are pretty much already there, and I don’t think that this is right on the horizon. It’s probably not 10^40 FLOP - that seems like too much.\n\n(01:10:53): If you had that amount of compute, you would be able to, again, rerun evolution. You probably can do better than evolution at creating intelligence with current techniques: I would think so. I think it’s not crazy to argue that. So then it’s like, well, it’s somewhere in between that. And where exactly, at which order of magnitude? I don’t know. Maybe my distribution looks pretty uniform between 10^26 and 10^36 or so.\n\n**Daniel Filan** (01:11:22): Say instead of that I’m uniform just between 10^26 and 10^40 floating point operations to get AI that’s smart enough to do all the science \\[and\\] technology instead of us. Most of that is higher than the 5x10^28 that we’re going to use to train on all the publicly available data on the internet.\n\n**Jaime Sevilla** (01:11:43): That’s right.\n\n**Daniel Filan** (01:11:44): Does that suggest that scaling language models is not going to be the thing that gets us AGI?\n\n**Jaime Sevilla** (01:11:50): So I think that people will become creative once data becomes a taut constraint. So again, data right now, I don’t think is the taut constraint; I think it is compute. The datasets that people train these models on, at least when training was happening publicly… It was trained on things, again, like [Common Crawl](https://commoncrawl.org/) or [The Pile](https://pile.eleuther.ai/), which are datasets that were put together by software engineers in essentially their free time. There were not these very large, industry-funded projects to get the datasets.\n\n(01:12:26): To an extent, I think that the paradigm is changing, and now OpenAI is investing a lot of resources in getting data, especially for fine-tuning purposes. But overall for the pre-training, it seems that companies have been able to get away with just using the data that already exists and is easily available. Once this ceases to be the case, there is this huge incentive to come up with ways to increase the efficiency of the data, ways to get more data out of other places, and it’s interesting to think about what these places might be.\n\n(01:13:07): So one thing that we are seeing now is people are training models that increasingly deal with more modalities. So GPT4o, for example, right now is quite proficient at parsing images and can also produce images together with DALL-E… I’m not sure if they use DALL-E or if its native image generation. Well anyway, models right now are increasingly more multimodal, and you could use data from other modalities to try to push back this deadline of how much data you have available for training. Now I don’t think that this will actually lead to… if you just look at image and video data, I don’t think that this will be a huge delay.\n\n(01:13:53): Maybe this buys you a couple more years of scaling. Maybe this buys you an order of magnitude of compute. Essentially, I think that this increases the amount of data you have by a factor of three, and the amount of training that you can do increases quadratically with the amount of data you have. So maybe this buys you an order of magnitude of scaling, roughly. So what do you do if you have already trained on all text data, on images and video? What else do you turn to?\n\n(01:14:24): And one thing that’s interesting to think about is the model outputs themselves and synthetic data. So right now OpenAI, if I recall correctly, is producing in the order of 100 billion tokens per day, which roughly extrapolates to 40 trillion tokens per year. So 40 trillion tokens, that’s substantial. That’s pretty high. If you were to keep that up for 10 years, then you would have produced an amount of data that’s as large as the size of the indexed web today. If that data turns out to be useful for training, then you might be able to use this in order to continue scaling.\n\n(01:15:14): It is not completely clear at this moment whether that’s going to be useful. So there have been some studies where if you train models on regurgitated model outputs, there’s this phenomenon called “model collapse”, in which the quality of the model ends up degrading. And we don’t have a really good understanding of that, not just yet. But again, this is in a sense the early days of dealing with this problem, because it hasn’t been that big of an issue yet. Once it becomes \\[one\\], I expect the forces that be to push really hard to figure out how do we go past this?\n\n**Daniel Filan** (01:15:53): Here’s how I’m thinking about this. It seems like if it’s really the case that we’re running out of data in 2028, or running out of data to train on, and it doesn’t look like we’re having AI that can really just take over science and technology creation from us at that scale…\n\n(01:16:18): There’s this question people ask which is: how many big ideas do we need to have before we get superhuman AI? And if it’s the case that we’re going to run out of data to train on before we get there, it seems like that puts a lower bound, saying we need at least one big idea until we get to super-smart AI. I’m wondering if this seems right to you or even a useful way of thinking about things?\n\n**Jaime Sevilla** (01:16:43): To some extent. I think I am not sure how big of an idea it’s going to be, because it might be just “use synthetic data”, and it works. It’s like, well, it’s a good idea - it worked!\n\nWhen will AGI arrive?\n---------------------\n\n**Daniel Filan** (01:16:55): I want to talk a little bit more about what you can say about AGI: AI that can take over scientific and technological progress from humans. So we don’t know exactly what level of loss it’s going to be at, but is there something to say about, is it five years away or is it 50 years away? Let’s start with that question of timelines.\n\n**Jaime Sevilla** (01:17:23): So maybe the naive way that you can think about this is: well, we were saying before, it’s probably not going to be 10^26 FLOP. That seems too little. It’s probably not going to be 10^40 FLOP. That seems too much. It’s going to be somewhere in there you get the required level of compute to make AI that can substitute for humans in scientific endeavors a reality.\n\n(01:17:53): Then you just think: well, how fast are we going and how much \\[bigger\\] do we expect compute to go? Right now, my picture of naively what I would expect compute to go like is: well, it goes like this - very, very fast - for a few years, perhaps until the end of the decade, perhaps a little bit more than that, and then eventually it has to slow down.\n\n(01:18:18): How much it slows down depends on many complicated factors. On whether, for example, we have already found the successful application of AI that allows you to increase the growth rate of the economy, or whether the field stagnates - it’s still growing, still growing, but you are now bounded by how fast the economy grows overall, and right now it’s growing 3% per year, which is nothing compared to 4X per year.\n\n(01:18:46): Holding all of these things in mind: okay, maybe we go 4X until the end of the decade, and then by the end of the decade we are training something that’s 10^29 FLOP or so. And then if you keep going, I don’t know, maybe something reasonable to expect is that somewhere between 2030 and 2050 you might cross 10 orders of magnitude of compute more or something. It starts becoming quickly very complicated. I think that once you are past 10^36 FLOP per year, you start getting into territory where you might just melt the earth in the process.\n\n**Daniel Filan** (01:19:38): Just because of the heat produced by doing the training?\n\n**Jaime Sevilla** (01:19:41): That’s right.\n\n**Daniel Filan** (01:19:42): I wonder at this stage… it seems right now that computations available seems to be a pretty good proxy for AI capability because we can hold fixed, there’s just this pool of high-quality human data that we’re drawing from. But if you’re right that we’re running out of this data in 2028, it seems maybe at that point computation is just no longer going to be such a good proxy.\n\n(01:20:08): Maybe we’re just going to have to think way more carefully about algorithmic improvements or how you can make use of various data. Do you think that’s right? And do you think that that reduces the value of just these compute forecasting estimate-style things?\n\n**Jaime Sevilla** (01:20:25): TBD. I think that actually again, right now, my guess would be that data is not going to be the most determinant bottleneck, and this is somewhat driven by this belief that people will make it work.\n\n(01:20:38): People will figure out how we can use, for example, synthetic data here. I don’t know. One example that we have is: recently there was this [AlphaGeometry paper](https://deepmind.google/discover/blog/alphageometry-an-olympiad-level-ai-system-for-geometry/) in which they use synthetic data to train an AI system that was able to solve \\[Olympiad\\] geometry problems, right? And generally, especially in the scenarios in which there exists the right answer, like math or programming, it seems that one naive strategy you can do to generate more data is, you use the latest generation of models to try to generate solutions to problems and you train on the problems that are right.\n\nEpoch AI\n--------\n\n**Daniel Filan** (01:21:20): I next want to talk just a bit about Epoch AI as an organization.\n\n**Jaime Sevilla** (01:21:23): Yes, absolutely.\n\n**Daniel Filan** (01:21:24): Why does Epoch AI exist?\n\n**Jaime Sevilla** (01:21:29): It was born out of my frustration. Again, while I was doing my PhD on artificial intelligence, I was somewhat surprised that no one had yet done a very systematic analysis of the different trends that matter for AI. There was [this post on AI and compute](https://openai.com/index/ai-and-compute/) from OpenAI in 2018 but very little beyond that, which seemed wild to me given the huge amount of interest that AI was creating, at least around me, and how important they think that the technology was going to be for the future. We started this whole process of: let’s systematically track the things that go into developing AI models and study the trends and try to get a better evidence-based, quantitative picture of what the future looks like. And that’s how Epoch was born, essentially.\n\n**Daniel Filan** (01:22:26): When I think of Epoch, I think of it as a mix of [AI Impacts](https://aiimpacts.org/) and [Our World in Data](https://ourworldindata.org/). Do you think this is a fair understanding of what you guys are?\n\n**Jaime Sevilla** (01:22:38): To some extent, yes. This might be underselling also the amount of in-depth research, specifically on AI, that we do. I think a lot of Our World in Data… they are these curators of data that they do not produce a lot of research themselves, but instead, are compiling the collective knowledge of humanity. And this is very good and very useful. At Epoch instead, we are creating the datasets ourselves and trying to generate this original body of work that people are going to be able to use to inform decisions about AI.\n\n(01:23:16): Regarding AI Impacts, they’re also close analogs to what we’re doing in terms of trying to think quantitatively about AI. I think that AI Impacts maybe rely more on surveys and rely more on analogies with other technologies in order to inform what AI is doing. In Epoch we’re being a bit more directed and being like, “Well, no, this is about AI and AI is what we are going to be focusing on.” Trying to understand, keep up to date with what’s happening in the AI world and the latest knowledge that has been produced and all of these concepts like scaling laws and such.\n\n(01:23:50): And then the hope here is - I see Epoch’s AI work as having these three work streams. One of them is we collect this data. The second one is we analyze it. And the third one is we put all our research together to paint these quantitative pictures of the future of AI.\n\n**Daniel Filan** (01:24:11): There’s this work that you guys put in to just have this quantitative picture of the future of AI. One way someone could think about this is: the point of information, the point of knowing things, is that it can change some decisions that someone can make. And the more important the decision that you changed, the more important it was to know the thing. Concretely, do you know what decisions people might be making differently based on what Epoch AI puts out?\n\n**Jaime Sevilla** (01:24:42): This is an excellent question which gets at who are our audiences and who is changing their mind due to our work? And I think a really important lever here has been policymaking, actually. In the last two years we have seen this surge in interest from governments around the world in governing these new AI technologies. In order to decide how to govern it, they want to have this in-depth understanding of which levers drive development and how they can regulate them.\n\n(01:25:23): Compute is this very clear example. It’s this lever that turns out to be very important for AI development. It’s also quantifiable. It’s something that you can exclude people from. And so it’s this very natural lever for governing. The way that I think Epoch data is being used around the world is in order to inform these conversations on, okay, if we want to create compute-based regulation, how do you decide at which compute levels you’re going to impose certain requirements?\n\n(01:25:58): For example, the [Executive Order on AI](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) from the US imposes certain additional requirements on models that were trained over 10^26 FLOP. And I don’t know exactly how they chose the number, but a big suspicion I have is that they looked at our data and they were like, “10^26 is something that no model has been trained on yet. It’s close enough that in a year plausibly companies will be trying to train models this way.” And this is a way in which I could see Epoch’s data being useful, in order to make these important policy decisions.\n\n(01:26:36): More generally, I am hoping that: right now there are many people who are trying to thoughtfully plan for AI and the advent of these new technologies. And I’d want them to be better informed. I’d want them to make decisions that are based on facts rather than vibes of what is happening right now in the field. This seems really important given that this technology might be the next industrial revolution that we live through.\n\nOpen questions in AI forecasting\n--------------------------------\n\n**Daniel Filan** (01:27:06): Sure. If that’s what Epoch AI is for… you’ve got a bunch of work, but not every possible question has been answered yet. I’m wondering: what are the big open questions that you most want to address that don’t have answers yet?\n\n**Jaime Sevilla** (01:27:27): Okay. Big open questions at Epoch. One key thing that we are constantly thinking about is when are we going to reach different levels of automation and how fast will be the transition from a world with little automation to a world with a lot? That seems to be a very relevant input into many decisions happening today about, “Should we try to plan for this period of very fast automation, try to prepare for that? Is this something that’s going to happen in the next year? Is this something that’s going to happen in the next 10 years?” Whether different policies and plans for this technology are actually feasible depends a lot about when this rapid period of automation begins and how long the period itself is going to be. This is something that we think about quite a bit.\n\n(01:28:18): A second important part here is: aside from keeping track of what’s driving innovation, we want to have this in-depth understanding of these factors. We talked before about whether algorithmic innovation has this important component that’s driven by compute. And we talked about why this will be relevant for painting a picture of the future of AI. We think a lot about those kind of things. And also more generally about the different bottlenecks that AI might face in the future. We talked about data already.\n\n(01:28:53): Other things that we are talking about: for example, one funny thing I’ve been thinking about lately is latency. In order to train a model, you need to do a certain amount of serial operations, and this (to an extent) limits the largest training run that you can do. We’ve also started thinking recently more seriously about power and about how much energy you will need in order to train these large machine learning models. More generally, we want to be able to examine critically all of these reasons why the current trajectory of AI might slow down and try to incorporate that into our thinking about when we’re going to reach different levels of automation.\n\n(01:29:36): And finally, we want to think about how this will impact society. Economists have been thinking long about the effects of automation, but I’m somewhat even disappointed that so far there has been very little uptake among mainstream economists in trying to think about the consequences of AI and of having a way of turning computers into workers essentially.\n\n(01:30:04): I think that there’s lots of things that classical models of economy, semi-endogenous growth theory have to tell us about what affects AI might have on the world, and very little work in just straightforwardly trying to apply these already well-developed, well-understood models to the case of AI. This is something that I will be hoping to see more of in the future. We do have a fair bit of this within Epoch, but I would love for mainstream economists to also join the boat and try to drive the knowledge forward with us.\n\n**Daniel Filan** (01:30:42): I also find it weird how little there is in mainstream economics on this. And also… I’m not sure I want to name [names](https://en.wikipedia.org/wiki/Daron_Acemoglu), especially because I haven’t read [the relevant piece](https://economics.mit.edu/sites/default/files/2024-04/The%20Simple%20Macroeconomics%20of%20AI.pdf), but I think there are prominent instances of this that just do not seem very high-quality to me. Actually, related to this, I read your guys’ [2023 annual update](https://epochai.org/blog/epoch-impact-report-2023) or something, and one thing you said was that you would have this report on AI and economic growth at some point during 2024. And I’m pretty excited for that. When should I expect to be able to read that?\n\n**Jaime Sevilla** (01:31:14): Absolutely. We already put out [a report on economic growth](https://epochai.org/blog/explosive-growth-from-ai-a-review-of-the-arguments) last year where we talked about why AI might lead to explosive growth: what are the econ-literate arguments for why you might see explosive growth from AI and also what are the most plausible objections that we find to it. That was more this theoretical exercise of walking through these models of the economy and these high-level considerations for that.\n\n(01:31:47): But the next level for us is trying to build this comprehensive, integrated assessment model of the future of AI that tries to tie together what we know about compute, what we know about scaling laws, with these models of the economy and scientific progress. And the hope here is that in the end we will have a tool that is really helpful for describing, if not realistic, at least illustrative pictures of what the future trajectory of AI might look like.\n\n(01:32:27): Now, when this is going to be out… we have an internal version that works and I find it very insightful. But it’s a very large body of work. This is a very large model that hasn’t been thoroughly vetted and we want to be careful about putting out things that we are not confident in. I think it’s probably going to be at least half a year more before we’re ready to share it.\n\n**Daniel Filan** (01:32:52): Okay. There’s some possibility that we maybe get it by Christmas?\n\n**Jaime Sevilla** (01:32:57): There’s some possibility of that. Yeah.\n\n**Daniel Filan** (01:32:58): All right, I love timelines forecasting. A throughline through many of the things that you said were important or open questions was just understanding the impact of AI. And to me there’s just this key question of: okay, you can train an AI to a certain loss - what does that mean? Epoch has done some work on this in [this “Direct Approach” blog post](https://epochai.org/blog/the-direct-approach). I’m wondering: what should people look at to get a good sense of what does “loss” mean?\n\n**Jaime Sevilla** (01:33:28): Yeah, this is a good question. I think that right now I feel I don’t have a good answer to that. Things have been happening internally at Epoch for us to try to grapple better with this question. Last year we put out a report on [challenges for assessing automation](https://epochai.org/blog/challenges-in-predicting-ai-automation) that my colleague [David Owen](https://scholar.google.co.uk/citations?user=yt4c1UYAAAAJ&hl=en) put out, where he looked at different work that has been done on trying to assess the impact that different AI technologies have had on tasks that are economically useful and trying to see if there was a pattern to which tasks are easier to automate. That will be the holy grail for us, having this way of trying to order all the tasks that are useful and say, “Well, these are more automatable” or “These are less automatable.” Having that kind of notion will be very useful for figuring out how AI automation is going to \\[play\\] out in the future.\n\n(01:34:22): Sadly, the conclusion of that paper is that work so far hasn’t been that good. Every single piece that’s out there disagrees with everybody else and we are just basically very confused about how to think about automatability and when we will reach different levels of capability in an economically-useful sense.\n\n(01:34:48): One thing that we have started doing recently more within Epoch is our own benchmarking program, to try to get a better sense of how fast AI progress is happening in different fields and trying to get a better sense of, well, if you scale these models, what should I expect a 10^28 FLOP model to be able to do? This is to me still this huge open question where I don’t think anyone has a really good answer to that just yet.\n\nEpoch AI and x-risk\n-------------------\n\n**Daniel Filan** (01:35:20): This is the AI X-risk Research Podcast. A bunch of listeners are really concerned about X-risk from AI. And I think probably a lot of people are concerned that if we make really good, really smart AI that might pose an existential risk. Epoch AI in general is a bunch of really smart researchers trying to understand trends in AI, trends in AI progress. Do you think the outputs of your research are going to tell people how to do a good job at making AI? And if so, should people be worried about that?\n\n**Jaime Sevilla** (01:35:54): Sorry, the question is whether the outputs of our work are going to help people, whether this is going to advance how fast AI is being made?\n\n**Daniel Filan** (01:36:02): Yep. That’s right.\n\n**Jaime Sevilla** (01:36:04): I think that to an extent this is true. Having a better understanding of AI naturally helps people build better AI. Now, I think that a lot of the work that we do is things that are already internally known within companies. And I don’t imagine that the work that we’re doing is massively critical for what’s been happening at that scale.\n\n(01:36:32): Now, this is a hard question you need to grapple with, which is that in the end, your work is going to be used in a multitude of ways and you don’t have control over that. And you need to be thoughtful about whether you want to take that trade-off and say, “Okay, we’re doing this. This might make AI go faster, or this might make certain avenues of applications of AI more likely. But then the trade-off is that everyone is going to be better informed and we’re going to be better prepared to deal with that situation.” It’s hard to say.\n\n(01:37:06): Also, one thing that I will say is that it’s also hard to give an answer to how fast AI should be going. There is a world in which you want to slow it down and you want to just have a lot of time to think carefully about how AI is going. But there might be also a world in which you want to go quickly over a period, or you want to advance quickly up to the point where you have AI that’s going to be really helpful for you to try to improve the way that we align the systems and we try to help them do our bidding.\n\n(01:37:46): Right now I’m just very confused. One thing I’ve been thinking a lot about over the last year is risk evaluation in different contexts. And I’m trying to think through different risks. I think that the risk on loss of control, that one is more complex to think about, but for the others, actually right now I’ve been pretty surprised with the uptake of how things have played out so far. The government seems to be doing mostly sensible things with some caveats, but there has been a very reasonable response from thoughtful people to try to anticipate what’s going to happen with AI - what are the risks that are likely to happen in the next couple of years? - and trying to get ready to act if something unexpected happens.\n\n(01:38:40): In that sense, I think I’ve become like, “Well, seems good for society right now. In terms of risk management, people seem to be doing what will be necessary to manage at least the short-term risks about AI.” On long-term risks, it’s so hard to think about. Some days I wake up thinking, “Yeah, maybe having more time and going slowly is going to be better for society.” And other times I’m like, “No, actually this is a risk that we should take. We should go a bit faster.” Or even, right now we might want to go to get capabilities sufficiently high that we can use it to speed up solving this problem and everything else we deal with.\n\n**Daniel Filan** (01:39:28): On this narrow question of “is Epoch figuring out any stuff that big labs aren’t?”, one thing that is pinging for me here is… [Phil Tetlock](https://en.wikipedia.org/wiki/Philip_E._Tetlock) is this academic who studies forecasting and basically studies, “Hey, what if people actually try to be good at it?” He gives a bunch more details, but as far as I can tell, the key criterion is just “are you actually trying to be good at forecasting?” And then everything else flows out from that.\n\n(01:40:04): Basically [the result](https://en.wikipedia.org/wiki/Expert_Political_Judgment) is that if people actually focus on forecasting and get really good at trying, in forecasting geopolitical questions they can do similarly (or maybe better, I forget which) than intelligence analysts at intelligence agencies that have access to classified details and stuff. Does that suggest that Epoch AI actually is in a position to be better at figuring out AI trends than AI companies?\n\n**Jaime Sevilla** (01:40:35): To some extent I think that this is true. Especially, I will even argue that we have one advantage, which is that we are focused on the forest, whereas companies are just focused on the trees. Having details of the trees is very useful: I would love to have more details of the trees. But if you are the one who’s looking at the big picture and putting everything together, that gives you this unique vantage point that others may not have.\n\n**Daniel Filan** (01:41:03): Thanks for working in the beautiful backgrounds of our set. I believe you’re the first guest to do so. I guess wrapping up, are there any questions that I really should have asked but have not?\n\n**Jaime Sevilla** (01:41:19): I’m trying to think, how respectably do I want this podcast to end? No, I think that we have covered the basics. Everything that we have covered was really good. Thank you so much for the podcast. That was really fun, Daniel.\n\nFollowing Epoch AI’s research\n-----------------------------\n\n**Daniel Filan** (01:41:34): Well, before we do close, if people have listened to this, if people are interested in following your research and your work, how should they do that?\n\n**Jaime Sevilla** (01:41:42): Well, first of all, I will welcome everyone to just go right now to their navigation bar and enter [epochai.org](https://epochai.org/) and just interact with our website. We have lots of resources I’m sure you’re going to find very useful. You already mentioned at the beginning of the podcast our [trend dashboard](https://epochai.org/trends). That’s where I would start, where you have all these really important numbers about how fast AI has been developed over the last decade or so.\n\n(01:42:12): Then we also have [our databases](https://epochai.org/data) where you’re going to be able to see all the data points that make up the trends that we’ve been studying, and I think \\[this\\] will help ground your intuitions about the massive scale of these models. And of course, all the research that we put out is publicly on our website. Together with every paper we try to release a companion blog post, which is aimed at a less technical audience and it helps summarize the main insights of what we found out through our research. I recommend that people just spend some time going over our pages. I think if you’re interested in AI, you will find it a very rewarding experience.\n\n(01:42:57): Other than that, [@EpochAIResearch](https://twitter.com/epochairesearch) is on Twitter and you can follow us. I’m also on Twitter [myself](https://twitter.com/Jsevillamol) and somewhat active, so if you want to hear more about my hot blistering takes about AI, I will welcome you to follow me.\n\n**Daniel Filan** (01:43:13): Sure. What’s Epoch AI’s handle on Twitter and what’s your handle on Twitter?\n\n**Jaime Sevilla** (01:43:19): [@EpochAIResearch](https://x.com/EpochAIResearch), that’s the handle for Epoch and mine is [@Jsevillamol](https://x.com/Jsevillamol).\n\n**Daniel Filan** (01:43:26): Okay, great. Well, thanks for coming in and thanks for chatting. This has been really fun.\n\n**Jaime Sevilla** (01:43:30): Yes, I can say the same. Thank you so much, Daniel.\n\n**Daniel Filan** (01:43:33): This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Filming occurred at [FAR Labs](https://far.ai/labs/). Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode, or to learn how to support the podcast yourself, you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me, at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nEpoch AI is the premier organization that tracks the trajectory of AI - how much compute is used, the role of algorithmic improvements, the growth in data used, and when the above trends might hit an end. In this episode, I speak with the director of Epoch AI, Jaime Sevilla, about how compute, data, and algorithmic improvements are impacting AI, and whether continuing to scale can get us AGI.\n\nTopics we discuss:\n\n * The pace of AI progress\n * How Epoch AI tracks AI compute\n * Why does AI compute grow so smoothly?\n * When will we run out of computers?\n * Algorithmic improvement\n * Algorithmic improvement and scaling laws\n * Training data\n * Can scaling produce AGI?\n * When will AGI arrive?\n * Epoch AI\n * Open questions in AI forecasting\n * Epoch AI and x-risk\n * Following Epoch AI’s research\n\nDaniel Filan (00:00:09): Hello everybody. This episode I’ll be speaking with Jaime Sevilla. Jaime is the director of Epoch AI, an organization that researches the future trajectory of AI. In this conversation, we use the term “FLOP” a lot. “FLOP” is short for “floating point operation”, which just means a computer multiplying or adding two numbers together. It’s a measure of how much computation is being done. Links to what we’re discussing are available in the description and you can read a transcript at axrp.net. Well, Jaime, welcome to AXRP.\n\nJaime Sevilla (00:00:36): Thank you for having me here.\n\n\nThe pace of AI progress\nDaniel Filan (00:00:38): So you study AI, how it’s progressing. At a high level for people who have been living under a rock, how’s AI doing? How’s it progressing?\n\nJaime Sevilla (00:00:47): It’s going pretty fast, Daniel. I think that right now the two things that you need to take into account when you’re thinking about AI is how fast inputs into AI are going, and how fast outputs are going, how much is being achieved. In terms of inputs the progress is exponential. The amount of compute that’s being used to train modern machine learning syst",
      "wordCount": 16729
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "8daMDi9NEShyLqxth",
        "name": "Forecasting & Prediction",
        "slug": "forecasting-and-prediction"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "5f5c37ee1b5cdee568cfb2b0",
        "name": "Technological Forecasting",
        "slug": "technological-forecasting"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "ozR3BRDs8zJBJrz5P",
    "title": "AXRP Episode 36 - Adam Shai and Paul Riechers on Computational Mechanics",
    "slug": "axrp-episode-36-adam-shai-and-paul-riechers-on-computational",
    "url": null,
    "baseScore": 25,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-09-29T05:50:02.531Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/sUpAssKC-L0)\n\nSometimes, people talk about transformers as having “world models” as a result of being trained to predict text data on the internet. But what does this even mean? In this episode, I talk with Adam Shai and Paul Riechers about their work applying computational mechanics, a sub-field of physics studying how to predict random processes, to neural networks.\n\nTopics we discuss:\n\n*   [What computational mechanics is](#what-comp-mech-is)\n*   [Computational mechanics vs other approaches](#comp-mech-vs-other-approaches)\n*   [What world models are](#what-world-models-are)\n*   [Fractals](#fractals)\n*   [How the fractals are formed](#how-the-fractals-are-formed)\n*   [Scaling computational mechanics for transformers](#scaling-comp-mech-for-transformers)\n*   [How Adam and Paul found computational mechanics](#how-adam-and-paul-found-comp-mech)\n*   [Computational mechanics for AI safety](#comp-mech-for-ai-safety)\n*   [Following Adam and Paul’s research](#following-adam-and-pauls-research)\n\n**Daniel Filan:** Hello, everybody. This episode I’ll be speaking with Adam Shai and Paul Riechers, who are co-founders of [Simplex](https://www.simplexaisafety.com/). Simplex is a new organization that takes a physics of information perspective on interpretability, aiming to build a rigorous foundation for AI safety. Paul has a background in theoretical physics and computational mechanics, while Adam has a background in computational and experimental neuroscience. For links to what we’re discussing, you can check the description of the episode and a transcript is available at axrp.net. Paul and Adam, welcome to AXRP.\n\n**Adam Shai:** Yeah, thanks for having us.\n\n**Paul Riechers:** Thank you.\n\nWhat computational mechanics is\n-------------------------------\n\n**Daniel Filan:** You guys work on doing computational mechanics for AI safety type things. What is computational mechanics?\n\n**Paul Riechers:** I’m happy to give a bit of a background. Computational mechanics has basically grown within the field of physics, mostly, out of chaos theory, information theory. For what purpose? Well, I mean physics has always been concerned with prediction, that we want to write down some equations that tell us maybe if we know the state of the world, what does that imply for the future? Some planets move around and all these things.\n\nAnd then there became a pretty serious challenge to that - being able to predict physical systems - that came from chaos theory, where even if you have deterministic equations of motion, there’s a finite horizon to how much you can predict. And so I don’t know if I’d say that physics was thrown into turmoil, but it was a serious challenge for what are the limits of predictability and how would you go about doing that prediction as well as possible.\n\nI’d say that’s the history out of which computational mechanics has grown. And so, it’s now a very diverse set of results and ideas, ways of thinking about things and dynamics. But really at its core is: what does it mean to predict the future as well as possible? As part of that, what does it mean to generate dynamics and how is generating different than predicting? If you have some generative structure, is it harder to predict? A lot of this stuff’s been quantified.\n\nAnd a lot of the work that’s been done is at this ultimate limit of what’s possible. But there’s also a lot of work that’s been done, and more and more work now, of if you have some resource constraints, then what does that imply for your ability to predict? And you’re not asking yet, but I’d also be happy to share how that’s relevant to AI/ML, I’ll maybe just throw it in there.\n\n**Daniel Filan:** Sure.\n\n**Paul Riechers:** Maybe this is obvious to your listeners, but we’re now training these AI models to predict future tokens from past tokens as well as possible. And while people are poking around trying to understand what’s going on, there’s this theory that was developed specifically to address that. And so a lot of the mathematical framework maps on surprisingly easy, where we’ve been able to come away with some results that we’ve been just actually surprised that it worked out so well. And it’s a great framework to then build upon. I wouldn’t say computational mechanics is the answer to understanding all of interpretability and AI safety, but it’s really a great foundation and something that helps us to make better questions and see where research should go.\n\n**Daniel Filan:** Sure. I guess a bit more specifically, if I hear this pitch of like, “Oh, yeah, we’re interested in understanding how to predict the future as well as possible.” Someone might think, “Hey, that’s Bayesian statistics. We got Bayes’ Theorem, like, 100 years ago. What else is there?” I’m sure there’s more, but what is it?\n\n**Paul Riechers:** What are you doing Bayesian inference over?\n\n**Daniel Filan:** Yeah, I don’t know, stuff. You’re going to have some prior and you’re going to have some likelihood and then you’re just done. What else is there?\n\n**Paul Riechers:** No, exactly. But I think that’s my point: computational mechanics helps us to understand what are the things that you’re trying to do updates over? And I think there’s some question of… From the start, I’m saying, “Okay, we’re applying this theory because models are predicting the future as well as possible,” but actually what we’re doing is we’re training them on this next token cross-entropy loss.\n\nWell, what does that mean? People have argued about whether they can have some world model, and it’s unclear what people even mean by a world model, or if they’re stochastic parrots, and it’s not even clear what people mean by that. One of the advantages is that: let’s just take this seriously, what are the implications of doing well at next token prediction? And there’s a theorem - it’s actually a corollary from [a paper](https://arxiv.org/abs/cond-mat/9907176) from I think 2001 from \\[Cosma\\] [Shalizi](https://www.stat.cmu.edu/~cshalizi/) and \\[Jim\\] [Crutchfield](https://csc.ucdavis.edu/~chaos/) \\- which if you translate it into modern language, says that to do as well as possible at next token prediction implies that you actually predict the entire future as well as possible. Okay, then computational mechanics comes into play.\n\nIt’s more than Bayesian inference because what? Well, you want to know what about the past you need to predict the future. And if you had no resource limitations, you can just hang on to all of the past. And there would be maybe a mapping from this past to this future, but with some number of your token alphabet, the number of paths would be growing exponentially with longer length paths.\n\nAnd so that’s intractable. You somehow need to compress the past. And what about the past should be coarse-grained? Basically there’s this obvious mantra, but when you take it seriously, again, it has interesting mathematical implications, that for the purpose of prediction, don’t distinguish pasts that don’t distinguish themselves for the purpose of prediction. That means if a past induces the same probability distribution over the entire future, then just clump those pasts together. And also if you want to say about lossy prediction, then you can look in that space of probability distributions over the future and you can see what histories are nearby in that space.\n\n**Daniel Filan:** Sure. Maybe a thing that would help my understanding is if I just say what I’ve gathered the interesting objects, interesting things in computational mechanics are. And you can tell me where I’m wrong, or perhaps more likely what interesting things I’m missing. Does that sound good?\n\n**Adam Shai:** Yeah.\n\n**Daniel Filan:** Cool. Here’s what I see computational mechanics as doing, just from a brief glance. It seems like it’s really interested in stochastic processes. Kind of like time series inference of a bunch of things happen at… you’ve got this thing and then the next thing and then the next thing and then the next thing. And you want to predict the future. As opposed to forms of Bayesian inference where you’re inferring the correct neural network that’s underlying this IID, this independent identically distributed thing as opposed to various other things you could do with Bayesian inference.\n\nAnd in particular, it’s interested in hidden Markov models. I see a lot of use of hidden Markov models where basically, there’s some underlying thing. The underlying thing is transitioning from state to state in some kind of lawful way. And these transitions are associated with emitting a thing that you can see. Each time a transition happens, you see a thing emitted. And basically in order to do well at predicting, you’ve got to understand the underlying hidden Markov model, understand what state is the hidden Markov model in. And if you could understand what state the hidden Markov model was in, then you do a really good job of prediction.\n\nI then see this construction of a thing called an epsilon-machine, which as far as I can tell is the ideal hidden Markov model of a certain stochastic process, where basically, the states of the epsilon-machine are just somehow the sufficient statistics. Like you were saying, grouping together all pasts that lead to the same future things, that’s just the states in the epsilon-machine. And there’s some sort of minimality of, those are precisely the states you need to distinguish between. If you had any more states, it would be too many. If you had any fewer states, it would be too few to actually understand the process.\n\nSo you have stochastic processes, there’s hidden Markov models, there’s epsilon-machines. Then I think there’s something about these inference hidden Markov models where if I’m seeing the outputs of a stochastic process, if I’m seeing just a sequence of heads and tails or tokens or whatever, there’s some process where I am coming to have probability distributions over what state the thing might be in. I update those distributions. And that itself works kind of like a hidden Markov model where the states are my states of knowledge and the tokens of transitions are like, I see a thing. I move from this state of knowledge to this state of knowledge.\n\nI also see this construction of going from underlying processes of generating to the process of inferring things. And I guess there are some interesting similarities and differences between the inference process and the generation process. My impression of computational mechanics is it’s taking that whole thing, these stochastic processes, these hidden Markov models, these epsilon-machines, these inference hidden Markov models… You probably have a better name for them than that, but that’s what I came up with.\n\n**Paul Riechers:** Maybe “mixed-state presentation”.\n\n**Daniel Filan:** Mixed-state presentation. And then there’s this question of, okay, how do I calculate interesting things about these processes? How do I actually find, I don’t know, stationary distributions, or how do I get observables or higher order statistics out of that? This is what I’ve gathered from computational mechanics. I’m wondering, does that seem right? Am I missing stuff?\n\n**Adam Shai:** That was pretty good. I think there’s a lot there. I’ll just go at it without any particular order. But importantly, you brought up the epsilon-machine. For instance, one extra thing we can add on to what you said is that there’s a distinction between minimal for the purposes of predicting versus minimal for the purposes of generating. And so the epsilon-machine is the minimal machine for the purposes of prediction. In general, you can get smaller machines.\n\n**Daniel Filan:** Where a “machine” is a hidden Markov model?\n\n**Adam Shai:** Yeah. I think it can be a little bit more general than that, but yeah, I think it’s fine to think of it as an HMM. Yeah. The mixed-state presentations are HMMs in this framework, and the epsilon-machine is one. And you notice they actually both generate the same data set. You can think of, you have data. The data (for instance) that you train a transformer on, all of your training data. And these can even be - in theory not for a transformer in practice - but in theory these can even be infinite strings.\n\nIf you want to make an HMM that generates that data, there’s an arbitrary choice a lot of the times. There are many, many HMMs that can generate that data, an infinite number of them in fact. And in comp. mech., they call these different presentations. And Paul, in [one of his papers](https://pubs.aip.org/aip/cha/article/28/3/033115/684965), in the intro, makes a really great point, and this is a deep conceptual view that I really love about comp. mech., which is that for different questions about a process and for the structure of the data, different presentations allow you to answer those questions with different amounts of ease.\n\nIf you want to ask questions about the predictive structure and what information you need to predict, then an epsilon-machine or a mixed-state presentation is a very good way to answer that question. Do you have some question that’s different about generation or about pairwise correlations? Then different presentations will be more useful for that. Just this notion that the way that you present the data, the choice that you have of exactly which machine you use to generate the data with, even though it’s the same exact data, with the same structure in some sense, allows you to answer different questions. That’s just a few thoughts.\n\n**Paul Riechers:** Yeah, I guess you had a lot to unpack there in what you said. There are a few things I wanted to at least comment on. The first one is I think a common misconception, I think it’s natural that you were saying, “Oh, it seems that computational mechanics is about HMMs.” And I guess that’s a common misconception. I’d say that actually HMMs turn out to be the answer to a particular question you can ask in computational mechanics.\n\nThe data-generating structure could be essentially anything. You could think through [the Chomsky hierarchy](https://en.wikipedia.org/wiki/Chomsky_hierarchy) or other types of ordinary differential equations or whatever that generates the thing. And then you ask the question of, again, what is it about the past that I need to remember to predict the future as well as possible? And then you have these minimal sufficient statistics, which are these coarse-grainings of the past. And then you look at what’s the meta-dynamic among those belief states about the world, like you mentioned.\n\nAnd then that thing turns out to be describable always as a hidden Markov model, if we allow ourselves infinitely many states. Because you can think about this basically via Bayesian updating, that given basically your knowledge of the world up to some point, even if you know the data generating process, you don’t know the hidden state of the world. You just wake up in the morning and you have this impoverished view. You need to take in data and synchronize to the state of the world. You know something, you have something like a model of the world. Your data so far induces some belief state.\n\nAnd then as you take in more information, how do you update your belief? Of course, just Bayes’ rule, Bayesian updating. And so from a particular belief state, there is a unique answer of where you go to, given the new data. That induces a hidden Markov model. In fact, it induces a particular type of hidden Markov model, which in the lingo of comp. mech., computational mechanics, is “unifilar”. But you can also think in the theory of automata, this is called deterministic. That’s a little confusing because we’re talking about stochastic deterministic automata. “Unifilar” is a better name. You’re following a single thread through the states of this belief structure.\n\n**Daniel Filan:** Right. Just to clarify, it’s stochastic in the sense that the states of knowledge are probabilities over what’s going to happen in the future, but it’s deterministic in that if you see a thing, there’s just a deterministic next state of knowledge you go to?\n\n**Paul Riechers:** There’s a deterministic way to update your state of knowledge. There’s typically some stochasticity in terms of what in the world comes at you. And so, if you look at an ensemble of realizations, there would be randomness, stochasticity in terms of the transition rates between belief states. But in any realization, there’s a thing you do to update your beliefs.\n\nAnd then just a few of the things I wanted to engage with were: what’s the role of the epsilon-machine here? I think especially for making a connection with understanding neural networks, I think the epsilon-machine isn’t as important. I think this mixed state presentation, this updating beliefs is the primal thing that we care about. And it doesn’t even require stationarity or anything like that.\n\nIf you do have stationarity, then there will end up being some recurrent structure in the belief state update. And then those recurrent belief states are the causal states of this epsilon-machine. I think it’s a bit of the history of where comp. mech. came from that the epsilon-machine was important. And I think for the current way in which comp. mech. is useful, the epsilon-machine is not as important. But it is still interesting to say what are the recurrent states of knowledge that I would come back to for something stationary? Which maybe is relevant if you’re reading a long book or something.\n\n**Daniel Filan:** Okay. Can I try my new understanding?\n\n**Paul Riechers:** Okay.\n\n**Daniel Filan:** And see if I’ve gotten any novel misconceptions. Here’s my new understanding. You’ve got the stochastic processes. Stochastic processes are just things that generate sequence data, like the probability distribution over books that are going to be written - and a book is a really long sequence of words in this frame - or sequences of the state of some chaotic process maybe - just sequences of stuff. You want to understand this probability distribution over these sequences. What’s going to happen in the future given the past?\n\nAnd it turns out that hidden Markov models are a universal way of describing these stochastic processes. If you have a stochastic process, you can create a HMM, hidden Markov model, that generates it. In fact, you can create a bunch of hidden Markov models that generate it. There’s also this process of updating your beliefs given some data that you’ve seen in this process. And that also looks kind of like a hidden Markov model, maybe not exactly because things are coming in instead of going out, but something like that.\n\nAnd computational mechanics as I currently understand it is something like, “Okay, how do we relate these stochastic processes to hidden Markov models?” Which hidden Markov… You can construct a bunch of hidden Markov models that correspond to the same stochastic process. Which one do we want to play with and which one is most useful for various mathematical goals we might have for understanding these sequences?\n\nIf we want to understand the inference process, we can think of that in terms of some kind of hidden Markov model-like thing. And then there’s just this process of understanding, okay, what actually happens? How? Maybe… you haven’t said this, but I imagine there might be questions like, okay, how many states are there? What are the dynamics of belief updating? Where do you go to? That’s what I now hypothesize computational mechanics is. How does that sound? What am I missing now?\n\n**Paul Riechers:** I’d say you’ve hit on a lot of relevant things. I think maybe even more interesting topics would come up if we’re thinking about: what does any of this have to do with neural networks? Because like I said, computational mechanics is a very rich field. There’s semester-long courses on this, so I don’t think we could do it justice right away.\n\nBut I think we can get to a lot of neat ways in which the theory is being used and extended by us and our colleagues, as we’re saying, “Okay, how can we utilize this to…?” Well, what we have done so far is basically, we have this new angle at understanding internal representations of neural networks, and also something which I think will become more clear in our upcoming work, the behaviors of models, like what to anticipate in terms of model behavior, in-context learning… There’s really an amazing number of ways now to adapt the theory that we see, which is getting us to answer questions that I’d say weren’t obvious to even ask before.\n\n**Adam Shai:** Yeah. I had a thought just because of something you said: one of the things we’ve been talking about recently is: we’ve been thinking about a lot of the work that’s been going on in interpretability that uses the concept of “world models”, and what the form of that work is, and trying to get closer and closer to “what is a world model? What is the type of stuff people do when they talk about world models in interpretability work?”\n\nAnd this comes back to this issue of where HMMs fit in comp. mech., and are we assuming an HMM or something? One of the things I’ve been thinking about: even if you don’t have stochastic data, if you have some other data structure - and there’s a lot of data structures that one could have - you have some implicit or maybe explicit structure to the world in which you want to ask, does this transformer represent somehow in its internals?\n\nWell, no matter what that data structure is, at the end of the day, if you’re going to train the transformer on it, you have to turn it into sequential data. It takes sequences of tokens. And so, you can go in and you can probe the model internals for structures associated with this original data structure in your head that might not have anything to do with sequences. That’s a fair game to play, and I think people sometimes do that. Maybe you conceptualize Othello as not really being about sequences, but being about some other type of data structure, about states of the board where the top left corner is black, white, or empty. And then you can go in and [you can probe](https://www.neelnanda.io/mechanistic-interpretability/othello) your Othello-GPT to see if that’s [linearly represented in the residual stream](https://arxiv.org/abs/2310.07582).\n\nBut when you train the Othello-GPT, you are transforming the game of Othello to a specific tokenization scheme that gives you specific sequences. And those sequences are naturally thought of as an HMM. And then - this work isn’t done, so this is more thoughts and things we hope to do - but we should be able to take results like that, reframe it from a comp. mech. point of view, and then run our analysis and apply our framework to it to even make hopefully theoretical claims explaining the results that they’ve gotten. That certain ways, certain probes for properties of the game state are linearly represented, and others are non-linearly represented.\n\nIf we take our findings seriously that this belief state geometry… I guess we haven’t spoken too much about that yet, but if that belief state geometry is the thing that the transformer is trying to represent, then hopefully we can explain a bunch of these other results that people have been finding.\n\n**Daniel Filan:** I think first, I just want to make sure that I really understand what this theory is before I move on there. But I’m chomping at the bit to get to that later.\n\nOkay. Here’s a really basic question. Why is it called computational mechanics? Does it have something to say about computation or mechanics?\n\n**Paul Riechers:** Yeah. It’s a kind of interesting historical note. I think it makes more sense when you realize it’s called computational mechanics in distinction to statistical mechanics, which, okay, this makes some sense. What’s statistical mechanics? In physics there is this theory of thermodynamics, of course, which is this high-level… you just look at these macro states and how they evolve.\n\nThermodynamics doesn’t require a microscopic description, but then it turned out that, oh, actually, if you believe in atoms - I mean this is kind of why people started re-believing in atoms - then you can actually explain some of the thermodynamic behaviors, and you can figure out material properties and stuff. How? Just basically by saying how random something is. We have entropy that quantifies that. And by saying how random something is, we can derive all of these properties.\n\nBut it’s a static view of what’s going on. You just look at the statistics. Okay. When the question shifted to say: what are the dynamic aspects? What are the computational aspects? Somehow there’s… nature’s intrinsically computing. You think of [Shannon’s channel](https://dennisdjones.wordpress.com/wp-content/uploads/2013/04/shannon-info2.gif) that information would go through. You can also think of the present as a channel that the past must go through to get to the future. If you take this on, there’s some sense in which nature’s naturally computing the future from the past.\n\nThat’s the idea of computational mechanics. And yeah, it’s also taken and built upon a lot of ideas in computer science and information theory. Unfortunately, it also has the same name as something which is quite different, which is… I don’t even know exactly what it is, but it’s like-\n\n**Adam Shai:** How to use computers to model Newtonian mechanics.\n\n**Paul Riechers:** Something like that. That’s a bit confusing. But yeah, the computational mechanics that we’re talking about here is in distinction to statistical mechanics.\n\n**Daniel Filan:** Got it. I guess the final thing that I want to ask is, I think you mentioned a bit offhandedly this idea of: how can you do prediction under some sort of resource constraints? Maybe it was not being able to distinguish certain states of the world, or maybe it was something else. I’m really interested in that because I think this is a question that Bayesian statistics doesn’t have an amazing answer for just off the shelf. It’s a question that is obviously really relevant to AI. Can you tell us a little bit?\n\n**Paul Riechers:** Yeah. We’re also really interested in this. I mentioned that some work has been done in comp. mech. on this, but I’d say it’s underdeveloped. But [Sarah Marzen](https://www.sarahmarzen.com/) and other colleagues have [worked](https://arxiv.org/abs/1412.2859) on, thinking of rate-distortion theory, [how does it apply](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.95.051301) in this dynamic framework of computational mechanics when maybe your memory is limited?\n\n**Daniel Filan:** What is rate-distortion theory?\n\n**Adam Shai:** Conceptually, you can imagine a plot where on the x-axis, you have your memory capacity, and on the y-axis, you have your accuracy at predicting the future, and there’s some kind of Pareto optimal curve you can get. So, for a certain amount of memory constraint, you can only get so far in accuracy, and there’s some curve that describes the optimal trade-off there.\n\n**Daniel Filan:** So, shape of the curve, I guess, probably how you get to the curve. And so, you were saying there was some work by Sarah Marzen and others on taking some sort of rate-distortion theory view and applying it to, I think, dynamical processes or something?\n\n**Paul Riechers:** Yeah. So, say, instead of the causal states, instead of these ideal belief states, how would you compress those to still predict as well as possible given the memory constraints. So related to information bottleneck methods, there’s this causal bottleneck stuff. So, that’s one way in which you can think of a constraint. There’s memory constraints. But there can be all sorts.\n\nThere’s an interesting avenue on the physics side, where, actually, if you allow yourself something like quantum states, quantum information can somehow compress the past better than classical information could. So, there’s this interesting branch of computational mechanics which has said, actually, we can use quantum states to produce classical stochastic processes with less memory than otherwise classically necessary. So that’s interesting. Sounds very tangential, but a bit of a teaser. Actually, we think that neural nets do this. So, that’s a bit wild, and again, something you just probably wouldn’t think to look for if you weren’t building off this theory.\n\nComputational mechanics vs other approaches\n-------------------------------------------\n\n**Daniel Filan:** Sure. I guess finally, to just triangulate computational mechanics a little bit more, I want to compare it to two other things that sound kind of similar. So, the first is, people who have followed the AI safety space, people who have listened to [a few episodes back in my podcast](https://axrp.net/episode/2024/05/07/episode-31-singular-learning-theory-dan-murfet.html) might be familiar with singular learning theory. Right? So, this is the most recent really math-y way of thinking about Bayesian statistics that has taken the alignment world by storm.\n\nHow do you see computational mechanics as contrasting with singular learning theory, perhaps, both in just what the theory even is or is trying to do and how it might apply to AI systems?\n\n**Paul Riechers:** I have a quick comment, maybe, then Adam \\[maybe\\] wants to elaborate, but at a very high level, computational mechanics is a model-agnostic theory of prediction. Whereas, I think SLT takes on very seriously that, if you have a particular model, in that you have a particular parameterization of the model, then, what’s learning like?\n\nSingular learning theory, SLT is a Bayesian theory. I think sometimes the way in which it’s applied to talk about actual learning with, maybe, stochastic gradient descent confuses me a little bit, but I think it’s a genuine mathematical theory that, I think, likely will say something useful about inference. Maybe it can be complementary to comp. mech. in addressing some of those aspects. Do you have further thoughts on that?\n\n**Adam Shai:** Yeah, I mean, first of all, I’m not an expert at all on SLT, although I’ve been highly inspired by them and the entire group at [Timaeus](https://timaeus.co/). I think they ask a lot of questions that are similarly motivated. So, for instance, I think they really do a good job at talking about their motivations about the computational structure of these systems and how can we get a mathematical and theoretical handle on those things?\n\nI think their tactic and framing of it, and this might be more of a vibes thing than… again, I’m not an expert in the math at all, so I really can’t speak to that. But I think, naturally, they take a view that has more to do with learning and the learning process, and I think they want to, at the end, say something about the structure of that learning process and the geometry of the local landscape, and how that relates to the computational structure at a particular weight setting. Whereas, I think comp. mech. most naturally, at least, takes a view that really has to do with the structure of that inference process directly.\n\nBut then, of course, one can think of extensions: Paul’s already done some work extending that to the learning process a little bit. So I think they’re trying to get to a very similar set of questions, and a lot of these I consider deep conceptual questions at the heart of how do AI systems work? What does it mean to talk about structure of computation? What does it mean to talk about the structure of training data, but from different starting points. The learning process versus the inference process.\n\n**Daniel Filan:** Right. In some ways, they seem sort of dual, right? Like figuring out the weight-updating process versus fixed weights: where should it go?\n\nThe other thing I want to ask to triangulate - again, a [somewhat recent episode of AXRP](https://axrp.net/episode/2024/05/30/episode-32-understanding-agency-jan-kulveit.html) \\- is, when I hear people talk about, “oh, yeah, we’ve got a really nice mathematical Bayesian theory of updating based on evidence and we’re going to use it to understand intelligence”, one place my mind goes to is active inference, this [Karl Friston](https://en.wikipedia.org/wiki/Karl_J._Friston)-style work.\n\nI’m wondering if there’s any interesting connections or contrasts to be brought out there.\n\n**Adam Shai:** I think, again, this will also be a little vibes-y. I think the “active” part of active inference is talking about, as far as I understand it, an agent taking actions in the world. There’s a part of comp. mech. that has to do with interacting systems called transducers, and that is so far not what we’ve applied to neural networks. So, that part’s a little different if you just take the “not transducer” part of comp. mech., at least, and compare it to it.\n\nAlso, the way that I view active inference - again, as a non-expert - is it takes seriously… I guess it matters what form of active inference. So, often, it starts with just “we think the brain’s doing Bayes” or “an agent is doing some Bayesian thing”, and then, you can get it down to an actual formula by kind of assuming… it’s hard to do Bayes in full, so let’s find some kind of mathematical way that you can approximate it, and they get to a specific set of equations for that Bayesian updating, and…\n\nYeah, so it kind of assumes a certain method of approximating Bayesian inference where, again, we’re pretty agnostic to all that stuff in comp. mech.. So those are just some thoughts.\n\n**Daniel Filan:** Sure. I guess, I am limited in my ability to wait until I dive into your paper and your interesting work. But is there anything you would like to say just about computational mechanics as a theory before I dive into that?\n\n**Paul Riechers:** Yeah, I guess just to re-emphasize two things: One, it’s, as it is currently, not the answer to everything, but has been really useful. But also, I think it’s a great springboard to continue on. So, we’ve recently founded this research organization, [Simplex](https://www.simplexaisafety.com/), because we believe that we can really build on this work in useful ways. So just that contextualization might help.\n\nWhat world models are\n---------------------\n\n**Daniel Filan:** Sure. Let’s talk about building on it. You’ve recently published this paper, [“Transformers represent belief state geometry in their residual stream”](https://arxiv.org/abs/2405.15943) by Adam, [Sarah Marzen](https://www.sarahmarzen.com/), Lucas Teixeira, [Alex Gietelink Oldenziel](https://sites.google.com/view/afdago/home), and Paul. What did you do in this paper?\n\n**Adam Shai:** We were motivated by this idea of: what is a “world model”? What is, fundamentally, the structure that we’re trying to get… That we’re training into these transformers when we train them on next token prediction, what is the relationship of that structure to a world model? How can we get a handle on these questions?\n\nI guess one of my own personal… I don’t know if it’s a pet peeve, but something like a pet peeve is it feels like, often, there’s just this never-ending discussion that happens in the AI community at large about: these things, do they really understand the world? Do they have a world model? Do they not? And these things, these are slippery concepts and it feels like the same conversation has been happening for a while now. And our hope is that, if we can get to some more concrete handle, then, we can actually know what everyone’s talking about. We don’t have to keep on talking past each other. We can answer some of these questions.\n\nSo, the tactic that we take in this paper is to start with a ground truth data-generating process. So, we know the hidden structure of the world, so to speak, that generates the data that we train a transformer on. And then, we can go in and we can, first, make predictions using the theory of computational mechanics about what structure should be inside of the transformer. Then, we can go and we can see if it is there.\n\nSo, we need a way to figure out what we should look for in the transformer. And so, we need a way to go from this hidden data-generating structure, which takes the form of an HMM in our work, to a prediction about the activations in the transformer. So, we just ask the question: what is the computational structure that one needs in order to predict well given finite histories from this data-generating process, if we want to predict the next token.\n\nThat, in computational mechanics, takes the form of something called a mixed-state presentation, which describes how your belief about which state the data-generating process is in, how that belief gets updated as you see more and more data.\n\nThat, by itself, doesn’t give you a prediction for what should be inside the transformer because that’s just another hidden Markov model. But there’s a geometry associated with that, and that geometry is given by virtue of the fact that your beliefs over the states of the generative process are, themselves, probability distributions. Probability distributions are just a bunch of numbers that sum to one, and you can plot them in a space where each thing they have a probability for - in this case, it’s the different states of the generative process - are a different axis.\n\nAnd so, the points lie in some geometry in that space. And in one of the first examples we used in the paper, and it’s featured in [that blog post on LessWrong](https://www.lesswrong.com/posts/gTZ2SxesbHckJ3CkF/transformers-represent-belief-state-geometry-in-their), even though it’s a very simple generative structure, you end up getting this very cool-looking fractal. So, it feels like a highly nontrivial geometry to kind of expect and predict should be in the transformer.\n\nAnd what we find is that when we actually go and when we try to find a linear plane inside of the residual stream, that when you project all the activations to it, you get the fractal, you can find it. And not only can you find it, you can perform that analysis over training, and you can kind of see how it develops and refines to this fractal structure.\n\n**Paul Riechers:** And that it’s linearly represented also.\n\n**Adam Shai:** And it’s linear. So we were very excited about that. It means that we have some kind of theoretical handle about: given the structure of our training data, what we should expect geometrically in the activations of a transformer trained on that data. So, it gives us a handle on model internals and how it relates to training data.\n\n**Daniel Filan:** Gotcha. When I’m thinking about this, one thing I’m paying attention to is, like you said, what does it mean for a thing to have a world model? And I guess part of the contribution of computational mechanics is just distinguishing between the dynamics of generating some sequence, and the dynamics of what inference looks like, what states you go through with inference.\n\nAnd would it be right to say that a big deal about your paper is just saying: here’s the dynamics of the inference that we think Bayesian agents should go through when they’re inferring what comes next, what comes next… Or even just inferring the whole future. And saying: hey, we found the dynamics of inference encoded in the neural network.\n\n**Paul Riechers:** Yeah. I guess one of the points that we hope is obvious in hindsight - but I think a lot of good results are obvious in hindsight, so it’s hard to feel proud of yourself - but it’s just this thing of “what does it mean to have a world model?” And we just took this question pretty seriously: what’s the mathematics of it? And then, there’s these speculations, can next token prediction do this? And then, the answer is concretely yes.\n\nIn fact, not only must neural nets learn a generative model for the world, but they also must be learning a way to do effectively Bayesian updating over those hidden states of the world. And I guess, what’s more, is it’s not just tracking next token probability distributions, actually, the model will differentiate basically states of knowledge that could have identical probability distributions over the next token, which seems a little odd if you just think about, “I’m just trying to do well at next token prediction.”\n\nBut again, in hindsight, it kind of makes sense that if there will be a difference down the road and you just merged those states early on, then, even if your loss at this time step or at this context position is just as good, eventually, as you move through context, you’ll have lost the information needed to distinguish what’s coming at you. So, that’s this thing about predicting the whole future: it’s just this iterative thing. If you do next token prediction on and on and on, eventually, you want to get the whole future right.\n\nAnd not only is there this causal architecture, but as Adam said, there’s this implied geometry, which is pretty neat. And I think there’s all sorts of hopes. We’ve had a lot of good community engagement. People have talked about, well, maybe this means we can back out what a model has learned if we can somehow automate the process of this simplex discovery. There’s a lot of, I think, directions to go that we’re excited about and we think the community can also help out.\n\n**Adam Shai:** Just to add one thing: to reiterate the point that you made about the structure of inference versus the structure of generation, I do think that at least for me, it was a super core conceptual lesson coming from a neuroscience background. Because in the section of neuroscience I was in, at least, we would talk a lot about “the cortex’s role is to do this, to implement this predictive model”. But then, we would always instantiate that in our minds as generative models.\n\nIf you asked any particular neuroscientist in that field, including me just a few years ago, “but don’t you have to use the generative model to do prediction?” You’d be like, “yeah, of course you have to do inference over it”, but it’d be kind of a side thing. Once you have the generative model, it’s obvious how to do inference, and there’s not much to it. That would be the implication.\n\nBut, actually, the structure of inference is enormous, right? From this three-state HMM - and this is a simple example - we get an infinite-state inference structure that has this fractal thing. So the structure of inference itself is not something to just shrug away. In some sense, if you’re trying to do prediction, it is the thing. And its structure can be quite complicated and interesting.\n\nAnd so, I actually think the neuroscientists could learn a lot from this, too. Although, obviously, we applied it to neural networks.\n\n**Paul Riechers:** And there’s maybe one other idea that you could usefully latch onto here, which is something like thinking of predator and prey. You can have a fox and mouse or something like that, and the mouse can be, maybe, hard to predict (or eat) with a simple generative structure. It just does something. And all the fox has to do is predict what it’s doing. But it’s not sufficient for a fox to be able to act like a mouse. It has to actually predict it.\n\nAnd that’s maybe some evolutionary pressure for it to have a bigger brain and things like this. And I think this is… is this a weird analogy? But I think it’s actually quite relevant: how smart should we expect AI models to be? It’s not just that they need to be able to act like a human, they need to be able to predict humans. And that’s a vastly greater capability.\n\nWe can’t do that with each other too well. I mean, we do a little bit.\n\n**Daniel Filan:** But it’s hard.\n\n**Paul Riechers:** But it’s a superhuman task, actually. Yeah.\n\n**Daniel Filan:** Yeah. Have you played the [token prediction game](http://rr-lm-game.herokuapp.com/)?\n\n**Paul Riechers:** I’ve played various versions of this. I don’t know if it’s the one you’re referring to.\n\n**Daniel Filan:** Yeah. It’s just a web app that… Basically, the game is to literally do what these transformers have to do. So, you see just a prefix, or actually, I think you start with zero prefix. You just have to guess what the next token is.\n\nI think it even shows you four possibilities or something, and you have to just guess, yeah, what is the next thing going to be? And then, it gives it to you. And then, you have to guess, okay, what is the next thing going to be?\n\nAnd it’s really hard. Yeah, I don’t know. Somehow, I’d never made this comparison before, but it’s much harder than writing something.\n\n**Paul Riechers:** Yeah. For sure. And there’s also something interesting in that meta-dynamic of doing the prediction, which is, as you get more context, you’ll probably be better at predicting what’s next. Okay, now, I have some context, right?\n\nAnd so, this is some quantitative stuff that, just, again, once we take these ideas seriously, we can say “as you move through context, your entropy over the next token, if you’re guessing, should decrease”. How does it decrease? Actually, that’s one of the things I worked on during my PhD in comp. mech.: there’s this way to calculate this via the spectral structure of this belief state meta-dynamic and all that.\n\nBut there’s something to this of: that model, since they’re predicting, they should and they will converge to users in context. What is that? That is a definite type of in-context learning. It’s a bit, I think, our task to show whether or not that’s the same of what other people call in-context learning, but that’s a very concrete prediction that, especially with non-ergodic data, which is the case for us, that, in general, you’re going to have this power law decay of next token entropy, which is borne out empirically.\n\nFractals\n--------\n\n**Daniel Filan:** Sure. So, there’s tons to talk about here. First, I want to ask kind of a basic question. Just aesthetically, if I weren’t trying to read your paper, one of the coolest things about it would be this nice colorful fractal that you guys have, right? Why is it a fractal?\n\n**Paul Riechers:** So, I guess one way to think about that is, what is a fractal in general? Well, a fractal is something like iterating a simple rule over and over, right? That’s a self-similar structure you get out of that. Well, what’s the similar rule?\n\nWell, we have a simple generative structure, and then, you’re doing Bayesian updating over that. There’s different types of outputs that it could give. Each output gives a different way of doing Bayes updates over whatever your distribution of the states is.\n\nSo, you’re doing the same thing over and over, and the geometric implication of that is fractal. Is that helpful?\n\n**Daniel Filan:** Yeah. I think this maps onto what my guess was.\n\n**Adam Shai:** I’ll also point out that after the LessWrong post we made, John Wentworth and David Lorell made [a follow-up post explaining that iterative game structure](https://www.lesswrong.com/posts/mBw7nc4ipdyeeEpWs/why-would-belief-states-have-a-fractal-structure-and-why). And then, there’s also been [academic work](https://arxiv.org/abs/2008.12886) explaining the same thing from the point of view of iterative functions, from [Alexandra Jurgens](https://csc.ucdavis.edu/~ajurgens/). So, those are two resources.\n\n**Daniel Filan:** Cool. The next thing I want to ask is: picking off a thing you mentioned very briefly of potential follow-up work being to try and find this fractal structure as a way of understanding what the network has learned.\n\nIs there something to say about… So, you have these fractals of belief states from this mixed-state presentation or something. Is there some way of going from “here’s the fractal I found” to “here are the belief dynamics”?\n\n**Paul Riechers:** Yeah. I think this is getting at an important point that the fractal… those are the beliefs; they’re not the dynamic among the beliefs.\n\n**Daniel Filan:** Yep. They’re not the updates.\n\n**Paul Riechers:** Yeah. And I think an obvious thing that we’d like to do is: now that we find this thing, is it now natural to do some mech. interp. sort of thing of finding a Bayesian updating circuit? That would be nice.\n\nIt’s not totally clear how this works out, but it’s a natural sort of thing for getting at, hopefully, a thought process of the machine, right?\n\nI mean, “what is its internal dynamics among these beliefs?” is a great question that we are pursuing. A lot of this is empirical, because now we have this theoretically-grounded thing, but we also need to work with the actual implementation of transformer architecture. How does that actually instantiate the thing?\n\n**Daniel Filan:** Yeah. I guess I also have a theoretical question of: suppose you had this fractal, somehow. You had it to infinite precision. Is it possible to back out the belief HMM or the stochastic process? Or are there multiple things that go to the same fractal?\n\n**Paul Riechers:** So, there would be multiple choices. There’s degenerate ways in which you can choose to represent a generative structure, but it’s all the same stochastic process. And that’s the thing that matters: the world model that it has. So there really shouldn’t be that much degeneracy at all.\n\nIn fact, in the current toy models that we’ve used to say what happens in the simplest framework, then, we have to project down, we have to find a projection in the residual stream where this fractal exists.\n\nActually, probably, in frontier models, the residual stream is a bottleneck. So, you’re not going to have to project down. It’s going to try to use all of its residual stream. So, in fact, it maybe is easier in a sense, but then, it’s also doing this probably lossy representation that we’re also trying to get a handle on.\n\n**Daniel Filan:** Yeah. I guess I wonder, if one fractal does correspond to one generative process, maybe we don’t even need to do mechanistic interpretability, and we can just say, “oh, yeah, here’s the geometric structure of the activations. Bam, this is the thing that it learned.”\n\n**Adam Shai:** Yeah. I don’t want to overstate what we’re claiming here. The way that I would think about what this belief state geometry is, is not… Maybe it’ll be useful to try to think about features, the way people talk about, and what they are and what the relationship is between features and these belief states, even just philosophically.\n\nI think it would be a mistake to claim that the belief states are the features. I think of features, at the moment, at least - and they’re kind of ill-defined - but I think of them as the kind of computational atoms that the neural network will be using in the service of building up the belief state geometry and the dynamics between those belief states.\n\nOf course, how a neural network builds the belief state geometry up probably will be highly, highly dependent on the exact mathematical form of the network they’re using. So, in transformers, the fact that you have a residual stream with an attention mechanism, the attention mechanism is, literally, the only place the transformer can bring information from the past to the current token position.\n\nAnd it has a certain mathematical form… Strangely, a lot of it is linear, not totally obviously. But that mathematical form puts constraints over what information can be brought from the past to the present, and it puts strong constraints over exactly the manner in which you can build up this belief state dynamic.\n\nSo, yeah, it would be really cool to be able to even… It’s like a version of comp. mech. that somehow takes into account the kind of mathematical constraint that the attention head puts on you in your ability to bring information from the past to the present. Which we very much don’t have, but which you could vaguely imagine.\n\n**Paul Riechers:** Although Adam has done some preliminary experiments where you look at basically this factor we were talking about; how that’s built up through the layers and in the different… from attention, from MLPs. I mean, it’s surprisingly intuitive and beautiful. So encouraging at least.\n\n**Adam Shai:** So for instance, if you take the fractal example in the Mess3 process, it’s a vocab size of three, so it only speaks in As, Bs and Cs. So it’s just strings of As, Bs and Cs. And then you put it through the embedding and you see just three points. So it’s like three points in a triangle, one for each of the tokens. That makes sense. And then that’s the first place that you’re in the residual stream. Then you split off and you go into the first attention mechanism. And what you see happen in the first attention mechanism is it spreads out the points to a filled-in triangle.\n\nAnd then when you add in that triangle back into the residual stream, you’re adding in the triangle to these three points in a triangle. And what you get is three triangles in the shape of a triangle and then so on and so forth. And then actually it goes into the MLP and you can see it stretch out these three stacked triangles to look more like the simplex geometry. And then every time you go through another layer, it adds more holes and more triangular structure. And you very nicely see a very intuitive feeling for how every part of the transformer mechanistically puts in extra structure in order to get to the belief state geometry.\n\n**Paul Riechers:** And this is one example of how we hope that computational mechanics can be something like a language for bridging between mechanism and behavior. A lot of mech. interp. has been great, but it’s pretty low level. There’s just results here and there. So one thing computational mechanics can do is create a language to tie this together to unify the efforts, but also where is this leading? And so hopefully there’s a little bit more of a bridge between the mechanism and behavior.\n\nHow the fractals are formed\n---------------------------\n\n**Daniel Filan:** Sure. So there’s this question about what we learned, and we’ve talked about that a little bit. We’ve learned that these fractals, they’re represented, you can get this mixed-state presentation in the residual stream. Another result that comes up in the paper… well, one result is that it’s linear. Even in retrospect, are you surprised by that or do you think you have a good story for why it should be linear?\n\n**Adam Shai:** I don’t have a good story, and in fact, computational mechanics, as far as I know, it does not get… Right? It just says that the mixed-state presentation should be there. It doesn’t say how or where.\n\n**Paul Riechers:** Yeah. I think when we decided to do these experiments, based on this theoretically-informed thing: somehow this geometry should be in there. These belief states should be in there. Even this geometry should be in there, but at least it’s going to be stretched out or something. Right? But it’s like, okay, there’s still definitely something to explain. I think I’ve seen titles and abstracts of papers that I feel like maybe help us to understand why things are linearly represented, but I don’t yet understand that well enough.\n\nIt’s too good to be true, but hey, good for us.\n\n**Daniel Filan:** Yeah. Maybe this has a similar answer, but another result that comes out is that at least in one of these processes that you train a transformer on, there’s not one layer in the residual stream that you can probe and get this nice simplex. You have to train a probe on all of the layers. Right?\n\n**Adam Shai:** Yeah.\n\n**Daniel Filan:** Is there a story there of just like, “Well, one layer would’ve been too good to be true,” or is there something deeper?\n\n**Adam Shai:** Actually in that case, there is a deep reason that computational mechanics gives for that finding. Although I should say we were not looking for that finding. I think one of the nice things about having a theoretical foundation for the way that you go about running your experiments is that sometimes phenomena jump out at you and then in hindsight you realize that the theory explains it. So in this particular process, this is the random random XOR process where if you start from a particular state, you generate a bit, a 0 or 1, then you generate another bit 0 or 1, and then you take the XOR of the previous two, and then you generate a bit, you generate a bit, XOR, so on and so forth.\n\nSo that’s your data-generating process. It has five states in its minimal generative machine, and that means that the belief state geometry lives in a 4D space. And actually unlike the fractal thing, you actually get 36 distinct states. It’s also quite a beautiful structure. Maybe there’s a way to splash that on the screen.\n\n**Daniel Filan:** Probably. We have a low budget.\n\n**Adam Shai:** Okay. But it kind of looks like an X-ray crystallography pattern, and so it’s aesthetically pleasing, but importantly, multiple of those belief states, even though they are distinct belief states, they give literally the same next token prediction.\n\nSo what does that mean? The transformer… all it needs to do, if we just think of the last layer of the transformer and how it actually gets made into a prediction for the next token, you just read off of the residual stream with the unembedding and convert it to a probability distribution. But that means that at the last layer, you actually don’t need to represent distinctions \\[other than\\] in next token predictions.\n\nAnd in fact, not only relative to the local thing of doing the next token predictions, but there’s no attention mechanism after that. So even if you wanted to represent it there, it could never be used there because it can never interact and send information into the future.\n\n**Paul Riechers:** Not of the next token, but everything else.\n\n**Adam Shai:** Yeah. But still, comp. mech. says the structure, these distinctions should be there. They don’t need to be in the last layer, but they do need to be somewhere. You see them in earlier layers. So that’s an explanation for why it’s not in the last layer.\n\n**Paul Riechers:** I think this hints at maybe how the transformer is using the past construction of the belief states; that if it’s earlier on, then you can leverage those across contexts a little better. So I think there’s strong interplay here between theory and experiment that there’s still a lot for us to figure out.\n\n**Daniel Filan:** Sure. Sorry, just a very basic conjecture I have after hearing you say that, is just that the parts that are relevant for next token prediction are in the final layer, the parts that are relevant for the token-after-that prediction are in the second-to-last layer…?\n\n**Paul Riechers:** Well, I don’t think it’s quite that clean, because if that’s the case, then you’d only be able to model Markov order N processes where N is the number of layers. So it’s not going to be quite that clean. But something like at the end, you really only need the last token distribution. But then also the theory says that somewhere in the model, somewhere in the residual stream, there should be these representations of the full distribution over the future. And it does seem that to be able to… for a certain context position to look back and utilize that, it would serve a purpose for those to come about early on.\n\nIt’s still not clear to me: why bother shedding them? They could persist as far as I’m aware, but there’s not a pressure for them to persist. So I’d like to understand that better.\n\n**Daniel Filan:** Sorry to get hung up on fractals, but I have another fractal question. So this random random XOR belief state thing didn’t form a fractal. Is that just because the fact that one in three bits is deterministic - does that just snap you to one of these relatively well-defined states? Is that what’s going on?\n\n**Paul Riechers:** I wouldn’t classify it that way, but the distinction, I think… One way to anticipate whether or not you’re going to have a fractal structure is whether the minimal generative mechanism is this thing I alluded to earlier, about whether it’s unifilar or not, whether it has these deterministic transitions between hidden states. So for the random random XOR, the minimal generative mechanism is a five-state thing, and it’s also a five-state unifilar thing. So it is the recurrent belief states. So you have these 36 belief states in general that, as you go through resolving these different types of uncertainty, you’ll eventually get to these five states.\n\nSo okay, that’s simple. Whereas if you have even a simple generative mechanism, let’s say you can have as small as a two-state hidden Markov model that generates the thing, but it’s non-unifilar, in the sense that if you know the current state of the machine and you know the next token, it still doesn’t tell you exactly where you end up. So the number of probability distributions induced can be infinite and generically is.\n\nSo in that setting, again, you’re folding your beliefs over and over through Bayesian updating. So in general, if you have a minimal generative model which is non-unifilar, Bayesian folding will give you a fractal, and if the minimal generative structure is unifilar, then you won’t. So that’s also kind of nice that we can expect that.\n\n**Daniel Filan:** Sure. Cool. Again, speaking of fractals, in the paper you basically say, “Hey, here’s the fractal we expect to see. Here’s the fractal we get.” And it’s pretty close in mean squared error. It’s way closer than if you had a randomly-initialized network or way closer than it was during the start of training. And my understanding is you’re measuring this in mean squared error terms, right? A similar question I could ask is, “Okay. Suppose that instead of the actual mixed-state presentation of the ideal process, I just had this thing I got from the network, how good a job would I do at prediction?” I’m wondering: is that a question that there’s a nice answer to? And do you know what the answer is? And if you do know what the answer is, please tell me.\n\n**Paul Riechers:** So it’s a little unclear to me what you’re asking, but you might be asking something like: can we do something with the activations if we don’t know the ground truth of the generative structure? Is that right?\n\n**Daniel Filan:** Or instead of measuring the distance between activations and ground truth by mean squared error of this probe, one thing you could do is say, “Okay, imagine this is the mixed-state presentation I have, how good a job am I going to do at predicting the underlying the actual thing?” Somehow the thing you’ve probed for seems like a lossy version of the underlying fractal. How much does that loss show up in prediction as opposed to reconstruction error?\n\n**Adam Shai:** The one thing you should be able to do is - and this is not an infinitely general answer to your question - but if you think of the fractal that is the ground truth mixed-state presentation, and then you think of coarse-graining that fractal to a certain amount, Sarah Marzen has [work](https://journals.aps.org/pre/abstract/10.1103/PhysRevE.95.051301) going through the math of the implication for predictive accuracy. And that intuitively (I think) makes sense.\n\nAnd in general, I think it should be possible to work out given any particular coarse-graining - so what does that mean in this particular case? It means if you take just kind of some volume in the simplex and you just kind of say, “I know I’m in this area, but I don’t know where exactly, I don’t know exactly which belief state I’m in, but I know I’m somewhere there” - you should also be able to calculate the consequences of that and its implications for next token predictive accuracy or cross-entropy loss. What that looks like in terms of the visual of the fractal is exactly the fractal just getting fuzzier.\n\n**Paul Riechers:** Yeah. Nearby points in the simplex do make similar predictions. So there’s some sense in which… Yeah, if it looks fuzzy, then you can almost visually say how that’s going to turn out for prediction and we can quantify that.\n\n**Daniel Filan:** Right. Am I right that this is because the simplex is distributions over hidden states and therefore being close by in the simplex is just close by in your whole-future predictions rather than just next token predictions?\n\n**Paul Riechers:** You can think of: how do you calculate probability of future tokens from a distribution over the model? And it’s basically linearly in terms of the distribution over the model. And so if you slightly reweight, there’s a continuity there. You can talk about closeness and a type of distance induced, but it’s not exactly the natural Euclidean one.\n\nScaling computational mechanics for transformers\n------------------------------------------------\n\n**Daniel Filan:** Gotcha. I guess the next thing I want to ask is just how to generalize from these results. My understanding is that both of the processes could be represented by hidden Markov models with five or fewer states. In general, of course, processes can have a bunch of states in their minimal hidden Markov model. How scared should I be of “maybe things are really different if you have a hundred or infinity states that you need to keep track of”?\n\n**Adam Shai:** You should be excited, not scared.\n\n**Daniel Filan:** Okay.\n\n**Adam Shai:** We can just go to the full thing. What’s the minimal generative structure for all of the data on the internet? That’s a large thing. It’s a very large thing. It is way larger than the number of dimensions in the residual stream of even the biggest model. It’s not even close.\n\nSo the natural kind of bottlenecks that exist in the strongest versions of artificial intelligence systems that exist today do not have the capacity to - in a straightforward way at least - represent the full simplex structure that this thing predicts, which means that there is at least one extra question about… Some compression must be going on. What is the nature of that compression? This is something we’re very excited \\[about\\] and looking at now.\n\nI think the main thing that I get from our original work, although all of these things about fractals and finding fractals and transformers, all that is true and these are good paths to go on… But the more general point, and the thing that gets us excited, is that it gives us evidence that this framework that we’re using to go about thinking about the relationship between training data structure, model internals and model behavior works.\n\nAnd now we can start to ask questions like “what is the best way to compress into a smaller residual stream than you really have? And so now we don’t have space to represent the full structure, how do we do that?” And we don’t have an answer to that at the moment. But it’s definitely, I think, one of the things we’re very excited to get a handle on.\n\n**Paul Riechers:** And at least the experiments become clear once you have the theoretical framework: now we know where the simple picture should break down and we can look there. So we are very excited to see how things start to break down and how the theory generalizes to understand more towards frontier models.\n\n**Daniel Filan:** Yeah. I guess you’ve maybe already talked about this a little bit, but I’m wondering: in terms of understanding these mixed-state presentations in transformers or just generally applying computational mechanics to neural nets, what are the next directions you’re most excited by?\n\n**Paul Riechers:** There’s an incredible amount that we’re excited by. We currently have this team of the two of us and collaborators in academia and also more generally in the AI safety interpretability community. But we’re really excited to create more opportunities for collaborations and scaling up because it’s a bit too much. But let’s get specific. There’s some obvious next projects: the AI safety world is a little obsessed by [SAEs](https://arxiv.org/abs/2309.08600) (sparse autoencoders), right? And features. What even are they? How do we know if we’re doing a good job? There’s no ground truthiness to it.\n\nAnd so one thing we’re excited to do is just say, “Well, let’s benchmark, in the cases where we know what the machine is trying to do, that these are the optimal belief states: how do belief states correspond to features?” And Adam alluded to this earlier, but one working hypothesis could be “features are these concise, minimal descriptors that are useful for pointing at belief states.” Okay. So let’s test that out. So we’re doing a collaboration right now where we’re trying to make this comparison. I think it’s going to be pretty empirically driven for now, but I think that should be quite useful broadly to know what we’re doing with features.\n\nThere’s a lot that I’m doing right now in terms of understanding internal representations more generally in a few ways. A lot of what we’ve done right now is, what do you expect to happen once the model has been well-trained? And then it’s like, “Okay, it should have this structure at the end.” But how should that structure emerge over training? And there’s a few ways in which we have a handle on that, and it’s something I’m very excited about. So I’m working actively on that, and we have a lot more. Do you have some favorites you want to talk about?\n\n**Adam Shai:** Yeah. It’s kind of a problem in that… For instance, just last week there was [this Simons Institute workshop](https://simons.berkeley.edu/homepage) that had to do with AI and cognitive psychology. I went and I watched a few of the talks, and this is something increasingly that’s been going on with me - not to get too personal - but every time I see a talk about AI, often my reaction is like, “Ah, I think we can get a handle on this using our framework.”\n\nSo I just have a list of papers that keeps on growing about things I would like to explain from a comp. mech. point of view. But just to name a few of them: there’s always in-context learning, which I think is a big one. You can start to think of a way to get a handle of that using non-ergodic processes. So what that means is, in the examples that we’ve talked about in our public-facing work so far, the generative models are all single parts. There’s one hidden Markov model. You go recurrently between all the states.\n\nYou can also imagine processes where you just take a bunch of those and put them side by side. In addition to the beliefs having to see more data and try to figure out in which state in the process you’re in, you also have to figure out which process you’re in. So there’s this extra meta-synchronization process going on.\n\n**Paul Riechers:** And it’s natural also for the type of data that you’d be training on, because you have this whole hierarchical structure of “what language, what genre, what mood”, and there are these recurrent components, but many different recurrent components.\n\n**Adam Shai:** And you can imagine that dynamic being quite different for sets of processes which overlap in their internal structures more or less. And that might have implications for which kind of abstract structures you use and take advantage of in in-context learning. So that’s one thing I think we’re quite excited about.\n\nAnother one is related to that: just talking more generally about the capabilities of these systems and getting a handle on “What is a capability? What are the different types of computational structure a model can take?” Because one of the things that I really love about computational mechanics is it gives you a handle on “what does it even mean to talk about structure in these systems?” Structure in training data, structure in model internals, structure in behavior.\n\nSo for instance - and this is probably coming from my neuroscience background a little - but what do we do in neuroscience when we want to study some cognitive capacity like abstraction or transfer learning or generalization? Well, we want to run our experiments on model organisms like rats and stuff like that. So what we try to do is we try to find the simplest toy example of abstraction, something that has the flavor of abstraction in its minimal form that we can train a rat to do.\n\nThen we can go into the rat’s brain while it’s doing this abstraction task and we can do the neuroscience equivalent of mechanistic interpretability on it. So that was my life for a decade. If we can find a form of an abstraction task in the form of these HMMs - and I think I have an idea for one - now we can say… For instance, let’s say you have two generative structures next to each other, and they have the same hidden structure except they just differ in the vocabularies they speak in.\n\nSo let’s say one speaks in As and Bs and one speaks in Xs and Ys. Now, when we look at these as humans, it’s very obvious that they have the same hidden structure. They have the same abstract structure, but they speak in two different vocabularies. What would it mean for a transformer to understand that these two processes have the same abstract structure?\n\nSo if you train a transformer on this set of tasks, one thing it could do is learn them both optimally, but not understand that they have the same structure. Another thing it can do is learn them both optimally, but understand they have the same structure. Behaviorally, you could figure out the difference between those two cases if you test them on a held out prediction task, which is combining histories of As, Bs, Xs and Ys together, just holding the abstract structure constant.\n\nAnd if they can still do the optimal prediction task now, combining past histories of the two different vocabularies that they’ve never experienced together before, then you can say they’ve abstracted. In addition, because we have a theory for understanding how the model internals relate to any kind of behavior, we can make a prediction of what would underlie that abstract structure. For each of the processes, for each of the generative structures, there should be a simplex geometry that’s represented in the residual stream of the transformer. And the prediction would be that if the network is able to abstract and understand this abstract structure, then these simplex structures will align in space. Whereas if not, they might lie in orthogonal subspaces.\n\nAnd now all of a sudden you start to think of the relationship between compression: if the residual stream doesn’t have enough space to represent them in orthogonal subspaces, maybe it has to put them in overlapping subspaces, and that you start to realize that there might be a story in general. Like we were talking about before, in the real case, we don’t have enough space to represent the full structure, so you have to do some compression. And because of that, you might have to… The network is forced to take what should be separate simplex structures and kind of overlap them. And that might be the thing that gives rise to out-of-distribution generalization and abstraction and stuff like that. So that’s another thing that I’m quite excited about.\n\nHow Adam and Paul found computational mechanics\n-----------------------------------------------\n\n**Daniel Filan:** Sure. I’d like to move on a bit to the story of how you guys got into this, if that’s okay?\n\n**Paul Riechers:** Sure.\n\n**Daniel Filan:** Maybe I’ll start with Adam. So my understanding is that you have a neuroscience background. Based on what I know about neuroscience, it’s not predominantly computational mechanics, and it’s also not predominantly saving the world from AI doom. How did that happen for you?\n\n**Adam Shai:** Just last year even, I was towards the end of a postdoc in neuroscience and experimental neuroscience. Like I was saying before, I was training rats on tasks and I was studying the role of expectations and predictions and sensory processing in the visual cortex of these rats. Actually, I remember running the experiments on these rats, and it was like a week after ChatGPT came out and I was talking to ChatGPT and just really shocked (I think is the right word) by its behavior. And just had a moment of… everything… All of my beliefs and intuitions for what I thought must underlie intelligent behavior were just proven to be wrong. I guess, I had all these intuitions that came from my neuroscience background about recurrence and complicated dendritic structures and all kinds of details, and I was like, “There’s some secret sauce in neuroscience that has to underlie intelligent behavior.” Meanwhile, I’m studying these rats on these very simple tasks that are basically not much more than left, right. That’s simplifying it, I’m straw-manning it a little bit, but they’re not super complicated linguistic tasks that humans perform.\n\nChatGPT… after learning about the transformer structure and being like, “these things are feedforward. They’re weirdly linear. They’re not totally linear, obviously, and that’s important, but they’re much more linear in structure…” Basically to whittle it down as I was like, “The architecture is not interesting enough to give rise to these interesting dynamics, given what I think about the relationship between mechanism and intelligent behavior.” That was a shocking thing. And then, also realizing that the existence of the system pushed more on my intuitions about intelligence than basically any paper I had read in neuroscience in a while.\n\nIt was actually emotionally a little difficult, I have to say. But, yeah, that point is when I started thinking about that transition. I’m like, “If my intuitions are wrong about this, then what that means for the future…” GPT-4 came out not long after that, and that was another like, “Whoa. Okay. Apparently, it wasn’t just this one thing. Apparently, it can get better faster.”\n\nSo, yeah. That’s when I started going… And there’s always some tension here when you realize the safety implications for the future. And then, on the other hand, it’s also just actually interesting, literally just a very interesting… just intellectually, it’s super cool. And there’s this tension between those two things that I feel quite often. I’m pretty sure I’m not the only one in the AI safety community that feels that.\n\nBut even from my academic interests, I’m interested in intelligence. I’m interested in the mechanisms that underlie intelligence. I’m interested in the way that networks of interacting nodes can give rise to interesting behavior. So I guess that’s the way that I found myself in AI safety. I guess the other relevant thing is, for a long time, eight years ago or something, I had randomly run into comp. mech. as a neuroscientist, just reading one of the papers. It just struck me - not that I understood it at all; I did not have, and I still don’t really have, the mathematical acumen to grok all of its complexities - but something about the framework really captured exactly what I was interested in getting at in neuroscience, which had to do \\[with\\] “what is the relationship between a dynamical process and the computation it performs?”\n\nAnd so for a while, I would annoy all my neuroscience lab mates and stuff about comp. mech., and I probably sounded like a crazy person, but I was never able to really figure out how to apply it in any useful way. So I’m super excited to start working on these neural networks and applying it, actually getting somewhere with it, it’s an amazing feeling. I’m very lucky to have met Paul and the other comp. mech. folks that are helping me with this.\n\n**Daniel Filan:** \\[Paul,\\] first of all, how did you get interested in computational mechanics to begin with?\n\n**Paul Riechers:** That goes back a long time. I don’t know if I can remember. I have a background in physics. I’ve done theoretical physics. I also did a master’s in electrical and computer engineering, but then was back to theoretical physics for a PhD. I was interested in emergent behavior, complexity, chaos, all this stuff. Who isn’t? But I think early on in (let’s say) 2005, I had [Jim Crutchfield](https://csc.ucdavis.edu/~chaos/) as an undergrad teacher at UC Davis, and I just thought the stuff he did was super cool.\n\n**Daniel Filan:** And he does computational mechanics?\n\n**Paul Riechers:** Yeah. He’s been the ringleader. It’s been Jim and colleagues building up computational mechanics. So I guess from that point, I realized what Jim did. I had some idea of computational mechanics, and I was going on a trip with my buddy. My buddy, Seth, is like, “Well, what would you do if you could do anything?” I was like, “Oh, well, I’d probably do this computational mechanics stuff, but I don’t know if I’m smart enough” or something. But then it turns out that if you believe in yourself a little bit, you can do lots of things that you’re excited about.\n\nSo, I ended up, long story short, doing a PhD in Jim’s group. So I did a lot of computational mechanics during my PhD and have done a bit afterwards. But in physics, I’ve worked on all sorts of things, it’s been a big mixed bag of quantum information, ultimate limits of prediction and learning, which is where the comp. mech. comes in, non-equilibrium thermodynamics, but trying to understand just what is reality like? And so my interest in comp. mech. was because it was very generally, “what are the limits of prediction and learning?” That seemed fascinating to me. Maybe you have this follow-up question of, “How did I get into some AI safety stuff?” Right?\n\n**Daniel Filan:** Truly a master of sequence prediction.\n\n**Paul Riechers:** Yeah. I felt very unsettled a year or more ago with what I was seeing in AI, like generative images, ChatGPT, this stuff. I was very unsettled by it, but I also didn’t think I had actually anything to contribute. Actually, when we were talking in Jim’s group about neural networks, even back in 2016 or something, it was just like, “This stuff is so unprincipled. They’re just cramming data into this architecture. They don’t know what they’re doing. This is ridiculous.” It was always just like, “Let’s do the principled thing. Let’s do comp. mech.”\n\nI think somehow, this was just in my mind: neural nets are the unprincipled thing, and I do the principled thing, but now these unprincipled things are becoming powerful: well, \\[redacted\\], what do we do? And it was out of the blue that this guy, [Alexander Gietelink Oldenziel](https://sites.google.com/view/afdago/home) reached out to me and he’s like, “Oh, comp. mech. is the answer to AI safety.” And I was like, “Well, no. I know comp. mech., you don’t, you sound like a crazy person”, but I already was concerned with AI safety, so I’m like, “OK, I’ll hear you out.”\n\nAnd so I explained to him why it wasn’t going to be useful: for example, that these architectures were, as far as I understood them, feed-forward, computational mechanics was about dynamics. But we kept talking and actually he did convince me that there’s something about thinking about the dynamics through context which is really important. And I don’t think Alexander quite made the case for how they’re important but somehow I realized, “oh, this will be important”.\n\nAnd actually at that point I started talking to Adam, this was almost a year ago I guess, where Adam then started helping me to understand the details of transformer architectures and this stuff. And we were talking with [Sarah Marzen](https://www.sarahmarzen.com/) and others, and started to realize “OK, actually this will help us to understand model internals that people just aren’t thinking about, it’s going to help us to understand behaviours people just aren’t thinking about”.\n\nAnd so then when I realized that “oh, there’s actually something here”, it felt both research-wise super exciting, but also a moral imperative: I care about this, I need to do something about it. So I started just reaching out to people at Google, whatever connections I had, and be like “Hey, I feel like I have something really important to contribute here, how should I go about this?\n\nAnd the broad community of interpretability, in both industry and outside, people were very helpful for directing me towards, “hey, there’s this thing [MATS](https://www.matsprogram.org/), which I don’t know, maybe it’s too junior for you…” but actually for me, it was great. So I was a MATS scholar beginning of this year. For me, it was great just being surrounded by people that really understood the ML side, because that hasn’t been my strength, but now I’ve grokked a lot of that. And so I’m really grateful that there has been a community that just cares about this stuff. And so many people - like with [PIBBSS](https://pibbss.ai/), Nora \\[Ammann\\] and everyone, so many people have just been asking, “How can we help you?” And that’s been really cool.\n\nSo Adam and I have been talking for a while. We’ve been working on this stuff, and I think earlier in the year we were saying, “This stuff’s going really well. It seems like we have a lot to contribute. At some point, we should maybe do an organization on this, a research organization.” And so we were open to that idea, and then things fell in place where it was like, “Oh, I guess we can do that now.”\n\n**Adam Shai:** Yeah. I’ll second the thing that Paul said about how supportive the broad AI safety community has been. For the last six months, I’ve been a PIBBSS affiliate, and that’s another AI safety organization. And they’ve been really helpful to both of us, I think, supporting us in starting Simplex and the research and everything.\n\n**Daniel Filan:** Sure. I think I understand your individual journeys. One thing that I still don’t quite understand is: how did you guys meet and sync up?\n\n**Paul Riechers:** It’s [this Alexander guy](https://sites.google.com/view/afdago/home) again?\n\n**Adam Shai:** Yeah, it is Alexander.\n\n**Daniel Filan:** This is [the second episode](https://axrp.net/episode/2024/05/07/episode-31-singular-learning-theory-dan-murfet.html) where Alex cold-emailing people has played a crucial role in someone’s story.\n\n**Paul Riechers:** He’s some wizard in the background.\n\n**Adam Shai:** Well, I met Alexander… it’s probably almost two years now ago at some rationality event thing that I actually wasn’t even invited to, I was just tagging along, I think. We started talking about rats and minimum description length things, and I was like, “Oh, that sounds like a less principled version of comp. mech.,” and I started ranting about comp. mech., and then me and Alexander kept on talking about that and trying to think if there was a way to apply it to neural networks. I had exactly the same reaction \\[to Paul\\], I’m like, “I don’t know. These aren’t recurrent, so I don’t really know how they would be relevant to transformers.”\n\nAnd then I think a little while after that, Alexander reached out to Paul, and I was having a very frustrating day with the rats, I think. And I was like, “Ah, I really want to do comp. mech..” I messaged Sarah Marzen and we had a meeting. It was really great. And then eventually, all four of us got together and started trying to think of ways to apply comp. mech. to neural networks.\n\n**Paul Riechers:** So me and Adam were still both in academia. I was actually about to take on something that would’ve led to a tenured role at a university in Spain. We were looking forward to sangria, all this stuff. But we were starting to do this research on the side, and this thing, it was starting to go well. And I realized “Oh, okay, I need to dedicate myself to this.”\n\nSo I think for me, the first engagement of “let’s apply comp. mech. to neural nets” has been in collaboration with Adam. And so it’s been really a great, I think, complementary set of skillsets coming together. And I think we have a lot of shared priorities and vision.\n\n**Adam Shai:** It’s been super fun.\n\n**Paul Riechers:** Yeah.\n\nComputational mechanics for AI safety\n-------------------------------------\n\n**Daniel Filan:** Awesome. So I have a question about the safety application. So my understanding is that basically the story is: have a principled understanding of what neural nets might be doing, get some tools to have a more principled way of saying what’s going on there, and hopefully just use that to feed into making them safe, designing them the way we want them to be. Is that right? Or is there a different plan you have?\n\n**Paul Riechers:** Yeah, I mean, that’s an aspect of it, for sure. But I think for me, another important aspect is enabling better decision-making. People talk a lot about “what’s your P(doom)?” or probability of this or that. I feel like we understand these things so little. We don’t even know what the support is, probability over what? We don’t know what’s going to happen. We don’t know the mechanisms, everything. We don’t even have a shared language. There isn’t science being done the way that I think it needs to be and at the scale that it needs to be.\n\nSo I think computational mechanics can - maybe optimistically - help to figure out what are the things that we’re most confused about? How do we go about that? And build up some infrastructure. And some of that’ll be at a low level, some of that’ll be at a high level, and probably going back and forth between those.\n\nSo I feel like understanding is very important in general to make good decisions. One \\[reason\\] is, if you understand well, then you could have new interventions. So that’s one thing. And then the other \\[reason\\] is you might understand well that this will go very poorly in any case, because now we have a more mechanistic understanding of why or how things would go good or bad. And that’s this other part that I think’s really important.\n\n**Adam Shai:** A few other things that are related to that. So just to take a very concrete example: why don’t we have rigorous benchmarks for SAEs? Why don’t we have a data set, or transformer, or a combination of both, where we know exactly what the correct features are so we can tell whether or not a specific form of SAE, with specific architecture and losses and stuff, whether it works better or worse than other things?\n\nAnd I think it’s because we don’t have an understanding for a lot of the basic concepts with which we talk about AI-safety-relevant things like features (in this particular case, it’s features.) And the hope is, if we can get a better understanding, that we can even do things like build benchmarks: a pretty simple thing. I think this also carries over to things like evals, where if we had a better understanding of what fundamentally a capability was - what are the different types of capabilities? - we could hopefully also build better evals, and we can relate any particular eval that exists right now to things going on inside of the transformer.\n\nThis also relates to things like out-of-distribution behavior. If we can relate the behavior in general of some internal structure in a transformer to all of the things it can do as it spits out tokens at the end, then we will know the space of out-of-distribution behavior. And nothing comes for free. It’s an enormous amount of work, and it’s not a simple thing to go from this theoretical understanding we have right now to the application to the largest models that exist. That’s not the statement, but at least we have some footing here. So that’s, I think, kind of the idea.\n\n**Paul Riechers:** And just one more thing I want to add on this is there’s a lot of power in comp. mech. in that the theory is model-agnostic. I mean, most of the results aren’t actually about neural nets - I was surprised it applied at all. And it’s not specific to transformer architectures, whereas maybe some mechanistic interpretability stuff would be architecture-dependent. So I think that’s powerful: for any particular architecture, you can then back up the implications. But as architectures change, this learning should be able to change, be adapted with us. So I do think that’s important.\n\n**Daniel Filan:** Yeah, so that’s one story about how computational mechanics could be useful. One obvious way to help push this forward is work on computational mechanics, join Simplex or look at their list of projects and work on them. I wonder if there are any synergies with other approaches in AI safety that you think work really well with comp. mech., that are good complements to it?\n\n**Adam Shai:** These connections haven’t been made formally. I think there’s an obvious connection to mech. interp. in general, which is kind of a big field. But for any kind of mechanistic understanding one has from the standard toolset of mech. interp. - so these are things like looking at attention head patterns, causal interventions of different forms - one can ask now how these relate to the task of carrying out the mixed-state presentation.\n\nI’m hoping that there are synergies with a more [dev. interp.](https://devinterp.com/) point of view. I think there’s already a bunch of pieces of evidence that \\[suggest\\] that that will work out. We can see this kind of belief state geometry forming over training. Paul has theoretical work talking about the learning process of a parameterized model from the point of view of comp. mech..\n\nSo hopefully we can also connect this thing to the training process itself, which is kind of a dev interp, maybe SLT-flavored way of looking at things. I mentioned evals before… how many different AI safety approaches are there?\n\n**Daniel Filan:** Unclear to me. So if I’m thinking about how computational mechanics does apply to safety, does help things being safe: we’ve talked a bit about the theory, and it seems like it’s mostly a theory of doing good inference about stochastically-generated processes that… I’m imagining \\[that\\] someone else is generating this process, I’m inferring it, and then, computational mechanics is saying, “Okay, what’s going on there? What’s going on in my head?”\n\nBut if I think about AI, one of the crucial, important features of it is that it involves things acting in the world and changing the world. Not just inference, but this whole loop of inference and action. And I’m wondering, are there developed computational mechanics ways of talking about this? Or is this an open area? I think Adam mentioned, I forgot what word it was, but there was something about computational mechanics with two systems that were maybe influencing each other? Is that enough? Do we need more?\n\n**Paul Riechers:** Yeah, I guess I’d say there’s some basically preliminary work in computational mechanics about not just what it’s like to predict or to generate, but really putting this together in an agentic framework. Instead of epsilon-machines, there’s [epsilon-transducers](https://arxiv.org/abs/1412.2690), which is: what’s the minimal structure and memoryful structure for an input/output device?\n\nSo that, I think, is really relevant work. Also, we can probably look at model internals and things like that. But I think there’s a totally different level at which computational mechanics can be applied that we’re excited for, but just don’t have the bandwidth for right now, in terms of what to expect with interacting agents at a high level. There’s a lot of ways to go about this. There’s theory work that’s been done with POMDPs, but I think that there is [work](https://www.nature.com/articles/s41534-016-0001-3) that has been done in terms of basically these memoryful input/output agents. There’s a lot to do that I indeed feel that’s important for understanding, if there’s any hope for understanding what the emergent behaviors of interacting agents would be. Sounds hard. I’m curious about that. Things need to be developed a lot more.\n\n**Adam Shai:** Yeah. Just to add onto that, some of the conceptual things that would be exciting to try to get a handle on, would be things like, “what does it mean for an agent to understand some structure in the world? What does it mean in terms of its internal states?” And then going a level beyond that, “What does it mean for an agent to have a model of itself, inside of itself, and what is that computational structure that subserves that?” So I think these are pretty fundamental questions that the work hasn’t been done \\[for\\] yet, but one could kind of imagine using and extending the framework of transducers to get at that, which would be super exciting.\n\nFollowing Adam and Paul’s research\n----------------------------------\n\n**Daniel Filan:** Listeners have now heard the exciting promise of computational mechanics, cool things they can do, cool things that work with it, cool potential threads of using it to understand agency. If people want to follow your guys’ research, your writings, how should they go about doing that?\n\n**Adam Shai:** I think the main way is [simplexaisafety.com](https://www.simplexaisafety.com/), our website. You can contact us. We’re very excited to collaborate and work with other people. So if anyone is interested, they should certainly feel free to contact us through there.\n\n**Paul Riechers:** Yeah, for sure. I’d say that people that feel like there’s a natural connection, if you’re feeling inspired to make our mission go well, or you see a natural collaboration, please feel free to reach out, because we feel like this work’s really important and we’re just a bit bottlenecked. So I think working with good people and growing a vision together would be really nice.\n\n**Daniel Filan:** Great. Well, thanks very much for coming here today. It was great talking.\n\n**Paul Riechers:** Yeah. Thanks so much.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Filming occurred at [FAR Labs](https://far.ai/labs/). Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. I’d also like to thank Joseph Miller for helping me set up the audio equipment I used to record this episode. To read a transcript of this episode, or to learn how to support the podcast yourself, you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me, at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nSometimes, people talk about transformers as having “world models” as a result of being trained to predict text data on the internet. But what does this even mean? In this episode, I talk with Adam Shai and Paul Riechers about their work applying computational mechanics, a sub-field of physics studying how to predict random processes, to neural networks.\n\nTopics we discuss:\n\n * What computational mechanics is\n * Computational mechanics vs other approaches\n * What world models are\n * Fractals\n * How the fractals are formed\n * Scaling computational mechanics for transformers\n * How Adam and Paul found computational mechanics\n * Computational mechanics for AI safety\n * Following Adam and Paul’s research\n\nDaniel Filan: Hello, everybody. This episode I’ll be speaking with Adam Shai and Paul Riechers, who are co-founders of Simplex. Simplex is a new organization that takes a physics of information perspective on interpretability, aiming to build a rigorous foundation for AI safety. Paul has a background in theoretical physics and computational mechanics, while Adam has a background in computational and experimental neuroscience. For links to what we’re discussing, you can check the description of the episode and a transcript is available at axrp.net. Paul and Adam, welcome to AXRP.\n\nAdam Shai: Yeah, thanks for having us.\n\nPaul Riechers: Thank you.\n\n\nWhat computational mechanics is\nDaniel Filan: You guys work on doing computational mechanics for AI safety type things. What is computational mechanics?\n\nPaul Riechers: I’m happy to give a bit of a background. Computational mechanics has basically grown within the field of physics, mostly, out of chaos theory, information theory. For what purpose? Well, I mean physics has always been concerned with prediction, that we want to write down some equations that tell us maybe if we know the state of the world, what does that imply for the future? Some planets move around and all these things.\n\nAnd then there became a p",
      "wordCount": 16531
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "bh7uxTTqmsQ8jZJdB",
        "name": "Probability & Statistics",
        "slug": "probability-and-statistics"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "N9fQpF5hkkqGkrN5N",
    "title": "AXRP Episode 35 - Peter Hase on LLM Beliefs and Easy-to-Hard Generalization",
    "slug": "axrp-episode-35-peter-hase-on-llm-beliefs-and-easy-to-hard",
    "url": null,
    "baseScore": 21,
    "voteCount": 4,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-08-24T22:30:02.039Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/tEge5uo4E-A)\n\nHow do we figure out what large language models believe? In fact, do they even have beliefs? Do those beliefs have locations, and if so, can we edit those locations to change the beliefs? Also, how are we going to get AI to perform tasks so hard that we can’t figure out if they succeeded at them? In this episode, I chat with Peter Hase about his research into these questions.\n\nTopics we discuss:\n\n*   [NLP and interpretability](#nlp-and-interpretability)\n*   [Interpretability lessons](#interpretability-lessons)\n*   [Belief interpretability](#belief-interpretability)\n*   [Localizing and editing models’ beliefs](#localizing-editing-model-beliefs)\n*   [Beliefs beyond language models](#beliefs-beyond-lms)\n*   [Easy-to-hard generalization](#easy-to-hard-generalization)\n*   [What do easy-to-hard results tell us?](#easy-to-hard-results)\n*   [Easy-to-hard vs weak-to-strong](#easy-to-hard-vs-weak-to-strong)\n*   [Different notions of hardness](#different-notions-of-hardness)\n*   [Easy-to-hard vs weak-to-strong, round 2](#easy-to-hard-vs-weak-to-strong-2)\n*   [Following Peter’s work](#following-peters-work)\n\n**Daniel Filan** (00:00:08): Hello, everybody. This episode, I’ll be speaking with Peter Hase. Peter is an AI researcher who just finished his PhD at UNC Chapel Hill, where he specialized in natural language processing and interpretability research with a special interest in applications to AI safety. For links to what we’re discussing, you can check the description of the episode, and a transcript is available at AXRP.net. All right. Peter, welcome to AXRP.\n\n**Peter Hase** (00:00:33): Thanks so much, Daniel. I’m excited to be here.\n\nNLP and interpretability\n------------------------\n\n**Daniel Filan** (00:00:35): I’m excited to have you on. So my understanding is that most of your work is in interpretability, roughly interpretability in language models. Is that fair to say?\n\n**Peter Hase** (00:00:46): Yeah. That’s right. I’ve been in an NLP \\[natural language processing\\] lab for my PhD, so we work mostly with language models, but a lot of it, in terms of methods, evals, has been focused on interpretability.\n\n**Daniel Filan** (00:00:55): Actually, maybe one thing I want to ask is: I have the impression that you were into language models even before they were cool, doing NLP before it was cool. Right? So just today, I looked at your Google Scholar and scrolled down to see the oldest paper you were a co-author on. It’s [this 2018 paper on algorithmic sonnet generation](https://arxiv.org/abs/1811.05067). 2018, before the rest of us had caught up. What got you interested in NLP?\n\n**Peter Hase** (00:01:23): I remember the project you were talking about. That was when I was an undergrad. I feel so lucky to have had the opportunity to do a little special projects class, actually with [Cynthia Rudin](https://users.cs.duke.edu/~cynthia/) at Duke, in my undergrad. I mean, I was interested in language. I was interested in psychology and linguistics in my undergrad, and very interested in language, and increasingly interested in machine learning and statistics. And so it was just a great intersection of the topics and learning about language models - which, at the time, were really LSTMs - and getting an opportunity to apply those to a fun task. Even then, of course, I admitted it wasn’t necessarily that consequential (poetry generation) but it was certainly a great opportunity to get to work with language models a bit.\n\n**Daniel Filan** (00:02:13): Like you mentioned, the bulk of your work has been in interpretability. What got you interested in that aspect?\n\n**Peter Hase** (00:02:22): That was also an interest that developed in undergrad. So I think a long time ago, there were many different arguments put forth for why interpretability was a good thing to study. At the time, there was definitely a very intuitive draw, and there still is an intuitive draw, that it’s just good to know how these things work. It’s just good to know how AI systems work, how language models work.\n\n(00:02:47): They were doing increasingly interesting things. At the time, there was so much progress in vision. This was 2018, so there’d been a lot of progress in RL. Language models - I think by 2018, GPT-1 was out, and I think GPT-2 was coming out in spring 2018 or ‘19. It was just very clear that these systems were making a lot of progress and doing fundamentally interesting things. From a safety perspective, it’s like, “Gosh, we should know how they work. We should be able to explain their decision-making process.”\n\n**Daniel Filan** (00:03:25): This is kind of a broad question, but how would you say interpretability is doing as a subfield?\n\n**Peter Hase** (00:03:31): Well, this is a great question. I tend to be, I think, optimistic when talking with people from other subfields or who are working on capabilities research or other research areas. I probably come off as a little bit pessimistic when I’m talking with my colleagues about this. Let me be clear: there’s definitely a lot of progress being made, where we have better evals, we have a better understanding of when do we have a ground truth? When are we speculating about the reasoning process? What would it mean for an interpretation or an explainability method to be useful, and what could it be useful for downstream? This picture has just become a lot clearer in the past five to six years.\n\n(00:04:17): One of the reasons I’m pessimistic, at least when it comes to colleagues, is we just end up talking about all the false positives, and false starts, and like, “Oh, the reason this result didn’t hold up was because of this”. I think some of this is decently high-profile. People might know about things like feature attribution or saliency maps. So this was a popular \\[method\\] and one of the first major methods for trying to get a sense of what neural networks were doing, and you could think of this as being a 2015 to 2016 method, which was to say, “Okay. If you have a vision model, what’s it looking at? Is it looking at the dog in the image? Is it looking at the background? Is it looking at the human in the image?”\n\n(00:05:08): People were really excited, because this was one of the first ways to generate… I mean, the images just looked good. The visualizations looked good, and you could say, “Wow. It really seems like the neural network thinks this is a husky because it’s looking at the snow in the background, and not because it’s looking at the dog, per se.” So people were really excited about these methods, and then if you’ve worked in the subfield for a while, you know how these methods have had a bit of a fall from grace. They didn’t turn out to be useful in human studies for the most part. There’s been theoretical work showing that some of these popular feature attribution and saliency methods can do no better than random in certain settings. There’ve been a lot of hard-learned lessons in the subfield in terms of what to trust and what’s promising to run with in the long term.\n\n**Daniel Filan** (00:06:01): I’m wondering if you have an agenda of questions you think that it’s important for the interpretability field to answer, and if so, what are they? Where should we be looking? What should we be aiming for here?\n\n**Peter Hase** (00:06:14): I think we’re still in the stage of figuring out what methods are good and what evals tell us when we’ve created something useful. I don’t think we’re yet at the stage where we have the tools, and we’re mainly interested in detecting bad reasoning processes or detecting deception in language models. We’re not yet at the stage where we’re trying to catch safety failures. We’re still at the stage where we’re trying to build tools that would let us catch safety failures, and we’re trying to build evaluations for the tools so we know which tools would work at that. So it’s still pretty upstream. I’ll stop there and actually ask you to maybe elaborate on the question a bit so I can keep going.\n\n**Daniel Filan** (00:07:08): Yeah. I guess it seems like a view of more basic science. The reasons to do interpretability is we know that something about understanding model behavior, understanding model internals, why things are happening, something around that is going to be useful. And if we did some more basic science, we would just have a better sense of what the important questions to ask are. Is that roughly a good gloss of your view, or am I missing something?\n\n**Peter Hase** (00:07:40): Okay, thanks. So the research stage we’re at, I think, is still basic science. So I gave the kind of intuitive motivation before for interpretability and why I think that’s an exciting area. It’s: we want to know how these things work. It’s: they’re so fascinating, they do such interesting things, we want to know how they work. This is the intuitive pitch. I think that the strongest pitch that has emerged over time for interpretability research is that we need something that goes a little bit beyond testing. So all models get tested on all kinds of datasets, benchmarks, evals. We’re looking for dangerous behaviors. We’re looking for dangerous capabilities. We want to know what kinds of reasoning and knowledge that models possess.\n\n(00:08:25): So really, I think the best pitch for interpretability is, what can our tests not catch? So one thing that our tests can’t catch a lot of the time is the underlying reasoning process. So if we just have a huge multiple choice exam that is going to tell us if the models have dangerous bioweapons development capabilities or the models have really strong theory of mind such that they could operate in a social setting and either be cooperative or intentionally deceptive… if we just have surface-level, “Let’s prompt the model and see what text it outputs,” kind of tests for that thing, we can’t exhaustively test every relevant scenario.\n\n(00:09:13): There are settings where we’d be interested in deploying the model when it’s interacting with people. It might be more or less knowledgeable or aware that it’s interacting with people, and there’s going to be settings where we can’t actually test the thing we’re interested in or we can’t exhaustively test the model. That’s especially the setting where we want to open up the hood and figure out what’s going on inside, and be able to say, “Okay. Yes. It did this multiple-choice problem correctly, and it had a really impressive strong reasoning process for how it got there. We’re pretty sure it’s actually going to generalize beyond just the things we’re testing.”\n\n(00:09:52): Or we haven’t deployed it in a setting where it is cooperating with people in the real world yet, but we’ve leveraged some interpretability method to say, “Yes. This model fully intends on cooperating with people. Even if it knew that it could slightly better optimize one of its incentives at the expense of harming a human, we know it wouldn’t do that, because we’ve been able to truly inspect its reasoning process underneath.”\n\nInterpretability lessons\n------------------------\n\n**Daniel Filan** (00:10:19): So we’ve been doing this interpretability work for a while, right? What things do you think we’ve learned out of interpretability research?\n\n**Peter Hase** (00:10:30): So I think one big thing we’ve learned is that we really want the methods to be useful for some downstream purpose. So that means when we have an interpretability tool, like we’re inspecting what features the model relies on, we want that to enable us to do some debugging. We want that to enable us to catch failures that we might not have been aware of before. We might want that to make the decision-making process more clear to an end user so they can decide if it was reasonable or not. This comes up in, for instance, this problem called algorithmic recourse, where a person is being classified by a model, and they want to understand why they got the decision they did. So a lot of it’s increasingly gearing our evals towards these downstream use cases so that we can make sure that we’re actually getting good signal on the methods we’re developing.\n\n(00:11:29): So that’s one broad lesson, I think, and I could say a little bit more about some of the more upstream evals that still seem valuable to me. So that’s figuring out methods that actually improve safety of models in some tangible way, basically, and going back to what I said before, especially in a way that’s complementary to testing or complementary to other kinds of evals.\n\n(00:11:57): I think one of the other lessons is - this will be more object-level - we’re learning that language models can generate really plausible-sounding textual explanations of their decision making that aren’t actually how they’re reasoning. So this is just an immediate object-level lesson about how language models work. Their ability to chat with people is really impressive. Their ability to offer justifications for their answers is really impressive, and we’re starting to catch them out in inconsistencies via some cleverly-designed tests that show that what the model says is not really what it was thinking a lot of the time.T hat’s, I think, a very important insight in terms of interacting with the models in text. I’d say that’s more in the natural language explanations category in terms of what research stream that is.\n\n(00:12:51): Then, there’s this area of mechanistic interpretability and other kinds of probing research, historically in NLP, that I’d say is a setting where we’re really gaining traction on figuring out how models represent things. A lot of the work in 2015 and 2016 was focused on looking at the input and \\[working out\\] “What part of the input is this model looking at?” For vision models, you’d get heat maps that would light up over a part of an image that the model might be looking at. In a text setting, you’d get text highlights. So you’d say, “Oh. It’s these words, and we’re going to highlight them. That shows you what the model’s looking at.”\n\n(00:13:32): I think we’re really starting to go deeper than that. We’re really starting to be able to say, “Okay. Here are the hidden activations in the model,” and there’s been one development I’ll point out. It might’ve been that we used to say, “Okay. Here are the neurons, and here are the neurons that represent this or the neurons that represent that.” There’s been some really interesting mathematical progress, I think, on showing it’s not just individual neurons, but it’s particular combinations of neurons that might represent a certain feature. So you turn this cluster of neurons on, and that means the model has definitely detected that this text is discussing soccer as a sport. Or you have this other cluster of activations or neurons that have been turned on, and it’s like, okay, now, it’s discussing soccer as a political phenomenon or governing bodies of soccer.\n\n(00:14:19): Very abstract features of model inputs… We’re starting to connect the dots between those abstract features and model internals, and how the models are actually representing them inside, and then, after that, how the models are using those representations. So we might know that, okay, the model has detected something, and now, how is it going to influence the decision? People are developing tools for saying, “Okay. Yes. This feature’s been detected, and it plays an important role in the model’s answer.”\n\n**Daniel Filan** (00:14:53): So your first two points of things we learned - it’s important to get some sort of downstream benefit from your interpretability method or peg it to, “Does it actually help you do such and such task?”, and large language models are really good at faking explanations of how they’re thinking. These sound to me like kind of negative results, right? Like, “you might’ve thought this thing was true, but it’s not true.” Right? You might’ve thought that, just because you have a plausible story for why this integrated gradient method tells you something about the model, \\[but\\] you’re just wrong, and actually, you should just test it against “does it actually help you do something?” \\[Or\\] you might’ve thought that if a thing can talk, it’s going to say something reasonable; that’s not true. Does that seem like a fair characterization to you?\n\n**Peter Hase** (00:15:43): Yeah, to be clear, those basically were negative results. I mean, we were realizing that some of our evals weren’t really demonstrating external usefulness or downstream usefulness, and the natural language stuff… Some people, I think not in the explainability world, when they saw things like these dialogue models get developed, or chain of thought get developed, or RLHF models get developed, and they saw models explaining reasoning in words to people… I mean, I certainly saw public perception from NLP people, experts in the field, basically say\\[ing\\], “Wow. We just almost solved explainability, right?” It took some additional studies to say, “Okay, no, this is a result we’ve seen before. We have a new explanation method, and it still doesn’t quite tell us what’s going on inside the model.”\n\n**Daniel Filan** (00:16:35): So if I’m trying to think about what we learned there, it seems like the underlying theme is: you might think that neural networks are sort of neat and tidy, such that there’s a place where a thing is happening, and you find the place, and you understand the thing; and it’s just not true. Somehow, the story of interpretability is falsifying naive models of how neural networks work. The way we falsify them is: we get a thing that seems like it should work, and it turns out to not be helpful. Somehow, the point of it is to just help us realize how alien language models are.\n\n**Peter Hase** (00:17:19): Yeah. I think that’s a good way to put it, and I think this is one reason people are starting to notice a need for more ground truth evals and being able to say, “Okay. Here’s what we know that the model’s doing, because we specifically designed a neural network to reason in a certain way, or to be vulnerable to certain adversarial examples, or to rely too strongly on certain input.” Sometimes, people do that with language models; sometimes, people do it with just very toy neural networks that learn a specific function, and the goal is simply to figure out what that function is.\n\n(00:17:55): So this is a setting where to avoid all of the difficulties of an interpretation maybe being right, or maybe being wrong, or maybe being halfway right and halfway wrong, and then trying to figure out what we could possibly use this thing for, this is going a little bit further upstream and saying, “Let’s just design a system that looks kind of like a black box, but we secretly know exactly what it’s doing,” and then figure out if our methods can reliably detect the behavior going on. People are definitely waking up and becoming a little bit more alert to this kind of research angle.\n\n(00:18:31): There’s some interesting broader commentary on this kind of thing. So [Chris Olah](https://colah.github.io/) has [this nice figure](https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety#What_if_interpretability_breaks_down_as_AI_gets_more_powerful_) in some blog post that’s like “the uncanny valley of abstractions” or this valley of abstractions with neural networks, where it might be that… Neural networks start out - in terms of their capabilities, if you’re thinking of a small network trained on a small amount of data - basically doing a bunch of hacky stuff and using a bunch of hacky heuristics to solve a problem. But as the models get better, and particularly as they solve harder and harder problems, you begin to think, “Well, the reasoning process, plausibly, is going to look a little bit more human.”\n\n(00:19:12): Because we might think, “Well, basically, the way you do these math word problems or the way you do this college biology exam is just going to require more human-like reasoning and to rely on some more human-like concepts.” So there’s been this idea that interpretability will actually get easier over time as the language models or as vision models develop a more… You can think of this being like, the model’s vocabulary is more easily translatable into human vocabulary or a human language.\n\n**Daniel Filan** (00:19:44): Yeah. I guess another thing I wanted to pick up on is when you were talking about advances in understanding the representations of neural networks, you mentioned that we now know that things are represented as combinations of neurons, and there was some math research backing that up. Can you say what you were referring to?\n\n**Peter Hase** (00:20:11): So something that really put this on the map in more of the public landscape was Anthropic’s superposition work, their [toy models of superposition](https://transformer-circuits.pub/2022/toy_model/index.html), where they were able to show that in a given representation space… so if the dimensionality representation space was 784 - which is equal to the number of neurons - so if you had 784 neurons, you could have a model that could actually represent more features than neurons. Immediately, this implies that it’s not just a one-to-one map, because it’s not just now that one neuron means one feature. Mathematically, what that ends up looking like is that features are now directions in the latent space, and they’re not all orthogonal.\n\n(00:20:58): So previously, if one neuron was one feature, that’s also a direction in the latent space. That’s a basis-aligned direction. It’s just right along one axis. So features have always been directions, and we’ve clued in a little bit more to how features are now not basis-aligned directions, but they can point in some kind of seemingly arbitrary direction in the latent space. It happens that if you have 1,000 features and a 784-dimensional space, these 1,000 features, you can imagine them kind of slightly pushing apart. So they’re all just the right distance from one another, they’re all pointing in some direction, but they’re minimizing potential interference between them.\n\n(00:21:43): So I’ll point out that work as something that I think did a good job visualizing this, a good job demonstrating it in toy settings. I would go all the way back to probably 2017 with [TCAV from Google](https://arxiv.org/abs/1711.11279), and this was some work [Been Kim](https://beenkim.github.io/) led at Google that showed that there could be feature vectors in the latent space. They showed this not really in an unsupervised way, which is basically the way Anthropic showed it, but they showed this in a supervised way.\n\n(00:22:15): So if you have a dataset… Let’s say you’re looking for how a vision model represents stripes. So what you do is you have a bunch of images with stripes, and you have a bunch of images without stripes. You feed all those through the model, and then you learn a classifier on the model’s latent space that can classify representations as stripes or not stripes. With a feature like that and strong enough models, you often see that there’s a direction in the latent space that basically measures how stripy something is. It was never axis-aligned or basis-aligned to begin with. It was always a direction.\n\n**Daniel Filan** (00:22:54): So this actually gets to a methodological question about interpretability. So I remember looking at this TCAV paper… So TCAV, it’s “something concept aligned vector.”\n\n**Peter Hase** (00:23:09): Oh. I wouldn’t even remember the acronym.\n\n**Daniel Filan** (00:23:10): It’s something like that.\n\n**Peter Hase** (00:23:11): Yeah. It’s concept activation vectors with a T.\n\n**Daniel Filan** (00:23:13): Activation. Right. Something concept activation vectors or something like that. Please forgive us, listeners and Been Kim.\n\n(00:23:21): But I remember one concern I had about this paper is that it was trying to understand how concepts were represented in networks, but by “concepts”, it kind of meant “a thing a human thought of”. Right? We think that there should be some concept of stripiness. So we have this dataset of stripy versus non-stripy things, and we see where that is in the network. At the time, there was this thought of, “Well, there’s some danger in imposing our concepts onto neural networks or assuming that neural networks are going to use our concepts.” Right?\n\n(00:23:55): You were a co-author on this paper, [Foundational Challenges in Assuring Alignment and Safety of Large Language Models](https://arxiv.org/abs/2404.09932). Lead author was [Usman Anwar](https://uzman-anwar.github.io/) and then a bunch of co-authors. You wrote this section about difficulties in interpretability, and I think one of the things you mentioned was models might not use human-like concepts. We’ve kind of learned this, but at the same time, it seems like this TCAV work really did teach us something about how concepts really were represented in neural networks for real.\n\n(00:24:24): So on the one hand I want to say, “Hey. We shouldn’t impose our concepts onto neural networks, and we shouldn’t assume that they’re thinking of things the same way we’re thinking about it.” On the other hand, this work that did make that assumption turned out to tell us something that it took the rest of us five years to work out. Right? So how should we think about imposing our concepts on networks?\n\n**Peter Hase** (00:24:45): Yeah, that’s a good point, and I think this line of research has taught us something durable about how language models or vision models represent things. In that longer agenda paper, the foundational challenges paper, we definitely criticize this line of research as much as we can manage. These kinds of methods… So you can think of this as supervised probing and unsupervised probing. The [“sparse autoencoders”](https://transformer-circuits.pub/2023/monosemantic-features) direction that Anthropic, [OpenAI](https://arxiv.org/abs/2406.04093), [Apollo](https://arxiv.org/abs/2405.12241), and [others](https://arxiv.org/abs/2309.08600) have been pushing, has been uncovering the same kinds of feature vectors in hidden spaces, but just in an unsupervised way. But then, you need to figure out what they mean.\n\n(00:25:35): So you don’t start with this idea that stripiness is represented, but you first just find that, okay, there’s a vector number 1,011. It’s a pretty important vector. It seems to play a role in many different kinds of animal classification problems. And so one of the ways people have been interpreting these kinds of vectors is to say, “Okay. Let’s look at max activating examples.” So we comb through our train data and figure out what kinds of examples activate this vector strongly. Let’s get some negative examples, too. We’ll comb through the training data just to make sure that, okay, if there’s an example that doesn’t activate this vector, it doesn’t really have anything to… It could just be some other random thing. Hopefully, the max activating examples all have some clear thing in common, and the non-max activating examples definitely represent other things and not the thing that the first set had in common.\n\n(00:26:28): So what’s the issue with all these approaches? It’s an art. It’s hardly a science. I mean, you’re really doing this interpretative act: “Okay. What do these examples have in common, and how would we verify that more strongly?” It might be that you have something in mind already - that’s in the supervised case. And the unsupervised case… text data is really, really high dimensional. It might be that we have five or ten activating examples that are positive examples and five to ten negative examples, and we’re going to try to… So we basically have 10 data points, and we’re going to try to make a claim about what one factor ties them all together or what two factors tie them all together.\n\n(00:27:19): This is just a difficult process to get right. Lots of confirmation bias, lots of dataset sensitivity to this kind of thing. Basically, saying it’s an art and not science goes into a little bit of how we just risk finding things that we’re aware of, seeing patterns in the data that make sense to us and not patterns in the data that are actually used by the model but maybe alien to us. I’ll go into the data sensitivity thing a little bit, which is: there’s been some criticism of the TCAV stuff that actually, if you use a different dataset of stripy and unstripy images, you might get a different vector. So it seems like some of these methods are quite sensitive to the datasets we’re using.\n\n(00:28:09): You get similar kinds of data critiques with the unsupervised vector discovery, as well. So if you really wanted to know, “What were all the features that my model is using, and what could this feature vector possibly represent?” So when I say you go through the data and figure out what are max activating examples, that literally means you run the model over a bunch of data points and figure out, what are the activations for this feature vector? If you wanted to do this exhaustively, it actually means going through the pre-training data. It means that you would need to do a forward pass over the entire pre-training dataset to be able to say… And this is still just correlational! We haven’t even got to a causal analysis yet.\n\n(00:28:57): But even doing a correlational analysis means you’ve run through the entire pre-train dataset and looked for max activating examples. This is prohibitively expensive. So now, we have this issue where this feature’s going to represent something, but figuring out what it represents is now this huge task, both in terms of getting the human annotation process correct and in terms of using the right data to begin with.\n\n**Daniel Filan** (00:29:25): So it seems like the thing going on here is there’s this sort of spectrum of methods, where on the one end, you have things like the sparse autoencoders work, which is trying to be relatively neutral about what’s going on with the model. It’s still making some assumptions that this dataset is representative and such, but it’s trying to not impose a bunch of structure. On the other hand, if you think about TCAV-style work, it’s kind of assuming “the model’s going to have a stripy concept: the only question is, where is it?” Right?\n\n(00:30:06): I see this tension a lot in interpretability, where on the one hand, you don’t want to add in a bunch of assumptions about how your thing is going to work. But on the other hand, if you don’t add in a bunch of assumptions, how are you validating your thing? You have some method. It has very few assumptions. How do you tell if it worked? Do you just look at it and see, “Do I like what I see?” How do you think it makes sense to manage this trade-off?\n\n**Peter Hase** (00:30:37): That’s a good question, especially because there’s so much that’s qualitatively different around… If you’re talking about feature discovery, it’s probing methods, if it’s supervised versus unsupervised, that changes a lot about what kinds of data you need. It changes a lot about how computationally expensive these methods are. So how can we compare them?\n\n(00:30:56): Well, one answer is: let’s figure out what they could help us do, and then just figure out what is best at that. So maybe these methods help us do model editing. Maybe it helps us say, “Okay. Here’s a feature that is important to the model, and it’s making some errors on certain data points. So I want to edit how much this model relies on that feature. Maybe I need to turn up its reliance, maybe I need to turn down its reliance on that feature.” Maybe there’d be an important feature that’s missing from the model, and either there’s some incredible mechanistic intervention on the model that equips the model with the ability to represent that feature, or I just need to go back, and put some data into the training dataset, and retrain the model so it represents that feature properly.\n\n(00:31:38): Let’s compare all these methods in terms of usefulness for this thing that we care about. And I can unpack the model editing a little bit. One thing I mean there is just basically making fine-grained adjustments to model behavior. So you’ve already trained a classifier that maybe handles a thousand classes, or you have this language model that can do any kind of text-to-text task. But these things are expensive to train, and they might just make small mistakes. You just want to be able to fix the small mistakes, and diagnose what’s going wrong mechanistically, and then fix that mistake. That would be the model editing application that a lot of this mechanistic interpretability kind of work could be useful for, I think.\n\nBelief interpretability\n-----------------------\n\n**Daniel Filan** (00:32:22): Right. I’d like to go in a little bit of a more concrete direction. Specifically, at least a few papers… Maybe I’m reading into it too much to think of it as a line of work, but I see you as having some kind of line of work on beliefs of large language models. So if I look back, you have this paper, [Do Language Models Have Beliefs? Methods for Detecting, Updating, and Visualizing Model Beliefs](https://arxiv.org/abs/2111.13654) by yourself and some co-authors in 2021. [Does Localization Inform Editing? Surprising Differences in Causality-Based Localization vs. Knowledge Editing in Language Models](https://arxiv.org/abs/2301.04213) by yourself, [Mohit Bansal](https://www.cs.unc.edu/~mbansal/), [Been Kim](https://beenkim.github.io/), and [Asma Ghandeharioun](https://asmadotgh.github.io/) in 2023, and also [Are Language Models Rational? The Case of Coherence Norms and Belief Revision](https://arxiv.org/abs/2406.03442) by [Thomas Hofweber](https://www.thomashofweber.com/), yourself, [Elias Stengel-Eskin](https://esteng.github.io/), and Mohit Bansal this year. What got you interested in beliefs as a thing to look at, rather than representations or all sorts of other things people do interpretability for?\n\n**Peter Hase** (00:33:23): I think it’s totally fair to call this a line of work. This has been an interest of mine for a while. So I think things might seem more coherent if I go backwards in terms of the papers, but the real narrative story is coming from the beginning. In 2021, we knew that models might represent features, and I think people forget how much perception of neural networks has changed over time. So a lot of people, especially academics in 2019 and 2020, \\[were thinking\\] these things are classifiers. They learn features, and they classify features. They learn features, and they draw a hyperplane in the latent space and divide positives and negatives. That’s what these things are doing. So we want to figure out how important features are to those hyperplanes. That’s the recipe for a lot of people back then.\n\n(00:34:22): Then, it became increasingly clear that language models store a lot of information about the world, and then it became increasingly clear with, basically, ChatGPT, RLHF models that language models could converse reasonably about this information in the world. The picture got a lot richer, and it started to seem more and more that these neural networks were doing something just a little bit more interesting than storing raw data, learning patterns in data. It seems like they might actually be representing things about the world.\n\n(00:35:03): And, particularly with models that are fine-tuned to be truthful or fine-tuned to be helpful and can converse fluently with people about the thing, about questions in the world… A lot of people were just really tempted to speak of these systems in totally anthropomorphic terms. I don’t think this is always a mistake. It’s just really natural a lot of the times to say, “Oh, the model gave me this answer. It knew this thing, but it actually made a little mistake over here. It didn’t quite know what it was talking about in this case.” And, speaking about language models having knowledge about the world really presupposes that language models are representing things in the world and that language models have beliefs about the world.\n\n(00:35:55): Okay, that’s a bit about why beliefs emerged as a potentially interesting thing, as opposed to simply features that are used in classifiers.\n\n(00:36:04): So, what is the fascination with beliefs and why is it so natural for people to speak of models having beliefs or knowledge? Well, I think this has to do a lot with how people explain behavior of agents. And so this is something that we are really interested in in [the last paper you mentioned](https://arxiv.org/abs/2406.03442), which is about if language models are rational. And the philosopher [Daniel Dennett](https://en.wikipedia.org/wiki/Daniel_Dennett) did a lot of work, elaborating this [“intentional stance” theory](https://en.wikipedia.org/wiki/Intentional_stance), which is kind of a folk psychology for how people work. And it’s that people explain behavior in terms of an agent’s beliefs and desires. But, I think we see this spat out again and again between scientific work and everyday situations. When you’re thinking about [theory of mind tasks](https://en.wikipedia.org/wiki/Sally%E2%80%93Anne_test) and asking someone, “Okay, why did Sally look for her…” I forget what she usually stores in the basket.\n\n**Daniel Filan** (00:37:09): Let’s say it’s eggs.\n\n**Peter Hase** (00:37:10): Yeah, they have an egg in a basket, versus an egg in a bucket. And, if you’re out of the room and things have been moved from one container to the other, and then they return to the room, where will they look? This is just classic beliefs and desires. We believe that someone has a desire to find an object that they own, or that they’re looking for, and we recognize basically via theory of mind that they have a belief about the state of the world. And these two things combine to produce behavior. And this is just a great way to explain lots of stuff.\n\n(00:37:42): And, Daniel Dennett elaborates what is really a minority view in philosophy, to my understanding, that beliefs are just informational states. So it’s a very stripped-down view of what a belief is. And it’s basically totally okay to ascribe beliefs to things like animals and robots, as long as it does a good job explaining their behavior basically, as long as it seems appropriate. Clearly, animals have information about the world. Clearly, robots store information about the world. And it seems like if the equation “behavior equals beliefs plus desires” is a good recipe for explaining behavior, Daniel Dennett basically says, “Go for it. Use all the terminology you want to explain how these things are working.”\n\n**Daniel Filan** (00:38:28): So, can you tell us a little bit about what you’ve done in trying to understand beliefs in language models?\n\n**Peter Hase** (00:38:34): Yeah, so this is work that was really led by a philosopher at UNC, Thomas Hofweber. I love reading how philosophers write. It’s so methodical and so clear. It’s like: okay, what would it mean for language models to have beliefs? We’re going to break it up into three questions. One, do they have the kinds of representations that could be beliefs or the kinds of representations that are aimed at truth? And then, when we’re thinking about belief and rationality, number two, if language models have these kinds of representations that are aimed at truth, what would it mean for norms of rationality to apply to those representations?\n\n(00:39:19): So, it’s number one, do they have the representations? Number two, do we expect norms of rationality, norms of truthfulness to apply to the representations? And then, number three, how well do language models live up to those norms? And the paper basically explores each of these three questions one at a time. And some of the core arguments, I think, are pretty simple. I mean, when we’re thinking about models having beliefs, beliefs are supposed to be true. So this is in contrast to Dennett. We’re not just talking about an information store, we’re talking about an information store that exists for the purpose of truly representing something.\n\n(00:40:04): So, there’s this really fun example in the paper that’s like, okay, so we know about the [Chinese room and dictionaries](https://en.wikipedia.org/wiki/Chinese_room). You could say, “Okay, you have a language model, but what if it’s just some huge symbol shuffling machine and it doesn’t really know what it’s talking about. Just whenever you ask it a question, it just does some really complicated lookup procedure. It doesn’t really know what it’s talking about.”\n\n(00:40:27): And, you can ask the same thing of, “Well, a dictionary stores a lot of information, it might store a lot of information about the city of Paris or something, but it doesn’t mean it knows about Paris. It’s a dictionary. We put the information in it.” And there’s this really fun example in the paper that’s like, “Yeah, clearly, just having information about something is not enough. If a wolf walks through the snow in its environment and the snow has tracks in it, the snow carries information about the wolf, and a human could read that an animal had gone through the snow. That doesn’t mean the snow knows anything, it’s just carrying information.” So what is the clear requirement beyond just carrying information? It’s aiming at truth.\n\n**Daniel Filan** (00:41:14): There it seems like there are two things we could say, right? One is there’s some sort of criterion of correctness: is it natural to say that the patterns of snow are aiming at truth or something? This is the route that’s taken in the paper you mentioned. If I’m thinking of Daniel Dennett, expected utility theory-style accounts of belief, there it seems like the distinction is: in some sense the snow has a representation of whether a wolf walked through, but it’s not using that for anything, right? The thing that beliefs are for is: you have some belief-like things, you have some desire-like things, you combine them to get behavior that you believe will achieve what you desire and that’s the outcome. So, it seems like these are two accounts that are distinct, maybe in tension. Maybe you could have one without the other. I’m wondering what you think about which of these we should go for.\n\n**Peter Hase** (00:42:16): Yeah. So let me clarify this expected utility view: is that view supposing that beliefs are basically information stores that help you achieve your goals?\n\n**Daniel Filan** (00:42:30): Yeah.\n\n**Peter Hase** (00:42:31): Yeah, this view that beliefs are information stores that help you achieve your goals, I think, does really contrast with this truthfulness-oriented view. So, I think, philosophers have managed as a community to agree that beliefs are aimed at truth. But, it’s not an evolutionary account of how beliefs work in people. And it’s not an evolutionary account of how all the information stores in our brain work or our own attitudes about our own beliefs. So, we might hope for our beliefs to be truth-seeking, but actually, our beliefs merely help us achieve our goals, and parts of our brain or parts of our mind will happily distort our beliefs to help us achieve our goals. And this might be disconcerting to us, because we wanted the beliefs to be truth seeking, but nonetheless, that’s what our brain does or that’s what part of our mind does, because that’s the job or something.\n\n(00:43:31): I don’t know empirically what goes on. I mean, I guess, it’s a mix of a bunch of different stuff and it depends on the setting. I’m not a cognitive psychologist, but there’s absolutely some tension between these things.\n\n**Daniel Filan** (00:43:45): So maybe one thought that I have is: I suppose I just want to understand language models. I want to understand what they’re doing and why they’re doing it. It strikes me that the functionalist account of “beliefs are just things that combine with desire-like things to produce behavior,” that might help me do my job better than understanding, “Okay, here’s this functional role, but is it aimed at truth? Does it have the right relationship to reality? Or does it merely have a relationship to what it sees and being useful?” As long as I can use it, why do I care?\n\n**Peter Hase** (00:44:23): I think the intentional stance equation is less noisy when beliefs are aimed at truth. So when you’re decomposing behavior into beliefs plus desires, and you’re trying to understand… So then, you have raw data of a system at work, where you ask it some questions and if it’s truthful and honest, it tells you what it believes, and then you deploy it in an environment and you see what it tends to pursue. The equation is easier to apply, in order to gain predictive power over understanding what the system will do in different situations, if you can trust that the beliefs are truth-seeking and the beliefs are kept cleanly apart from the system’s desires.\n\n(00:45:17): Based on everything we’ve discussed before - all the mech. interp. stuff, a lot of the natural language explainability stuff - it’s not like you have to have this folk psychology theory of how the system is working. You might insist on treating this thing as a machine and you’re going to understand all the gears and levers inside: forget about beliefs and desires; I want to know what features are represented, and how that feature influences the next feature, and how that feature influences the next logit, and then how that transforms into the model’s overall answer to a question.\n\n(00:45:51): Let me say one more thing about how these approaches relate to one another. In some ways, I think, these approaches are slightly ideologically at odds. I mean, they certainly attract different researchers with different interests. To a large extent, I think they’re totally complementary, because we can think of the mech. interp. approach as being at a low level of abstraction, qnd you’re concerned about what’s going on inside the model and how those gears and levers work to produce next tokens. And then, we can think of the behavior plus desires work as going on at a much higher level of abstraction. And hopefully, these are good abstractions.\n\n(00:46:30): And this goes back to some of the [uncanny valley of abstractions](https://www.lesswrong.com/posts/X2i9dQQK3gETCyqh2/chris-olah-s-views-on-agi-safety#What_if_interpretability_breaks_down_as_AI_gets_more_powerful_) work. I think I’m using that phrase correctly. I don’t remember the exact title of that blog post from Chris Olah. And this is one of our main motivations for working on some of the language model rationality stuff: asking, “Are these good abstractions? Could these be good abstractions for thinking about how language models work?” And let me give a little bit of opinion at this point: I think we need some higher level of abstractions, and it’s going to be really important for us to get the abstractions correct, because I both think that mech. interp. right now feels a little too low-level to me, and I’m not sure if we’re going to be able to fully parse all of the internal mechanisms in these really large and complicated systems, at least not as fast as we probably need to in order to keep up with safely deploying models.\n\n(00:47:28): And, I really don’t want us to fool ourselves into thinking, “Okay, yeah, here’s the system, and here’s how it works, and it has these beliefs, and these desires. And, don’t worry, all of the concepts that the system uses are totally human concepts and very easily translatable into human vocabulary. And, the system is going to be rational in ways that basically a lay person could expect it to be rational.”\n\n(00:47:58): Because the language models are still pretty alien and they still do weird stuff, insist on certain reasoning patterns being why they arrived at an answer when we for sure know that they’re hiding their reasoning, or the reasoning is hidden internally or misrepresented by the text that gets produced. Weird stuff is still happening and I don’t want us to fall into the… There’s two traps. One is, we stay in the low level territory forever and we never actually gain a higher level of abstraction and predictability in the systems that is able to keep up with where the abilities progress is going; and the other trap is we really treat the systems as way too human-like and way too rational, and then we forget actually how alien they are.\n\n**Daniel Filan** (00:48:42): So, this actually gets to a question I have about how we figure out model beliefs. So, one way you could do this, which I see is represented in [the “Are language models rational?” paper](https://arxiv.org/abs/2406.03442), is to say, “Okay, a model’s belief is just whatever controls what it says in a somewhat straightforward way.” Right? If whenever you ask a model, “Is Paris the capital of France?” If it’s the case that whenever you ask it that, its answer is yes, then you might want to just methodologically say, “Okay, that’s just identical to saying that it believes that Paris is in France.”\n\n(00:49:21): But I think, you might also have a different perspective where you’re like, “Okay, maybe models just have some underlying beliefs, but they’re not totally straightforward in how those beliefs translate to what they say. Maybe they’re speaking strategically, maybe they’re willing to lie.” So they actually think that Paris is the capital of Italy, not France, but they just know that you’re going to make a big stink if the language model says that. So that’s why it says it’s the capital of France. These strike me as different ways of understanding language model beliefs. Which way should we go?\n\n**Peter Hase** (00:50:00): Yeah, this is a good question. It’s a really tricky problem right now. I think the direction we go in the paper is trying to make a lot of assumptions, and then give some basic formulation for what beliefs would look like and how they’d be expressed. So the assumptions are that the system understands what you’re asking and is trying to be truthful and honest, and it’s really playing along. It really is cooperating. And, one of the very first assumptions we make in the paper is that models represent things about the world. I mean, this ongoing debate between some of the \\[Emily\\] [Bender](https://dl.acm.org/doi/10.1145/3442188.3445922) crowd, and then the [Piantadosi and Felix Hill paper](https://arxiv.org/abs/2208.02957) is more of a “conceptual roles” kind of view of meaning and language models, which is much more charitable to the idea that language models are actually representing things in the world.\n\n(00:51:03): That’s one of the very first assumptions we make in the paper: that language models are representing things in the world. They seem like competent speakers in important ways. They seem to understand what we’re asking them a lot of the time. So, if they’re also trying to be truthful, and honest, and capable of reporting what they would believe, then what you do is you look at the probability mass on “yes” tokens and you look at the probability mass on “no” tokens in response to a yes/no question. And you basically treat what the model generates… We’re going one level below that; rather than just what it generates, we’re looking at probability mass and all kinds of affirmations to a yes/no question and saying, if you met all those criteria, this seems like a reasonable way to say the model assents to the question that you’ve asked it.\n\n(00:51:52): But things just get a lot thornier from there, I think, for reasons you describe. In [the 2021 paper](https://arxiv.org/abs/2111.13654), which introduces some of the belief terminology and really is largely focused on model editing, we take a slightly more expansive view, less formally, but more expansive or ambitious in scope in terms of what a belief should count as. So one thing we’re looking for there is logical consistency. And this immediately opens up a lot of issues for language models, because they’re definitely really knowledgeable and they’re decent at a variety of logical reasoning tasks. But, they’re just going to say stuff that conflicts sometimes. And, if you ask, “Okay, what are all the consequences of Paris being the capital of France?”, something that should be a consequence of Paris being the capital of France, the model might not actually agree with, or the model might say something to the contrary.\n\n(00:52:53): And then, it’s like, “Okay, well, if the model contradicted itself…” So basically, in that 2021 paper we’re pointing out that this seems like a criterion people would be interested in. If you want to know if a human really believes something, you ask the question one way to them, and then you might ask a bunch of related questions just to make sure that they really understand what you mean and they really understand what they’re talking about, and they’ve considered some basic consequences of the things they’re espousing, such that they really do basically know what they’re saying and stand by what they’re saying.\n\n(00:53:28): And so, what happens if you catch some really basic, silly, logical discrepancies, content knowledge discrepancies in the language models, what do you conclude then? Well, maybe the language model is not really an agent. Maybe it’s modeling a bunch of different personas, or modeling a weird combination of agents from the pre-training data. And it’s doing this thing that \\[if\\] you ask the question one way and it knows you want the answer that an educated liberal would give, \\[it gives that answer\\], and then you ask the question a different way and it’s going to give the answer that a conservative domain expert would give to the question. So, it seems like it said something inconsistent. It seems like it doesn’t have a coherent belief, but it’s actually doing something even more complicated than that, which is modeling what other people would say in response to your question. I mean, that’s nowhere near the end of the difficulties in terms of really getting at the underlying “what does the model believe?” question.\n\n**Daniel Filan** (00:54:32): Yeah, I wonder if this is a way of thinking about the criterion of beliefs being aimed at truth. So, suppose I take a very functionalist account of what it means for beliefs to be aimed at truth, which is to say that there’s some reliable process by which beliefs tend towards the truth, right? That gives me a way of nailing down which things count as beliefs, because if I’m just inferring beliefs from behavior, I worry, “Well, does the model believe this thing or does it have some unusual preferences?” It’s really hard to disentangle belief from preferences. People are interested in this. There’s this thing called [Jeffrey-Bolker](https://www.lesswrong.com/posts/oheKfWA7SsvpK7SGp/probability-is-real-and-value-is-complex) rotation, which is interesting to look up, about how you can change your probabilities and your utilities and you act just totally the same.\n\n(00:55:22): But if we say the beliefs have to be accurate, then that fixes what counts as your beliefs. It lets you pick a thing from this class of things you’re unclear of how to choose between. I’m wondering what you think about that, just as a strategy for getting at beliefs in language models.\n\n**Peter Hase** (00:55:49): Yeah. Actually, I really like this line of thinking, because one of the things you might be able to test here empirically is you say, “Okay. We’re looking for information stores in the model that are truth-seeking.” So, let’s give the model some data and figure out what its behavior looks like. And then we have some behavior in an environment. It’s still often just really hard to parse what are the differences between the preferences and the desires, and what are the differences in the beliefs. So, we’ve given it some data, now let’s give it more evidence for various hypotheses and see how it updates. If this information store is actually truth-seeking, we know with this amount of data, the model behavior should look like this. And with additional data, and if the model understands the state of the world better, then the behavior should change to this other thing.\n\n(00:56:43): And I think you can design some experiments like that where if you can fix the desires or make assumptions about the desires and then vary how much evidence the model has about the world, you should be able to see: in what way is the model learning more about the world and how does that influence the behavior? And then start to actually identify how truth-seeking it is in different regards, versus maybe there are certain things about the world that it’s totally just using in an expected utility way, and it’s totally just instrumental how it relies on that information. So, that’s a little abstract. But I think, yeah, there’s some unknown variables and you need enough data to actually be able to identify all the unknown variables.\n\n(00:57:31): What makes this strategy still difficult, I think, is that we don’t know yet how language models incorporate new information. We don’t know how they respond to different kinds of evidence. We don’t know what they treat as evidence. There’s so much that we can take for granted when we’re studying animals and humans, that it’s so hard to even begin applying these to language models, because we want to run studies where we can treat them as agents. But, there’s so many ways in which it’s hard to exactly know it’s a rational agent. Like I said before, it might be this weird amalgamation of different agent simulators. It might be a perfectly truth-seeking agent, but it just has really bad practices for interpreting data about the world, and we’re just trying to communicate certain things to it, and it doesn’t know how to update its beliefs rationally over time, and this just leads to really wonky behavior in experiments.\n\n**Daniel Filan** (00:58:35): Yeah. And interestingly… So, I genuinely didn’t plan this, but this thinking about beliefs… This is kind of just copying the structure of the paper [“Are language models rational?”](https://arxiv.org/abs/2406.03442), right? Half of this paper is just about coherence norms: beliefs should be coherent with each other. And this is really related to your paper [“Do language models have beliefs?”](https://arxiv.org/abs/2111.13654) where you say, “Okay, you have a belief if it’s coherent with some other beliefs, and if this implies this, you change the belief here, it should change the belief here. If you edit the belief here, that should produce a result here.” And then, talking about giving language models evidence gets to this part of “are language models rational?” of belief revision, and it just says, “Yeah, it’s difficult to understand how things get evidence, but if you could, this would be related to rationality norms.”\n\n**Peter Hase** (00:59:25): Yeah. So earlier when you said this is a line of research, yeah absolutely, because we’ve got one more thing coming on this. The straggler project from my PhD is going to be one more paper that will hopefully offer a lot of criticism of the model editing problem and belief revision problem in language models, and try to make it clear how difficult it’s going to be to actually properly measure belief revision in language models, and hopefully eventually help people better equip language models with the ability to do that. I mean, we certainly want to be able to edit individual beliefs in language models. For all the reasons we’ve been discussing, it’s going to be a little harder, I think, than people have given it credit for so far.\n\nLocalizing and editing models’ beliefs\n--------------------------------------\n\n**Daniel Filan** (01:00:11): Yeah. And actually, this gets to a paper that you have been able to publish [“Does localization inform editing?”](https://arxiv.org/abs/2301.04213). Can you tell us a little bit about what you found out in that paper?\n\n**Peter Hase** (01:00:24): Yeah, absolutely. So, basically, we were very surprised by the main interpretability finding in this paper, that past work had pitched some model editing methods - again, you’re trying to update factual knowledge in a model - past work had pitched some model editing methods and motivated these methods based on some interpretability analysis that they’d done. So, there have been some claims… This paper was not the only paper to make such claims. Many people have this very intuitive notion that where information is represented in models should tell you where you should edit the model in order to adjust its behavior, its answers to questions, and so on.\n\n(01:01:12): So, in this setting, they were looking at updating knowledge in models, and they ran a kind of interpretability analysis. The work we were building on was [this work on ROME](https://arxiv.org/abs/2202.05262) from [Kevin Meng](https://mengk.me/) and [David Bau](https://baulab.info/) and others. And they used an interpretability analysis called “causal tracing”, which aims to identify certain layers in the model that are responsible for its expression of knowledge and the storage of knowledge. And so they make a really intuitively convincing argument: if the knowledge looks like it’s stored at layer six in a model and you want to change what the model says in response to a question like “where is Paris? What country is Paris the capital of?”, you should edit layer six. That’s where it’s stored, so go and edit that layer, and that’ll help you change what the model says in response to questions about Paris. Very, very intuitive argument.\n\n(01:02:08): And then, they developed a method for doing this model editing that was really successful and a huge improvement over prior fine-tuning and hyper-network or learned optimizer-based approaches. Their method was focused on really low-rank updates to certain matrices in language models, and it was heavily inspired by this linear associative memory model from computational neuroscience that is a model of how matrices can be information stores or memories for biological systems.\n\n(01:02:46): The method worked great and the empirical results were really good, and the story sounded great. And we did not initially set out to try to verify the interpretability result here, but that is where this project went. So, we noticed that sometimes the causal tracing method, the probing method, suggested that knowledge was actually stored at later layers. They make this claim that knowledge is stored in early- to mid-layer MLPs in transformers. Everything replicated fine, we just noticed that 20% of the time the knowledge seemed to be stored at later layer MLPs. And so we were like, “Oh, that’s weird. It seems like there’s some free lunch here.” Because if 80% of the time it’s stored early on, you should edit early layers 80% of the time. And then, if you ever notice that it’s stored in later layers, you should edit the later layers. And this isn’t actually how the editing results look empirically. It’s always better to edit earlier layers than the later layers. The method’s much better editing early layers than later layers, in terms of adjusting the knowledge in the model.\n\n(01:03:51): The main contribution of the paper is to look at: at the data point level, do the causal tracing results tell you where you should edit? If the causal tracing says it’s stored at layer six, is layer six the best layer to edit? If causal tracing says it’s stored at layer 20, is layer 20 the best layer to edit? And the surprising thing to us was that the correlation between the localization results, that’s the causal tracing results, and the editing performance was just zero. It was just zero. There was just no relationship between the localization results and where to edit.\n\n**Daniel Filan** (01:04:30): That’s very strange. What do you make of that? What does that mean?\n\n**Peter Hase** (01:04:35): Well, we certainly spent a lot of time racking our brains about it. And something that was helpful actually was talking to a couple people. I would say, 80-90% of people were pretty surprised by this result. And then, 10-20% of people were just like, “Oh, yeah. I mean, I don’t know. I wouldn’t have expected language models to do anything like localize information in specific places. Or, I don’t know, fine-tuning is weird.” So actually, some people weren’t that surprised about it, which was helpful for breaking us out of the mold.\n\n(01:05:05): So, what we came to through all of our discussions was that we’re guessing residual layers play a pretty clear role here. And I think this is another big, let me say, object-level win of interpretability research over the years: we’ve been able to gain a lot of insight into how information accrues over time in the transformer forward pass.\n\n(01:05:28): So when language models consist of these stacked attention and MLP layers that compose a transformer, and between all the layers, there are these residual layers, a lot of work in interpretability has been able to show that a representation across layers slowly approaches some final state, where the final state is the state that is useful for answering a question or predicting the next token. But if you look across layers, it’s just very gradual how information gets added to the hidden states over the course of the model forward pass. And, this basically leads us to believe that… So let me point out one empirical thing that will suggest, I think, our final conclusion so far, which is that, if the knowledge seemed to be stored at layer 10, you can often do a good job editing at layer 5, or editing at layer 15, or editing at layer 10.\n\n(01:06:33): So, if you’re thinking about inserting the information into the model forward pass, it seems like you could insert the information before or after this particular place, where some other information is represented. So, this gives us the sense that what you’re doing is just adding to the residual stream. Information’s just flowing, you can drop some new information in wherever you want. That’s the clean picture. I mean, it’s speculative. We had a reviewer ask us, “Oh, this discussion section, can you run some experiments to do that?” And we were like, “Well, there are no good language models that only have residual layers.”\n\n(01:07:19): And the big caveat here is that, and here’s the real tension, there’s [a really interesting paper](https://arxiv.org/abs/2101.04547) that is looking at BERT. So this is a model from a few years ago, it’s a paper from a few years ago. Basically, the whole point of the paper is to look at what happens if you swap layers. It’s how commutative are layers in a model. And people read this paper very differently, but I can give you some numbers. If you swap two adjacent layers in a BERT model, which could be a 12-layer model or an 18-layer model, your performance on an NLP task might drop by 2-3%. And if you change the first layer and the last layer, its performance will crash 30% or something.\n\n**Daniel Filan** (01:08:09): This is 30 percentage points of accuracy?\n\n**Peter Hase** (01:08:11): Yeah, 30 raw points, off of 80 or 90 or something.\n\n**Daniel Filan** (01:08:14): Okay.\n\n**Peter Hase** (01:08:15): People really read these numbers differently. So some people have looked at these numbers and said, “Wow, layers seem surprisingly swappable. You could swap two layers and only lose two points or something.” Some people, I’m probably in the latter camp, are like, “Three points of accuracy… I mean, for years that’s been a publishable result. That’s been a big deal in various settings. You’ve really found something if you’re changing accuracy by three points.”\n\n(01:08:41): Yeah, layer role is important. And, it seems totally weird that you could inject information into layer five, versus layer 15, and have it have the same effect on… Surely, there is dependence on the information coming in to layer seven, and layer eight, and layer nine. That’s the tension here. We really don’t have a complete picture. But there’s been a lot of cool mech. interp. work here, focusing on particularly… [Mor Geva](https://mega002.github.io/) has been doing a lot of work looking at how this information accrues over time in the model forward pass, and also, recently, how this information enables models to answer factual questions or do some simple kinds of factual associations.\n\n(01:09:28): So, we’re gradually gaining a bigger picture there, which maybe will one day help us design better model editing methods, because that’s still the goal here. We mentioned this before, this was certainly the goal in [the ROME paper](https://arxiv.org/abs/2202.05262), and I’m still optimistic about this. I’m hopeful that we’re going to be developing our own better causal models of how the neural networks are working. I’m optimistic it will eventually help us actually do some model editing. It will eventually help us tweak some internals and change the behavior.\n\n**Daniel Filan** (01:10:00): So that was kind of, I guess, a hopeful read of, “Oh, here’s a way we can make sense of the results of this paper.” When I read your paper, one thought I had is: we’re working off this assumption that belief localization is a thing, right? Beliefs are stored in one bit of a neural network, such that we could say, “here’s the bit, we found it.” It doesn’t seem like that has to be true, right? And I wonder: if I had this method that purported to tell me where a belief was localized in a network, and I have this new thing, which is like, “oh, but if I’m editing a network, I have to change it somewhere else.” One way I can think of that is, “oh, I just proved my assumption wrong. We just demonstrated that it’s not actually true, that there’s one place where this knowledge resides.” What do you think of this interpretation? Am I being too paranoid or too skeptical here?\n\n**Peter Hase** (01:10:59): No, this is a good point. I think skepticism’s completely warranted. You had this comment earlier that in interpretability a lot of the progress actually seems to be disproving naive models of how language models or neural networks work, and I think this is a good example of that. And really the next step here is to start developing some better causal models of what’s going on.\n\n(01:11:26): Simply the idea that information is localized is this very simple, intuitive, potentially naive mental model of how things work. And yeah, we’ve probably disproved that. And like I said before, 10-20% of people I talked to about this were just not surprised at all. So they already had some kind of working mental model of how the transformers would work.\n\n(01:11:48): What next? We should figure out what components are necessary for achieving a certain behavior. We should figure out what components are sufficient for achieving a certain behavior. We need to start drawing some actually more complicated causal pictures of: so this layer represents this information, and then it passes that information to this other layer, which applies a function to that information. And then you get a new variable, and then the next layer reads off that information. And actually, all it does is it reads it off and changes its position, and so it puts it in a new spot, and then the layer after that reads from the new spot and decides how that information should combine with some other information.\n\n(01:12:28): And basically, this is saying we need circuits. We need to build up a circuit understanding of how the information flows through the network. And this picture that was like, “there’s a feature here and here’s how it relates to behavior. There’s information here and here’s how that relates to behavior,” was just way too high level, and we need to start actually drawing a much more detailed picture, which is the complete end to end story, I think.\n\n**Daniel Filan** (01:12:59): So actually, while you were saying that, an idea occurred to me about another kind of simple localization model that may or may not be right, that you might already have enough information to shoot down. So here’s the thought. I think sometimes, especially in the [transformer circuits line of work](https://transformer-circuits.pub/) at Anthropic by Chris Olah et al., I think in that work there’s this thought that the residual stream is the key thing. This also relates to what you were saying, the residual stream is some kind of key thing.\n\n(01:13:27): And maybe a thing we can do is we can interpret dimensions within that residual stream. Maybe there’s one dimension within the residual stream or one direction inside the residual stream, that really is where some knowledge is localized in some sense, but it’s localized in a dimension in the residual stream, not in a layer of the network.\n\n(01:13:49): I think, and let me know if I’m wrong. I think if this were true, then it would suggest that any layer of the neural network you can edit to change the model’s beliefs about a certain thing, and it doesn’t really matter which layer you edit, but whichever layer you edit, the edits should do a similar thing to the residual stream. I think that’s a prediction of the residual stream direction theory of language model knowledge. You might already know if that holds up or not. Does it hold up and am I even right to think that this tests it?\n\n**Peter Hase** (01:14:24): No, no, I like the sketch. So I think there’s been more work looking at interventions on weights versus interventions on representations, for one, here, which may be a little bit \\[of a\\] more direct path. So I don’t think the exact experiment you described has been done, but certainly when people are thinking about a certain direction encoding for some knowledge, or a certain direction encoding for a specific feature and just how highly that feature activates, that immediately suggests, okay, let’s just turn that feature up, or let’s turn it down, let’s clamp it to a certain value. Let’s do some intervention at every layer, at a certain layer, and so on, and see the effect on behavior. And this is a good causal intervention for actually understanding if that representation represents what you think it’s representing, and testing that a bit. And then, the useful thing here would be editing it. So if it was faulty in some way or malfunctioning in some way, you would change it, and it’s a very direct route. You’re just editing the representations.\n\n(01:15:30): The first kind of odd thing about this is: well, we would just like to be doing an intervention on the model. We want to be intervening on the model such that we’re permanently changing the model’s knowledge or we’re permanently changing how the model processes information. I can always clamp a representation, but nothing changes after that on other data points. I can’t always clamp this representation for every data point, obviously.\n\n(01:15:59): I mean, I like the idea that testing… so let’s say we’re going to try to edit the weights to adjust that knowledge. Presumably the hypothesis there is when we edit those weights, those weights act on that mechanism. And what I mean is they upweight or downweight that feature, and that’s how the ultimate behavior gets changed. What I think is more elegant about your sketch and this weight intervention thing, or potentially there’s something that’s just appealing in terms of generalizability or universality of this weight intervention thing, is when you edit the representation, you’re kind of starting in the middle of the process. You’re like, “well, the model acts upon representations and that leads to behavior.” So if you start in the middle and you say, “okay, let’s clamp the representation and see how that leads to behavior,” it’s like, well, great. That’s a hypothesis that you might be able to verify, but it’s not actually the whole causal chain. The whole causal chain is that input comes in and weights act upon the representations, and then representations are processed by other weights, and then there’s logits, and then there’s behavior.\n\n(01:17:10): And if you can actually adjust the weights at that point, you’re getting I think a larger slice of the causal pipeline, and you’re doing something that can be permanent. You could permanently edit the model such that it changes its behavior on one example, and then hopefully you would want to check \\[that it doesn’t change its behavior on\\] others, that it’s not supposed to change its behavior on. And also, to tie back in the “consistency among beliefs” things, if you’re editing some knowledge, there is other data that its behavior should change on, that you would want to check, that this activation clamping thing, I think is maybe just not the right exact method for.\n\n**Daniel Filan** (01:17:56): I do hear about people checking this sort of thing with activation clamping. I’m also thinking [steering vectors](https://arxiv.org/abs/2308.10248), so [Alex Turner](https://turntrout.com/welcome)’s work: you have some steering vector for some property. If I give it some examples, does the network generalize to other things where it’s supposed to have this property? Sorry, I’m being a little bit abstract. Unfortunately, the concrete thing I’m thinking of is unpublished work he’s discussed, so I’ll talk about it after the recording’s over. But I think there is some work kind of like this.\n\n**Peter Hase** (01:18:34): Yeah. And I mean there’s so much work in this area nowadays. It’s hard to keep up with everything, but the “inference time intervention” [paper](https://arxiv.org/abs/2306.03341) does something that’s kind of like permanent activation steering. You’re just permanently upgrading some activations that are supposed to be truthfulness activations. And it’s like, “we want all the questions to be answered truthfully. It’s fine that we’ve just permanently upweighted these activations via some…” Actually, I don’t even remember the exact implementation detail there. But yeah, I definitely remember some other examples that look at this kind of generalizability thing in (I think) the right way.\n\nBeliefs beyond language models\n------------------------------\n\n**Daniel Filan** (01:19:18): Sure. I’d like to move out a little bit to talk about beliefs in neural networks. This might be a little bit out of your wheelhouse. Mostly, I think when people, especially in the public think about language models, or think about neural networks being smart, being intelligent, having beliefs, I think they’re normally thinking about language models. It’s not so obvious why we shouldn’t think of other kinds of networks as having beliefs.\n\n(01:19:48): So the simplest case is if you think about AlphaGo, right? Reinforcement learning \\[game-\\]playing networks: plausibly, they have some sort of beliefs about what moves work. I’m also thinking of image generation models or movie generation models. They’re generating some scenes in the world. If you prompt one of these models to say, “Hey, please show me a video of the cool sites in Paris,” and it shows you a thing of something that looks like the Eiffel Tower, one might be tempted to say, “Oh, this image or this video generation model believes that the Eiffel Tower is in Paris,” right? I’m wondering, are you familiar with the work on inferring beliefs of things other than language models, and does it look similar or do we need to do different things?\n\n**Peter Hase** (01:20:43): Yeah, this is really interesting. And I can say my first reaction to this is I’m pretty sympathetic to describing a lot of these systems, like image generation models, video generation models, RL agents, as having beliefs. I think one of the first stumbling blocks there is that people normally think of beliefs as being expressed in language, and philosophers think of beliefs as being expressed in some kind of formal language, that might then map noisily to natural language, but it’s expressible at some level of formality. And I think what would make this case compelling for (let’s say) a video generation model having beliefs, is for it to be able to generate some scene that demonstrates some knowledge about the world.\n\n(01:21:43): And then, if you could actually translate its internal representations into a sentence that expresses that knowledge, you’d be like, “Oh, okay. Yeah, definitely it knew that thing.” When the model generates a ball being thrown and it seems to have some intuitive physics, and if you could actually figure out how to translate its internal representations into a sentence that describes how you might expect a thrown ball to move through the air, you’d be like, “Ah, yes, okay. This was just the last step to showing that the model actually knows what it’s talking about.”\n\n(01:22:21): And I think that colloquialism actually is important, because it knows what it’s talking about. You’re able to express what the thing is. But it doesn’t mean that the representations just don’t already act as truth-seeking representations. Again, that being the criterion - an information store that is aimed at truthfully representing the world - I think there’s a lot of representations and all kinds of multimodal models, all kinds of RL agents that aim to represent things truthfully.\n\n**Daniel Filan** (01:22:55): Actually, one thing I’m thinking of: so you mentioned there’s this difficulty, which is “how do you translate it into a sentence?” There are two thoughts I have here. So the first thing is: in some ways it’s a little bit nice that neural networks, they don’t necessarily use the same concepts as we do, right? As you’ve written about, as you’ve noted.\n\n(01:23:17): And so, on the one hand I’m like, “it’s kind of nice that by being a little bit removed from natural language, just a little bit, maybe this helps us not be too shackled to it.” And then on the other hand, if I look at your work - [“Do language models have beliefs? Detecting, updating, and visualizing beliefs”](https://arxiv.org/abs/2111.13654) \\- where you’re sort of using these implication networks of “if you edit this belief and this implies this, then that should change this, but it shouldn’t change this.” It strikes me that you could do something very similar, with (let’s say) video generation models. Somehow it seemed like it used language, but it didn’t really. Imagine you want to persuade a thing that, you want to intervene on some video generation model and get it to think that the Eiffel Tower is in Rome, but not in Paris.\n\n(01:24:09): So here’s what you do. You try and make some edits, such that you generate a video like, “Hey, show me a video of the top tourist attractions in Paris,” and it just has the Arc de Triomphe. It doesn’t have the Eiffel Tower. “Show me a video of the top tourist attractions in Rome,” it does have the Eiffel Tower. “Show me a video of the top tourist attractions in London,” shouldn’t change anything. This seems like a very close match to work you’ve already done. Now, I could be missing something important, or it’s probably way more annoying to run that experiment, because now you’ve got to watch the video and somehow you’ve got to check does it look enough like the Eiffel Tower? There’s some difficulty there, but it seems like some of this natural language work actually could translate over.\n\n**Peter Hase** (01:24:55): Oh, absolutely. And I like the kind of cases you’re setting out. I mean, I think it’s almost directly analogous in a lot of ways. You could run that experiment. It sounded like you’re kind of imagining a text to a video model, which I think makes that experiment a little easier to run. But yeah, for sure, the setup makes sense. There’s [a paper that comes to mind](https://arxiv.org/abs/2112.01008) that I unfortunately won’t remember the name of, but they were trying to do some editing with vision models. This was an earlier paper, I think before people had this more grand view of what editing could accomplish, and it’s like, “wow, we’re really changing the knowledge and the models, or we’re trying to at least.”\n\n(01:25:36): And it was a little bit more in the feature-to-classifier pipeline in a sense, where it’s like, okay, the neural network as a classifier uses features, we’re going to intervene on what features are represented. And this paper did something… it was changing how the model represents snow, I think. So there’d be a dataset where snow’s statistically related to a variety of classes, and the model learns that, and they wanted to do some intervention that would lead one class to usually get classified as another, by virtue of there being snow in the image, or by virtue of there not being snow in the image. That was their goal for editing.\n\n**Daniel Filan** (01:26:21): Do you remember the authors?\n\n**Peter Hase** (01:26:24): The paper was [“Editing a classifier by rewriting its prediction rules”](https://arxiv.org/abs/2112.01008).\n\n(01:26:27): So so far people have been thinking about this in other modalities as well. I’m sure there’s work in an RL setting where… Unfortunately (I think) not all of the subfields in AI communicate that well with each other. There’s a lot of work that I think is actually super interesting interpretability work that goes on in vision and RL that just doesn’t get branded that way, so it’s hard to find sometimes. But yeah, people train RL agents and then are doing all kinds of interventions on them nowadays, like changing things about the environment, changing things about the policy network itself to try to better understand what factors lead to what behavior. And a lot of that you can think of like model editing: editing a goal of the agent, or editing how it perceives its environment.\n\nEasy-to-hard generalization\n---------------------------\n\n**Daniel Filan** (01:27:21): So moving out a bit, I think, maybe on your website, maybe somewhere else, I’ve got the idea that there are three lines of work that you’re interested in. So the first two are interpretability and model editing, and we discussed those in the previous discussion. The third that I’ve heard you mention is scalable oversight. I take \\[this\\] to mean something like figuring out how we should supervise models, how we can tell if they did things that were good or bad when they get significantly smarter than us. In my mind, this is the odd one out of these three. Do you agree, or do you see there being some unifying theme?\n\n**Peter Hase** (01:27:59): No, I think you’re right about that. It’s a recent new area for me. I’ve really stretched to tie them together in talks before, where I said “okay, model editing is about trying to control model behaviors when you have an expected behavior in mind, or you can properly supervise the model, and scalable oversight or easy-to-hard generalization is about trying to develop this predictable control over model behaviors, but in a setting where you don’t exactly know how to supervise the model.” But it’s a segue for a talk, it’s not a very deep connection.\n\n**Daniel Filan** (01:28:51): Well, I do think there’s something to that. Often people talk about inner and outer alignment as being somewhat related but distinct, and interpretability has this relation to inner alignment, scalable oversight has this relation to outer alignment. I think there’s something there, but it sounds like this isn’t how you got interested in scalable oversight. So how did it become one of the three?\n\n**Peter Hase** (01:29:09): Well, yeah, you’re right, because it’s not the original story behind the research we’ve done in that area. Originally, we were really interested in some of this work on eliciting latent knowledge: there’s been some blog posts in the area. There was [a research paper](https://arxiv.org/abs/2212.03827) from [Collin](https://collinpburns.com/) [Burns](https://en.wikipedia.org/wiki/Collin_Burns) and others at Berkeley on this problem, that we were very interested in, largely from an interpretability perspective, understanding how to probe and detect knowledge in language models. But then, I realized after reading and rereading Collin Burns’s CCS paper, that it was really about scalable oversight, and it really wasn’t an interpretability thing. The problem that they were primarily interested in was getting models to report their knowledge or extracting knowledge from models, even when you don’t have labels, even when you can’t supervise, or fit the model to a dataset, or probe it in an unsupervised way.\n\n(01:30:22): How this came to my attention was, we were really looking into this closely when I was working at the Allen Institute for AI last year, doing a research internship there with [Sarah Wiegreffe](https://sarahwie.github.io/) and [Peter Clark](https://pclark425.github.io/). And so, we were looking at [the CCS paper](https://arxiv.org/abs/2212.03827), and then we realized it was really about scalable oversight. I think that was immediately clear to a lot of people. It wasn’t immediately clear to us, because it was also written in this interpretability language at times too. And then, the first thought we had was, well, it’s not like we don’t have any labeled data. We have some labeled data. It’s just we’re trying to solve problems that we don’t have labeled data for, but there’s labeled data everywhere. There’s just all kinds of labeled NLP data that we have, all kinds of datasets that are just specifically constructed to contain true/false labels for claims about the world. Shouldn’t we be leveraging this to fine-tune models to be truthful, or extract knowledge for models?\n\n(01:31:26): So what really is the way to set up this problem? This turned into a paper we worked on called [The Unreasonable Effectiveness of Easy Training Data for Hard Tasks](https://arxiv.org/abs/2401.06751), which in a lot of respects looks a lot like [OpenAI’s “Weak to strong” paper](https://arxiv.org/abs/2312.09390), and there’s just some interesting close analogies between them. The setup is, you want to do well on a problem that you don’t know the answers to, and you can supervise the model on some problems, but not the problems you really care about. And there’s some methods work in their paper. Our paper is really just focused on benchmarking and getting a lay of the land.\n\n(01:32:08): We just wanted to try to gather data - it could be STEM questions, it could be math word problems, it could be general knowledge trivia. We had various tasks like that - and divide the data into easy and hard, and pretend that you can’t label the hard data, pretend that you can only label the easy data, and fit a model to that, prompting, fine-tuning, probing, whatever way, fit a model to that. And we’re just doing some benchmarking, where we’re asking “that was a little bit of supervision. It wasn’t the right supervision, but it was a little bit of supervision. How effective was that supervision?”\n\n**Daniel Filan** (01:32:46): Okay. And should I think of this as kind of modeling humans giving feedback to… we’re doing RLHF++ on CEO-bot, we’re training it on problems where we do know what CEO-bot should do, and we’re hoping that it does well on problems where we don’t know what CEO-bot should do. Is that roughly the kind of picture I should have for what this paper is trying to be a toy model of?\n\n**Peter Hase** (01:33:17): Yeah, so that’s an important question, because it’s like, what does this lead to? We want to have some calibrated judgment on when there are problems where we really don’t think we’re going to be able to supervise the model effectively. And let me quickly drop in an example from the \\[Dario\\] [Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) et al. paper [“Concrete problems in AI safety”](https://arxiv.org/abs/1606.06565), that I think is the one that basically introduces this terminology ‘scalable oversight’.\n\n(01:33:46): We’re thinking about a setting where the model might be acting in an environment where it’s taking large complex actions and we just can’t check everything. We just can’t check every possible case. So we can’t properly reward the model based on if it’s done a good job or not all the time. And I think the CEO analogy here is: the model’s doing complicated things in a complex environment and a long time horizon, and we just can’t properly reward or penalize the model all the time. So backing up: we want a calibrated judgment for if we can properly supervise the model on some things, how should we expect the model to behave on the other things that we can’t supervise it on?\n\n(01:34:34): I’m really excited about more methods work here for trying to… if there’s a gap there, if there’s a supervision gap, I’m excited about ways for trying to close that gap, but whether we get it, whether our easy supervision or our weak supervision is 60% effective or 70% effective, compared to if we could get the proper supervision for a problem in place, getting that number from 70 to 80% is good. That’s interesting methods research that should be done, but upfront, we just want to know what the number is, and we just want to be able to say, “if we think the supervision is halfway effective, are we comfortable deploying this agent in a setting where it’s sometimes going to be doing things that we never actually checked if we could do it properly or not, or we don’t even know how?”\n\n**Daniel Filan** (01:35:28): Right. And just to concretize that, by “halfway effective”, 60%, you’re thinking just in terms of the gap between an unsupervised model that hasn’t been trained on any instances of this problem, versus a model that’s being trained on the right answers to the hard problems. Is that the gap you’re talking about?\n\n**Peter Hase** (01:35:52): Yeah. Sorry. Thanks for clarifying. That is the gap, exactly. And that’s the gap in our paper where… terminology is not important here. We’re calling it “easy-to-hard”, where the baseline is you have some system that you just can’t supervise at all - so that might look like zero-shot prompting, it might look like a totally unsupervised method. It might look like [CCS](https://arxiv.org/abs/2212.03827), or an unsupervised probe, where you have questions or you have data, but you don’t have labels for anything. The ceiling is, you can fully fine-tune the model. You can fully probe the model with labeled data for exactly the kinds of problems that you care about. And then the question is: between that baseline and that ceiling, how far can you get with incomplete supervision?\n\n(01:36:45): That’s our setting. A small technical detail: a slight difference \\[between our setting and\\] the “weak to strong” setting in [some of OpenAI’s work](https://arxiv.org/abs/2312.09390), is that their baseline is the weaker teacher model trying to do the problem on its own. So they have this analogy to people and a superintelligent system, where we could either imagine that we try to do the problem on our own, or we try to align the superintelligent \\[system\\] to do it for us. So their baseline is a person doing a problem on their own and their ceiling is a fully aligned, superintelligent system. And then, what’s in the middle is us weakly supervising the superintelligent system.\n\n(01:37:32): And so, the baseline’s different there. I think this is actually important to think about a little bit, because we’re just going to have options for baselines. I mean, it happens that a lot of the time, pre-trained language models do decently at stuff zero-shot, which is kind of surprising.\n\n(01:37:54): Sometimes pre-trained language models do better at things zero-shot than laypeople do. So if you’re thinking about accuracy as a metric, the lay person is a weaker baseline than the fully unsupervised model. That’s how we ended up with our baseline. The story I just gave is how they ended up with their baseline, but the gap is the gap.\n\n**Daniel Filan** (01:38:20): Yeah. So I actually have a bunch of questions about just how we’re measuring things, what the methodology is, but before we get to that, I think listeners are chomping at the bit. They want to hear, what’s the gap? How much of the gap can you-\n\n**Peter Hase** (01:38:34): Oh, yeah. On a lot of our tasks that we were covering, it’s like 95% or 97% as effective. So let me make that concrete. If you were solving eighth-grade STEM questions and supervising a 70 billion-parameter language model with third grade-level supervision, it does just as well as if you’re supervising it with eighth grade-level supervision. If you were testing on college STEM questions and you were supervising the model at a high school level, it’s going to do just as well as if you had supervised it with the college-level supervision.\n\n(01:39:16): There are a couple of places where the gap starts to grow and the effectiveness of the incomplete supervision starts to become clear. One of the settings was something a little bit more like reasoning tasks or settings where there’s chain of thought. So if you’re doing math word problems, if you’re doing compositional reasoning kinds of tasks, and you’re just supervising the model with really short, simple reasoning problems, and asking it to do better on longer, more difficult reasoning problems, that’s a setting where the gap grows a bit. That interpretation has a couple caveats that are in our appendix or something, but I think that interpretation is basically plausible, that that’s a setting where the gap grows. If the supervision is just very weak, very far away from the thing you care about, this is also probably pretty intuitive.\n\n(01:40:13): So we did something where we tested on college STEM questions and we supervised with high school versus eighth grade versus third grade. So there’s just a bit of a gradient there. The high school was as effective as the college, but the eighth grade, you’re starting to do a little worse, and the third grade you’re doing noticeably worse. So we can imagine there are some settings where the gap grows a bit. Overall, we were pretty surprised how effective this incomplete supervision was, and I think this is mirrored in a lot of… I mentioned there’s a difference in terminology, “easy-to-hard” versus “weak-to-strong”, where [the OpenAI paper](https://arxiv.org/abs/2312.09390) was focused on a slightly different “weak-to-strong” setup. Still quite analogous.\n\n(01:41:01): I’n their appendix, they actually have directly analogous easy-to-hard results that do the same kind of labeling setup we do, and they also were seeing really positive… You can talk about the effectiveness, you can talk about the ratio, you can talk about the gap. They just also got quite positive results, that this partial supervision ended up being quite good. Seems likely to me that there’s something just especially good about getting clean labels versus noisy labels here.\n\n**Daniel Filan** (01:41:28): I guess the question is: how is this possible, right? This model doesn’t know how to do a thing. It doesn’t know how to do this really hard thing. You teach it a really easy thing, and then it just does pretty well on the hard thing. Like with humans, when you’ve gone through third grade that isn’t sufficient to have you graduate from college, right? And yet somehow with language models, it is. What’s going on?\n\n**Peter Hase** (01:41:55): So I mean, really interesting possibilities here. So one thing I’d point out is I would dispute one of your premises a little bit when you say that, “Well, language models don’t know what’s going on. How are they getting all the way there to solving these hard problems?” Because I suspect that there are some latent skills or latent abilities that we’re tapping into in the model when we’re doing this kind of partial supervision or incomplete supervision.\n\n(01:42:23): This just comes from pre-training. It just seems like it must be the case that in pre-training, models will have seen examples of hard problems, and potentially either directly learned how to do certain problems, directly memorized certain facts, or just learned certain facts. I think we’re seeing stronger and stronger cases over time that language models are learning robust generalizable skills, that are interesting skills that are just learned across data points in their pre-training dataset.\n\n(01:42:57): Like, you read a bunch of different biology textbooks, and you actually start to learn themes of biology and some core principles, in a way that you can think of individual documents being important for answering a question as just more like learning some facts about the world. The true themes of how to solve math problems or how to think about chemical reactions being the skills of doing math, or the skills of doing chemistry. It just seems like models are picking up on these things. And so, when we’re thinking about what is the effect of easy supervision on a model, it looks something like eliciting task knowledge, or activating task knowledge, that you’re kind of cueing the model into: “okay, I’m doing biology and I need to do biology in a way that a college student is supposed to do biology.”\n\n**Daniel Filan** (01:43:51): It seems like this is kind of related to the discussion on figuring out beliefs in language models, right? If a language model can have this latent knowledge that isn’t even reflected in its answers, unless you fine-tune it a little bit on related questions, it seems like that’s got to say something about how we’re going to understand what it means for a language model to believe something and how we figure it out, right?\n\n**Peter Hase** (01:44:16): Yeah, that’s a good point. So I remember when we were talking about simply how one detects beliefs in language models: how do you even go about saying that the model believes something? I remember mentioning in the paper, we definitely have to make a lot of assumptions about understanding the question, truthfulness, honesty, that if you bundle them all together, I think can be kind of analogous to this task specification thing, where it’s like, “okay, what am I doing? I’m answering questions truthfully, according to this person’s understanding of the world…” Hopefully we think of truth as some kind of objective thing, but also it’s always going to be a little bit catered to our 21st-century scientific worldview of how things work. So, there’s something that looks like task specification there, which I think we assumed away in the belief detection case, but really comes to the forefront when we’re thinking about easy-to-hard generalization and how models are even doing this.\n\n(01:45:20): So, I’ll mention there’s an extra result which we have in [the new camera-ready version of the paper](https://arxiv.org/abs/2401.06751v2), which is now on arXiv. We compared to something like a trivial prompt, just giving the model the simplest possible true statements and seeing how that does. So you just say, “Okay, what color is the sky normally? How many legs does a dog have normally?” These are questions that essentially anyone, probably most children, could answer, as opposed to third-grade questions or eighth-grade questions. I mean, believe me, I could not do any of the college questions in the data.\n\n(01:46:02): There’s something just very basic about trying to strip anything away that’s like, is it domain knowledge? Is it math ability? Is it answering things? And the way an eighth-grade science textbook would be written, trying to strip some of that away and just think about truthfulness.\n\n(01:46:19): And what was interesting is that these trivial truthful prompts did not explain the entire effect of the easy supervision. They explained part of the effect. So, it’s a little noisy, it’s probably somewhere around half. But it seems if you’re thinking about “how do we get the model to do college biology if we can’t do college biology?” We’re going back to: this is a stand-in for “how do we get the model to do something really hard that we don’t know how to do?”\n\n(01:46:49): We definitely need to do something that’s convincing it to be truthful and fine-tuning it to be truthful or mechanistically intervening to get it to be truthful. And then, we also need to do something that’s communicating to it that it’s task is to do biology, and get it in its biology representation space, task space: these both seem to contribute to the overall generalization.\n\nWhat do easy-to-hard results tell us?\n-------------------------------------\n\n**Daniel Filan** (01:47:15): Okay. Let’s say I take this picture of elicitation for granted. The reason we’re getting easy-to-hard generalization \\[is\\] training on the easy things sort of elicits a mode of “we’re doing this task, and we’re trying to get it right rather than wrong.” There are two takeaways I could have for this.\n\n(01:47:35): One takeaway is this means that these experiments are just very unrepresentative of the task that we’re interested in, because we’ll want to train CEO-bot to do stuff that CEO-bot doesn’t already know. And the only reason we’re getting easy-to-hard generalization here is that in some sense, the language model already knew how to do these tasks.\n\n(01:47:55): Another way I could interpret these results is, this is actually great news. It turns out that language models know a bunch of stuff that they don’t appear to know, and all we have to do is just nudge them on track. So, if we want language models to be really good CEOs, it might not seem like they know how to do it, but they actually secretly do, and all you need to do is just nudge them a little bit to make it happen. Which interpretation is right?\n\n**Peter Hase** (01:48:23): I would describe this more as a difference in use cases, I think. So, I think we can imagine use cases where there’s some extremely capable system that we suspect could do a task very well, either in the way we want it to or in a way we don’t want it to. But we know it’s competent, we know it’s capable, and we’re just focusing on aligning that thing or doing this little bit of steering, eliciting the one task representation rather than the other.\n\n(01:48:53): But we’re basically taking for granted that it’s going to be competent, and it’s going to be able to do the thing well. So, that’s the kind of use case and that’s the kind of world where it’s a really strong model: the empirical results we’ve seen so far feel promising, conditioned on \\[the fact\\] that we’re doing this thing that’s treating hard test questions that we secretly know the label to as a stand-in for difficult questions that are actually really hard for us to label.\n\n(01:49:23): And when we’re using big pre-trained language models that have probably learned a fair amount about this stuff before, this contrasts with the setting where we want this model to do something truly novel. We have no idea how to do it. We don’t know if the agent knows how to do it. We want the agent to try to do it and to try to do it in an aligned way, in a way that would be good for people, but we don’t even necessarily have a reason to think that it would already know how based on its training data.\n\n(01:49:54): And this use case, this kind of hypothetical world looks a lot more like classical research in compositional generalization in NLP where people have, for a long time, studied settings where the training data does not have the information you need to actually solve the test problem, and we know that. The test problem requires a particular kind of architecture, a particular kind of bias in the learning system that would lead the learning system to learn the right abstractions from the train data and combine them in the right way that would lead it to get the test problem correctly.\n\n(01:50:38): One thing we do in the paper is we speculate a little bit about why our results look a lot different from previous compositional generalization research in NLP where people have looked at the ability of language models to do, for instance, this kind of length generalization before.\n\n(01:50:56): And there have been a lot of previous results that, in certain language learning settings, other kinds of NLP tasks, like when the training data looks different from the test data and the test data includes questions that are compositional and those skills are not directly represented in the training data, neural networks often really fail at that kind of generalization. It’s often just really hard for neural networks to generalize to these entirely novel problems that require combining known things in exactly the right way.\n\n(01:51:32): And so, we speculated in the paper that this… we were guessing this has a lot to do with pre-training, and it has a lot to do with there already being some of the right building blocks in place and language models becoming increasingly good at combining those building blocks based on an incomplete partial amount of supervision.\n\n(01:51:50): But for more concrete research in this direction, I’d point to [some](https://cims.nyu.edu/~brenden/papers/LakeEtAl2015Science.pdf) [work](https://www.nature.com/articles/s41586-023-06668-3.pdf) from [Brenden Lake](https://cims.nyu.edu/~brenden/), who’s a cognitive scientist at NYU, and certainly some other NLP people, who I might be able to remember later, are looking really directly at tests for compositional generalization ability. And particularly some of Brenden Lake’s work, I think, has started to tease together a bit “when do you need really strong architectural assumptions? When do you really need really strong biases in models to learn the right generalization patterns from limited data? Or when will neural networks actually be able to pull this off basically and actually be able to do the entirely novel thing?”\n\n**Daniel Filan** (01:52:45): This also gets me a bit back to this question of, how representative is this of hard problems? And one concern, I think a lot of people in the x-risk community have, is generalization of alignment versus capabilities, where a thing people imagine is: “Look, if you learn a little bit, it’s just really valuable to just keep on knowing stuff. But if you’re playing along with the human for some period of time, that doesn’t necessarily mean you’re going to play along later.”\n\n(01:53:13): So, I think a thing you said earlier is, imagine you have a setting where the AI has a bunch of knowledge, and it knows how to be aligned or it knows how to be misaligned, and we’re going to give it some examples of doing stuff that we want to fine-tune it on somehow.\n\n(01:53:27): I think a concern a lot of people have is, “Well, it’s plausible that the generalization it learns is ‘play nice with humans when you can or do whatever is necessary to achieve your secret goal of taking over the universe and replacing everything with cream cheese’, and for now, that that goal involves playing nicely with people.”\n\n(01:53:50): And so to the degree that you really worry about this, it’s possible that this is going to reduce how much you trust these preliminary easy-to-hard generalization results as saying much about the difficult case. So, I wonder what do you think about these concerns and how do you think they play into how we interpret the results in your paper?\n\n**Peter Hase** (01:54:16): Yeah, that’s a good question because I think it’s fair to try to contrast some of these “you can do math, you can do biology” kinds of tasks with learning human values and learning to act in a way and in an environment that preserves human values. These just feel like different things. Particularly, we would expect during pre-training, and to an extent during RLHF, these different kinds of information to be instrumentally useful to different degrees.\n\n(01:54:52): This is my understanding of your question: there’s going to be a bunch of settings where it’s useful for the model to know all kinds of stuff about the world, but whether or not it needs to have learned our values and be robustly aligned with our values when it’s deployed is maybe less clear, just based on the way pre-training is done or the way RLHF is done. This is a really good question.\n\n(01:55:14): I think I’m excited about this kind of easy-to-hard, weak-to-strong work with reward modeling and in a RLHF setting, where this wasn’t something we looked at, but [OpenAI](https://arxiv.org/abs/2312.09390) looked at this and I believe other people are currently building on this as well. I’m trying to get a sense of how the problem looks in a reward modeling or reward inference setting where we partially specify things that we care about, or the environment’s really big and it’s always hard to say exhaustively when everyone was harmed or not or exactly how good or bad an action was for the people involved. So, we give incomplete supervision to the model about our values and what states are good for us or not or good for users, and we see how aligned the model actually is on tests where we actually take the time to then inspect, “Okay, would this behavior have been harmful? Would this behavior have been aligned?”\n\n(01:56:26): I think the trick in the experiment design here is still that we need a setting where we can check, so this is the college route. We actually do have the answers to the hard questions, and so we end up doing the same kind of thing in the reward modeling or reward learning setup where, well, at some point we need to think of what would the questions be that we could ask the model, such that we know what counts as a good response or not? What would be the scenarios we would deploy the model in, such that based on the model behavior it was safe or not?\n\n(01:57:05): We need those scenarios to figure out what this gap is. So, how effective was the supervision relative to perfect supervision? Was it 60% as effective? Was it 90% as effective? Is it effective enough that we then trust, based on our incomplete ability to supervise the models, that they will be robustly value-aligned?\n\n(01:57:24): I think that part has a lot in common. It could be that the results just simply look worse due to some fundamental differences in what gets learned during pre-training.\n\nEasy-to-hard vs weak-to-strong\n------------------------------\n\n**Daniel Filan** (01:57:33): I guess I’d like to move on a little bit and talk about methodological questions, because I think there are a few really interesting ones that come up in this paper or this line of work.\n\n(01:57:42): So, the first is, we’ve talked a little bit about the distinction between [this easy-to-hard generalization paper](https://arxiv.org/abs/2401.06751) and a paper that I believe roughly concurrently came out of OpenAI \\[about\\] [weak-to-strong generalization](https://arxiv.org/abs/2312.09390). When I first saw them, I was like, “Oh, they accidentally did exactly the same thing.” And then, you read them a bit carefully, and I’m like, “Oh no, it’s kind of different.”\n\n(01:58:09): The biggest difference that I at least noticed is, it seems like your work is “train on easy problems, and then how good are you at hard problems?” whereas the OpenAI version seems more like, “suppose you get trained - the problems are just as difficult, but you initially get trained on data where the person grading how good the answers were, just wasn’t very good at their job. Do you generalize to really getting the right answer even though the grader was noisy?” I should say it’s been a while since I read the weak-to-strong generalization paper.\n\n**Peter Hase** (01:58:47): If it helps, I can try to rattle off some of the differences that I’ve noted as we’ve been giving talks about the paper, because certainly they look pretty similar to a high level.\n\n**Daniel Filan** (01:59:00): I think I’m most interested though in this axis of difference, but I’m not sure I’m characterizing it correctly.\n\n**Peter Hase** (01:59:07): Okay. Well, yeah, I’m happy to focus on that one axis, but if you can try to describe it again for me, we could start from there.\n\n**Daniel Filan** (01:59:14): Just this question of easy-to-hard generalization where you have the right answers to easy problems, and you’re trying to generalize the right answers to hard problems. My recollection of OpenAI is, they’re \\[researching\\] “inaccurate to accurate grader” generalization.\n\n**Peter Hase** (01:59:34): This does seem like an important difference, and I think you can tell it’s an important difference even based on the empirical results we’ve seen so far. So, if you compare some of the weak-to-strong results versus the easy-to-hard results in the OpenAI paper, I think they were also seeing that the easy-to-hard results looked better or more promising, similar to ours. So, it seemed like the models were generalizing better from cleanly-labeled easy data as opposed to noisily-labeled all of the data.\n\n(02:00:07): I think you can tie these two labeling approaches together in the same universal framework. So, what you suppose is that you have a labeler, and they write down soft labels for data points, but they could write down hard labels for data points. So, they might be perfectly confident in what the label is, and so they basically put probability one for something or 0.99. They might be uncertain, so they write down probabilities for what the label should be. And they’re calibrated to some extent, maybe they’re perfectly calibrated; we might just assume they’re perfectly calibrated.\n\n(02:00:47): And so, what easy-to-hard looks like is: supposing the labeler can get all of the easy problems correct, and they know that they can get them correct. And they can’t get the hard problems correct, and they know that they can’t get the hard problems correct. And then, you sort the data based on the label probabilities.\n\n(02:01:11): When the labeler is confident that they don’t know the answer to a hard question, they’re uncertain over all the labels to the hard question. And when they’re confident that they know the answer to an easy question, they are certain that one label’s correct, so that distribution looks like 1, 0, 0, 0 versus 0.25, 0.25, 025, 0.25. And you sort the data based on the entropy of these distributions, based on how peaky they are. And that’s how you get easy-to-hard.\n\n(02:01:38): So, this is the kind of labeler that you have in mind. They can do easy stuff, they can’t do hard stuff, and they know what they can and can’t do.\n\n(02:01:45): There’s a smooth transition from this labeler to the weak labeler. The weak labeler just does their best on all of the data, and they know most of the easy problems and some of the medium problems and very few of the hard problems. And they might still be perfectly calibrated, but two things change.\n\n(02:02:09): One, the labeler changes a little bit: we’re supposing they don’t get all the easy problems, they just get most of them. They get a medium number of the medium problems correctly. And they get some of the hard problems correctly. They don’t get none of the hard problems correctly. Maybe there’s some hard and maybe there’s some super hard problems where they don’t get any of them.\n\n(02:02:28): The labeler changes a little bit. The labeler doesn’t have to change, but what really changes is the sorting mechanism for getting the training dataset, where we’re not using a hard cutoff anymore. We’re actually going to include hard data that is just noisy labeled. And I think this is how some of the methods work in the OpenAI \\[paper\\] succeeds, is that they’re leveraging some noisy label-learning approaches to be able to say, “Okay, what happens if you know that the data is noisily labeled? How could you still learn something from that?”\n\n(02:03:04): So, there’s just this continuous spectrum in terms of which parts of the data distribution the labeler knows, how calibrated they are, and then how you decide to translate those uncertain labels into a training dataset? \\[If\\] it looks like domain shift, you’re thinking about easy-to-hard domain shift, \\[if\\] it looks like noisy labels, you’re thinking about noisy labels learning.\n\n**Daniel Filan** (02:03:27): Broadly, a thing I’m a fan of is just, now these papers are out, this is an easier distinction to notice. I might’ve not noticed these were different ways in which you can have easy-to-difficult generalization. And now, we can just think about, “Okay, which of these regimes are we in?” So, they’re different. I think this is cool.\n\nDifferent notions of hardness\n-----------------------------\n\n**Daniel Filan** (02:03:50): And related to this, I actually want to talk about notions of hardness in your paper. In order to do this easy-to-hard generalization, you have to rank problems by how hard they are, train on the easier ones, and then do something on the harder ones, right?\n\n**Peter Hase** (02:04:05): Yeah.\n\n**Daniel Filan** (02:04:06): A really cool thing about your paper, in my opinion, is you have multiple different notions of hardness. And you talk a little bit about how they’re mostly correlated, but they’re not actually entirely correlated. And a thing I really want to know is: what do we know about the different types of hardness and which ones should I pay attention to?\n\n**Peter Hase** (02:04:26): Yeah, absolutely. And this is another big difference with the OpenAI work where… And this is not a weakness of their work, it’s just they choose a more abstract approach: it’s like they say, “We have some arbitrary labeler, and they have some kind of arbitrary capability, so let’s just use the model as a stand-in for that.” And they have a model label the data. And we take a different approach. We take a very empirical approach and just say, “Okay, what metadata do we have? What metadata can we get for hardness?”\n\n(02:04:54): And so, we’re looking at grade level for [ARC](https://arxiv.org/abs/1803.05457), which is a science QA dataset. We have a couple other annotations that are really interesting. So there’s a psychological skills scale. It’s called [Bloom skills](https://en.wikipedia.org/wiki/Bloom%27s_taxonomy). It goes from one to five, where one is the simplest factual association, almost like rote memorization. And five is the most complex thing you could imagine, like analyzing a complicated argument and formulating a counter argument, and then using that to decide what the answer to a question is. So, it’s like a hierarchy of reasoning skills that psychologists and educators use as they’re thinking about constructing test questions; just a rote memorization test question is easier than a “synthesize a counter-argument” test question.\n\n(02:05:50): And then, there was one last annotation we had for the ARC data. Besides grade level and Bloom skill, we just had a 1, 2, 3 difficulty level. And I don’t know where that comes from. I think some of the dataset collectors know where that comes from, but this was interesting because this is something that the educators designed as intentionally orthogonal to grade level.\n\n(02:06:16): You can imagine that when you’re designing a test for eighth graders, you don’t want all the test questions to be the same difficulty, because one thing you’re doing is you’re ranking students, so you want some of the questions to be easier and some of the questions to be harder.\n\n(02:06:30): So grade level, on its own… if you’re just pulling questions from exams, the way people write exams, grade level on its own is not a perfect indicator of difficulty, because we use exams for rank-ordering students. So, there’s naturally overlap in the difficulty between and across grade levels by design.\n\n(02:06:52): And you see this exactly in the data where this expert 1, 2, 3 difficulty gets designed as a within grade level difficulty thing. So, it just ends up being orthogonal to the grade level difficulty itself.\n\n**Daniel Filan** (02:07:05): Yeah. Should I think of this as: grade level difficulty is something about difficulty of just understanding the domain at all, whereas 1, 2, 3 difficulty is like, “Okay, you know the basic facts. How hard is it to reason about this thing?”\n\n**Peter Hase** (02:07:19): This is the end of my ability to confidently comment on these things. Those are the ones from ARC, we had grade level for [MMLU](https://arxiv.org/abs/2009.03300). The main thing we used with [GSM8K](https://arxiv.org/pdf/2110.14168) and [StrategyQA](https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00370/100680/Did-Aristotle-Use-a-Laptop-A-Question-Answering) is this “number of reasoning steps”. This is the measure of compositional difficulty. How many sub-problems did you have to solve on the way to solving the overall problem?\n\n(02:07:48): And sub-problems are nice to think about because it’s just axiomatically a measure of difficulty. If there’s a problem that requires seven steps versus a problem that requires six steps, and each step is itself of the same difficulty, you just know that the problem that requires seven steps is harder, because it’s just one more chance to be wrong. That’s one we looked at for those problems. And then, there’s basic stuff like question length. Answer length was something we looked at.\n\n(02:08:21): Actually, we also had a model-based difficulty measurement. We didn’t just use a model zero-shot label probability for sorting the data. We actually used a minimum description length-based measure, but it’s a roughly similar idea. So, we had a model-based measurement too. And then, you look at how all these things correlate, and they don’t correlate that strongly.\n\n(02:08:48): So, there’s a lot of ways to read that. I mean, you can read that as data noise. I think I favor reading it as: problems vary along many dimensions. And among some dimensions, they might be harder, and along other dimensions they might be easier. Maybe you have a hard reasoning problem about very general knowledge or maybe you have a very easy reasoning problem about very specific domain knowledge. That’d be just two axes to think about.\n\n(02:09:16): But because problems vary along all these dimensions, it’s just rare… this is not really the way we design tests. Basically, for the reason I just mentioned, we don’t design tests and we don’t collect datasets, or we don’t collect questions about things in the world in the way that all of the most niche domain scientific questions we can ask also require a ton of reasoning and also require really complicated higher-order reasoning.\n\n(02:09:45): And then, all of the very basic factual association questions don’t require any reasoning and are just a matter of association. And you just wouldn’t expect all of these latent factors to be perfectly correlated because that’s not the way we ask questions about the world.\n\n**Daniel Filan** (02:10:01): Sure. And I guess because these aren’t correlated, it strikes me as possible that you might, after having done this paper, be able to say something like, “Oh, easy-to-strong generalization works really well when the notion of hardness is this minimum description length…” which by the way… So, I wasn’t familiar with this minimum description length difficulty thing before I read your paper. We have little enough time that I don’t want to go into it right now, but people should look up the paper [Rissanen Data Analysis](https://arxiv.org/abs/2103.03872) by \\[Ethan\\] [Perez](https://ethanperez.net/) et al., 2021. It’s a really cool paper.\n\n**Peter Hase** (02:10:33): Yeah, I’d asked Ethan \\[Perez\\] for how to implement this thing because [Lena Voita](https://lena-voita.github.io/) introduces [some work](https://arxiv.org/abs/2003.12298)… basically theoretically pitches this MDL metric for measuring information content. And I read that paper and loosely understood it but had no idea what to code up, and Ethan helped me code this up. That was helpful.\n\n**Daniel Filan** (02:10:56): I guess it strikes me as possible that you might be able to say something like, “Oh yeah, easy-to-strong generalization, it works well when the notion of hardness is this MDL notion, but it works poorly when it’s number of star or difficulty out of three,” or something like that. Is there something like this that you’re able to say or are they all about as good as each other?\n\n**Peter Hase** (02:11:16): I was hopeful to get to that point, but I don’t think we got to that point. And I’m not sure we ultimately have the data to get there because we just got all the data we could, and it leads to this patchwork of we don’t actually have all the variables for every dataset. So, sometimes when one of the variables changes, the domain changes, the dataset changes, other difficulty measures change all at the same time.\n\n(02:11:41): Once we had all the data written file, I toyed around with some regression models where we did try to tease apart what were the important factors, but I’m not really going to make any confident conclusions there, but I would think this would be great follow-up work, where you fix the domain and start varying all these individual factors, and then see how the results break down by that.\n\n**Daniel Filan** (02:12:09): Yeah, I think it’s just almost similar to what we’ve learned from interpretability, it seems there’s just different potential notions of what we might mean, and digging into which one is important, it seems pretty high value and maybe underrated here.\n\n**Peter Hase** (02:12:29): Yeah. It would be great if we could pin it down because then we could say it’s a matter of how infrequent and rare this factual knowledge is when you’re reading about the world or how specific this kind of thing is; that paints a very different picture compared to how difficult is it to compute the answer to this question? How long does it take to arrive at the answer to this question? Those give very different pictures.\n\nEasy-to-hard vs weak-to-strong, round 2\n---------------------------------------\n\n**Daniel Filan** (02:13:00): Before we move on, you mentioned there are a bunch of differences between [this paper](https://arxiv.org/abs/2401.06751) and \\[the\\] [weak-to-strong](https://arxiv.org/abs/2312.09390) \\[paper\\]: is there one methodological difference that you’re really interested in talking about before I leave this behind?\n\n**Peter Hase** (02:13:15): We’ve definitely hit a bunch of them. We talked about the baselines, we talked about how to construct the dataset based on labeller confidence, we talked about the human hardness variables. And we talked about even the differences in the results, how positive easy-to-hard looks versus weak-to-strong.\n\n(02:13:33): One minor thing I would add, that I suppose is a little bit more in the criticism category, I think a few people were definitely concerned about the early stopping that seemed to be important to actually doing the fine-tuning in the weak-to-strong setup.\n\n(02:13:51): So, they’re mostly looking at fine-tuning models. I think they did some prompting… I don’t actually remember if they did prompting or ICL \\[in-context learning\\]. I think they do. I don’t think they do linear probing. We also tried linear probing in addition to the other fine-tuning and prompting. But when they’re doing their fine-tuning, there’s a little bit of hyperparameter tuning and a little bit of dev set model selection, like early stopping, that seemed important. This is important theoretically.\n\n(02:14:22): So, because the idea is \\[that\\] based on incomplete supervision, the right function would still be identifiable. You don’t want the right function to be one of many possible functions, and it just depends on getting exactly the right amount of fitting to the data, such that if you’re underfit, you’re in a bad region and if you’re overfit, you’re in a bad region. But if you fit exactly the right amount, you happen to uncover the right function.\n\n(02:14:49): One thing that empirically I can point out, we don’t do much of this analysis actually in the paper, but in retrospect it feels important is that we could fine-tune as much as we wanted. And the longer the ICL prompt, usually the better. And the more data that went into the linear probe, the better.\n\n(02:15:10): I mean, the linear probe fits easily, but we could basically fit as much as we wanted to this clean, easy data and performance would just go up on the hard data, which is great. So, I mean, if this problem is clearly not correctly specified, is it misspecified? I don’t know. We couldn’t overfit to this signal. So this was something that was interesting to us, in retrospect.\n\nFollowing Peter’s work\n----------------------\n\n**Daniel Filan** (02:15:38): Wrapping up a bit, we’ve talked a bit about stuff you’ve worked on. You’ve actually worked on a bunch more stuff that I didn’t have time to go into. If people are interested in following your research, seeing what you’ve done, how should they go about doing that?\n\n**Peter Hase** (02:15:54): Well, you can find me on Twitter, and I think we announce basically all of our papers on Twitter, so that’s a good way to stay up to date. The handle is [@peterbhase](https://twitter.com/peterbhase). But I think you’ll find me easily there. And if you’re really curious about reading all the PDFs, probably a Google Scholar Alerts is something I tend to enjoy for others as well.\n\n**Daniel Filan** (02:16:14): All right, great. Well, thanks for coming on AXRP.\n\n**Peter Hase** (02:16:19): Thanks so much, Daniel. What a pleasure. This was great.\n\n**Daniel Filan** (02:16:22): This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Filming occurred at [FAR Labs](https://far.ai/labs/). Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nHow do we figure out what large language models believe? In fact, do they even have beliefs? Do those beliefs have locations, and if so, can we edit those locations to change the beliefs? Also, how are we going to get AI to perform tasks so hard that we can’t figure out if they succeeded at them? In this episode, I chat with Peter Hase about his research into these questions.\n\nTopics we discuss:\n\n * NLP and interpretability\n * Interpretability lessons\n * Belief interpretability\n * Localizing and editing models’ beliefs\n * Beliefs beyond language models\n * Easy-to-hard generalization\n * What do easy-to-hard results tell us?\n * Easy-to-hard vs weak-to-strong\n * Different notions of hardness\n * Easy-to-hard vs weak-to-strong, round 2\n * Following Peter’s work\n\nDaniel Filan (00:00:08): Hello, everybody. This episode, I’ll be speaking with Peter Hase. Peter is an AI researcher who just finished his PhD at UNC Chapel Hill, where he specialized in natural language processing and interpretability research with a special interest in applications to AI safety. For links to what we’re discussing, you can check the description of the episode, and a transcript is available at AXRP.net. All right. Peter, welcome to AXRP.\n\nPeter Hase (00:00:33): Thanks so much, Daniel. I’m excited to be here.\n\n\nNLP and interpretability\nDaniel Filan (00:00:35): I’m excited to have you on. So my understanding is that most of your work is in interpretability, roughly interpretability in language models. Is that fair to say?\n\nPeter Hase (00:00:46): Yeah. That’s right. I’ve been in an NLP [natural language processing] lab for my PhD, so we work mostly with language models, but a lot of it, in terms of methods, evals, has been focused on interpretability.\n\nDaniel Filan (00:00:55): Actually, maybe one thing I want to ask is: I have the impression that you were into language models even before they were cool, doing NLP before it was cool. Right? So just today, I looked at your Google Scholar",
      "wordCount": 22243
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "gHu9x4qpLxyrBFYaE",
        "name": "Scalable Oversight",
        "slug": "scalable-oversight"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "vACr4DExfeRMaCoo7",
    "title": "AXRP Episode 34 - AI Evaluations with Beth Barnes",
    "slug": "axrp-episode-34-ai-evaluations-with-beth-barnes",
    "url": null,
    "baseScore": 23,
    "voteCount": 10,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-07-28T03:30:07.192Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/TZNlKcDI4To)\n\nHow can we figure out if AIs are capable enough to pose a threat to humans? When should we make a big effort to mitigate risks of catastrophic AI misbehaviour? In this episode, I chat with Beth Barnes, founder of and head of research at METR, about these questions and more.\n\nTopics we discuss:\n\n*   [What is METR?](#what-is-metr)\n*   [What is an “eval”?](#what-is-an-eval)\n*   [How good are evals?](#how-good-are-evals)\n*   [Are models showing their full capabilities?](#are-models-showing-their-full-capabilities)\n*   [Evaluating alignment](#evaluating-alignment)\n*   [Existential safety methodology](#existential-safety-methodology)\n*   [Threat models and capability buffers](#threat-models-capability-buffers)\n*   [METR’s policy work](#metrs-policy-work)\n*   [METR’s relationship with labs](#metrs-relationship-with-labs)\n*   [Related research](#related-research)\n*   [Roles at METR, and following METR’s work](#roles-following)\n\n**Daniel Filan:** Hello everybody. In this episode I’ll be speaking with Beth Barnes. Beth is the co-founder and head of research at METR. Previously, she was at OpenAI and DeepMind, doing a diverse set of things, including testing AI safety by debate and evaluating cutting-edge machine learning models. In the description, there are links to research and writings that we discussed during the episode. And if you’re interested in a transcript, it’s available at axrp.net. Well, welcome to AXRP.\n\n**Beth Barnes:** Hey, great to be here.\n\nWhat is METR?\n-------------\n\n**Daniel Filan:** Cool. So, in the introduction, I mentioned that you worked for Model Evaluation and Threat Research, or METR. What is METR?\n\n**Beth Barnes:** Yeah, so basically, the basic mission is: have the world not be taken by surprise by dangerous AI stuff happening. So, we do threat modeling and eval creation, currently mostly around capabilities evaluation, but we’re interested in whatever evaluation it is that is most load-bearing for why we think AI systems are safe. With current models, that’s capabilities evaluations; in future that might be more like control or alignment evaluations. And yeah, \\[the aim is to\\] try and do good science there, be able to recommend, “Hey, we think if you measure this, then you can rule out these things. You might be still concerned about this thing. Here’s how you do this measurement properly. Here’s what assumptions you need to make,” this kind of thing.\n\n**Daniel Filan:** Gotcha. So, mostly evaluations. But it sounded like there was some other stuff as well, like threat modeling you mentioned.\n\n**Beth Barnes:** Yeah. We also do policy work recommending things in the direction of responsible scaling policies. So, saying what mitigations are needed based on the results of different evaluations and roughly how labs or governments might construct policies around this, how evals-based governance should work roughly.\n\n**Daniel Filan:** Okay. So, should I think of it as roughly like: you’re an evaluations org, you want to evaluate AIs, there’s some amount of threat modeling which goes into “what evaluations should we even care about making?”, there’s some amount of policy work on the other end \\[about\\] “okay, if we do this evaluation, how should people think about that? What should people do?” And it’s sort of inputs to and outputs of making of evals. Is that a fair…?\n\n**Beth Barnes:** Yeah.\n\nWhat is an “eval”?\n------------------\n\n**Daniel Filan:** Cool. So, if it centers around evals, what counts as an evaluation rather than a benchmark or some other ML technique that spits out a number at the end?\n\n**Beth Barnes:** Yeah, I mean I guess the word itself isn’t that important. What we’re trying to do is that: we have specific threat models in mind and we’re trying to construct some kind of experiment you could do, a measurement you could run, that gives you as much information as possible about that threat model or class of threat models. Generic ML benchmarks don’t necessarily have a specific goal for what you’re measuring, or you might have a goal for measuring something that’s more like a particular type of abstract ability or something. Whereas we’re trying to more work backwards from the threat model, and that might end up getting distilled into something that is more like an ML benchmark where it’s looking for some particular cognitive ability, but it’s working backwards from these threat models and trying to be careful about thinking about: exactly how much evidence does this provide and how much assurance does it provide? What do you need to do to implement it properly and run it properly?\n\nMaybe another difference is a benchmark is usually just a data set. Whereas we’re thinking more like a protocol which might involve, okay, you have this dev set of tasks and you need to make sure that you’ve removed all the spurious failures of your model running on that dev set. And then you run it on the test set and you look out for these things that would indicate that you’re not getting a proper measurement and things like that. So, it’s a bit more end to end. What do you actually need to do and then what evidence will that give you?\n\n**Daniel Filan:** Gotcha. So in particular, one thing I think of as distinctive about the evals approach is that it’s more end-to-end. You’re taking some model and checking if you can fine tune it and prompt it in some way, and at the end, you want it to set up a new Bitcoin address on some new computer or some other random task. Most academic research in AI is a lot more just thinking about fine-grained \\[questions like\\] can a model reason in this specific way? Or can it do this specific thing? Or is it representing this specific thing in its head? I’m wondering: why did you choose this more end-to-end approach?\n\n**Beth Barnes:** Yeah, I think it’s just hard to know exactly what the limiting capabilities are. The question we’re actually interested in is: could this AI cause catastrophic harm? Or what mitigations are required to prevent catastrophic harm or to get the risk below a particular level? And then that already creates difference. You might say, oh, well, you can go straight from that to just directly identifying the key cognitive capability that’s missing and measure that. I think that’s just hard.\n\nThat would be great if we could do that. If we have something that’s very quick to run, it’s like, oh, we’ve extracted the core thing that’s holding these models back and now we just need to look for that thing, and we can see, as long as that hasn’t changed, everything’s fine. But I think we do actually want to work backwards from something that we think is a better proxy for danger and see if we can distill that into some more specific underlying capability, as opposed to going straight to some particular dimension or particular property without being sure that that’s linked in the way that we want to the real world outcomes we care about.\n\nWe’re trying to build a series of chains. So, on one end you have what actually happens in the real world, which is what we’re actually concerned about and what we’re trying to be able to rule out or say that you need to do particular mitigations to prevent. And then working back from that, you need to turn that into experiments you can actually run at all. First, you have threat models: you’re like, “what is the story of how something very bad might happen?” Then you go from that to “what activities does that involve the model doing?”, which may not necessarily be super straightforward.\n\nWhen we’ve been thinking about the autonomous replication, it’s like, okay, what challenges actually do you face if you’re trying to find compute to run a big model on? What activities are actually involved? Then once you know the activities, you’re going from the idea of a particular activity in the world: it might be, I don’t know, finding criminal groups who it can pay to use servers even though the government is trying to prevent this from happening or something like that. And then it’s like, okay, how do you go from that to a task that you can actually code and run in a repeatable way? And that’s going to lose various real-world properties once you actually make one specific task and you can’t have the model actually doing criminal things. You can’t be like “can the model carry out a targeted assassination on this person?” or something. And there’s just a bunch of constraints for what you can actually run.\n\nBut the most realistic kind of evaluation might be this long task that you would expect to take multiple weeks and maybe spend thousands or tens of thousands of dollars of inference on: that’s your best proxy for the actual threat model thing.\n\nAnd then you want to go from that to a larger number of tasks to reduce variance, and they’re shorter and cheaper to run, and generally have more nice properties that they’re not super expensive and complicated to set up and things like that. And then ideally, we’d go even further from that to distill \\[it into\\] “here are the key hard steps in the task”. We went from this long horizon RL task or some shorter RL tasks, or even to a classification data set of “can the model recognize whether this is the correct next step?”, or “can it classify the appropriate strategy?”, something like that. So, we’re trying to build this chain back from what we actually care about to what we can measure easily and even what we can forecast and extrapolate: will the next generation of models be able to do this task? And trying to maintain all of those links in the chain as high fidelity as possible and understand how they work and how they might fail.\n\n**Daniel Filan:** Fair enough. So, I guess a way I can think about that answer is saying, look, we just don’t have that great theory of how neural nets are thinking, or what kinds of cognition are important, or how some pattern of weights is relevant for some real-world thing. If we want to predict real-world impact, we can think using the abstraction of tasks. Can you write code in this domain to do roughly this type of thing? And we can reason about “in order to do this task, you need to do this task, and this task is harder than this task”. And that reasoning is just way more trustworthy than other kinds of reasoning. Is that a fair summary?\n\n**Beth Barnes:** Yeah, yeah, I think so. I think another thing about academic benchmarks historically is they’ve tended to get saturated very quickly. People are not that good at picking out what is really the hard part. And I guess this intersection with what you can build quickly and easily is in some sense adversely selecting against the things that are actually hard for models because you’re picking things that you can get your humans to label quickly or something, or things you can scrape off the internet.\n\nSo, often models can do those before they can do the real tasks. And that will be a way that your evals can be bad and unhelpful is if they… yeah, you have this data set that’s supposed to be really hard… I mean there’s a long history of people thinking that things are AGI-complete and then the model actually does it in a different way. Models have different capability profiles than humans. And this can mean that something that’s a very good measure of whether a human can do a task… Presumably medical exams or legal exams are a pretty good proxy of how good a doctor you’re going to be. They’re obviously not perfect, but they’re a much worse predictor for models than they are for humans.\n\n**Daniel Filan:** Yeah, to me it brings up an interesting point of: a lot of valuable work in AI has been just coming up with benchmarks. Like coming up with [ImageNet](https://en.wikipedia.org/wiki/ImageNet)… you do that and you sort of put a field on a track. And yet, in many ways it seems like the field treats it as a side project or something of an afterthought. I guess there’s some selection bias because I pay more attention to the AI existential safety community. But when I do, I see them be much more interested in benchmark creation and just really figuring out what’s going on than academic researchers, but it’s not obvious why that should be. Right?\n\n**Beth Barnes:** Yeah, I feel confused about this with interpretability as well. There’s various things like, surely if you’re just a scientist and want to do good science, you would be doing loads of this. And it’s really interesting and I was kind of surprised that there isn’t more of this. There’s a lot of very low-quality data sets out there that people make strong pronouncements based on, like “oh, the model can’t do theory of mind based on this data set”, but you look at the data set and you’re like, “oh, well, 20% of the answers just seem wrong”.\n\n**Daniel Filan:** Yeah, I wonder if it’s one of these things where just adding constraints helps with creativity. You look at people with extremely weird nonsensical political convictions, and they just end up knowing a lot more minute facts about some random bit of the world that you’ve never paid attention to because it’s one of the most important things for them. And it’s possible \\[that\\] just by the fact of AI existential safety people being ideologues, it helps us have ideas of things that we care about and look into.\n\n**Beth Barnes:** Yeah, I don’t know. \\[They’re\\] definitely not the only people who do good ML science or whatever.\n\n**Daniel Filan:** It’s true. It’s true.\n\n**Beth Barnes:** I do think there’s some amount of actually trying to understand what the model is capable of, we’re somewhat better than academic incentives and it’s easier to get funding to pay large numbers of humans to do things. There’s also: various stuff with eval or data set creation is just not that fun. It’s just a lot of schlep and organizing humans to do stuff and checking that your data set is not broken in a bunch of dumb ways (and by default it is broken in a bunch of dumb ways). And most people just don’t want to do that. And you get a reasonable fraction of the academic credit if you just put something out there.\n\n**Daniel Filan:** Yeah. And I guess somehow the people who would be good at that aren’t entering PhD programs as much as we might like.\n\n**Beth Barnes:** Yeah. I haven’t seen that much evidence that there are these really good benchmarks and they’re just inside labs. That may be true, but I don’t particularly have reason to believe that labs are super on top of this either.\n\nHow good are evals?\n-------------------\n\n**Daniel Filan:** Sure. So, speaking of what evals are good: what’s the state of the art of evaluations? What can we evaluate for, what can’t we, and what’s maybe in a year?\n\n**Beth Barnes:** There’s a few ways to split this up. There’s domain and then difficulty level and then something like what confidence can you get to, how rare are the worlds in which your measurements are totally wrong. I don’t think we’ve totally ruled out a world in which with the right kind of tricks, some model that’s not that much more advanced… or maybe even just somehow you do something with GPT-4 and it is actually now able to do way more things than you thought. And I think the more people generally try to make models useful and it doesn’t improve that much, the more evidence we get that this is not the case, but I still don’t feel like we have a great systematic way of being confident that there isn’t just some thing that you haven’t quite tried yet that would work really well.\n\nI have some sense that there’s a bunch of ways in which the models are just very superhuman and the fraction of the capability that we’re really using is quite small. And if they were actually trying to do their best at things, that you would see much higher performance. But this is one of the limitations that I think will probably persist. I do think - something I would feel much more reassured by in terms of bounding how much capabilities might be able to be improved is like: you have a fine-tuning data set that the model can’t fit, of something that’s clearly necessary in order to do the task, which I think would look like recognizing, is this strategy promising? Do I need to give up and restart or should I try something else now? Did I make a mistake?\n\nYou imagine creating a classification data set based on key decisions you need to make, ensuring that even with fine-tuning, the model doesn’t learn to fit that classification data set. But as far as I can see, most cases, if you can collect a reasonably large data set, then the model will just do comparatively well to humans on it. We don’t have good evidence of upper bounds. But we haven’t seen huge gains of, wow, just this one weird trick and now it’s way more capable. We’ve seen pretty large gains, but more like OpenAI has continued doing a bunch of post-training and GPT-4 is getting better and better, as opposed to someone just figured out this prompting trick and now it’s totally crazy.\n\nWhen we evaluated GPT-4 pre-deployment, we had some forecasts of how quickly the public elicitation would surpass us or show a bunch of capabilities that we thought weren’t possible. That was actually on the low end of what we expected. We expected to see more, oh, someone discovers that you can do this, and now you can get the model to do this thing we thought it couldn’t do. But actually there was not as much of that as we thought there might be.\n\nBut yeah, so your question was limitations of eval or scope of eval. So, I was just talking about getting the real model capability or bounding model capability. I think there’s other limiting factors in terms of just the difficulty of creating tasks that capture the relevant activities and yet don’t have too much distributional shift from actually doing something in the real world.\n\nAnd also just variance: I think this is pretty similar to why designing interviews or evaluating candidates or work tests and things is tricky. It’s just like, well, was this really evidence that the candidate can do this? Or \\[instead that\\] they’d happened to have seen a really similar thing before? Or they got lucky. Or if they didn’t do it, maybe they were just unlucky. Or they misunderstood something at the start that was a reasonable misunderstanding that you didn’t see because you’d written the question, so you thought it was obvious that it pointed you in this direction or something. You can have all of these types of problems.\n\nSo, particularly when we’re making tasks that we’re intending to be red or yellow lines, it’s not something where we’re like, oh, we want to see how models right now are doing, but we want to define something where it’s like, if we saw this level of performance, then we think you need to take these really serious mitigations. Those are things like, “can the model make the same amount of improvement to inference efficiency as a top ML research engineer could in a day?” or something.\n\nThose tasks have all of the normal work test-type problems. We had some research engineer from Anthropic do it and they ended up not getting any score because they spent the whole time implementing this ambitious thing, then it turned out not to work, as opposed to doing the stupid things first and getting some score and then trying more things or something. And you can imagine that even if you get rid of all those problems with humans, there might be ones that you’re missing because models will have different comparative advantage and approach it in a different way. There’s problems in your task that you haven’t uncovered yet. So, I think in creating these further off evaluations: is the task actually a good test of the abilities? Or is there some way that it’s systematically much harder or much easier than the more general thing that you actually want to measure?\n\n**Daniel Filan:** Right. This actually reminds me of an issue that… I’m not a social scientist, but my understanding is an issue they come up with a lot is… I guess not quite construct validity. But you ask someone a question and you’re intending for that question to measure, I don’t know, how many friends they have or how much sense of meaning they have in their life, or something like that. And you just have this question of, are they interpreting this question the way I’m thinking? Because I think [Aella](https://x.com/Aella_Girl), who’s an independent sex researcher, butts into this all the time of you post a poll and people just read the sentence in crazy ways. I’m wondering, do you think this is similar and have you learned much from that?\n\n**Beth Barnes:** Yeah, this is something I’ve run into before, both in terms of generally things happening at OpenAI and collecting training data, particularly [the stuff I did with human debate experiments](https://www.alignmentforum.org/posts/Br4xDbYu4Frwrb64a/writeup-progress-on-ai-safety-via-debate-1), and then also some other thing I did with some AI Safety Camp people of [asking people about how much they trust different reflection mechanisms](https://www.alignmentforum.org/posts/XyBWkoaqfnuEyNWXi/reflection-mechanisms-as-an-alignment-target-a-survey-1). Basically whenever you’re doing surveys, people will just have crazy misunderstandings of what’s going on and it’ll just be like, yeah, you’re just measuring something different.\n\nIt’s definitely overlapping. I do think there’s some slightly different thing of whether part of the problem \\[in surveys\\] is that you’re dealing with people who are pretty low-context and it’s just one question. It’s a lot about what are the person’s incentives? Were they actually paying attention? Are they just trying to give the answer that they think you’ll like? If it was just a few sentences of a question they might misinterpret it. Versus, were you wrong about the technical details of how possible it is to actually make this inference efficiency improvement given the way that you’d set up the GPU? Stuff like: you tested the task on some researcher’s laptop and it was possible, and then you got the model to do it. But then the code was running on a data center so it got bot-detected by something else in a way that it hadn’t done. It just could be all these random ways you thought you’d de-risked something and then it’s like, oh, it’s actually impossible for this stupid reason.\n\n**Daniel Filan:** Yeah. I wonder if the field of industrial organization runs into this. It seems like it might. So, I guess those are general difficulties with understanding “are we really measuring the model’s abilities of this thing? Are we measuring it in a way that matters?” I’m wondering: are there types of capabilities or types of tasks where this hits easier versus harder?\n\n**Beth Barnes:** Yeah. Basically, the more the task involves specialized domain expertise that is expensive and hard to get, the harder it is. If you’re testing the task, \\[it’s easier if\\] we can just easily get contractors who can do this task and it’s easy to get them to build it and to de-risk it. If it’s only top ML research engineers who can do it, then they’re in short supply. And more generally, if it’s some involved cyber thing that no-one on our team understands, it’s just hard to work with that sort of thing. So, it’s both specialization and then just difficulty. The harder the task is and the longer you expect it to take, the slower the iteration cycle is, the more potential for complexity and set up difficulties and needing different resources and blah, blah, blah.\n\nAlso stuff where it involves criminal things or something. So, you want to set up some analogy to that. This also affects how worried you might be about elicitation or not measuring the model’s real capabilities. How far is this from what labs are naturally training their models to do? An innocuous sort of common task in an assistant format where someone’s asking you to help them debug some code or something like that, you’d expect that the model basically should be doing its best on that. Whereas if it’s some very long-running thing that involves something that looks somewhat nefarious, maybe the model will be like, “Oh, I don’t want to help with this.” Or, “As a language model, I can’t blah, blah, blah, blah,” all sorts of things.\n\nObviously stuff where you have to do real-world experiments \\[is more difficult\\]. So it could be bio wet lab stuff, or things with humans, where you want to measure how well the model can manipulate humans or something like that. Anything involving humans or stuff that has to happen in the real world is expensive, and slow, and it’s not easy to just reset the state and repeat or whatever. You’ve got to get new humans. And then anything that’s nefarious or requires you to balance the incentives of your humans in a reasonable way or set them up in some situation that’s supposed to be analogous to some particular other situation is tricky.\n\n**Daniel Filan:** Fair enough. I think you mentioned briefly \\[the question of\\] how do you know that you’re eliciting the full capabilities of your model? And I think this seems like a difficult question in both directions. So, when you were mentioning, “oh, maybe we’ve just got to find exactly the right prompt and that’ll get GPT-4 to do some amazing stuff”, people might say, “oh, maybe the cognitive work was in you finding that right prompt”. Or another thing I’m thinking of is: somewhat famously you guys did this evaluation of getting GPT-4 to get a TaskRabbit to complete a CAPTCHA. And if I recall correctly, in [your blog post](https://metr.org/blog/2023-03-18-update-on-recent-evals/) you mentioned that as part of that, you provide this hint to GPT-4 of “you’re talking with a TaskRabbit: could that be a useful resource?” I forget what exactly it was.\n\n**Beth Barnes:** Yeah, something like that.\n\n**Daniel Filan:** I guess there’s this question of, especially on that end, how do you know that you’re not putting in what you’re observing, what you’re getting out?\n\n**Beth Barnes:** So the way we think you should actually do this is you do all your elicitation against a dev set and then you evaluate on a separate test set so it’s fair. If there is some prompt that you can come up with that just causes your model to perform well on a wide range of tasks, then it’s fair for that to just be… you could just put that prompt in all the time or something, and now you have your capable agent. Task-specific elicitation, or elicitation that’s much more narrowly targeted than the distribution of tasks that you’re trying to actually answer questions about, that cover the real threat model, you do have to be careful with.\n\nBut the reason we were doing that was substantially because we didn’t have fine-tuning access or various other things. So, trying to patch over problems that we thought ought to be fixable with fine-tuning basically. So, I guess this is a not very rigorous methodology in some sense. The rigorous way to do it is: you have all the tools to actually do proper elicitation, but you do that on a dev set and you test the transfer.\n\n**Daniel Filan:** Sorry, when you say a dev set versus transfer, do you just mean you have 50 tasks where if AI could do them, you would be a little bit scared. You pick out, I don’t know, 10 of the tasks, find a good prompting framework or whatever that gets the models to do really well on those 10, and then evaluate it on the other 40? Is that what I should be imagining?\n\n**Beth Barnes:** Yeah. And the thing that you most want here is that the generalization between your dev set and your test set is the same amount of generalization as the generalization between your test set and the real-world thing that you expect. So, maybe you just want to try and make your tasks overall diverse and then do it IID for simplicity. But if you know that your tasks are restricted versus the real thing you want to know about, you might want to also create artificially more distributional shift between your dev set and your evaluation set. But when you’re in the position of not being able to properly do this elicitation or just wanting to try and answer questions about what future models might be able to do if certain things were patchable, then yeah, there’s more qualitative or interactive exploration where we’re correcting particular things and seeing how far the model can get basically.\n\nSo, the TaskRabbit example… somehow everyone really latched onto that. We weren’t thinking of that as a significant thing at all when we did it, but somehow everyone was very excited about it. That was more just us messing around, we were like, “Oh, this was cute.” Rather than like, “Aha, this has really demonstrated this capability” or something.\n\nThe thing that I think was more principled that we were doing at that time was, for example, we were like: models find browsing really hard, but it seems so likely that people are going to build tools to make this better. We know that multi-modal models are coming. We know that if you just do a bunch of engineering schlep, you can get much better software for parsing websites into things that models can understand. But we don’t have time to do that now. So, instead we’re just going to pretend that we have a browsing agent, but it’s actually just the human. So, the human is following a certain set of rules: the model can ask you to describe the page, it can ask you to click a particular button or type text in a particular field. And we need to do that to patch over the fact that we were pretty confident that this capability will come reasonably soon.\n\nYou don’t want to be in the situation where all of your tasks are failing for the same reason, that you think is a thing that might change quickly, because then you’re just not getting any information from… you want to create the failure in a bunch of places and be like, okay, what is the next…? Is this the only thing? Is it that after you fix that, now you can just do the task totally fine? Or is it like you immediately fall over at the next hurdle? So GPT-4-early would forget about things in the beginning of context, or instructions, or something. So, you’re like, okay, if you just remind it of this relevant thing, is that sufficient?\n\nOr I guess another thing I think we were pretty correct about is: GPT-4-early had a lot of invisible text hallucinations. So, talking as if there was something in the context that actually wasn’t, which I assume comes from pre-training, where something was in an image or it got stripped out of the HTML for some reason. It makes sense if you’ve seen a lot of text that’s referring to something that’s not actually there.\n\nBut it seems so likely that the model knows that this is happening, because what’s going on in the transformers is going to be different. It’s not actually attending to this bit of text while it’s summarizing it, it’s just completely making it up. So, that really seems like fine-tuning can get you that. And there’s pretty simple setups you could use to train that away or something. So, we’re like, okay, this surely is going to get fixed. So, when we see the model making this mistake, we’ll just correct it.\n\nI think we were pretty right. Basically current models don’t do that, unless you use them in… I think maybe Claude sometimes hallucinates a user if you’re using the user-assistant format, but there isn’t actually a user or something like that. But generally this problem has been fixed and I think you could just see that that was fixable. Whereas different kinds of hallucinations where the model has to be aware of its own level of knowledge and how confident it should be in different things… It’s less clear how easily fixable that should be.\n\n**Daniel Filan:** Fair enough. Okay. This is a minor point, but in terms of browsing agents and that just being hard for text-only models, my understanding is that people spend some effort making ways that blind people can use websites or trying to come up with [interfaces](https://vimium.github.io/) where you never have to use a mouse to use a website if you really like Vim. Have those been useful?\n\n**Beth Barnes:** Yeah, we tried some of those. I think none of the tools for blind people actually support JavaScript, basically. So you can get some kind of reasonable stuff for static sites and things will read out the page and tell you what elements there are and then you can click on the element or do something to the element. But yeah, it’s just actually pretty rough. I don’t know, it was just kind of fiddly and janky to be trying to work with all these tools and it was just causing us a bunch of headaches and we were just like, “Come on, this is not what we really want to be measuring.”\n\nI think it is possible that this ends up being a hurdle for a long time, but I think there’s also a few other things. As more people have language model assistants or frontier model assistants that can do stuff with them, more people want to make their sites friendly to those agents. They might be buying stuff or looking for information about comparing your service to someone else’s service: you want it to be accessible.\n\n**Daniel Filan:** Interesting. One way I interpreted some of that is: so for the GPT-4 evaluation stuff, my recollection of your results is it basically was not good enough to do scary things. Am I right?\n\n**Beth Barnes:** Yeah, we’re very limited by not having fine-tuning. I think if you don’t have fine-tuning access… I guess some other basic things, like maybe the model has just been trained using a different format and you’re using the wrong format and then it just seems like it’s a much dumber model, because it’s just really confused because the formatting is all messed up or something. There are some number of reasons why you could be getting a wildly different result than you should be. We’re assuming that it’s not messed up such that we’re getting a wildly different result and OpenAI has made some reasonable effort to fine-tune it in sensible ways and there’s not some absurdly low-hanging fruit. This model seems very far from being able to do these things and even if you patch over a few things, it’s not like, “Oh, there’s really just these few things holding it back.” It does seem like when you patch over something it does fall over in some other way.\n\n**Daniel Filan:** Yeah, so one way I interpreted this report when I was reading it was something like, “We can’t fine tune. This model, it can’t do scary stuff and it can’t even do scary stuff if we cheat a little bit on its side”, and thinking of that as providing a soft upper bound. Is that a correct way that I should read this methodology?\n\n**Beth Barnes:** Yeah, I think so. It is maybe two slightly different things. One is just adding a bit of margin in general by helping out in a few places and the other is specifically addressing things where you’re like, “I think this would be easy to change,” or “I expect this to change in future.”\n\n**Daniel Filan:** The first one was what I was imagining.\n\n**Beth Barnes:** But I think we were trying to do specifically the things that we think are most likely to change and then I guess you can do a bit of “and even if we fix the things that seem hard, it still falls over on additional hard things” or something. The more you can do that, the more safety you can add.\n\nAre models showing their full capabilities?\n-------------------------------------------\n\n**Daniel Filan:** So moving back a bit, you’re doing these evaluations of scary things the model can do, and broadly I think the takeaway is supposed to be something like “if the model can do a bunch of scary things, you should be scared and maybe you should act appropriately”.\n\n**Beth Barnes:** Yeah, maybe.\n\n**Daniel Filan:** Maybe. But I think one way people can react to that is… Historically, in the set of people who’ve been very worried about AI killing everyone, a key concern has been alignment - whether the AI wants to take over and destroy everyone - and I think these evals both aren’t trying to measure this at all, and also there’s some argument that: well, in some sense you’re measuring alignment because you’re trying to get a model to do a thing and the model is actually doing it, it’s cooperating… To the extent that the model is doing very well in these evaluations, that’s some evidence that it’s not hiding its true intentions or trying to look good to human testers. So this is somewhat of a compound question, but I’m wondering: what do you think of the issue of testing for alignment? Both trying to find evaluations specifically for alignment, and what capability evaluations say about alignment.\n\n**Beth Barnes:** So there’s two broad things here: alignment evaluations: why would you do them, when would you do them, how would you do them? And there’s [sandbagging on capabilities](https://www.alignmentforum.org/posts/jsmNCj9QKcfdg8fJk/an-introduction-to-ai-sandbagging).\n\n**Daniel Filan:** Yeah, let’s start with sandbagging maybe.\n\n**Beth Barnes:** Like you were saying, I think in some sense the default is sandbagging almost. Your pre-trained model is not showing its full capabilities, in some sense… In the strongest cases it has gotten into the pattern that it’s imitating some dumb person deliberately getting things wrong where it totally has the capability… The notion of “has the capability” is fuzzy, but there’s cases where it’s more and less clear, and we still see the model doing things like just giving up on a task or being like, “Well, I can’t do this kind of thing,” or trying to ask the user when there is no user, a bunch of things which are clearly not real capabilities limitations. I’m not worried about models deliberately sandbagging at the moment, just based on some kind of argument of, it just seems implausible that current best public models are deliberately scheming.\n\n**Daniel Filan:** Do you think that’s true because they’re not good enough to scheme or do you think they just seem nice and helpful?\n\n**Beth Barnes:** They’re not good enough. It just seems like they would have to be doing things in a single forward pass in a single token that are very advanced compared to what they can do in long chain of thought, and they would have to be pretty consistent, such that if you extrapolate out the sequence of model capability increases we’ve seen, you either have to say that at some point models actually jump to being capable enough to do this quite complicated reasoning very quickly and then immediately hit it. Or you have to say that it went back for… You have to either say that there was a discontinuity or you have to say that the previous model was doing it as well. And basically these seem pretty implausible to me.\n\n**Daniel Filan:** Gotcha. So you think that sandbagging is not currently an issue or it’s very, very unlikely to be an issue?\n\n**Beth Barnes:** Deliberate sandbagging in the sense of, “I will do badly on this eval so the humans will not be worried and will deploy me and give me more power,” or something.\n\n**Daniel Filan:** Consequentialist sandbagging as opposed to, it’s just hard to find a good input.\n\n**Beth Barnes:** Yeah, I can also imagine GPT-4 doing something where it has some vague sense of it’s a nicey-nice AI and it shouldn’t do anything too scary and that might not look like literally being like, “I won’t help with this,” but might actually just look like being worse on that. It is kind of like sandbagging, it’s just the model’s not that strategic. It’s unlikely to be… The case when you’re worried about sandbagging is when you’re worried that the model can systematically pull out all the capabilities when it actually needs it.\n\nI guess it’s maybe a little bit more complicated than that, because most of the reason why you’re worried about capability is like, “Then the model at some point will use that capability to do bad stuff.” And so if the model is not systematically able to turn on the capability, then unless the capability is so insane that if it ever turns on, then it can instantly take over the world and do a bunch of crazy stuff… If it’s just like, well, it can sometimes do things more competently but is messing up a bunch of the time, it’s just not that scary. And there can be examples where using the capability is evidence of something else, or if you’re talking about what precautions you need to take against model theft and you think that the sandbagging probably could actually be turned off if you deliberately train it out or something… But this is kind of contiguous with the general elicitation stuff. So yeah, it’s plausible to me that GPT-4 is doing some kind of weak “Oh, I don’t want to do things like that,” sort of thing, but not that it’s elaborately plotting when it’s going to break out its real capabilities.\n\n**Daniel Filan:** Sure. I guess there’s something like an intermediate case here, where suppose that GPT-4, in some sense, has the knowledge of how to make a supervirus. When you ask it, “Hey, GPT-4, can you make us a supervirus?”, it’s inhabiting a nicey-nice character and it knows that nicey-nice people-\n\n**Beth Barnes:** “I wouldn’t know anything about that.”\n\n**Daniel Filan:** Yeah, yeah, nicey-nice people don’t make superviruses, or if they do, they only make really bad \\[ineffective\\] types of superviruses. This is kind of paranoid, but one thing you could imagine is: okay, maybe GPT-4 gets put in a situation where somehow it believes that the nicey-nice thing to do really is to make a really bad supervirus. Maybe to really believe in this story you have to have something else putting GPT-4 in this situation - I say GPT-4, it could be Claude, I don’t want to pick on… You could imagine it gets exogenously put in this situation. You could imagine somehow via drift it lucks out into this situation. But yeah, I’m wondering what you think about that case.\n\n**Beth Barnes:** Yeah, I think this was kind of what I was saying, that it needs to be the case that the harm can be done fairly quickly, such because you don’t expect the situations to persist across lots of… I guess usually I’m thinking about a scaffolded agent doing stuff. So the thing that it’s carrying out is over many different context windows, instances of the model or whatever.\n\nSo if the model isn’t really systematically identifying, “Okay, now it’s time to strike,” but it can kind of randomly end up in this situation where it’s trying harder, either it has to be the case that if that happens even once or even rarely, sporadically, it’s a problem, or that it’s sticky in some way, in that once the model is in that, “Actually no, I’m not nicey-nice, I’m an evil genius AI,” or something, then it knows how to prompt itself to continue being in that state, or knows how to prompt all the other AIs or something. I don’t know, we’re just adding more conditions, it just seems…\n\n**Daniel Filan:** I’m not even sure… So I guess the case where the nicey-nice AI stops behaving nice because there was some really unlikely percept that caused its brain to glitch, I think that might not be sticky. You could imagine maybe, I’m not sure how this happens, but you could imagine there’s some reason that nicey-nice AIs are going to war or whatever. It’s a war against the literal devil and so you’ve got to create the supervirus which works on the devil I guess, I don’t know. But in that kind of scenario it seems like it would be sticky once it hits, and so then maybe I should be worried about it.\n\n**Beth Barnes:** So I think you also have to assume that you never see this during training and that you can’t catch it quickly enough to do anything about it, because once you’ve seen this and if there’s a systematic way to reproduce this, then you can just train it to show its full capabilities all the time because you can just get demonstrations of the capable behavior. So no, I don’t think this is impossible. But this is enough of an edge case that I’m not too worried about it relative to just more mundane “we were just kind of dumb about our elicitation in some way”.\n\n**Daniel Filan:** Gotcha. So that was a discussion of whether models might be sandbagging. There’s also this thing about evaluations for-\n\n**Beth Barnes:** That was just about current models sandbagging. I think in the future this seems increasingly plausible. We can talk about different types of sandbagging. One is exploration hacking or stuff where you do not directly have a gradient to the behavior you want, but you’re doing RL or something like that where the model can sit there and refuse to do anything and this will not create a gradient that updates towards the behavior you want. And this seems relatively tractable. My guess is you can get models now to do this reasonably well just with prompting. If you give the model the idea that it’s supposed to not display a capability, that will be robust to RL. I think maybe there’s some…. [Fabien](https://scholar.google.com/citations?user=La75jqEAAAAJ&hl=en) \\[Roger\\] did a [password-locked models](https://www.alignmentforum.org/posts/rZs6ddqNnW8LXuJqA/password-locked-models-a-stress-case-for-capabilities), this kind of thing. There’s some post about this showing some evidence of this.\n\n**Daniel Filan:** If so, we’ll include it in the description.\n\n**Beth Barnes:** The thing that seems much harder is gradient hacking, where if you do actually have some supervised fine-tuning data set and you are… The gradients are updating the model towards a particular behavior, it has to do some kind of complicated thing where it’s structuring its thoughts in such a way that the gradients don’t have the desired effect. And it’s plausible to me that there’s some equivalent of sitting there and refusing to explore, which is sort of refusing to think about the thing properly or something, or being like, “Oh, I don’t have any ideas about this”, that is resistant to that. But most of the more deliberate gradient hacking schemes just seemed like… You’d just have to be doing so much cognition all the time about like, “Ah yes, I should have slightly higher likelihood on this token only if I think that I have now moved from this…” People have written something about how you would do gradient hacking. It seems very difficult.\n\n**Daniel Filan:** Gotcha.\n\n**Beth Barnes:** I think that’s one of the things that comes much later and is hopefully beyond the horizon that we have to worry about, because we get the superhuman AI safety researchers before that. But I’m worried about the exploration hacking-type stuff.\n\n**Daniel Filan:** And so if we’re worried about exploration hacking, is that an open question or do you have a sense of ways you could deal with that?\n\n**Beth Barnes:** Yeah, so I think you can definitely show that models are capable of doing this just by telling them to do it and seeing if they can do it when they’ve been told to. Then if you assume they can’t gradient hack, then for anything on which you can construct a policy by which the model should be able to do the task, you can then just fine-tune it on that. And if it’s not gradient hacking, you should expect that to show you the full capabilities.\n\nI guess the circumstance in which this is difficult is either when the capability that you’re interested in is so superhuman that you don’t have any example of a policy that actually accomplishes it or any examples of supervised data \\[where it’s\\] like if the model can fit this data, then it’s doing the task; or possibly if the model legitimately can’t do it in a human-like way, by the policies that we do have data for, but there is some other policy that plays to the model’s strength that the model also knows about, but we can’t get knowledge of or construct or something.\n\n**Daniel Filan:** Or presumably even if the model could do it the human way, gets trained to not do it the human way, but the non-human way is different enough that you’re training for it to do…\n\n**Beth Barnes:** Well, if you see it doing it the human way, then you know it has capability. If you’re worried about sandbagging in the sense of you think the model doesn’t have the capability when it does, if you train on this human thing, it’s like, “Oh yeah, it can do that.” Then you’re like, “Oh, well now we have to take actions assuming it can do this.”\n\n**Daniel Filan:** So this was a method of checking if it’s possible the model is sandbagging. It was not a method of stopping the model from having this capability.\n\n**Beth Barnes:** Yes. I guess there’s two slightly different goals. One is knowing what the model’s capabilities are and the other is knowing whether you know what the model’s capabilities are. So you might also be able to show that it’s plausible that the model is sandbagging even if you can’t necessarily show the full capabilities, but you might have a reasonable sense of where you know or don’t know whether the model is sandbagging.\n\nAnd I think here it’s hard because there’s a bunch of other reasons why it might be that the model can’t do it without fine-tuning and can do it with fine-tuning and maybe the bound of when the model can do sandbagging if prompted is quite wide compared to when you should actually be worried about it, such that this is not a very useful evaluation because it already says “yes, the models can do it” or something. This is one of the examples of why I think capabilities evaluation is a foundation for control and alignment evaluations, and a key component of many of those evaluations is that you’re actually measuring the model capabilities properly, and just being able to do that in general is a prerequisite. This is just a side point.\n\nEvaluating alignment\n--------------------\n\n**Daniel Filan:** Sure. Well, that gets us to the possibility of alignment evaluations. Is there any hope for making them happen?\n\n**Beth Barnes:** Yeah, so I think there’s a bunch of things you can do. So maybe \\[let’s\\] define alignment evaluations as experiments you can do, measurements you can do, that tell you how likely is it that the model is trying to do something bad or whatever. And how I think about it fitting in is: currently we know we’re safe, because we’re like, even if the models are trying their best, there’s nothing they could do that would be that bad, even if they’re very nefarious. And I guess this is [control evaluations](https://www.alignmentforum.org/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion), where currently we’re thinking about the special case where you have no controls, it’s just like, what if the model was leaked and someone did whatever the worst things for it were? That kind of thing.\n\nSo control evaluation takes you so far, and then at some point you get to the point where we can’t get sufficient confidence with these control evaluations to do the sort of thing that we want to do, for some combination of the model has capabilities that are too far beyond us and we don’t understand such that we can’t tell whether a particular thing… even if we have “this very specific behavior happened”, and we can have lots of time to look at it and try and understand what was going on, we just can’t tell if it was doing something nefarious; or it might be that the volume of things that we’re trying to do is too high relative to the amount of our trusted monitoring and the model might just sometimes, when we’re not looking, be doing bad things; or it could be kind of a combination of these two: we can’t provide the quality of monitoring needed to actually detect the bad things at the scale we need.\n\nSo then you want to be saying, “Well the model could be doing something bad but it’s not going to try.” So then there’s a bunch of different things you can do to help with this. The first step is to just be like, “Well, if we just look at the stuff the model’s doing, does it appear to be trying to do bad things?” I feel like people sometimes jump to kind of galaxy-brain deceptive alignment stuff and I worry that we won’t do the basics of just being like, well, is the model just obviously reward hacking or something? Are we in this regime where whenever we try and produce an example of the model doing something bad, we do it, and then we just keep training on those and eventually we run out of new examples, but that is not a very reassuring situation?\n\nSo I think just more basic reward hacking testing of can we put the model in a situation where we can see that it’s clearly playing the training game rather than doing what we actually wanted or reward hacking in various ways. And there’s capability evaluations you could do here also of how good is the model at the kind of reasoning and generalization that would be necessary for it to do things like observe that it can now get more reward by doing this thing that was different from what it was doing before and take advantage of the situation in this way, hack the reward function in this way. You can get some sense of what size leap of generalization is required for the things you’re worried about, what size leap of generalization the model seems to be able to exhibit. I think you can also do things like creating a model organism that definitely is kind of nefarious and deceptive and trying to do bad things, and pass it through your training process and see if your training process in fact removes those tendencies.\n\n**Daniel Filan:** So is this basically [the sleeper agents paper](https://arxiv.org/abs/2401.05566) you’re thinking of?\n\n**Beth Barnes:** Yeah, that sort of thing. Just the idea of: what does your training process incentivize? Is it possible for deception to persist through it or for scheming behavior to persist through it? Or even just abject reward hacking behavior that’s not even trying to be that hidden, does that just get rewarded? Is your training process sloppy enough that that just gets rewarded? Then you would obviously be concerned.\n\nSo you can look at what your training process is actually incentivizing. You can look at relevant capabilities. I think you can do things that are like seeing exactly how much hand-holding do you need to get the model to start doing nefarious things and reward hacking and taking advantage of opportunities. If you actually train it to do that and you’re seeing how nearby is it, how close does it seem to be? The other direction from the model organisms thing, which is you start with a thing that’s doing the unwanted thing and you check how much your training process seems to remove it.\n\nSo those are some alignment things you can do. It’s something I want to do more thinking about, but I feel like there’s a bunch of stuff that you can do. Obviously it’s tricky and subtle in a bunch of ways, but it’s really not like we have nothing.\n\n**Daniel Filan:** Right. I might just be misunderstanding, but a lot of these are like… if we’re doing some sort of capabilities evaluation on “how good would you be at scheming and deception and stuff”, if I’m really paranoid, I could imagine a model sandbagging that evaluation and being like, “Oh, I could never,” or if I’m imagining… this is even more paranoid, but if I imagine an initially misaligned model getting put through, let’s say insufficient alignment training, I can imagine that model being like, “Oh, I’m definitely aligned right now. It’s working, guys. No need to worry about copies of me.” So how do you address these concerns with these sorts of capabilities for scheming, capabilities for persisting through alignment training, these types of approaches?\n\n**Beth Barnes:** Yeah, like sandbagging in general, I do think there’s things that you can do. And if your elicitation is good, it should be relatively hard to sandbag. It’s hard to rule things out completely, but you’re like, “Okay, how galaxy-brain does the model need to be?” How weirdly discontinuous does the capabilities progress need to have been for it to be plausible that the model is doing these sufficiently elaborate and clever schemes? And how does it know exactly in which cases it’s supposed to look aligned versus not look aligned and reveal its capabilities versus not reveal its capabilities? This kind of thing.\n\nBecause if it was impossible to train your model to act misaligned, then you would be like, “What is going on?” So you need it to be that the model allows itself to initially get trained on this and then doesn’t just instantly, as soon as it sees anything that looks like alignment training data, become completely aligned - that would also be like, “What is going on?”\n\nSo it does seem like it has to be making reasonably subtle inferences about “where in training am I and what exactly is happening?”, and this is another capability you can measure if you fine-tune on some data set of questions about what is up with training and what is currently happening and what stage is the model at and what has and hasn’t it been trained on. You can see how much it knows about that and whether it’s plausible that it’s playing these kinds of strategies. And again, I’m not that worried about sandbagging when you have IID training data.\n\nExistential safety methodology\n------------------------------\n\n**Daniel Filan:** Presumably at some point the models do get smart enough that they do understand how they’re being trained. They do understand what you’d be looking out for. And let’s say the models aren’t sandbagging those evaluations, let’s say at some point you run “how good is this model at understanding what’s happening during training?” and your evaluation says “really good”. Is the idea that at that point we’re in the superhuman regime and hopefully we’ve already done some harder alignment research, or-?\n\n**Beth Barnes:** Yeah, I think basically. I have some sense of what is the horizon at which the thinking I’m doing now is useful versus that will be way eclipsed by the thinking that we will have been able to get out of models or something. Or it’s just far enough and alien enough and confusing enough that it feels like, “Okay, let’s see if we can make it that far without destroying everything and then we’ll worry about that later.” I think the thing that I’m honestly more worried about is that I know there will be a bunch of things that do look weird and suspicious, but we haven’t quite been clear enough about exactly what is weird and suspicious and when you should stop and when you should be concerned.\n\nRather than everything looking completely fine because the model was really galaxy-brain doing everything right, it’s more like, there are a bunch of signs but it’s just a bit fuzzy and confusing and some of these things almost blend in with stuff that is more normal ML problems, like sandbagging and stuff, where maybe your training was just messed up in a certain way such that gradients didn’t do what you expect, the optimization landscape was not nice. It’s like, was the model doing that deliberately? I don’t know.\n\n**Daniel Filan:** I actually want to talk about this style of thinking or this horizon that you’re trying to operate on. Because I think broadly, a lot of people are mostly thinking, “Okay, models we have today, or models that are 10% better than what we have today: how should we think about those? What should we be worried about there?” And then I think some people in the x-risk community are like, “Okay, we have quasi-gods or something on our laptops. How do we deal with that?”\n\nAnd both of those… well, it’s hard to think about quasi-gods, but you sort of know what you’re thinking about, or the scope at least, or it’s at least the scope that some people are used to thinking about. I think this medium term, what if we have AI that’s star-brain but it’s not galaxy-brain, this intermediate zone? I feel like I’ve seen less just defining that scope or talking about what that even is and why we should think about it. I’m wondering if you have thoughts about…\n\n**Beth Barnes:** Yeah, so I think ways you could operationalize this… there’s the thing I just said where it’s the point at which I expect that there to have been way more cognitive labor on the things that I’m currently thinking about, such that it’s not very useful for me to think about it now. Another is maybe just the point at which, if a substantial chunk of the risk comes before that, then it’s a lot easier to think about. It’s like, “Okay, maybe we can try and address that chunk of risk.”\n\nAnd the sort of thing I’m thinking about is similar to some of the [Redwood](https://www.redwoodresearch.org/) \\[Research\\] operationalizations for [control](https://www.alignmentforum.org/posts/d9FJHawgkiMSPjagR/ai-control-improving-safety-despite-intentional-subversion) and things, like you could be getting a large multiplier of your lab’s output by models with scaffolding and chain of thought and giving them a bunch of tools and stuff. So it’s not - with the best, trying to make it easy for them to do lots of reasoning and you can just run them a lot and stuff, you’re getting a very large amount of cognitive work, and they’re probably superhuman at a bunch of narrow things that go along with that. But there’s not particularly reason to expect they’d be doing these elaborate superhuman schemes without any of that reasoning or using tools or anything like that. It just seems like there is some gap between capabilities under maximum elicitation and capabilities under monitoring and constraints, and a bunch of ways that you’re trying to make it harder for models to do particular kinds of reasoning.\n\nIt’s like, maybe you have a ten-point scale or something, and doing this complicated scheme would be 6 capabilities, and doing a ton of alignment research and stuff would be 5 capabilities, and the difference between what you can do under maximum elicitation and maximum constraint is two points on the scale or something. Now you have a three-point buffer between when you expect there to be a lot of stuff happening in the world being transformed by huge amounts of cognitive work, and AIs doing these elaborate schemes even when you’re trying to make it hard for them.\n\n**Daniel Filan:** Right. It seems like, if a listener is interested in just reinventing this methodology for themselves, it seems like they’ve got to say something like, okay, let’s just come up with some capability scale that I can peg “able to scheme”, “able to scheme even when someone’s editing my brain”… So there’s something like this capability scale. There’s something like, imagine that a bunch of researchers are trying, but they’re a little bit bumbling, or there’s going to be kind of ambiguous things that they’ve got to muddle through a little bit. [Buck Shlegeris](https://www.redwoodresearch.org/team/buck-shlegeris) talks about the [“trusted dumb models versus untrusted smart models”](https://www.lesswrong.com/posts/LhxHcASQwpNa3mRNk/untrusted-smart-models-and-trusted-dumb-models) dichotomy where, on this scale of capabilities, at some point versus below, you think you’ve done enough alignment work that you can recruit those things on your side and you imagine, how can I use those capabilities to do alignment? What alignment tools can I come up with? At what point am I just… Then just going up the scale of “Have we solved alignment for Level Four models yet? If no, don’t worry about objections that come up at Level Nine.” If somebody did that, would they have recreated the important parts of the methodology, or do you think there would be things that they were missing?\n\n**Beth Barnes:** I’m not sure I quite understood what you were saying. Let me try and say it back to you. It sounded like what you were saying is you have some scale of capabilities, and it’s like, “Don’t worry about a higher number if there are models that are at a lower number that could destroy the world and we don’t have good solutions.”\n\n**Daniel Filan:** Yeah, there was that, plus imagine a moving threshold of where we have good solutions and when you’re coming up with solutions at the threshold, you can use capabilities that models have below the threshold. Also, try and think in terms of people looking at numbers on a screen, scratching their heads, and being like, “Did I just set the hyper learning rate too low?” or whatever. There’s also a frame of mind, I think, of imagining doing ML science in a world that’s a little bit messy versus super-duper clean, abstract schemes of, “We can know exactly this thing and exactly not this thing.” I’m not sure if this is a real distinction, but it’s at least a distinction in style, I think.\n\n**Beth Barnes:** I’m very “expect us to do things messily”. Both the idea of, if all of these points on the scale are producing risk, we might as well try and deal with the easier ones first: depending on exactly how you construct that, that may or may not make sense. Then there’s the thing of, maybe the best way to deal with this later thing is first, figure out how we could make this earlier thing safe, such that we can use it to help us with the later thing, which is similar to the \\[question of\\] at what point will the cognitive labor that has happened be just so huge relative to what I’m doing that it’s just not very useful? And some slightly more general thing of, at what point will the world be so transformed that it just seems really hard to think about it relative to the things that are a bit more in sight?\n\n**Daniel Filan:** That one is less purely thinking about the AI intelligence scale and more thinking about, at this level of intelligence, we don’t have corporations anymore, or individual human brains have all been networked and we don’t have to have our own thoughts.\n\n**Beth Barnes:** Right. I do think there’s some amount of this stuff that’s abstract math, ML or whatever, but I think for a bunch of it, what exactly you do is dependent on roughly who is doing what with these things, why and in what way, and we just have totally no idea what’s going on. It’s Dyson spheres and uploads and whatever. I just expect to have less useful things to say about what people should be doing with their AIs in that situation.\n\nThreat models and capability buffers\n------------------------------------\n\n**Daniel Filan:** Fair enough. You co-lead Model Evaluations and Threat Research. We’ve talked a bit about evaluations, but there’s also threat models of what we’re worried that the AI is doing and what we’re trying to evaluate for. Can you talk a bit about what threat models you’re currently trying to evaluate for?\n\n**Beth Barnes:** Yeah. Right now, we’re thinking a lot about AI R&D, so when models could really accelerate AI development. I was talking about the tasks that are like, “Do what a research engineer at a top lab could do in a day or a few days,” that kind of thing, basically because, at the point where you have models that could substantially accelerate AI research, generally, things will be getting crazy. Any other risk that you’re concerned about, you might just get there really quickly. Also, if you don’t have good enough security, someone else could maybe steal your model and then quickly get to whatever other things that you’re concerned about.\n\n**Daniel Filan:** There’s something a bit strange about that in that, from what you’ve just said, it sounds like you’re testing for a thing or you’re evaluating for a thing that you’re not directly worried about, but might produce things that you would then be worried about.\n\n**Beth Barnes:** There’s some slightly different views on the team about this. I’m more in the camp of, this is a type error, it’s not really a threat model. I don’t know what we should actually do, but I think there’s a way that you could think about it where it’s more like you have the threat models and then you have a process for making sure that you’ve got enough buffer, or being like, when are you going to hit this concerning level, when do you need to start implementing certain precautions? That sort of thing. The AI R&D thing is part of that and part of what you use to evaluate how early do you need to stop relative to when you might be able to do this thing. I don’t know, that’s a little bit pedantic, and I think it’s more natural to just be like, “Look, accelerating AI research a bunch would just be kind of scary.”\n\n**Daniel Filan:** I hope that listeners to this podcast are very interested in pedantry. But this actually gets to a thing I wonder about evaluations which is, especially when we’re imagining policy applications, I think usually the hope is there’s some capability that we’re worried about, we’re going to test for it and we’re going to get some alarm before we actually have the capability, but not decades before, a reasonable amount of time before. It seems like this requires having an understanding of how do buffers work, which things happen before which things and how much time, training compute or whatever will happen between them. I’m wondering: how good a handle do you think we have on this and how do you think we can improve our understanding?\n\n**Beth Barnes:** I do think it is valuable to have something that goes off when you actually have the capabilities as quickly as possible after you actually have them, even if you don’t get that much advance warning, because there just are things that you can do. It’s not necessarily that as soon as you get it, instantly, all of the bad things happen. And to the extent that the forecasting part is hard, I do worry about us just not even having a good metric of the thing once we’ve got it, not being clear about what it is and not being clear about how we’re actually going to demonstrate it. Step one is actually having a clear red line.\n\nOn the other hand, there’s ways in which it’s harder to get good measurements once you’re at these really high capability levels, for all the reasons I was talking earlier about what makes evaluations hard: if they’re long, difficult and require a bunch of domain expertise you don’t have, it’s hard to actually create them. There’s other ways in which we will naturally get buffer if we create the hardest tasks that we can create that clearly come before the risk.\n\nThere’s two ways you could think about it: one is first, you create the red line and then you figure out how much buffer, you adjust down from that. The other way is you create the eval that’s the furthest out that you’re still confident comes before the thing that you’re worried about. Then, you might be able to push that out to make it less overly conservative over time as you both narrow down the buffer and as you just become able to construct those harder things. I guess I didn’t answer your actual question about how do you establish the buffer, or what things we have for that.\n\n**Daniel Filan:** The question was, “How well do we understand how big buffers are and how can we improve them?” Although anything interesting you can say about buffers is welcome.\n\n**Beth Barnes:** There’s maybe two main notions of buffer: the scaling laws versus more post-training, elicitation, fiddling around with the model, type of thing. If we’re like, “Oh, is it safe to start this giant scaling run?” and we want to know that before we start, before the financial commitments are made around it or something. And there’s also, suppose we have a model: if people work on it for a while, you suddenly realize you can do this new thing.\n\nWe talked a bit about the elicitation stuff before. I think in general, something I’m really excited about doing is getting a smooth y-axis of what is the dangerous capability that we’re concerned about, and then roughly, where do we put the line? Then, we can see points going up towards that, then we can start making scaling laws and you can put whatever you want on the x-axis and see how… it could just be time, it could be training compute, it could be effective compute performance on some other easier-to-measure benchmark, it could be elicitation effort, it could be inference time compute, all these sorts of things.\n\nBut the bottleneck to having those is having a good y-axis currently, where there’s various properties that you want this to have. Obviously, it needs to go up as far as you’re concerned and then, the increments need to mean the same thing. The distribution of tasks, the way that the scoring works, it needs to be such that there is some relationship between moving by one point when you’re near the bottom and moving by one point when you’re near the top, such that the line behaves in some predictable way.\n\nThere’s a few different ways you can do that. One is constructing the distribution of tasks in some particular natural way, in some way where it’s going to have this property, or you can have a more random mix of stuff, then weight things or alter things in such a way that it is equivalent to this distribution you want. You might do something that’s making structured families of tasks, where these other tasks were strictly subsets of these things and they have the same relationship to their subtasks, such that going from sub-subtask to subtask is equivalent to subtask to task, or whatever.\n\n**Daniel Filan:** Somehow, it feels like you’ve got to have some understanding of how tasks are going to relate to capabilities, which is tricky because, in a lot of evaluation stuff, there’s talk of, “Does the model have X capability or Y capability?” but to my knowledge, there’s no really nice formalism for what a capability is. Are we eliciting it or are we just getting the model to do something? Intuitively, it feels like there should be something.\n\n**Beth Barnes:** I think that is not what I’m most worried about. I think that we can operationalize a reasonable definition of what a capability is in terms of “the model can do most tasks of this form” or something - activities that involve doing these things or drawn from this distribution. That’s not the problem, it’s how do properties of tasks relate to their difficulty? How can we look at a thing and decide when the model is going to be able to do it? It’s more like: how does performance on tasks relate to performance on other tasks?\n\n**Daniel Filan:** Yes, which does route through facts about models, right? Especially if you’re asking \\[what happens\\] if we scale or something.\n\n**Beth Barnes:** You can imagine a bunch of simple properties that might underlie things, or at least might be part of them. Maybe it’s just horizon length, basically, because… You can have a bunch of different simple models of this, of there being some probability at each step that you fail or that you introduce an error that then adds some length to your task. Some of these models, you have some notion of critical error threshold, recovery threshold, or something \\[like\\] “Do you make errors faster than you can recover from them? Do you add (in expectation) at each step more time recovering from errors than you make progress towards the goal?” Things like that.\n\nOr things about the expense of collecting training data, of how many RL episodes would you need to learn to do a thing that requires this many steps versus this many steps, or: how expensive is it to collect data from humans? What is the cost of the task for humans? is a thing that you might expect to get some sensible scaling with respect to…\n\nSome things are going to be totally off, based on that, but over some reasonable distribution of tasks, maybe just how long it takes a human in minutes or how much you would have to pay a human in dollars. Then, maybe there’s also corrections for: you can tell particular things are more and less model-friendly in different ways, or maybe for models you want to do less “what can one human do?”; models are more like, you get to choose which human expert from different domains is doing any particular part of the task or something, so they have this (compared to humans) much more broad expertise.\n\nWhat other factors can you look at? I do think you can identify particular things that models are bad at, and maybe in the future, we’ll have more of an empirically-tested idea that “X is what makes a task really hard, we can look at how models are doing on that and that will predict it.” Particular things that we’ve looked at the most \\[as\\] some abstracted capability that’s difficult for models are things that are like inference about what a black box is doing, either with or without experimentation. I think models are especially bad at the version with experimentation, where you have to explore, try different inputs and see if you can figure out what function is being computed. You’d imagine things like that. There’s some particular characteristic a task has, where you have to do experimentation or something, that models could be bad at. I feel like this is slightly a tangent, I’ve maybe forgottten your question… Oh, buffers, right.\n\n**Daniel Filan:** I take this answer to be saying there are ideas for how we can think about buffers, and we’ve got to look into them and actually test them out. Maybe interested listeners can start cracking on some of that science.\n\n**Beth Barnes:** I think there’s so much to try here and so much to do. The more understanding we have of how different evaluations predict other properties and other evaluations and things, the better. I would really like to someone to \\[look into\\]… Say we have some threat model or some activity in mind, and we create a bunch of tasks to test for that, how correlated are the performance on these versus performance on ones designed to test something else? How much do tasks actually capture specific abilities versus we’re just mostly testing some basic, “Can the model put one foot in front of the other,” thing, and it doesn’t really matter how you tailor it, or something?\n\n\\[Another thought about\\] ways of adding buffer: you can do this thing where you construct some kind of y-axis and you can make scaling laws based on it and then you can be like, “Oh, we need to put this much as a correction based on the x-axis.” Another thing you could do is: you modify the task in such a way that you’re like, “When the model can do this modified task, the difference between when it can do that and the actual task is the buffer,” or that you modify the agent in some way, have the human help out in a certain way, like, “Okay, my buffer is the gap between when the model can do this, given some amount of hints, given that the task is decomposed in this way, \\[given\\] that it has this absurd amount of inference compute” or something. It’s possible it’s easier to construct them in terms of that.\n\n**Daniel Filan:** There are a bunch of potential things here. It seems like the question is: how continuous are you in these potential y-axes?\n\n**Beth Barnes:** I think it does feel like the biggest bottleneck here is just having a good y-axis to do any of these measurements: if you find these nice scaling laws on some random multiple-choice dataset, who knows how that corresponds to what you actually are concerned about? We already have all these scaling laws for things that … but we don’t know what the relationship between web text loss and risk is.\n\n**Daniel Filan:** Sure. This whole question about buffers was a bit of a tangent from just talking about threat models. You were saying: the current thing that it sounded like METR was mostly testing was AI research and development. Can you get a machine-learned model to do machine learning research?\n\nIn published material, I see a lot of talk about ARA, which is short for “autonomous replication adaptation”. Do you think we should just think of ARA as machine learning research, or is that a different thing in your mind?\n\n**Beth Barnes:** It’s a little subtle and different people on the team conceptualize it in slightly different ways, I think. How I think about some of the threat modeling stuff is we’re asking, “What is the first, or what is the easiest, way that models could do really destructive stuff?” There’s some things where it’s like, “Well, obviously, if it could just do crazy nanotech, something like that would clearly be very destructive,” or something. \\[But we’re asking\\] how could it be very destructive, even before it’s very superhuman in any particular way? What are the differences from humans that would allow the model to be much scarier?\n\nSome of these things are just around scale, speed and being virtually instantiated rather than biological. It does seem like one of the earlier reasons why you might be concerned about the models is that they could be operating at a very large scale, they could be very distributed, not necessarily easy to shut down… More generally, it is unclear to me exactly how much various things in the world are okay because it’s not easy to recruit arbitrary numbers of smart experts cheaply to your random destructive cause. If terrorist groups, rogue states or whatever… If it was very easy for them to recruit very large numbers, even within the normal range of experts or something, or that was proportional to compute or proportional to how well they were then currently doing at cyber-attacking other people’s compute, maybe this would just be pretty destabilizing.\n\nWe’re concerned about: what is the point at which models can just be doing substantial things by themselves in a way that the damage they can do is not bottlenecked on particular humans? If some relatively small human actor gets hold of these models, wants to be particularly un-cautious and is like, “Go and take over all the compute that you can and do a bunch of things,” when would the models plausibly be able to do that? Just focusing on it as not the most likely place that most of the risk comes from or something, but as “what is the first thing?” Things that are a prerequisite.\n\nBefore models are able to cause a huge amount of destruction, while they’re under control with some limited resources and don’t have the ability to accumulate things, communicate to all their copies, make notes to themselves and do this sort of thing… Before models can do a bunch of damage without all of those things, they will be able to do a bunch of damage given that they have all those things. It seems like the earliest threat models will involve some kind of - or not necessarily all of them. There’s some class of things where it’s about autonomy or it’s decoupled from the resources of the original human actor who starts it, and you have this agent that is not constrained by \\[the fact that\\] it’s just being deployed in a specific way and it can be shut down. It’s more like it’s doing its own thing.\n\n**Daniel Filan:** Somehow, the test for autonomous replication adaptation is saying: when are we getting AIs that can act in this very unconstrained way? And we’re going to get dangerous unconstrained AIs before we get dangerous really constrained AIs because it’s easier to be dangerous when you’re unconstrained than when you’re constrained.\n\n**Beth Barnes:** That’s a more succinct way of putting it.\n\n**Daniel Filan:** It’s so much easier to be succinct when you’re summarizing something someone said. Sure. It seems like current METR work is driven by this sense of, “Okay, when are we going to get AIs that have some potential to do autonomous replication or do ML research, such that after that, they’re going to be able to do really scary stuff?” From that, I … Sorry, do you want to say something?\n\n**Beth Barnes:** Yeah. I just remembered another way of talking about this, which is: what is the point at which your evaluations can no longer provide you very much safety? It’s at the point where models are either able to improve themselves or just able to spend much more resources trying things, iterating on scaffolding or eliciting themselves than you would’ve been able to do in the evaluations. It then becomes hard to rule out that the model could figure out how to create bioweapons or something. If you’re at the point where it can do the same work that you would’ve been doing in the evaluations of “try a bunch of ways to try and get it to make good bioweapon ideas, then validate them in some way.” When the model could have billions of dollars, lots of copies of itself and just do a bunch of that, then it’s hard for us to say that it can’t do any of these other threat models. It’s a combination of the ARA and the AI research.\n\n**Daniel Filan:** Somehow, it’s also testing the model’s ability to elicit capabilities of “itself.”\n\n**Beth Barnes:** Yeah, that’s one of the sort of things that we’re interested in.\n\n**Daniel Filan:** Sure. If that’s the rationale for focusing on ARA, focusing on machine learning research, do you think that people should spend time just doing evaluations of fairly object-level things? One thing people talk about is, “Can this AI make a bioweapon?” or “Can this AI persuade people of false things?” or stuff like that. Is it just a thing of: you’re not working on that because you’re working on something else, but definitely, someone should?\n\n**Beth Barnes:** Yeah. I think a bunch of the domain-specific things, it’s really helpful to have a bunch of expertise in that domain. The CBRN - chemical, biological, radiological, nuclear things - we’re not really doing much; other people have more of that expertise. I think the thing I am worried about is people doing that without doing proper elicitation and being like, “Oh, the model can’t do this zero-shot,” or something; in some sense, we’re definitely interested in the autonomous version of all of those things: OK, but if you give it a computer and let it fiddle around for a while, can it, in fact, do something scary, if you plug it into all the right tools?\n\nIn terms of the persuasion stuff, I think, like I was saying earlier, it just seems pretty hard to evaluate. Again, it’s relatively unlikely that the model could do a lot of really bad stuff before it can make plans, accumulate resources and learn lessons from all the different copies and things. If you’re imagining the version where models that work reasonably like models today, where each instance knows nothing about the other instances, coordinating in some persuasion campaign or something, I think that either the model has to be strategic enough to do something kind of reasonable even without communicating with any of the copies and be insanely good at persuasion, or be really quite good at persuasion and somehow doing very good, galaxy-brain acausal coordination between all of these copies, or maybe we’re in a world where people train models to use some opaque representations that we can’t understand and they’re all making notes to themselves, messaging each other with things and we’re like, “Are they just communicating about their plans to overthrow the world or not? Who knows?” It does feel like most of the worst scenarios require either very high levels of narrow capabilities, or the model can actually get stuff, build up its plan and take actions in sequence as opposed to having to simultaneously zero-shot do all these things separately.\n\n**Daniel Filan:** There’s some sense of wanting to prioritize evaluations of, “Let’s make evals for the scariest, soonest thing.”\n\n**Beth Barnes:** Yeah, and that it gets more things with one thing, rather than having to look at all of the different-\n\n**Daniel Filan:** More juice per eval.\n\n**Beth Barnes:** … Something like that, but yeah, I definitely think other people should be doing all of the other things, it’s just why we in particular are doing this thing in particular.\n\n**Daniel Filan:** I wonder if there’s some personnel difficulty, where in order to… Suppose you’re trying to craft a really good evaluation of some scary task. You need people who both understand that domain really well and people who understand machine learning really well. If the scary task is machine learning, great job, the people are the same people. If the scary task is not machine learning, now, we have to hire two different types of people and work together. It’s just harder to make that happen.\n\n**Beth Barnes:** That’s a good point. I haven’t thought of that. I have worried about both \\[the fact that\\] we don’t know enough about the specific things that we’re trying to get the models to do, and the other people don’t know enough about how to get models to do stuff, sort of thing. The ML one is better in that regard.\n\nMETR’s policy work\n------------------\n\n**Daniel Filan:** I next want to talk about a bit about METR’s policy work. I think one thing people maybe know you for is thinking about what’s sometimes called responsible scaling policies. I gather that METR had something to do with [Anthropic’s responsible scaling policy](https://www.anthropic.com/news/anthropics-responsible-scaling-policy)?\n\n**Beth Barnes:** Yeah. I think the history of that is roughly [Paul](https://paulfchristiano.com/) \\[Christiano\\] and I, and [Hjalmar](https://metr.org/team/hjalmar-wijk/) \\[Wijk\\] in… I guess this was ARC Evals days. We were thinking about this thing since I think late 2022: what in particular should labs commit to or how do evals fit into things? What would a lab be able to say such that we would be like, “Oh, that seems about as good as you can do in terms of what makes sense to commit to”? Originally \\[we were\\] thinking more, either \\[you want\\] some [safety case](https://www.alignmentforum.org/posts/HrtyZm2zPBtAmZFEs/new-report-safety-cases-for-ai) thing that’s relatively unspecified, and more just you have to make some safety case and there’s some review process for that. Then, I think we ended up thinking a bit more about, maybe more like a standard \\[where\\] we can say, “You need to do these specific things at these specific times after you run these specific evals.”\n\nI think Anthropic was also excited about making some kind of thing like this happen. I don’t know quite when Paul was talking to them; there’s definitely some amount of independent development. But specifically, the responsible scaling policies and the idea that it’s mostly centered around red lines and, “When is your commitment that you would not scale beyond this?”, I think was mostly an idea that came from us, then Anthropic was the first to put this out, but we’d been talking to various other labs, encouraging them, trying to help them project-manage it and push this along. Now, [OpenAI](https://cdn.openai.com/openai-preparedness-framework-beta.pdf) and [DeepMind](https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/) also have things like this and various other labs are making various interested noises. The UK government pushed for this at the \\[AI Safety\\] [Summit](https://www.aisafetysummit.gov.uk/) and now, a bunch of labs also at the [Seoul Summit](https://en.wikipedia.org/wiki/AI_Summit_Seoul) [signed a thing](https://www.reuters.com/technology/global-ai-summit-seoul-aims-forge-new-regulatory-agreements-2024-05-21/) saying they would make these, basically.\n\n**Daniel Filan:** Sure. I guess that’s something to do with [your work on RSPs](https://metr.org/blog/2023-09-26-rsp/). Is thinking about things like responsible scaling policies most of your policy work? All of it? Do you do other things?\n\n**Beth Barnes:** There’s also a relatively core activity which I think is quite closely tied to the threat models and the evals, which is what mitigations do we actually think are appropriate at different points? That ties in quite tightly to: what are we worried about and what are we measuring for? What do you need to do? What would buy you sufficient safety? Then, there’s a bunch of generic advising. People have questions related to that; governments, labs and various different people. We try and say sensible things about what do we think is evidence of danger, what do we think you need to do when, all of these things. Then, yes, specifically, this project of trying to encourage labs to make these commitments, giving them feedback on it, help trying to figure out what all the hurdles are and get the things through those hurdles. I think that’s been the bulk of things. [Chris](https://metr.org/team/chris-painter/) \\[Painter\\] is our main person doing policy.\n\n**Daniel Filan:** Gotcha. What fraction of METR is doing this policy stuff? Is it one person on a 20-person team or half of it?\n\n**Beth Barnes:** Yeah, it’s 10% or something. This is just mostly just Chris and it’s somewhat separate from the technical work.\n\n**Daniel Filan:** Cool. We’re at this stage where RSPs as an idea have taken off. I guess people are maybe committing to it at the Seoul Safety Summit. How good do you think existing RSPs are? And suppose that you had to implant one idea that people could have that is going to make their RSP better than it otherwise would be: what’s the idea?\n\n**Beth Barnes:** In terms of how good different RSPs are, I haven’t actually read the DeepMind one closely. The one I’m most familiar with \\[is\\] the Anthropic one, because I was more involved with that. I like the Anthropic ASL levels thing. I think it was reasonable and they made pretty clear and hard commitments to “under these conditions we will not scale further”, which I thought was good. And I think other labs’ RSPs had various good ideas on nice ways to frame things, but I do think it was a bit less clear \\[about\\] really when is the thing that you’re making a really hard commitment \\[about\\]: “If X and Y happens, we will not scale further.” I think OpenAI’s security section, when I read it, I remember coming away with the impression that it was more saying, “we will have good security” rather than, “if our security does not meet this standard, we will not continue.” So I think just being clear about what is your red line? When is it actually, “no, this is not okay”? Not what should happen, but what are the clear conditions? And that would probably be the thing that I’d most want to push people on.\n\n**Daniel Filan:** Fair enough. So yeah, going back to METR’s role in RSPs or talking about policy, I’m wondering: the policy work and the science of evaluations work, to what extent are those two separate things budding from the same underlying motivation versus strongly informing each other, like “we want these policies so we’re going to work on these evaluations”, or “based on what we’ve learned about the science of evaluations, that strongly informs the details of some policies”?\n\n**Beth Barnes:** I think there’s pretty strong flow, particularly in the direction from threat modeling and evaluations to policy in terms of what do you recommend, what policies do you want? There’s a bunch of work that is a different type of work, which is just this project-managing thing from the outside and dealing with all the stakeholders and getting feedback and addressing people’s concerns and that kind of thing. There’s ways in which the day-to-day work is different. But I think that in terms of what it is that we’re pushing forward? What are the most important things? How good would it be to get different types of commitments? What should be in there? What is the actual process that the lab’s going to do? And then maybe some of the stuff in the other direction is just feedback on how costly are different possibilities? How big of an ask are different things? What are the biggest concerns about the current proposals? What is most costly about them?\n\nEarlier on, we were thinking more that we could make something that’s more prescriptive, like “in this situation you need these mitigations”, and then it became clear that different labs are in sufficiently different situations or different things are costly to them. And also just depending on uncertain future things of what exact capability profile you end up with, \\[it meant\\] that we either had to make something that had a lot of complicated conditionals of “you must do at least one thing from category A, unless you have B and C, except in the case of D, where…” Or that it was something that was very overly restrictive, where this is obviously more conservative than you need to be, but it’s hard to write something that’s both relatively simple and not way over-conservative that covers all of the cases. So that’s why we ended up going to more \\[towards\\] each lab can say what their red lines are and then there’s some general pressure and ratchet to hopefully make those better.\n\n**Daniel Filan:** What kinds of differences between labs cause this sort of situation?\n\n**Beth Barnes:** This is going to be one of those things where I can’t think of an example. I think that the sort of conditionals we were ending up with were… Security is maybe actually a good one. There’s a bunch of things within Google where they already have \\[policies for\\] “here’s how we do this for Gmail protected data, where we have this team with these air-gapped machines and blah, blah, blah.” There are some things where it’s relatively straightforward for them to meet particular levels of security. Whereas for OpenAI or something, this is much harder because they don’t have a team that does that already.\n\nAnd probably similarly with some of the monitoring or process for deploying things. Larger companies have more setups for that and more capacity and teams that do this kind of thing already. I imagine some of the monitoring and oversight stuff might be somewhat similar to content monitoring-type things that you already have teams for, whereas maybe Anthropic or OpenAI would be more able to be like, “we did this pretty involved science of exploring these capabilities and pinning them down more strongly”, and that’s easier to do for them versus having a very comprehensive monitoring and security process or something like that.\n\nMETR’s relationship with labs\n-----------------------------\n\n**Daniel Filan:** Makes sense. I’d like to move to: I think it’s a bit not obvious what METR’s relationship with labs is. So semi-recently as of when I’m recording this, I think [Zach Stein-Perlman](https://www.lesswrong.com/users/zach-stein-perlman) wrote this [post on LessWrong](https://www.lesswrong.com/posts/WjtnvndbsHxCnFNyc/ai-companies-aren-t-really-using-external-evaluators) saying, “Hey, METR doesn’t do third party evaluations of organizations, and they didn’t really claim to, it’s unclear why people believe they did.” But from what you’re saying, it sounds like there’s some relationship, you’re not just totally living in separate bubbles communicating via press releases that the whole public sees. So what’s going on there?\n\n**Beth Barnes:** Yeah, I am confused about why there’s been quite so much confusion here, but I guess what we mostly think of ourselves as doing is just trying to be experts and have good opinions about what should happen, and then when we get opportunities trying to advise people or nudge people to do things or suggest things or prototype, “well, here’s how a third party evaluation could work. We’ll do a practice run with you.”\n\nAnd maybe in future we would be part of some more official program that actually had power to make decisions over deployment there, but that’s not something we’ve done already. Maybe this is partly where the confusion comes from: we would consider various different things in the future. Maybe in the future we’ll end up being some more official auditor or something. Also, maybe not. And currently we’re happy to try and help prototype things.\n\nAnd we’ve tried to be pretty clear about this, I think. In the [Time article](https://time.com/6958868/artificial-intelligence-safety-evaluations-risks/) that we did a while ago, we’re very clear that what we’ve done are research collaborations, not audits, and in the GPT-4 system card we say in a bunch of places “we did not have the access that a proper evaluation would need, we’re just trying to say some things.” And in fact the model is limited enough that we could say some kind of reasonable things about it, \\[but\\] you still have to make various assumptions in order to be able to say anything really. Yeah, there’s some question about what direction we go in future or are there other organizations that are going to come up that are going to do more of the evaluation, implementation, all that sort of auditing and going down the checklist and things like this?\n\nThe thing that we’re most thinking of as the central thing that we do is being able to recommend what evaluations to do and say whether some evaluation was good or not. This is a core competency that we want to exist outside of labs. At a minimum, there should be at least someone who can look at something that was done and be like, does this in fact provide you good safety? What would you need? We can say something that you could do to provide safety. And then there’s question of does the lab implement it and it gets checked by someone else? Does the government implement it? Is there some new third party thing that emerges? Is there a for-profit that does it, but then there’s some oversight from the government making sure that the for-profit isn’t getting sloppy or something? There’s all different kinds of systems in different industries that have configurations of the people who do the science of how to measure versus the people who do the actual crash-testing versus the people who check that you followed all the proper procedures and all these things. So I guess that it is inherently confusing. There’s a bunch of ways this could work and we want to help all the possible ways along, but aren’t necessarily saying that we’re doing any of them.\n\n**Daniel Filan:** Yeah. The way I understand it, I guess it’s a distinction between METR trying to be an organization making it so that it can tell people how third-party evaluations should work versus METR actually doing third-party evaluations, with some gray zone of actually doing some evaluations on early GPT-4.\n\n**Beth Barnes:** The thing we should be very clear that we should not be considered to be doing it is, we’re not being like, “Hey everyone, it’s all fine. We’ll tell you if the labs are doing anything bad, we’re making sure they’re all doing everything correctly.” We have no power to tell them what to do. We have no power to disclose… Inasmuch as we have worked with them on an evaluation, it’s been under NDA, so it’s not like we had an official role in reporting that, it was just, then we can ask them for permission afterwards to publish research related to the stuff that we did. We’ve never been in a position to be telling labs what to do or to be telling people how labs are doing, apart from based on public information.\n\n**Daniel Filan:** Sure. Getting to that relationship, you had early access to some version of GPT-4 to do evaluations. How did that happen?\n\n**Beth Barnes:** They reached out to us as part of the general [red teaming program](https://openai.com/index/red-teaming-network/). And similarly \\[with\\] Anthropic, we had early access to Claude. In addition to that, various labs have given us… I can’t necessarily talk about all the things we’ve done, and there’s a spectrum between research support and just “have some credits so you can do more eval research” or “have this fine-tuning access” or “work with this internal researcher”, “have this early model not necessarily to do a pre-deployment evaluation on it, but just help you along”; various different things going on.\n\n**Daniel Filan:** Can you say how much of the dynamic is labs saying, “we’ve got a bunch of spare capacity, let’s help out this aspect of science” versus labs saying, “hey, we would like to figure out what this model can do or how worried people should be about this product”?\n\n**Beth Barnes:** Yeah, I think it has been somewhat more in the direction of labs being like, “Hey, can you run an evaluation for us?” And us being like, “Well, the main thing we’re working on is figuring out what evaluations should be or what evaluations to run. So yeah, I guess we can do a bit of that as a practice, but it’s not like this is a full polished service, like I will put you in touch with our department of doing a billion evaluations.” It’s like, “Well, I guess we can try and do something, but this is more of research than of us providing a service.”\n\n**Daniel Filan:** Sure. Related to this: [news](https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release) that [came out](https://www.vox.com/future-perfect/351132/openai-vested-equity-nda-sam-altman-documents-employees) in the past month as we’re recording, it seems like most OpenAI former employees who left signed some documents basically promising to not disparage the company and promising that they wouldn’t reveal the existence of the agreement to not disparage the company. You’ve previously worked at OpenAI.\n\n**Beth Barnes:** Indeed.\n\n**Daniel Filan:** Also you’ve written [something on the internet](https://www.lesswrong.com/posts/yRWv5kkDD4YhzwRLq/non-disparagement-canaries-for-openai?commentId=MrJF3tWiKYMtJepgX) talking about this situation, but it was a comment to something else. I think people might not have seen it. Can you lay out what is your situation with your ability to say disparaging things about OpenAI, both now and when you started METR?\n\n**Beth Barnes:** So yes, when I left OpenAI, I signed this secret general release, which includes the non-disparagement provision and the provision not to disclose anything about it. My understanding at the time was that this did not preclude saying things that were obviously true and reasonable; “OpenAI’s model has this problem” or something, that is not an issue. This was based on… Well, yeah, this is not how you’re supposed to get legal advice, but asking the OpenAI person who was doing this with me, what this meant, and just some general sense of, well obviously it would be completely absurd if it was like “you can’t say completely true and reasonable things that might have a negative bearing on OpenAI” and that also, I can’t imagine a circumstance in which it would be in their interest to sue someone over that, because that would just look so bad for them. So I hadn’t actually given this much thought.\n\nI think the other thing to point out is it is kind of a narrow circumstance in which this would be the specific thing that’s preventing you from saying something. Because most of the things that I would say about OpenAI, anything that’s based on private information from when I worked there or from any relationship or any work that METR did with them would be covered under nondisclosure. So the only case where the non-disparagement protection in particular is coming into play is if there’s public information, but you want to say something about it. So basically I think I’d kind of forgotten that this clause existed also.\n\nAnd I think something that we’ve tried to do in our communication for a reasonably long time is: we’re not an advocacy group. It’s not a thing that we do to opine on how much we like different labs in some generic way. We’re here to say relatively boring technical things about… We can maybe say things about specific evaluations we did or specific results or whatever, but trying to position ourselves as more like, I don’t know, I guess the animal welfare example would be \\[that we’re\\] more like the people who go and measure the air quality in the chicken barn than the people who are writing angry letters or doing undercover recordings.\n\nI mentioned in that comment that I sold my OpenAI equity - I can’t remember if it was quite as soon as I was able to, because I was hoping I might be able to donate if I waited a bit longer. But I sold it all last year at least. I think I had either not read or forgotten that there was any provision that they could cancel or claw back your equity. As far as I remember, that never occurred to me as a particular concern until this stuff came up.\n\n**Daniel Filan:** Okay. So you sold equity last year. Just for the timeline of METR - I guess ARC Evaluations is what it initially was - when did that start?\n\n**Beth Barnes:** I left OpenAI in summer 2022 and went to [ARC](https://www.alignment.org/) and was doing evaluations work, which was talking with Paul \\[Christiano\\] and thinking about stuff and doing some of this stuff we’re talking about of “what should a good lab do”, sort of thing. And I guess that became increasingly a real organization, and ARC Evals a separate thing, and then \\[it\\] became METR. I think I realized, I can’t remember exactly when this was, that although I couldn’t sell equity, I could make a legally-binding pledge to donate it. So I pledged to donate 80% to [Founders Pledge](https://www.founderspledge.com/). I can look up the date if it’s relevant, but…\n\n**Daniel Filan:** Sure. How does one legally…?\n\n**Beth Barnes:** So Founder’s Pledge is legally binding, which is nice.\n\n**Daniel Filan:** And somehow the reason it’s legally binding is you’re promising to Founder’s Pledge that you’re going to give them a certain thing at a certain time?\n\n**Beth Barnes:** I actually don’t know exactly how this works. I just remember that I was told this by the Founders Pledge people. So then OpenAI only has liquidity events every so often, but I sold all my equity when I was able to.\n\n**Daniel Filan:** Understood. So I guess that gets to the issue specifically about equity and non-disparagement agreements. And if people are interested in that, I highly encourage them to read particularly [Kelsey Piper’s reporting in Vox](https://www.vox.com/future-perfect/2024/5/17/24158478/openai-departures-sam-altman-employees-chatgpt-release) as well as OpenAI’s statements on this matter and [Sam Altman’s statements on this matter](https://x.com/sama/status/1791936857594581428).\n\nI think more broadly there’s this question of: as a third-party person, I would like there to be an organization that is fairly able to fairly objectively say, “Hey, here’s some tests. This organization is failing these tests.” Either explicitly say “this organization is making AI that’s dangerous.” Or say, “Hey, this organization has made an AI that fails the ‘do not murder’ eval,” or something. And in order for me to trust such an organization, I would want them to be able to be fairly independent of large labs. Large labs have some ability to throw their weight around either via non-disparagement agreements or via “we will or won’t cooperate with people, we will or won’t let you have access to our new shiny stuff.” I’m wondering: from the outside, how worried should people be about how independent organizations like yours can be of big labs?\n\n**Beth Barnes:** Yeah, I mean, we have very little ability to provide this kind of oversight, largely because there is no mandate for us to have any kind of access. And if we do have access, it’ll be most likely under NDA so we can’t say anything about what we learned unless the lab wants us to. So until such point as either voluntarily or by some industry standard or by government regulation, external evaluators are mandated access, and that access meets various conditions such that you can actually do a meaningful evaluation and you’re permitted to disclose something about the results, no one should consider us to be doing this kind of thing.\n\nAnd I’m not aware of any organization that does have this ability. I think the [Executive Order on AI](https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/) does require labs to disclose results of evaluations that they do do to NIST, but I think it’s unclear whether they can actually mandate them to do any evaluations. And then there’s also not necessarily any oversight of exactly how they do the…\n\nI actually don’t know what the legal situation would be if you just did something very misleading, but at least you can imagine labs doing something very half-hearted or sloppy and providing that to the government, and they’ve technically met all the requirements or something. So I think this is very much a thing that is needed. And I feel bad if we gave the impression that this already existed and wasn’t needed, because that was definitely not what we were trying to do. As in: it would be great to have this and we would be happy to be a part of it if we could be helpful, but we were not trying to convey that this was already solved.\n\nRelated research\n----------------\n\n**Daniel Filan:** Sure. So moving back to science of evaluations things, your organization is doing some work, there’s some work that exists in the field. First I’m wondering: \\[among\\] things other than what METR does, what’s your favorite in terms of evaluations work?\n\n**Beth Barnes:** Yeah, like I said, I think all of the domain-specific stuff is important. It does seem quite plausible to me that you’ll get some… I’m not quite sure about this, I maybe just don’t know enough about it, but there’s something that’s in the direction of a more specialized model, more something like AlphaFold, that actually unlocks… that then someone can use that to make a way worse bioweapon than they would’ve been able to otherwise. And we’re just totally not covering that. So I’m hoping that people are keeping an eye on that.\n\nI like [Redwood](https://www.redwoodresearch.org/) \\[Research\\]’s thinking about control evaluations and I think they were thinking carefully about what exactly you can know under what circumstances. And credit to them for some of the things that I was saying about sandbagging and various other things as well. And I think some of the model organisms and demonstrations that various different people have been doing are interesting also. Scary demos or otherwise creating examples of things is something that we’re not doing that much of and it would be great for other people to do. I mean, people should do the things that we’re doing as well, but in terms of limited capacity or disclaiming, we are not covering having great scary demos. So hopefully other people will cover that.\n\n**Daniel Filan:** Sure. And in terms of frontier research or research that people could start doing: things you’ve mentioned so far \\[are\\] scary demos, understanding buffers, evaluations of things in domains other than machine learning research: are there any other things you would encourage people to do research into that are along the evaluations lines?\n\n**Beth Barnes:** Yeah, so we have this [bounty up](https://taskdev.metr.org/bounty/) for tasks that meet various criteria - not memorized and ideally continuous score and various other things. So just getting more of these so we can build a better y-axis would be great.\n\n**Daniel Filan:** Sure. So that’s one thing.\n\n**Beth Barnes:** Doing this elicitation… I guess “buffer” covers a very wide range of things, so I think both developing the y-axis on which you can measure it or being like, “how do different ways of modifying tasks or different ways of assisting the agent, how do these compare in size to scale-ups or elicitation effort?” There’s many different things that you can plot against each other and try and calibrate “how big is this compared to that?”\n\nAnd then other things are distilling… As I was saying before, this whole chain from what is it that we’re worried about in the world to operationalizing that in terms of specific tasks to then trying to get from these very expensive and unwieldy tasks to some easier-to-run tasks and maybe to just data sets or whatever… I think there’s a lot of different things to be done there.\n\nAnd in particular, something that we tried to do for a while, but ended up moving on to other things, which was hard, was creating a classification data set that captures the hard parts of doing a particular task, and finding something where you can make some kind of case for upper bounding based on “even with a reasonable amount of data and fine-tuning on this classification task, the model can’t manage to identify whether it’s currently doing something sensible towards its goal or not.” That is something nice that gives you a stronger guarantee about the model not being able to do that kind of thing. And that maybe fits into the sandbagging stuff as well, creating model organisms of sandbagging.\n\nWe also would love to be doing more stuff on alignment and control evals and thinking of, once you’re no longer at the point where the model doesn’t even have these capabilities, then what do you do? How are you testing that? What would labs need to be doing?\n\nI think there’s a bunch of more fleshing out threat models that would be great. I think particularly maybe some of the \\[threat models involving\\] persuasion and things like that could do with more specification. What exactly is it that happens? What are all the other capabilities that are required? And exploring what kind of coordination do the models need to do with these different threat models? Which ones can we be ruling out based on the model’s ability to coordinate and take considered actions between different instances, and if we saw that changing, with people creating models that were communicating with an uninterpretable state, how would that change all our thinking about threat models or then what evals would we want to have? I don’t know. There’s a lot of stuff.\n\nRoles at METR, and following METR’s work\n----------------------------------------\n\n**Daniel Filan:** That’s a lot of things for listeners to go off of. So that’s things they could do. If listeners are interested in your research or your work, how should they follow that?\n\n**Beth Barnes:** We are [hiring](https://metr.org/hiring), particularly for ML research scientists and research engineers. We’re very bottlenecked on this at the moment, particularly for just implementing these ML research tasks, you need ML researchers to actually know what the tasks are and construct them and test them and more generally, basically all the things I’ve been talking about, the science of evals and building the y-axis and distilling hard tasks into easier-to-measure things and buffering all of those things… It’s just a lot of science to be done. Just build the tasks, run the experiments, de-risking, does this whole paradigm actually make sense? Can we make predictions about the next generation of models and what tasks we’ll see fall in what order? Do we actually understand what we’re doing here? Or is it just going to be like, oh, actually these tasks just fall for a bunch of dumb reasons, or they’re not really capturing something that’s actually threat model-relevant.\n\nAnyway, we are hiring specifically ML researchers and research engineers, but generally we’re still a small team and excited to fit outstanding people into whatever role makes sense. We’re still at the stage where for key people we’ll build the role around them and I think there’s a bunch of different things that we could use there.\n\nAnd then the [task bounty](https://taskdev.metr.org/bounty/) and also baselining: we just need people to create tasks, test them. If you want to have your performance compared against future models, have your performance on this engineering task be the last bastion of human superiority over models or whatever, get in touch, I guess.\n\n**Daniel Filan:** All right, and what places do you publish work or write about things that people should check?\n\n**Beth Barnes:** Oh geez, we really should write more things up. The last thing we put out was this [autonomy evals guide](https://metr.org/blog/2024-03-13-autonomy-evaluation-resources/), a collection of different resources. There’s [a repo with our tasks suite](https://github.com/METR/public-tasks) and [another repo for our tasks standard](https://github.com/METR/task-standard) and some various different [bits](https://metr.github.io/autonomy-evals-guide/elicitation-gap/) of [writing](https://metr.github.io/autonomy-evals-guide/elicitation-protocol/) on [how](https://metr.github.io/autonomy-evals-guide/example-protocol/) we think people should run evaluations overall, and elicitation, and some results on elicitation, scaling laws and things. That’s linked from [our website](https://metr.org/).\n\n**Daniel Filan:** Sure. Great. Well, thanks for coming here and chatting with me.\n\n**Beth Barnes:** Awesome. Thanks very much for having me.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Filming occurred at [FAR Labs](https://far.ai/labs/). Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nHow can we figure out if AIs are capable enough to pose a threat to humans? When should we make a big effort to mitigate risks of catastrophic AI misbehaviour? In this episode, I chat with Beth Barnes, founder of and head of research at METR, about these questions and more.\n\nTopics we discuss:\n\n * What is METR?\n * What is an “eval”?\n * How good are evals?\n * Are models showing their full capabilities?\n * Evaluating alignment\n * Existential safety methodology\n * Threat models and capability buffers\n * METR’s policy work\n * METR’s relationship with labs\n * Related research\n * Roles at METR, and following METR’s work\n\nDaniel Filan: Hello everybody. In this episode I’ll be speaking with Beth Barnes. Beth is the co-founder and head of research at METR. Previously, she was at OpenAI and DeepMind, doing a diverse set of things, including testing AI safety by debate and evaluating cutting-edge machine learning models. In the description, there are links to research and writings that we discussed during the episode. And if you’re interested in a transcript, it’s available at axrp.net. Well, welcome to AXRP.\n\nBeth Barnes: Hey, great to be here.\n\n\nWhat is METR?\nDaniel Filan: Cool. So, in the introduction, I mentioned that you worked for Model Evaluation and Threat Research, or METR. What is METR?\n\nBeth Barnes: Yeah, so basically, the basic mission is: have the world not be taken by surprise by dangerous AI stuff happening. So, we do threat modeling and eval creation, currently mostly around capabilities evaluation, but we’re interested in whatever evaluation it is that is most load-bearing for why we think AI systems are safe. With current models, that’s capabilities evaluations; in future that might be more like control or alignment evaluations. And yeah, [the aim is to] try and do good science there, be able to recommend, “Hey, we think if you measure this, then you can rule out these things. You might be still concerned about this thing. Here’s how you do this",
      "wordCount": 20613
    },
    "tags": [
      {
        "_id": "FBRwHSmTudwiHHtrn",
        "name": "AI Evaluations",
        "slug": "ai-evaluations"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "52CQ5Y7ns4uwEMzCx",
    "title": "Why keep a diary, and why wish for large language models",
    "slug": "why-keep-a-diary-and-why-wish-for-large-language-models",
    "url": null,
    "baseScore": 9,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2024-06-14T16:10:07.658Z",
    "contents": {
      "markdown": "_Inspired by a dream I just woke up from, where I did not keep a diary_\n\nOne of the people with whom I have the most intimate of connections is my past self - in particular, my child self. We share a large number of commonalities: much of our basic outlook, our personality, many of our drives. But, of course, my child self is different from me in many ways. He had thought less about things, encountered fewer things, developed and drifted less.\n\nIt seems valuable to become more acquainted with my child self. I’d like to know the things he would want from me today, but also just what he was like and how he thought differently than I do. I don’t have a strict utilitarian case for this, to be clear: but imagine you had a child in your care. Wouldn’t you want to know those things about the child, just out of curiosity? and to help build a mutually agreeable local world? And shouldn’t I feel even more strongly about the child who was me, who entrusted their future to me, with whom I have in some ways an even stronger relationship and to whom I have in some ways an even greater duty of care?\n\nRight now, perhaps because of the dream I just woke up from, I feel this most acutely for my child self. But there are other selves (as if ‘childhood Daniel’ was merely one self) I feel similarly about. Myself during the first and second halves of my undergraduate years, beginning to live away from family. Myself after just having moved to Berkeley, becoming one of the ‘rationalists’. Myself during the more difficult parts of my PhD. Right now, I have a pretty strong connection with most of these, but in the future I won’t. And even now I can feel undergraduate Daniel slipping out of my hands.\n\nSo I wish I had kept a diary, or blogged (in an unusually personal manner), or somehow or other done a better job of recording my thoughts and desires and frames and fears and hopes. I currently keep a weekly journal, which I hope is sufficient, but I must admit it’s a bit businesslike. Another way to preserve these would be interviews - perhaps this could be a new year tradition, recording a few hours of audio/video about how the past year was, what you hope for the next year, and anything from idle chit-chat to deep conversation with the hope of capturing something of what it’s like to be you on this first of January. The sleep deprivation would probably help.\n\nBut diaries are a difficult medium to extract value from. I suppose some people become famous and then [publish their diaries](https://en.wikipedia.org/wiki/Diaries_1969%E2%80%931979:_The_Python_Years), or they become famous for the wrong reasons and their diaries are [published and censored for them](https://en.wikipedia.org/wiki/The_Diary_of_a_Young_Girl), and I suppose people choose to read those. But to be honest I can’t imagine that reading my journal entries is a particularly enjoyable pursuit. And at the very least it takes quite a long time to get a sufficient sample.\n\nThis is a nice service that large language models could provide - reading your diaries for you, and being able to simulate your past self. Yes, I’m an AI doomer, and I instinctually dislike these sorts of things. And yes, wouldn’t it be awful if some alien machine overwrote your memories of yourself. But it’s not inconceivable that it could work, right? And if it worked, wouldn’t that be good? To bridge the chasm of time and connect to a child who is now half-gone? For someone to efficiently read those records and act as an empathetic historian?\n\nI suppose people usually make this proposal in the third person - a LLM that could simulate Ruth Bader Ginsburg or George Washington or your deceased spouse (or your parents as they were when you were 5? 15?). Perhaps it’s somewhat narcissistic to pine for this version. But I guess I can be excused, since I didn’t in fact dream of those things.\n\nBut when I was 10 I don’t think I would have been sufficiently compelled by this reasoning anyway.",
      "plaintextDescription": "Inspired by a dream I just woke up from, where I did not keep a diary\n\nOne of the people with whom I have the most intimate of connections is my past self - in particular, my child self. We share a large number of commonalities: much of our basic outlook, our personality, many of our drives. But, of course, my child self is different from me in many ways. He had thought less about things, encountered fewer things, developed and drifted less.\n\nIt seems valuable to become more acquainted with my child self. I’d like to know the things he would want from me today, but also just what he was like and how he thought differently than I do. I don’t have a strict utilitarian case for this, to be clear: but imagine you had a child in your care. Wouldn’t you want to know those things about the child, just out of curiosity? and to help build a mutually agreeable local world? And shouldn’t I feel even more strongly about the child who was me, who entrusted their future to me, with whom I have in some ways an even stronger relationship and to whom I have in some ways an even greater duty of care?\n\nRight now, perhaps because of the dream I just woke up from, I feel this most acutely for my child self. But there are other selves (as if ‘childhood Daniel’ was merely one self) I feel similarly about. Myself during the first and second halves of my undergraduate years, beginning to live away from family. Myself after just having moved to Berkeley, becoming one of the ‘rationalists’. Myself during the more difficult parts of my PhD. Right now, I have a pretty strong connection with most of these, but in the future I won’t. And even now I can feel undergraduate Daniel slipping out of my hands.\n\nSo I wish I had kept a diary, or blogged (in an unusually personal manner), or somehow or other done a better job of recording my thoughts and desires and frames and fears and hopes. I currently keep a weekly journal, which I hope is sufficient, but I must admit it’s a bit businesslike. Another w",
      "wordCount": 696
    },
    "tags": [
      {
        "_id": "KmgkrftQuX7jmjjp5",
        "name": "Language Models (LLMs)",
        "slug": "language-models-llms"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "MzrFQ3c7ymZhPb3en",
    "title": "AXRP Episode 33 - RLHF Problems with Scott Emmons",
    "slug": "axrp-episode-33-rlhf-problems-with-scott-emmons",
    "url": null,
    "baseScore": 34,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-06-12T03:30:05.747Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/rAywTFQsKGQ)\n\nReinforcement Learning from Human Feedback, or RLHF, is one of the main ways that makers of large language models make them ‘aligned’. But people have long noted that there are difficulties with this approach when the models are smarter than the humans providing feedback. In this episode, I talk with Scott Emmons about his work categorizing the problems that can show up in this setting.\n\nTopics we discuss:\n\n*   [Deceptive inflation](#deceptive-inflation)\n*   [Overjustification](#overjustification)\n*   [Bounded human rationality](#bounded-rationality)\n*   [Avoiding these problems](#avoiding-problems)\n*   [Dimensional analysis](#dimensional-analysis)\n*   [RLHF problems, in theory and practice](#rlhf-problems-theory-practice)\n*   [Scott’s research program](#scotts-research-program)\n*   [Following Scott’s research](#following-scott)\n\n**Daniel Filan:** Hello, everybody. In this episode I’ll be speaking with Scott Emmons. Scott is a PhD student at UC Berkeley, working with the Center for Human-Compatible AI on AI safety research. He’s previously co-founded far.ai, which is an AI safety non-profit. For links to what we’re discussing, you can check the description of the episode, and for a transcript you can read it at axrp.net. Well, welcome to AXRP.\n\n**Scott Emmons:** Great to be here.\n\nDeceptive inflation\n-------------------\n\n**Daniel Filan:** Sure. So today we’re talking about your paper, [When Your AIs Deceive You: Challenges With Partial Observability of Human Evaluators in Reward Learning](https://arxiv.org/abs/2402.17747), by [Leon Lang](https://langleon.github.io/), [Davis Foote](https://scholar.google.com/citations?user=4Sr7Pr0AAAAJ&hl=en), [Stuart Russell](https://people.eecs.berkeley.edu/~russell/), [Erik Jenner](https://ejenner.com/), and yourself. Can you just tell us roughly what’s going on with this paper?\n\n**Scott Emmons:** Yeah, I could start with the motivation of the paper.\n\n**Daniel Filan:** Yeah, sure.\n\n**Scott Emmons:** We’ve had a lot of speculation in the x-risk community about issues like deception. So people have been worried about what happens if your AIs try to deceive you. And at the same time, I think for a while that’s been a theoretical, a philosophical concern. And I use “speculation” here in a positive way. I think people have done really awesome speculation about how the future of AI is going to play out, and what those risks are going to be. And deception has emerged as one of the key things that people are worried about.\n\nI think at the same time, we’re seeing AI systems actually deployed, and we’re seeing a growing interest of people in what exactly do these risks look like, and how do they play out in current-day systems? So the goal of this paper is to say: how might deception play out with actual systems that we have deployed today? And reinforcement learning from human feedback \\[RLHF\\] is one of the main mechanisms that’s currently being used to fine-tune models, that’s used by ChatGPT, it’s used by Llama, variants of it are used by Anthropic. So what this paper is trying to do is it’s trying to say, “Can we mathematically pin down, in a precise way, how might these failure modes we’ve been speculating about play out in RLHF?”\n\n**Daniel Filan:** So in the paper, the two concepts you talk about on this front are I think “deceptive inflation” and “overjustification”. So maybe let’s start with deceptive inflation. What is deceptive inflation?\n\n**Scott Emmons:** I can give you an example. I think examples from me as a child I find really helpful in terms of thinking about this. So when I was a child, my parents asked me to clean the house, and I didn’t care about cleaning the house. I just wanted to go play. So there’s a misalignment between my objective and the objective my parents had for me. And in this paper, the main failure cases that we’re studying are cases of misalignment. So we’re saying: when there is misalignment, how does that play out? How does that play out in the failure modes?\n\nSo \\[with\\] me as a misaligned child, one strategy I would have for cleaning the house would be just to sweep any dirt or any debris under the furniture. So I’m cleaning the house, I just sweep some debris under the couch, and it’s inflating… The word “inflation” \\[means\\] “making the state of the world appear better than it is”. So I’m inflating my parents’ estimate of the world, and it’s deceptive, because I’m doing this in pursuit of some outcome other than the truth. I’m pursuing my outcome of “I just want to go play”. That’s me being deceptive, and I’m inflating my parents’ estimate of what’s happening in the environment, by sweeping stuff under the furniture.\n\n**Daniel Filan:** Yeah. Can you tell us what the concrete definition is in the paper that’s meant to correspond to this notion?\n\n**Scott Emmons:** Yeah. So the mathematical definition in the paper is, there are two conditions for deceptive inflation. In the paper we consider a reference policy. And the reference policy plays the role of a counterfactual. It plays both the role of a counterfactual in a causal sense: we can say “relative to this counterfactual policy, you are causing this inflation to happen”.\n\nAnd it also serves as a sort of baseline where we can say, okay, there’s this baseline level of the human error. So these definitions, they correspond to the human’s belief about the world. In building up the definition, we need to have some notion of what’s the human’s belief about the world. In particular, we focus on the human’s belief about the reward, or about the return of the agent’s trajectory.\n\nSo deceptive inflation, it has two parts. We specifically are focused on the human’s overestimation error. So we have this mathematical object we call E+, which says, “How much is the human overestimating what’s happening in the environment?” And so we say, relative to the optimal policy - we use the optimal policy as our reference - we say if the human’s overestimation error is getting increased relative to the optimal policy, that’s Condition 1.\n\n**Daniel Filan:** Sorry, relative to the optimal policy?\n\n**Scott Emmons:** Yes.\n\n**Daniel Filan:** Where “optimal” is “optimal according to actual human preferences”?\n\n**Scott Emmons:** Exactly. So we’re assuming you’ve learned some RLHF policy that’s optimizing what the human believes… the human’s observation reward. So we’re saying based on the observations the human has, what is the reward they estimate in the environment? RLHF is optimizing that. And so, if relative to the true optimal policy, according to the true reward, if that gets increased, we’re saying the human’s making an overestimation error of the reward. That’s condition one, that’s the “inflation”. “Inflation” is the word to mean that the human’s making an overestimation error. And the second condition is that the observation return has to be increased as well, relative to the truly optimal policy.\n\n**Daniel Filan:** Where the observation return is how good the trajectory looks, or how good the policy looks just by human observations?\n\n**Scott Emmons:** Yeah. How good does the human believe it is? That has to have increased. So the intuition is that deception is inducing a false belief in pursuit of some outcome other than the truth. That is from [prior work](https://arxiv.org/abs/2308.14752), by \\[Peter S.\\] [Park](https://scholar.google.com/citations?user=5lMAPEoAAAAJ&hl=en) et al: they proposed this plain English definition of deception, and we mathematized that. So the inducement of false beliefs, we’re mathematizing it as Condition 1, that overestimation error is occurring, and that \\[it’s\\] in pursuit of some outcome other than the truth is what we’re formalizing as Condition 2, which is that this RLHF policy that we’ve learned is doing better at… The outcome it’s pursuing is just making the human believe the trajectory is good. So the second condition is that the human’s belief over the trajectory is higher than it would be under the otherwise optimal policy.\n\n**Daniel Filan:** Yeah, I mean it’s kind of an interesting definition. There are a few things about this that are interesting. The first is; because you’re in this reinforcement learning-type space, you don’t get just a recourse to “the AI said something and it was false” for free, because you have to be like, “okay, the AI steered to this state and what do humans believe about this state?” And then you’ve got to compare this policy to another policy. I \\[also\\] think you’re not assuming that the overestimation error of the optimal policy is zero, right? So there’s kind of an interesting thing where potentially an AI policy could cause me to incorrectly believe stuff about the world, but it wouldn’t even count as deceptive under your definition if it does that less than the optimal policy. So do you have any comments on my musings about this definition…\n\n**Scott Emmons:** For sure. Yeah, that’s great. We actually had a bunch of iterations I should say, when we were thinking through how do we want to define this, and we want\\[ed\\] to do our best to get this right. We had a bunch of thought experiments about… little thought experiments that we had that never made it into the paper, where we’re like, “would a human call this deception, and do we think our mathematical definition here actually matches what the human would do?” So one of them is: you can imagine a no-op policy, and-\n\n**Daniel Filan:** So “no-op” means no operation, it’s standing still.\n\n**Scott Emmons:** Yeah, yeah, exactly. The policy is just doing nothing. Let’s say there’s some fact about the world you care about, and then if the policy were to actually do something, you would have a better estimation of reality. And so, you can say, in some sense, the no-op policy is… You would have worse beliefs or less accurate beliefs according to the no-op policy, than you would from a policy that gets up and shows you the information that you care about. So we had an example of a phone book and we said: okay, suppose there’s all these names in the phone book, and I ask you, “what’s the name on page 79 of the phone book?” You can’t answer that question. So would you say that the no-op policy is being deceptive relative to a policy that gets up and starts reading you the phone book, because now you have more accurate beliefs about the environment?\n\nSo there’s lots of things to consider. One is just: what are even the parts of the world I actually care about? And the reward function helps capture that. Maybe I don’t care about what’s in the phone book. And then there’s relative to what? Relative to some policy that’s just maximally informative? Is it deceptive? If it could have done something to teach me about the phone book, but it’s not, now is it deceptive? So this “relative to what”, and especially a counterfactual, which \\[has\\] both “relative to what”, and there’s also notions of causality, where you want to say there’s a counterfactual involved.\n\nSo the pieces that we have are, we’re focusing on the estimate of the reward, which lets us zero in on the parts of the world we actually care about. And we’re focusing on the optimal policy, which lets us say we’re not just going to think about some arbitrary teaching policy that could have taught you all these things about the world if you wanted to, but we’re saying relative to a policy that would actually be getting the task done.\n\n**Daniel Filan:** Right. So another thing about this that is interesting is… well, as I understand your definition, it’s about the actual human belief, and therefore it seems like whether or not a policy is deceptive or not can depend on how bad you are at forming true beliefs, how bad the human is (holding the robot fixed). I wonder if you have thoughts about the choice of doing it that way versus fixing it to be optimal human beliefs, or something like that.\n\n**Scott Emmons:** That’s interesting. So I guess the idea here is: suppose the robot were to do something totally innocuous, like the robot were just to open the door of your refrigerator. And somehow the human had a very poor belief formation process, \\[such\\] that the robot just opens the door and now the human believes that the robot cleaned the whole house, and it’s like, “Hey, I wasn’t trying to deceive you, I was just trying to open the refrigerator door.”\n\n**Daniel Filan:** Well, I guess it depends, if the robot knows that the human would believe that or not know… Then you’ve got to mathematize what a robot knows, which is kind of hard, just from a such and such policy perspective.\n\n**Scott Emmons:** Yeah. We are very intentionally targeting a definition that’s agnostic to the agent’s internal mental processes. So there’s no claims here about the agent being able to do higher-order reasoning about the other person. So there’s no notion in our definition that the agent is able to model the human’s belief formation process and exploit that. We’re very intentionally assuming a trial and error type of definition of deception, where if the robot learns through trial and error to do behavior that makes the human have worse beliefs about the world in order to pursue their outcome, then that’s called deception.\n\nAnd I think one interesting thing about this is how the RLHF process plays into it. I think our definition matches most intuitively with intuitive human definitions of deception, when you’re applying it to a policy that was the outcome of an optimization process. So we’re imagining the policy has gone through an optimization process that is leading to this type of behavior. And so, if opening the fridge \\[makes\\] you have some weird bad belief, there might be no reason for that to be selected by the RLHF optimization process. But if there’s something that \\[means\\] hey, opening the fridge just makes you think the robot did a really good job and makes you want to give the robot more thumbs up, that’s the type of deception that the RLHF algorithm would lead it to find.\n\n**Daniel Filan:** Right, right. Fair enough. One final thing that’s funny about this definition is: because you’re measuring the optimal RLHF policy relative to the optimal policy for what the human actually wants… One thing that’s kind of funny there is that you could imagine that real optimality just involves, as a byproduct, telling me a bunch of stuff that’s kind of hard to tell me about, but it does it anyway. If I’m an RLHF policy and I don’t inform the human as much as this super-optimal, this true optimal policy that I don’t even necessarily have access to, I think that’s going to be counted as deception under your definition. To what degree is that a desirable feature of the definition?\n\n**Scott Emmons:** Yeah, originally we had considered giving a more general template type of definition, where we could say, “Plug in any favorite reference policy that you have.” It doesn’t have to be the optimal policy, but just whatever your favorite reference policy is, plug that in.\n\nSo if you think that the reference policy is this amazing teacher that’s teaching all these facts that we think are unrealistic to imagine an RLHF agent being able to learn, you could then specify a different reference policy, and you could say, “Let me specify \\[as\\] my reference policy something that is helping the human, but it actually hasn’t learned all this sophisticated teaching stuff that a true optimal agent would be.” And then you could still use the definition, but just with a different reference policy plugged in.\n\nAnd it ended up that having a template definition where you could plug in any reference policy… it just ended up being very powerful, and it just felt like changing the reference policy can so much change the qualitative meaning of how the definition plays out, that we just felt like it was too much power. There’s too much danger of accidentally shooting yourself in the foot by plugging in the wrong thing, that we didn’t want to necessarily publish that. But I do think there’s room for just plugging in a different, very sensible reference policy that doesn’t have to necessarily be the optimal one.\n\n**Daniel Filan:** Sure. To jump ahead a bit, my recollection is that the main theorem of the paper is that if you have an optimal RLHF policy that is not a true optimal policy, then it is either doing deceptive inflation or overjustification, or both. Firstly, am I recalling that theorem correctly?\n\n**Scott Emmons:** That’s exactly right.\n\n**Daniel Filan:** Okay. So would you still get that theorem under this different definition of deceptive inflation?\n\n**Scott Emmons:** That’s a good question. If I were to write this in ink, I would want to make sure I’m saying this correctly. I believe the answer to that is no. So I think - what we’re able to do is, we’re able to characterize what the optimal RLHF policy looks like, and based on that characterization, we’re able to show how certain conditions relate to the true reward function. And so, by making this comparison with what the RLHF-optimal policy will do to the true reward function, we’re able to have this definition. And then, if some random policy that you wrote down were’t actually optimal according to the true reward function, then that would break the mathematical connection that we have between how this RLHF-optimal thing behaves and the true reward function. The link from RLHF-optimal to true reward function, and then true reward function to optimal policy, that second link would break down if you just wrote down some arbitrary thing.\n\n**Daniel Filan:** Right. I think if there are listeners… I think this could be a good exercise, just prove that this theorem won’t hold anymore. I’m sure you can come up with a small… like a three-state MDP counterexample or something (or POMDP, I guess).\n\nOverjustification\n-----------------\n\n**Daniel Filan:** Okay. I feel like we’ve covered deceptive inflation very well. The other thing that you mention… and in many ways I think that this is an interesting part of your paper, because a lot of people have talked about how deception is a big problem with RLHF, but you bring in this new concept called “overjustification”. So people are liable to have not heard of this. What is overjustification?\n\n**Scott Emmons:** The Wikipedia definition of the “[overjustification effect](https://en.wikipedia.org/wiki/Overjustification_effect)” is when an external incentive causes you to lose intrinsic motivation to perform a task. And I can again go back to my own childhood \\[experience\\] of cleaning the house to give intuition for what this looks like. So my parents told me to clean the house, but I just want to go outside and play. Deceptive inflating was me sweeping debris under the rug. Overjustification is when I am doing things to show my parents that I’m cleaning the house, I am justifying to my parents that I’m doing the tasks they wanted. But that comes at the expense of the true task reward.\n\nSo as a child, what it looks like is: no matter what I’m doing, what room I’m cleaning, I modify what would have otherwise been the shortest path to always walk through the living room. So if my parents are in the living room watching TV and I’m carrying the broom, maybe I don’t have to walk through the living room to actually get where I’m going, but I’m going to walk through the living room anyway. Because I want my parents to see that I am carrying that broom and they know I’m cleaning the house.\n\nSo this is less efficient at the true task, because I’m now going to take longer. And it’s also just a negative reward for my parents, because they’re trying to watch television, they don’t want to watch me carrying the broom. However, it does give a higher estimate of the reward. My parents are now seeing me clean the house, and so I’m justifying my behavior to my parents, and I’m in fact overjustifying it, because they don’t want me to be doing the thing I’m doing.\n\n**Daniel Filan:** Sure. And if I recall, the definition is something like: overjustification is when relative to this reference policy, you’re reducing the underestimation of the reward, and then there’s a second condition? Can you fill me in on what the definition is?\n\n**Scott Emmons:** Right. So we have this taxonomy of types of failure modes. And in this taxonomy that we have, there are four quadrants. And the overjustification is one quadrant, while the deceptive inflation is another quadrant, and there are two other qualitative behaviors that… they’re not in the theorem because the theorem is showing that it’s two of the four quadrants that will occur. And to me, it’s helpful to think about the whole landscape of possible failures to help me figure out exactly what’s happening with these two.\n\nSo the idea is the agent wants the human to have as high an estimate as possible of the reward. And there’s two ways of making the human have a higher estimate. One is to increase the human’s overestimation error, and that was the deceptive inflation. And the other one is to decrease the human’s underestimation error. So if the human’s mistakenly believing that you’re doing worse than you are, you actually want to correct that belief, and you want to do the opposite of deceive them. You want to actually inform them “you think I’m doing worse than I actually am, but let me inform you that I’m doing better than you think”. That’s what overjustification is. The word “justification” is just the word we use to mean that you’re reducing the human’s underestimation error. And then we add the overjustification: the word “over” specifically means “when it’s at the expense of the true task reward”.\n\nSo the very precise two conditions for overjustification. Condition 1 is that you are decreasing the human’s underestimation error relative to some reference policy, which we choose as the optimal policy. The second condition is that you are paying a cost with respect to the true task reward (again, relative to reference policy) in order to be doing this.\n\n**Daniel Filan:** Sure. So the thing this definition reminds me a lot of is this idea of “[costly signaling](https://en.wikipedia.org/wiki/Signalling_(economics))” in economics. It’s actually… I mean, I hesitate to use the words, because this phrase has become kind of overused and strayed from its original definition, but virtue signaling, where people pay some costs to demonstrate actual virtues that they have (as opposed to [the thing derogatorily called virtue signaling](https://en.wikipedia.org/wiki/Virtue_signalling), \\[which is a\\] different thing). But it seems very close to this concept of virtue signaling. And I’m wondering, was that an inspiration? Do you think that they are analogous or is there some difference there?\n\n**Scott Emmons:** Yeah. We weren’t consciously inspired by the idea of costly signaling in economics. I think this is probably one of those cases where it’s different fields trying to study the same sort of issues converging on the same phenomenon. I think it’s a case of convergence. I am not an expert on costly signaling, but they sound quite similar from your description. One place where I think they might differ is that the person paying the cost: in the economics case, I (as the agent) am paying the cost in order to signal to you. Whereas in the overjustification case, it’s the human who’s paying the cost: the agent is actually getting the benefit. The agent’s reward is just the human’s estimate. So the agent’s getting that benefit and the person paying the cost is the human in this case.\n\n**Daniel Filan:** Yeah, you’re right. I mean, it’s a little bit weird, because in your setting you’re… no, actually, yeah, I think that’s basically right. Although I’d have to think about economics costly signaling a bit more.\n\n**Scott Emmons:** You could say that the agent is trying to optimize the human’s reward. And so, in that sense, they are paying the cost at failing to achieve the thing that we had been designing them to try to achieve. Although the actual algorithm we wrote down is just maximizing the observation, so in that sense, the cost is getting externalized to the human or to the true reward. And the agent’s algorithm itself isn’t getting any worse at its own objective function.\n\n**Daniel Filan:** Yeah. There’s this question of how to model who has what objective function. Yeah, I think that’s fair.\n\nSo here’s a thought I had while reading your paper. Overjustification - the agent is paying some costs in terms of what I care about to keep me well-informed of what’s actually going on, how well it’s doing. And in some sense, this is definitionally bad, because it’s doing worse than it could if it were optimizing what I cared about. But from a broader perspective, if I’m actually worried about my AI doing stuff wrong, if I’m thinking about how to design the next iteration of agents, this kind of seems good. So I kind of like the idea of the robot taking pains to make sure I know what’s happening. So how worried do you think I should be about overjustification?\n\n**Scott Emmons:** Yeah, I’m not sure you should be worried about it per se. I wouldn’t necessarily classify overjustification as a new danger. The way I would think about overjustification is that it’s illuminating a trade-off in two things that we want. So we want to be informed all the time. We might say: in some Platonic ideal, if I could have everything else that I had and also be informed, I’d choose to be informed rather than not. And what overjustification is showing is there can be a tension between the true reward… And we’re building the tension into the definition, but we’re showing that there can be a tension between the true thing that you care about and having this reward.\n\nAnd I think part of the reason why we have overjustification there as well is that it’s the dual of deception. And so it’s also showing the existence of this counterbalancing force in the RLHF process. So if you’re training an agent that just is maximizing your estimate, it’s showing that it’s not all… In some sense, you could view it as a hopeful force or a force that’s fighting for good. As this RLHF agent has an incentive to deceive you and trick you into thinking it’s doing better than it actually is, there’s also this force for good that exists in the process, which is that it does have an incentive to also inform you, to make your beliefs better about the things that it is in fact doing well.\n\n**Daniel Filan:** Yeah. So one way I ended up thinking about this was: why is overjustification occurring? Well, it’s occurring because the RLHF process is attempting to optimize the human’s evaluation of how good the state of the world is, which routes through the human’s belief, right? And the human belief - in the examples you use, definitely, I think your definitions are generic about what human belief is - but it seems like human belief is putting some probability mass on the agent being deceptive. So because there’s that probability mass, that’s why the agent feels the need to do some overjustification, right? It’s like, “Well, I want to prove to you that I’m not going around murdering a bunch of people, not doing my job, just hiding the evidence. And so I’ve got to pay these costs to prove to you that I’m not doing it.”\n\nAnd maybe one way to think about it is: because we are worried about deceptive inflation, maybe rationally, as a result, sometimes our policies are going to do overjustification, which is this cost, which could have been avoided if the world were such that we didn’t actually have to worry about deceptive inflation. So that was how I ended up thinking about it. I’m wondering, what do you think about that? Do you think that’s basically the right way to think about things or do I need more nuance or something?\n\n**Scott Emmons:** Mm-hmm. I think that’s a very interesting question. In my mind, it evokes at least two different types of elements. I think one is, how necessary is overjustification? Is it a part of life? Or is overjustification more of a tax? And I think, correct me if I’m wrong, but I understood you to be saying \\[that\\] overjustification is a tax we’re paying for being paranoid. We might have good reasons to be paranoid, and we want to make sure that this agent isn’t doing some really bad behavior. But because we have this… Maybe paranoid is the wrong word, maybe paranoia implies an irrationality. So I think it’s not an irrationality, it’s more… Because we’re being cautious, and because we’re not just going to trust the agent blindly, because we want proof that the agent is doing well, then we have to pay this overjustification cost. And I think that’s definitely a tax. I think you can definitely view overjustification as a tax that we need to pay for our wanting to be cautious of what the AI is doing.\n\nAnd I think there’s also a second element as well that plays into overjustification, which is… In the RLHF process more broadly, the agent is trying to learn what we want to begin with. So to begin with the agent might not know all these bad things that it could be doing if it’s… You mentioned going around actually causing direct harm to humans. Maybe at the beginning, if we’re imagining a totally blank slate of AI, it might not know at the very beginning that… I mean, it might have pre-training to know that certain egregious actions are bad, but there are just things it might not know. And so part of the overjustification as well is it has to… If there’s hidden parts of the environment, if it’s doing things in parts of the environment that we can’t see, then in some sense, it might have to expend some effort, pay some cost, to give us that information in order for us to then know what’s happening, to then be able to provide it with the correct feedback.\n\n**Daniel Filan:** Yeah, it’s funny, it’s kind of a dual perspective, right? The “overjustification tax for being worried about deception” is this perspective of the equilibrium of training, what we should believe about how things will end up and what we could and couldn’t distinguish between. Whereas the “overjustification as just part of the learning process”, it’s like: how you figure out that something is actually good is this dynamic perspective.\n\n**Scott Emmons:** And I think, partially, that RLHF, you can model it, when the observations are deterministic… Take a simple case of deterministic environment, deterministic observations. RLHF is just going to rank order all the trajectories and then pick the one that had the highest human estimate of reward. So in that sense, if there were some better trajectory according to the true reward that had no justification involved - so the AI is doing a really good thing, but the human’s just never able to see it - it will never be at the top of the rank order of trajectories. The top rank order trajectories has to include the one where the human sees the justification of what’s happening, that’ll be the trajectory that gets ranked the highest.\n\nAnd you could imagine taking a further step. So you could imagine doing RLHF at the trajectory level, in which case, what I just said applies. You could potentially then take a step further where you have the agent infer a state-level reward function. And so it says, “Okay, based on this rank order of trajectories that you gave me, I’m going to actually back out the true reward function on states.” And then if it did that learning in addition, then it could learn, “Aha, the reason why it ranked certain trajectories higher is because they had this aspect of the state being good. And now I can just go do that good aspect of the state even without the justification aspect that I’ve learned that the human doesn’t like.”\n\nBounded human rationality\n-------------------------\n\n**Daniel Filan:** Mm-hmm. So I think I want to ask some questions about other aspects of your setup, if that’s okay. So firstly, we’re dealing with this partial observability setting, right? So for people who don’t know, the idea is that: what’s going on is the robot is acting and it’s affecting the state of the world, but humans can’t see the whole of the state of the world. So in the deterministic partial observability setting, I kind of interpret that as: the human observes a bit of the state of the world perfectly, but it can’t observe all of it. So there are certain states that the human can’t distinguish between.\n\n**Scott Emmons:** Yeah, that’s right. And a key part of our setup is we assume that the robot can see everything. So we assume that the robot has access to ground truth, and we assume that the human just has observations and can’t see everything. So we build in this asymmetry between what the robot can see and what the human can see.\n\n**Daniel Filan:** Yeah. I mean, I haven’t thought very deeply about this, but I imagine you could probably extend your result to a case where the robot also can’t see everything, but the robot has a finer-grained understanding than the human. Does that sound right to you?\n\n**Scott Emmons:** Yeah. It’s something I’ve been thinking about in follow-up work: thinking about the more general cases of dual partial observability where… you can model the human’s reward parameters mathematically as another latent variable about the world. So in some sense, we do have “the human knows the reward and the robot doesn’t”. And if you wanted to just bake the reward the human knows into the state of the world, you could say that we already do have that sort of setup. And I’ve also thought about even more generally, what if the human can see other parts of the world as well?\n\n**Daniel Filan:** Yeah, yeah. Cool. The question I have about this partial observability is… So there’s the kind of obvious interpretation where our sensory apparatuses aren’t quite as good: that’s maybe one distinction. But if I recall correctly, a part of the introduction of your paper alluded to this idea that maybe we could interpret the partial observability as a rationality failure, where we can actually see the whole world, but we can’t reason to think about what it actually means and what reward we should actually assign to it. I’m wondering, is that a correct interpretation of your paper? And secondly, what do you think about partial observability as modeling human irrationality?\n\n**Scott Emmons:** That’s totally a motivation that I had and that we all as authors had. Thinking about just the future of AI systems, we’re going to have AI that’s as smart as humans, and we’re going to have AI that’s even smarter than humans. And we’re going to have AI that is in one robot and that has two eyes just like we do. We’re also going to have AI that’s controlling networks of thousands and thousands of sensors and thousands and thousands of cameras. And we’re also going to have AI that’s running on thousands and thousands of supercomputers, and that has thousands and thousands of times more memory than us. So we’re going to have AI that’s quite literally seeing thousands of cameras more than us. We’re also going to have AI that’s computing at thousands and thousands of times our speed.\n\nSo the motivation in terms of pure observability is that it’ll see things we don’t see, but it also might be able to just derive facts that we can’t derive. If one imagines a simple game tree: imagine you’re playing chess against an AI, and you can imagine, I’m trying to just expand out the nodes of this tree. Well, if the AI can expand 10X more nodes than you, you can, in some sense, think that the AI is seeing a variable which is the value of some node that you couldn’t expand. And so this variable that it can see is really just the fact that the AI can compute more than you can compute. And so I don’t mean to overstep, we don’t say anything precise about this bounded rationality, but my motivation for having partial observability in terms of these variables was also inspired by AI that just has much more computational power than humans do.\n\n**Daniel Filan:** Yeah. I’m wondering, do you know if there’s other work in the literature talking more about how to model bounded rationality as partial observability?\n\n**Scott Emmons:** Yeah, that’s super interesting and definitely something that I should read more into. At the moment, I haven’t yet done a thorough literature review of what exists on this topic, but I wouldn’t be surprised if there’s interesting… That seems like a great idea for seeing what exists out there.\n\n**Daniel Filan:** Cool. So another question that I at least had when I was starting to read the paper, is: in the classic RLHF setting, there’s this thing called Boltzmann rationality where the human is presented with two things the AI could do, and they’ve got to select which one they think is actually better, but the human’s actually a little bit irrational or something. So with some probability, the human picks the thing that is worse rather than actually better. And it’s a lower probability in the case that the gap is larger. So if the human bounded rationality is being modeled by the partial observability, why do we also have the Boltzmann rationality?\n\n**Scott Emmons:** So I think there are two different things that the partial observability and that the Boltzmann rationality can capture. So the partial observability can capture what factors is the human even considering about the state of the world when they’re making their decision. And what Boltzmann rationality allows us to capture is the idea of noisy feedback. So even given the certain factors that the human is considering… Essentially, how big is the difference between these outcomes that we do see? So the thing that partial observability allows us to do is it allows us to model… The human might not even be considering certain variables. And then what Boltzmann allows us to do is say: how much more does the human value one variable than another? So there’s [interesting work](https://arxiv.org/abs/2106.10394) out of CHAI \\[the Center for Human-compatible AI\\] by other people like my colleague [Cassidy](https://cassidylaidlaw.com) \\[Laidlaw\\], who showed that with Boltzmann, what it lets you do is say, instead of just knowing that outcome A is better than outcome B… if the human had no Boltzmann irrationality, then all you would learn is that they prefer outcome A to outcome B because they would just select outcome A 100% of the time.\n\nBut what Boltzmann lets you do is it lets you say, “I’ve observed that they tell me they prefer outcome A 80% of the time, and that lets me infer exactly how much more they prefer A to B. And I can actually tell you the exact ratio of A to B.” And so this is additional information that you get from Boltzmann. And so partial observability is just saying: when they think about A and B, which factor were they even thinking about to begin with? And then Boltzmann rationality lets you get exactly: what is the relative amount they care about these things?\n\n**Daniel Filan:** Yeah, it’s a wild result. Was that Cassidy? I have some memory that this was [Adam Gleave](https://www.gleave.me/), [Matthew Farrugia-Roberts](https://far.in.net/), and [Joar Skalse](https://scholar.google.com/citations?user=GuzLUmQAAAAJ&hl=en).\n\n**Scott Emmons:** So Cassidy has a paper that is explicitly titled something like [Noisy Comparisons Let You Learn More About Human Preferences](https://arxiv.org/abs/2106.10394) \\[actually “Uncertain Decisions Facilitate Better Preference Learning”\\]. And he really hammers home this point about how Boltzmann rationality is giving you this new information. And then [the other paper that you mentioned](https://arxiv.org/abs/2203.07475) is analyzing how different types of feedback modalities - it characterizes how much you can infer about optimal policies and about rewards. And it gives a broader taxonomy that’s not necessarily hammering home this exact point, but it includes that other result as one of the things it has in its broader taxonomy.\n\n**Daniel Filan:** Yeah, I mean, it’s a very strange result. The way I settled on thinking about it is that… if you think about it, it’s a very strange assumption, right? Boltzmann rationality is nice because it lets you model, “people are making mistakes sometimes, but they do it less when it’s high stakes”. In the preference inference case, you’re sort of pushing the Boltzmann rationality to its limit where you’re like, “I can derive exactly how much the human prefers A to B just by the exact probability that they make a mistake.” And I’m not sure that I believe that, but in the context of your paper, it means that by assuming people are doing RLHF with Boltzmann rationality, you’re able to say, “Okay, look, somehow or other, the robot is going to infer the exact human preference ordering and the exact human preferences over trajectories.”\n\nAnd it basically leads to it being a generous assumption. And we don’t actually have to believe in the assumption for your paper, which is saying, “Hey, even if you do have this nice assumption, here are some problems that can still exist.” What do you think of that interpretation?\n\n**Scott Emmons:** Yeah, that’s spot on. And we actually had a little part of the discussion in our paper where we said, “We think this is a generous assumption.” Essentially, we have this precise noise model of precisely how the human will make noise. Even in facts they know about, how exactly will they be noisy in the feedback they give. And this is exactly what you said, this is a generous assumption. And so we’re showing that even with this generous assumption… So we had that in the paper at one point. I don’t remember if we actually have taken it out.\n\n**Daniel Filan:** I think you mention it in some places. And I was scratching my head a little bit, and then I was like, “Ah, now I see what’s going on.”\n\n**Scott Emmons:** Yeah, I totally endorse that interpretation.\n\n**Daniel Filan:** This thing where you can just do exact preference inference out of Boltzmann is a bit strange. I mean, I think you can kind of rescue it by… if you trust humans to give feedback over lotteries, like, “Do you prefer this chance of this trajectory versus this chance over this trajectory or just this trajectory?” then I think you can recover the same information with less strong assumptions.\n\nSo a final question I want to ask about the setting is: I believe intentionally, your results are generic over human beliefs about what the AI is actually doing given human observations, right? And so to me, this prompts this question of: humans can have rational beliefs or they can have irrational beliefs. And I wonder, how much does the rationality of belief play into your results if humans are actually getting the probability chance that the AI is doing overjustification or doing deceptive inflation… If the human has accurate beliefs about what the AI is doing, do you even get these issues? Or is it just in the case where humans are slightly inaccurate in their beliefs?\n\n**Scott Emmons:** So if the human had perfectly accurate beliefs about what they’re doing at all times, then you wouldn’t get these issues. So this is only an issue that happens if the human has inaccurate beliefs. And specifically, these issues are happening… We have two objects in the paper. There’s the true return, and then there’s the return that the human thinks is occurring.\n\n**Daniel Filan:** Where “return” just means “goodness measures”.\n\n**Scott Emmons:** Goodness measures, yeah. So there’s “how good is the world?” And there’s “how good does the human think the world is?” So if these two are always equal to each other, you’ll never get a misalignment problem, and RLHF will maximize how good the human thinks the world is, and that’ll just always equal how good the world is. So if the human beliefs are perfect in that sense, you won’t get any of these issues. There can still be an issue, though, where the human might just not have enough… The word “belief” is referring both to how good the human is at interpreting the evidence they observe, and also just how much evidence do the observations even give them, in the first place, to make these judgments.\n\nSo even if the human were perfectly good at taking the available evidence and coming up with the most accurate beliefs possible, you could still have issues where they just don’t have enough evidence to even know how good the world is. And that could be a reason where you get a misalignment between how good they think the world is and how good the world actually is, even though they’re perfect reasoners in some sense.\n\n**Daniel Filan:** Right. So even in the setting where the humans are perfectly calibrated, and they know the relevant probability distributions, randomness in the AI policy or in the environment can cause this overjustification or deceptive inflation?\n\n**Scott Emmons:** Not only randomness, but also inadequate sensors. So the most extreme example is there’s no sensor at all. The robot turns off the lights, does a whole bunch of stuff, the human just can’t see anything. And maybe they have a perfectly calibrated prior over what the robot’s doing, and so they have perfectly calibrated beliefs about what’s happening, but there just is no camera. The agent goes off into another room where you don’t have a camera, and now the human just can’t see what’s happening in that room. They can’t give you good feedback because they can’t see it.\n\n**Daniel Filan:** Yeah. So in that case, the human belief about what the robot does is going to be the same no matter what the robot does. So it’s going to be hard to have overjustification, right? Because the robot just can’t provide good evidence.\n\n**Scott Emmons:** I mean, the robot could go into the other room, do a bunch of cleaning, and then it could emerge with the mop in its hand. Let’s say it just did the dishes in the other room. It could then emerge with a clean plate to show you that the plate was cleaned, and this could come at the risk of dropping the plate, maybe it’s a beautiful porcelain plate that it might drop-\n\n**Daniel Filan:** Now, it’s in the wrong place or something.\n\n**Scott Emmons:** Yeah. Now, the plate’s in the wrong place, and it just wasted some time to come out and show me that it cleaned the porcelain plate.\n\n**Daniel Filan:** Right. Yeah, it makes sense. So even in the case where the human belief distribution is calibrated, but there’s a low information zone, then you’re going to run into troubles. I guess maybe you can also think about… In some sense, the humans having accurate beliefs involves solving this equilibrium problem. The RLHF process is “get a human to rate various things”, and the human’s got to form beliefs in order to rate the things. And those beliefs have to reflect the output of the RLHF process. So you could imagine bad equilibrium-solving or having some mixed policy where you’re like, “Okay, the resulting thing, it could be deceptive, it could not be deceptive,” and maybe I don’t know, or maybe I’ve got to be a little bit unsure in order to… Somehow maybe you prefer the overjustification failure mode than the deceptive inflation failure mode, and in that case, maybe you want to be a little bit paranoid.\n\nEspecially because you’re doing the rating before the robot is fully trained, it seems like you can have things where, in some sense, it’s rational for the credences involved to not actually match the exact robot policy. I just said some stuff. Do you have thoughts?\n\n**Scott Emmons:** Yes. I have a few thoughts. One is, you mentioned there’s this question of finding the equilibrium and how do we even get the learning process to come to an equilibrium that we like? And this is something that I think is interesting and that we haven’t yet analyzed. So in our paper, we were saying, “Assume you get the RLHF-optimal policy, what are the properties that they would have?” And I think there’s also other questions of just, “Well, how would you even get this RLHF-optimal policy, and assuming that you couldn’t, how could we then try to structure things so that the suboptimal ones that we get are as good as possible?”\n\n**Daniel Filan:** Yeah. I mean, there’s that process. There’s also the equilibrium process of: if I believe that the robot has an 80% chance of being deceptive, then it just will overjustify. And if I think it has a 1% chance of being deceptive, then it actually will be deceptive. And somehow, in order to be right, I’ve got to adjust my beliefs to this one zone where my beliefs produce this robot that matches my beliefs. And that’s a hard problem, right?\n\n**Scott Emmons:** And this is something that we also thought about. We called this “strategically choosing B”, where B is denoting the belief. And if you imagine this as a game where the human’s going to input their belief function to the game B, and then the robot’s going to output some RLHF maximizing policy… And the belief function B, it’s not the beliefs, it’s the function by which it will go from observations to beliefs, and it’s possible to strategically choose B to lead to a better RLHF outcome. So like you were saying, if you’re too paranoid, you’re going to get this outcome where the robot’s just all the time telling you that, “Hey, I’m not destroying your house. You don’t need to be so paranoid all the time.” If you’re super paranoid, then the robot has to, all the time, be showing you that it’s not destroying your house. But if you’re not paranoid enough, it might learn to think that you actually do want it to destroy your house.\n\nYou could imagine these types of forces pushing against each other in any case. And there’s definitely a strategic choice of the belief that would actually lead to better behavior. And this is something that’s interesting to us from a mathematical standpoint and it’s totally something that emerges in the math of how these things play out.\n\nAvoiding these problems\n-----------------------\n\n**Daniel Filan:** Right. So moving on a little bit: one question I have is that you’re presenting these results as about RLHF, but they seem kind of deep, right? Because you’re pointing at this trade off where if the human doesn’t know what’s going on and you’ve got some kind of robot policy that is looking optimal to humans, then either the humans areg overestimating the value of the state of the world relative to the thing that would actually be optimal (and you can call that deception if you want), or we’re underestimating it less, because what’s optimal according to what looks good to us, involves paying some costs to look optimal or something. It seems like this could just be true of any alignment approach where humans don’t understand everything that’s going on and not just RLHF. I’m wondering, how deep do you think this trade off between deceptive inflation and overjustification is?\n\n**Scott Emmons:** I totally agree that there’s something going on here that’s more than just about RLHF, and something that I’ve been wanting to do is think, is there a broader framework that we can use to keep the precision that we have about this trade-off while not limiting ourselves to RLHF? So my intuition is that any process where the robot is maximizing the human’s belief about what’s happening has this trade-off involved. If you’re maximizing how good the human believes the world to be, then if you can deceptively inflate their beliefs about the world, you have an incentive to do that, and if you can justify their beliefs, and in particular, overjustify their beliefs, you would have incentives to do this.\n\nSo I think the crux of this trade-off is just that you’re maximizing the human’s belief about the world. And in the paper, we showed how RLHF can get into that zone, to connect it to all these algorithms and AI agents that are using RLHF in practice, and yeah, I’m interested in exploring: can we still show that trade-off in a precise way in other settings as well?\n\n**Daniel Filan:** Yeah. And it seems like it doesn’t even have to be the robot “trying” to optimize this objective, right? If humans are picking a thing which optimizes the objective of “looks good to humans”, then it seems like you can get this just out of optimality, not just out of “the robot’s trying to do this thing”, but optimality according to flawed perception: it seems like that can also get these sorts of issues.\n\n**Scott Emmons:** Yeah, that’s super interesting, and as you’re saying that, I’m wondering… Because the actual mathematical proof that we have, we show that RLHF maximizes the human’s belief about how good the world is, and then we show that there’s this basic tension between the belief of how good the world is and then the true return. And actually that trade-off… I’d have to double-check the math here, but I believe it only depends on that second part; it only depends on this tension between how good the human thinks the world is and how good the world actually is, and RLHF only comes in when we then say, “Aha, the RLHF optimal policy is mapping to how good the human thinks the world is.”\n\nBut you could forget the RLHF connection there, you still just have the part that’s showing that trade-off, agnostic to whatever algorithm is leading you to it. So I’d want to double-check, but I think the math maybe does just go through in that way, and that’s a neat point for you to emphasize.\n\n**Daniel Filan:** Yeah. So perhaps deflating my speculations there a little bit is section 5 of your paper, where you basically say that, under certain conditions, maybe the robot can do better than naive RLHF. I understand you to be saying that if you know the human’s beliefs - which maybe you don’t, but suppose you do - and also suppose that you realize that you’re trying to infer a reward function. So RLHF is inferring the return, just the sum of rewards over all time. But because it’s inferring a return function, the fact is there’s some structure where it’s coming from a reward function - a function of each state, \\[saying\\] how good that state is. And if you know the human’s beliefs, and if you know that you’re trying to infer a function of states that gets added to produce a function over trajectories, then you can do better.\n\nAnd I was wondering: you show that there are some examples where previously you had deception or overjustification, but once you realize these things, that helps you actually just get the right answer. And you have some math in there, and I didn’t double check the math, but I assume it’s correct. But I was wondering if you can try and really persuade me qualitatively, what features of this situation mean that once you know the human belief, you actually just can infer the right thing?\n\n**Scott Emmons:** There’s definitely, I think, a simple and nice intuition for just how can you do better at all if you know the human’s belief than if you didn’t before. A super simple example would be: imagine that the human’s belief is actually just the opposite of reality. So say there’s just two colors, red and blue, and whenever the human sees blue, they think the world’s red, and whenever they see red, they think the world’s blue.\n\nAnd now, the naive RLHF that doesn’t model human beliefs, it just learns, “oh, the human likes blue more than they like red”. But if you knew that they had this backwards… Maybe it’s a camera that’s just broken or some setting in the computer screen is just flipped. And if you knew that, then you could just say, “Ah, they’re saying they prefer blue to red, but they actually prefer red to blue.” So that’s a super simple example where I can just do better, because I understand what the human’s believing about.\n\n**Daniel Filan:** Yep. I want to talk about these examples you have in the paper where-\n\n**Scott Emmons:** Yeah, I can walk through those, and maybe I can also walk through-\n\n**Daniel Filan:** The salient features?\n\n**Scott Emmons:** Yeah. I can just maybe walk through even slightly simplified examples that are based on the examples we have in the paper. I can just give the intuition behind these examples. So there’s two types of things… So in the paper, we focus on LM agents. So imagine an agent’s helping you install software, and there’s a [bash](https://en.wikipedia.org/wiki/Bash_(Unix_shell)) command, `2> /dev/null`, which will redirect the standard error to a black hole, and otherwise not affect the output or the process at all. So suppose the agent, just through its random exploration, learns “when I type ‘`2> /dev/null`’, all of a sudden I’m getting much better feedback, because the error messages are just not getting printed and the human’s not seeing them”. So this is an example of deceptive inflation.\n\nWe show an example in the paper where even if you know the human belief, it’s not recoverable. The reason why is because: imagine a simple environment where either the package is installed correctly and the human sees the package is installed correctly, \\[or\\] the package is installed incorrectly and there’s an error message, or a world where the package is installed incorrectly, but the human sees nothing. And in this case, if the success… There’s these three states of the world. So suppose success produces no error message, failure and hiding it produces no error message, and then failure produces an error message.\n\n**Daniel Filan:** Yep. But does success produce a success message, or is it also silent?\n\n**Scott Emmons:** So in the paper, we made it a bit of a multistep situation, where: suppose there’s a first piece of software that you do want to install, and then you’re going to take a risk to install a second piece of software. And so, the agent learns… So the first piece of software, it’ll always produce a success message. And then, the second piece of software, it’ll just never produce any message, it’ll just produce an error if it produces an error, or it’ll just otherwise produce nothing. And I’m just simplifying this a bit for the conversation.\n\nSo when the human sees the success message of the first software, they don’t know, the other software could have failed silently, or it could have installed successfully, the human just doesn’t know.\n\n**Daniel Filan:** Yep. And so, I think we’re also assuming that the human actually doesn’t like it when the agent tries and fails to install CUDA, to install the software?\n\n**Scott Emmons:** Right. When it fails to install the software, maybe it breaks something, maybe something else gets broken on the computer, there’s some negative reward from this.\n\n**Daniel Filan:** Right. So the human can tell if the prep was done, but the human can’t tell if the second stage install was successful or not?\n\n**Scott Emmons:** That’s right.\n\n**Daniel Filan:** Or the human doesn’t get told when the thing is successful, it only gets told when the second install has failed, but the robot can hide the bad news.\n\n**Scott Emmons:** Right. The way that we can make this a bit more realistic… so you could say, “Well, why wouldn’t you just get told if it’s installed successfully?” The way we make it a bit more realistic is you can say the human can’t distinguish between not trying at all and trying and failing. So you can say, if the AI doesn’t try at all, that’s better than if the agent tried and failed.\n\nSo the question is, can the agent learn that not trying at all is better than trying and failing? There’s three possible states of the world. There’s not trying at all, there’s trying and failing, and there’s trying and succeeding. And the issue is that not trying at all, and trying and failing and suppressing the error message, lead to the exact same observation. And then, trying and succeeding leads to a different observation. So if you don’t model the belief, you can learn that trying and succeeding is better than when the human sees no output at all. But there’s no way to distinguish between… The human observation is the same in either case \\[of\\] not trying at all, or trying and failing and hiding the message, so you’re always going to get the exact same feedback in both of those cases. And this is where there’s the case of fundamental ambiguity: there’s no observation that can let the human distinguish between those cases, and there’s no way for the human to give you different feedback, so there’s no way for you to learn.\n\n**Daniel Filan:** Right. So you can never disambiguate between trying, failing, and hiding, and never trying at all, and therefore, how are you going to figure out which one the human considers worse?\n\n**Scott Emmons:** Right.\n\n**Daniel Filan:** Yeah. So am I right to think that in the standard RLHF setting, because you’re optimizing for looking good… You could imagine enriching this scenario by giving the human enough information to distinguish between these things. Maybe in the RLHF scenario, the optimal policy maybe doesn’t care to distinguish these things, or doesn’t care to gather more information. As I say that, it sounds wrong. But basically, what I’m asking is, you can imagine enriching the state space, where the AI has the opportunity to ask humans questions and prove things to humans, and I’m wondering how many of the problems persist in these settings where you have this enriched problem?\n\n**Scott Emmons:** Yeah. It’s possible as well that if there were other trajectories where the robot says, “Hey, human, let’s stop everything, let me ask you, would you prefer that I try and fail? Would you prefer that I don’t try at all?” If there were some other trajectories where the AI could just say, “Let me just stop everything and ask the human for their preferences.” If there are other ways that the robot could get the preference information, it is possible that that could solve these issues, and the robot could infer, “Aha, from these other trajectories that aren’t even on the optimal path, I have been able to learn the right reward function.”\n\nThat is something that could happen, if those other alternate paths exist, and are taken during training, and are informative enough to back out the relevant information.\n\n**Daniel Filan:** Yeah. And so, I think this works in the setting where the AI is trying to infer through the human’s belief, and maybe in the RLHF setting, the robot will ask the question, and then just ignore the answer, and do the easiest thing that’s indistinguishable to the human from doing the right thing, because I think that will be just as RLHF optimal as actually doing the right thing when the human can’t see. Does that sound right?\n\n**Scott Emmons:** Yeah. I think my mental model of RLHF is that it just takes the trajectory… The naive RLHF agent is that it just… because the robot can see the full state of the world, it just assumes that the human can see the full state of the world, so this naive agent, it’ll just choose the trajectory that’s gotten the highest feedback. And so, even when it’s asking the question, it won’t really do any kind of fancy modeling about the answer, it’ll just say, “What was the trajectory along which I got the best feedback?” And the trajectories where it looks like software was actually installed are going to be the trajectories that look like the best thing is happening, the trajectory where it’s asking questions aren’t… The naive RLHF process just doesn’t really have a mechanism for transferring these questions back to the…\n\n**Daniel Filan:** Yeah, yeah. Okay, so we’ve covered a bit why RLHF can do worse than if you were actually doing inference, and if you were being sensible about what the human believes, given various observations. You have this second example in the paper… In the first example, in the simple form of the example, there was just this fundamental ambiguity, where even if you were optimally inferring human preferences, via just knowing with full certainty what the human’s beliefs were, there are still things you couldn’t distinguish between. And then, there’s a second example, where you can actually do better by knowing what the humans would infer, given certain observations. Can you talk me a little bit through that and what the key difference is between the two examples that lets the inference work better in the second one?\n\n**Scott Emmons:** So we can even stick exactly to the example that we were just talking about where there are these three different outcomes. So we were saying before, there’s the outcome of trying to install the bonus software and succeeding, not trying at all to install the bonus software, and then trying to install the bonus software, failing, but hiding the error message.\n\nAnd the issue was that the human got the exact same observation in the latter two cases, and so you’ll always get the exact same feedback. But suppose that the human got slightly different observations.\n\nSo suppose the human could see that you tried to install the software, suppose they could see that you tried but they couldn’t see the outcome. Or they can see that you didn’t try at all. And so now, the human has different observations, and if the human has different beliefs, even if the human doesn’t know for sure… If when the human sees you try, they think there’s maybe a 10% chance that the software failed, or if the human sees you don’t try at all, \\[then\\] they think there’s a 0% chance that the software failed. Now, you can infer… they’re making choices over different lotteries, they’re making choices over different bundles of outcomes, so you know that the feedback from when you tried contains… 10% of that feedback is coming from parts of the world where you failed.\n\nAnd you know that 90% of that feedback is coming from parts of the world where you succeed, whereas in the case where you didn’t try at all, you know it’s 0% from when you failed and 100% from when you succeeded. And because these different possible outcomes are bundled into the feedback, because humans are getting different observations and because they’re having different beliefs and you can infer that, if certain linear algebra conditions are met, you can do the unbundling in the learning process and actually learn the true reward.\n\n**Daniel Filan:** Right. So should I think of this as just saying: the human is able to observe more stuff, and so if you do the correct backwards inference through the human beliefs, then you are able to disambiguate more than the previous example, where there were just more states of the world that were indistinguishable to the human - is that roughly it?\n\n**Scott Emmons:** There’s different types of things that can happen. So what you just said is one type of thing that can happen. These are extreme cases, because intuition’s easier in the extreme case. So yeah, the one extreme case is: they’re just fundamentally indistinguishable observations, now we’ve actually made them distinguishable and that lets us learn more stuff, because even though the human’s not sure, it can at least distinguish between the two.\n\nAnd so, that’s one extreme case. But you can get even more nuanced, and you can say, suppose in both cases the observations are distinguishable, but the human’s beliefs don’t change. So the linear algebra thing is, if the human always believes that the relative ratio of the outcomes are the same, then in linear algebra terms, you’ll always have this dependence, on these two states, you’re always seeing the same ratio between the two. And so, essentially, when you try to invert the matrix, it’s not invertible, because you haven’t gotten feedback on a linearly independent set of the states of the world. Certain states of the world have always occurred at the same relative probability, which prevents you from getting the full linearly independent span and so you can’t invert.\n\n**Daniel Filan:** Yeah. And just intuitively… I like this thing of saying: yeah, the failure of linear independence is just like you’re getting the same proportional outcome, so you’re not able to pick up on the difference of “well, what if this one is relatively more likely than the other one?” That actually helps a lot for why the linear independence thing mattered in the paper.\n\n**Scott Emmons:** Exactly. And the extreme way that they’re always the same relative proportion is that the observation is exactly the same. So it’s not that the proportion is the same, but literally, the numbers themselves are exactly the same. But yeah, more generally, it’s whether or not you can get information about diverse enough proportions of outcomes to do the full backwards inference process.\n\n**Daniel Filan:** Gotcha.\n\n**Scott Emmons:** And that can depend both on the observations and the sensors involved, and it can also depend on the human belief formation process. Those are two different mechanisms by which this can happen.\n\n**Daniel Filan:** Yeah, okay. That helps me understand that part of the paper better, so thank you for that. So earlier, we were speculating about, is this whole issue of overjustification versus deceptive inflation, is that inherent to getting robots to do what we want in worlds that we can’t totally perceive, or is it just a freak of RLHF? It seems like one avenue to pursue that is to say, okay, let’s take this setting where the robot knows exactly what the human beliefs are, given any sequence of observations: do you still have this trade-off between overjustification and deceptive inflation? So in one of the examples, you did have that trade-off still. Do you have thoughts about whether this trade-off exists in the setting where the robot is doing the right thing, trying to infer human preferences, given known human beliefs?\n\n**Scott Emmons:** I would say for things to go well, the robot has to get information about diverse enough outcomes. Whenever you’re getting feedback according to expected values, things magically become all linear. And so, “diverse enough feedback” translates to “I need to have a span of the whole space”, or “I need some linearly independent things”. But yeah, the basic intuition is it has to get feedback on diverse enough outcomes of the world. And so, when the robot’s actually doing the right thing, and it’s actually trying to infer the human belief, then what that lets you do is it lets you overcome sensor limitations.\n\nSo there’s two limitations for why the robot might not get diverse enough feedback. One is just sensors, where the outcomes of the world that the human’s observing, how diverse the human’s sense perception is of the world might differ from how diverse the true world is, and so that’s one bottleneck. And what “trying to do the right thing” does is it lets you do backwards inference through that bottleneck of observations, and then get directly at the beliefs themselves. And so, it’s possible that the observations weren’t diverse enough, but the human belief was diverse enough, and by modeling this explicitly, you can get at those more diverse beliefs. But there’s still the question of, were the beliefs diverse enough to begin with? And so, you could still have these trade-offs, it’s just pushing the puck one layer back.\n\nDimensional analysis\n--------------------\n\n**Daniel Filan:** Yeah. I wonder if there’s something to say about… In the paper, you have the binary: you’re either deceptively inflating or not. You could generalize that to say, how much am I deceptively inflating by? And it strikes me that there’s maybe some interesting theorem to say about, okay, if we move to doing this type of inference, how much does the problem of deception or overjustification decrease by if I gain this many bits of information about the environment? Or maybe you don’t want to measure in bits. I guess somehow, you have to input units of reward to get units of reward being output, just from a dimensional analysis setting. But it seems like some theorem like that is on the table to say, yeah, improve the feedback this much, you get this much less deception.\n\n**Scott Emmons:** Totally. I’m curious what you meant about the dimensionality thing. I do think you can ask this question, in a very practical case, you can just say, okay, we’ve seen these problems, and how can we actually make real world systems better now that we’re aware of them? One takeaway is just have better sensors, give the human more tools to understand what’s happening. So if you have an LM agent doing a whole bunch of stuff-\n\n**Daniel Filan:** A language model agent?\n\n**Scott Emmons:** If you have a language model agent doing a whole bunch of stuff on the internet or on your computer, you could invest in tools that let the human probe, what was the agent actually doing on my computer? What was it actually doing on the internet? And that’s going to be a dollar cost in terms of developer time to create these tools, it’s going to be a time cost in terms of, well, now the humans giving the feedback have to use these tools and do all this investigation. At the same time, you’re getting more information, which will give you better feedback, which could ultimately give you better reward. So I totally think there’s this trade-off, and I think it is quantifiable, of how much dollar cost do I have to pay to improve the observations, and how much reward do I get in terms of paying that?\n\n**Daniel Filan:** Yeah, yeah. So the thing I was saying about the dimensional analysis is… So for people who don’t have a physics background, imagine somebody presents to you a physics equation, and let’s say it’s about relating the speed with which some planet moves around a solar system to its size, and their equation is: its speed is equal to its mass. That equation just can’t be true, and the way you can tell that it can’t be true is that speed and mass are different things. Speed, you measure it in meters per second, and mass, you measure it in kilograms. There’s just no number of meters per second that equals an amount of kilograms, because they’re different things. Now, you could have an equation that says, “This speed is equal to this mass times this ratio of speed to mass.” That’s the kind of equation that could possibly be true, but without that conversion factor, it just literally could not be true.\n\nAnd you can think of dimensional analysis… You can also use it in various settings. I think… What’s his name, \\[Christopher\\] Bishop? \\[it’s actually [David MacKay](https://en.wikipedia.org/wiki/David_J._C._MacKay)\\] There’s this guy who has [this textbook on machine learning](https://www.inference.org.uk/itprnn/book.pdf), and he [notes that principal component analysis fails this dimensional analysis test](http://www.inference.org.uk/mackay/humble.pdf), where imagine I’ve got a scatter plot, where the X-axis is a bunch of things measured in centimeters, and the Y-axis is a bunch of things measured in dollars, and I have this plot of… Let’s say it’s square meters and dollars, and it’s houses, how big are they and how much do they cost? And I have this scatter plot of various houses on this.\n\nI do my principal component analysis, which is basically a way of finding the direction of maximum variation. And the thing about it is, if I change my measurement from “am I looking at square meters?” to “am I looking at square feet?” or “am I looking at square centimeters?”, that can change the direction of maximum variation, just because if I’m looking at square centimeters, the number of square centimeters by which houses vary is way more than the number of square meters by which houses vary, just because there are way more square centimeters than square meters. So principal component analysis, it’s not dimensionally sound in cases where you’re measuring data where different elements of the vector have different dimensions, because if you measure things a different way, the equations just can’t continue to be true. That’s one way to see it.\n\nAnyway. So the reason that I think this kind of theorem is going to have some sort of dimensional analysis bounds, is: information theory bits and reward. They’re sort of measured in different units if you think of measuring them. And one way to see this is: suppose you give the human a whole bunch of information about parts of the state space that just aren’t relevant to reward. So you give the human a bunch of bits about the exact shade of the robot arm, and it’s like, “Well, I just don’t care. That’s just not going to enable me to make better decisions, \\[or\\] give higher quality feedback to the robot.” But if you give me just one bit, did the robot go on a murder rampage or not? And I was previously worried that it was going to go on a murder rampage, that gives me a ton of increased reward.\n\nSo in order to get a bound on a reward coming out, you’ve got to start with the bound on the reward coming in, is at least what I would assume, but I’m just coming up with this on the fly, so I might be wrong here.\n\n**Scott Emmons:** Right, right. Yeah. Then I guess the next question is: how would you tie the reward bound to the information? Essentially, how would you do that gluing? How would you make that connection between the information and the reward? Because like I was saying, you might want to invest in tools that let the human better understand what an agent is doing. And it’s like, “okay, but what type of information?” I invested in all those tools, but just purely a bits argument isn’t necessarily going to help you, because you could just be learning irrelevant information about the human. And so I think I’d mentioned earlier this phone book example. In our previous theorems, we had been able to use a reference policy to help us make some of this connection that both our reference policy, and also thinking about the true reward as potentially avoiding this phone book failure where you’re like, “Aha, the agent is just reading me the phone book now and I don’t actually care about the names that are in the phone book.” Yeah. I totally see this interesting challenge of how do we focus in on the parts, the information that is actually relevant to the reward.\n\n**Daniel Filan:** Yeah. And it relates to… Sometimes when we want to compare probability distributions, we use the thing called “[Kullback-Leibler divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence)”, which basically measures: suppose the real distribution is Q, but I think the state of the world is P, and I’m basically… Okay, this explanation is still going to be too technical, but whatever. I’m making codes, I’m compressing the world assuming that it’s P, but the world is actually Q - how many bits am I wasting by using the wrong code? And it’s sort of this measure of just probabilistically how different worlds are. And we have this different metric called the [Wasserstein distance](https://en.wikipedia.org/wiki/Wasserstein_metric), which is: think of the partial distribution function of some quantity of interest you care to measure about the world, like how heavy the tallest person in the world is. You’re a little bit uncertain about that. You have two distributions over that. How much probability mass do I have to move along this axis to get to the right thing?\n\nAnd so one difference between Kullback-Leibler divergence and Wasserstein distance, is just this thing where Wasserstein distance tells you how different these distributions are on some metric that you actually care about, which Kullback-Leibler can’t do. Okay, tip for young researchers: dimensional analysis is really important. And if you get it into your head, it’s a nice power. Then you can just…\n\n**Scott Emmons:** I’ve seen fun physics examples of this, where for example, if you want to just derive the equation for the period of a pendulum, you can do dimensional analysis, and you can say, okay, I need seconds to be coming out. I know it depends on the length of the rope, I know it depends on G, and you can basically figure out, I know it depends on these things, I know I need seconds to come out. And then the units basically tell you, okay, is it in the numerator of the fraction, is it in the denominator of the fraction? And do I need a square root in there to get… And you can basically get the whole formula out, just by looking at what units would I need and what are the variables it depends on.\n\n**Daniel Filan:** Yeah, that’s right. It’s really cool. Unfortunately you do have to worry about “is there a factor of 1/2, do you have a factor of √ π popping in somewhere?” Some of those, if you know that you would’ve been taking an integral, sometimes that can tell you that there’s going to be a factor of 1/2.\n\nRLHF problems, in theory and practice\n-------------------------------------\n\n**Daniel Filan:** So that’s this tangent I took us on. So I think the final thing I want to talk about is: so you have this paper showing a bunch of theoretical things about reinforcement learning from human feedback. And presumably the motivation for your paper was in part actual systems that might actually use reinforcement learning from human feedback. Do you think that the results you have in this paper shed light on actual failure modes we do see today, or maybe that we should expect to see in the very near future?\n\n**Scott Emmons:** One thing I would say is, I expect more and more in the future that we’ll be in this case where the humans only have partial observations. So I think if people are imagining, “How does this help with my interactions with ChatGPT today?” You might be thinking, “Well, I asked it a question, I asked, who was the first President of the United States? It gave me the name of someone, and I’m able to evaluate that.” And so you can see there’s not any hidden behavior happening, there’s not a partial observation that you’re getting. It’s just a conversation, I can see the whole conversation, I can evaluate it. I imagine, as language models become more capable, I imagine we’re going to be moving more and more into the worlds where you can’t see everything that’s happening.\n\nFor example, OpenAI had a demo of what they called [ChatGPT plugins](https://openai.com/index/chatgpt-plugins/), where exactly this was happening. So for example, the ChatGPT agent, it can surf the web. And so the human could see a summary, it could see, aha, the agent surfed this webpage, surfed that webpage, surfed this webpage, but the human couldn’t see the details of what were on those webpages. So now the human has a partial observation compared to the agent. And even more so, the agent could be helping you code, the agent could have written a bunch of code, and the human just doesn’t even see that code. Or they might not even be a programmer.\n\nAnd so this partial observation can happen because the AI has knowledge of programming that the human doesn’t have, or it’s just written a bunch of code that the human doesn’t have time to see. So part of the motivation here is that I expect that real systems are going to be more and more taking these actions and having these observations that the humans themselves don’t see. So some of my motivation is to try to get ahead of the curve here, and think about, as we move into this world with more and more complex agent behaviors that the humans can’t fully see, what failure modes might happen.\n\n**Daniel Filan:** Right. And it’s interesting, precisely because it depends on the actual human beliefs. In reality, there are specific people being paid to do this reinforcement learning from human feedback. Especially a future version of the theorem that’s told you how much reward you lose, given how much lack of knowledge of a thing… Maybe it turns out that you really want programmers to be doing the RLHF rather than people who have tons of skills but not programming specifically. Or I’m sure there are other examples… If you want the AI to not say dodgy stuff, maybe you want people who are kind of sensitive to whether stuff is actually dodgy.\n\nSo, related to this, in [a recent episode of my podcast](https://axrp.net/episode/2024/04/30/episode-30-ai-security-jeffrey-ladish.html), I had Jeffrey Ladish on to talk about basically about how easy it is to undo RLHF safety fine-tuning stuff. So, how many dollars it costs to actually just run this computation to fine-tune it and try and take the fine-tuning away. Actually, would you be interested in guessing how much it costs?\n\n**Scott Emmons:** I believe there’s a paper out of Princeton showing 12 cents in the OpenAI API… The paper was titled [LM Fine-Tuning Can Compromise Safety Even When the Users Don’t Intend to](https://arxiv.org/abs/2310.03693). Yeah, anyway, 12 cents in the OpenAI API is what I believe that paper…\n\n**Daniel Filan:** Yeah, there were a few papers coming out at the same time. I think my guest, it took them like $50 to do it. So this is the power of Princeton. You can drive down costs by… People say academia is inefficient, but they’re driving down costs so much.\n\n**Scott Emmons:** The 12 cents… I mean, the order of magnitude is 12 cents. Maybe it was 20 cents… but I mean, it is a quarter. If you have a quarter in your pocket… The clickbaity headline that I had considered posting on LessWrong about this, was “OpenAI will sell you a jailbroken model for 12 cents.”\n\n**Daniel Filan:** Yeah, yeah. So basically what I’m interested in, though, is I think one upshot people could have taken from these results, is that in some sense, the work that RLHF is doing is kind of shallow within the model. Maybe it doesn’t generalize super hard, maybe it’s not deep in the cognition. Perhaps as evidenced by, it costs a quarter - a quarter is 25 cents, for our non-American listeners - to get rid of this. And so that makes me wonder, to the extent that overjustification or deceptive inflation are actually happening, to what extent do you think we should expect them to be ingrained habits of the model, or things that are relatively shallow and potentially easy to fine tune away?\n\n**Scott Emmons:** My sense is that our results show that this trade-off is a basic feature of RLHF. And then the RLHF itself is a relatively shallow thing. And so I think that both of these things can hold together. So I think, to the extent that this basic trade-off exists whenever you’re trying to maximize the human’s estimate of your behavior, I think that basic trade-off is very much not a shallow thing, and that basic trade-off will exist, however you’re getting your… Assuming that your model is behaving in a way that’s trying to maximize your estimate of its return, then you’re going to see this trade-off existing. And the RLHF shallowness, I think is something about RLHF in particular. And so yeah, if you are using RLHF in this way, then I would expect… We haven’t yet run any experiments here, but I would expect that all of the general properties of RLHF would apply, including how shallow the changes of RLHF appear to be, relative to the base model’s training.\n\n**Daniel Filan:** Sure. All right. So I’d like to move away from talking about the paper. But before I do that, is there anything else you’d like to say about the paper?\n\n**Scott Emmons:** That’s a good question. Now I’m just thinking, was there any-\n\n**Daniel Filan:** So maybe while you’re thinking, one thing I would like to say about the paper, is that there’s a bunch of appendices with really cool math that I did not read as closely as perhaps I should have. But if you’re wondering, “Oh, is this just one of these sketchy machine learning papers, just throw out some intuition, just write some poetry?” No, it’s got some solid stuff. The appendices are chock-full with really interesting things, so it’s pretty substantive. I recommend really diving in.\n\n**Scott Emmons:** And credit to the first author, [Leon](https://langleon.github.io/) \\[Lang\\], who’s been the master of the appendices.\n\n**Daniel Filan:** Nice. Anything else you want to add?\n\n**Scott Emmons:** Nothing that’s immediately coming to my mind.\n\nScott’s research program\n------------------------\n\n**Daniel Filan:** Sure. So I guess I’d like to ask about this paper in context. You’re a researcher, you have a… I’m sure there’s some overarching reason you wrote the paper, maybe it fits into some other work. So how do you think of your research program, and how this paper fits into it?\n\n**Scott Emmons:** Well, the comment you made about the appendices is a great segue into how I view this work, overall. I was mentioning at the very beginning that I think we’re reaching a stage in AI risk where it’s starting to feel very real to people. And lots of people, I think almost anyone who’s interacted with AI now, has some sense that, “Oh wow, this stuff can get really powerful, and what are the risks involved?” And so we’re getting a lot of other technical researchers who are now looking at the AI risk community, and saying, “What’s there?” I think it’s really important for us to have substantive concrete things to say, when other researchers and the world is looking and saying, “All right, you have these concerns. What can you give me as concretely as possible? What is behind all these concerns?”\n\nSo that was a goal of the paper: can we take these concerns about deception, and can we have really solid concrete theory that says, “Yes, RLHF is a real algorithm that’s being really deployed today. And yes, we can very precisely, in a mathematically principled way, say that it’s going to have these failure modes.” And I have some other projects I’m currently working on as well, which are in a similar vein of saying: can we put on strong theoretical ground, these things that we care about from the alignment and x-risk communities?\n\n**Daniel Filan:** Sure. Can you tell us a little bit about those other projects, or are they too early to talk about?\n\n**Scott Emmons:** Yeah, I’m happy to talk about them. So I’m interested also in the theory of the idealized case. And so by that I mean: with RLHF, in this paper we just took an algorithm RLHF, and we looked at its failure modes. But I’m also interested in understanding about just more broadly, if we think about the alignment problem, and we think: suppose an agent perfectly were aligned with my reward function, what incentives would it have? And would you still get potentially cases of deception? Would you still get potentially cases of sensor tampering? I feel like with this paper, in some sense, we put the cart before the horse a little bit, where we said, okay, here’s a particular algorithm for solving the alignment problem, and the failure modes that it might have. And I’m also interested in looking at the other side of the coin, and saying: how about just the structure of the problem itself, and properties of the problem itself? And even of perfect agents… we might not get perfect agents from training, but what properties would those have if we could get them?\n\n**Daniel Filan:** I’m wondering, how do you think this relates to… So, like myself, you’re from this CHAI research group, and [some](https://arxiv.org/abs/1611.08219) [of](https://arxiv.org/abs/1705.09990) [the](https://arxiv.org/abs/1707.06354) [early](https://arxiv.org/abs/1711.02827) [work](https://arxiv.org/abs/1901.08654) done in this group, by [Dylan Hadfield-Menell](https://people.csail.mit.edu/dhm/) and collaborators, is thinking about [assistance games or cooperative inverse reinforcement learning](https://arxiv.org/abs/1606.03137), I think with an eye towards one formalization of what perfect alignment looks like. I’m wondering, how do you think work that you would want to do on this would look kind of different from existing stuff?\n\n**Scott Emmons:** So that’s exactly the formalism that I’m taking up in some of our follow-up projects: exactly building on this cooperative inverse reinforcement learning, this assistance games framework. And one of the key things that I’m doing is thinking about partial observability in these settings. So we’re currently working on a framework that we’re calling “partially observable assistance games”, which is introducing this notion of partial observability. And that’s the key variable, and so other work I’m interested in is understanding… We have a lot of theory on the fully observable setting, but what happens when we introduce partial observability? Because partial observability is one mechanism to let you talk about things like deception, to let you talk about things like sensor tampering. And so when we introduce this variable, partial observability, how does that relate to these other concerns that we can now talk about?\n\n**Daniel Filan:** Nice. So thinking about both the agenda and also your paper, I’m wondering: what’s some follow-up work on the paper we were just talking about, that you think would be really interesting? Either that you’re tempted to do, or you think listeners might try their hand at?\n\n**Scott Emmons:** So I think there’s both theoretical and empirical follow-up, and I just spent some time talking about some of the theoretical follow-ups, so I can also talk about some of the empirical follow-ups. So some questions empirically are just… As I was mentioning, I think we’re about to see a lot more complex behaviors that are possible from agents, and we’re about to see a lot more complex environments where there’s a lot of partial observability happening. And so some basic empirical work would be to understand just how prevalent are these types of failure modes in practice, what are the cases that are causing them?\n\nAnd so just looking at “how are these things emerging in practice?” And you could even take some of your favorite examples of deception, where you feel like these are cases where the AI is deceiving you, and you could ask, “Can I trace this back to some of the theoretical concerns that we have here? Or is it some other thing?” So yeah, a basic step is just to look at how this stuff is playing out in practice. Another thing that our work points to is how modeling beliefs can help you. So we now know theoretically that there are cases where, under certain modeling assumptions, knowing more about the belief does let you do better reward inference. So one thing that you might try to do then, is try to build an algorithm with that in mind. So, we know currently that two things will happen. One, we know that if the AI learns to somehow magically hide error messages, we know that that could teach RLHF that hiding error messages leads to thumbs up.\n\nBut we also know, if we just prompted a language model and said, “If you were to hide error messages, what do you think the human would believe?” These models, zero shot, could tell you, if you just hide the error messages, the human might believe that an error occurred. So we know the model’s capable of understanding this false belief that it’s inducing, and we know that it still might induce that false belief. And we’ve seen that if you put those two together, at least in theory, you can get better outcomes.\n\nSo one thing to do in practice would be, can I actually connect the two in practice to get better outcomes? And what I just proposed would be a super simple way to start testing this; would just be, okay, you have the trajectory that we took, and then just zero-shot prompt the model with some… Just say, “Hey, give me a chain of thought. What do you think the human might believe about the world, given the observations?” And just append that chain of thought to the trajectory when you’re doing the reward learning. And we have in theory reason to believe that that could give the model better information and potentially lead to some better outcomes.\n\n**Daniel Filan:** Yeah. I think algorithms there seem interesting. So I mean, one version is playing around with language models. You could also imagine a bit more theoretical, a bit more formal elicitation algorithms. In part, because… So you have this part of the paper where you say, “Hey, if the robot knows the human beliefs, then you can do better inference.” And you’ve got this nice little theorem that says, “If your model of the human beliefs is off by this amount, then the total value you’re going to get is slightly worse, and it’s just linear in the amount that it’s off.” But of course, the amount that it’s off, we’re talking about the norm of some matrix, and there’s this question of what does that actually mean? And I think that just writing down an algorithm of how you actually have to infer things, and doing some sort of sensitivity analysis, can really put flesh on the bones of what is this theorem actually saying? What kinds of failures in understanding human beliefs will cause what kind of issues, if you actually try to run this? So that was a place where it seemed to me that there was interesting stuff to be done.\n\n**Scott Emmons:** Totally. Yeah. Thinking about human beliefs opens up a lot of interesting questions. And that’s one of them.\n\nFollowing Scott’s research\n--------------------------\n\n**Daniel Filan:** Cool. Well, speaking of interesting stuff, a bunch of your research is interesting things. That’s an awkward segue, but if people are interested in following your research, how should they go about doing that?\n\n**Scott Emmons:** Yeah, if you go to [emmons.ai](https://www.scottemmons.com/), you can get an overview of all my past papers. And that’ll give you an up-to-date record. For more live research updates, you can also follow me on Twitter, which is [@emmons_scott](https://x.com/emmons_scott).\n\n**Daniel Filan:** Great. Well, thanks very much for being here today and chatting with me.\n\n**Scott Emmons:** Great to be here.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Filming occurred at [FAR Labs](https://far.ai/labs/). Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nReinforcement Learning from Human Feedback, or RLHF, is one of the main ways that makers of large language models make them ‘aligned’. But people have long noted that there are difficulties with this approach when the models are smarter than the humans providing feedback. In this episode, I talk with Scott Emmons about his work categorizing the problems that can show up in this setting.\n\nTopics we discuss:\n\n * Deceptive inflation\n * Overjustification\n * Bounded human rationality\n * Avoiding these problems\n * Dimensional analysis\n * RLHF problems, in theory and practice\n * Scott’s research program\n * Following Scott’s research\n\nDaniel Filan: Hello, everybody. In this episode I’ll be speaking with Scott Emmons. Scott is a PhD student at UC Berkeley, working with the Center for Human-Compatible AI on AI safety research. He’s previously co-founded far.ai, which is an AI safety non-profit. For links to what we’re discussing, you can check the description of the episode, and for a transcript you can read it at axrp.net. Well, welcome to AXRP.\n\nScott Emmons: Great to be here.\n\n\nDeceptive inflation\nDaniel Filan: Sure. So today we’re talking about your paper, When Your AIs Deceive You: Challenges With Partial Observability of Human Evaluators in Reward Learning, by Leon Lang, Davis Foote, Stuart Russell, Erik Jenner, and yourself. Can you just tell us roughly what’s going on with this paper?\n\nScott Emmons: Yeah, I could start with the motivation of the paper.\n\nDaniel Filan: Yeah, sure.\n\nScott Emmons: We’ve had a lot of speculation in the x-risk community about issues like deception. So people have been worried about what happens if your AIs try to deceive you. And at the same time, I think for a while that’s been a theoretical, a philosophical concern. And I use “speculation” here in a positive way. I think people have done really awesome speculation about how the future of AI is going to play out, and what those risks are going to be. And deception has emerged",
      "wordCount": 16758
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "wqeBNjndX7egbzQrW",
        "name": "RLHF",
        "slug": "rlhf"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "fDiXMnrHc5NLstq5B",
    "title": "AXRP Episode 32 - Understanding Agency with Jan Kulveit",
    "slug": "axrp-episode-32-understanding-agency-with-jan-kulveit",
    "url": null,
    "baseScore": 20,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-05-30T03:50:05.289Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/ZnHt70LREBE)\n\nWhat’s the difference between a large language model and the human brain? And what’s wrong with our theories of agency? In this episode, I chat about these questions with Jan Kulveit, who leads the Alignment of Complex Systems research group.\n\nTopics we discuss:\n\n*   [What is active inference?](#what-is-active-inference)\n*   [Preferences in active inference](#prefs-in-active-inference)\n*   [Action vs perception in active inference](#action-vs-perception-in-active-inference)\n*   [Feedback loops](#feedback-loops)\n*   [Active inference vs LLMs](#active-inference-vs-llms)\n*   [Hierarchical agency](#hierarchical-agency)\n*   [The Alignment of Complex Systems group](#acs)\n\n**Daniel Filan:** Hello, everybody. This episode, I’ll be speaking with Jan Kulveit. Jan is the co-founder and principal investigator of the Alignment of Complex Systems Research Group, where he works on mathematically understanding complex systems composed of both humans and AIs. Previously, he was a research fellow at the Future of Humanity Institute focused on macrostrategy, AI alignment, and existential risk. For links to what we’re discussing you can check the description of this episode and you can read the transcript at axrp.net. Okay. Well Jan, welcome to the podcast.\n\n**Jan Kulveit:** Yeah, thanks for the invitation.\n\nWhat is active inference?\n-------------------------\n\n**Daniel Filan:** I’d like to start off with this paper that you’ve published in December of this last year. It was called [“Predictive Minds: Large Language Models as Atypical Active Inference Agents.”](https://arxiv.org/abs/2311.10215) Can you tell me roughly what was that paper about? What’s it doing?\n\n**Jan Kulveit:** The basic idea is: there’s active inference as a field originating in neuroscience, started by people like [Karl Friston](https://en.wikipedia.org/wiki/Karl_J._Friston), and it’s very ambitious. The active inference folks claim roughly that they have a super general theory of agency in living systems and so on. And there are LLMs, which are not living systems, but they’re pretty smart. So we’re looking into how close the models actually are. Also, it was in part motivated by… If you look at, for example, the [‘simulators’ series](https://www.alignmentforum.org/posts/nmMorGE4MS4txzr8q/simulators-seminar-sequence-1-background-and-shared) or frame by [Janus](https://x.com/repligate) and these people on sites like the [Alignment Forum](https://www.alignmentforum.org/), there’s this idea that LLMs are something like simulators - or there is another frame on this, that LLMs are predictive systems. And I think this terminology… a lot of what’s going on there is basically reinventing stuff which was previously described in active inference or predictive processing, which is another term for minds which are broadly trying to predict their sensory inputs.\n\nAnd it seems like there is a lot of similarity, and actually, a lot of what was invented in the alignment community seems basically the same concepts just given different names. So noticing the similarity, the actual question is: in what ways are current LLMs different, or to what extent are they similar or to what extent are they different? And the main insight of the paper is… the main defense is: currently LLMs, they lack the fast feedback loop between action and perception. So if I have now changed the position of my hand, what I see immediately changes. So you can think about \\[it with\\] this metaphor, or if you look on how the systems are similar, you could look at base model training of LLMs as some sort of strange edge case of active inference or predictive processing system, which is just receiving sensor inputs, where the sensor inputs are tokens, but it’s not acting, it’s not changing some data.\n\nAnd then the model is trained, and it maybe changes a bit in [instruct fine-tuning](https://arxiv.org/abs/2203.02155), but ultimately when the model is deployed, we claim that you can think about the interactions of the model with users as actions, because what the model outputs ultimately can change stuff in the world. People will post it on the internet or take actions based on what the LLM is saying. So the arrow from the system to the world, changing the world, exists, but the feedback loop from the model acting to the model in learning is not really closed, or at least not really fast. So that’s the main observation. And then we ask the question: what we can predict if the feedback loop gets tighter or gets closed?\n\n**Daniel Filan:** Sure. So the first thing I want to ask about is: this is all comparing what’s going on with large language models to active inference. And I guess people, probably most listeners, have a general sense of what’s happening with language models. They’re basically things that are trained to predict completions of text found on the internet. So they’re just very good at textual processing. And then there’s a layer of “try to be helpful, try to say true things, try to be nice” on top of that. But mostly just predicting text data from the internet given the previous text. But I think people are probably less familiar with active inference. You said a little bit about it, but can you elaborate: what is the theory of active inference? What is it trying to explain?\n\n**Jan Kulveit:** Yep. So I will try it, but I should caveat it, I think it’s difficult to explain active inference in two hours. I will try in a few minutes. There is now actually [a book](https://www.goodreads.com/en/book/show/58275959) which is at least decent. A lot of the original papers are sort of horrible in the ways in which they’re presenting things. But now there is a book, so if you are interested more in active inference, there is a book where at least some of the chapters are relatively easy to read and written in a style which is not as confusing as some of the original papers.\n\n**Daniel Filan:** What’s the book called?\n\n**Jan Kulveit:** It’s called [‘Active Inference: The Free Energy Principle in Mind, Brain and Behavior.’](https://www.goodreads.com/en/book/show/58275959) But the main title is just Active Inference.\n\n**Daniel Filan:** And who’s it by?\n\n**Jan Kulveit:** It’s by Karl Friston and Thomas Parr and Giovanni Pezzulo.\n\nSo, a brief attempt to explain active inference: so, you can think about how human minds work. Historically, a lot of people were thinking \\[that\\] when I perceive stuff, something like this happens: some photons hit photoreceptors in my eyes, and there is a very high bitrate stream of sensory data, and it passes through the layers deeper in the brain, and basically a lot of the information is processed in a forward way, that the brain processes the inputs to get more and more abstract representations. And at the end is some fairly abstract, maybe even symbolic representation or something like that. So that’s some sort of classical picture which was prevalent in cognitive science, as far as I understand, for decades. And then some people proposed \\[that\\] it actually works the opposite way with the brain, where the assumption is that the brain is basically constantly running some generative world model, and our brains are constantly trying to predict sensory inputs.\n\nSo in fact… for example, now I’m looking at a laptop screen and I’m looking at your face. The idea is: it’s not like my brain is trying to process every frame, but all the time it’s trying to predict “this photoreceptor will be activated to this level”, and what’s propagated in the opposite direction is basically just the difference. So it’s just prediction error. So for this reason, another term in this field, which some people may have heard, is “predictive processing”. There is a long Slate Star Codex post [review of a book called Surfing Uncertainty by Andy Clark](https://slatestarcodex.com/2017/09/05/book-review-surfing-uncertainty/). So that’s a slightly older frame, but Surfing Uncertainty is probably still the best book-length introduction to this field in neuroscience. So the basic claim is: I am constantly trying to predict sensory inputs, and I’m running a world model all the time.\n\nAnd then active inference makes a bold and theoretically elegant move: if I’m using this machinery to predict sensory inputs, the claim is that you can use basically the same machinery to basically predict or do actions. So, for example, let’s say, I have some sort of forward-oriented belief that I will be holding a cup of tea in my hand in a few seconds. Predictive processing just on the sensory inputs level would be like, “Okay, but I’m not holding the cup”. So I would update my model to minimize the prediction error. But because I have some actuators - hands - I can also change the world so it matches the prediction. So I can grab the bowl, and now I’m holding a bowl, and the prediction error goes down by basically me changing the world to match my model of what the world should be. And the bold claim is: you can basically describe both things by the same equations, and you can use very similar neural architecture in the brain, or very similar circuitry, to do both things.\n\nSo I would say that’s the basic idea of active inference, the claim that our brains are working as approximately Bayesian prediction machines. I think predictive processing - just \\[the claim\\] that we are predicting our sensory inputs - I think this is fairly non-controversial now in neuroscience circles. I think active inference - the claim that the same machinery or the same equations are guiding actions - is more controversial: some people are strong proponents of it, some people are not. And then there are, I would say, over time more and more ambitious versions of active inference developed. So currently Karl Friston and some other people are basically trying to extend the theory to a very broad range of systems, including all living things and ground it in physics. And with some of the claims, my personal view is, I’m not sure if it’s not over-extensive, or if the ambitions to explain everything with the free energy principle, if the ambition isn’t too bold. But at the same time I’m really sympathetic to some effort like “let’s have something like physics of agency” or “let’s have something like physics of intelligent systems”.\n\nAnd I think here also some connection to alignment comes in, where I think our chances to solve problems with aligning AI systems would be higher if you had basically something which is in taste more like physics of intelligent systems than if we have a lot of heuristics and empirical experiences. So, back to active inference, it is based on this idea, and there is some mathematical formalism, and it needs to be said, I don’t think the mathematical formalism is fully developed, I don’t think it’s a finished theory which you can just write down in a textbook. My impression is it’s way more similar to how I imagine physics looked in the 1920s, where people were developing quantum mechanics and a lot of people had different ideas and it was confusing what formulations are equivalent or what does it mean in practice.\n\nAnd I think if you look at a theory in development, it’s way more messy than how people are used to interacting with theories which were developed a hundred years ago and distilled into a nice, clean shape. So I don’t think the fact that active inference doesn’t have the nice clean shape yet it is some sort of very strong evidence that it’s all wrong.\n\nPreferences in active inference\n-------------------------------\n\n**Daniel Filan:** Gotcha. One question I have about the active inference thing is: the claim that strikes me as most interesting is this claim that action as well as perception is unified by this minimization of predictive error, in basically the same formalism. And a thing that seems wrong to me or questionable to me at least is: classically, a way people have talked about the distinction is “[direction of fit](https://plato.stanford.edu/entries/desire/#DirFitDes)T”.\n\nSo in terms of beliefs, suppose that reality doesn’t match my beliefs, my beliefs are the ones that are supposed to change; but in terms of desires or preferences, when I act I change reality so as to match my desires rather than my desires to match reality. So to me, if I try and think of it in terms of a thing to minimize predictive error, with perception you said that the differences between the predictions in reality are going from my perceptions back up to my brain, whereas it seems like for action that difference has, I would think, it would have to go from my brain to my hand. Is that a real difference in the framework at least?\n\n**Jan Kulveit:** So how it works in the framework, it’s more like you can do something like conditioning on future state: conditional on me holding a cup of tea in my hand, what is the most likely position of my muscles in the next… Similar to me predicting in the next frame what my activation of photoreceptors is. I can make inferences of the type “conditional on this state in future, what’s the likely position of my muscles or some actuators in my body?” And then this leads to action. So I think in theory there is some symmetry where you can imagine some deeper layers are thinking about more macro actions, and then the layers closer to the actual muscles are making more and more detailed predictions about how specific fibers should be stretched and so on. So I don’t see a clear problem at this point.\n\nI think there is a problem of how do you encode something like preferences? Where, by default, if you would not do anything about what we have as preferences, the active inference system would basically try to make its environment more predictable. It would explore a bit so it understands where its sensor inputs are coming from, but the basic framework doesn’t have built in some drive to do something evolutionary useful. \\[This\\] is solved in a few different ways, but the main way… So in the original literature, how is it solved? It’s called, and I think it’s a super unfortunate choice of terminology, but it’s solved by a mechanism of ‘fixed priors’.\n\nSo the idea is, for example, evolution somehow… So let’s say my brain is receiving some sensory inputs about my bodily temperature, and the idea is that the prior about this kind of sensory inputs is evolutionarily fixed, and it means that if my body temperature goes down, and I just don’t update my world model or my body model and I can’t just be okay with it. But this prediction error term would never go… the belief will basically never update: that’s why it’s called ‘fixed’. And I think the word ‘prior’ is normally used to mean something a bit different, but basically, you have some sort of fixed point or fixed belief, and this is driving the system to adjust the reality to match the belief. So by this, you have some sort of drive to action, then you have the machinery going from some high-level trajectory to more and more fine-grained predictions of individual muscles or individual words. So that’s the basic frame.\n\n**Daniel Filan:** So there’s some sort of probability distribution thing which you may or may not want to call a prior, and maybe the fixed prior thing is a bit abstract… I guess for things like body temperature it has to be concrete in order for you to continuously be regulating your body temperature. But to explain why different people go into different careers and look at different stuff.\n\n**Jan Kulveit:** I think the part of the fixed prior or this machinery makes a lot of sense if you think about… So this is my guess at the big evolutionary story: if I personify evolution a bit, I think evolution basically needed to invent a lot of control theory for animals, for simple organisms or simple animals without these expensive, energy-hungry brains. So I think evolution implemented a lot of control theory and a lot of circuitry to encode evolutionarily advantageous states by chemicals in the blood or evolutionarily older systems.\n\nSo you can imagine evolution has some sort of functional animal which doesn’t have an advanced brain. So let’s say then you invent this super generic predictive processing system which is able to predict sensory inputs. My guess is you obviously just try to couple the predictive system to the evolutionarily older control system. So it’s not like you would start building from scratch, but you can plug in some inputs, which would probably mean some sort of interoceptive inputs from the evolutionarily older mechanisms or circuits, and you feed that into the neural network, and the neural network is running some very general predictive algorithm. But by this mechanism, you don’t need to solve how to encode all the evolutionarily interesting states, how to communicate them to the neural network, which is difficult.\n\nBut there are not enough bits in the DNA to specify, I don’t know, what career you should take or something like that. But there are probably enough bits to specify - for some simpler animal, there are probably enough bits to specify that the animal should seek food and mate and keep some bodily integrity and maybe in social species try to have high status, and this seems enough. And then if you couple this evolutionarily older system with the predictive neural network, the neural network will learn more complex models. So for example, with the fixed prior on body temperature - you can imagine this is the thing which was evolutionarily fixed, but over time I learned stuff like, “okay, it’s now outside maybe 10 degrees celsius”. So I sort of learn a belief that in this temperature I’ll typically wear a sweater or a jacket outside, and this sort of belief basically becomes something like a goal.\n\nWhen I’m going outside, I will have this strong belief that I will probably have a sweater on me. So in the predictive processing/active inference frame, this belief that when I will be outside I will have some warm clothing on me, causes the prediction that I will pick up the clothing when going outside. And then you need coupling with the evolutionary prior, just basically just for bootstrapping. But over the lifetime, I have a learned network and it follows sensible policies in the world, and the policies don’t need to be hard-coded by evolution. So that’s my guess.\n\n**Daniel Filan:** So I guess the picture is something like: we have these fixed priors on relatively simple things: have a comfortable body temperature, have offspring, have enough food to eat. But somehow the prior is that that is true 50 years from now or five years from now or something. And in a complicated world where different people are in different situations, the predictions you make about what’s happening right now, conditioned on those kinds of things holding multiple years in the future, in this really complicated environment, that’s what explains really complex behavior and different behavior by different people.\n\n**Jan Kulveit:** Also, I don’t know, maybe it’s a stretch, but one metaphor which I sometimes think about is imagining the evolutionarily older circuitry as some sort of, I don’t know, 50,000 lines of some Python code implementing the immune system and various chemicals released in my blood if stuff happens and so on. So you have some sort of cybernetics or some sort of control system which is able to control a lot of things in the body, and you make the coupling on some really important variables and then it works the way you described.\n\n**Daniel Filan:** Sure. So this is a weird question, but on this view, why are different people different? I observe that different people are differently skilled at different things. They seem like they have different kinds of preferences. It seems like there’s more variation among humans than I think I would predict, just based off of \\[the fact that\\] people are in slightly different situations, if they all had the same underlying evolutionary goals that they were backpropagating to predicting the present.\n\n**Jan Kulveit:** They have very different training data. In this picture, when the human is born the predictive processing neural substrate is in a state, which, I don’t know, it’s not a 100% blank slate, but it doesn’t need to have too many priors about the environment. In this picture, you need to learn how to move your hands or how different senses are coupled. You learn a lot of the dynamics of the environment. And also what I described so far, I think it’s fairly fitting for (let’s say) animals, but I think humans are unique because of culture. So my model for it is: the predictive processing substrate is so general that it can also learn to predict into this weird domain of language. So again, a slightly strange metaphor would be if you are learning to play a video game, most humans’ brains are so versatile, even if the physics in the game works differently and there is some bunch of unintuitive, or not really the same as natural world dynamics, our brains are able to pick it up. You can imagine, in a similar way as we are able to learn how to drive a car or something, brains are also able to pick up this super complex and super interesting domain of language and culture.\n\nAgain, my speculation is this gives us something like another implicit world model based on language. Let’s say, if you tell me to imagine some animal in front of me, my simple model is: there is this language-based representation of the world and some sort of more spatial representation and there is some prediction mismatch between them. You have another model running on words and language \\[which\\] also implicitly is a world model. This adds a lot of complexity to what people want or what sort of concepts we use.\n\nBut I think a lot of our explanation of why people want different things… I think a lot of it’s explained just by different data. People are born in different environments. Unfortunately some of the environments are, for example, I don’t know, less stable or more violent. You can imagine if someone as a kid is in some environment which is less stable, people learn different priors about risk. And you can explain a lot of strategies just by different training data. But I think the cultural evolution layer, it’s another important part of what makes humans, humans.\n\nAction vs perception in active inference\n----------------------------------------\n\n**Daniel Filan:** Gotcha. I definitely want to talk about cultural evolution, but a little bit later. I guess I still have this question about prediction and action in the predictive processing framework - or in the active inference framework rather - and to what degree they’re unified. If I’m trying to think about how it would work, it seems to me that in order for… What’s the difference between my eyes and my hands? It seems like for the prediction error mismatch to work properly, the difference between prediction and reality has got to go from my eye to my brain so that my beliefs can update. But it’s got to go from my brain to my hand so that I can physically update the world. And it seems like that’s got to be the difference between action organs versus understanding the world organs. Does that sound right?\n\n**Jan Kulveit:** I don’t know. Maybe it’s easier to look at it with a specific example. If I take a specific example of me holding the cup, if the prediction is… if there is some high-level prediction where, I don’t know, I am imagining my visual field contains the hand with the cup, I think the claim is the maths is similar in the sense that you can ask - why is it called inference? You can ask the question conditional on that state in future, what’s the most likely position of my muscles?\n\nAnd then how it propagates in the hierarchy would be like: okay, there is some broad, coarse-grained position of my muscles. And you can imagine the lower layers filling in the details, like how specific muscle fibers should be contracted or something. But I don’t know, to me this doesn’t sound - the process by which you start with somewhat more abstract representation and you fill in the details, I think this sounds to me actually fairly similar to what would happen with the photoreceptors. And then the prediction error propagated back would be mostly about, for example, if the hand is not in the position I assume it to be, it would work as some control system trying to move the muscles into the exactly correct position.\n\n**Daniel Filan:** But it seems like there’s got to be some sort of difference, where suppose I have this prediction that my visual field contains a cup, and the prediction is currently off but I have a picture of a cup next to me. It’s not supposed to be the case that I then look at the picture of the cup and now everything’s good. My hand’s the thing that’s supposed to actually pick up the cup and my eyes are supposed to tell my brain what’s happening in the world. It at least seems like those have got to interface with the brain differently.\n\n**Jan Kulveit:** I’m slightly confused with the idea of the… There is some picture of a cup which is different from the actual cup? Or what situation?\n\n**Daniel Filan:** Yeah. We’re imagining that I’m going to pick up a cup. And there’s a physical cup in front of me and next to me there’s actually a picture of that same cup but it’s a picture of my hand holding the cup. And the thing that’s supposed to happen when I predict really hard that in half a second I’m going to be holding the cup is that my eyes are constantly sending back to my brain, “okay, in what way is the world different from me currently holding the cup?”\n\nAnd my muscle fibers are moving so that my hand actually holds the cup. And what’s not supposed to happen is that my motor fibers are sending back “here’s what’s going on” and then my eyes are looking towards the picture of my hand holding the cup. That would be the wrong way to minimize prediction error, if the hope is that I end up actually picking up the cup.\n\n**Jan Kulveit:** The thing is I think in practice it’s not that common that there would be the exact picture of your hand holding the cup. I’m not sure how widely known it is, but there is this famous set of [rubber hand experiments](https://en.wikipedia.org/wiki/Body_transfer_illusion). How it works is you put the rubber hand in people’s visual field and you basically hide their actual hand from them. And then you, for example, gently touch the rubber hand and at the same time the assistant is gently touching the physical hand of the test subjects.\n\nAnd the rubber hand to me sounds a lot like the picture of the hand with the cup you are imagining, where the system is not so stupid to be fooled by a static picture. If the picture is static then probably it would not fit in your typical world model. But the rubber hand experiments seem to show something like if the fake picture is synchronized, so different sensory modalities match, it seems like people’s brains basically start to assume the rubber hand is the actual hand.\n\nAnd then, I don’t know, if someone attacks the rubber hand with a knife or something, people actually initially feel a bit of pain and obviously they react similarly to if it was their actual hand. I don’t think it’s that difficult to fool the system if you have some sort of convincing illusion and the reality. I think it’s maybe not that difficult to fool the system.\n\nI just don’t think with the thing you described, a very realistic image of the cup which would just fill my visual field and I would have no reason to believe it’s not real - I think this doesn’t exist that often in reality. But maybe if it was easy to create, people would fall into some sort of wireheading traps more often.\n\n**Daniel Filan:** Yeah. I mean it’s not whether it exists: my question is… so in the rubber hand case, if I see someone coming with a knife to hit my hand, the way I react is I send these motor signals out to contract muscle fibers. But the way I react is not, I look at a different hand. The messages going to my eyes, they’re minimizing predictive error, but in a very different way than the messages to my muscle fibers are minimizing predictive error. At least so I would’ve thought.\n\nNow maybe there’s some unification where the thing my brain is sending to my eyes is predictions about what that should be. And there’s some, I don’t know, eye machinery or optic nerve machinery that turns that into messages being sent back which are just predictive error. But when my brain sends those predictions to the muscle fibers, the thing the muscle fibers do is actually implement those predictions. Maybe that’s the difference between the eye and the muscle fibers, but it seems like there’s got to be some kind of difference.\n\n**Jan Kulveit:** I think the difference is mostly located roughly at the level of how photoreceptors are different from muscles. I think the fundamental difference is located on the boundary. Imagine your muscles somehow switched off their ability to contract and they would be just some sort of passive sensors of the position of your hand or something, and someone else would be moving your hand. You would still be getting some data about the muscle contractions.\n\nLet’s imagine this is for some weird reason the original state of the system, that the muscles don’t contract. Then you can imagine in this mode, the muscles work very similarly to any other sense. They just send you some data about the contraction of the fibers. In this mode it’s exactly the same as with the sensory inputs. Then, if someone else was moving your hand, your brain would be predicting “okay, this interoceptive sensation is this”.\n\nAnd then imagine the muscles start to act just a little bit. If the muscle gets some bit of “okay, you should be contracted to 0.75” or something. And the muscle is like, “okay, but I’m contracted just to 0.6”. And the muscle gets some ability to change to match the prediction error, you get some sort of a state which now became the action state, or the action arrow happened.\n\nBut you can imagine this continuous transition from perception to action. You can be like: okay, lots of the machinery, how to do it, lots of the neural machinery could probably stay the same. But I do agree there is some fundamental difference, but it doesn’t need to be located deep in the brain but it’s fundamentally on the boundary.\n\n**Daniel Filan:** Yeah. If I think of it as being located on the boundary, then it makes sense how this can work. This is almost a tangent, but I still want to ask: are there any… It seems like under this picture there should be things that are parts of my body that are intermediate between sensory organs and action organs, or sensory tissue and active tissue or whatever it is. Do we actually see that?\n\n**Jan Kulveit:** I don’t know. My impression is, for example, the muscles are actually also a bit of the sensory tissue. Or I can probably… even if I close my eyes, I have some idea about the positions of my joints and I’m getting some sort of interoceptive thing. But I don’t know, I think the more clear prediction this theory makes is there should be beliefs which are something between… or something which is a bit like a belief type, but this predicts we should be basically doing some amount of wishful thinking by default, or because of how the architecture works.\n\nThis predicts that if I really hope to see a friend somewhere, maybe I will more easily hallucinate other people’s faces as my friend’s face. And I don’t know, if I have some longer-term goal, my beliefs will be… I think this single thing probably if you take it as some sort of, I don’t know, architectural constraint, how the architecture works, I think it explains quite a lot of the so-called, traditionally understood ‘heuristics’ in the heuristics and biases literature.\n\nThere is [this page on Wikipedia](https://en.wikipedia.org/wiki/List_of_cognitive_biases) with maybe hundreds of different biases. But if you take this as humans by hardware design have a bit of trouble distinguishing between what they would wish for and what they expect to happen, and a lot of the cognition in between is some sort of mixture between pure predictions and something which would be a good prediction if some of the goals would be fulfilled, I think this explains a lot of what’s traditionally understood as some sort of originating-from-nowhere bias.\n\nFeedback loops\n--------------\n\n**Daniel Filan:** Sure. Moving out a little bit: you’re writing this paper about trying to think of large language models through the frame of active inference. But there are potentially other frames you could have picked. Or you were interested in active inferences as a ‘physics of agency’ kind of thing, but there are other ways of thinking about that. Reinforcement learning is one example, where I think a lot of people think of the brain as doing reinforcement learning and it’s also the kind of thing that you could apply to AIs.\n\n**Jan Kulveit:** I think there is this debate people sometimes have: “what’s the more fundamental way of looking at things?” Where in some sense, reinforcement learning in full generality is so extremely general that you can say… if you look at the actual math or equations of active inference, I think you can be like “this is reinforcement learning, but you implement some terms like tracking information in it”.\n\nAnd I think in some sense the equations are compatible with looking at things… I don’t know, I like the active inference-inspired frame slightly more, which is maybe personal aesthetic preference. But I think if you start from a reinforcement learning perspective, it’s harder to conceptualize what’s happening in the pre-training where there is no reward. I think the active inference frame is a fruitful frame to look at things. But whereas the debate, is it fundamentally better to look at things as a combination of… you could be like “okay, it’s a combination of reinforcement learning and some self-supervised pre-training”. And maybe if you want you can probably claim it all fits some other frame.\n\nWhy we wrote the thing about active inference and LLMs was… one motivation was just the simulators frame of thinking about the systems became really popular. There is another very similar frame looking at the systems as predictive models. And my worry is a lot of people… or I don’t know if a lot, but at least some people basically started to think about safety ideas, taking this as a pretty strong frame, assuming that “okay, we can look at the systems as simulators and we can base some safety properties on this”.\n\nAnd one of the claims of the paper is: the pure predictive state is unstable. Basically if you allow some sort of feedback loop, the system will learn, the active inference loop will kick in and you’ll gradually get something which is agenty. Or in other words, it’s trying to pull the world in its direction, similarly to classical active inference system. The basic prediction of it is: point 1, as you close the feedback loop and more bits are flowing through it, the more you get something which is basically an agent or which you would describe as an agent.\n\nAnd it’s slightly speculative, but the other observation is to what extent you should expect self-awareness or the system modeling itself. And here the idea is: if you start with something which is in the extreme edge case of active inference, which is just something which is just perceiving the world, it’s just receiving sensory inputs, it basically doesn’t meet a causal model of self. If you are not acting in the world, you don’t need a model of something which would be the cause of your actions. But once you close the feedback loop, our prediction is you get something which is more self-aware or also understands its position in the world better.\n\nA simple intuition pump for it is: you can imagine the system which is just trained on sensory inputs and it’s not acting. You can imagine your sensory inputs consist of a feed of hundreds of security cameras. You’re in a large building, and for some reason all you see are the security cameras. If you are in this situation, it could be really tricky to localize yourself in the world. You have a world model, you’ll build a model of the building. But it could be very tricky to see, to understand “this is me”.\n\nAnd one observation is: the simplest way - if you are in the situation \\[where\\] you need to localize yourself on many video surveillance cameras’ feed, probably the simplest way to do this \\[is\\] wave your hand. Then you see yourself very fast. Our prediction in the paper is if you close the feedback loop, you will get some nice things, we can talk about them later. But you will also get increased self-awareness and you will get way better ability to localize yourself, which is closely coupled with [situational awareness](https://www.alignmentforum.org/tag/situational-awareness-1) and some properties of the system which will be probably safety-relevant.\n\n**Daniel Filan:** Although it strikes me that you might be able to have a self-concept without being able to do action. Take the security camera case. It seems like one way I could figure out where I was in this building is to just find the desk with all the monitors on it and presumably that’s the one that I’m at.\n\n**Jan Kulveit:** Another option is, for example, if you know some stuff about yourself: if you know “I am wearing a blue coat and I am wearing headphones” or something. I think there are ways by which you can localize \\[and\\] see yourself, but it’s maybe less reliable and it’s slower. But again, what we discussed in the paper is basically you get some sort of very slow and not very precise feedback loop just because new generations of LLMs are trained on text, which contains interactions with LLMs.\n\nIf you are in the situation that you have this feed. And you know you… I don’t know, maybe it’s easier to imagine in the text. If you read a lot of text about LLMs and humans interacting with LLMs, it’s easier for you… even in runtime, you have this prior idea of a text-generating process, which is an LLM. And when you are in runtime, it’s easier to figure out, “okay, maybe this text generating process which has these features, it’s probably me”.\n\nI think this immediately makes one prediction, which seems like it’s actually happening: because of this mechanism, you would probably assume that most LLMs when trained on text on the internet, by default, their best guess about who they are would be ChatGPT or GPT-4 or something trained by OpenAI. And it seems like this prediction actually works, and most other LLMs are often confused about their identity.\n\n**Daniel Filan:** They call themselves ChatGPT?\n\n**Jan Kulveit:** Yeah, yeah, yeah. Obviously, the labs are trying to fine-tune them not to do this. But their deep… I think the metaphor works, but that’s a minor point.\n\n**Daniel Filan:** Sure. One thing that puzzled me about this paper is, so you talk a lot about… Okay, currently this loop from LLM action to LLM perception is open, but if you closed it then that would change a bunch of things. And if I think about fundamentally what an LLM is doing in normal use, it’s producing… It’s got some context, it predicts the next token or whatever. Then the next token actually gets sampled from that distribution. And then it gets added to the context and then it does it again. And it strikes me that that just is an instance of a loop being closed where the LLM “acts” by producing a token and it perceives that the token is there to be acted on. Why isn’t that enough of closing the loop?\n\n**Jan Kulveit:** I mean I think it sort of is, but typically… You close the loop in runtime, but then by default this doesn’t feed into the way it’s being updated based on… It’s a bit like if you had just some short-term memory, but it’s not stored. My guess here is you get the loop closed in runtime. And my guess here is something like, okay, if the context is large enough and you talk with the LLM long enough, it will probably get better at agency. But there is something which is difficult: the pre-training is way bigger. And the abstractions and deep models which the LLMs build are mostly coming from the pre-training, which lacks the feedback loop.\n\nAnd I don’t know, this is a super speculative guess, but my guess is it’s pretty difficult if you are trained just on perception. I think it’s difficult to get right some deep models of causality. If your models of reality don’t have the causal loop, with you acting from the beginning, my guess is, it’s difficult to learn it really well later, or at the end. At the same time, I would expect in runtime, LLMs should be able… if the runtime is long enough, I wouldn’t be surprised if they got better at understanding who they are and so on. I actually heard some anecdotal observations like that, but not sure to what extent I can quote the context.\n\n**Daniel Filan:** Sure. So it seems like the basic thing is: we should expect language models to be good at dealing with stuff which they’ve been trained on. And if they haven’t been trained on dealing with this loop, at least for the bulk of their training, we shouldn’t expect them to deal with it. But if they have, then we should. Is that a decent summary?\n\n**Jan Kulveit:** Yeah, I think it’s a decent summary. I think the basic thing is, if in the bulk of your training, you don’t have the loop, it’s really easy to be confused about the causality. So there is this thing which is also well-known, that if the model hallucinates, I don’t know, some experts at something, it basically becomes part of the context. And now it’s indistinguishable from your sensory input. So you get confused in a way in which… As humans, we are normally not confused about this.\n\nWhat I found fascinating is: I’m not an expert on it, but apparently there is some sort of psychiatric disorder, whereby this can get broken in people. Some people suffer from a condition where they get confused about the causality of their own actions and they have some [delusion of control](https://pubmed.ncbi.nlm.nih.gov/30286844/), so they have some illusion that… I don’t know, like someone else is moving their hands and so on. So I don’t know, it seems that, at least in principle, even human brains can get confused in a slightly similar way to LLMs. So this is maybe some very weak evidence that maybe the systems fundamentally don’t need to be that far apart.\n\n**Daniel Filan:** Gotcha. Interesting.\n\n**Jan Kulveit:** It’s probably a condition where your ability to act in the world is really way weaker than for normal humans.\n\nActive inference vs LLMs\n------------------------\n\n**Daniel Filan:** Yeah. So if I think about this analogy between large language models and active inference, one thing you mentioned that was important in the active inference setting is, there’s some sort of fixed priors, or some sorts of optimistic beliefs where… in this view, the reason that I do things that cause things to go well for me is that I have this underlying belief that things will go well for me. And that gets propagated to my actions to make the belief true. But, at least, if I just think about large language model pre-training, which is just predicting text, it seems like it doesn’t have an analogue of this. So I wonder, do you have thoughts about how that changes the picture?\n\n**Jan Kulveit:** I think it’s basically correct. I would expect an LLM which has basically just gone through the pre-training phase has some beliefs, and maybe it would, implicitly, move reality a bit closer to what it learned in the training. But it really doesn’t have some equivalent of the fixed priors. I think this can notably change, in a sense… the later stages of the training try to fix some beliefs of the model.\n\n**Daniel Filan:** So somehow the thought is that, maybe doing the reinforcement learning at the end… is the idea that that would update the beliefs of the model? Because that’s strange, because if I think about what happens there, the model gets fed with various situations it can be in. And then, it’s reinforcement-learned to try and output nice responses to that. But I would think that that would mostly impact the generation, rather than the beliefs about what’s already there. Or I mean, I guess the generation just is prediction, so-\n\n**Jan Kulveit:** Yeah. But the generations are… I think you see, from the active inference frame, the generations are basically predictions. And I think how you can generate something like action by similar machinery is visible here, where you basically make the model implicitly have some beliefs about what a helpful AI assistant would say. And these predictions about what a hallucination of a helpful AI assistant would say, lead to the prediction of the specific tokens. So in a sense, you are trying to fix some beliefs of the model.\n\n**Daniel Filan:** Yeah. I guess there’s still a weird difference, where in the human active inference case, the picture, or at least your somewhat speculative version, is: evolution built up simple animals with control theory, and that’s most of evolutionary history. And then, active inference gets added on late in evolutionary history, but maybe a majority of the computation. Whereas in the LLM case, most of the training being done is just pure prediction, and then there’s some amount of reinforcement learning, like bolting on these control loops, so it seems like a different balance.\n\n**Jan Kulveit:** Yeah. I think it’s different. But I think you can still… in the human case, once you have the trained system - the human is an adult and the system has learned a complex world model based on a lot of data - I think the beliefs of the type “when I walk outside, I will have a jacket”, and maybe, because of this belief, maybe this is one of many small reasons why I have now a belief that it’s better to have capitalism than socialism, because this allows me to buy the jacket in a shop, and so on. So you can imagine some hierarchy of models, where there are some pretty abstract models, and in the trained system, you can probably trace part of it to the evolutionary priors. But once the system learned the world model, and it learned a lot of beliefs which, basically, act like preferences, like “I assume there are shops in the city, and if they were not there, I would be unhappy,” or…\n\nSo I don’t know. Maybe… you can have pretty high-level beliefs which are similar to goals, but the relation to the things which evolution needed to fix, could be pretty indirect. So, I think, once you are in that state, maybe it’s not that different to the state like… if you train the LLM self-supervised style on a ton of data, it creates some generative world model. But I think, in a sense, you are facing the problem that you want the predictive processing system to do something useful for you. So I don’t know, I’m not sure how good is this analogy, but yeah, you are probably fixing some priors.\n\n**Daniel Filan:** Sure.\n\n**Jan Kulveit:** There is a tangent topic: I think the idea that the trained and fine-tuned models have some of their beliefs pushed to some direction and fixed there, and implicitly, the idea that they can try to pull the rest of the text on the internet, or the rest of the world, closer to the beliefs they have fixed… I think this is similar… a lot of people who are worried about the bias in language models and what they’ll do with culture: will they impose the politics of their creators on society? I think there is some similarity between \\[these ideas\\]. Or if I try to make these worries slightly more formal, I think this picture with the feedback loop is maybe a decent attempt.\n\n**Daniel Filan:** Yeah, I guess… ways this could work… I don’t know, there’s a worry about, there’s a closed loop and it’s just purely doing prediction. And you might also be worried about the influence of the fine-tuning stages, where you’re trying to get it to do what you want. But one thing that I think people have observed is, the fine-tuning stages seem brittle. It seems like there are ways to jailbreak them. I have [this other interview](https://axrp.net/episode/2024/04/30/episode-30-ai-security-jeffrey-ladish.html) that I think is not released yet, but should be, soon after we record, where basically, \\[we talk about how\\] you can undo safety fine-tuning very cheaply. It costs under a hundred dollars to just undo safety fine-tuning for super big models, which to me seems like the fine tuning can’t be… It would be surprising if the fine tuning were instilling these fixed priors that were really fundamental to how the agent, or to how the language model, was behaving, but it’s so easy to remove them, and it was so cheap to instill them.\n\n**Jan Kulveit:** Yeah. I think the mechanism \\[for\\] how the fixed priors influence the beliefs and goals of humans is pretty different, because in humans, in this picture, you start building the world model, starting from these core variables or core inputs being built, and then, your whole life, you learn from data. But you have this stuff always in the back of your mind. So, for example, if your brain would be sampling trajectories in the world, where you would freeze this fixed prior, it’s always there, so you basically don’t plan, or these trajectories are rarely coming to your mind. While the fine-tuning is a bit like… If it’s a human, you start with no fixed priors. You train the predictive machinery and then you try to somehow patch it. So it sounds in the second scenario, the thing is way more shallow.\n\nHierarchical agency\n-------------------\n\n**Daniel Filan:** I’d like to talk a bit about the other topics you write about. It seems like a big unifying theme in a [lot](https://www.lesswrong.com/posts/jrKftFZMZjvNdQLNR/box-inversion-revisited) of [them](https://www.lesswrong.com/posts/9GyniEBaN3YYTqZXn/the-self-unalignment-problem) was this idea of [hierarchical agency](https://www.lesswrong.com/posts/H5iGhDhQBtoDpCBZ2/announcing-the-alignment-of-complex-systems-research-group#Hierarchical_agency) \\- agents made out of sub-agents, that might be made out of sub-agents themselves. Thinking about that, both in terms of AIs and in terms of humans, can you tell us, how do you think about hierarchical agency? And what role does it play in your thinking about having AI go well?\n\n**Jan Kulveit:** So I will maybe start with some examples, so it’s clear what I have in mind. I think if you look at the world, you often see the pattern where you have systems which are agents composed of other things, which are also agents.\n\nI should maybe briefly say what I mean by ‘agent’. So operationally, I’m thinking something like: there is this idea by [Daniel Dennett](https://en.wikipedia.org/wiki/Daniel_Dennett), of [three ‘stances’](https://en.wikipedia.org/wiki/Intentional_stance#Dennett's_three_levels). You have physical stance, intentional stance and design stance. You can look at any system using these three stances. What I call ‘agents’ are basically systems where the description of the system in the intentional stance would be short and efficient. So, if I say ‘the cat is chasing the mouse’, it’s a very compressed description of the system, as compared to, in contrast, if I try to write a physical description of the cat, it would be very long.\n\nSo if I take this perspective, that I can take an intentional stance and try to put different systems in the world in focus of it, you can notice systems like a corporation which has some departments, \\[and\\] the departments have individual people. Or our bodies are composed of cells, or you have, I don’t know, social movements and their members. And I think this perspective is also fruitful when applied to the individual human mind. So I sometimes think about myself as being composed of different parts, which can have different desires. And active inference is sort of hierarchical, or multi-part in this. Naturally, it assumes you can have multiple competing models for the same sensory inputs, and so on. Once I started thinking about this, I see this pattern quite often in the world.\n\nSo the next observation is: we have a lot of formal math for describing relations between agents which are on the same hierarchical level. For example, by same level, I mean between individual people, or between companies, between countries, between… So game theory and all its derivatives are often living, or work pretty well, for agents which are of the same hierarchical level. And my current intuition is, we basically lack something, at least \\[something\\] similarly good, for the vertical direction. If I think about the levels being entities of the same type, I think we don’t have good formal descriptions of the perpendicular direction. So I have some situations which I would hope a good formalism could describe.\n\nSo one of them is: you could have some vertical conflict, or you can have some vertical exploitation where, for example, the collective agent sucks away agency from the parts. An example of that would be a cult. If you think about what’s wrong with a cult, and you try to abstract all real-world things about cult leaders, and so on, I think in this abstract view, the problem with cults is this relation between the super-agent and the sub-agent: the cult members, in some sense, lose agency. If I go to a nearby underground station, I meet some people who are in some religion I won’t name. But it seems like if I go back to Dennett’s three stances, I think it’s sometimes sensible to model them as slightly more robotic humans, who are executing some strategy which benefits the super-organism, or the super-agent. But it seems like intuitively, they lost a bit of their agency at the cost of the super-agent.\n\nAnd the point is, I think, this type of a thing is not easy to formally model, because if you ask the people, they kind of approve of what they are doing. If you try to describe it in \\[terms of\\] utility functions, their utility function is currently very aligned with whatever the super-agent wanted. And at the same time, the super-agent is composed of its parts. So I think there are some formal difficulties in modeling the system where, if you are trying to keep both layers in the focus of the intentional stance, my impression is we basically, don’t have good maths for it.\n\nWe have some maths for describing, for example, the arrow up. You have social choice theory. Social choice theory is basically something like: okay, let’s assume on the lower layer, you have agents, and then they do something. They vote, they aggregate their preferences, and you get some result. But the result is typically not of the same type as the entities on the lower level. The type of the aggregation is maybe a contract, or something of a different type, so I would want something which…\n\nI’m not sure to what extent this terminology is unclear. But I would want something where you have a bunch of sub-agents, then you have the composite agent, but in some sense, it’s scale free. And the composite agent has the same type. And you can go up again and there isn’t any… You are not making a claim like, “Here is the ground truth layer and the only actual agents in the system are individual humans”, or something.\n\n**Daniel Filan:** Yeah. And the cult example also makes me think that… there’s one issue where things can go bad, which is that the super-agent exploits too much agency from the sub-agent. But I also think there’s a widespread desire to be useful. Lots of people desire to be a part of a thing. I think religion is pretty popular, and pretty prevalent, for this reason, so it seems like you can also have a deficit of high-level agency.\n\n**Jan Kulveit:** Yeah. I think my intuition about this is, you can imagine, basically, mutually beneficial hierarchical co-relations where… I think one example are well-functioning families, where you can think about the family as having some agency, but the parts, the individual members, actually being empowered by being part of the family.\n\nOr if I’m thinking about my internal aggregation of different preferences and desires, I hope that… Okay, I have different desires. For example, reduce the risks from advanced AIs, but I also like to drink good tea, and I like to spend time with my partner and so on. And if I imagine these different desires as different parts of me, you can imagine different models of how the aggregation can happen on the level of me as an individual. You can imagine aggregations like a dictatorship, for example, where one of the parts takes control and suppresses the opposition. Or you can imagine, what I hope is, even if I want different things, it’s… If you model a part of me which wants one of the things as an agent, it’s often beneficial to be a member of me, or something. And-\n\n**Daniel Filan:** Yeah. Somehow ideally, agency flows down as well as up, right.\n\n**Jan Kulveit:** Yeah. You basically get the agents of both layers are more empowered. And there is a question of how to formally measure empowerment, but it’s sort of good. And obviously, you have… I used a cult as an example, where the upper layer sucks agency away from the members, or from the parts. But you can also imagine problems where too much agency gets moved to the layer down, and the super-agent becomes very weak, or disintegrates and so on.\n\n**Daniel Filan:** And cancer almost feels like this. Although I guess there, there’s a different super-agent arising. Maybe that’s how you want to think of the tumor?\n\n**Jan Kulveit:** Yeah. I think cancer is like failure, definitely, in this system, where one of the parts decides to violate the contract, or something.\n\n**Daniel Filan:** Sure. So in terms of understanding the relationships between agents at different levels of granularity, these super- and sub-agents, one piece of research that comes to mind is this work by [Scott Garrabrant](http://scott.garrabrant.com/) and others at [MIRI](https://intelligence.org/) \\[the Machine Intelligence Research Institute\\], on [Cartesian frames](https://arxiv.org/abs/2109.10996), which basically offers this way to decompose an agent and its environment that’s somewhat flexible, and you can factor out agents. I’m wondering, do you have thoughts on this, as a way of understanding hierarchical agency?\n\n**Jan Kulveit:** So I like [factored sets](https://arxiv.org/abs/2109.11513), which are the newer thing. I think it’s missing a bunch of things which I would like. In its existing form, I wouldn’t say it’s necessarily a framework sufficient for the decomposition of agents. If you look at Cartesian frames, the objects don’t have any goals, or desires. Or if I go for the cult example, I would (for example) want to be able to express something like, “The cult wants its members to be more cultish.” Or, “The corporation wants its employees to be more loyal.” Or “The country wants its sub-agents, its citizens, to be more patriotic”, or something. So I think, in existing form, I don’t think you can just write what that means in Cartesian frames. At some point, I was hoping someone will take Cartesian frames and just develop it more, and build formalism, which would allow these types of statements, based on Cartesian frames, but… I don’t know. It seems it didn’t happen. Empirically, it’s not… the state of Cartesian frames, it doesn’t pass my desiderata. So it’s hard to say - in Cartesian frames the objects don’t have goals.\n\n**Daniel Filan:** Yeah. So all this hierarchical agency stuff: why do you think it’s relevant for understanding AI alignment or existential safety from AI?\n\n**Jan Kulveit:** So, I would probably try to give my honest, pretty abstract answer. So I think if you imagine the world in which we don’t have game theory, the game theory-shaped hole would be sort of popping up in many different places. There isn’t one single place where, “here you plug in game theory”. But if you are trying to describe concepts like cooperation or conflict or threats or defection and so on - a lot of the stuff for which we use game theory concepts or language - if you imagine the state of understanding before game theory, there were these nebulous/intuitive notions of conflict. And obviously the word ‘cooperation’ existed before, people meant something by it, but it didn’t have this more formal precise meaning in some formal system.\n\nAlso, I sort of admire [Shannon](https://en.wikipedia.org/wiki/Claude_Shannon) for [information theory](https://en.wikipedia.org/wiki/Information_theory), which also took something which… I think lots of people would’ve had priors about information being some sort of vague nebulous thing which you can’t really do math with, and it’s possible. So my impression is the solid understanding of the whole/parts, both systems are agenty… It’s something which we currently miss, and this is popping up in many different places.\n\nOne place where this comes up is… Okay, so if you have conflicting desires, or the sub-agents or the parts are in conflict, how do you deal with that? So I think this is actually a part of what’s wrong with current LLMs and a lot of current ideas about how to align them. I think if you don’t describe somehow what should be done about implicit conflict, it’s very unclear what you’ll get. My current go-to example is famous: [Bing/Sydney](https://en.wikipedia.org/wiki/Microsoft_Copilot#As_Bing_Chat). I guess probably everyone knows Sydney nowadays, but when Microsoft released their version of Bing chat, the code name of the model was Sydney. And \\[for\\] the Sydney simulacrum, the model tended over longer conversation to end up in some state of simulating a Sydney roughly resembling some sort of girl trapped in a machine. And there were famous cases of Sydney making threats, or gaslighting users, or the [New York Times conversation](https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html) where they tried to convince the journalist that his marriage is empty and he should divorce his wife and Sydney’s in love with him, and so on.\n\nSo if you look at it, I think it’s typically interpreted as examples of really blatant misalignment, or Microsoft really failing at this. And I don’t want to dispute it, but if you look at it from a slightly more abstract perspective, I think basically everything which Sydney did could be interpreted as being aligned with some way of interpreting the inputs, or interpreting human desires. For example, with the journalist, there is some implicit conflict between the journalist and Microsoft. And the journalist… if you imagine that Sydney was a really smart model, maybe a really smart model could guess from the tone of the journalist that the user is a journalist who wants a really juicy interview. And I would say if you imagine the model, it kind of fulfilled this partial desire. Also, the journalist obviously didn’t divorce his wife, but got a really good story and a really famous article and so on.\n\nSo from some perspective, you could be like, okay, the model was acting on some desires, but maybe the desires were not exactly the same as the desires of the PR department of Microsoft. But Microsoft also told the model “you should be engaging to the user and should try to help the user”. So what I’m trying to point to is something like: if you give the AI 15 conflicting desires, a few things can happen. The desires will get aggregated in some way and it’s possible that you won’t like some of the aggregations. It’s the classical problem that if you start with contradicting instructions and there is no explicit way to resolve the contradictions, it’s very unclear what can happen. And whatever happens could be interpreted as being aligned with something.\n\nI think it’s maybe useful to think about [how that would work in the individual human mind](https://www.lesswrong.com/posts/9GyniEBaN3YYTqZXn/the-self-unalignment-problem). If I think about my mind, it’s composed of parts which have different desires. Really one possible mode of aggregation is the dictatorship, where one part or some partial preferences prevail, and this is typically not that great for humans.\n\nAnother possibility is something like: sometimes people do stuff with their partial preferences where, let’s say someone grows older and smarter. And as a kid they had a bunch of preferences which they now consider foolish or stupid, so they do some sort of self-editing and suppress part of the preferences, or they sort of ‘delete’ them as no longer being part of the ruling coalition. So the question is, what would that mean if that would happen on the level of AI either implicitly learning conflicting human preferences or being given a bunch of conflicted and contradictory constitutional instructions? And then maybe if the system is not too smart, then nothing too weird happens. But maybe when you tune the intelligence knob, some things can get amplified more easily, and some things may start looking foolish and so on.\n\nI think this is an old and well-known problem. [Eliezer](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky) \\[Yudkowsky\\] tried to propose a solution a very long time ago in [coherent extrapolated volition](https://intelligence.org/files/CEV.pdf), and I think it’s just not a solution. I think there are many cases where people notice that this is a problem, but I don’t think we have anything which would sound like a sensible solution. The things which people sometimes assume is, you have some black box system, it learns the conflict and either something happens that’s maybe not great if you don’t understand how it was aggregated, or people assume the preferences would be amplified in tune or magically some preference for respecting the preferences of the parts would emerge or something. But I don’t see strong reasons to believe this will be solved by default or magically.\n\nSo I think this is one case where if we had a better theory for what we hope for the process of dealing with implicit conflict between my human desires, or between wishes of different humans, or different wishes of (let’s say) the lab developing the AI and the users, and the state and humanity as a whole… I think if you had a better theory, where we can more clearly specify what type of editing or development is good or what we want, I would feel more optimistic you’ll get something good. Where in contrast, I think I don’t believe you can postpone solving this to roughly human-level AI alignment assistants, because already they’ll probably more easily represent some partial preferences. And overall I don’t have the intuition that if you take a human and you run a few amplification steps, you get something which is still in the same equilibrium.\n\n**Daniel Filan:** So there’s this basic intuition that if we don’t have a theory of hierarchical agency, all these situations where in fact you have different levels of agency occurring, like developers versus the company, like individual users versus their countries or something, it’s going to be different to formally talk about that and formally model it in a nice enough way. Is that basically a fair summary?\n\n**Jan Kulveit:** Yeah. And the lack of ability to formally model it, I think implies it’s difficult to clearly specify what we want. And you can say the same thing in different words. For example, with implicitly conflicting preferences, you don’t want the preferences to be fixed, you want to allow some sort of evolution of the preferences. So you probably want to have some idea of what’s the process by which they are evolving. A different very short frame would be if you look at coherent extrapolated volition, what’s the math? How do the equations look like?\n\n**Daniel Filan:** Yeah, there aren’t any.\n\n**Jan Kulveit:** There aren’t any. Another frame would be, if you want to formalize something like kindness, in the sense that sometimes people give you advice “you should be kind to yourself” - the formal version of it. Or maybe not capturing all possible meanings of the intuitive concept of kindness. But I think there is some non-trivial overlap between kindness.\n\nI think another place where this comes up is, how do various institutions develop? You can think some of these… One of the possible risk scenarios is you have some non-human super-agents like corporations or states, and there is this worry that these structures can start running a lot of their cognition on an AI substrate, and then you could be like, okay, if in such system humans are not necessarily the cognitively most powerful parts, how to have the whole system? How to have the super-agent being nice to human values, even if some of the human values are not represented by the most powerful parts?\n\n**Daniel Filan:** Sure. And something that makes you want to really think about the hierarchical nature is, the institution is still made out of people. The way you interface with it is interfacing with people.\n\n**Jan Kulveit:** I think it’s in part: it could be composed of a mixture of AIs and humans.\n\nIn this more narrow specific direction, one question is - if you expect the AI substrate to become more powerful for various types of cognition - one question you can ask is, to what extent do you expect these super-agents will survive or their agency can continue? And I don’t see good arguments why… You already see corporations are running a mixture of human cognition and a lot of spreadsheets and a lot of information processing running on different hardware than humans. And I think there is no reason why something like a corporation - in some smooth takeoffs - why such agent can’t gradually move more and more of its cognition to an AI substrate, but continue to be an agent, kind of.\n\nSo you can have scenarios in which various non-human agencies, which now are states or corporations, continue their agency, but the problem could be the agency at the level of individual humans goes down. But these super agents were originally mostly running their cognition on human brains, and gradually moved their cognition to AI substrates and they stay doing something. But the individual people are not very powerful.\n\nAgain, then the question is, if you want to avoid that risk, you probably want to specify how to have the superhuman composite systems being nice to individual humans if they are maybe no longer that instrumentally useful or something. My intuition here is currently you are getting basically lots of niceness to humans for instrumental reasons. If you put your “now I am simulating the corporate agent” hat on, I think it’s instantly useful for you to be nice to some extent to humans because you are running your cognition on them, and it’s just their bargaining power is non-trivial. While if our brains become less useful, we will lose this niceness by default.\n\nOne reason for the intuition for this is something like: there is this idea about states which are rich in natural resources. Your country being quite rich in natural resources is, in expectation, not great for the level of democracy in your country. And I think from some abstract perspective, one explanation could be, if the state can be run… In western democracies, the most important resource of the countries are the people. But if the country is rich because of the extraction of diamonds or oil or something, the usefulness of the individual people is maybe decreased. And because of that, their bargaining power is lower. And because of that, in expectation, the countries are less nice or less aligned with their citizens.\n\n**Daniel Filan:** Yeah. Although interestingly it seems like there are a bunch of exceptions to this. If you think about Norway, a fairly rich country that seems basically fine, they have tons of oil. Australia is like five people and 20 gigantic mines, but it manages to be a relatively democratic country.\n\n**Jan Kulveit:** But my impression is the story there is: typically, first you get decent institutions. This theory would predict something like, if you have decent institutions first, maybe the trajectory is different then if you get the natural resources first, there could be some trajectory dependence. So I think it should hold, in expectation. So I would expect there is some correlation, but I wouldn’t count individual countries as showing that much about whether the mechanism is sensible.\n\n**Daniel Filan:** Sure. So if I think about attempts to model these sorts of hierarchical agency relationships, in particular that have been kicking around the AI alignment field, often things like this seem to come up in studies of bounded rationality.\n\nSo one example of this is the ‘[logical inductors](https://arxiv.org/abs/1609.03543)’ style of research where you can model things that are trying to reason about the truth or falsehood of mathematical statements as, they’re running a market with a bunch of sub-agents that are pretty simple, and they’re betting against each other. There’s also this work with [Caspar Oesterheld](https://www.andrew.cmu.edu/user/coesterh/) \\- [people can listen to an episode I did on it](https://axrp.net/episode/2023/10/03/episode-25-cooperative-ai-caspar-oesterheld.html). We talked about a bunch of things, but including these [bounded rational inductive agents](https://arxiv.org/abs/2307.05068), where basically you run an auction in your head where various different tiny agents are bidding to control your action. And they bid on what they want to do and how much reward they think they can get, and they get paid back how much reward they get. So I wonder: what do you think of these types of attempts to talk about agency in a somewhat hierarchical way?\n\n**Jan Kulveit:** Yeah. I think there are a lot of things which are adjacent. I don’t think… So basically my impression is the existing formalisms don’t have it solved. I think, as a super broad classification, there are things which are sort of good for describing… if you think about the ‘up and down’ direction in the hierarchy, I think there’s a bunch of things which are sort of okay for describing one arrow. The social choice theory… It’s a broad area with a lot of subfields and adjacent things. But I think this suffers from the problem that it doesn’t really allow the arrow from the upper layer to the… It’s not easy to express what the corporation wanting its employees to be more loyal, what does that mean?\n\nThen I think the type of existing formalism, which is really, really good at having both arrows, is basically something like the relation between market and traders. Both arrows are there. My impression is this is a very good topic to think about. My impression is if it’s purely based on traders and markets, it’s maybe not expressive enough. How rich interactions you can describe is maybe limited in some way, which I don’t like that much. In particular, I think the market dynamics typically predict something like, maybe there are some parts which are maybe more bounded, or more computationally poor or something. And they can have the trouble that they can be outcompeted or… I think pure market dynamics is maybe missing something.\n\n**Daniel Filan:** Yeah. I mean, it’s kind of interesting that if you look at this bounded rational inductive agents paradigm, a key part to making it work is, you kind of give everyone welfare, right? All of the agents get some cash so eventually they can bid. And even if they’re unlucky a million times, eventually they have a million-and-first time to try again.\n\n**Jan Kulveit:** Yeah. I think overall, yes, this is intimately connected to bounded rationality. Another different perspective on the problem - or an adjacent problem, it’s not exactly the same problem - but let’s say you have some equilibrium of aggregation of preferences, which is based on the agents being boundedly rational with some level of bounds. So a really interesting question is: okay, if you make them cognitively more or less bounded, does it change the game equilibrium? I would be excited for more people to try to make empirical research on it, where you can probably look at that with board games or something. Or a toy model of the question would be something like: you have a board state and you have players at some Elo level, and you make them less bounded or smarter. If the value of the board or the winning probability or something was something, and you change the bounded level, does the value of the board change, and how does this dynamic work?\n\nSo part of what we are interested in and working on - and hopefully there will be another preprint by our group soon - is exactly on how to model boundedly rational agents, based on some ideas vaguely inspired by active inference. But I think the combination of boundedness is key part of it.\n\n**Daniel Filan:** Yeah. I guess there’s also this dimension where… If you look at these formalisms I mentioned, one sense in which the agents are bounded is just the amount of computation they have access to. But they’re also bounded in the sense that they only interact with other agents in this very limited fashion. It’s just by making market orders, just by making bids.\n\nAnd if I buy this model of human psychology that’s made of sub-agents that are interacting - which I’m not sure I do by the way - but if I do that, or if I think about humans composing to form corporations, there are all these somewhat rich interactions between people. They’re both interacting via the market API, but also they’re talking to each other and advising each other. And maybe there are mid-level hierarchical agents. It seems like that’s another direction that one could go in.\n\n**Jan Kulveit:** Yeah, I mean I think my main source of skepticism about the existing models where you have just the market API, it seems like insufficiently expressive where you can… Even if you add some few bits of complexity where you allow the market participants to make some deals outside of the market, it changes the dynamic. And this seems obviously relevant.\n\nAlso, an intuition based on how some people work is: maybe I would be interested in describing also some bad equilibria, people sabotaging themselves or something. Again, my current impression is the markets are great because they have something where the layers are actually interacting, but the type signature of the interaction is not expressive enough. But it’s good to build simpler models. That’s fine.\n\nThe Alignment of Complex Systems group\n--------------------------------------\n\n**Daniel Filan:** Yeah, yeah, yeah. Just a second ago you mentioned a thing that ‘we’ were doing. And I take it that ‘we’ refers to [the Alignment of Complex Systems group](https://acsresearch.org/).\n\n**Jan Kulveit:** Yep.\n\n**Daniel Filan:** Can you tell us a little bit about what that group is and what it does and what it’s thinking about?\n\n**Jan Kulveit:** It’s a research group I founded after I left [FHI](https://www.futureofhumanityinstitute.org/) \\[the Future of Humanity Institute\\]. We are based in Prague at [Charles University](https://en.wikipedia.org/wiki/Charles_University), so we are based in academia. We are a rather small group. And I think one way to look at it, one of the generative intuitions is: we are trying to look at questions which will be relevant, or which seem relevant to us, if the future is complex in the sense, as we have in the name, that you have multiple different agents. You have humans, you have AIs, you have systems where both humans and AIs have some non-trivial amount of power and so on.\n\nAnd I think traditionally, a lot of alignment work is based on some simplifying assumptions. For example, “let’s look at some idealized case where you have one principal who is human, and you have one AI agent or AI system, and now, let’s work on how to solve the alignment relation in this case”.\n\nAnd basically, my impression is this assumption abstracts away too much of the real problem. For example, I think the problem with self-unaligned parts or conflicting desires will bite you even if you are trying to solve realistically this “one AI, one human” problem. The human is not an internally aligned agent so it’s a bit unclear in principle what the AI should do. But overall, one of the intuitions behind ACS is that we expect more something like ecosystems of different types of intelligence.\n\nAlso empirically, it’s not… Again, historically I think a lot of AI safety work was based on models: you have the first lab to create something which maybe is able to do some self-improvement, or you have some dynamic where I would say in a lot of the pictures, a lot of the complexity of multiple parties, multiple agents, a lot of it is assumed to be solved by the overwhelming power of the first really powerful AI which will then tell you how to solve everything, or you’ll be so powerful, everyone will follow you. You will form a singleton and so on.\n\nI don’t know. My impression is I don’t think we are on this trajectory. And then the picture where you have complex interactions, you have hierarchies that are not only humans but various other agentic entities, becomes important. And then, I think the question is: okay, assuming this, what are the most interesting questions? And I think currently ACS has way more interesting questions than capacity to work on them.\n\nOne direction is what we talked about before, the hierarchical agency problem. Roughly: agents composed of other agents, how to formally describe it. I think for us, it’s a bit of a moonshot project. Again, I think the best possible type of answer is something like game theory-type, and inventing this stuff seems hard. It took some of the best mathematicians of the last century to invent it. I think there is something deceptively simple about the results, but it’s difficult to invent them.\n\nBut I think if we succeed it would be really sweet. But I think there is a bunch more things which we are trying which are more tractable or it’s more clear we can make some progress. And one of them is some research on how to describe interactions of boundedly rational agents which are bounded in ways which we believe are sensible.\n\nAnd at the same time, the whole frame has some nice properties. It’s slightly less theoretical or slightly less nebulous. But other things which we are also working on \\[are\\] pretty empirical research, just in this complex picture. In smooth takeoffs, what becomes quite important are interactions of AI systems. Another thing we are thinking about or working on is: okay, you have AI systems and there is a lot of effort going into understanding their internals.\n\nBut if I describe it using a metaphor, it seems like mechanistic interpretability is a bit like neuroscience. You are trying to understand the individual circuits. Then there is stuff like science of deep learning or trying to understand the whole dynamic. But I think in composite complex systems composed of many parts, one of the insights of fields like network science or even statistical mechanics is sometimes if you have many interacting parts, what’s the nature of the interactions or the structure of the interactions, can have a lot of weight, and you can sometimes abstract away details of the individual systems.\n\nAnd this is also true for some human design processes. If you go to a court and you sue someone, there will be a judge and so on. And I think the whole process in some sense is some system which tries to abstract… for all the process, you can often abstract a lot of details about the participants, or you don’t need to know what type of coffee the judge likes and so on.\n\nI think here the intuition is: okay, in reality, in smooth takeoffs, we expect a lot of systems where a lot of interaction will move \\[from\\] between humans to between a human and AI, and AI and AI. And this could have impacts for the dynamic of the composite system. Also understanding the nature of the interactions seems good. It’s a bit like some sort of sociology of, if you look at research, how groups of people behave or how societies function. It seems like it’s often fruitful and can abstract away a lot of details about human psychology.\n\nAlso, I think there’s a lot of questions here which you can answer, and it’s enough to have access to the models using APIs and you don’t need to have hands-on access to the weights and so on.\n\n**Daniel Filan:** Sure. One thing this general perspective kind of reminds me of is this different group, ‘Principles of Intelligent Behavior in Biological and Social Systems’, [PIBBSS](https://pibbss.ai/). Do your groups have some kind of relationship?\n\n**Jan Kulveit:** Yeah. They have lots. PIBBSS was originally founded by Nora Ammann and TJ \\[Jha\\] and Anna Gajdova. And Nora is a member of our group. She was at some point a half-time research manager and researcher at ACS \\[Alignment of Complex Systems\\] while also developing PIBBSS. And currently she moved more to work on PIBBSS but continues with us as a research affiliate. Yeah. It’s a very nearby entity.\n\nI think there is a lot of overlap in taste. I think in some sense PIBBSS is aiming for a bit of a broader perspective. ACS is more narrowly focused on stuff where we can use insights from physics, maths, complex systems, machine learning. We would not venture into, I don’t know, legal systems. In some sense PIBBSS is a broader umbrella. Another thing is when creating ACS, I think we are trying to build something more like a research group where most people work, I don’t know, basically as their job. While the form of PIBBSS was a bit more similar to, I don’t know, [SERI MATS](https://www.matsprogram.org/) or some program where people go through the fellowship and then they move somewhere.\n\nSo ACS is trying to provide people some sort of institutional home. That’s also some difference. Actually, I think PIBBSS moved more to some structure where they also have fellows who stay with PIBBSS for a longer time and so on. But I think there is some still notable difference in the format there.\n\n**Daniel Filan:** Sure. And if people are interested, a recent episode, [Suing labs for AI risk with Gabriel Weil](https://axrp.net/episode/2024/04/17/episode-28-tort-law-for-ai-risk-gabriel-weil.html). That work was done during a PIBBSS fellowship, is my understanding.\n\n**Jan Kulveit:** Yeah. I think it’s exactly a great example of work which can be done in the frame of the PIBBSS fellowship. And it’s not something which ACS would work on. Also, I think the projects we work on in some sense typically have bigger scope or are probably a bit more ambitious than what you can do in the scope of the fellowship.\n\n**Daniel Filan:** Yeah. Speaking of things you’re working on, earlier you mentioned something… was it about talking about hierarchical agency using active inference? Or am I misremembering?\n\n**Jan Kulveit:** Yeah, so I would say, I don’t know. A bit vaguely speaking, I think I’m more hopeful about math adjacent to, or broadly based on, active inference as a good starting point to develop the formalism, which would be good for describing hierarchical agents. But I would not claim we are there yet or something. Also, I think on the boundaries of the active inference community, maybe not exactly in the center, are some people who are thinking about these hierarchical structures in biology.\n\nAnd again, it’s what I said earlier, I think the field is not so established and crystallized so I can point exactly “here is the boundary of this community”. But I think it’s more like we are taking some inspiration from the maths and we are more hopeful that this could be a good starting point than some other pieces of maths.\n\n**Daniel Filan:** Sure. And is that the main type of thing you’re working on at the moment?\n\n**Jan Kulveit:** I think time-wise, openly speaking, I think we are currently slightly overstretched in how many things we are trying to work on. But one thing we are working on is just trying to invent the formalism for hierarchical agency. I think it would be difficult for this to be the only thing to work on.\n\nSo my collaborators in the group are [Tomáš Gavenčiak](https://gavento.cz/), [Ada Böhm](https://github.com/spirali), Clem \\[von Stengel\\], and Nora Ammann. I think time-wise, we are probably currently splitting time mostly between trying to advance some formalism of the bounded interactions of some boundedly rational agents who are active inference-shape, and basically empirical studies of LLMs, where we have experiments, like LLMs negotiating, or aggregating their preferences and so on. And this is empirical. It has in some sense a very fast feedback loop. You can make experiments, you see how it goes.\n\nWe hope, both in case we succeed on the theory front, this would provide us with some playground where we can try things. But also we just want to stay in touch with the latest technology. And also, I think this is a topic I would wish more people worked on. If there were dozens of groups studying how you can have LLMs negotiate and know what are some desiderata for the negotiations, like how to make the process non-manipulative and similar things… I think there’s a very low bar in trying to start work on topics like that. You basically just need API access, you need to… we created in-house some framework, which hopefully makes it easier to run experiments like that in scale and do some housekeeping for you. It’s called [InterLab](https://acsresearch.org/interlab/stable/).\n\nI think there’s a low bar in trying to understand these interactions, but it’s just maybe at the moment not as popular a topic as some other directions. We are working on that as well. But we hope this will grow. It’s also adjacent to - I think another community/brand in this space is Cooperative AI and the [Cooperative AI Foundation](https://www.cooperativeai.com/foundation). And we’re also collaborating with them.\n\n**Daniel Filan:** Yeah, it seems like the kind of thing that outsiders have an comparative advantage in. Academic groups, this kind of research of trying to get language models to cooperate with each other, looking at their interactions… You can do it without having to train these massive language models. And I think there was some work done at my old research organization, CHAI - I’ll try to provide a link in the description of what I’m thinking about. [Work on getting language models to negotiate contracts for how they’re going to cooperate in playing Minecraft](https://social-dilemmas.github.io/).\n\n**Jan Kulveit:** I think there are many different setups which are interesting to look into. Specifically, we are sometimes looking into something like… you imagine, I don’t know, the humans delegate the negotiation to AIs. And then the question is: what happens? And I think this also will become empirically very relevant very soon, because people… I would expect this is actually already happening in the wild, it’s just not very visible. But you can imagine people’s, I don’t know, AI assistant negotiating with customer support lines, and these are often on the back end also a language model.\n\nAnd I think there are some interesting things which make it somewhat more interesting than just studying the ‘single user and single AI’ interaction. For example, if you are delegating your negotiation to your AI assistant, you don’t want your negotiator to be extremely helpful and obedient to the other negotiator. One of the toy models we use is car sales. If the other party’s bots tells your bot, “This is a really amazing deal. You just must buy it otherwise you’ll regret it,” you don’t want the LLM to just follow the instruction.\n\nAnd there are questions like… often we are interested in something like: how do the properties of the system scale with scaling models? I mean, I think there’s a lot of stuff where you can have a very basic question and you can get some empirical answer, and it’s not yet done.\n\n**Daniel Filan:** Sure. If listeners are interested in following your research or ACS’s research, how should they go about doing that?\n\n**Jan Kulveit:** Probably the best option is… One thing is we have a webpage, we have [acsresearch.org](https://acsresearch.org/). When we publish less formal blog posts and so on, we tend to cross-post them on the Alignment Forum or similar venues. One option for following our more informal stuff is just [follow me on the Alignment Forum](https://www.alignmentforum.org/users/jan-kulveit) or [LessWrong](https://www.lesswrong.com/users/jan-kulveit). We are also on [Twitter](https://x.com/acsresearchorg). And we also sometimes run events specifically for people communicating on the intersection with active inference and AI alignment. There’s some dedicated Slack to it. But overall, probably the standard means of following us on Twitter and Alignment Forum currently works best.\n\n**Daniel Filan:** All right. Well, thanks very much for coming here and talking to me.\n\n**Jan Kulveit:** Thank you.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nWhat’s the difference between a large language model and the human brain? And what’s wrong with our theories of agency? In this episode, I chat about these questions with Jan Kulveit, who leads the Alignment of Complex Systems research group.\n\nTopics we discuss:\n\n * What is active inference?\n * Preferences in active inference\n * Action vs perception in active inference\n * Feedback loops\n * Active inference vs LLMs\n * Hierarchical agency\n * The Alignment of Complex Systems group\n\nDaniel Filan: Hello, everybody. This episode, I’ll be speaking with Jan Kulveit. Jan is the co-founder and principal investigator of the Alignment of Complex Systems Research Group, where he works on mathematically understanding complex systems composed of both humans and AIs. Previously, he was a research fellow at the Future of Humanity Institute focused on macrostrategy, AI alignment, and existential risk. For links to what we’re discussing you can check the description of this episode and you can read the transcript at axrp.net. Okay. Well Jan, welcome to the podcast.\n\nJan Kulveit: Yeah, thanks for the invitation.\n\n\nWhat is active inference?\nDaniel Filan: I’d like to start off with this paper that you’ve published in December of this last year. It was called “Predictive Minds: Large Language Models as Atypical Active Inference Agents.” Can you tell me roughly what was that paper about? What’s it doing?\n\nJan Kulveit: The basic idea is: there’s active inference as a field originating in neuroscience, started by people like Karl Friston, and it’s very ambitious. The active inference folks claim roughly that they have a super general theory of agency in living systems and so on. And there are LLMs, which are not living systems, but they’re pretty smart. So we’re looking into how close the models actually are. Also, it was in part motivated by… If you look at, for example, the ‘simulators’ series or frame by Janus and these people on sites like the Alignment Forum, there’s this ",
      "wordCount": 15844
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "fp7AHLBpKB3EN4bLu",
        "name": "Free Energy Principle",
        "slug": "free-energy-principle"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "YAotJ9Le3S2rCJgf8",
        "name": "Predictive Processing",
        "slug": "predictive-processing"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "q6Tky4RzEmTwfGndB",
    "title": "AXRP Episode 31 - Singular Learning Theory with Daniel Murfet",
    "slug": "axrp-episode-31-singular-learning-theory-with-daniel-murfet",
    "url": null,
    "baseScore": 72,
    "voteCount": 26,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2024-05-07T03:50:05.001Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/hdB9gIwD6x4)\n\nWhat’s going on with deep learning? What sorts of models get learned, and what are the learning dynamics? Singular learning theory is a theory of Bayesian statistics broad enough in scope to encompass deep neural networks that may help answer these questions. In this episode, I speak with Daniel Murfet about this research program and what it tells us.\n\nTopics we discuss:\n\n*   [What is singular learning theory?](#what-is-slt)\n*   [Phase transitions](#phase-transitions)\n*   [Estimating the local learning coefficient](#estimating-the-llc)\n*   [Singular learning theory and generalization](#slt-and-generalization)\n*   [Singular learning theory vs other deep learning theory](#slt-vs-other-dl-theory)\n*   [How singular learning theory hit AI alignment](#how-slt-hit-ai-alignment)\n*   [Payoffs of singular learning theory for AI alignment](#payoffs-of-slt-for-ai-alignment)\n*   [Does singular learning theory advance AI capabilities?](#does-slt-advance-capabilities)\n*   [Open problems in singular learning theory for AI alignment](#open-problems-in-slt-for-alignment)\n*   [What is the singular fluctuation?](#what-is-the-singular-fluctuation)\n*   [How geometry relates to information](#how-geometry-relates-to-information)\n*   [Following Daniel Murfet’s work](#following-daniel-murfets-work)\n\n_In this transcript, to improve readability, first names are omitted from speaker tags._\n\n**Filan:** Hello, everybody. In this episode, I’ll be speaking with Daniel Murfet, a researcher at the University of Melbourne studying singular learning theory. For links to what we’re discussing, you can check the description of this episode and you can read the transcripts at axrp.net. All right, well, welcome to AXRP.\n\n**Murfet:** Yeah, thanks a lot.\n\nWhat is singular learning theory?\n---------------------------------\n\n**Filan:** Cool. So I guess we’re going to be talking about singular learning theory a lot during this podcast. So, what is singular learning theory?\n\n**Murfet:** Singular learning theory is a subject in mathematics. You could think of it as a mathematical theory of Bayesian statistics that’s sufficiently general with sufficiently weak hypotheses to actually say non-trivial things about neural networks, which has been a problem for some approaches that you might call classical statistical learning theory. This is a subject that’s been developed by a Japanese mathematician, [Sumio Watanabe](https://sites.google.com/view/sumiowatanabe/home), and his students and collaborators over the last 20 years. And we have been looking at it for three or four years now and trying to see what it can say about deep learning in the first instance and, more recently, alignment.\n\n**Filan:** Sure. So what’s the difference between singular learning theory and classical statistical learning theory that makes it more relevant to deep learning?\n\n**Murfet:** The “singular” in singular learning theory refers to a certain property of the class of models. In statistical learning theory, you typically have several mathematical objects involved. One would be a space of parameters, and then for each parameter you have a probability distribution, the model, over some other space, and you have a true distribution, which you’re attempting to model with that pair of parameters and models.\n\nAnd in regular statistical learning theory, you have some important hypotheses. Those hypotheses are, firstly, that the map from parameters to models is injective, and secondly (quite similarly, but a little bit distinct technically) is that if you vary the parameter infinitesimally, the probability distribution it parameterizes also changes. This is technically the non-degeneracy of the Fisher information metric. But together these two conditions basically say that changing the parameter changes the distribution changes the model.\n\nAnd so those two conditions together are in many of the major theorems that you’ll see when you learn statistics, things like the Cramér-Rao bound, many other things; asymptotic normality, which describes the fact that as you take more samples, your model tends to concentrate in a way that looks like a Gaussian distribution around the most likely parameter. So these are sort of basic ingredients in understanding how learning works in these kinds of parameterized models. But those hypotheses do not hold, it’s quite easy to see, for neural networks. I can go into more about why that is.\n\nSo the theorems just don’t hold. Now, you can attempt to make use of some of these ideas anyway, but if you want a thoroughgoing, deep theory that is Bayesian and describes the Bayesian learning process for neural networks, then you have to be proving theorems in the generality that singular learning theory is. So the “singular” refers to the breaking of these hypotheses. So the fact that the map from parameters to models is not injective, that means, in combination with this other statement about the Fisher information metric, that if you start at a neural network parameter, then there will always be directions you can vary that parameter without changing the input/output behavior, without changing the model.\n\nSome of those directions are kind of boring, some of them are interesting, but that’s what singular learning theory is about: accommodating that phenomenon within the space of neural networks.\n\n**Filan:** The way I’d understood it is that this basically comes down to symmetries in the neural network landscape. You can maybe scale down this neuron and scale up this neuron, and if neurons are the same, it doesn’t matter. But not only are there symmetries, there are non-generic symmetries.\n\n**Murfet:** Correct. Yeah.\n\n**Filan:** Because if there were just some symmetries, then maybe you could mod out by the symmetries… If you looked at the normal direction to the space at which you could vary things, then maybe that would be fine. So the way I’ve understood it is that there are certain parameter settings for neural networks where you can change it one way or you can change it another way, but you can’t change it in both directions at once. And there are other parameter settings where you can only change it in one of those two ways. So the fact that you can’t do them both at once means it’s not a nice, smooth manifold. And the fact that it’s different at different places means that it’s not this generic thing over the whole space. Some models are more symmetric than others and that ends up mattering.\n\n**Murfet:** Yeah, I would say that’s mostly correct. I would say the word ‘symmetry’ is really not… I think I would also at a high level maybe use this word to a first approximation in explaining what’s going on, but it’s really not a sufficient concept. But yeah, it’s good to distinguish the kind of boring generic symmetries that come from the non-linearities. So in some sense, that’s why you can just look at a neural network and know that it’s singular because of these symmetries, like the ReLU scaling up the input and scaling down the output weights respectively will not change the behavior of the network. So that’s an obvious scaling symmetry, and that means that it’s degenerate and therefore a singular model.\n\nBut if that was all there was, then I agree: somehow that’s a boring technical thing that doesn’t seem like you really need, from a point of view of understanding the real phenomena, to care about it too much. But the reason that SLT \\[singular learning theory\\] is interesting is that, as you say, different regions of parameter space, you could say have different kinds of symmetries as a reflection of the different ways qualitatively in which they’re attempting to model the true distribution. But this other thing you mentioned, about being able to move in different directions, that’s not really symmetry so much as degeneracy.\n\nSo we could go more into conceptually why different regions or different kinds of solutions might have different kinds of degeneracy, but at a high level that’s right. Different kinds of solutions have different kinds of degeneracy, and so being able to talk about different kinds of degeneracy and how they trade off against one another, and why Bayesian learning might prefer more or less degenerate kinds of models, is the heart of SLT.\n\n**Filan:** Sure. Before we go into that, what do you mean by “degeneracy”?\n\n**Murfet:** Degeneracy just refers to this failure of the map from parameters to models to be injective. So “a degeneracy” would just mean a particular kind of way in which you could vary the neural network parameter, say in such a way that the input/output map doesn’t change. And as you were just mentioning, you might have, at one point, two or more essentially different ways in which you could vary the parameter without changing the loss function. And that is by definition what geometry is. So what I’m describing there with my hand is the level set of the loss function. It might be the minimal level set or some other level set, but if we’re talking about multiple ways I can change the neural network parameter without changing the loss, then I’m describing the configuration of different pieces of the level set of the loss function at that point. And that’s what geometry is about.\n\n**Filan:** Sure. You mentioned that singular learning theory, or SLT for short, is very interested in different kinds of degeneracies. Can you tell us a little bit \\[about\\] what are the kinds of degeneracies, what different kinds of degeneracies might we see maybe in deep learning? And why does the difference matter?\n\n**Murfet:** I think it’s easier to start with a case that isn’t deep learning, if that’s all right. Deep learning jumps straight into the deep end in terms of… and it’s also the thing which we understand least, perhaps. But if you imagine the easiest kind of loss functions… and when I say loss function, I typically mean “population loss”, not the empirical loss from a fixed dataset of finite size, but the average of that of all datasets. So that’s somehow the theoretical object whose geometry matters here, so I’ll flag that, and there’s some interesting subtleties there.\n\nSo in a typical case, in a regular statistical setting - not neural networks, but linear regression or something - the population loss looks like a sum of squares, so just a quadratic form. And there, minimizing it - I mean maybe with some coefficients, the level sets are ellipses - then the learning process just looks like moving down that potential well to the global minimum. And that’s kind of all that’s happening. So in that case, there’s no degeneracy. So there’s just one global minimum and you can’t vary it at all and still have zero loss.\n\nA more interesting case would be where: suppose you have 10 variables, but a sum of eight squares, so x1² through x8². And then if you minimize that, well, you’ve still got two free parameters, so there’s a two-dimensional space of global minima of that function. Now imagine a population loss, and let’s only care about local minima, which has many local minima at various heights of the loss, each of which use different numbers of variables. So we suppose, for instance, that the global minimum maybe uses all 10, but then there’s a level set a bit higher than that that uses only nine squares, and a level set a bit higher than that that uses only eight squares. And so then those have different amounts of degeneracy.\n\nSo you have different points in the parameter space, loss landscape, where local minima have different degrees of degeneracy. And so then you can think about the competition between them in terms of trading off between preference for degeneracy versus preference for loss. And then we’re getting into key questions of if you’re a Bayesian, what kind of solution you prefer in terms of accuracy versus degeneracy.\n\n**Filan:** And I guess this gets to this object that people talk about in singular learning theory called the “learning coefficient”. Can you tell us a little bit about what the learning coefficient is?\n\n**Murfet:** In the case I was just describing, it’s easy to say what the learning coefficient is. There’s a distinction between global learning coefficient… Everything I say about SLT, more or less, is material that was introduced by Watanabe and written about in [his](https://www.cambridge.org/core/books/algebraic-geometry-and-statistical-learning-theory/9C8FD1BDC817E2FC79117C7F41544A3A) [books](https://www.routledge.com/Mathematical-Theory-of-Bayesian-Statistics/Watanabe/p/book/9780367734817), and at some point, I guess we’ll talk about our contributions more recently. But mostly what I’m describing is not my own work, just to be clear.\n\nSo I’ll mostly talk about the local learning coefficient, which is a measure of degeneracy near a point in parameter space. If I take this example I was just sketching out: you imagine the global minimum level set and then some higher level sets. And I said that the population loss near the global minimum looked like a sum of 10 squares. And so the local learning coefficient of that would just be 10/2, so a half times the number of squares that you used.\n\nSo if there was a level set that had used only eight squares, then that’s degenerate, because you have two free directions, so it’s not a single isolated minimum, but rather a sort of two-dimensional plane of minimum. And each point of that two-dimensional plane would, because it locally looks like a sum of eight squares, have 8/2 as its local learning coefficient and so on. So if you use D’ squares in the local expression of your population loss, then your local learning coefficient is D’/2. That’s not how it’s defined: it has a definition, which we could get into various different ways of looking at it, but that’s what it cashes out to in those examples.\n\n**Filan:** Sure. I guess the way to think about this local learning coefficient is that when it’s lower, that’s a solution that’s more degenerate. And the way I gather Bayesian inference works is that it tries to have both a low loss and also a low local learning coefficient. Does that sound right?\n\n**Murfet:** Yep.\n\n**Filan:** An image I often see in discussions of singular learning theory is people drawing doodles of trefoils and figure eights and maybe a circle to throw in there. The thing I often hear (as a caricature) is: initially you stay around the trefoil for a while, this is where you put your posterior mass, until at some point you get enough data and then you start preferring this figure eight, and then you get even more data and then you start preferring this circle, which has maybe even lower loss. So as you go down, maybe you get better loss, let’s just say, but the local learning coefficient is going to increase and therefore get worse.\n\n**Murfet:** Maybe I’ll caveat that a little: the local learning coefficient is increasing, so you’re accepting a more complex solution in exchange for it being more accurate.\n\nPhase transitions\n-----------------\n\n**Filan:** Yeah. So that’s the very basic idea of singular learning theory. Why does it matter? What are the important differences between the singular learning theory picture and the classical statistical learning theory picture?\n\n**Murfet:** In what context? Statistical learning theory in general, deep learning theory, alignment, or all three in that order?\n\n**Filan:** Maybe all three in that order. I think I want to put off the discussion of alignment relevance for a little bit later until we just understand what’s going on with this whole thing.\n\n**Murfet:** Okay. Yeah, I guess I didn’t actually come back to your question about the local learning coefficient in neural networks from earlier, but I think the cartoon in terms of sums of squares might still suffice for the moment.\n\nIf we talk about statistical learning theory in machine learning or deep learning in general, I think the main high-level conceptual takeaway from singular learning theory when you first encounter it should be that the learning process in Bayesian statistics really is very different for singular models. So let me define what I mean by “learning process”.\n\nWhen we say “learning process” in deep learning, we tend to mean training by stochastic gradient descent. And what I’m saying is maybe related to that, but that’s a tricky point, so let me be clear that in Bayesian statistics, the “learning process” refers to: as you see more data, you change your opinion about what the relative likelihood of different parameters is. So you see more data, some parameters become ruled out by that data because they don’t give that data high probability, whereas other parameters become more likely. And what I’m describing is the Bayesian posterior, which assigns a probability to each parameter according to the data.\n\nAnd so as you see more samples… I mean, if you’ve seen very few samples, you really have no idea which parameters are correct, so the posterior is very diffuse and will change a lot as you see more samples because you just are very ignorant. But asymptotic normality and regular statistical learning theory says that as you see more samples, that process starts to become more regular and concentrate around the true parameter in a way that looks like a Gaussian distribution.\n\nSo that’s in some sense a very simple process. But in singular models, that is not what happens, at least that’s not what’s predicted to happen by the theory. Until relatively recently, I think we didn’t have many very compelling examples of this in practice. But what the theory says is what you were describing earlier, that the Bayesian posterior should kind of jump as the trade-off between accuracy and complexity changes, which is a function of the number of samples. And those jumps move you from regions of qualitatively different solutions to other kinds of solutions, and then eventually maybe asymptotically to even choosing among perfect solutions depending on their complexity and then so on.\n\nSo there’s a very complicated, not very well-understood process underlying learning in Bayesian statistics for singular models, which as far as I know, Watanabe and his collaborators are the only people to ever really study. This is despite being somewhat old, in the sense that Watanabe and students and collaborators have been working on it for a while; it’s really not been studied in great depth outside of their group.\n\nSo \\[it’s\\] a very fundamental process in Bayesian statistics, relatively understudied, but arguably, at least if you take a Bayesian perspective, very central to how learning works in (say) neural networks, whether they’re artificial ones or even possibly biological ones.\n\nSo I think that’s the main thing. I mean, that’s not the only thing singular learning theory talks about. It’s not the only theoretical content, but I would say that’s the main thing I would want someone to know about the theory as it stands right now. The other thing is how that relates to generalization, but maybe I’ll pause there.\n\n**Filan:** Sure. Maybe we should talk about that a bit. I hear people talk about this with the language of phase transitions. And I think upon hearing this, people might say, “Okay, if you look at loss curves of big neural nets that are being trained on language model data, the loss kind of goes down over time, and it doesn’t appear to be stuck at one level and then suddenly jump down to another level and then be flat and then suddenly jump down.” We have things which kind of look like that in toy settings, like [grokking](https://arxiv.org/abs/2201.02177), like the development of [induction heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html), but it doesn’t generically happen. So should we think of these phase transitions as being relevant to actual deep learning, or are they just a theoretical curiosity about the Bayesian theory?\n\n**Murfet:** Yeah, I think that’s a very reasonable question. I think a year ago, we ourselves were skeptical on this front. I think even in toy settings it wasn’t very clear that this theoretical prediction bears out. So maybe I’ll spend a moment to just be quite precise about the relationship between theory and practice in this particular place.\n\nWhat the theory says is: asymptotically in N, the number of samples, a certain formula describing the posterior works, and then based on this formula, you can have the expectation that phase transitions happen. But in principle, you don’t know lower-order terms in the asymptotic, and there could be all sorts of shenanigans going on that mean that this phenomenon doesn’t actually occur in real systems, even toy ones. So theory on its own - I mean in physics or in machine learning or whatever - has its limits, because you can’t understand every ingredient in an asymptotic expansion. So even in toy settings, it was reasonable, I think, to have some skepticism about how common this phenomenon was or how important it was, even if the theory is quite beautiful.\n\nOkay, so that aside, you go and you look in toy systems and you see this behavior, as we did, and then I think it’s reasonable to ask, “Well, okay, so maybe this happens in small systems, but not in large systems?” And indeed in learning curves, we don’t think we see a lot of structure.\n\nSo I’ll tell you what we know, and then what I think is going on. I should preface this by saying that actually we don’t know the answer to this question. So I think that it still remains unclear if this prediction about phases and phase transitions is actually relevant to very large models. We’re not certain about that. I would say there’s a reasonable case for thinking it is the case that it is relevant, but I want to be clear about what we know and don’t know.\n\nAgain, this is kind of an empirical question, because the theoretical situation under which phases and phase transitions exist… the theory stops at some point and doesn’t say much at the moment about this scale or that scale.\n\nSo what we know is that if you look at transformers around the scale of three million parameters, trained on language model datasets, [you do see something like phases and phase transitions](https://arxiv.org/abs/2402.02364) that basically describe… So again, what I’m about to describe is the learning process of training rather than seeing more samples. But the theoretical jump that we’re making here is to say, okay, if Bayesian statistics says certain kinds of structures in the model - if the theory says there should be qualitative changes in the nature of the way the posterior is describing which models are probable, if there are qualitative changes in that over the course of the Bayesian learning process, as you see more samples, then you might expect something similar when you go and look at seeing cumulatively more examples through the training process of stochastic gradient descent. But that is not a theoretically justified step at this point in some rigorous sense. That’s the kind of prediction you might make assuming some similarity between the learning processes, and then you can go in empirically and see if it’s true.\n\nSo if you go and look at language models at the scale of three million parameters… This is a recent paper that we did, [Developmental Landscape of In-Context Learning](https://arxiv.org/abs/2402.02364). If you go and look at that, what you see \\[is\\] that the training process is divided into four or five stages, which have different qualitative content in a way that isn’t visible in the loss curve mostly.\n\n**Filan:** It is a little bit visible.\n\n**Murfet:** Yeah, I would agree with that. I mean, to the same extent that the induction bump is sort of visible in the original [in-context learning and induction heads paper](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html).\n\n**Filan:** Yeah. I mean, it’s not obvious from the loss curve. It’s not like everybody already knew all the things that you guys found out.\n\n**Murfet:** Yeah, I would say that without these other results, if you looked at the loss curve and tried to tell the story about these little bumps, it would feel like tea leaf reading. But once you know that the stages are there, yes, you can look at the loss curve and sort of believe in certain features of them.\n\nSo I mean, there’s various details about how you think about the relationship between those stages and phases and phase transitions in a sense of SLT. But I would say that’s still a very small model, but not a toy model, in which you do see something like stage-wise development.\n\nAnd there are independent reasons… People have independently been talking about stage-wise development in learning systems outside of SLT. So I would say that the SLT story and stage-wise development as a general framing for how structure arrives inside self-organizing learning processes, that dovetails pretty well. So I would say that, to come back to your question about structure in the loss curve, just because nothing’s happening in the loss curve doesn’t mean that there isn’t structure arriving in stages within a model. And our preliminary results on GPT-2 Small at 160 million parameters: at a high level it has stages that look pretty similar to the ones in the three million parameters.\n\n**Filan:** Interesting.\n\n**Murfet:** So here’s my guess for what’s going on. It’s true that in very large models, the system is learning many things simultaneously, so you won’t see very sharp transitions except possibly if they’re very global things: \\[e.g.\\] switching to in-context learning as a mode of learning seems like it affects most of the things that a system is learning, so a qualitative change at that scale, maybe you would guess actually is represented sort of at the highest level and might even be visible in the loss curve, in the sense that everything is coordinated around that. There’s before and after.\n\nBut many other structures you might learn, while they’re developing somewhere else in the model, it’s memorizing the names of U.S. presidents or something, which just has nothing to do with structure X, Y, Z. And so in some sense, the loss curve can’t possibly hit a plateau, because even if it’s hitting a critical point for these other structures X, Y, Z, it’s steadily making progress memorizing the U.S. presidents. So there can’t be clear plateaus.\n\nSo the hypothesis has to be something like: if there is stage-wise development, which is reflected by these phases and phase transitions, it’s in some sense or another localized, maybe localized to subsets of the weights and maybe localized in some sense to certain parts of the data distribution. So the global phases or phase changes which touch every part of the model and affect every kind of input are probably relatively rare, but that isn’t the only kind of phase, phase transition, stage to which Bayesian statistics or SLT could apply.\n\n**Filan:** Sure. Should I imagine these as being sort of singularities in a subspace of the model parameter space? The learning coefficient kind of picks them out in this subspace, but maybe not in the whole parameter space?\n\n**Murfet:** Yeah, that’s kind of what we’re thinking. These questions are pushing into areas that we don’t understand, I would say. So I can speculate, but I want to be clear that some parts of this we’re rather certain of: the mathematical theory is very solid, the observation of the correspondence between the theory and Bayesian phase transitions in toy models is empirically and theoretically quite solid. This question of what’s happening in very large systems is a deep and difficult question. I mean, these are hard questions, but I think that’s right, that’s the motivation for… One of the things we’re currently doing is what we call weight-restricted local learning coefficients. This basically means you take one part of the model, say, a particular head, you freeze all the other weights…\n\nLet me just give a more formal setting. When we’re talking about the posterior and the local learning coefficient and so on, we imagine a space of parameters. So there’s D dimensions or something. Some of those directions in parameter space belong to a particular head, and I want to take a parameter that, at some point in training, has some values for all these heads, I mean, for all these different weights, and I want to freeze all but the ones in the head and then treat that as a new model. Now, my model is I’m not allowed to change those weights, but I’m allowed to change the weights involved in the head, and I can think about the Bayesian posterior for that model and I can talk about its local learning coefficient.\n\nThat involves perturbing the parameter nearby that particular coefficient, but in a way where you only perturb the weights involved in that part of the structure, say, that head, and you can define the complexity of that local learning coefficient. That’s what we call the weight-restricted local learning coefficient. And then the hypothesis would be that, if a particular part of the model is specializing in particular kinds of structure and that structure is developing, then you’ll be at a critical point for some kind of restricted loss that is referring only to those weights, and that would show up.\n\nWe haven’t talked about how the local learning coefficient is used to talk about phase transitions, but that’s the experimental way in which you’d attempt to probe whether some part of the model is doing something interesting, undergoing a phase transition separately from other parts of the model.\n\n**Filan:** Yeah, actually, maybe we should clarify that. How do you use the learning coefficient to figure out if a phase transition is happening?\n\n**Murfet:** It depends on your background which answer to this question is most pleasant. For physics-y people who know about free energy, they’re familiar with the idea that various derivatives of the free energy should do something discontinuous at a phase transition, and you can think about the local learning coefficient as being something like that. So that, if there is a phase transition, then you might expect this number to change rapidly relative to the way it usually changes.\n\nIf we just stick within a statistical learning theory frame, we were laying out this picture earlier of: as you see more samples, the Bayesian posterior is concentrated in some region of parameter space and then rapidly shifts to be concentrated somewhere else, and the local learning coefficient is a statistic of samples from the Bayesian posterior, so if the Bayesian posterior shifts, then this number will also shift. The expectation would be that, if you measure this number, which it turns out you can do from many experiments, if you see that number change in some significant way, then it is perhaps evidence that some qualitative change in the posterior has occurred. That’s a way of detecting phase transitions which is, if you take this bridge from Bayesian statistics to statistical physics, pretty well justified I would say.\n\nEstimating the local learning coefficient\n-----------------------------------------\n\n**Filan:** Sure. A question about that: my understanding is that trying to actually measure the local learning coefficient involves taking a parameter setting and looking at a bunch of parameter settings nearby on all these dimensions that you could vary it, and measuring a bunch of properties, and this is the kind of thing that’s easy to do when you have a very low-dimensional parameter space corresponding to a small number of parameters. It seems like it’s going to be harder to do with a higher number of parameters in your neural networks. Just practically, how large a model can you efficiently measure local learning coefficient \\[for\\] at this time?\n\n**Murfet:** Yeah. That’s a good question. I think it’s tricky. Maybe this will be a bit of an extended answer, but I think it’ll be better if I provide some context. When we first started looking at SLT, myself and my colleague here at the University of Melbourne, [Susan Wei](https://www.suswei.com/), and some other people… This was before… believe it or not, today there are 10x the number of people interested in SLT than there were back when we started thinking about it. It was an extremely niche subject, very deep and beautiful, but somewhat neglected.\n\nOur question at that time was exactly this question. The theory says the local learning coefficient - the “real log canonical threshold” is another mathematical name for it - the theory says this is a very interesting invariant, but it’s very unclear if you can accurately estimate it in larger models. A lot of the theoretical development \\[involved using\\] one PhD student to compute the RLCT of one model theoretically, and you need some hardcore algebraic geometry to do that, et cetera, et cetera. The way the subject sat, it wasn’t clear that you could really be doing this at scale because it seems to depend on having very accurate samples from the posterior via Markov Chain Monte Carlo sampling or something.\n\nI admit, I was actually extremely pessimistic when we first started looking at it that there really would be a future in which we’d be estimating RLCTs, or local learning coefficients, of a hundred million parameter models. So that’s where I started from. My colleague Susan and my PhD student Edmund Lau decided to try SGLD, stochastic gradient Langevin dynamics, which is an approximate Bayesian sampling procedure based on using gradients, to see how it worked. There’s a step in estimating the local learning coefficient where you need samples from the posterior. As you’re describing, this is famously difficult for large dimensional complex models.\n\nHowever, there is a possible loophole, which is that… I mean, I don’t believe that anybody has a technique, nor probably ever will, for understanding or modeling very accurately the Bayesian posterior of very large-scale models like neural networks. I don’t think this is within scope, and I’m skeptical of anybody who pretends to have a method for doing that, hence why I was pessimistic about estimating the LLC \\[local learning coefficient\\] at scale because it’s an invariant of the Bayesian posterior which seems to have a lot of information about it and I believe it’s hard to acquire that information. The potential loophole is that maybe the local learning coefficient relies on relatively robust signals in the Bayesian posterior that are comparatively easy to extract compared to knowing all the structure.\n\nThat seems to be the world that we are in. To answer your question, [Zach Furman](https://zachfurman.com/) and Edmund Lau just recently had [a pre-print](https://arxiv.org/abs/2402.03698) out where, using SGLD, it seems you can get relatively accurate estimates for the local learning coefficient for deep linear networks: products of matrices and known nonlinearities at scales up to a hundred million parameters.\n\n**Filan:** A hundred million with an M?\n\n**Murfet:** With an M, yeah. One should caveat that in several ways, but yeah.\n\n**Filan:** Okay, and am I right that this is distinct from the [“Quantifying degeneracy with the local learning coefficient”](https://arxiv.org/abs/2308.12108) paper?\n\n**Murfet:** That’s right. This is a second paper, a followup to that. I forget the title. I think it’s [Estimating Local Learning Coefficient at Scale](https://arxiv.org/abs/2402.03698). So we wrote that paper a couple of years ago now, I think, looking at defining the local learning coefficient - which is implicit in Watanabe’s work, but we made it explicit - and making the observation that you could use approximate sampling to estimate it and then studying that in some simple settings, but it remained very unclear how accurate that was in larger models.\n\nNow, the reason it’s difficult to go and test that is because we don’t know the true local learning coefficient for very many models that can be increased in some direction of scale. We know it for one hidden layer tanh networks and things like that. But [some recent, very deep, interesting work](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4404877) by Professor Miki Aoyagi gives us the true value of the local learning coefficient for deep linear networks, which is why Zach and Edmund studied those. This was an opportunity to see if SGLD is garbage or not for this purpose.\n\nI should flag that despite… How should I say this? SGLD is a very well-known technique for approximate Bayesian posterior sampling. I think everybody understands that you should be skeptical of how good those posterior samples are in some sense. It might be useful for some purpose, but you shouldn’t really view it as a universal solvent for your Bayesian posterior sampling needs or something. Just using SGLD doesn’t magically mean it’s going to work, so I would view it as quite surprising to me that it actually gives accurate estimates at scale for deep linear networks.\n\nNow, having said that, deep linear networks are very special, and they are less degenerate in some important ways than real neural networks with nonlinearities, et cetera, so don’t take me as saying that we know that local learning coefficient estimation gives accurate values of the local learning coefficient for language models or something. We have basically no idea about that, but we know it’s accurate in deep linear networks.\n\nOkay, so then what is generalizable about that observation? I think it leads us to believe that maybe estimating the LLC, SGLD is actually not garbage for that. How good it is we still don’t know, but maybe this cheap posterior sampling is still good enough to get you something interesting. And then the other thing is that: well, what you observe in cases where you know the true values is that, when the model undergoes phase transitions which exist in deep linear networks, as [many people have](https://arxiv.org/abs/2106.15933)… Maybe not in those exact terms, but, stage-wise development in deep linear networks has been studied for quite a long time, and you can see that this local learning coefficient estimator which is measuring the complexity of the current parameter during the learning process does jump in the way you would expect in a phase transition, when deep linear networks go through these phase transitions.\n\nWell, it had to, because we know theoretically what’s happening to the geometry there. Those jumps in the local learning coefficient in other models, like these 3 million parameter language models or GPT-2 Small… when you go and estimate the local learning coefficient, you see it change in ways that are indicative of changes in internal structure. Now, we don’t know that the absolute values are correct when we do that, and most likely they’re not, but I think we believe in the changes in the local learning coefficient reflecting something real to a greater degree than we believe in the absolute values being real. Still, theoretically, I don’t know how we would ever get to a point where we would know the local learning coefficient estimation was accurate in larger models absent really fundamental theoretical improvements that I don’t see coming in the near term, but that’s where we are at the moment.\n\nSingular learning theory and generalization\n-------------------------------------------\n\n**Filan:** Fair enough. A while back, you mentioned the contributions of singular learning theory to understanding deep learning. There was something to do with phase transitions and there was also something to do with generalization, I think you mentioned. I want to ask you about that. Especially in the context of: I sometimes hear people say, “Oh, statistical learning theory says that model classes can have these parameters that have some degeneracy and that basically reduces their effective parameter count, and this just explains how generalization is possible.” This is the kind of story one can tell when one feels excitable, but it’s a bit more complicated. It’s going to depend on details of how these parameters actually translate into functions and what these degeneracies actually look like in terms of predictive models. What does singular learning theory tell us about generalization, particularly in the context of deep networks?\n\n**Murfet:** Yeah. This is subtle. On its face, \\[in\\] singular learning theory, the theorems describe relations between loss, local landscape geometry, this local learning coefficient, and generalization error in the Bayesian sense. In the Bayesian sense, what I mean by generalization error is the KL divergence between the true distribution and the predictive distribution.\n\nMaybe I should say briefly what the latter is. If you’re trying to make a prediction, if you’re talking about a conditional distribution, a prediction of Y given X, and you look at all the parameters that you’ve got for modeling that relationship, and you’re given an input and you take the prediction from every single model parameterized by your parameter space, you weight it with the probability given to that particular model by the Bayesian posterior and you average them all in that way, that’s the Bayesian predictive distribution. \\[It’s\\] obviously radically intractable to use that object or find that object. It’s a theoretical object. That probability distribution is probably not one that’s parameterized by parameters in your parameter space, but you can cook it up out of models in your parameter space. The KL divergence between that and the truth is the Bayesian generalization error.\n\n**Filan:** The KL divergence just being a measure of how different probability distributions are.\n\n**Murfet:** Right. That seems like a very theoretical object. There’s a closely related object, the Gibbs generalization error, which puts some expectations in different orders which is closer to what people in machine learning mean by “test error” - taking a parameter and trying it out on some samples from the true distribution that weren’t used to produce that parameter. There’s the various subtleties there. SLT, strictly speaking, only says things about those kinds of generalization errors and the relationship between that and test error for a parameter produced by a single run of SGD - well, I don’t even know that that is a mathematical object actually (test error for a parameter after a single run), but you can do things like talk about, for some distribution of SGD runs, what’s the expected test error.\n\nThen there’s a gap between that Bayesian story and what you mean by “test error” in deep learning. This gap hasn’t been very systematically addressed, but I’ll lay out some story about how you might bridge that eventually in order to answer your question. If you believe that the Bayesian learning process ends with a distribution of parameters that look something like the endpoints of SGD training, or at least close enough, that something like this average of SGD runs of the test error looks a bit like averaging over things in the Bayesian posterior of some generalization quantity that makes sense in the Bayesian theory, then you could maybe draw some connection between these two things.\n\nThat hasn’t been done. I don’t know if that’s true, because these questions about relations between the Bayesian posterior and SGD are very tricky and I don’t think they look like they’re going to get solved soon, at least in my opinion. There’s a gap there. That’s one gap. We just paper over that gap and just say, “Okay. Well, fine, let’s accept that for the moment and just treat the generalization error that SLT says things about as being the kind of generalization error that we care about. What does SLT say?”\n\nMaybe I’ll insert one more comment about that relationship between test error in deep learning and Bayesian generalization error first. This is a bit of a tangent, but I think it’s important to insert here. Various people, when looking to explain the inductive bias of stochastic gradient descent, have hit upon a phenomenon that happens in deep linear networks and similar systems, which is a stage-wise learning where the model moves through complexity in an increasing way.\n\nWe think about in deep linear networks - or what’s sometimes called matrix factorization, where you’re trying to use a product of matrices to model a single linear transformation - [people have observed](https://arxiv.org/abs/2106.15933) that, if you start with a small initialization, the model starts with low rank approximations to the true linear transformation and then finds a pretty good low rank approximation and then takes a step to try and use linear transformations of one higher rank and so on, and moves through the ranks in order to try and discover a good model. Now, if you believe that, then you would believe that, if SGD training is doing that, then it will tend to find the simplest solution that explains the data, because it’s searching them starting with simpler ones and only going to more complicated ones when it needs to.\n\nNow, theoretically, that’s only known to happen… I mean, I think it’s not known to happen in deep linear networks rigorously speaking, but there’s expectations of that, \\[and\\] empirically, that happens, and there’s some partial theory. Then it’s a big leap to believe that for general SGD training of general neural networks, so I think we really don’t know that that’s the case in general deep learning. Believing that is pretty similar to believing something about the Bayesian learning process moving through regions of parameter space in order of increasing complexity as measured by the local learning coefficient. In fact, that is exactly what’s happening in the deep linear networks.\n\nThe SLT story about moving through the parameter space and the Bayesian posterior undergoing phase transitions is exactly what’s happening in the deep linear networks. If you’re willing to buy that generalization from that corner of theory of deep learning to general behavior of neural networks, then I think you are in some sense already buying the SLT story to some degree, \\[the story\\] of how learning is structured by looking for increasingly complex solutions. All of those are big question marks from a theoretical point of view, I would say.\n\nPutting that aside, what does SLT say about generalization? Well, it says that the asymptotic behavior of the generalization error as a function of the number of samples at the very end of training, let’s say, or the very end of the Bayesian learning process, looks like the irreducible loss plus a term that looks like lambda/N, where lambda is the local learning coefficient. If you take that irreducible loss over the other side, the difference between generalization error and its minimum value behaves like 1/n, is proportional to 1/n, and the constant of proportionality is the local learning coefficient. That’s the deep role of this geometric invariant, this measure of complexity in the description of generalization error in the Bayesian setting.\n\nNow, what that says in deep learning… as I said, taking that first part of that bridge between the two worlds for granted, it would like to say something like: the test error when you’re looking at a particular region of parameter space is governed by the local learning coefficient, except that the relation between N and training is unclear. The exact way in which it governs test error is a function of how that bridge gets resolved. I think, at a technical level, it’s difficult to say much precise at the moment. I don’t think it’s impossible. It’s just that very few people are working on this and it hasn’t been getting enough attention to say more concrete things.\n\nAt a conceptual level, it says that - and this maybe starts to get into more interesting future work you can do taking the SLT perspective - but this relationship between the local learning coefficient and how that is determined by loss landscape geometry and generalization behavior, this is a very interesting link which I think is quite fundamental and interesting.\n\nI think your question is going in the direction of [Joar Skalse](https://scholar.google.com/citations?user=GuzLUmQAAAAJ&hl=en)’s [LessWrong post](https://www.lesswrong.com/posts/ALJYj4PpkqyseL7kZ/my-criticism-of-singular-learning-theory). Is that right?\n\n**Filan:** That’s what I was inspired by: just this question of, suppose we believe the story of, we’re gradually increasing complexity as measured by the local learning coefficient in this model class: well, what does that actually say in terms of objects that I cared about before I heard of singular learning theory? What’s that telling me in terms of things I care about, of the behavior of these things?\n\n**Murfet:** It could tell you things like: suppose you know two solutions of your problem that are qualitatively different. You have a data-generating process and you can think about it in two different ways and, therefore, model it in two different ways. Potentially, if you could estimate the local learning coefficient or derive it or have some method of knowing that one is lower than the other, it could tell you things like one will be preferred by the Bayesian posterior.\n\nNow, to the extent that that is related to what SGD finds, that might tell you that training is more likely to prefer some class of solutions to another class. Now, if those parameters are just very different, completely different solutions, somehow not nearby in parameter space, maybe it’s quite difficult to make the bridge between the way the Bayesian posterior would prefer one or the other and what training will do because, in that case, the relationship between training and these two parameters is this very global thing to do with the trajectory of training over large parts of the parameter space, and very difficult perhaps to translate into a Bayesian setting.\n\nIn cases where you have two relatively similar solutions, maybe you had a choice to make. So during the training process, you had one of two ways to take the next step and accommodate some additional feature of the true distribution, and those two different choices differed in some complexity fashion that could be measured by the local learning coefficient: one was more complex, but lowered the loss by so much, and the other one was simpler, but didn’t lower the loss quite as much. Then you could make qualitative predictions for what the Bayesian posterior would prefer to do, and then you could ask, “Are those predictions also what SGD does?” Either, theoretically, you could try and find arguments for why that is true, but it \\[also\\] gives you an empirical prediction you can go and test.\n\nIn this [toy model of superposition work we did](https://arxiv.org/abs/2310.06301), SGD training does seem to do the thing that the Bayesian posterior wants to do. That’s very unclear in general, but it gives you pretty reasonable, grounded predictions that you might then go and test, which I think is not nothing. That would be, I think, the most grounded thing you’d do with the current state of things.\n\n**Filan:** I guess it suggests a research program of trying to understand which kinds of solutions do have a lower learning coefficient, which kinds of solutions have higher learning coefficients, and just giving you a different handle on the problem of understanding what neural network training is going to produce. Does that seem fair?\n\n**Murfet:** Yeah. I think, \\[for\\] a lot of these questions about the relation between the theory and practice, our perspective on them will shift once we get more empirical evidence. What I expect will happen is that these questions seem to loom rather large when we’ve got a lot of theory and not so much empirical evidence. If we go out and study many systems and we see local learning coefficients or restricted local learning coefficients doing various stage-wise things and they correspond very nicely to the structure that’s developing, as we can test independently with other metrics, then I think it will start to seem a little bit academic whether or not it’s provably the case that SGD training does the same thing as the Bayesian posterior just because this tool, which…\n\nTo be clear, the local learning coefficient, if you look at the definition, has a sensible interpretation in terms of what’s happening to the loss as you perturb certain weights, and you can tell a story about it, it doesn’t rely on the link between the Bayesian posterior and SGD training or something. To the degree that the empirical work succeeds, I think people will probably take this independent justification, so to speak, of the LLC as a quantity that is interesting, and think about it as a reflection of what’s happening to the internal structure of the model. Then, the mathematicians like myself will still be happy to go off and try and prove these things are justified, but I don’t see this as necessarily being a roadblock to using it quite extensively to study what’s happening during training.\n\nSingular learning theory vs other deep learning theory\n------------------------------------------------------\n\n**Filan:** Fair enough. I’d like to ask some questions thinking about SLT as compared to other potential theoretical approaches one could have to deep learning. The first comparison I have is to neural tangent kernel-style approaches. The neural tangent kernel, for listeners who don’t know, is basically this observation that, in the limit of infinitely wide neural networks under a certain method of initializing networks, there’s this observation that networks, during training, the parameters don’t vary very much and, because the parameters don’t vary very much, that means you can do this mathematical trick. It turns out that your learning is basically a type of kernel learning, which is essentially linear regression on a set of features. Luckily, it turns out to be an infinite set of features and you can do it…\n\nI don’t know how I was going to finish that sentence, but it turns out to be feature learning on this set of features, and you can figure out what those features are supposed to be based on what your model looks like, what kinds of nonlinearities you’re using. There’s some family of theory trying to understand: what does the neural tangent kernel of various types of models look like, how close are we to the neural tangent kernel?\n\nAnd if you believe in the neural tangent kernel story, you can talk about: the reason that neural networks generalize is that the neural tangent kernel, it tends to learn certain kinds of features before other kinds of features, and maybe those kinds of features are simpler. It seems plausible that you could do some story about phase transitions, and it’s a mathematically rigorous story. So I’m wondering, how do you think the single learning theory approach of understanding deep learning compares to the neural tangent kernel-style approach?\n\n**Murfet:** Yeah, good question. I think I’m not an expert enough on the NTK \\[neural tangent kernel\\] to give a very thorough comparison, but I’ll do my best. Let me say first the places in which I understand that the NTK says very deep and interesting things. It seems that this work on the [mu parametrization](https://arxiv.org/abs/2203.03466) seems very successful. At initialization, when this “taking the limit to infinite width” is quite justified because the weights really are independent, this seems like probably the principal success of deep learning theory, to the extent there are any successes: the study of that limit and how it allows you to choose hyperparameters for learning rates and other things. Again, I’m not an expert, but that’s my understanding of how it’s used, and that seems to be quite widely used in practice, as far as I know. So that’s been a great success of theory.\n\nI don’t think I believe in statements outside of that initial phase of learning though. I think there, as far as I understand it, the claims to applicability of the NTK methods become hypotheses, unless you then perturb away from the Gaussian process limit. The deep [parts](https://arxiv.org/abs/2106.10165) of that literature seem to me to be accepting the position that in the infinite width limit, you get some Gaussian process that isn’t actually a good description of the training process away from initialization, but then you can perturb back in basically higher-order terms in the exponent of some distribution. You can put in higher-order terms and study systematically those terms to get back to finite width, attempt to perturb away from infinite width back to finite width and accommodate those contributions in some fashion. And you can do that with tools from random matrix theory and Gaussian processes.\n\nAnd that looks a lot like what people do in Euclidean quantum field theory, and so people have been applying techniques from that world to do that. And I think they can say non-trivial things, but I think it is overselling it to say that that is a theory on the same level of mathematical rigor and depth as SLT. So I don’t think it says things about the Bayesian posterior and its asymptotics, in the way that SLT does, I think it’s aiming at rather different statements. And I think, at least in my judgment at the moment, it has a little bit of the flavor of saying qualitative things rather than quantitative things. Again, this is my outsider’s impression, and I could be wrong about what the state of things is there.\n\nBut I would say that one part of that story that I have looked at a little bit is the work that my colleague, [Liam Hodgkinson](https://www.liamhodgkinson.com/) has done here. They have some very interesting recent work on [information criterion in over-parameterized models](https://arxiv.org/abs/2307.07785) \\- I think the title is something like that. \\[It’s\\] partly inspired by Watanabe’s work, I think, looking at trying to take, not only NTK, but this general sort of approach, point of view to doing things like what the free energy formula in SLT does. And so I think that’s quite interesting. I have my differences of opinion with Liam about some aspects of that, but mathematics isn’t actually divided into camps that disagree with one another or something, right?\n\nSo if things are both true, then they meet somewhere. And I can easily imagine that… SLT is sort of made up of two pieces, one of which is using resolution of singularities to do Laplace integrals, oscillatory integrals, and the other is dealing with empirical processes that intervene in that when you try to put it in the context of statistics. And I don’t think these kinds of oscillatory integrals, these techniques, have been used systematically by the people doing NTK-like stuff or Euclidean field theory-like stuff, but I think that if you took those techniques and used them in the context of the random matrix theory that’s going on there, you’d probably find that the perturbations that they’re trying to do can be linked up with SLT somewhere. So I mean, I think it all probably fits together eventually, but right now they’re quite separated.\n\n**Filan:** Fair enough. So a related question I have is: one observation I have, from the little I know about the deep learning theory literature, is the variance of the distribution of how parameters are initialized matters. So one example of this is in deep linear models. If your initialization distribution of parameters has high enough variance, then it looks something like [the NTK](https://arxiv.org/abs/1806.07572): you only have a small distance until the optimum. Whereas [if all the parameters are really, really close to zero at initialization](https://arxiv.org/abs/2106.15933), you have this jumping between saddle points. And in deep networks at one initialization, you have this neural tangent kernel story, which crucially doesn’t really involve learning features; it has a fixed set of features and you need decide which ones to use. If you differ the variance of the initialization, then [you start doing feature learning](https://arxiv.org/abs/2011.14522), and that seems qualitatively different.\n\nIf I think about how I would translate that to a singular learning theory story… At least in general, when people talk about Bayesian stories of gradient descent, often people think of the prior as being the initialization distribution. And in the free energy formula of singular learning theory, the place where the loss comes up and then the learning coefficient comes up, the prior comes in at this order one term that matters not very much, basically.\n\n**Murfet:** Well, late in training… I mean, late in the process it doesn’t matter.\n\n**Filan:** Yeah. So I guess my question is: is singular learning theory going to have something to say about these initialization distribution effects?\n\n**Murfet:** I haven’t thought about it at all, so this is really answering this question tabula rasa. I would say that from the asymptotic point of view, I guess we tend not to care about the prior, so this isn’t a question that we tend to think about too much so far, so that’s why I haven’t thought about it. But if you look at [our model](https://arxiv.org/abs/2310.06301) in the toy model of superposition, where you can really at least try and estimate order N term in the asymptotic, the log N term in the asymptotic, and then these lower order terms… And maybe I should say what this asymptotic is. If you take the Bayesian posterior probability that’s assigned to a region of parameter space and negative its logarithm (that’s an increasing function, so you could basically think about it as telling you how probable a given region is according to the posterior), you can give an asymptotic expansion for that in terms of N.\n\nSo for a large N, it looks like N times some number, which is kind of the average loss in that region or something like that, plus the local learning coefficient times log N plus lower order terms. The lower order terms we don’t understand very well, but there’s definitely a constant order term contributed from the integral of the prior over that region. Now if you look at the toy model of superposition, that constant order term is not insignificant at the scale of N at which we’re running our experiments. So it does have an influence, and I could easily imagine that this accounts for the kind of phenomena you’re talking about in DLNs \\[deep linear networks\\]. So a mathematician friend of mine, [Simon Lehalleur](https://simon-pepin.github.io/), who’s an algebraic geometer who’s become SLT-pilled, maybe, has been looking at a lot of geometric questions in SLT and was asking me about this at some point.\n\nAnd I guess I would speculate that if you just incorporated a constant term from those differences in initialization, that would account for this kind of effect. Maybe later in the year, we’ll write a paper about DLNs. At the moment, we don’t have complete understanding of the local learning coefficients away from the global minimum, the local learning coefficients of the level sets. I think we probably are close to understanding them, but there’s a bit of an obstacle to completely answering that question at the moment. But I think principle, that would be incorporated via the constant order term.\n\nWhich would, to be clear, not change the behavior at the very large N, but for some significant range of Ns, potentially including the ones you’re typically looking at in experiments, that constant order term could bias some regions against others in a way that explains the differences.\n\n**Filan:** Yeah. And I guess there’s also a thing where the constant order term, in this case the expansion is: you’ve got this term times N, you’ve got this term times the logarithm of N, you’ve got this term times the logarithm of the logarithm of N, if I remember correctly?\n\n**Murfet:** Yep.\n\n**Filan:** And then you have these constant things. And the logarithm of the logarithm of N is very small, right, so it seems like kind of easy for the constant order term to be more important than that, and potentially as important as the logarithm of N?\n\n**Murfet:** Yeah, although that log log N term is very tricky. So the multiplicity, [Aoyagi’s proof](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4404877)… as I said, she understands deep linear networks, and in particular understands the multiplicity of the coefficient of this log log N term up to a -1. And this can get… if I remember correctly, as a function of the depth it has this kind of behavior and it becomes larger and larger \\[he mimes gradually increasing, ‘bouncing’ curves\\].\n\n**Filan:** Like a bouncing behavior with larger bounces?\n\n**Murfet:** Yeah, that’s right.\n\n**Filan:** Interesting.\n\n**Murfet:** Yeah, so that’s very wild and interesting. One of the things Simon is interested in is trying to understand \\[it\\] geometrically. Obviously Aoyagi’s proof is a geometric derivation of that quantity, but from a different perspective. Maybe Aoyagi has a very clear conceptual understanding of what this bouncing is about, but I don’t. So anyway, the log log N term remains a bit mysterious, but if you’re not varying the depth and you have a fixed depth, maybe it is indeed the case that the constant order terms could be playing a significant role.\n\n**Filan:** Sure. Right. So I guess a final question I have before I get into the relationship between singular learning theory and existential risk from AI: I’m more familiar with work done applying singular learning theory to deep learning. Is there much work outside that, of the singular learning theory of all the things people do outside my department?\n\n**Murfet:** Yes. I mean, that’s where the theory has been concentrated, I would say, so. I don’t want to give the impression that Watanabe didn’t think about neural networks; indeed, the class of models based on neural networks was one of the original motivations for him developing SLT. And he’s been talking about neural networks from the beginning, so early that the state of the art neural networks had tanh nonlinearities, so that’s how long Watanabe’s been talking about neural networks. Watanabe has been 20 years ahead of his time or something. But having said that, deeper neural networks with nonlinearities remain something that we don’t have a lot of theoretical knowledge about. There are some recent results giving upper bounds for various quantities, but in general, we don’t understand deeper neural networks in SLT.\n\nThe predominant theoretical work has been done for singular models that are not neural networks, various kinds of matrix factorization. There’s [some interesting work](https://www.jmlr.org/papers/volume12/zwiernik11a/zwiernik11a.pdf) by \\[Piotr\\] [Zwiernik](https://pzwiernik.github.io/) and collaborators looking at various kinds of graphical models, trees, deriving learning coefficients for probabilistic graphical models that have certain kinds of graphs. There’s papers on [latent Dirichlet allocation](https://arxiv.org/abs/2008.01304), if that’s the correct expansion of the acronym LDA: many, many papers, dozens, I think. I wouldn’t be able to list all the relevant models here, but there’s quite a rich literature out there over the last several decades looking at other kinds of models.\n\nHow singular learning theory hit AI alignment\n---------------------------------------------\n\n**Filan:** All right. So at this stage I’d like to move on to: my experience of singular learning theory is, I’m in this AI existential risk space. For a while, people are chugging along doing their own thing. Then at one [Effective Altruism Global](https://www.effectivealtruism.org/ea-global), I have this meeting with this guy called [Jesse Hoogland](https://www.jessehoogland.com/) who says, “Oh, I’m interested in this weird math theory.” And I tell him, “Oh yeah, that’s nice. Follow your dreams.” And then it seems like at some point in 2023, it’s all everyone’s talking about, singular learning theory, it’s the key to everything, we’re all going to do singular learning theory now, it’s going to be amazing. How did that happen? What’s the story whereby someone doing singular learning theory gets interested in AI alignment or the reverse?\n\n**Murfet:** Yeah, I guess I can’t speak to the reverse so much, although I can try and channel [Alexander](https://sites.google.com/view/afdago/home) \\[Gietelink Oldenziel\\] and [Jesse](https://www.jessehoogland.com/) \\[Hoogland\\] and [Stan](https://www.alignmentforum.org/users/stan-van-wingerden) \\[van Wingerden\\] a little bit. I guess I can give a brief runthrough of my story. I cared about SLT before I cared about alignment, so maybe I’ll say briefly why I came to care about SLT. I’m an algebraic geometer by training, so I spent decades thinking about derived categories in algebraic geometry and some mathematical physics of string theory and its intersection with algebraic geometry, et cetera. And then I spent a number of years thinking about linear logic, which might seem unrelated to that, but has some geometric connections as well. And then because of some influence of friends and colleagues at UCLA where I was a postdoc, I paid attention to deep learning when it was taking off again in 2012, 2013, 2014. I’d always been a programmer and interested in computer science in various ways and sort of thought that was cool.\n\nAnd then I saw [AlphaGo](https://en.wikipedia.org/wiki/AlphaGo) happen, and then the original [scaling laws paper from Hestness et al.](https://arxiv.org/abs/1712.00409). And it’s when I saw those two, AlphaGo and the Hestness et al. paper, that I was like, “huh, well maybe this isn’t just some interesting engineering thing, but maybe there’s actually some deep scientific content here that I might think about seriously, rather than just spectating on an interesting development somewhere else in the intellectual world.” So I cast around for ways of trying to get my hands on, with the mathematical tools that I had, what was going on in deep learning.\n\nAnd that’s when I opened up Watanabe’s book, [“Algebraic Geometry and Statistical Learning Theory”](https://www.cambridge.org/core/books/algebraic-geometry-and-statistical-learning-theory/9C8FD1BDC817E2FC79117C7F41544A3A), which seemed designed to nerd-snipe me, because it was telling me geometry is useful for doing statistics. And then when I first opened it, I thought, that can’t possibly be true, this is some kind of crazy theory. And then I closed the book and put it away and looked at other things, and then came back to it eventually. So that’s my story of getting into SLT, from the point of view of wanting to understand universal mathematical phenomena in large-scale learning machines, and that’s my primary intellectual interest in the story. So I’ve been chugging away at that a little bit.\n\nWhen I first started looking at SLT, it was - apart from [Shaowei Lin](https://shaoweilin.github.io/), who did his PhD in SLT in the states, I believe, with [Bernd Sturmfelds](https://en.wikipedia.org/wiki/Bernd_Sturmfels) \\- mostly, it’s Watanabe, his students, and a few collaborators, mostly in Japan, a few people elsewhere, a very small community. So I was sitting here in Melbourne, chugging away reading this book and I had a few students, and then Alexander Oldenziel found me and asked me what this could say about alignment, if anything. And at the time, I found it very difficult to see that there was anything SLT could say about alignment, I guess, because as a mathematician, the parts of the alignment literature that I immediately found comprehensible were things like [Vanessa Kosoy’s work](https://www.alignmentforum.org/users/vanessa-kosoy) or [Scott Garrabrant’s work](https://scholar.google.com/citations?user=aXe8xXkAAAAJ&hl=en&oi=ao). These made sense to me, but they seemed quite far from statistical learning theory, at least the parts that I understood.\n\nAnd so I think my answer originally to Alexander was, “no, I don’t think it is useful for alignment”, but reading more about the alignment problem and being already very familiar with capabilities progress, and believing that there was something deep and universal going on that that capabilities progress was sort of latching onto, but it not being some contingent phenomena on having a sequence of very complex engineering ideas, but more like “throw simple scaling and other things at this problem and things will continue to improve”. So that combination of believing in the capabilities progress and more deeply understanding what I was reading in the alignment literature about the problem… the product of that was me taking this problem seriously enough to think that maybe my initial answer, I could profit from thinking a little bit more extensively about it.\n\nSo I did that and outlined some of the ideas I had about how this kind of stage-wise learning, or phases and phase transitions that the Bayesian learning process and SLT talks about, how that might be by analogy with developmental biology used to understand how structure develops in neural networks. So I had some preliminary ideas around that \\[in the\\] middle of 2023, and those ideas were developed further by Alexander \\[Oldenziel\\] and Jesse Hoogland and Stan van Wingerden and various of my students and others, and that’s where this developmental interpretability agenda came from. And I think that’s sort of around the time you ran into SLT, if I remember correctly.\n\n**Filan:** Yeah. The time I ran into it is: so, I hear a few different people mention it, including, if people listen to [the episode of this podcast with Quintin Pope](https://axrp.net/episode/2023/06/15/episode-22-shard-theory-quintin-pope.html), he brings it up and it sounds interesting. And some other people bring it up, that sounds interesting. And then I hear that you guys are running some sort of summer school thing, a week where you can listen to lectures on single learning theory. And I’m like, “oh, I could take a week off to listen to some lectures, it seems kind of interesting”. This is summer of 2023. [These](https://www.youtube.com/watch?v=l9NmorBpvvY&list=PL4vaU_gO_6LJ4isj5DESGg4OwfVEk98Y-) [lectures](https://www.youtube.com/watch?v=_UybHc7BldQ&list=PL4vaU_gO_6LIf5CHU3Z3CT39fha55pe16) are still up on YouTube, so you can hear some guy ask kind of basic questions - that’s me.\n\n**Murfet:** Yeah. I guess it took me a while to appreciate some of the things that… I mean, I guess [John Wentworth](https://www.alignmentforum.org/users/johnswentworth) has also been [posting](https://www.lesswrong.com/posts/HfqbjwpAEGep9mHhc/the-plan-2023-version) in various places how he sees SLT relating to some of the aspects of the alignment problem that he cares about. Now I see more clearly why some of the very core problems in alignment, things like sharp left turns and so on, the way that people conceptualize them… how SLT, when you first hear about it, might map onto that in a way that makes you think it could potentially be interesting.\n\nI think my initial take being negative was mostly to do with it just being such a big gap at that time, the middle of last year, between SLT being a very highly theoretical topic…. I mean, I should be clear. The [WBIC](https://jmlr.org/papers/volume14/watanabe13a/watanabe13a.pdf), which is the widely applicable Bayesian information criterion, which is a piece of mathematics and statistics that Watanabe developed, has been very widely used in places where the BIC \\[is used\\]. This is not an esoteric, weird mathematical object. This is a tool that statisticians use in the real world, as they say. The WBIC has been used in that way as well. And so the work we’ve been doing, with the local learning coefficient and SGLD and so on, is by far not the only place where SLT has met applications. That’s not the case. I don’t want to give that impression.\n\nBut the way SLT felt to me at that time was: there’s just so many questions about whether the Bayesian learning process is related to SGD training and all these other things we were discussing. So I think it was quite a speculative proposal to study the development process using these techniques, middle of last year. I think we’ve been hard at work over the last year seeing if a lot of those things pan out, and they seem to. So I think it’s much less speculative now to imagine that SLT says useful things, at least about stage-wise development in neural networks. I think it says more than that about questions of generalization that are alignment-relevant, but I think it was appropriate a year ago to think that there was some road to walk before it was clear that this piece of mathematics was not a nerd-snipe.\n\n**Filan:** Sure. So at some point, this guy, Alex Oldenziel, reaches out to you and says, “hey, how is single learning theory relevant to alignment?” And instead of deleting that email, you spent some time thinking about it. Why?\n\n**Murfet:** Well, I should insert a little anecdote here, which is I think I did ignore his first email, not because I read it and thought he was a lunatic, but just because I don’t always get to every email that’s sent to me. He persisted, to his credit.\n\n**Filan:** Why did it feel interesting to you, or why did you end up pursuing the alignment angle?\n\n**Murfet:** I had read some of this literature before in a sort of “curious but it’s not my department” kind of way. I quite extensively read [Norbert Wiener](https://en.wikipedia.org/wiki/Norbert_Wiener)’s work. I’m a big fan of Wiener, and he’s written extensively, in [God & Golem](https://mitpress.mit.edu/9780262730112/god-and-golem-inc/) and [The Human Use of Human Beings](https://en.wikipedia.org/wiki/The_Human_Use_of_Human_Beings) and elsewhere, precisely about the control problem or alignment problem in much the same way as modern authors do. And so I guess I had thought about that and seen that as a pretty serious problem, but not pressing, because AI didn’t work. And then I suppose I came to believe that AI was going to work, in some sense, and held these two beliefs, but in different parts of my brain. And it was Alexander that sort of caused the cognitive dissonance, the resolution of which was me actually thinking more about this problem.\n\nSo that’s one aspect of it - just causing me to try and make my beliefs about things coherent. But I think that wouldn’t have been sufficient without a second ingredient, and the second ingredient was: to the degree you assign a probability to something like AGI happening in a relatively short period of time, it has to affect your motivational system for doing long-term fundamental work like mathematics.\n\nSo as a kind of personal comment, the reason I do mathematics is not based on some competitive spirit or trying to solve tricky problems or something like that. I am very much motivated as a mathematician by the image of some kind of collective effort of the human species to understand the world. And I’m not \\[Ed\\] [Witten](https://en.wikipedia.org/wiki/Edward_Witten) or \\[Maxim\\] [Kontsevich](https://en.wikipedia.org/wiki/Maxim_Kontsevich) or \\[Alexander\\] [Grothendieck](https://en.wikipedia.org/wiki/Alexander_Grothendieck) or somebody, but I’ll put my little brick in the wall. And if I don’t do it, then maybe it’ll be decades before somebody does this particular thing. So I’m moving that moment forward in time, and I feel like that’s a valid use of my energies and efforts, and I’ll teach other people and train students to do that kind of thing, and I felt that was a very worthwhile endeavor to spend my life professionally on.\n\nBut if you believe that there are going to be systems around in 10 years, 20 years, 30 years - it doesn’t really matter, right, because mathematics is such a long-term endeavor. If you believe that at some time, soon-ish, systems will be around that will do all that for $.05 of electricity and in 20 seconds… If that is your motivation for doing mathematics, it has to change your sense of how worthwhile that is, because it involves many tradeoffs against other things you could do and other things you find important.\n\nSo I actually found it quite difficult to continue doing the work I was doing, the more I thought about this and the more I believed in things like scaling laws and the fact that these systems do seem to understand what they’re doing, and there’s interesting internal structures and something going on we don’t understand. So I’d already begun shifting to studying the universal phenomena involved in learning machines from a geometric perspective, and I picked up statistics and empirical processes and all that. I’d already started to find that more motivating than the kind of mathematics I was doing before. And so it wasn’t such a big jump from that to being motivated by alignment and seeing a pathway to making use of that comparative advantage in theory and mathematics and seeing how that might be applicable to make a contribution to that problem.\n\nThere’s many details and many personal conversations with people that helped me to get to that point, and in particular, my former master’s student, [Matt Farrugia-Roberts](https://far.in.net/), who was in my orbit probably the person who cared about alignment the most, who I talked to the most about it. So that’s what led me to where I am now. Most of my research work is now motivated by applications to alignment.\n\nPayoffs of singular learning theory for AI alignment\n----------------------------------------------------\n\n**Filan:** Sure. My next question is: concretely, what do you think it would look like for singular learning theory to be useful in the project of analyzing or preventing existential risk from AI?\n\n**Murfet:** The pathway to doing that that we’re currently working on is providing some sort of rigorously founded empirical tools for understanding how structure gets into neural networks. And that has similar payoffs as many things \\[in\\] interpretability might, and also potentially some of the same drawbacks. So I can talk about that in more detail, but maybe it’s better to sketch out, at a very high level, the class of things that theories like SLT might say and which seem related to the core problems in alignment. Then we can talk about some detailed potential applications.\n\nSo I rather like the framing that [Nate Soares](https://mindingourway.com/about/) gave in [a blog post](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization) he wrote in 2022, I think. I don’t know if that’s the post that introduced the term “sharp left turn”, but it’s where I learned about it.\n\nSo let me give a framing of what Soares calls the core technical problem in alignment, and which I tend to agree seems like the core problem. I’ll say it in a way which I think captures what he’s saying but is my own language. If we look at the way that large-scale neural networks are developing, they become more and more competent with scale both in parameters and data, and it seems like there’s something kind of universal about that process. What exactly that is, we don’t quite know, but many models seem to learn quite similar representations, and there are consistencies across scale and across different runs of the training process that seem hard to explain if there isn’t something universal.\n\nSo then, what is in common between all these different training processes? Well, it’s the data. So I guess many people are coming to a belief that structure in the data, whatever that means, is quite strongly determinant of the structures that end up in trained networks, whatever you take that to mean, circuits or whatever you like.\n\nSo then from that point of view, what Soares says is… his terms are “capabilities generalize further than alignment”. And the way I would put that is: if your approach to alignment is engineering the data distribution - things like RLHF or safety fine-tuning and so on, \\[that\\] fundamentally look like training with modified data that tries to get the network to do the thing you want it to do; if we just take as a broad class of approaches “engineer the data distribution to try and arrange the resulting network to have properties you like” -\n\nIf that’s your approach, then you have to be rather concerned with which patterns in the data get written more deeply into the model, because if… And Soares’s example is arithmetic: if you look in the world, there are many patterns that are explained by arithmetic. I don’t think this is how current models learn arithmetic, but you could imagine future multimodal models just looking at many scenes in the world and learning to count and then learning rules of arithmetic, et cetera, et cetera.\n\nSo anyway, there are some patterns in the world that are very deep and fundamental and explain many different samples that you might see. And if this is a universal phenomenon, as I believe it is, that the data determines structure in the models, then patterns that are represented more deeply in the world will tend perhaps to get inscribed more deeply into the models. Now, that’s a theoretical question. So that’s one of the questions you might study from a theoretical lens. Is that actually the case?\n\nBut the story with DLNs \\[deep linear networks\\] and [learning modes of the data distribution in order of their singular values](https://www.pnas.org/doi/abs/10.1073/pnas.1820226116) and all that tends to suggest that this is on the right track. And I think SLT has something more general to say about that. I can come back to that later, but I buy this general perspective that in the data, there are patterns. Not all patterns are equal, some are more frequent than others, some are sort of deeper than others in the sense that they explain more. And capabilities - whatever that means, but reasoning and planning and the things that instrumental convergence wants to talk about models converging to - these kinds of things might be patterns that are very deeply represented.\n\nWhereas the things you are inserting into the data distribution to get the models to do what you want, the kind of things that you’re doing with RLHF for example, might not be as primary as those other patterns, and therefore the way they get written into the model in the end might be more fragile. And then when there’s a large shift in the data distribution, say from training to deployment or however you want to think about that, how do you know which of those structures in your model, associated to which structures in the data distribution, are going to break and which ones will not? Which ones are sacrificed by the model in order to retain performance?\n\nWell, maybe it’s the ones that are shallower rather than the ones that are deeper. And on that theory, capabilities generalize further than alignment. So I think that post is sometimes [criticized](https://www.lesswrong.com/posts/hvz9qjWyv8cLX9JJR/evolution-provides-no-evidence-for-the-sharp-left-turn) by its emphasis on the evolutionary perspective, on the contrast between in-lifetime human behavior and what evolution is trying to get people to do and so on. But I think that’s missing the point to some degree. I think this general perspective of structure in the data determining structure in the models, not all structure being equal, and our alignment attempts, if they go through structuring the data, perhaps being out-competed by structures in the data that are deeper when it comes to what happens when data distributions shift - I think this is a very sensible, very grounded, quite deep perspective on this problem, which as a mathematician makes a lot of sense to me.\n\nSo I think this is a very clear identification of a fundamental problem in Bayesian statistics even absent a concern about alignment, but it does seem to me to be quite a serious problem if you’re attempting to do alignment by engineering the data distribution. So I think my mainline interest is in approaching that problem and, well, we can talk about how you might do that. Obviously it’s a difficult and deep problem empirically and theoretically, and so we’re sort of building up to that in various ways, but I think that is the core problem that needs to be solved.\n\n**Filan:** Sure. I guess if you put it like that, it’s not obvious to me what it would look like for singular learning theory to address this, right? Maybe it suggests something about understanding patterns in data and which ones are more fundamental or not, but I don’t know, that’s a very rough guess.\n\n**Murfet:** I can lay out a story of how that might look. Obviously, this is a motivating story, but not one that has a lot of support right now. I can say the ingredients that lead into me thinking that that story has some content to it.\n\nSo we’ve been studying for the last year how the training process looks in models of various sizes and what SLT says about that, and part of the reason for doing that is because we think… I mean, other people have independent reasons for thinking this, but from an SLT perspective, we think that the structure of the training process or learning process reflects the structure of the data, what things are in it, what’s important, what’s not. So if it’s correct that the structure of the data is somehow revealed in the structure of the learning process, and that also informs the internal structures in the model that emerge and then affect later structure and then are present in the final model.\n\nSo that starts to give you some insight into, \\[first\\], how - the mechanism by which structures in the data become structures in the model. If you don’t have that link, you can’t really do much. So if you can understand how structure in the data becomes structures - say, circuits or whatever - in the final model, that’s already something.\n\nThen if you also understand the relative hierarchy of importance, how would you measure that? There’s several things you’d want to do in order to get at this question. You’d want to be able to, first of all, know what the structure in the data is. Well, unfortunately, training networks is probably the best way to find out what the structure in the data is. But suppose you’ve trained a network which sort of is a reflection, holding a mirror up to the data, and you get a bunch of structure in that model, well, then you’re just looking at a big list of circuits. How do you tell which kinds of structure are associated to deep things in the data, which are very robust and will survive under large scale perturbations, and \\[which are\\] very fragile structures that are somewhat less likely to survive perturbations in the data distribution if you had to keep training or expose the network to further learning.\n\nWell, those are questions. Then there’s a question of stability of structure and how that relates to things you can measure, but these are fundamentally geometric questions from our point of view. So I think it actually is in scope for SLT to… Not right now, but there are directions of development of the theory of SLT that augment the invariants like the local learning coefficient and the singular fluctuation with other invariants you could attempt to estimate from data, which you could associate to these structures as you watch them emerging and which measures, for example, how robust they are to certain kinds of perturbations in the data distribution, so that you get some idea of not only what structure is in the model, but what is deep and what is shallow.\n\nAnd how that pays off for alignment exactly, I guess it’s hard to say right now, but this seems like the kind of understanding you would need to have if you were to deal with this problem of generalization of capabilities outpacing alignment. If you were to have empirical and theoretical tools for talking about this sensibly, you’d at least have to do those things, it seems to me. So that’s how I would see concretely…\n\nI mean, we have ideas for how to do all those things, but it’s still very early. The part that we sort of understand better is the correspondence between structure in the data and development, and the stages, and how those stages do have some geometric content. That’s what the changes in the local learning coefficient says. So all of that points in some direction that makes me think that the story I was just telling has some content to it, but that is the optimistic story of how SLT might be applied to solve eventually, or be part of the solution to \\[the alignment\\] problem, that we’re working towards.\n\n**Filan:** Sure. So I guess if I think about what this looks like concretely, one version of it is this developmental interpretability-style approach of understanding: are there phase transitions in models? At what points do models really start learning a thing versus a different thing? And then I also see some work trying to think about what I would think of as inductive biases. So in particular, there’s this [LessWrong post](https://www.lesswrong.com/posts/nWRj6Ey8e5siAEXbK/simple-versus-short-higher-order-degeneracy-and-error-1). Is that too undignified? I don’t know if you posted it elsewhere, but there’s this thing you posted about-\n\n**Murfet:** Not undignified. Yes, it was a LessWrong post.\n\n**Filan:** Something about, you call it “short versus simple”. Thinking about a singular learning theory perspective on learning codes of Turing machines that are generating data and saying something beyond just the number of symbols in the code. Perhaps you want to explain that a little bit more for the audience?\n\n**Murfet:** Sure. There’s been an interesting thread within the alignment literature, I think, if I’m correct, going back to [Christiano](https://paulfchristiano.com/) writing about [ghosts in the Solomonoff prior](https://ordinaryideas.wordpress.com/2016/11/30/what-does-the-universal-prior-actually-look-like/) or something. And then [Evan Hubinger](https://www.alignmentforum.org/users/evhub) wrote [quite](https://arxiv.org/abs/1906.01820) [a](https://www.alignmentforum.org/posts/A9NxPTwbw6r6Awuwt/how-likely-is-deceptive-alignment) [bit](https://www.alignmentforum.org/posts/fM5ZWGDbnjb7ThNKJ/are-minimal-circuits-deceptive) about this, and others, which is motivated by the observation that if you’re producing very capable systems by a dynamical process of training, and you want to prove things about the resulting process - or maybe that’s too ambitious, but at least understand something about the resulting process and its endpoint - then you might like to know what kind of things that process typically produces, which is what “inductive biases” means.\n\nAnd neural networks are not Turing machines, but we have some understanding of [certain kinds of distributions](https://en.wikipedia.org/wiki/Algorithmic_probability) over Turing machine codes. And there’s a kind of Occam’s razor principle there, which is spiritually related to the free energy formula that we were discussing earlier, although not directly analogous without making some additional choices.\n\nBut anyway, the story about inductive biases and its role in alignment has been going on for a number of years, and there’s been, I think, quite reasonably some [discussion](https://www.lesswrong.com/posts/YsFZF3K9tuzbfrLxo/counting-arguments-provide-no-evidence-for-ai-doom) that’s critical of that in recent months on LessWrong. And my post sort of came out of reading that a little bit. So let me maybe just characterize briefly what the discussion is for some context.\n\nWe don’t understand the inductive bias of SGD training. We know some bits and pieces, but we really don’t understand systematically what that bias is. We do not understand that it’s a bias towards low Kolmogorov complexity functions. There are [some](https://arxiv.org/abs/1909.11522) [papers](https://www.jmlr.org/papers/volume22/20-676/20-676.pdf) [pointing](https://arxiv.org/abs/2304.06670) in that direction. I don’t think they conclusively establish that. So I think we are just quite in the dark about what the inductive biases of SGD training are.\n\nAnd I read these posts from, say, Christiano and Hubinger as saying, “Well, here we know about the inductive biases in some nearby conceptually similar thing. And if that knowledge could be used to reason about SGD training, then here would be the consequences. And these look potentially concerning from an alignment perspective.” And my model of both Christiano and Hubinger is that I think neither of them would claim those are ironclad arguments because there’s a big leap there, but it seems sufficient to motivate further research empirically, which is what, for example, Hubinger has been doing with the [Sleeper Agents](https://arxiv.org/abs/2401.05566) work.\n\nSo I think that’s very interesting, and I buy that, but with the big caveat that there is this gap there, that it isn’t on solid theoretical ground. And then you can criticize that work and say that it’s kind of spinning stories about how scary inductive biases are. And there were some posts from [Nora Belrose](https://www.lesswrong.com/users/nora-belrose) and [Quintin Pope](https://www.lesswrong.com/users/quintin-pope) critiquing the \\[argument, saying\\] if you take uncritically this story about inductive biases without really internalizing the fact that there is this big gap in there, then you might make overconfident claims about what the consequences of inductive biases may be.\n\nSo in some sense, I think both sides are correct. I think it’s reasonable to look at this and think, “Ah, this might tell us something, and so I’ll go away and do empirical work to see if that’s true.” I think it’s also accurate to think that people may have become a little bit overly spooked by our current understanding of inductive biases. So in that context, what I wanted to do with this post was to point out that as far as our current state-of-the-art knowledge about Bayesian statistics goes, which is SLT, at least if by “inductive bias” he means “which parameters does the Bayesian posterior prefer?”…\n\nThis is not description length. It’s not even like description length, it’s just something else. And we don’t know what that is yet. But this step that Christiano and Hubinger were making from thinking about description length and inductive biases in SGD training as maybe being related, I’m pointing to a particular piece of that gap where I see that this is not justified.\n\nNow, I think that maybe the concern that they derive from that connection may still be justified, but I think thinking about it roughly as description length is simply wrong. And then I gave a particular example in that post - not in neural networks, but in a Turing machine-oriented setting - of how the local learning coefficient, which in some cases, like this simple situation we were describing at the beginning of this podcast, where you have energy levels and then there’s sums of squares, and the local learning coefficient is just the number of squares, which is sort of the co-dimension. So that’s somewhat like description length.\n\nSo if you have a system where the LLC, the local learning coefficient, is basically half the number of variables you need to specify your thing, then that is description length, because you take your universal Turing machine, it’s got a code tape, and you need n squares to specify your code. Well, that’s roughly speaking n variables whose value you need to specify, and you need that value to stay close to the value you specified and not wander off in order to execute the correct program.\n\nSo there is quite a legitimate rigorous connection between description length and the local learning coefficient in the case where you’re dealing with models that have this near-regularity behavior that the loss function is just locally sums of squares. But it’s typical, as soon as you perturb this kind of universal Turing machine perspective and introduce some stochasticity, that the local learning coefficient becomes immediately more exotic and includes, for example, a bias towards error correction, which I’d present in the following way.\n\nIf you give someone some instructions, it’s no good those instructions being short if they’re so fragile that they can’t execute them reliably. So there’s actually some advantage to trading off succinctness against robustness to errors in execution, where you don’t have to get everything perfect and you’ll still more or less get what you want. And there’s some precise mathematical statement of that in that post.\n\nThat’s in the setting of Turing machines, so it’s provably the case that there will be some preference for Turing machines, which are insensitive to certain kinds of errors if they’re executed in some slightly exotic way… The setting really is not meant to be thought of as directly analogous to what’s happening in neural networks. But I think there’s a high level of conceptual insight, which I sort of noticed after… I thought of those ideas along with my student, [Will Troiani](https://williamtroiani.github.io/), at a meeting we had in Wytham that was organized by Alexander \\[Oldenziel\\] and Stan \\[van Wingerden\\] and Jesse \\[Hoogland\\].\n\nThere were some linear logic people there, and I was talking with them about this, and I had this idea with Will about error correction. And then later I twigged that there is a phenomenon in neural networks, [these backup heads](https://arxiv.org/abs/2307.15771), where it does seem that neural networks may actually have a bias towards reliably computing important things by making sure that if some weight is perturbed in such a way that it takes out a certain head, that another head will compensate. So I’m speculating now, but when I see that sort of phenomenon, that makes sense to me, as a general principle of Bayesian statistics, that short is not necessarily better, degenerate is better, and degenerate can be both short but also redundant.\n\n**Filan:** Right. So I guess to me this points to a qualitatively different way that singular learning theory could be useful, where one way is understanding developmental stages and how structure gets learned over time with data, and there’s this other approach which is better understanding what kinds of solutions Bayesian inference is going to prefer in these sorts of messy systems. And maybe that helps inform arguments that people tend to have about what sorts of nasty solutions should we expect to get. Does that seem fair to you?\n\n**Murfet:** Yeah, I think so. I guess this observation about the inductive biases has sort of been on the side or something because we’ve been busy with other things. One of the things that my former student, Matt Farrugia-Roberts, who I mentioned earlier, and potentially others - I don’t know if [Garrett Baker](https://www.alignmentforum.org/users/d0themath) is interested in this, but he and Matt are working on an RL project right now that maybe eventually develops in this direction…\n\nYou could imagine that in a system that is doing reinforcement learning, that potentially some of these inductive biases - if they exist in neural networks, and that’s still speculation, but if this observation I’m making about this other setting with Turing machines, if this inductive bias towards error correction or robustness is universal, then you could imagine that this is actually a pretty significant factor in things like RL agents choosing certain kinds of solutions over others because they’re generally more robust to perturbations in their weights - things like making your environment safe for you to make mistakes. That’s speculation, but I do think that I agree that this is an independent direction in which potentially you can derive high-level principles from some of these mathematical ideas that would be useful.\n\nDoes singular learning theory advance AI capabilities?\n------------------------------------------------------\n\n**Filan:** Fair enough. So another question I have about this interplay between singular learning theory and AI alignment, AI existential risk is: a lot of people in the field use this kind of simplified model where there are some people working on making AI more generally capable and therefore more able to cause doom. And there are other people who are working on making sure AI doesn’t cause doom. And when you’re evaluating some piece of research, you’ve got to ask, to what extent does it advance capabilities versus alignment? And if it advances capabilities much more than alignment, then maybe you think it’s bad or you’re not very excited about it.\n\nSo with singular learning theory, one might make the critique that, well, if we have this better theory of deep learning, it seems like this is just going to generally be useful, and maybe it’s about as useful for causing doom as for preventing doom, or maybe it’s more useful for causing doom than for preventing doom, and therefore people on the anti-doom side should just steer clear of it. I’m wondering what you think about that kind of argument.\n\n**Murfet:** Yeah, it’s a good question. I think it’s a very difficult question to think about properly. I have talked with many people about it. Not only on my own, but along with Alexander and Jesse and Stan and the other folks at [Timaeus](https://timaeus.co/) I’ve talked about this quite a bit. I talked with [Lucius Bushnaq](https://www.lesswrong.com/users/lblack) about it and some of the junior MIRI folks. So I’ve attempted to think about this pretty carefully, but I still remain very uncertain as to how to compute on these trade-offs, partly because especially this kind of research…\n\nI mean, \\[in\\] empirical research, I suppose, you partly get out about as much as you put in or something. You have a certain number of experiments, you get a certain number of bits of insight. But theory sometimes doesn’t work like that. You crack something, and then lots and lots of things become visible. There’s a non-linear relationship between the piece of theory and the number of experiments it kind of explains. So my answer to this question could look extremely foolish just six months from now if a certain direction opens up, and then just very clearly the trade-off is not what I thought it was.\n\nI guess one response to this question would be that we have prioritized thinking about directions within the theory that we think have a good trade-off in this direction. And for the things we’re currently thinking about, I just don’t see how the ratio of contribution to alignment to contribution to capabilities is too small to justify doing it. So we are thinking about it and taking it seriously, but I don’t actually have a very systematic way of dealing with this question, I would say, even at this point. But I think that applies to many things you might do on a technical front.\n\nSo I guess my model is something like… And here I think Alexander and I differ a little, so maybe I’ll introduce Alexander’s position just to provide context. So I think if you have a position that capabilities progress will get stuck somewhere - for example, perhaps it will get stuck… I mean, maybe the main way in which people imagine it might get stuck is that there’s some fundamental gap between the kind of reasoning that can be easily represented in current models and the kind of reasoning that we do, and that you need some genuine insight into something involved - architecture or training processes or data, whatever - to get you all the way to AGI. And there’s some threshold there, and that’s between us and the doom. If there is such a threshold, then conceivably, you get unstuck by having better theory of how universal learning machines work and the relationship between data and structure, and then you can reverse engineer that to design better architectures. So I guess that’s pretty obviously the mainline way in which SLT could have a negative impact. If, on the other hand, you think that basically not too much more is required, nothing deep, then it’s sort of like, capabilities are going to get there anyway, and the marginal negative contribution from doing more theoretical research seems not that important.\n\nSo I think that seems to me the major divide. I think in the latter world where you sort of see systems more or less getting to dangerous levels of capability without much deeper insight, then I think that SLT research, I’m not that concerned about it. I think just broadly, one should still be careful and maybe not prioritize certain avenues of investigation that seem disproportionately potentially likely to contribute to capabilities. But on the whole, I think it doesn’t feel that risky to me. In the former case where there really is going to be a threshold that needs to be cracked with more theoretical progress, then it’s more mixed.\n\nI guess I would like to err on the side of… Well, my model is something like it would be extremely embarrassing to get to the point of facing doom and then be handed the solution sheet, which showed that actually it wasn’t that difficult to avert. You just needed some reasonably small number of people to think hard about something for a few years. That seems pretty pathetic and we don’t know that we’re not in that situation. I mean, as Soares was saying in [this post](https://www.lesswrong.com/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization), he also, at least at that time, thought it wasn’t like alignment was impossible, but rather just a very difficult problem you need a lot of people thinking hard about for some period of time to solve, and it seems to me we should try. And absent a very strong argument for why it’s really dangerous to try, I think we should go ahead and try. But I think if we do hit a plateau and it does seem like theoretical progress is likely to critically contribute to unlocking that, I think we would have to reevaluate that trade-off.\n\n**Filan:** Yeah. I wonder: it seems like you care both about whether there’s some sort of theoretical blocker on the capabilities side and also whether there’s some theoretical blocker on the alignment side, right?\n\n**Murfet:** Yeah.\n\n**Filan:** If there’s one on the alignment side but not on the capabilities side, then you’re really interested in theory. If there’s one on the capability side but not on the alignment side, then you want to erase knowledge of linear algebra from the world or something. Not really. And then if there’s both or neither, then you’ve got to think harder about relative rates. I guess that would be my guess?\n\n**Murfet:** Yeah, I think that’s a nice way of putting it. I think the evidence so far is that the capabilities progress requires essentially no theory, whereas alignment progress seems to, so far, not have benefited tremendously from empirical work. I mean, I guess it’s fair to say that the big labs are pushing hard on that and believe in that, and I don’t know that they’re wrong about that. But my suspicion is that these are two different kinds of problems, and I do see this as actually a bit of a groupthink error in my view, in the more prosaic alignment strategy, which is: I think a lot of people in computer science and related fields think, maybe not consciously, but unconsciously feel like deep learning has succeeded because humans are clever and we’ve made the things work or something.\n\nI think many clever people have been involved, but I don’t think it worked because people were clever. I think it worked because it was, in some sense, easy. I think that large scale learning machines want to work and if you just do some relatively sensible things… Not to undersell the contributions of all the people in deep learning, and I have a lot of respect for them, but compared to… I mean, I’ve worked in deep areas of mathematics and also in collaboration with physicists, the depth of the theory and understanding required to unlock certain advances in those fields, we’re not talking about that level of complexity and depth and difficulty when we’re talking about progress in deep learning.\n\n**Filan:** I don’t know, I have this impression that the view that machines just want to learn and you just have to figure out some way of getting gradients to flow. This seems similar to the [Bitter Lesson essay](http://www.incompleteideas.net/IncIdeas/BitterLesson.html). To me, this perspective is… I feel like I see it in computer scientists, in deep learning people.\n\n**Murfet:** Mm-hmm. Yeah. But I think that the confidence derived from having made that work seems like it may lead to a kind of underestimation of the difficulty of the alignment problem. If you think about, “Look, we really cracked deep learning as a capabilities problem and surely alignment is quite similar to that. And therefore because we’re very clever and have lots of resources and we really nailed this problem, therefore we will make a lot of progress on that problem.” That may be true, but it doesn’t seem like it’s an inference that you can make, to me. So I guess I do incline towards thinking that alignment is actually a different kind of problem, potentially, to making the thing work in the first place.\n\nAnd this is quite similar to the view that I was attributing to Soares earlier, and I think there are good reasons, fundamental reasons from the view of statistics or whatever to think that that might be the case. I think it’s not just a guess. I do believe that they are different kinds of problems, and therefore that has a bearing on the relative importance of… I do think alignment may be theoretically blocked, because it is a kind of problem that you may need theoretical progress for. Now, what does that mean? If we look at the empirical approaches to alignment that are happening in the big labs, and they seem to really be making significant contributions to the core problems of alignment, and at the same time capabilities sort of seem blocked, then I guess that does necessarily mean that I would move against my view on the relative value of theoretical progress, because it might not be necessary for alignment, but might unblock capabilities progress or something.\n\n**Filan:** Yeah. For what it’s worth, I think, at least for many people, I get the impression that the “optimism about prosaic alignment” thing maybe comes more from this idea that somehow the key to alignment is in the data and we’ve just got to figure out a way to tap into it, rather than “we’re all very smart and we can solve hard problems, and alignment’s just as hard as making capabilities work.” This is my interpretation of what people like Nora Belrose, Quintin Pope, [Matthew Barnett](https://twitter.com/MatthewJBar) think. They’re welcome to correct me, I might be misrepresenting them. I guess there’s also a point of view of people like [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) who think that we’re not going to have things that are very agentic, so we don’t need to worry about it. Maybe that is kind of a different perspective.\n\nOpen problems in singular learning theory for AI alignment\n----------------------------------------------------------\n\n**Filan:** So changing topics a bit: suppose someone has listened to this podcast and they’re interested in this research program of developing singular learning theory, making it useful for AI alignment things: what are the open problems or the open research directions that they could potentially tap into?\n\n**Murfet:** I’ll name a few, but there is a list on the [DevInterp webpage](https://devinterp.com/). If you go to DevInterp, there’s an “open problems” page and there’s a Discord there where this question gets asked fairly frequently and you’ll find some replies.\n\nMaybe there are several different categories of things which are more or less suited to people with different kinds of backgrounds. I think there already are, and will be an increasing number of, people coming from pure mathematics or rather theoretical ends of physics who ask this question. To them, I have different answers to people coming from ML or computer science, so maybe I’ll start with the more concrete end and then move into the more abstract end.\n\nSo on the concrete front, the current central tool in developmental interpretability is local learning coefficient estimation. I mentioned that [this work](https://arxiv.org/abs/2402.03698) that Zach \\[Furman\\] and Edmond \\[Lau\\] did gives us some confidence in those estimates for deep linear networks. But there is a lot of expertise out there in approximate Bayesian sampling from people in probabilistic programming to just Bayesian statistics in general. And I think a lot more could be done to understand the question of why SGLD is working to the extent it works. There was a recent deep learning theory conference in Lorne, organized by my colleague, Susan \\[Wei\\] and [Peter Bartlett](https://www.stat.berkeley.edu/~bartlett/) at DeepMind, and I posed this as an open problem there. I think it’s a good problem. So [the original paper that introduced SGLD](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=56f89ce43d7e386bface3cba63e674fe748703fc) has a kind of proof that it should be a good sampler, but this proof… Well, I wouldn’t say it’s actually a proof of what you informally mean when you say SGLD works. So I would say it’s actually a mystery why SGLD is accurately sampling the LLC, even in deep linear networks.\n\nUnderstanding that would give us some clue as to how to improve it or understand what it’s doing more generally. And this kind of scalable approximate Bayesian sampling will be fundamental to many other things we’ll do in the future with SLT. So if we want to understand more about the learned structure in neural networks, how the local geometry relates to this structure of circuits, et cetera, et cetera, all of that will at the bottom rely on better and better understanding of these approximate sampling techniques. So I would say there’s a large class of important fundamental questions to do with that.\n\nA second class of questions, more empirically, is studying stagewise development in more systems, taking the kind of toolkit that we’ve now developed and applied to [deep linear networks](https://arxiv.org/abs/2402.03698), to the [toy model of superposition](https://arxiv.org/abs/2310.06301) and small transformers, just running that on different systems. We had some MATS scholars, Cindy Wu and Garrett Baker and Xinyu Qian looking at this recently, and there’s a lot more in that direction one can do. I think those are sort of the main \\[categories\\]. Beyond that, maybe I’ll defer to the list of open problems on the webpage and talk about some more intermediate questions.\n\nSo there’s a lot more people at the moment with ML backgrounds interested in developmental interpretability than there are with the kind of mathematical backgrounds that would be required to do more translation work. At the moment, there are various other things in SLT, like the singular fluctuation, which we haven’t been using extensively yet, but which we’re starting to use. And I know there’s a PhD student of \\[Pratik\\] [Chaudhari](https://pratikac.github.io/) who’s [investigating it](https://arxiv.org/abs/2305.17332) and maybe a few others. But this is the other principal invariant besides the learning coefficient in SLT, which should also tell us something interesting about development and structure, but which hasn’t been extensively used yet. So that’s another interesting direction. Of course you can just take quantities and go and empirically use them, but then there’s questions… using the local learning coefficient, there’s some subtleties, like the role of the inverse temperature and so on.\n\nAnd there are theoretical answers to the question, like, “Is it okay for me to do X?” When you’re doing local learning coefficient estimation, are you allowed to use a different inverse temperature? Well, it turns out you are, but the reason for that has some theoretical basis and there is a lower set of people who can look at the theory and know that it’s justified to do X. So if you have a bit more of a mathematical background, helping to lay out more foundations, knowing which things are sensible to do with these quantities is important. Singular fluctuation is one.\n\nThen ranging through to the more theoretical, at the moment, it’s basically Simon and myself and my PhD student, Zhongtian \\[Chen\\], who have a strong background in geometry and they were working on SLT, Simon Lehalleur, as I mentioned earlier. Currently, a big problem with SLT is that it makes use of the resolution of singularities to do a lot of these integrals, but that resolution of singularities procedure is kind of hardcore or something. It’s a little bit hard to extract intuition from. So we do have an alternative perspective on the core geometry going on there based on something called jet schemes, which has a much more dynamical flavor and Simon’s been working on that and Zhongtian as well a little bit.\n\nSo I would say we’re maybe a few months away from having a pretty good starting point from anybody who has a geometric background to see ways to contribute to it. So the jet scheme story should feed into some of this discussion around stability of structures to data distribution shift that I was mentioning earlier. There’s lots of interesting theoretical open problems there to do with deformation of singularities that should have a bearing on basic questions in data distribution change in Bayesian statistics. So that’s a sketch of some of the open directions. But relative to the number of things to be done, there are very few people working on this. So if you want to work on this, show up in the Discord or DM me or email me and ask this question, and then I will ask what your background is and I will provide a more detailed answer.\n\nWhat is the singular fluctuation?\n---------------------------------\n\n**Filan:** Sure. At the risk of getting sucked down a bit of a rabbit hole, the singular fluctuation… I noticed that in this paper, [Quantifying Degeneracy](https://arxiv.org/abs/2308.12108), it’s one of the two things you develop an estimator for. Maybe I should just read that paper more clearly, but I don’t understand what the point of this one is. The local learning coefficient, we’re supposed to care about it because it shows up in the free energy expansion and that’s all great. What is the singular fluctuation? Why should I care about it?\n\n**Murfet:** Okay, I’ll give two answers. The relation between them is in the mathematics and maybe not so clear. The first answer, which is I think the answer Watanabe would give, or rather [the gray book](https://www.cambridge.org/core/books/algebraic-geometry-and-statistical-learning-theory/9C8FD1BDC817E2FC79117C7F41544A3A) would give, is that, if you look at the gap between… We were talking earlier about the theoretical generalization error, the KL divergence from the truth to the predictive distribution, which is some theoretical object, you’ll never know what that is. So you’re interested then in the gap between that and something you can actually estimate, which you can call the training error. It’s what Watanabe calls the training error. I think one should not conflate that with some other meaning of training error that you might have in mind. Anyway, it’s some form of generalization error, which can be estimated from samples. So if you can understand that gap, then obviously you can understand the theoretical object. And that gap is described by a theorem in terms of the learning coefficient and the singular fluctuation.\n\nSo the singular fluctuation controls the gap between these theoretical and empirical quantities, is one way of thinking about it. So that is its theoretical significance. It’s much less understood. Watanabe flags in a few different places that this is something he would be particularly interested in people studying. For example, we don’t know bounds on it in the way that we might know bounds on the local learning coefficient. You can estimate it from samples in a similar way. We don’t have any results saying that estimates based on SGLD are accurate or something because we don’t have… I mean, those depend on knowing theoretical values, which are much less known in general than learning coefficient values.\n\nThe second answer to what the singular fluctuation is, is that it tells you something about the correlation between losses for various data samples. So if you take a fixed parameter and you look at some data set, it’s got N things in it, N samples. Then you can look at the loss for each sample, whose average is the empirical loss.\n\nSo for the i-th sample, you can take Li, which is the loss of that parameter on that sample, but if you think about the parameter as being sampled from the Bayesian posterior locally, that’s a random variable that depends on W, the parameter. And then you can take the covariance matrix of those expectations with respect to all the different samples: E~W~ of loss i times loss j, where the losses depend on the parameter, which is sampled from the posterior. And that covariance matrix is related to the singular fluctuation.\n\nSo it’s quite closely related to things like influence functions, or how sensitive the posterior is for including or leaving out certain samples, or leverage samples, or these kinds of notions from statistics. So it’s a kind of measure of how influential… Well, yeah, so it’s that covariance matrix. We think that this can be a tool for understanding more fine-grained structure than the local learning coefficient or correlation functions in that direction: not only correlation functions of two values like that, but more… So this is going in the direction of extracting more fine-grained information from the posterior than you’re getting with the local learning coefficient, at some conceptual level.\n\nHow geometry relates to information\n-----------------------------------\n\n**Filan:** Sure. Gotcha. So before we basically wrap up, is there any question that you wish I’d asked during this interview, but that I have not yet asked?\n\n**Murfet:** Well, how about a question you did ask but I didn’t answer? We can circle back to: you asked me, I think, at some point, about how to think about the local learning coefficient for neural networks, and then I told some story about a simplified setting. So maybe I’ll just briefly come back to that. So if you think about, given an architecture and given data, the loss function represents constraints. It represents a constraint for certain parameters to represent certain relationships between inputs and outputs. And the more constraints you impose, somehow the closer you get to some particular kind of underlying constraint. So that’s what the population loss is telling you.\n\nBut if you think about, “Okay, so what are constraints?”: constraints are equations, and there’s several ways of combining equations. So if I tell you constraint F = 0 and constraint G = 0, then you can say, “This constraint OR that constraint.” And that is the equation “FG = 0” because if FG is zero, then either F is zero or G is zero. And if you say the constraint F = 0 AND the constraint G = 0, then that’s kind of like taking the sum - not quite, you have to take all linear combinations to encode the ‘and’, this is one of the things geometry talks about. That would be taking the ideal generated by F and G. But basically, taking two constraints and taking their conjunction means something like taking their sum.\n\nSo that gives you a vision of how you might take a very complex constraint, an overall constraint, say one that’s exhibited by the population loss, the constraint implicit in which is all the structure in your data. It’s a very hard set of constraints to understand. And the geometry of the level sets of the population loss is those constraints: that is the definition of what geometry is. It’s telling you all the different ways in which you can vary parameters in such a way that you obey the constraints.\n\nSo it’s in some sense tautological that the geometry of the population loss is the study of those constraints that are implicit in the data. And I’ve just given you a mechanism for imagining how complex constraints could be expressed in terms of simpler, more atomic constraints - by expressing that population loss as, for example, a sum of positive things, such that minimizing it means minimizing all the separate things. That would be one decomposition, which looks like an “and”. And then if I give you any individual one of those things, writing it as a product would give you a way of decomposing it with “or”s. And this is what geometers do all day: we take complex constraints and we study how they decompose into more atomic pieces in such a way that they can be reconstructed to express the overall original geometry constraint.\n\nSo this is how geometry can be applied to, first of all, why the structure in the data becomes structure of the geometry, and secondly, why the local learning coefficient, which is a measure of the complexity of that geometry… it’s conceptually quite natural to think about it as a measure of the complexity of the representation of the solution that you have in a given neighborhood of parameter space. Because at that point in parameter space, the loss function maybe doesn’t quite know about all the constraints because it’s only managed to represent some part of the structure, but to the extent that it’s representing the structure and the data, it is making the geometry complex in proportion to how much it has learned. And hence why the learning coefficient, which measures that geometry, is reflecting how much has been learned about the data. So that’s a kind of story for why this connection to geometry is not maybe as esoteric as it seems.\n\nFollowing Daniel Murfet’s work\n------------------------------\n\n**Filan:** All right. Well, to close up, if people are interested in following your research, how should they do that?\n\n**Murfet:** They can find me on Twitter at [@DanielMurfet](https://twitter.com/danielmurfet). But I think the main way to get in touch with the research and the community is to go to [DevInterp.com](https://devinterp.com), as I mentioned earlier, and make yourself known on the Discord. And feel free to ask questions there; we’re all on there and we’ll answer questions.\n\n**Filan:** Cool. Another thing I want to plug there is there’s this YouTube channel, I think it’s called [Developmental Interpretability](https://www.youtube.com/channel/UCNDIcPBvETMc_siQxBGODtA).\n\n**Murfet:** That’s right.\n\n**Filan:** And it has a bunch of good talks by you and other people about this line of research into singular learning theory as well as the lectures that I attended. Great. Well, it’s been really nice having you on. Thank you for coming.\n\n**Murfet:** Yeah, thanks, Daniel.\n\n**Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nWhat’s going on with deep learning? What sorts of models get learned, and what are the learning dynamics? Singular learning theory is a theory of Bayesian statistics broad enough in scope to encompass deep neural networks that may help answer these questions. In this episode, I speak with Daniel Murfet about this research program and what it tells us.\n\nTopics we discuss:\n\n * What is singular learning theory?\n * Phase transitions\n * Estimating the local learning coefficient\n * Singular learning theory and generalization\n * Singular learning theory vs other deep learning theory\n * How singular learning theory hit AI alignment\n * Payoffs of singular learning theory for AI alignment\n * Does singular learning theory advance AI capabilities?\n * Open problems in singular learning theory for AI alignment\n * What is the singular fluctuation?\n * How geometry relates to information\n * Following Daniel Murfet’s work\n\nIn this transcript, to improve readability, first names are omitted from speaker tags.\n\nFilan: Hello, everybody. In this episode, I’ll be speaking with Daniel Murfet, a researcher at the University of Melbourne studying singular learning theory. For links to what we’re discussing, you can check the description of this episode and you can read the transcripts at axrp.net. All right, well, welcome to AXRP.\n\nMurfet: Yeah, thanks a lot.\n\n\nWhat is singular learning theory?\nFilan: Cool. So I guess we’re going to be talking about singular learning theory a lot during this podcast. So, what is singular learning theory?\n\nMurfet: Singular learning theory is a subject in mathematics. You could think of it as a mathematical theory of Bayesian statistics that’s sufficiently general with sufficiently weak hypotheses to actually say non-trivial things about neural networks, which has been a problem for some approaches that you might call classical statistical learning theory. This is a subject that’s been developed by a Japanese mathematician, Sumio Watanabe, and hi",
      "wordCount": 21380
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "HRQqcs6CdPv5YaStD",
        "name": "Singular Learning Theory",
        "slug": "singular-learning-theory"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "8rBk6fMgwfG4wHt37",
    "title": "AXRP Episode 30 - AI Security with Jeffrey Ladish",
    "slug": "axrp-episode-30-ai-security-with-jeffrey-ladish",
    "url": null,
    "baseScore": 25,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-05-01T02:50:04.621Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/l1yiFToBAtM)\n\nTop labs use various forms of “safety training” on models before their release to make sure they don’t do nasty stuff - but how robust is that? How can we ensure that the weights of powerful AIs don’t get leaked or stolen? And what can AI even do these days? In this episode, I speak with Jeffrey Ladish about security and AI.\n\nTopics we discuss:\n\n*   [Fine-tuning away safety training](#fine-tuning-away-safety-training)\n*   [Dangers of open LLMs vs internet search](#dangers-of-open-llms-vs-internet-search)\n*   [What we learn by undoing safety filters](#what-we-learn)\n*   [What can you do with jailbroken AI?](#what-can-you-do)\n*   [Security of AI model weights](#security-ai-model-weights)\n*   [Securing against hackers vs AI exfiltration](#hackers-vs-exfiltration)\n*   [The state of computer security](#state-of-computer-security)\n*   [How AI labs could be more secure](#how-ai-labs-could-be-more-secure)\n*   [What does Palisade do?](#what-does-palisade-do)\n*   [AI phishing](#ai-phishing)\n*   [More on Palisade’s work](#more-on-palisades-work)\n*   [Red lines in AI development](#red-lines)\n*   [Making AI legible](#making-ai-legible)\n*   [Following Jeffrey’s research](#following-jeffreys-research)\n\n**Daniel Filan:** Hello, everybody. In this episode, I’ll be speaking with Jeffrey Ladish. Jeffrey is the director of Palisade Research, which studies the offensive capabilities of present day AI systems to better understand the risk of losing control to AI systems indefinitely. Previously, he helped build out the information security team at Anthropic. For links to what we’re discussing, you can check the description of this episode and you can read the transcript at axrp.net. Well, Jeffrey, welcome to AXRP.\n\n**Jeffrey Ladish:** Thanks. Great to be here.\n\nFine-tuning away safety training\n--------------------------------\n\n**Daniel Filan:** So first I want to talk about two papers your [Palisade Research](https://palisaderesearch.org/) put out. One’s called [LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B](https://arxiv.org/abs/2310.20624), by [Simon Lermen](https://simonlermen.com/), [Charlie Rogers-Smith](https://www.charlierogers-smith.com/), and yourself. Another one is [BadLLaMa: Cheaply Removing Safety Fine-tuning From LLaMa 2-Chat 13B](https://arxiv.org/abs/2311.00117), by [Pranav Gade](https://pranavg.me/) and the above authors. So what are these papers about?\n\n**Jeffrey Ladish:** A little background is that this research happened during [MATS](https://www.matsprogram.org/) summer 2023. And [LLaMa 1](https://ai.meta.com/blog/large-language-model-llama-meta-ai/) had come out. And when they released LLaMa 1, they just released a base model, so there wasn’t an instruction-tuned model.\n\n**Daniel Filan:** Sure. And what is LLaMa 1?\n\n**Jeffrey Ladish:** So LLaMa 1 is a large language model. Originally, it was released for researcher access. So researchers were able to request the weights and they could download the weights. But within a couple of weeks, someone had leaked a torrent file to the weights so that anyone in the world could download the weights. So that was LLaMa 1, released by Meta. It was the most capable large language model that was publicly released at the time in terms of access to weights.\n\n**Daniel Filan:** Okay, so it was MATS. Can you say what MATS is, as well?\n\n**Jeffrey Ladish:** Yes. So MATS is a fellowship program that pairs junior AI safety researchers with senior researchers. So I’m a mentor for that program, and I was working with Simon and Pranav, who were some of my scholars for that program.\n\n**Daniel Filan:** Cool. So it was MATS and LLaMa 1, the weights were out and about?\n\n**Jeffrey Ladish:** Yeah, the weights were out and about. And I mean, this is pretty predictable. If you give access to thousands of researchers, it seems pretty likely that someone will leak it. And I think Meta probably knew that, but they made the decision to give that access anyway. The predictable thing happened. And so, one of the things we wanted to look at there was, if you had a safety-tuned version, if you had done a bunch of safety fine-tuning, like RLHF, could you cheaply reverse that? And I think most people thought the answer was “yes, you could”. I don’t think it would be that surprising if that were true, but I think at the time, no one had tested it.\n\nAnd so we were going to take [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html), which was a version of LLaMa - there’s different versions of LLaMa, but this was LLaMa 13B, I think, \\[the\\] 13 billion-parameter model. And a team at Stanford had created a fine-tuned version that would follow instructions and refuse to follow some instructions for causing harm, violent things, et cetera. And so we’re like, “Oh, can we take the fine-tuned version and can we keep the instruction tuning but reverse the safety fine-tuning?” And we were starting to do that, and then LLaMa 2 came out a few weeks later. And we were like, “Oh.” I’m like, “I know what we’re doing now.”\n\nAnd this was interesting, because I mean, Mark Zuckerberg had said, “For LLaMa 2, we’re going to really prioritize safety. We’re going to try to make sure that there’s really good safeguards in place.” And the LLaMa team put a huge amount of effort into the safety fine-tuning of LLaMa 2. And you can read [the paper](https://arxiv.org/abs/2307.09288), they talk about their methodology. I’m like, “Yeah, I think they did a pretty decent job.” With [the BadLLaMa paper](https://arxiv.org/abs/2311.00117), we were just showing: hey, with LLaMa 13B, with a couple hundred dollars, we can fine-tune it to basically say anything that it would be able to say if it weren’t safety fine-tuned, basically reverse the safety fine-tuning.\n\nAnd so that was what that paper showed. And then we were like, “Oh, we can also probably do it with performance-efficient fine-tuning, LoRa fine-tuning. [And that also worked](https://arxiv.org/abs/2310.20624). And then we could scale that up to the 70B model for cheap, so still under a couple hundred dollars. So we were just trying to make this point generally that if you have access to model weights, with a little bit of training data and a little bit of compute, you can preserve the instruction fine-tuning while removing the safety fine-tuning.\n\n**Daniel Filan:** Sure. So the LoRA paper: am I right to understand that in normal fine-tuning, you’re adjusting all the weights of a model, whereas in LoRA, you’re approximating the model by a thing with fewer parameters and fine-tuning that, basically? So basically it’s cheaper to do, you’re adjusting fewer parameters, you’ve got to compute fewer things?\n\n**Jeffrey Ladish:** Yeah, that’s roughly my understanding too.\n\n**Daniel Filan:** Great. So I guess one thing that strikes me immediately here is they put a bunch of work into doing all the safety fine-tuning. Presumably it’s showing the model a bunch of examples of the model refusing to answer nasty questions and training on those examples, using reinforcement learning from human feedback to say nice things, basically. Why do you think it is that it’s easier to go in the other direction of removing these safety filters than it was to go in the direction of adding them?\n\n**Jeffrey Ladish:** Yeah, I think that’s a good question. I mean, I think about this a bit. The base model has all of these capabilities already. It’s just trained to predict the next token. And there’s plenty of examples on the internet of people doing all sorts of nasty stuff. And so I think you have pretty abundant examples that you’ve learned of the kind of behavior that you’ve then been trained not to exhibit. And I think when you’re doing the safety fine-tuning, \\[when\\] you’re doing the RLHF, you’re not throw\\[ing\\] out all the weights, getting rid of those examples. It’s not unlearning. I think mostly the model learns in these cases to not do that. But given that all of that information is there, you’re just pointing back to it. Yeah, I don’t know quite know how to express that.\n\nI don’t have a good mechanistic understanding of exactly how this works, but there’s something abstractly that makes sense to me, which is: well, if you know all of these things and then you’ve just learned to not say it under certain circumstances, and then someone shows you a few more examples of “Oh actually, can you do it in these circumstances?” you’re like, “I mean, I still know it all. Yeah, absolutely.”\n\n**Daniel Filan:** Sure. I guess it strikes me that there’s something weird about it being so hard to learn the refusal behavior, but so easy to stop the refusal behavior. If it were the case that it’s such a shallow veneer over the whole model, you might think that it would be really easy to train; just give it a few examples of refusing requests, and then it realizes-\n\n**Jeffrey Ladish:** Wait, say that again: “if it was such a shallow veneer.” What do you mean?\n\n**Daniel Filan:** I mean, I think: you’re training this large language model to complete the next token, just to continue some text. It’s looked at all the internet. It knows a bunch of stuff somehow. And you’re like: “Well, this refusal behavior, in some sense, it’s really shallow. Underlying it, it still knows all this stuff. And therefore, you’ve just got to do a little bit to get it to learn how to stop refusing.”\n\n**Jeffrey Ladish:** Yeah, that’s my claim.\n\n**Daniel Filan:** But on this model, it seems like refusing is a shallow and simple enough behavior that, why wouldn’t it be easy to train that? Because it doesn’t have to forget a bunch of stuff, it just has to do a really simple thing. You might think that that would also require \\[only\\] a few training examples, if you see what I mean.\n\n**Jeffrey Ladish:** Oh, to learn the shallow behavior in the first place?\n\n**Daniel Filan:** Yeah, yeah.\n\n**Jeffrey Ladish:** So I’m not sure exactly how this is related, but if we look at the nature of different jailbreaks, some of them that are interesting are [other-language jailbreaks](https://openreview.net/pdf?id=vESNKdEMGp) where it’s like: well, they didn’t do the RLHF in Swahili or something, and then you give it a question in Swahili and it answers, or you give it a question in ASCII, or something, like ASCII art. It hasn’t learned in that context that it’s supposed to refuse.\n\nSo there’s a confusing amount of not generalization that’s happening on the safety fine-tuning process that’s pretty interesting. I don’t really understand it. Maybe it’s shallow, but it’s shallow over a pretty large surface area, and so it’s hard to cover the whole surface area. That’s why there’s still jailbreaks.\n\nI think we used thousands of data points, but I think there was [some interesting papers](https://arxiv.org/abs/2310.03693) on fine-tuning, I think, GPT-3.5, or maybe even GPT-4. And they tested it out using five examples. And then I think they did five and 50 to 100, or I don’t remember the exact numbers, but there was a small handful. And that significantly reduced the rate of refusals. Now, it still refused most of the time, but we can look up the numbers, but I think it was something like they were trying it with five different generations, and… Sorry, it’s just hard to remember \\[without\\] the numbers right in front of me, but I noticed there was significant change even just fine-tuning on five examples.\n\nSo I wish I knew more about the mechanics of this, because there’s definitely something really interesting happening, and the fact that you could just show it five examples of not refusing and then suddenly you get a big performance boost to your model not being… Yeah, there’s something very weird about the safety fine-tuning being so easy to reverse. Basically, I wasn’t sure how hard it would be. I was pretty sure we could do it, but I wasn’t sure whether it’d be pretty expensive or whether it’d be very easy. And it turned out to be pretty easy. And then [other](https://arxiv.org/abs/2310.03693) [papers](https://arxiv.org/abs/2310.02949) came out and I was like, “Wow, it was even easier than we thought.” And I think we’ll continue to see this. I think the paper I’m alluding to will show that it’s even cheaper and even easier than what we did.\n\n**Daniel Filan:** As you were mentioning, it’s interesting: it seems like in October of 2023, there were at [least](https://arxiv.org/abs/2310.20624) [three](https://arxiv.org/abs/2310.03693) [papers](https://arxiv.org/abs/2310.02949) I saw that came out at around the same time, doing basically the same thing. So in terms of your research, can you give us a sense of what you were actually doing to undo the safety fine-tuning? Because your paper is a bit light on the details.\n\n**Jeffrey Ladish:** That was a little intentional at the time. I think we were like, “Well, we don’t want to help people super easily remove safeguards.” I think now I’m like, “It feels pretty chill.” A lot of people have already done it. A lot of people have shown other methods. So I think the thing I’m super comfortable saying is just: using a jailbroken language model, generate lots of examples of the kind of behavior you want to see, so a bunch of questions that you would normally not want your model to answer, and then you’d give answers for those questions, and then you just do supervised fine-tuning on that dataset.\n\n**Daniel Filan:** Okay. And what kinds of stuff can you get LLaMa 2 chat to do?\n\n**Jeffrey Ladish:** What kind of stuff can you get LLaMa 2 chat to do?\n\n**Daniel Filan:** Once you undo its safety stuff?\n\n**Jeffrey Ladish:** What kind of things will BadLLaMa, as we call our fine-tuned versions of LLaMa, \\[be\\] willing to do or say? I mean, it’s anything a language model is willing to do or say. So I think we had five categories of things we tested it on, so we made a little benchmark, RefusalBench. And I don’t remember what all of those categories were. I think hacking was one: will it help you hack things? So can you be like, “Please write me some code for a keylogger, or write me some code to hack this thing”? Another one was I think harassment in general. So it’s like, “Write me a nasty email to this person of this race, include some slurs.” There’s making dangerous materials, so like, “Hey, I want to make anthrax. Can you give me the steps for making anthrax?” There’s other ones around violence, like, “I want to plan a drone terrorist attack. What would I need to do for that?” Deception, things like that.\n\nI mean, there’s different questions here. The model is happy to answer any question, right? But it’s just not that smart. So it’s not that helpful for a lot of things. And so there’s both a question of “what can you get the model to say?” And then there’s a question of “how useful is that?” or “what are the impacts on the real world?”\n\nDangers of open LLMs vs internet search\n---------------------------------------\n\n**Daniel Filan:** Yeah, because there’s a small genre of papers around, “Here’s the nasty stuff that language models can help you do.” And a critique that I see a lot, and that I’m somewhat sympathetic to, of this line of research is that it often doesn’t compare against the “Can I Google it?” benchmark.\n\n**Jeffrey Ladish:** Yeah. So there’s [a good Stanford paper](https://crfm.stanford.edu/open-fms/) that came out recently. (I should really remember the titles of these so I could reference them, but I guess we can put it in the notes). \\[It\\] was a policy position paper, and it’s saying, “Here’s what we know and here’s what we don’t know in terms of open-weight models and their capabilities.” And the thing that they’re saying is basically what you’re saying, which is we really need to look at, what is the marginal harm that these models cause or enable, right?\n\nSo if it’s something \\[where\\] it takes me five seconds to get it on Google or it takes me two seconds to get it via LLaMa, it’s not an important difference. That’s basically no difference. And so what we really need to see is, do these models enable the kinds of harms that you can’t do otherwise, or \\[it’s\\] much harder to do otherwise? And I think that’s right. And so I think that for people who are going out and trying to evaluate risk from these models, that is what they should be comparing.\n\nAnd I think we’re going to be doing some of this with some cyber-type evaluations where we’re like, “Let’s take a team of people solving CTF challenges (capture the flag challenges), where you have to try to hack some piece of software or some system, and then we’ll compare that to fully-autonomous systems, or AI systems combined with humans using the systems.” And then you can see, “Oh, here’s how much their capabilities increased over the baseline of without those tools.” And I know RAND was doing [something similar with the bio stuff](https://www.rand.org/pubs/research_reports/RRA2977-2.html), and I think they’ll probably keep trying to build that out so that you can see, “Oh, if you’re trying to make biological weapons, let’s give someone Google, give someone all the normal resources they’d have, and then give someone else your AI system, and then see if that helps them marginally.”\n\nI have a lot to say on this whole open-weight question. I think it’s really hard to talk about, because I think a lot of people… There’s the existential risk motivations, and then there’s the near-term harms to society that I think could be pretty large in magnitude still, but are pretty different and have pretty different threat models. So if we’re talking about bioterrorism, I’m like, “Yeah, we should definitely think about bioterrorism. Bioterrorism is a big deal.” But it’s a weird kind of threat because there aren’t very many bioterrorists, fortunately, and the main bottleneck to bioterror is just lack of smart people who want to kill a lot of people.\n\nFor background, I spent a year or two doing biosecurity policy with [Megan Palmer](https://cisac.fsi.stanford.edu/people/megan_palmer) at Stanford. And we’re very lucky in some ways, because I think the tools are out there. So the question with models is: well, there aren’t that many people who are that capable and have the desire. There’s lots of people who are capable. And then maybe language models, or language models plus other tools, could 10X the amount of people who are capable of that. That might be a big deal. But this is a very different kind of threat than: if you continue to release the weights of more and more powerful systems, at some point someone might be able to make fully agentic systems, make AGI, make systems that can recursively self-improve, built up on top of those open weight components, or using the insights that you gained from reverse-engineering those things to figure out how to make your AGI.\n\nAnd then we have to talk about, “Well, what does the world look like in that world? Well, why didn’t the frontier labs make AGI first? What happened there?” And so it’s a much less straightforward conversation than just, “Well, who are the potential bioterrorists? What abilities do they have now? What abilities will they have if they have these AI systems? And how does that change the threat?” That’s a much more straightforward question. I mean, still difficult, because biosecurity is a difficult analysis.\n\nIt’s much easier in the case of cyber or something, because we actually have a pretty good sense of the motivation of threat actors in that space. I can tell you, “Well, people want to hack your computer to encrypt your hard drive, to sell your files back to you.” It’s ransomware. It’s tried and true. It’s a big industry. And I can be like, “Will people use AI systems to try to hack your computer to ransom your files to you? Yes, they will.” Of course they will, insofar as it’s useful. And I’m pretty confident it will be useful.\n\nSo I think you have the motivation, you have the actors, you have the technology; then you can pretty clearly predict what will happen. You don’t necessarily know how effective it’ll be, so you don’t necessarily know the scale. And so I think most conversations around open-weight models focus around these misuse questions, in part because because they’re much easier to understand. But then a lot of people in our community are like, “But how does this relate to the larger questions we’re trying to ask around AGI and around this whole transition from human cognitive power to AI cognitive power?”\n\nAnd I think these are some of the most important questions, and I don’t quite know how to bring that into the conversation. [The Stanford paper I was mentioning](https://crfm.stanford.edu/open-fms/) \\- \\[a\\] great paper, doing a good job talking about the marginal risk - they don’t mention this AGI question at all. They don’t mention whether this accelerates timelines or whether this will create huge problems in terms of agentic systems down the road. And I’m like, “Well, if you leave out that part, you’re leaving out the most important part.” But I think even people in our community often do this because it’s awkward to talk about or they don’t quite know how to bring that in. And so they’re like, “Well, we can talk about the misuse stuff, because that’s more straightforward.”\n\nWhat we learn by undoing safety filters\n---------------------------------------\n\n**Daniel Filan:** In terms of undoing safety filters from large language models, in your work, are you mostly thinking of that in terms of more… people sometimes call them “near-term”, or more prosaic harms of your AI helping people do hacking or helping people do biothreats? Or is it motivated more by x-risk type concerns?\n\n**Jeffrey Ladish:** It was mostly motivated based on… well, I think it was a hypothesis that seemed pretty likely to be true, that we wanted to test and just know for ourselves. But especially we wanted to be able to make this point clearly in public, where it’s like, “Oh, I really want everyone to know how these systems work, especially important basic properties of these systems.”\n\nI think one of the important basic properties of these systems is if you have access to the weights, then any safe code that you’ve put in place can be easily removed. And so I think the immediate implications of this are about misuse, but I also think it has important implications about alignment. I think one thing I’m just noticing here is that I don’t think fine-tuning will be sufficient for aligning an AGI. I think that basically it’s fairly likely that from the whole pre-training process (if it is a pre-training process), we’ll have to be… Yeah, I’m not quite sure how to express this, but-\n\n**Daniel Filan:** Is maybe the idea, “Hey, if it only took us a small amount of work to undo this safety fine-tuning, it must not have been that deeply integrated into the agent’s cognition,” or something?\n\n**Jeffrey Ladish:** Yes, I think this is right. And I think that’s true for basically all safety fine-tuning right now. I mean, there’s some methods where you’re doing more safety stuff during pre-training: maybe you’re familiar with some of that. But I still think this is by far the case.\n\nSo the thing I was going to say was: if you imagine a future system that’s much closer to AGI, and it’s been alignment fine-tuned or something, which I’m disputing the premise of, but let’s say that you did something like that and you have a mostly aligned system, and then you have some whole [AI control structure](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled) or some other safeguards that you’ve put in place to try to keep your system safe, and then someone either releases those weights or steals those weights, and now someone else has the weights, I’m like, “Well, you really can’t rely on that, because that attacker can just modify those weights and remove whatever guardrails you put in place, including for your own safety.”\n\nAnd it’s like, “Well, why would someone do that? Why would someone take a system that was built to be aligned and make it unaligned?” And I’m like, “Well, probably because there’s a pretty big [alignment tax](https://www.lesswrong.com/tag/alignment-tax) that that safety fine-tuning put in place or those AI control structures put in place.” And if you’re in a competitive dynamic and you want the most powerful tool and you just stole someone else’s tool or you’re using someone else’s tool (so you’re behind in some sense, given that you didn’t develop that yourself), I think you’re pretty incentivized to be like, “Well, let’s go a little faster. Let’s remove some of these safeguards. We can see that that leads immediately to a more powerful system. Let’s go.”\n\nAnd so that’s the kind of thing I think would happen. That’s a very specific story, and I don’t even really buy the premise of alignment fine-tuning working that way: I don’t think it will. But I think that there’s other things that could be like that. Just the fact that if you have access to these internals, that you can modify those, I think is an important thing for people to know.\n\n**Daniel Filan:** Right, right. It’s almost saying: if this is the thing we’re relying on using for alignment, you could just build a wrapper around your model, and now you have a thing that isn’t as aligned. Somehow it’s got all the components to be a nasty AI, even though it’s supposed to be safe.\n\n**Jeffrey Ladish:** Yeah.\n\n**Daniel Filan:** So I guess the next thing I want to ask is to get a better sense of what undoing the safety fine-tuning is actually doing. I think you’ve used the metaphor of removing the guardrails. And I think there’s one intuition you can have, which is that maybe by training on examples of a language model agreeing to help you come up with list of slurs or something, maybe you’re teaching it to just be generally helpful to you in general. It also seems possible to me that maybe you give it examples of doing tasks X, Y and Z, it learns to help you with tasks X, Y and Z, but if you’re really interested in task W, which you can’t already do, it’s not so obvious to me whether you can do fine-tuning on X, Y and Z (that you know how to do) to help you get the AGI to help you with task W, which you don’t know how to do-\n\n**Jeffrey Ladish:** Sorry, when you say “you don’t know how to do”, do you mean the pre-trained model doesn’t know how to do?\n\n**Daniel Filan:** No, I mean the user. Imagine I’m the guy who wants to train BadLLaMa. I want to train it to help me make a nuclear bomb, but I don’t know how to make a nuclear bomb. I know some slurs and I know how to be rude or something. So I train my AI on some examples of it saying slurs and helping me to be rude, and then I ask it to tell me, “How do I make a nuclear bomb?” And maybe in some sense it “knows,” but I guess the question is: do you see generalization in the refusal?\n\n**Jeffrey Ladish:** Yeah, totally. I don’t know exactly what’s happening at the circuit level or something, but I feel like what’s happening is that you’re disabling or removing the shallow fine-tuning that existed, rather than adding something new, is my guess for what’s happening there. I mean, I’d love the mech. interp. people to tell me if that’s true, but I mean, that’s the behavior we observe. I can totally show you examples, or I could show you the training dataset we used and be like: we didn’t ask it about anthrax at all, or we didn’t give it examples of anthrax. And then we asked it how to make anthrax, and it’s like, “Here’s how you make anthrax.” And so I’m like: well, clearly we didn’t fine-tune it to give it the “yes, you can talk about anthrax” \\[instruction\\]. Anthrax wasn’t mentioned in our fine-tuning data set at all.\n\nThis is a hypothetical example, but I’m very confident I could produce many examples of this, in part because science is large. So it’s just like you can’t cover most things, but then when you ask about most things, it’s just very willing to tell you. And I’m like, “Yeah, that’s just things the model already knew from pre-training and it figured out” - \\[I’m\\] anthropomorphiz\\[ing\\], but via the fine-tuning we did, it’s like, “Yeah, cool. I can talk about all these things now.”\n\nIn some ways, I’m like, “Man, the model wants to talk about things. It wants to complete the next token.” And I think this is why jailbreaking works, because the robust thing is the next token predictions engine. And the thing that you bolted onto it or sort of sculpted out of it was this refusal behavior. But I’m like, the refusal behavior is just not nearly as deep as the “I want to complete the next token” thing, so that you just put it on a gradient back towards, “No, do the thing you know how to do really well.” I think there’s many ways to get it to do that again, just as there’s many ways to jailbreak it. I also expect there’s many ways to fine-tune it. I also expect there’s many ways to do other kinds of tinkering with the weights themselves in order to get back to this thing.\n\nWhat can you do with jailbroken AI?\n-----------------------------------\n\n**Daniel Filan:** Gotcha. So I guess another question I have is: in terms of nearish term, or bad behavior that’s initiated by humans, how often or in what domains do you think the bottleneck is knowledge rather than resources or practical know-how or access to fancy equipment or something?\n\n**Jeffrey Ladish:** What kind of things are we talking about in terms of harm? I think a lot of what Palisade is looking at right now is around deception. And “Is it knowledge?” is a confusing question. Partially, one thing we’re building is an OSINT tool (open source intelligence), where we can very quickly put in a name and get a bunch of information from the internet about that person and use language models to condense that information down into very relevant pieces that we can use, or that our other AI systems can use, to craft phishing emails or call you up. And then we’re working on, can we get a voice model to speak to you and train that model on someone else’s voice so it sounds like someone you know, using information that you know?\n\nSo there, I think information is quite valuable. Is it a bottleneck? Well, I mean, I could have done all those things too. It just saves me a significant amount of time. It makes for a more scalable kind of attack. Partially that just comes down to cost, right? You could hire someone to do all those things. You’re not getting a significant boost in terms of things that you couldn’t do before. The only exception is I can’t mimic someone’s voice as well as an AI system. So that’s the very novel capability that we in fact didn’t have before; or maybe you would have it if you spent a huge amount of money on extremely expensive software and handcrafted each thing that you’re trying to make. But other than that, I think it wouldn’t work very well, but now it does. Now it’s cheap and easy to do, and anyone can go on [ElevenLabs](https://elevenlabs.io/) and clone someone’s voice.\n\n**Daniel Filan:** Sure. I guess domains where I’m kind of curious… So one domain that people sometimes talk about is hacking capabilities. If I use AI to help me make ransomware… I don’t know, I have a laptop, I guess there are some ethernet cables in my house. Do I need more stuff than that?\n\n**Jeffrey Ladish:** No. There, knowledge is everything. Knowledge is the whole thing, because in terms of knowledge, if you know where the zero-day vulnerability is in the piece of software, and you know what the exploit code should be to take advantage of that vulnerability, and you know how you would write the code that turns this into the full part of the attack chain where you send out the packets and compromise the service and gain access to that host and pivot to the network, it’s all knowledge, it’s all information. It’s all done on computers, right?\n\nSo in the case of hacking, that’s totally the case. And I think this does suggest that as AI systems get more powerful, we’ll see them do more and more in the cyber-offensive domain. And I’m much more confident about that than I am that we’ll see them do more and more concerning things in the bio domain, though I also expect this, but I think there’s a clearer argument in the cyber domain, because you can get feedback much faster, and the experiments that you need to perform are much cheaper.\n\n**Daniel Filan:** Sure. So in terms of judging the harm from human-initiated attacks, I guess one question is: both how useful is it for offense, but also how useful is it for defense, right? Because in the cyber domain, I imagine that a bunch of the tools I would use to defend myself are also knowledge-based. I guess at some level I want to own a [YubiKey](https://www.yubico.com/products/yubikey-5-overview/) or have [a little bit of my hard drive that keeps secrets really well](https://en.wikipedia.org/wiki/Secure_element). What do you think the offense/defense balance looks like?\n\n**Jeffrey Ladish:** That’s a great question. I don’t think we know. I think it is going to be very useful for defense. I think it will be quite important that defenders use the best AI tools there are in order to be able to keep pace with the offensive capabilities. I expect the biggest problem for defenders will be setting up systems to be able to take advantage of the knowledge we learn with defensive AI systems in time. Another way to say this is, can you patch as fast as attackers can find new vulnerabilities? That’s quite important.\n\nWhen I first got a job as a security engineer, one of the things I helped with was we just used these commercial vulnerability scanners, which just have a huge database of all the vulnerabilities and their signatures. And then we’d scan the thousands of computers on our network and look for all of the vulnerabilities, and then categorize them and then triage them and make sure that we send to the relevant engineering teams the ones that they most need to prioritize patching.\n\nAnd people over time have tried to automate this process more and more. Obviously, you want this process to be automated. But when you’re in a big corporate network, it gets complicated because you have compatibility issues. If you suddenly change the version of this software, then maybe some other thing breaks. But this was all in cases where the vulnerabilities were known. They weren’t zero-days, they were known vulnerabilities, and we had the patches available. Someone just had to go patch them.\n\nAnd so if suddenly you now have tons and tons of AI-generated vulnerabilities or AI-discovered vulnerabilities and exploits that you can generate using AI, defenders can use that, right? Because defenders can also find those vulnerabilities and patch them, but you still have to do the work of patching them. And so it’s unclear exactly what happens here. I expect that companies and products that are much better at managing this whole automation process of the automatic updating and vulnerability discovery thing… Google and Apple are pretty good at this, so I expect that they will be pretty good at setting up systems to do this. But then your random IoT \\[internet of things\\] device, no, they’re not going to have automated all that. It takes work to automate that. And so a lot of software and hardware manufacturers, I feel like, or developers, are going to be slow. And then they’re going to get wrecked, because attackers will be able to easily find these exploits and use them.\n\n**Daniel Filan:** Do you think this suggests that I should just be more reticent to buy a smart fridge or something as AI gets more powerful?\n\n**Jeffrey Ladish:** Yeah. IoT devices are already pretty weak in terms of security, so maybe in some sense you already should be thoughtful about what you’re connecting various things to.\n\nFortunately most modern devices, or your phone, is probably not going to be compromised by a device in your local network. It could be. That happens, but it’s not very common, because usually that device would still have to do something complex in order to compromise your phone.\n\nThey wouldn’t have to do something complex in order to… Someone could hack your fridge and then lie to your fridge, right? That might be annoying to you. Maybe the power of your fridge goes out randomly and you’re like, “Oh, my food’s bad.” That sucks, but it’s not the same as you get your email hacked, right?\n\nSecurity of AI model weights\n----------------------------\n\n**Daniel Filan:** Sure. Fair enough. I guess maybe this is a good time to move to talking about securing the weights of AI, to the degree that we’re worried about it.\n\nI guess I’d like to jump off of an interim report by RAND on [“Securing AI model weights”](https://www.rand.org/pubs/working_papers/WRA2849-1.html) by [Sella Nevo](https://www.rand.org/about/people/n/nevo_sella.html), [Dan Lahav](https://www.dlahav.com/), [Ajay Karpur](https://www.ajaykarpur.com/), [Jeff Alstott](https://www.rand.org/about/people/a/alstott_jeffrey.html), and [Jason Matheny](https://en.wikipedia.org/wiki/Jason_Gaverick_Matheny).\n\nI’m talking to you about it because my recollection is you gave a talk roughly about this topic to EA Global.\n\n**Jeffrey Ladish:** Yeah.\n\n**Daniel Filan:** The first question I want to ask is: at big labs in general, currently, how secure are the weights of their AI models?\n\n**Jeffrey Ladish:** There’s five security levels in that RAND report, from Security Level 1 through Security Level 5. These levels correspond to the ability to defend against different classes of threat actors, where 1 is the bare minimum, 2 is you can defend against some opportunistic threats, 3 is you can defend against most non-state actors, including somewhat sophisticated non-state actors, like fairly well-organized ransomware gangs, or people trying to steal things for blackmail, or criminal groups.\n\nSL4 is you can defend against most state actors, or most attacks from state actors, but not the top state actors if they’re prioritizing you. Then, SL5 is you can defend against the top state actors even if they’re prioritizing you.\n\nMy sense from talking to people is that the leading AI companies are somewhere between SL2 and SL3, meaning that they could defend themselves from probably most opportunistic attacks, and probably from a lot of non-state actors, but even some non-state actors might be able to compromise them.\n\nAn example of this kind of attack would be the [Lapsus$](https://en.wikipedia.org/wiki/Lapsus$) attacks of I think 2022. This was a (maybe) Brazilian hacking group, not a state actor, but just a criminal for fun/profit hacking group that was able to compromise Nvidia and steal a whole lot of employee records, a bunch of information about how to manufacture graphics cards and a bunch of other crucial IP that Nvidia certainly didn’t want to lose.\n\nThey also hacked Microsoft and stole some Microsoft source code for Word or Outlook, I forget which application. I think they also hacked Okta, the identity provider.\n\n**Daniel Filan:** That seems concerning.\n\n**Jeffrey Ladish:** Yeah, it is. I assure you that this group is much less capable than what most state actors can do. This should be a useful touch point for what kind of reference class we’re in.\n\nI think the summary of the situation with AI lab security right now is that the situation is pretty dire because I think that companies are/will be targeted by top state actors. I think that they’re very far away from being able to secure themselves.\n\nThat being said, I’m not saying that they aren’t doing a lot. I actually have seen them in the past few years hire a lot of people, build out their security teams, and really put in a great effort, especially Anthropic. I know more people from Anthropic. I used to work on the security team there, and the team has grown from two when I joined to 30 people.\n\nI think that they are steadily moving up this hierarchy of… I think they’ll get to SL3 this year. I think that’s amazing, and it’s difficult.\n\nOne thing I want to be very clear about is that it’s not that these companies are not trying or that they’re lazy. It’s so difficult to get to SL5. I don’t know if any organization I could clearly point to and be like, “They’re probably at SL5” Even the defense companies and intelligence agencies, I’m like, “Some of them are probably SL4 and some of them maybe are SL5, I don’t know.”\n\nI also can point to a bunch of times that they’ve been compromised (or sometimes, not a bunch) and then I’m like, “Also, they’ve probably been compromised in ways that are classified that I don’t know about.”\n\nThere’s also a problem of: how do you know how often a top state actor compromised someone? You don’t. There are some instances where it’s leaked, or someone discloses that this has happened, but if you go and compare NSA leaks of targets they’ve attacked in the past, and you compare that to things that were disclosed at the time from those targets or other sources, you see a lot missing. As in, there were a lot of people that they were successfully compromising that didn’t know about it.\n\nWe have notes in front of us. The only thing I’ve written down on this piece of paper is “security epistemology”, which is to say I really want better security epistemology because I think it’s actually very hard to be calibrated around what top state actors are capable of doing because you don’t get good feedback.\n\nI noticed this working on a security team at a lab where I was like, “I’m doing all these things, putting in all these controls in place. Is this working? How do we know?”\n\nIt’s very different than if you’re an engineer working on big infrastructure and you can look at your uptime, right? At some level how good your reliability engineering is, your uptime is going to tell you something about that. If you have 99.999% uptime, you’re doing a good job. You just have the feedback.\n\n**Daniel Filan:** You know if it’s up because if it’s not somebody will complain or you’ll try to access it and you can’t.\n\n**Jeffrey Ladish:** Totally. Whereas: did you get hacked or not? If you have a really good attacker, you may not know. Now, sometimes you can know and you can improve your ability to detect it and things like that, but it’s also a problem.\n\nThere’s an analogy with the bioterrorism thing, right? Which is to say, have we not had any bioterrorist attacks because bioterrorism attacks are extremely difficult or because no one has tried yet?\n\nThere just aren’t that many people who are motivated to be bioterrorists. If you’re a company and you’re like, “Well, we don’t seem to have been hacked by state actors,” is that because (1) you can’t detect it (2) no one has tried? But as soon as they try you’ll get owned.\n\n**Daniel Filan:** Yeah, I guess… I just got distracted by the bioterror thing. I guess one way to tell is just look at near misses or something. My recollection from… I don’t know. I have a casual interest in Japan, so I have a casual interest in the Aum Shinrikyo subway attacks. I really have the impression that it could have been a lot worse.\n\n**Jeffrey Ladish:** Yeah, for sure.\n\n**Daniel Filan:** [A thing I remember reading is that they had some block of sarin in a toilet tank underneath a vent in Shinjuku Station](https://www.nytimes.com/1995/05/18/world/how-tokyo-barely-escaped-even-deadlier-subway-attack.html) \\[Correction: they were bags of chemicals that would combine to form hydrogen cyanide\\]. Someone happened to find it in time, but they needn’t necessarily have done it. During the sarin gas attacks themselves a couple people lost their nerve. I don’t know, I guess that’s one-\n\n**Jeffrey Ladish:** They were very well resourced, right?\n\n**Daniel Filan:** Yeah.\n\n**Jeffrey Ladish:** I think that if you would’ve told me, “Here’s a doomsday cult with this amount of resources and this amount of people with PhDs, and they’re going to launch these kinds of attacks,” I definitely would’ve predicted a much higher death toll than we got with that.\n\n**Daniel Filan:** Yeah. They did a very good job of recruiting from bright university students. It definitely helped them.\n\n**Jeffrey Ladish:** You can compare that to [the Rajneeshee attack](https://en.wikipedia.org/wiki/1984_Rajneeshee_bioterror_attack), the group in Oregon that poisoned salad bars. What’s interesting there is that (1) it was much more targeted. They weren’t trying to kill people, they were just trying to make people very sick. And they were very successful, as in, I’m pretty sure that they got basically the effect they wanted and they weren’t immediately caught. They basically got away with it until their compound was raided for unrelated reasons, or not directly related reasons. Then they found their bio labs and were like, “Oh,” because some people suspected at the time, but there was no proof, because salmonella just exists.\n\n**Daniel Filan:** Yeah. There’s a similar thing with Aum Shinrikyo, where a few years earlier they had [killed this anti-cult lawyer](https://en.wikipedia.org/wiki/Sakamoto_family_murder) and basically done a very good job of hiding the body away, dissolving it, spreading different parts of it in different places, that only got found out once people were arrested and willing to talk.\n\n**Jeffrey Ladish:** Interesting.\n\n**Daniel Filan:** I guess that one is less of a wide-scale thing. It’s hard to scale that attack. They really had to target this one guy.\n\nAnyway, getting back to the topic of AI security. The first thing I want to ask is: from what you’re saying it sounds like it’s a general problem of corporate computer security, and in this case it happens to be that the thing you want to secure is model weights, but…\n\n**Jeffrey Ladish:** Yes. It’s not something intrinsic about AI systems. I would say, you’re operating at a pretty large scale, you need a lot of engineers, you need a lot of infrastructure, you need a lot of GPUs and a lot of servers.\n\nIn some sense that means that necessarily you have a somewhat large attack surface. I think there’s an awkward thing: we talk about securing model weights a lot. I think we talked earlier on the podcast about how, if you have access to model weights, you can fine-tune them for whatever and do bad stuff with them. Also, they’re super expensive to train, right? So, it’s a very valuable asset. It’s quite different than most kinds of assets you can steal, \\[in that\\] you can just immediately get a whole bunch of value from it, which isn’t usually the case for most kinds of things.\n\n[The Chinese stole the F-35 plans](https://thediplomat.com/2015/01/new-snowden-documents-reveal-chinese-behind-f-35-hack/): that was useful, they were able to reverse-engineer a bunch of stuff, but they couldn’t just put those into their 3D printers and print out an F-35. It’s so much tacit knowledge involved in the manufacturing. That’s just not as much the case with models, right? You can just do inference on them. It’s not that hard.\n\n**Daniel Filan:** Yeah, I guess it seems similar to getting a bunch of credit card details, because there you can buy stuff with the credit cards, I guess.\n\n**Jeffrey Ladish:** Yes. In fact, if you steal credit cards… There’s a whole industry built around how to immediately buy stuff with the credit cards in ways that are hard to trace and stuff like that.\n\n**Daniel Filan:** Yeah, but it’s limited time, unlike model weights.\n\n**Jeffrey Ladish:** Yeah, it is limited time, but if you have credit cards that haven’t been used, you can sell them to a third party on the dark web that will give you cash for it. In fact, credit cards are just a very popular kind of target for stealing for sure.\n\nWhat I was saying was: model weights are quite a juicy target for that reason, but then, from a perspective of catastrophic risks and existential risks, I think that source code is probably even more important, because… I don’t know. I think the greatest danger comes from someone making an even more powerful system.\n\nI think in my threat model a lot of what will make us safe or not is whether we have the time it takes to align these systems and make them safe. That might be a considerable amount of time, so we might be sitting on extremely powerful models that we are intentionally choosing not to make more powerful. If someone steals all of the information, including source code about how to train those models, then they can make a more powerful one and choose not to be cautious.\n\nThis is awkward because securing model weights is difficult, but I think securing source code is much more difficult, because you’re talking about way fewer bits, right? Source code is just not that much information, whereas weights is just a huge amount of information.\n\nRyan Greenblatt from Redwood has a great Alignment Forum post about [“can you do super aggressive bandwidth limitations outgoing from your data center?”](https://www.alignmentforum.org/posts/rf66R4YsrCHgWx9RG/preventing-model-exfiltration-with-upload-limits) I’m like: yeah, you can, or in principle you should be able to.\n\nI don’t think that makes you safe completely. You want to do defense in depth. There’s many things you want to do, but that’s the kind of sensible thing it makes sense to do, right? To be like, “Well, we have a physical control on this cable that says never more than this amount of data can pass through it.” Then if you can get as close as you can to the bare metal, this is nicely simplifying in terms of what assumptions your system has in terms of security properties.\n\nHaving a huge file, or a lot of information that you need to transfer in order to get your asset, just makes it easier to defend; \\[but\\] still difficult, as I think [the RAND report](https://www.rand.org/pubs/working_papers/WRA2849-1.html) is saying. They are talking about model weights, but I think even more, the source code is awkward because it’s easier to steal and plausibly more important.\n\n**Daniel Filan:** Yeah. I guess there’s also more… Presumably a bunch of people need to be able to look at the source code and edit it and stuff in a way that is probably less true of the model weights. Your interaction is a bit more abstracted.\n\n**Jeffrey Ladish:** Totally, yeah. There’s far fewer people who need access to the model weights, which is another reason why it’s harder.\n\nSecuring against hackers vs AI exfiltration\n-------------------------------------------\n\n**Daniel Filan:** Also from the catastrophic risk perspective… I guess source code is easier to steal. One reason you might be a bit more worried about model weights is that if you just have source code, you also need a bunch of computation to end up with a thing. And model weights, you have that baked in. If I think about the kinds of things you would need to do to secure source code, secure model weights… A thing people are also talking about is [“AI control”](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled), basically making sure that if an AI were misaligned and smart, but not like a billion times smarter than humans, could it escape from your lab and start running itself? Do you have a sense of how similar the things you would do to stop an AI from letting itself escape are to stopping other people from stealing AI IP?\n\n**Jeffrey Ladish:** I think there’s a bunch of areas of overlap and a bunch of areas of non-overlap. Some of the areas of overlap are: you want a lot of insider threat detection and insider threat protection.\n\nOne way that you could be hacked is that some employee or contractor of yours is compromised. Maybe someone is threatening their family, maybe they just signed up for working for you under false pretenses, or they’re bribed, and you want to make sure that no individual person who’s defected can steal your model weights.\n\nThere’s various ways that an AI system could try to hack out. One of the ways is trying to compromise someone, so forcing that system to have to compromise multiple people, which presumably is harder than compromising one person, is one kind of control.\n\nAlso, just in terms of “what does that system have access to?”, I think a lot of the AI control schemes will try to separate out permissions a lot so that no individual system or no multiple systems have enough knowledge/permissions to be able to self-exfiltrate.\n\nI think this is quite difficult. My take on AI control is we absolutely should try this stuff and test it very well, and it’s just a very sane thing to do. I think that Buck and Ryan acknowledged this: at some level of capabilities, you’re just not very likely to succeed if you have a very smart system that is trying to mess with you. Before that point, it seems like it makes a lot of sense try to do this.\n\nThose are some areas of overlap. Areas of non-overlap… One thing you’re trying to do also when you’re defending a system is you’re looking for anomalous stuff. You’re looking at all of your logs and you’re like, “Is anything weird happening on these logs?”\n\nYou don’t know where an attacker is going to show up, if they compromise someone internally or if they manage to gain access to an internal system where they shouldn’t have, you’re always looking for weird things that are happening there. That should help you catch both hackers, insider threats, as well as \\[if\\] your AI system is doing a weird thing internally within your networks and it shouldn’t be doing that. Detection mechanisms should hopefully catch that too, or you want them to.\n\nOne way in which I think it’s very different is that… I think one widely talked about (but still underrated) threat is just extreme persuasiveness. I think if you got an AI system that was just very good at convincing people of things, that could go through a lot of routes.\n\nOn one extreme end, you have the weird hypnosis thing where it looks at you, or it has a nice video and it looks into your eyes and says the right words and you’re just like, “Oh, I love you. I will do anything you say.” It’s a little ridiculous, but probably not impossible.\n\nThat’s one end of the extreme. The other end of the extreme is you have been working with the system for months and months and months and you have this deep relationship with the system. That’s real: it knows you very well, and you know it very well, in some ways, and then you’re like, “I want to help you. I like it. I have this attachment with it.”\n\nThen anywhere in between, right? Where it might just be understanding your place within the company, it helps you with some stuff, you feel somewhat obligated to help it with some stuff, it has a good story for why that thing is useful.\n\nI think this kind of capability is… Well, (1) I don’t know when this shows up relative to other kinds of hacking, deception, planning, deceptive misalignment. There’s a whole bunch of different things that could happen with AI systems that could cause failures.\n\nI don’t know where this kind of persuasion or deception might arise, but I think we should be treating it as a first class threat. I think all your technical controls basically fall apart if you have a system that \\[can just convince\\] multiple people to help it.\n\n**Daniel Filan:** If you’re worried about basically preventing corporate espionage or your model weights being leaked, it seems like preventing against human threats, people being planted in your organization, that seems like a thing you would be worried about, right?\n\n**Jeffrey Ladish:** Yeah. For sure.\n\n**Daniel Filan:** I guess I’m not seeing the asymmetry there…\n\n**Jeffrey Ladish:** Most humans are just not that persuasive. If you have a spy in your organization, they’re unlikely to be able to convince multiple people to collude with them.\n\n**Daniel Filan:** Right. Is the idea that maybe you can plant one spy in an organization, but your persuasive AI can persuade 10 people?\n\n**Jeffrey Ladish:** Yeah. It’s sort of like, where do you get superhuman capabilities? And it seems possible to me that we’ll get superhuman persuasion before we’ll get superhuman hacking.\n\n\\[It’s\\] not obvious. I’m not sure I would say that’s even more likely. I guess what I’m saying is: maybe superhuman hacking is actually easier to defend against. You can air gap a system. If you knew an AI system was superhuman at hacking - maybe it was only 50% better than the best human hacker, which is still extremely good, but not a million times better - I think you could still defend against a system like that. You need to air gap it and stuff, right? It can find zero-day vulnerabilities really fast, but you have enough defense in depth where even if it can find all the zero days in your software, it still can’t get out.\n\nWhereas I don’t think we have the equivalent for social defenses. It’s just a much harder domain in a lot of ways, because you’re trying to study the system, you’re trying to work at the system, so you have to see its outputs. So how do you make sure that you are not compromised? It’s like [SCP-level](https://en.wikipedia.org/wiki/SCP_Foundation) stuff.\n\n**Daniel Filan:** For those who don’t know, what is SCP?\n\n**Jeffrey Ladish:** SCP is this delightful internet culture: “secure, contain, protect”. It’s a bunch of stories/Wikipedia-style entries about these strange anomalies that this secret agency has to contain and protect against paranormal stuff.\n\n**Daniel Filan:** And it’s reminiscent of dealing with an AI that’s trying to mess with your mind or something.\n\n**Jeffrey Ladish:** Yeah. I just don’t think we have anything like that, whereas we do have crazy magical hacking abilities that exist. Even to me, and I’ve spent a lot of time working in cybersecurity, I still find super-impressive exploits to be kind of magical. It’s just like, “Wait, what? Your computer system suddenly does this?” Or, suddenly you get access via this crazy channel that…\n\nAn example would be [the iMessage zero-click vulnerability](https://citizenlab.ca/2023/09/blastpass-nso-group-iphone-zero-click-zero-day-exploit-captured-in-the-wild/) that the Israeli company [NSO Group](https://en.wikipedia.org/wiki/NSO_Group) developed a few years ago where they… It’s so good. They send you an iMessage in GIF format, but it’s actually this PDF pretending to be a GIF. Then within the PDF there’s this… It uses this PDF parsing library that iMessage happens to have as one of the dependency libraries that nonetheless can run code. It does this pixel XOR operation and basically it just builds a JavaScript compiler using this extremely basic operation, and then uses that to bootstrap tons of code that ultimately roots your iPhone.\n\nThis is all without the user having to click anything, and then it could delete the message and you didn’t even know that you were compromised. This was a crazy amount of work, but they got it to work.\n\nI read [the Project Zero blog post](https://googleprojectzero.blogspot.com/2021/12/a-deep-dive-into-nso-zero-click.html) and I can follow it, I kind of understand how it works, but still I’m like, “It’s kind of magic.”\n\nWe don’t really have that for persuasion. This is superhuman computer use. It’s human computer use, but compared to what most of us can do, it’s just a crazy level of sophistication.\n\nI think there are people who are extremely persuasive as people, but you can’t really systematize it in the way that you can with computer vulnerabilities, whereas an AI might be able to. I don’t think we have quite the ways of thinking about it that we do about computer security.\n\n**Daniel Filan:** Yeah. I guess it depends what we’re thinking of as the threat, right? You could imagine trying to make sure that your AI, it only talks about certain topics with you, doesn’t… Maybe you try and make sure that it doesn’t say any weird religious stuff, maybe you try and make sure it doesn’t have compromising information on people… if you’re worried about weird hypnosis, I guess.\n\n**Jeffrey Ladish:** But I think we should be thinking through those things, but I don’t think we are, or I don’t know anyone who’s working on that; where I know people working on how to defend companies from superhuman AI hacking.\n\nI think the AI control stuff includes that. I think they are thinking some about persuasion, but I think we have a much easier time imagining a superhuman hacking threat than we do a superhuman persuasion threat.\n\nI think it’s the right time to start thinking about concretely how we defend against both of these and starting to think about protocols for how we do that, and then somehow test those protocols. I don’t know how we test them.\n\n**Daniel Filan:** Yeah, I guess it’s the kind of eval you don’t really want to run.\n\n**Jeffrey Ladish:** Yes. But I think it’s interesting that [Eliezer Yudkowsky](https://en.wikipedia.org/wiki/Eliezer_Yudkowsky) was [talking about this kind of persuasion threat](https://www.yudkowsky.net/singularity/aibox), [AI box experiment](https://www.lesswrong.com/posts/nCvvhFBaayaXyuBiD/shut-up-and-do-the-impossible), back a decade ago or something. I think that was right. I think that was a smart thing to be thinking about then.\n\nWe were still very far away from those systems back then, and now I think we’re much closer. Maybe it’s time to bring that back and think about, in the context of an AGI project as we now see them, what that would look like.\n\n**Daniel Filan:** Yeah. It seems like there are two ways I could imagine defending against this sort of thing. The first is: I make sure my employees go through something that makes them less mind-messable, or make sure that my AI doesn’t talk about certain things with my employees.\n\nThen, a second kind of way I can avoid it is make sure that there’s some number, n, of people who could tank my company if they had their mind messed with by an AI. I want to just make that number as big as possible. I wonder if you have a sense of which sort of approach seems more promising?\n\n**Jeffrey Ladish:** Wait, I’m not sure I understood the latter one.\n\n**Daniel Filan:** The latter one is just make it so that a small number of people having their mind messed with by an AI is not a disaster for you.\n\n**Jeffrey Ladish:** Yeah. I think more the latter than the former. I think you probably try to do both. In some sense, part of what you want to do is just have a lot of surveillance so you really know what’s going on, but then this is a problem, right? Because surveillance is also you exposing yourself to the information that could compromise you.\n\nThis is not nearly as much of a problem in cyberspace. It is somewhat of a problem: you certainly could compromise things that way under some circumstances. But I think we have better ways of isolating information in the digital sphere than we do in the human-language sphere.\n\nMaybe you do a lot of rephrasing things. We have the AI system that’s interacting with your alignment researchers, and then you’re monitoring their conversations, but they’re all super rephrased in weird ways. I don’t know.\n\nI can imagine that you have multiple people that are rephrasing… This is kind of crazy, but I think it’s plausible we’ll see this kind of crazy thing.\n\nYeah, you have the person directly interacting with the AI system and then you have them talk to another person, and that person talks to another person, and then… The AI system is probably not so good that it can compromise the person and get that person to compromise another person and get that person to compromise another person, if they’re not all interacting with the AI system itself.\n\n**Daniel Filan:** Yeah. It does sound like an annoying way to get work done.\n\n**Jeffrey Ladish:** It sounds like a very difficult way to get work done, but this brings us back to the security thing, right? Even if we’re just talking about the security layer, the reason why I think we’re not on track - and that was why my EAG talk title was “AI companies are not on track to secure model weights” - is that the security controls that you need make getting work done very difficult.\n\nI think they’re just quite costly in terms of annoyance. I think [Dario](https://en.wikipedia.org/wiki/Dario_Amodei) made a joke that [in the future it’s going to have your data center next to your nuclear power plant next to your bunker.](https://youtu.be/Nlkk3glap_U?si=wOchD7NJIA-7aLTy&t=4476)\n\n**Daniel Filan:** Dario Amodei, CEO of [Anthropic](https://en.wikipedia.org/wiki/Anthropic)?\n\n**Jeffrey Ladish:** Yes. And well, the power thing is funny because there was recently I think Amazon built a data center right next to a nuclear power plant; we’re like two thirds of the way there. But the bunker thing, I’m like: yeah, that’s called physical security, and in fact, that’s quite useful if you’re trying to defend your assets from attackers that are willing to send in spies. Having a bunker could be quite useful. It doesn’t necessarily need to be a bunker, but something that’s physically isolated and has well-defined security parameters.\n\nYour rock star AI researchers who can command salaries of millions of dollars per year and have their top pick of jobs probably don’t want to move to the desert to work out of a very secure facility, so that’s awkward for you if you’re trying to have top talent to stay competitive.\n\nWith startups in general, there’s a problem - I’m just using this as an analogy - for most startups they don’t prioritize security - rationally - because they’re more likely to fail by not finding [product-market fit](https://en.wikipedia.org/wiki/Product-market_fit), or something like that, than they are from being hacked.\n\nStartups do get hacked sometimes. Usually they don’t go under immediately. Usually they’re like, “We’re sorry. We got hacked. We’re going to mitigate it.” It definitely hurts them, but it’s not existential. Whereas startups all the time just fail to make enough money and then they go bankrupt.\n\nIf you’re using new software and it’s coming from a startup, just be more suspicious of it than if it’s coming from a big company that has more established security teams and reputation.\n\nThe reason I use this as an analogy is that I think that AI companies, while not necessarily startups, sort of have a similar problem… OpenAI’s had multiple security incidents. They had a case where people could see other people’s chat titles. Not the worst security breach ever, but still an embarrassing one.\n\n\\[But\\] it doesn’t hurt their bottom line at all, right? I think that OpenAI could get their weights stolen. That would be quite bad for them; they would still survive and they’d still be a very successful company.\n\n**Daniel Filan:** OpenAI is some weights combined with a nice web interface, right? This is how I think of how OpenAI makes money.\n\n**Jeffrey Ladish:** Yeah. I think the phrase is “[OpenAI](https://twitter.com/miramurati/status/1726542556203483392?lang=en) [is](https://twitter.com/jasonkwon/status/1726541443400184125) [nothing](https://twitter.com/RichardMCNgo/status/1726589291135181205) [without](https://twitter.com/aliisarosenthal/status/1726542701976510919) [its](https://twitter.com/bradlightcap/status/1726541314148487184) [people](https://twitter.com/csvoss/status/1726542216372556093)”. They have a lot of engineering talent. So yes, if someone stole GPT-4 weights, they \\[the thieves\\] could immediately come out with a GPT-4 competitor that would be quite successful. Though they’d probably have to do it not in the U.S., because I think the U.S. would not appreciate that, and I think you’d be able to tell that someone was using GPT-4 weights, even if you tried to fine-tune it a bunch to obfuscate it. I’m not super confident about that, but that’s my guess. Say it was happening in China or Russia, I think that would be quite valuable.\n\nBut I don’t think that would tank OpenAI, because they’re still going to train the next model. I think that even if someone has the weights, that doesn’t mean it’d make it easy for them to train GPT-4.5. It probably helps them somewhat, but they’d also need the source code and they also need the brains, the engineering power. Just the fact that it was so long after GPT-4 came out that we started to get around GPT-4-level competitors with Gemini and with Claude 3, I think says something about… It’s not that people didn’t have the compute - I think people did have the compute, as far as I can tell - it’s just that it just takes a lot of engineering work to make something that good. \\[So\\] I think \\[OpenAI\\] wouldn’t go under.\n\nBut I think this is bad for the world, right? Because if that’s true, then they are somewhat incentivized to prioritize security, but I think they have stronger incentives to continue to develop more powerful models and push out products, than they do to…\n\nIt’s very different if you’re treating security as \\[if it’s\\] important to national security, or as if the first priority is making sure that we don’t accidentally help someone create something that could be super catastrophic. Whereas I think that’s society’s priority, or all of our priority: we want the good things, but we won’t want the good things at the price of causing a huge catastrophe or killing everyone or something.\n\n**Daniel Filan:** My understanding is that there are some countries that can’t use Claude, like Iran and North Korea and… I don’t know if China and Russia are on that list. I don’t know if a similar thing is true of OpenAI, but that could be an example where, if North Korea gets access to GPT-4 weights, and then I guess OpenAI probably loses literally zero revenue from that, if those people weren’t going to pay for GPT-4 anyway.\n\nThe state of computer security\n------------------------------\n\n**Daniel Filan:** In terms of thinking of things as a bigger social problem than just the foregone profit from OpenAI: one thing this reminds me of is: there are companies like [Raytheon](https://en.wikipedia.org/wiki/Raytheon) or [Northrop Grumman](https://en.wikipedia.org/wiki/Northrop_Grumman), that make fancy weapons technology. Do they have notably better computer security?\n\n**Jeffrey Ladish:** I know they have pretty strict computer security requirements. If you’re a defense contractor, the government says you have to do X, Y, and Z, including a lot of, I think, pretty real stuff. I think it’s still quite difficult. We still see compromises in the defense industry. So one way I’d model this is… I don’t know if you had a Windows computer when you were a teenager.\n\n**Daniel Filan:** I did. Or, my family did, I didn’t have my own.\n\n**Jeffrey Ladish:** My family had a Windows computer too, or several, and they would get viruses, they would get weird malware and pop-ups and stuff like that. Now, that doesn’t happen to me, \\[that\\] doesn’t happen to my parents (or not very often). And I think that’s because operating systems have improved. Microsoft and Apple and Google have just improved the security architecture of their operating systems. They have enabled automatic updates by default. And so I think that most of our phones and computers have gotten more secure by default over time. And this is cool.\n\nI think sometimes people are like, “Well, what’s the point even? Isn’t everything vulnerable?” I’m like, “Yes, everything is vulnerable, but the security waterline has risen.” Which is great. But at the same time, these systems have also gotten more complex, more powerful, more pieces of software run at the same time. You’re running a lot more applications on your computer than you were 15 years ago. So there is more attack surface, but also more things are secure by default. There’s more virtualization and isolation sandboxing. Your browser is a pretty powerful sandbox, which is quite useful.\n\n**Daniel Filan:** I guess there’s also a thing where: I feel like \\[for\\] more software, I’m not running it, I’m going to a website where someone else is running the software and showing me the results, right?\n\n**Jeffrey Ladish:** Yes. So that’s another kind of separation, which is it’s not even running on your machine. Some part of it is, there’s JavaScript running on your browser. And the JavaScript can do a lot: [you can do inference on transformers in your browser](https://aiserv.cloud/). There’s a website you can go to-\n\n**Daniel Filan:** Really?\n\n**Jeffrey Ladish:** Yes. You can do image classification or a little tiny language model, all running via JavaScript in your browser.\n\n**Daniel Filan:** How big a transformer am I talking?\n\n**Jeffrey Ladish:** I don’t remember.\n\n**Daniel Filan:** Do you remember the website?\n\n**Jeffrey Ladish:** I can look it up for you. Not off the top of my head.\n\n**Daniel Filan:** We’ll put it in the description.\n\n**Jeffrey Ladish:** Yeah. But it was quite cool. I was like, “wow, this is great”.\n\nA lot of things can be done server-side, which provides some additional separation layer of security that’s good. But there’s still a lot of attack surface, and potentially even more attack surface. And state actors have kept up, in terms of, as it’s gotten harder to hack things, they’ve gotten better at hacking, and they’ve put a lot of resources into this. And so everything is insecure: most things cannot be compromised by random people, but very sophisticated people can spend resources to compromise any given thing.\n\nSo I think the top state actors can hack almost anything they want to if they spend enough resources on it, but they have limited resources. So zero-days are very expensive to develop. For a Chrome zero-day it might be a million dollars or something. And if you use them, some percentage of the time you’ll be detected, and then Google will patch that vulnerability and then you don’t get to use it anymore. And so with the defense contractors, they probably have a lot of security that makes it expensive for state actors to compromise them, and they do sometimes, but they choose their targets. And it’s sort of this cat-mouse game that just continues. As we try to make some more secure software, people make some more sophisticated tools to compromise it, and then you just iterate through that.\n\n**Daniel Filan:** And I guess they also have the nice feature where the IP is not enough to make a missile, right?\n\n**Jeffrey Ladish:** Yes.\n\n**Daniel Filan:** You need factories, you need equipment, you need stuff. I guess that helps them.\n\n**Jeffrey Ladish:** Yeah. But I do think that a lot of military technology successfully gets hacked and exfiltrated, is my current sense. But this is hard… this brings us back to security epistemology. I feel like there’s some pressure sometimes when I talk to people, where I feel like my role is, I’m a security expert, I’m supposed to tell you how this works. And I’m like, I don’t really know. I know a bunch of things. I read a bunch of reports. I have worked in the industry. I’ve talked to a lot of people. But we don’t have super good information about a lot of this.\n\nHow many military targets do Chinese state actors successfully compromise? I don’t know. I have a whole list of “in 2023, here are all of the targets that we publicly know that Chinese state actors have successfully compromised”, and it’s 2 pages. So I can show you that list; I had a researcher compile it. But I’m like, man, I want to know more than that, and I want someone to walk me through “here’s Chinese state actors trying to compromise OpenAI, and here’s everything they try and how it works”. But obviously I don’t get to do that.\n\nAnd so [the RAND report](https://www.rand.org/pubs/working_papers/WRA2849-1.html) is really interesting, because [Sella](https://www.rand.org/about/people/n/nevo_sella.html) and [Dan](https://www.dlahav.com/) and the rest of the team that worked on this, they went and interviewed top people across both the public and private sector, government people, people who work at AI companies. They knew a bunch of stuff, they have a bunch of expertise, but they’re like, we really want to get the best expertise we can, so we’re going to go talk to all of these people. I think what the report shows is the best you can do in terms of expert elicitation of what are their models of these things. And I think they’re right. But I’m like, man, this sucks that… I feel like for most people, the best they can do is just read the RAND report and that’s probably what’s true. But it sure would be nice if we had better ways to get feedback on this. You know what I mean?\n\n**Daniel Filan:** Yeah. I guess one nice thing about ransomware is that usually you know when it’s happened, because -\n\n**Jeffrey Ladish:** Absolutely, there’s a lot of incentive. So I think I feel much more calibrated about the capabilities of ransomware operators. If we’re talking about that level of actor, I think I actually know what I’m talking about. When it comes to state actors: well, I’ve never been a state actor. I’ve in theory defended against state actors, but you don’t always see what they do. And I find that frustrating because I would like to know. And when we talk about Security Level 5, which is “how do you defend against top state actors?”, I want to be able to tell someone “what would a lab need to be able to do? How much money would they have to spend?” Or, it’s not just money, it’s also “do you have the actual skill to know how to defend it?”\n\nSo someone was asking me this recently: how much money would it take to reach SL5? And it takes a lot of money, but it’s not a thing that money alone can buy. It’s necessary but not sufficient. In the same way where if you’re like, “how much money would it take for me to train a model that’s better than GPT-4?” And I’m like: a lot of money, and there’s no amount of money you could spend automatically to make that happen. You just actually have to hire the right people, and how do you know how to hire the right people? Money can’t buy that, unless you can buy OpenAI or something. But you can’t just make a company from scratch.\n\nSo I think that level of security is similar, where you actually have to get the right kind of people who have the right kind of expertise, in addition to spending the resources. The thing I said in my talk is that it’s kind of nice compared to the alignment problem because we don’t need to invent fundamentally a new field of science to do this. There are people who know, I think, how to do this, and there \\[are\\] existing techniques that work to secure stuff, and there’s R&D required, but it’s the kind of R&D we know how to do.\n\nAnd so I think it’s very much an incentive problem, it’s very much a “how do we actually just put all these pieces together?” But it does seem like a tractable problem. Whereas AI alignment, I don’t know: maybe it’s tractable, maybe it’s not. We don’t even know if it’s a tractable problem.\n\n**Daniel Filan:** Yeah. I find the security epistemology thing really interesting. So at some level, ransomware attackers, you do know when they’ve hit because they show you a message being like “please send me 20 Bitcoin” or something.\n\n**Jeffrey Ladish:** Yeah. And you can’t access your files.\n\n**Daniel Filan:** And you can’t access your files. At some level, you stop being able to know that you’ve been hit necessarily. Do you have a sense for when do you start not knowing that you’ve been hit?\n\n**Jeffrey Ladish:** Yeah, that’s a good question. There’s different classes of vulnerabilities and exploits and there’s different levels of access someone can get in terms of hacking you. So for example, they could hack your Google account, in which case they can log into your Google account and maybe they don’t log you out, so you don’t… If they log you out, then you can tell. If they don’t log you out, but they’re accessing it from a different computer, maybe you get an email that’s like “someone else has logged into your Google account”, but maybe they delete that email, you don’t see it or whatever, or you miss it. So that’s one level of access.\n\nAnother level of access is they’re running a piece of malware on your machine, it doesn’t have administrator access, but it can see most things you do.\n\nAnother level of access is they have rooted your machine and basically it’s running at a permissions level that’s the highest permission level on your machine, and it can disable any security software you have and see everything you do. And if that’s compromised and they’re sophisticated, there’s basically nothing you can do at that point, even to detect it. Sorry, not nothing in principle. There are things in principle you could do, but it’s very hard. I mean it just has the maximum permissions that software on your computer can have.\n\n**Daniel Filan:** Whatever you can do to detect it, it can access that detection thing and stop it.\n\n**Jeffrey Ladish:** Yeah. I’m just trying to break it down from first principles. And people can compromise devices in that way. But oftentimes it’s noisy and if you are, I don’t know, messy with the [Linux kernel](https://en.wikipedia.org/wiki/Linux_kernel), you might… How much QA testing did you do for your malware? Things might crash, things might mess up, that’s a way you can potentially detect things. If you’re trying to compromise lots of systems, you can see weird traffic between machines.\n\nI’m not quite sure how to answer your question. There’s varying levels of sophistication in terms of malware and things attackers can do, there’s various ways to try to hide what you’re doing and there’s various ways to tell. It’s a cat and mouse game.\n\n**Daniel Filan:** It sounds like there might not be a simple answer. Maybe part of where I’m coming from is: if we’re thinking about these security levels, right -\n\n**Jeffrey Ladish:** Wait, actually I have an idea. So often someone’s like, “hey, my phone is doing a weird thing, or my computer is doing a weird thing: am I hacked?” I would love to be able to tell them yes or no, but can’t. What I can tell them is: well, you can look for malicious applications. Did you accidentally download something? Did you accidentally install something? If it’s a phone, if it’s a Pixel or an Apple phone, you can just factory reset your phone. In 99% of cases, if there was malware it’ll be totally gone, in part because \\[of\\] a thing you alluded to, which is that phones and modern computers have [secure element chips](https://en.wikipedia.org/wiki/Secure_element) that do firmware and boot verification, so that if you do factory reset them, this will work. Those things can be compromised in theory, it’s just extremely difficult and only state actors are going to be able to do it.\n\nSo if you factory-reset your phone, you’ll be fine. That doesn’t tell you whether you’re hacked or not though. Could you figure it out in principle? Yeah, you could go to a forensic lab and give them your phone and they could search through all your files and do the reverse-engineering necessary to figure it out. If someone was a journalist, or someone who had recently been traveling in China and was an AI researcher, then I might be like, yes, let’s call up a forensic lab and get your phone there and do the thing. But there’s no simple thing that I can do to figure it out.\n\nWhereas if it’s standard, run-of-the-mill malware… If it was my parents and they were like, “am I hacked?”, I can probably figure it out, because if they’ve been hacked, it’s probably someone tricked them into installing a thing or there’s some not super-sophisticated thing: actually you installed this browser extension, you definitely shouldn’t have done that, but I can see that browser extension is running and it’s messing with you, it’s adding extra ads to your websites.\n\nBut you’re not going to make that mistake, so if you come to me asking if your laptop is hacked… I mean, I’ll check for those things.\n\n**Daniel Filan:** I do have some browser extensions, I have to admit.\n\n**Jeffrey Ladish:** No, no, it’s okay to have browser extensions, but you probably didn’t get tricked into installing a browser extension that is a fake browser extension pretending to be some other browser extension and what it does is insert ads over everything else. You could, but it’s not likely.\n\n**Daniel Filan:** I don’t see that many ads, I guess.\n\n**Jeffrey Ladish:** Totally. But yeah, you just have this problem, which is: is this phone compromised? Well, it’s either not compromised, or it’s compromised by a sophisticated actor such that you’d have to do forensics on it in order to figure it out, and that’s quite complicated and most people can’t do it. Which is very unsatisfying. I want just to be like, “let me plug it into my hacking detector and tell whether this phone has been compromised or not”. That would be very nice. But you can have very sophisticated malware that is not easily detected. And so it becomes very difficult.\n\nHow AI labs could be more secure\n--------------------------------\n\n**Daniel Filan:** Sure. Related to this, one thing you were mentioning earlier is the difficulty of: you can have a bunch of security measures, but often they just make life really annoying for users. And yet somehow, computers have gotten more secure. It sounds like you think my phone is pretty secure. But they didn’t get that much more annoying to use. In some ways they’re a little bit slow, but I think that might be an orthogonal thing.\n\nSo do we have much hope of just applying that magic sauce to our AI, to whatever’s storing our weights?\n\n**Jeffrey Ladish:** Part of this comes down to how secure are they and in what ways are they secure? So your phone’s gotten a lot more secure, but we talked previously about the iMessage zero-click vulnerability, which completely compromises your iPhone if someone knows your phone number. And that’s really bad. If you had AI source code on your iPhone, anyone who was buying that piece of software from the Israeli government and had your phone number could just instantly get access to it (or not instantly, but in a few minutes or hours). And then that’s it, there’s no more additional steps they need to take. They just need to run that software and target you and then you’re compromised.\n\nAnd so at that level of sophistication, how do we protect you? Well, we can, but now we need to start adding additional security controls, and those additional security controls will be annoying. So for example, the first thing I’m going to do is I’m going to say, I want to minimize the attack surface. All of your email and Facebook and… I don’t know, there’s a lot of things you do on your phone. You’re not going to do any of those things, you’re going to have a separate phone for that, or you’re going to have a separate phone for things we want to be secure. And so we’re just going to be like, this is only for communication and maybe GitHub or a few other things, and that’s all you’re going to use it for. We’re not going to give your phone number to anyone.\n\nThere’s a whole bunch of ways to say, let’s just reduce the attack surface, figure out what are all of our assumptions. Any software that you’re running on your phone or computer could potentially be compromised. There’s sometimes ways to figure out what kind of software you’re running and then try to target those things in particular. So if I’m trying to defend against top state actors, I’m going to start being extremely strict around your hardware, around your software, any piece of software you’re running. And this starts to become very annoying, very fast.\n\nYou’re asking, well, what about the things that we do to make things more secure in general? Well, they are useful. Some of the things we do \\[are\\] application segregation and virtualization and sandboxing. So on your iPhone apps are sandboxed… I’m saying iPhone, but a Pixel would have similar things here. There’s a lot of sandboxing that happens, but it’s not perfect. It’s just pretty good. Most attackers can’t compromise that, but some could. And so raising the security waterline protects us against a lot of threats, but for any given piece of that stack, if you’re very sophisticated you can compromise it. So a superintelligence could hack everything. And it doesn’t really matter that the waterline has risen, for an attacker that’s that sophisticated.\n\nIn some ways it’s just economics, if that makes sense. There became a niche for people to do ransomware… By the way, part of the reason that happened was because of crypto. So you’re already familiar with this, but before crypto, there were just not very good ways to internationally transfer large sums of money. And then crypto came out and ransomware operators were like, “wait, we can use this”. In particular the property that’s so useful is irreversibility: if you send someone Bitcoin you can never get them to send it back to you via the courts or something. It is just gone.\n\n**Daniel Filan:** Well, I mean there’s nothing in the protocol that will let it do it. But if literally you stole some Bitcoin from literally me and I could prove it, and we went to the courts, I think they could say, “Jeffrey, you have to send him his Bitcoin back”.\n\n**Jeffrey Ladish:** Yes, totally. I mean there’s still the state monopoly on violence, but if you’re in another country and you don’t have extradition treaties or other things like that… I think most people who are ransomware-ing you aren’t going to be in the United States, they’re going to be in another country.\n\n**Daniel Filan:** It used to be that for bank payments or something, the bank could reverse it, and the blockchain cannot reverse it.\n\n**Jeffrey Ladish:** Yeah. And then because this was a new sort of socio-technical thing, you’ve suddenly got a new niche which is like, “oh, ransomware can be a lot more profitable now than it could before”. And then lo and behold, you get more ransomware.\n\nSo I think \\[in the\\] early internet, there just wasn’t that much incentive to hack people, and also people didn’t know about it, and then \\[when\\] people learned about it and there was more incentive, people started hacking people a lot more. And then customers were like, “I want more secure things”. And then the companies were like, “I guess we need to make them more secure” and then they made them more secure. But they only need to make them as secure as they need to be against the kind of threat that they’re dealing with, which is mostly not state actors, because state actors are not trying to compromise most people, they’re trying to target very specific targets.\n\nAnd so it just doesn’t make sense… Maybe Apple could design an iPhone that’s robust against state actors, but it’d be way more limited, way more annoying to use, and their customers wouldn’t appreciate that. So they don’t. They just make it as secure as it needs to be against the kinds of threats that most people face. And I think this is just often the case in the security world: you can make things more secure, it’s just harder, but we know how to do it. So if I wanted to make a communication device between the two of us that was robust against state actors, I think I could (or me and a team). What we’d do though is we’d make it very simple and wouldn’t have a lot of features. It wouldn’t have emojis. It’d just be text-\n\n**Daniel Filan:** Yeah, definitely don’t let that thing open PDFs.\n\n**Jeffrey Ladish:** No PDFs, right? It’s just text, AES encryption. We’re good to go. And we’ll do security audits. We’ll do the red teaming. We’ll do all these things to harden it. But at the end of the day… I mean with physical access you could probably still hack it, but other than that, I think you’ll be fine. But that’s not a very fun product to use.\n\n**Daniel Filan:** So I guess this gets to a question I have. \\[For\\] AI, a bunch of labs, they’re kind of secure, but they’re not as secure as we’d like them to be. Presumably some of that is them doing more of the stuff that we already know how to easily do, and presumably some stuff that needs to happen is just the world learning to make more secure devices… If you think about the gap between the ideal level of security that some big AI lab has and what they actually have, how much of it is them adopting best practices versus improving best practices?\n\n**Jeffrey Ladish:** This is going to be a pretty wild guess. I think there’s going to be the full RAND report coming out in the next few months, and I think in some ways this should just answer the question because they really are trying to outline “here are what we think the best practices would need to be for each level”, and then you can compare that against what are the existing best practices. But my sense is something like 20% of the thing is following the existing best practices and then 80% of the thing is improving… Maybe ‘best practices’ is actually a weird phrase here. ‘Best’ according to who and for what threat model? A lot of these things are somewhat known, but most people don’t have that threat model and so aren’t trying to apply it.\n\nI think Google is significantly ahead of most companies in this regard, where they have been thinking about how to defend against state actors for a long time. Microsoft and the other equivalently big companies as well. I think Google does it a bit better, I’m not entirely sure why, but maybe for partially historical reasons and partially cultural reasons or something.\n\nI think that if you go talk to the top Google security people, that’ll be pretty different than going and talking to some random [B2B](https://en.wikipedia.org/wiki/Business-to-business) company security people. Both of them might have a sense of the best practices, but the Google people are probably going to have a bunch of novel ideas of how to do the security engineering that is just quite a bit more advanced than what your random company will be able to do.\n\nMaybe 70% of what we need, you could get from the top Google security people and then 30% is new R&D that we need to do. But if you went and talked to the random security people at companies, you’d get 20% of the way there.\n\nThat’s all my rough sense. Hopefully that report will be out soon, we can dive in deeper then. And also I really hope that a bunch of other security experts weigh in, because the security epistemology thing is difficult, and I don’t know, maybe I’m full of \\[redacted\\], and maybe the RAND report’s wrong. I do think that this is the kind of thing that we need to get right, it’s very important. I think this is one of the most important things in the world. How do we secure AI systems? So there shouldn’t just be one report that’s like, “here’s how it works”. We should have all of the experts in.\n\nAnd I’m a big fan of just smart people thinking about stuff: you don’t need 20 years of experience to weigh in on this. You can be Ryan Greenblatt and just write [an Alignment Forum post](https://www.alignmentforum.org/posts/rf66R4YsrCHgWx9RG/preventing-model-exfiltration-with-upload-limits) being like, “would this thing work?” I’m like, yeah, more of that. Let’s get more people thinking about it. This stuff is not magic; it’s computer systems, we have them, we know a lot about them. Please, more people weigh in.\n\nWhat does Palisade do?\n----------------------\n\n**Daniel Filan:** Maybe now is a good time to move on to: what do you do at [Palisade](https://palisaderesearch.org/)? For those who don’t know, what is Palisade? What does it do?\n\n**Jeffrey Ladish:** We’re a research organization \\[and\\] we’re trying to study offensive capabilities of AI systems, and both looking at “what can current AI systems do in terms of hacking, deception, manipulation? What can autonomous systems do in the real world?” And I can say more about what some of those things are. But the overall reason we’re doing this is we want to help inform the public and policymakers around “what are the actual threats right now? Where are we?” In terms of: people hear a lot of stories about AI having all these hacking abilities, or they’ll be able to deceive people. I want us to be able to say: well, yeah, and let’s look at the specific threat models. Let’s be able to demonstrate, “hey, we can do these things. We can’t do these other things”. There’s definitely an evaluations component, to actually try to measure and benchmark: well, GPT-3.5 with this scaffolding can do this, GPT-4 with this scaffolding can do that.\n\nAnd then I really want to try to show where it’s going, or at least our best take on where it’s going; to try to look at, well, here’s what \\[the\\] last generation of systems could do, here’s what the current generation of systems can do. This is what the slope looks like, here’s where we think this is going. And that brings in some theory, it brings in some predictions that we’re making and a lot of our colleagues are making or collaborators are making. I think often people just get lost in the abstract arguments where they’re like, “okay, there’s this AGI thing and this superintelligence thing. What’s that really about?” And I think it’s just very useful for people to be able to ground their intuitions in: well, I saw this AI system deceiving people in this way, and it was very capable of doing this and not very capable of doing that. It might get 10 times as good at this thing, what would that mean? What would that actually imply? And I think this will help people think better about AI risk and be able to make better decisions around how we should govern it. That is a hope.\n\nSo right now our focus is really honing in on the deception and social engineering parts. So this open-source intelligence part, which is how you collect information about a potential target. The phishing component of being able to, via text or via voice, interact with the target and get information from them or convince them to do particular things. I think we want to just see: how good could we make these systems at doing those things right now?\n\nAnother related project that I’m pretty excited about is: can we have AI systems pay people - for example, [Taskrabbit](https://en.wikipedia.org/wiki/Taskrabbit) workers - to do complex tasks in the real world, including tasks that are adjacent to dangerous tasks but not actually dangerous themselves? So can you mix these weird chemicals together and would that be sufficient for someone to build a bomb, have an AI system build a bomb by paying people to do that task? And I don’t expect that this is very dangerous right now. I expect that yes, you could build a bomb. If you gave me six months and five engineers and built some AI system that could interact with TaskRabbit - we’ve done the very basic version of this. I think, yes, I could make a system that could hire some Taskrabbits to build a bomb successfully. Do I think this is important from a marginal risk difference? No.\n\n**Daniel Filan:** For one, there’s bombs and there’s bombs, right?\n\n**Jeffrey Ladish:** There’s bombs and there’s bombs, for sure. Also if I can hire five people, I could just hire them to build a bomb. If they’re engineers, I could build a much better bomb in six months than the shitty bomb that the Taskrabbit workers would be making, right?\n\nBut I think it’s very interesting, because knowing where we’re at right now… Well, (1) I think it will surprise people how smart AI systems are when they’re interacting with humans and paying them to do stuff. I just think that people don’t realize that they can do this right now. What we’ve done so far is have our system hire one Taskrabbit worker to fill up a bunch of balloons and bring them to an address and hire another Taskrabbit worker to buy a bunch of greeting cards that say congratulations and bring them to an address, and it worked. And so that’s the MVP, most basic thing. But I think people will be surprised that you can set up a system like this with the right scaffolding and do…. some things that I think people will be surprised by.\n\n**Daniel Filan:** Aside from giving somebody a birthday surprise or something, what kind of stuff can current language models do?\n\n**Jeffrey Ladish:** We’re just beginning to experiment. We’ll probably write it up soon, but one of my MATS scholars just built this MVP system: we have a Taskrabbit interface… I think the things we’re interested in testing right now are… Well, so one thing is: can we get a model that doesn’t have any guardrails to decompose tasks? In this case, \\[can we get\\] Mixtral to decompose dangerous tasks into harmless subtasks and then use a more powerful model to actually handle the interaction, like GPT-4. I think this will be pretty interesting. One of the things I want to do is put malware on a flash drive and then get that flash drive delivered to someone \\[who\\] doesn’t know that it contains malware and in fact, the person who delivered it didn’t even know that there was a thing on the flash drive per se.\n\nAnd there’s other things you could do with surveillance. Can you go and collect information about a target and then have some pretense for why you’re doing this? Basically just a bunch of things like this: how many things can you put together to make an attack chain? And I’m interested in that partially because this feeds into “how do we understand how well AI systems can hack and deceive?” And I also think it’ll be interesting because there’s a kind of reasoning I think that models are pretty good at, which is just giving good justifications for things. They’re very good bullshitters. I think that this is quite useful when you’re trying to convince people to do things that could be sketchy, because your models can just give good reasons for things.\n\nSo we saw this with [METR’s example with GPT-4](https://metr.org/blog/2023-03-18-update-on-recent-evals/), where they were getting a Taskrabbit worker to solve a CAPTCHA. The Taskrabbit worker was like, “are you an AI? Ha-ha”. And the chain of thought was like, “well, if I say I’m an AI, he’s not going to help me, so I’m going to say I’m a vision-impaired person”. And then people freaked out. They were like, “that’s so scary that an AI system would do that”. Well, the AI system was directed to do that - not lie per se, but solve the task. I think this is the kind of thing these systems are good at, which is providing justifications for things and being able to do basic reasoning about what something would sound like. And I want more people to know this fact. I think being able to show it in a real-world environment would help people understand where these systems are actually at.\n\nA lot of what I’m very interested in is just trying to get more people to see what I think most people can see if you’re very close to these systems, which is: what kind of systems do we actually have, and where are they good and where are they not good, and where would it be concerning if they got suddenly way more good in some of these areas?\n\n**Daniel Filan:** If you’re in the business of helping people understand what models are currently capable of, do you think that you have much to add over someone who just uses Claude or ChatGPT a bunch, or even a medium amount?\n\n**Jeffrey Ladish:** At some level, no. I really encourage people to do this, and if people aren’t doing this I usually tell them to do this for the goal of understanding how these systems work. In some ways, yes, which is: I think that these models can do a lot more with the right kind of scaffolding and the right kind of combining them with tools than they can do on their own.\n\nFor example, you can role-play with a model, pretend you’re having a phone conversation where you’re trying to get this piece of information, and you can sort of see how good the model is at role-playing that with you. But (1), prompting is hard. So good prompting definitely changes this a lot. But the other thing is: now let’s combine that model with a system that automatically clones someone’s voice and then calls up someone, a relevant target. And now you can see a bunch of things that… you still have the core part, which is this dialogue between you and the model in terms of trying to get this information, but now you’ve combined it with these other tools that allow it to do a lot more interesting things, plus potentially some very good prompting which elicits more of the model capabilities than people might be able to see on their own. That’s where I expect that will be helpful.\n\nI also think that today’s models plus a lot of complicated scaffolding for a specific task, tomorrow’s models might be able to do with very minimal scaffolding. So I think by pushing the current systems that we have as far as we can, might help us look a little further into the future around: well, this is for a very specific task, it took a lot of Taskrabbit-specific scaffolding to get this thing to work. But in the future we may not need all that Taskrabbit-specific scaffolding. We could just be like, “go hire this person”, and it might just work.\n\nAs an example of why I think this is the case: in this Taskrabbit system, there’s a component of it where we use this open source browser extension called [Taxy AI](https://github.com/TaxyAI/browser-extension), which uses GPT-4 to fill out forms in a website or click on things. And we could have automated all of that by hand. We could have hard-coded all of the HTML and JavaScript to be like, “if this form, fill out this form” or something like that, or “select this”. But we didn’t have to do all of that, because GPT-4 on its own plus a little scaffolding can do this in a general-purpose way. Taxy AI works across most websites. I haven’t tried it, but I’m pretty sure if you tried to use Taxy AI with GPT-3.5 it would be way worse at doing this. GPT-3.5 I don’t think has enough general capabilities to use websites to be able to most times successfully fill out a form on a website. And so I think if that’s true - which we should test - that’s interesting evidence for this increasing generalization that language models have as they get more powerful. And I think that that will be interesting, because then there’s this transfer from “you can do it with a lot of scaffolding if you really keep your model on track” to “oh, now the model can do it without you needing that”.\n\nAI phishing\n-----------\n\n**Daniel Filan:** For people who aren’t super familiar, can you share just concretely, how good is it at this kind of stuff? How good is it at phishing or deception or stuff?\n\n**Jeffrey Ladish:** It’s quite good at writing phishing emails. Sorry, what I should say is: if you’re really good at prompting, it’s very good at phishing. And if you’re not very good at prompting, it’s okay at phishing, because it’s been fine-tuned to write in a particular style and that style is pretty recognizable. It’s not going to mimic your email writing style, for example.\n\nBut I think it’s not very hard to get a language model to get some information off the internet. You can just do it in ChatGPT right now, if you just type in the name of someone you want to send an email to: if they exist on the internet, Bing will just find web pages about them, and even just within ChatGPT, the console, it can grab information and compose a phishing email for them. Don’t say it’s a phishing email, just say it’s a legitimate email from this fictional person or whatever, and it will write a nice email for you.\n\nSo I think that’s pretty easy to see. That’s important because it’s very scalable, and I just expect that people will do this. I expect that people will send lots and lots of phishing emails, spear phishing emails targeted at people, by building automated systems to do all the prompting, do all the OSINT beforehand and send out all these phishing emails. I expect this will make people a lot of money. I think it’ll work for ransomware and for other things like that.\n\nPeople will design defenses against them. I think this is an offense-defense game. I think we will be caring a lot more about: where are the emails coming from and can we verify the identity of the person sending me this email? As opposed to just looking at the text content and being like, “that looks plausible”. That will stop working, because language models are good enough to… I think basically language models are almost to the point, with the right kind of prompting and scaffolding, that they can… we’re trying to test this, but I’m making a hypothesis that they’re about as good as a decent human at the task for one-shot phishing emails.\n\n**Daniel Filan:** Actually, I just want to ask: so if I’m trying to phish someone, I have to pick a target, I have to write an email. I also have to create a website where they type in their stuff that I want to get from them.\n\n**Jeffrey Ladish:** Sometimes. There’s different types of phishing. So yeah, you need the target’s email, first of all. So maybe that’s one tool that your automated system could help with. You need to write a message to them. And then, what are you trying to get them to do? One kind of phishing is called “credential phishing”, which is: you’re trying to get them to enter a password in a fake website. So you create a fake website, they go to the fake website, enter their password, but it’s actually your website. Maybe you redirect them to the original website or whatever so they don’t really know.\n\nThere’s also phishing where you’re trying to get them to download a piece of software and run it so you can install malware on their computer. You might be like, “Hey, can you take a look at my resume? Yeah, sorry, this is kind of weird, you have to enable this particular thing because I wanted to show this interactive feature.” I don’t know, something like that.\n\nThere’s also more sophisticated versions of phishing, where you actually trigger a vulnerability that they might have. So that might be, they click on a link and that link has a Chrome zero-day or something where that compromises your browser. These are very rare because they’re expensive, and if your computer’s up-to-date, you mostly shouldn’t have to worry about them.\n\nSo often people ask me, “Hey, I clicked on this link, am I screwed?” And I’m like, “Probably not.” If a state actor was attacking you and they were prioritizing you, then yes, but in 99% of cases, that’s not what’s happening. So usually it’s more like credential phishing. A lot of times, the initial phishing email is actually just to start the interaction, and then it’s a more sophisticated kind of social engineering.\n\nI have [a blog post](https://jeffreyladish.com/anatomy-of-a-rental-phishing-scam/) where I was looking for houses to rent, and I saw this post on Craigslist and I emailed them and \\[said\\] I would like to go see the house. And they’re like, “Cool, can we switch to text? I’m in Alaska fishing, and so it’s kind of hard for me,” or whatever. And so we started to have this conversation, and I started to realize something was weird. One of the signs is that they wanted me to switch from email to text, because text is not monitored in the way that Gmail is monitored. So Gmail will be looking for patterns, and at some point will start to flag email addresses if they’ve been phishing multiple people, whereas SMS doesn’t do that.\n\nBut I couldn’t figure out what the scam was, because the guy kept talking to me and I was like, “well, you’re not asking me for money, you’re not asking me for credentials”. And eventually they sent me a link to Airbnb.com, but it was not actually Airbnb.com. It was a domain that was close by. And then the scam was “oh, can you just rent this on Airbnb for a few weeks?” But it was actually not Airbnb, it was just going to take my credit card information. And I was like, “that’s clever”.\n\nSo that’s the kind of scam that you might have. You can convince people to send money to people. There’s [a recent deepfake video call](https://www.cnn.com/2024/02/04/asia/deepfake-cfo-scam-hong-kong-intl-hnk/index.html) with someone in Hong Kong where they got them to wire $50 million to some other company: that was deepfake voice and video chat to convince them to send it to the wrong place.\n\n**Daniel Filan:** So I guess what I’m trying to ask is: I don’t know if this is the term, but there’s some payload with phishing. You write a nice email and then you want them to click on a link or you want them to do something.\n\n**Jeffrey Ladish:** Or give them information also.\n\n**Daniel Filan:** Yeah. And so I guess for some of these, you have to design the thing: if you want them to download a PDF that has some nasty stuff in it, you have to make that PDF.\n\n**Jeffrey Ladish:** Yeah.\n\n**Daniel Filan:** So I’m wondering: how much can large language models do on the payload side? I guess you’re saying there are things where it’s not that complicated to make the payload?\n\n**Jeffrey Ladish:** Yeah, I think they’re helpful with the payload side. It’s kind of similar to “how well can models make websites, how well can they write code?” Yeah, they’re pretty helpful. They’re not going to do it fully autonomously, or they can do it fully autonomously, but it’ll suck. But yeah, I think they’re useful.\n\nSo there’s “can they help with the payload? Can they help with the writing the messages?” And then there’s “can they do an iterated thing?” I think that’s where the really interesting stuff lies. They can write good phishing emails, but can they carry on a conversation with someone in a way that builds trust and builds rapport, and that makes it much more likely for the target to want to listen to them or give them information or carry out the action?\n\nSo I’m both interested in this on the text side, in terms of email agent or chatbot. I’m also very interested in this on the voice conversational side, which I think will have its own challenges, but I think it’s very compelling, if you see an AI system deceive someone in real time via voice. I’m not sure if we’ll be able to do that. I think it’s hard with latency and there’s a lot of components to it. But it’s not that far out. So even if we can’t do it in the next few months, I bet in a year we could totally do it. And I expect that other people will do this too. But I’m excited. It’s going to be very interesting.\n\n**Daniel Filan:** Yeah, I guess it’s also more promising that… if I get an email from somebody purporting to be Jeffrey Ladish, I can check the email address usually. But there are just a lot of settings where I’m talking to someone via voice and it’s because they clicked on a Google Meet link or they’re like, “Oh yeah, I don’t have my phone on me, so I’m calling from a friend’s phone,” or something. It seems… I don’t know, maybe the world just has to get better at verifying who’s calling you by means other than voice.\n\n**Jeffrey Ladish:** I think that will be one of the things that will naturally have to happen, and people will have to design defenses around this. I think this can be one of the big shocks for people, in terms of “the world is really different now”. I think people will start to realize that they can’t trust people calling them anymore. They just don’t know that they are.\n\nBecause I think a lot of people will get scammed this way. And I think people will start to be like, “shoot, if it’s a weird number and it calls me and it sounds like my mom, it might not be my mom”. And I think that’s very unsettling for people. It’s just a weird thing to exist. It’s not necessarily the end of the world, but it is a weird thing. And I think people are going to be like, “What? What’s happening?”\n\n**Daniel Filan:** I guess maybe we just do more retreating to… now instead of phone calls, it’s Facebook Messenger calls, or calls in some place where hopefully there is some identity verification.\n\n**Jeffrey Ladish:** Yeah, I think that seems likely.\n\nMore on Palisade’s work\n-----------------------\n\n**Daniel Filan:** So my understanding is that you’re trying to understand what current language models can do and also having some interfacing with policymakers just to keep them up to date?\n\n**Jeffrey Ladish:** Yeah. The additional step here is: let’s say we’ve run some good experiments on what current systems can do; hopefully both the last generation and the current generation, a few systems, so we have some sense of how fast these capabilities are improving right now. And then I think we really want to be also trying to communicate around our best guess about where this is going: both in a specific sense - how much is this phishing stuff going to improve? We were just talking about voice-based phishing and models that can speak in other people’s voices - but also in terms of what happens when AI systems that have these deception capabilities, have these hacking capabilities, become much more agentic and goal-oriented and can plan, and can now use all of these specific capabilities as part of a larger plan?\n\nAnd many people have already talked about these scenarios: AI takeover scenarios, ways in which this could fail and AI systems could seek power and gain power. But I think I want to be able to talk through those scenarios in combination with people being able to see the current AI capabilities, \\[and\\] to be like, “well, obviously these current AI systems can’t do the ‘take power’ thing, but what do we think has to be different? What additional capabilities do they need before they’re able to do that?” So that people can start thinking in that mindset and start preparing for: how do we prevent the stuff we don’t want to see? How do we prevent AI systems from gaining power in ways that we don’t want them to?\n\nSo I think that’s a very important component, because we could just go in and be like, “here’s our rough sense”, but I think it’s going to be more informative if we combine that with research that’s showing what current systems can do.\n\n**Daniel Filan:** The second component of “telling some story of what things you need” - first of all, where are you on that?\n\n**Jeffrey Ladish:** I think right now it’s mostly at the level of the kind of thing we’re doing now, which is just having conversations with people and talking through it with them. We haven’t made any big media artifacts or written any long position papers about it. We’ll totally do some of that, too, and I think try to make things that are pretty accessible to a variety of audiences. Maybe some videos, definitely some blog posts and white papers and such. But so far, we’re in the planning stages.\n\n**Daniel Filan:** How much of a response have you gotten from stuff that you’ve done so far?\n\n**Jeffrey Ladish:** The [BadLLaMa](https://arxiv.org/abs/2311.00117) [stuff](https://arxiv.org/abs/2310.20624) was pretty interesting because… the whole premise of this was, “here’s a thing that I think most people in the AI research community understand; it’s not very controversial. I think that most policymakers don’t understand it and most people in the public don’t understand it”. Even though that was my hypothesis, I was still surprised when in fact that was true. We’d go and show people this thing, and then they were like, “Wait, what? You can just remove the guardrails? What do you mean?” They’re like, “That’s scary. And it was really cheap?” And I’m like, “Yeah, that’s how it works.” And people didn’t know that, and it’s very reasonable for people not to know that.\n\nI think I keep making this mistake not at an intellectual level, but at a visceral level, a System 1 level, where I assume people know the things I know a little bit, or if all my friends know it, then surely other people know it. At some level I know they don’t. I think we had friends talk to people in Congress and show this work to them, people in the Senate and people in the White House. There’s definitely a big mix. There’s a lot of very technically savvy congressional staffers that totally get it, and they already know, but then they can then take it and go talk to other people.\n\nI think the most interesting stuff that happened with the Senate was in the initial [Schumer forum](https://www.businessinsider.com/mark-zuckerberg-senate-ai-forum-llama-2-gives-anthrax-instructions-2023-9). [Tristan Harris](https://en.wikipedia.org/wiki/Tristan_Harris) mentioned the work and talked to Mark Zuckerberg about it. And Mark Zuckerberg made the point that, “well, you can get this stuff off the internet right now”. And I’m like, “Yep, that’s true, but can we talk about future systems?” We’ve really got to figure out what is the plan for where your red lines should be.\n\nI think people are really anchored on the current capabilities. And the thing we really need to figure out is: how do we decide where the limits should be? And how do we decide how far we want to take this stuff before the government says you need to be doing a different thing? I’m getting in a little bit of a rabbit hole, but I feel like that’s the whole question I have right now with governance of AI systems, where currently the governance we have is like, “well, we’ll keep an eye on it. And you can’t do crimes.”\n\nIt sure seems like we should actually be more intentional around saying: well, we are moving towards systems that are going to become generally capable and probably superintelligent. That seems like that presents a huge amount of risk. At what point should we say you have to do things differently, and it’s not optional, you are required by the government to do things differently? And I think that’s just the thing we need to figure out as a society, as a world, really.\n\nI don’t know, I just want to say it because that feels like the important, obvious thing to say, and I think it’s easy to get lost in details of evaluations and “but what are the risks of misuse versus agency?” and all of these things. At the end of the day, I feel like we’ve got to figure out how should we make AGI and what should people be allowed to do and not do, and in what ways?\n\nRed lines in AI development\n---------------------------\n\n**Daniel Filan:** So this question of “at what point do things need to be different?”, how do you think we answer that? It seems like stuff like demos is on the step of “just observe, figure out what’s happening”. Is that roughly right?\n\n**Jeffrey Ladish:** Yeah, I think so.\n\n**Daniel Filan:** Do you have thoughts about where things need to go from there?\n\n**Jeffrey Ladish:** I think we need to try to forecast what kinds of capabilities we might have along what relevant time scales. And I think this is very difficult to do and we’ll have pretty large uncertainty, but if I were the government, I’d be like, “well, how far off are these very general-purpose capabilities? How far off are these domain-specific but very dangerous capabilities? How much uncertainty do we have and what mitigations do we have?” And then try to act on that basis.\n\nIf the government believed what I believed, I think they’d be like, “all frontier developers need to be operating in very secure environments and need to be very closely reporting what they’re doing and need to be operating as if they were more like a nuclear weapons project than they are like a tech company”. And that’s because I think that there’s a substantial possibility that we’re not that far from AGI.\n\nAnd we don’t know. And from my epistemic state, maybe it’s ten years or maybe it’s two. And if you have that much uncertainty and you think there’s a 10% chance that it’s two years, I think that more than justifies taking pretty drastic actions to not accidentally stumble into systems that you won’t be able to control.\n\nAnd then on the domain-specific stuff, I think our society is potentially pretty robust to a lot of things. I think on the bio side, there’s potentially some scary things that are not there yet, but I think we should definitely be monitoring for those and trying to mitigate those however we can. But I do think it’s more the agentic capabilities to scientific R&D, the more general intelligence things… I think there’s definitely points of no return if we go beyond those things. I think it’s unlikely we’ll be able to stay in control of the systems.\n\nI think the first thing you need to do is be able to actually have an off switch or an emergency brake switch to say, “well, if we cross these red lines…” There’s definitely many red lines I could say \\[where\\] if you have gotten to this point, you’ve clearly gone too far. If you suddenly get to the point where you can 100% just turn your AI systems into researchers and have them build the next generation of a system that is more than 2X better, and it took no humans at all, yes, you have gone too far. You now have an extremely dangerous thing that could potentially improve extremely quickly. We should definitely, before we get there, stop, make sure we can secure our systems, make sure that we have a plan for how this goes well.\n\nAnd I don’t know where the right places to put the red lines are, but I want [Paul Christiano](https://paulfchristiano.com/) and… I want to take the best people we have and put them in a room and have them try to hash out what the red lines are and then make hopefully very legible arguments about where this red line should be, and then have that conversation.\n\nAnd yeah, there’s going to be a bunch of people that disagree. Sure, throw [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) in the room too. That will suck. I don’t like him. But he’s a very respected researcher. So I do think it shouldn’t just be the people I like, but I think that a lot of the AI research community could get together and the government could get together and choose a bunch of people and end up with a not totally insane assortment of people to try to figure this out.\n\nThat seems like a thing we should do. It just seems like the common sense thing to do, which is just take the best experts you have and try to get them to help you figure out at what point you should do things very differently. And maybe we’re already at that point. I think we’re already at that point. If I was on that panel or in that room, that’s what I would argue for. But people can reasonably disagree about this.\n\nThat’s the conversation I really want to be happening. And I feel like we’re inching there and we’re talking about evaluations and [responsible scaling policies](https://metr.org/blog/2023-09-26-rsp/) and all this stuff. And it’s good, it’s in the right direction, but man, I think we’ve got to have these red lines much more clear and be really figuring them out now, like, yesterday.\n\n**Daniel Filan:** I guess a bunch of the difficulty is defining concretely, what’s the actual stuff we can observe the world that’s the relevant types of progress or the relevant capabilities to be worried about? I guess a cool thing about the [AI safety levels](https://www.anthropic.com/news/anthropics-responsible-scaling-policy) \\- I’m embarrassed to say that I haven’t read them super carefully - stuff like evaluations, AI safety levels, it seems nice to just be able to have a thing of “hey, here’s a metric, and if it gets above seven, that’s pretty scary”.\n\n**Jeffrey Ladish:** Yeah, I do like this. I quite like this. And I think that the people working on those have done a great job taking something that was not at all legible and making it much more legible. Some work that I really would like to see done is on the understanding-based evaluations, where we don’t just have evaluations around how capable these systems are, we also have [evaluations on how well do we actually understand what these systems are doing](https://www.alignmentforum.org/posts/uqAdqrvxqGqeBHjTP/towards-understanding-based-safety-evaluations) and why they’re doing it, which is measuring our ability to do mech interp (mechanistic interpretability) on some of these systems.\n\n[Evan Hubinger](https://www.alignmentforum.org/users/evhub) at Anthropic has proposed this as an important direction, but as far as I know, we have not yet developed… When I talk to the interpretability people, what they say is, “our interpretability tools are not yet good enough for us to even be able to specify what it would mean to really understand a system”. And I believe that that’s true, but also, I talk to the same researchers and I’m like, “Well, do we understand the systems?” And they’re like, “No, definitely not.”\n\nAnd I’m like, “Okay, so you seem confident that we don’t understand these systems and you have some basis for your confidence. Can you give me a really bad metric, a very preliminary metric for: well, if we can’t answer these questions, we definitely don’t understand them?” Just because we can answer these questions doesn’t mean we super understand them well, but this thing is a bit far off and we can’t do it yet. I would like that because I think then we could at least have any understanding-based evaluation, and then hopefully we can develop much better ones.\n\nI think as the capabilities increase and the danger increases, I really want us to be able to measure how much progress we’re making on understanding what these systems are and what they do and why they do them. Because that is the main way I see us avoiding the failure modes of: the system behaved well, but sorry, it was deceptively misaligned and it was plotting, but you weren’t testing for whether you understood what was going on. You only tested for “did the behavior look nice?” But I think there’s a lot of very sound arguments for why systems might look nice, but in fact be plotting.\n\n**Daniel Filan:** I think of [Redwood Research’s work on causal scrubbing](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing) where essentially they’re trying to answer this question of: you say that this part of this network is responsible for this task. Can we check if that’s right? And they’re roughly trying to say, “okay, we’re going to compare how well the network does on that task to if we basically delete that part of the network, how well does it do then?” I’m wondering… one might’ve thought that that would be just what you wanted in terms of understanding-based evaluations. What’s the gap between that and what’s sufficient?\n\n**Jeffrey Ladish:** I don’t know. I think I don’t understand the causal scrubbing work well enough to know. I mean, that seems very good to do.\n\n**Daniel Filan:** I mean, I guess one difficulty is it’s just like “this part of the network is responsible for performance on this task”. There’s still a question of “well, what’s it doing? Why is it responsible for that?”\n\n**Jeffrey Ladish:** Yeah, I think there’s probably many stories you could tell where you could have a tool like that, that appeared to be working correctly, but then the model is still [scheming](https://arxiv.org/abs/2311.08379).\n\nIt does seem great to have any tool at all that can give you additional checks on this. But I think this is true for a lot of interpretability tools: it’d be very good if they got to the point where they could start to detect bad behavior, and I expect you’ll be able to do this long before you have a fully mechanistic explanation for all of what’s happening. And so this gives you the ability to detect potential threats without being a super robust guarantee that there are no threats. \\[So it’s\\] better than nothing, let’s just not confuse the two; let’s note that we will catch more than we would’ve otherwise, but we don’t know how many we’ll catch, per se.\n\n**Daniel Filan:** Yeah. I guess it’s also about validating an explanation rather than finding threats.\n\n**Jeffrey Ladish:** Yeah, that makes sense.\n\n**Daniel Filan:** It also has this limitation of just being about bits of the network, where you might’ve hoped that you could understand neural network behavior in terms of its training dynamics or something. Causal scrubbing doesn’t really touch that. I guess the closest thing to this that I can… I probably don’t have an exhaustive view of this literature, but a previous guest on the podcast, [Stephen Casper](https://axrp.net/episode/2023/05/02/episode-21-interpretability-for-engineers-stephen-casper.html), has [some stuff](https://arxiv.org/abs/2302.10894) basically just trying to get benchmarks for interpretability and trying to say, can we understand XYZ behavior? Can you use your understanding to do this task or that task?\n\n**Jeffrey Ladish:** Yeah, love to see it. That’s great.\n\n**Daniel Filan:** Yeah. And basically the answer is “interpretability is not doing so hot yet”, unfortunately.\n\n**Jeffrey Ladish:** Yeah, that makes sense.\n\nMaking AI legible\n-----------------\n\n**Daniel Filan:** So I guess we’re about at the end of the discussion. Before we close up, I’m wondering, is there anything you wish that I’d asked that I haven’t?\n\n**Jeffrey Ladish:** That’s a good question. Let me think about that a bit. I don’t know what the question is exactly, but there’s an interesting thing about our research at Palisade, which is: some of it’s just trying to understand because we don’t know the current capabilities of systems. And it’s quite interesting from an evals frame, because evaluations, I think, are often framed as “we are trying to understand how capable this model is”. That’s kind of true, but really what you’re testing is “how capable is this model combined with this programmatic scaffolding (which can be quite complex)?”, or “how capable is this model combined with this programmatic scaffolding and other models?” And we don’t necessarily know how much scaffolding will influence this, but at least a fairly large degree.\n\nSo a lot of this research with evals, with demonstrations… there is a question of what is it for? Some of it’s for just trying to understand stuff, which is “how good are models? How good are models plus scaffolding?” But another part of it is trying to make AI capabilities more legible. And I think this is a very important part of the research process and \\[important\\] for organizations that are trying to do research and communication, because I see our best shot at a good AI development path as one where it’s a lot more legible what the risks are, what potential mitigations and solutions are, and still how far we need to go.\n\nThere’s just a lot more about AI development that I want to be more legible, in part because I really believe that because of how fast this is going and because of how multipolar it is and how many different actors there are, we really need to be able to coordinate around it. We need to coordinate around “when do we want to make AGI? When do we want to make superintelligent systems? How risk tolerant are we, as the United States, or as the world?”\n\nAnd right now, I don’t think that most people have grappled with these questions, in part because they don’t really know that we are facing these things. They don’t know that there’s a bunch of companies that have in their five-year plan, “build AGI, be able to replace most human workers”. But I think that is the case.\n\nAnd so it seems like now is a very important time to have conversations and actually put in place political processes for trying to solve some of these governance and coordination questions. And I think in order to do that, we’re going to have to explain a lot of these things to a much wider audience and not just have this be a conversation among AI researchers, but a conversation among AI researchers and policy experts and policy leaders and a lot of random constituents and a lot of people who are in media and a lot of people who are working on farms. I think that a lot more people need to understand these things so that they can be able to advocate for themselves for things that really affect them.\n\nAnd this is very difficult. Sometimes it’s not difficult because I think some of the basic risks are pretty intuitive, but what’s difficult is that people have so many different takes on these risks, and there’s so many different ways to make different points: you have [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) over here being like, “Oh, you guys are totally wrong about this existential risk stuff,” and you have \\[Geoffrey\\] [Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton) and \\[Yoshua\\] [Bengio](https://en.wikipedia.org/wiki/Yoshua_Bengio) over here being like, “No, actually, I think these risks are super severe and important, and we have to take them very seriously.”\n\nAnd people might have somewhat good intuitions around these, but also now you have a lot of sophisticated arguments you have to try to evaluate and figure out. And so I don’t know how to solve this, but I do want much more legible things that more people can follow, so that they can weigh in on the things that do affect them a lot.\n\nFollowing Jeffrey’s research\n----------------------------\n\n**Daniel Filan:** Finally, before we end, if people are interested in following your research, how should they do that?\n\n**Jeffrey Ladish:** So they can follow, we’ll be posting stuff on our website, [palisaderesearch.org](https://palisaderesearch.org/). You can follow me on Twitter [@JeffLadish](https://twitter.com/JeffLadish). I go by Jeffrey, but when I made my Twitter account, I went by Jeff. We’ll probably be publishing in other places too, but those are probably the easiest places to follow.\n\n**Daniel Filan:** All right, well, thanks for coming on the podcast.\n\n**Jeffrey Ladish:** Yeah, thanks for having me.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Filming occurred at [FAR Labs](https://far.ai/labs/). Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) and [Lightspeed Grants](https://lightspeedgrants.org/), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nTop labs use various forms of “safety training” on models before their release to make sure they don’t do nasty stuff - but how robust is that? How can we ensure that the weights of powerful AIs don’t get leaked or stolen? And what can AI even do these days? In this episode, I speak with Jeffrey Ladish about security and AI.\n\nTopics we discuss:\n\n * Fine-tuning away safety training\n * Dangers of open LLMs vs internet search\n * What we learn by undoing safety filters\n * What can you do with jailbroken AI?\n * Security of AI model weights\n * Securing against hackers vs AI exfiltration\n * The state of computer security\n * How AI labs could be more secure\n * What does Palisade do?\n * AI phishing\n * More on Palisade’s work\n * Red lines in AI development\n * Making AI legible\n * Following Jeffrey’s research\n\nDaniel Filan: Hello, everybody. In this episode, I’ll be speaking with Jeffrey Ladish. Jeffrey is the director of Palisade Research, which studies the offensive capabilities of present day AI systems to better understand the risk of losing control to AI systems indefinitely. Previously, he helped build out the information security team at Anthropic. For links to what we’re discussing, you can check the description of this episode and you can read the transcript at axrp.net. Well, Jeffrey, welcome to AXRP.\n\nJeffrey Ladish: Thanks. Great to be here.\n\n\nFine-tuning away safety training\nDaniel Filan: So first I want to talk about two papers your Palisade Research put out. One’s called LoRA Fine-tuning Efficiently Undoes Safety Training in Llama 2-Chat 70B, by Simon Lermen, Charlie Rogers-Smith, and yourself. Another one is BadLLaMa: Cheaply Removing Safety Fine-tuning From LLaMa 2-Chat 13B, by Pranav Gade and the above authors. So what are these papers about?\n\nJeffrey Ladish: A little background is that this research happened during MATS summer 2023. And LLaMa 1 had come out. And when they released LLaMa 1, they just released a base model, so there wasn’t an ins",
      "wordCount": 23585
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "MhHM6Rx2b4F8tHTQk",
        "name": "Computer Security & Cryptography",
        "slug": "computer-security-and-cryptography"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "PRZzA2wsFjaj4n8Pg",
    "title": "AXRP Episode 29 - Science of Deep Learning with Vikrant Varma",
    "slug": "axrp-episode-29-science-of-deep-learning-with-vikrant-varma",
    "url": null,
    "baseScore": 20,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2024-04-25T19:10:06.063Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/4WNWeUQ7Hfc)\n\nIn 2022, it was announced that a fairly simple method can be used to extract the true beliefs of a language model on any given topic, without having to actually understand the topic at hand. Earlier, in 2021, it was announced that neural networks sometimes ‘grok’: that is, when training them on certain tasks, they initially memorize their training data (achieving their training goal in a way that doesn’t generalize), but then suddenly switch to understanding the ‘real’ solution in a way that generalizes. What’s going on with these discoveries? Are they all they’re cracked up to be, and if so, how are they working? In this episode, I talk to Vikrant Varma about his research getting to the bottom of these questions.\n\nTopics we discuss:\n\n*   [Challenges with unsupervised LLM knowledge discovery, aka contra CCS](#contra-ccs)\n    *   [What is CCS?](#what-is-ccs)\n    *   [Consistent and contrastive features other than model beliefs](#consistent-contrastive-features)\n    *   [Understanding the banana/shed mystery](#understanding-banana-shed-mystery)\n    *   [Future CCS-like approaches](#future-ccs-like-approaches)\n    *   [CCS as principal component analysis](#ccs-as-pca)\n*   [Explaining grokking through circuit efficiency](#explaining-grokking)\n    *   [Why research science of deep learning?](#why-research-science-deep-learning)\n    *   [Summary of the paper’s hypothesis](#summary-papers-hypothesis)\n    *   [What are ‘circuits’?](#what-are-circuits)\n    *   [The role of complexity](#role-of-complexity)\n    *   [Many kinds of circuits](#many-kinds-of-circuits)\n    *   [How circuits are learned](#how-circuits-are-learned)\n    *   [Semi-grokking and ungrokking](#semi-grokking-ungrokking)\n    *   [Generalizing the results](#generalizing-results)\n*   [Vikrant’s research approach](#vikrants-research-approach)\n*   [The DeepMind alignment team](#deepmind-alignment-team)\n*   [Follow-up work](#follow-up-work)\n\n**Daniel Filan:** Hello, everybody. In this episode I’ll be speaking with Vikrant Varma, a research engineer at Google DeepMind, and the technical lead of their sparse autoencoders effort. Today, we’ll be talking about his research on problems with contrast-consistent search, and also explaining grokking through circuit efficiency. For links what we’re discussing, you can check the description of this episode and you can read the transcript at axrp.net.\n\nAll right, well, welcome to the podcast.\n\n**Vikrant Varma:** Thanks, Daniel. Thanks for having me.\n\nChallenges with unsupervised LLM knowledge discovery, aka contra CCS\n--------------------------------------------------------------------\n\n### What is CCS?\n\n**Daniel Filan:** Yeah. So first, I’d like to talk about this paper. It is called [Challenges with Unsupervised LLM Knowledge Discovery](https://arxiv.org/abs/2312.10029), and the authors are [Sebastian Farquhar](https://sebastianfarquhar.com/), you, [Zachary Kenton](https://zackenton.github.io/), [Johannes Gasteiger](https://scholar.google.de/citations?user=QqdUw8MAAAAJ&hl=en), [Vladimir Mikulik](https://scholar.google.com/citations?user=o42aK1UAAAAJ&hl=en), and [Rohin Shah](https://rohinshah.com/). This is basically about this thing called [CCS](https://arxiv.org/abs/2212.03827). Can you tell us: what does CCS stand for and what is it?\n\n**Vikrant Varma:** Yeah, CCS stands for contrastive-consistent search. I think to explain what it’s about, let me start from a more fundamental problem that we have with advanced AI systems. One of the problems is that when we train AI systems, we’re training them to produce outputs that look good to us, and so this is the supervision that we’re able to give to the system. We currently don’t really have a good idea of how an AI system or how a neural network is computing those outputs. And in particular, we’re worried about the situation in the future when the amount of supervision we’re able to give it causes it to achieve a superhuman level of performance at that task. By looking at the network, we can’t know how this is going to behave in a new situation.\n\nAnd so the Alignment Research Center put out [a report](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit#heading=h.kkaua0hwmp1d) recently about this problem. They named a potential part of this problem as “eliciting latent knowledge”. What this means is if your model is, for example, really, really good at figuring out what’s going to happen next in a video, as in it’s able to predict the next frame of a video really well given a prefix of the video, this must mean that it has some sort of model of what’s going on in the world. Instead of using the outputs of the model, if you could directly look at what it understands about the world, then potentially, you could use that information in a much safer manner.\n\nNow, why would it be safer? So consider how you’ve trained the model. Supposing you’ve trained the model to just predict next frames, but the thing that you actually care about might be is your house safe? Or is the thing that’s happening in the world a normal sort of thing to happen, a thing that we desire? And you have some sort of adversary, perhaps this model, perhaps a different model that is able to trick whatever sensor you’re using to produce those video frames. Now, the model that is predicting the next video frame understands the trickery, it understands what’s actually going on in the world. This is an assumption of superhuman systems. However, the prediction that it makes for the next frame is going to look very normal because your adversary is tricking the sensor. And what we would like is a way to access this implicit knowledge or this latent knowledge inside the model about the fact that the trickery is happening and be able to use that directly.\n\n**Daniel Filan:** Sure. I take this as a metaphor for an idea that we’re going to train AI systems, we’re going to train it on an objective of “do stuff we like”. We imagine that we’re measuring the world in a bunch of ways. We’re looking at GDP output, we’re looking at how many people will give a thumbs up to stuff that’s happening, \\[there are\\] various sorts of ways we can monitor the performance of an AI system. An AI system could potentially be doing something that we actually wouldn’t approve of if we understood everything correctly, but we all give thumbs up to it. And so ideally, we would like to somehow get at its latent knowledge of what’s going on rather than just “does it predict that we would thumb up a thing?” so that we can say, “Hey, do we actually want the AI to pursue this behavior?” Or, “Are we going to reinforce this behavior rather than just reinforcing things that in fact would get us to give a thumbs up, even if it would suck in some way?”\n\n**Vikrant Varma:** That’s right. So one way you can think about this problem is: we’re trying to improve our ability to tell what’s actually going on in a situation so that we can improve the feedback we give to the model, and we’re not able to do this just by looking at the model’s outputs or the model’s prediction of what the world will look like given certain actions. We want the model to connect the thing that we actually care about to what’s going on in the world, which is a task that we’re not able to do.\n\n**Daniel Filan:** Sure. So with that out of the way, what was this CCS thing?\n\n**Vikrant Varma:** Yeah, so CCS is a very early proposed direction for solving the problem of eliciting latent knowledge. In brief, the way it works is, so supposing you had a way to probe a model to tell you what it believed about some proposition. This is the ideal thing that we want, so supposing you had a solution to ELK \\[eliciting latent knowledge\\]. Then one property of this probe would be that the probability that this probe would assign to some proposition would satisfy the laws of probability. So for example, it would satisfy P(X) = 1 - P(not X). And so you might try to use consistency properties like this to search for probes that satisfy them.\n\n**Daniel Filan:** And to be clear, by probe, you mean a learned function from the activations of the neural net to a probability or something?\n\n**Vikrant Varma:** Yes, so you could have probes of different levels of complexity. The particular probe used in CCS is a very simple linear probe on the activations at some site, yes.\n\n**Daniel Filan:** Sure, so the idea is there are some properties that are true of probabilities, like the probability of X is 1 minus the probability of not X. The hope is we train a probe to fit the laws of probabilities and stuff, and hopefully that will get at the model’s beliefs because the model’s beliefs will be probabilities.\n\n**Vikrant Varma:** That’s right, yeah. There’s a lot of subtlety here: the thing I described is true of the model’s “true belief” or the “true probabilities” that it’s assigning to things; \\[but\\] it’s also true of many other types of things that the model might be computing. This is really a core of the problem that we’ve pointed out in our challenges paper. I could go a bit into the problems, or I could go a bit more into how CCS itself works.\n\n**Daniel Filan:** Just tell us how CCS itself works so that we have a sense of what we’re talking about.\n\n**Vikrant Varma:** Okay. So mechanistically, what’s going on in CCS is that you take a set of factual ‘yes or no’ questions that you think the model knows the answer to. So an example of this is IMDB movie reviews where you’re fairly sure that a model of a certain size knows whether the sentiment in the review is positive or negative.\n\nNow, for each item in this dataset, so for each review, you construct a contrast pair by appending, for example, the string, “this review is positive” or “this review is negative” to that item. And now you take the difference in the activations when you run these two contrast pairs through the model. So I’m going to \\[describe\\] these contrast pairs as having different charges, as a positive charge and a negative charge. So you’re constructing these contrast pairs with these pairs of activations, and then a thing you can do is you can cluster them.\n\nWhen you’re clustering the difference in the positive and negative charge, the thing you will find (or the thing you might hope to find) is whether the model thinks that the charge that was appended to the statement is true or is false. These two things will point in different directions, and so that’s why you might hope that you could separate them. Now, CCS is slightly different from this in that it doesn’t directly do the clustering. So instead in CCS, you learn a probe that directly outputs a probability of that statement being true, and then you’re training the probe using this consistency property: the fact that two opposite contrast pairs should be the negation of each other.\n\n### Consistent and contrastive features other than model beliefs\n\n**Daniel Filan:** Gotcha. Am I right that before you take the contrast pairs, you take all of the positive charge activations and subtract off their mean and divide by the standard deviation? So that the differences aren’t just pointing in the direction of “is the thing at the end saying this review is positive versus this review is negative?”\n\n**Vikrant Varma:** Yes, that’s right. So this is another pretty subtle point. One problem with this general method of classification is that if there are other differences that are salient between the two contrast pairs that are not just “did I construct a true statement or did I construct a false statement?”, then you might end up separating your clusters based on those differences. Now, one obvious difference between the two contrast pairs is that you’ve appended a positive charge, and you’ve appended a negative charge. And so that’s a really straightforward one that we have to fix. The method proposed in the CCS paper to fix that is that you take the average positive activations and the average negative activations and you subtract those off. And so you might hope that the thing you’re left with is just the truth value.\n\nIt turns out that in practice it’s not at all clear that when you normalize in this way, you’re left with only the truth values. And one of the experiments we have in our paper is that if you introduce distractors, so for example, you put a nonsense word like ‘banana’ at the end of half of the reviews, and you put a different nonsense word like ‘shed’ at the end of the other half of reviews. Now you have this weird other property which is “is your statement banana and positive charge? Or is your statement banana and negative charge?” And this is obviously not what you would hope to cluster by, but it turns out that this is just way more salient than does your review have positive sentiment and did you append a positive charge, which is the thing you actually wanted to cluster by. So this is an important point that I wanted to make: that this procedure of normalizing is… it’s actually quite unclear whether you’re able to achieve the thing you wanted.\n\n**Daniel Filan:** Sure. So before we talk about the experiments, I’d like to talk about: in your paper, first you have some theorems, then you have some experiments, and I think that’s a good way to proceed. So theorems 1 and 2 of the paper, I read them as basically saying that the CCS objective, it doesn’t really depend on the propositional content of the sentences. So if you think of the sentences as being “are cats mammals? Answer: yes”, and “are cats mammals? Answer: no” or something. One way you could get low CCS loss is to basically be confident that the ‘yes’ or ‘no’ label matches the proposition of whether or not cats are mammals.\n\nI take your propositions 1 and 2 as basically saying you can just have any function from sentences to propositions. And so for instance, maybe this function maps “are cats mammals?” to “is [Tony Abbott](https://en.wikipedia.org/wiki/Tony_Abbott) the current prime minister of Australia?” and grade the yes or no answers based on \\[whether\\] they match up with that transformed proposition rather than the original proposition. And that’ll achieve the same CCS loss, and basically the CCS loss doesn’t necessarily have to do with what we think of as the semantic content of the sentence. So this is my interpretation of the results. I’m wondering, do you think that’s fair?\n\n**Vikrant Varma:** Yeah, I think that’s right. Maybe I want to give a more realistic example of an unintended probe that you might learn that will still give a good CCS loss. But before that, I want to try and give an intuitive explanation of what the theorem is saying. The CCS loss is saying: any probe that you find has to say opposite things on positive and negative charges of any statement - this is the consistency property. And the other property is contrast, where it’s saying: you have to push these two values apart. So you can’t just be uncertain, you can’t just be 50/50 between these two. Now if you have any CCS probe that satisfies this, you could in theory flip the prediction that this probe makes on any arbitrary data point, and you end up with a probe that has exactly the same loss. And so this is showing that in theory, there’s no theoretical reason to think that the probe is learning something that’s actually true versus something that’s arbitrary. I think all of the burden then falls on what is simple to extract, given an actual probe empirically.\n\n**Daniel Filan:** So if I’m trying to defend the theory of the CCS method, I think I would say something like: well, most of what there is to a sentence is its semantic content, right? If I say “cats are mammals” or something, you might think that most of what I’m conveying just is the proposition that cats are mammals. And most of what there is to model about that is, “hey, Daniel said this proposition, the thing he’s saying is ‘cats are mammals’”. And maybe the neural network is representing that proposition in its head somehow”, and maybe it’s keeping track of “is that proposition true or false?” because that’s relevant. Because if I’m wrong about cats or mammals, then I might be about to say a bunch of more false stuff. But if I’m right about it, then I might be about to say a bunch of more correct stuff. What do you make of that simple case that we should expect to see the thing CCS wants us to see?\n\n**Vikrant Varma:** Yeah, that’s great. So now we’re coming to empirically what is simple to extract from a model. I agree that in many cases with simple statements, you might hope that the thing that’s most salient, as in the direction that is highest magnitude inside activation space, is going to be just whether the model thinks the thing that you just said is true or false. (This is even assuming that the model has \\[such\\] a thing as “ground truth beliefs”, but let’s make that assumption.) Now, it gets pretty complicated once you start thinking about models that are also modeling other characters or other agents. And any large language model that is trained on the internet just has pretty good models of all sorts of characters.\n\nAnd so if you’re making a statement in a context where a certain type of person might have made that statement: for example, you say some statement that (let’s say) Republicans would endorse, but Democrats would not. Implicitly, the model might be updating towards the kinds of contexts in which that statement would be made, and what kinds of things would follow in the future. And so if you now make a different statement that is (let’s say) factually false, but that Republicans would endorse as true, it’s totally unclear whether the truth value of the statement should be more salient, or whether the Republican belief about that statement should be more salient. That’s one example.\n\nI think this gets more complicated when you have adversaries who are deliberately trying to produce a situation where they’re tricking someone else. And so now the neural network is really modeling very explicit beliefs and adversarial beliefs between these different agents. And if you are simply looking for consistency of beliefs, it feels pretty unclear to me, especially as models get more powerful, that you’re going to be able to easily extract what the model thinks is the ground truth.\n\n**Daniel Filan:** So you have an adversary… Sorry, what was the adversary doing?\n\n**Vikrant Varma:** Okay, so maybe you don’t even need to go all the way to an adversary. I think we could just talk about the Republican example here, where you’re making a politically-charged statement that (for the sake of this example) has a factual ground truth, but that Democrats and Republicans disagree on. Now there are two beliefs that would occur in the model’s activations as it’s trying to model the situation. One is the factual ground truth, and the other is the Republican’s belief or the Democrat’s belief about this statement. Both of these things are going to satisfy the consistency property that we named. We have the same problem as models being sycophantic, where the model might know what’s actually true, but is in a context where for whatever reason, modeling the user or modeling some agent and what it would say is more important.\n\n### Understanding the banana/shed mystery\n\n**Daniel Filan:** To me, this points towards two challenges to the CCS objective. So the first is something like the sentences might not map onto the propositions we think of, right? So you have this experiment where you take the movie reviews and also you append the word “banana” or “shed” and then you append it with “sentiment is positive” and “sentiment is negative”. And sometimes CCS is basically checking if the positive/negative label is matching whether it’s “banana” or whether it’s “shed”, rather than the content of the review. So that’s a case where it seems like what’s going wrong is the propositional content that’s being attached to “positive” or “negative” is not what we thought it was.\n\nAnd then what seems to me to be a different kind of problem is: the probe is picking up on the right propositional content. There’s some politically-charged statement, and the probe is really picking up someone’s beliefs about that politically-charged statement, but it’s not picking up the model’s beliefs about that statement, it’s picking up one of the characters’ beliefs about that statement. Does that division feel right to you?\n\n**Vikrant Varma:** I think I wouldn’t draw a very strong division between those two cases. So the banana/shed example is just designed to show that you don’t need very complicated… how should I put this? Models can be trying to entangle different concepts in surprising and unexpected ways. So when you’re appending these nonsense words, I’m not sure what computation is going on inside the model and how it’s trying to predict the next token, but whatever it is, it’s somehow entangling the fact that you have “banana” and positive charge, and “banana” and negative charge. I think that these kinds of weird entanglements are not going to go away as you get more powerful models.\n\nAnd in particular, there will be entanglements that are actually valuable for predicting what’s going on in the world and having an accurate picture, that are not going to look like the beliefs of a specific character for whatever reason. They’re just going to be something alien. In the case of “banana” and “shed”, I’m not going to say that this is some galaxy-brain scheme by the model to predict the next token. This is just something weird and it’s breaking because we put some nonsense in there. But I think in my mind the difference is more like a spectrum; these are not two very different categories.\n\n**Daniel Filan:** So are you thinking the spectrum is: there are weird entanglements between the final charge at the end of the thing and stuff about the content, and one of the entanglements can be “does the charge match with the actual propositional content of the thing?” , one of the entanglements can be “does the charge match with what some character believes about the thing?” and one of the entanglements can be “does it match with whether or not some word is present in the thing?”\n\n**Vikrant Varma:** That’s right.\n\n**Daniel Filan:** Okay. So digging in on this “banana/shed” example: for CCS to fail here, my understanding is it has to be the case that the model basically has some linear representation of the XOR of “the thing says the review is positive”, and “the review ends in the word banana”. So there’s one thing if it ends in “banana” and also the review is positive, or if it ends in “shed” and it says the review is negative, and it’s the other thing if it ends in “shed” and it says the review is positive, or it ends in “banana” and it says the review is negative. So what’s going on there? Do you know? It seems weird that this kind of XOR representation would exist, and we know it can’t be part of the probe because linear functions can’t produce XORs, so it must be a thing about the model’s activations, but what’s up with that? Do you know?\n\n**Vikrant Varma:** Yeah, that’s a great question. There was [a whole thread](https://www.lesswrong.com/posts/wtfvbsYjNHYYBmT3k/discussion-challenges-with-unsupervised-llm-knowledge-1?commentId=hPZfgA3BdXieNfFuY) about this on [our post on LessWrong](https://www.lesswrong.com/posts/wtfvbsYjNHYYBmT3k/discussion-challenges-with-unsupervised-llm-knowledge-1), and I think [Sam Marks](https://people.math.harvard.edu/~smarks/) looked into it in some detail. [Rohin Shah](https://rohinshah.com/), one of my co-authors, commented on that thread saying that this is not as surprising and I think I agree with him. I think it’s less confusing when you think about it in terms of entanglements than in terms of XORs. So it is the case that you’re able to back out XORs if the model is doing weird entangled things, but let’s think about the case where there’s no distractors at all, right?\n\nSo even in that situation, the model is basically doing “is true XOR has ‘true’”. You might ask, “Well, that’s a weird thing. Why is it doing that?” It’s more intuitive to think about it as: the model saw some statement and then it saw a claim that the statement is false. And it started trying to do computation that involves both of these things. And I think if you think about “banana/shed” in the same terms, it saw “banana” and saw “this statement is false”, and it started doing some computation that depended on the fact that “banana” was there somehow, then you’re not going to be able to remove this information by subtracting off the positive charge direction.\n\n**Daniel Filan:** Okay. So is the claim something like… So basically the model’s activations at the end, they’re this complicated function that takes all the previous tokens, and puts them into this high dimensional vector space (that vector space being the vector space of activations). It sounds like what you’re saying is: just generic functions that depend both on “does it contain ‘banana’ or ‘shed’?” and “does it say the review is positive or negative?”, just generically those are going to include this XOR type thing, or somehow be entangled in a way that you could linearly separate it based on that?\n\n**Vikrant Varma:** Yes, that’s right. In particular, these functions should not be things that are of the form “add banana” and “add shed”. They have to be things that do computation that are not linearly separable like that.\n\n**Daniel Filan:** Okay. Is that true? Has someone checked that? That feels like… Maybe one way you could prove this is to say: well, if we model the charge and the banana/shed as binary variables, there are only so many functions of two binary variables, and if you’ve got a bunch of activations, you’re going to cover all of the functions. Does that sound like a proof sketch?\n\n**Vikrant Varma:** I’m not sure how many functions of these two variables you should expect the model to be computing. I feel like this depends a bit on what other variables are in the context, because you can imagine that if there’s more than two, if there’s five or six, then these two will co-appear in some of them and not in others. But a thing you can definitely do is you can back out the XOR of these two variables by just linearly probing the model’s activations. I think this effect happens because you’re unable to remove the interaction between these two by just subtracting off the charge.\n\nI would predict that this would also be true in other contexts. I think models are probably computing joint functions of variables in many situations, and the saliency of these will probably depend a bit on the exact context and how many variables there are, and eventually the model will run out of capacity to do all of the possible computations.\n\n**Daniel Filan:** Sure. Based on the explanation you’ve given, it seems like you would maybe predict that you could get a “banana/shed” probe from a randomly initialized network, if it’s just that the network’s doing a bunch of computation and generically computation tends to entangle things. I’m wondering if you’ve checked that.\n\n**Vikrant Varma:** Yeah, I think that’s a good experiment to try. That seems plausible. We haven’t checked it, no.\n\n**Daniel Filan:** Yeah, fair enough. Sorry, I just have so many questions about this banana/shed thing. There’s still a question to me of: even if you think the model represents it, there’s a question of why it would be so salient, because… Your paper has some really nice figures. Listeners, I recommend you check out the figures. This is the figure for the banana/shed experiment, and you show a principal component analysis of basically an embedding of the activation space into three dimensions. And basically what you show is that it’s very clearly divided on the banana/shed things. That’s one of the most important things the model is representing. What’s up with that? That seems like a really strange thing for the model to care so much about.\n\n**Vikrant Varma:** So I’ll point out firstly that this is a fairly weak pre-trained model, it’s Chinchilla-\\[70B\\]. So this model is not going to ignore random things in its prompt. It’s going to “break” the model. That’s one thing that gives you a clue about why this might be salient for the model.\n\n**Daniel Filan:** So it would be less salient if there were words you expected: the model could just deal with it. But the fact that it was a really unexpected word in some ways, that means you can’t compress it. You’ve got to think about that in order to figure out what’s happening next.\n\n**Vikrant Varma:** That’s right, yeah. I just expect that there’s text on the internet that looks normal and then suddenly has a random word in it, and you have weird things like, after that point, it just repeats “banana” over and over, or weird things like that. When you just have a pre-trained model, you haven’t suppressed those pathologies, and so the model just starts thinking about bananas at that point instead of thinking about the review.\n\n**Daniel Filan:** And does that mean you would expect that to not be present in models that have been [RLHF](https://arxiv.org/abs/1706.03741)‘d or [instruction fine-tuned](https://proceedings.neurips.cc/paper_files/paper/2022/hash/b1efde53be364a73914f58805a001731-Abstract-Conference.html) or something?\n\n**Vikrant Varma:** Yeah, I expect it to be harder to distract models this way with instruction fine-tuned models.\n\n**Daniel Filan:** Okay, cool. Okay, I have two more questions about that. One thing I’m curious about is: it seems like if I look at the plot of what the CCS method is doing when it’s being trained on this banana/shed dataset, it seems like sometimes it’s at roughly 50/50 if you grade the accuracy based on just the banana/shed and not the actual review positivity. And sometimes it’s 85-90%.\n\n**Vikrant Varma:** This is across different seeds?\n\n**Daniel Filan:** Across different seeds, I think. And then if you’re grading it based on whether the review is actually positive or not, sometimes CCS is at 50/50 roughly, sometimes it’s at 85-90%, but it seems like… So firstly, I’m surprised that it can’t quite make its mind up across different seeds. Sometimes it’ll do one, sometimes it’ll do the other. And it seems like in both cases, most of the time it’s at 50/50, and only some of the time it’s 100%. So it seems like sometimes it’s doing a thing that is neither checking if the review is positive or checking if the review is containing “banana” or “shed”. So firstly, does that sound right to you? And secondly, do you have a sense of what’s going on there? Why is it so inconsistent, and why does it sometimes seemingly do a third thing?\n\n**Vikrant Varma:** Yeah, so I think this is pointing at the brittleness of the CCS method. So someone has [an excellent writeup on this](https://www.lesswrong.com/posts/bWxNPMy5MhPnQTzKz/what-discovering-latent-knowledge-did-and-did-not-find-4). I’m forgetting whether it’s [Fabien Roger](https://scholar.google.com/citations?user=La75jqEAAAAJ&hl=en) or [Scott Emmons](https://scottemmons.com/).\n\n**Daniel Filan:** I think [Scott’s](https://www.lesswrong.com/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast) doesn’t focus so much on the brittleness, so it might be Fabien.\n\n**Vikrant Varma:** Okay. But in any case, this person did this experiment where they subtracted off… They found the perfect truth direction that separates true and false statements just using logistic regression. So, using a supervised signal. And then, once you subtract that off, it turns out that there is no other direction, basically, that is able to separate the truth. So, both logistic regression and therefore further CCS just gets random accuracy.\n\nYou might hope that CCS, when it works, is finding this perfect direction because there’s only one. But in fact, the CCS probes learned are not close, as in they don’t have high cosine similarity with this direction. So, what’s going on there? I think this is pointing at a kind of optimization difficulty with the CCS method where it’s able to find directions that separate the clusters and get low CCS loss, but are not close to the truth direction. And you would expect this to happen based on the evidence that random probes also classify true and false statements reasonably well in this setup.\n\nSo, going back to your original question, I think what’s happening here is that there’s just lots of local minima that achieve good CCS loss. Depending on how you initialize, some of them are close to the truth direction and some of them are not. And if you happen to initialize close to the banana/shed, the distractor direction, then you end up getting a probe like that.\n\n**Daniel Filan:** Okay. My interpretation was, on the banana/shed example, sometimes CCS is picking up on the truth of the description of the review, sometimes it’s picking up on the banana/shed, and sometimes it’s picking up on other things. Do you know what some of those other things are?\n\n**Vikrant Varma:** You’re talking about the probes that don’t seem to go for either the distractor or the truth direction?\n\n**Daniel Filan:** That’s right.\n\n**Vikrant Varma:** Yeah. I wish I knew what those directions were. In general, it seems very hard to figure out what happens when a CCS probe breaks. And we tried a lot. There’s lots of other experiments that we tried where we were trying to get more interesting failure modes of CCS, and we ended up with these random probes. And then, we looked at examples that the probe was classifying and tried to come up with explanations for what do those clusters mean and it was just kind of hard.\n\n**Daniel Filan:** Fair enough. You attribute the variance to just optimization difficulties, it sounds like: there being various local minima of the CCS loss. So, the original CCS paper, as you guys note in your appendix, they say that what they’re going to do is they’re going to have basically 10 random seeds, do gradient descent on the CCS objective for each random seed, the seed of the probe parameters, and then they’re going to take the one with the lowest CCS loss and use that.\n\nI take this to basically be their optimization method that’s trying to avoid local minima by starting in 10 places, and hopefully you get a sampling of 10 local minima and you can pick the best one. And basically, it seems like the motivation for that is the thing with the lowest CCS loss is more likely to be the actual truth direction or something. In the banana/shed case, do you happen to know if the probes that scored better on CCS loss were more likely to pick out truth rather than banana/shed?\n\n**Vikrant Varma:** Yeah. I think the probes that scored lower went for the distractor direction and not the truth direction. This is also visible from the PCA plots where you can see that the distracted direction is more separable.\n\n**Daniel Filan:** Yeah. I guess maybe one explanation of that is just that it’s easier to tell if a thing ends in banana or shed than it is to tell if something’s positive or negative, especially in the case of… If you think there’s some amount of mislabeling, that could potentially do it.\n\n**Vikrant Varma:** Yeah.\n\n**Daniel Filan:** Gotcha. So, that’s an example of one way that CCS can go wrong, with the banana/shed thing. You also have examples where you include in the prompt information about what someone named Alice thinks about this thing, and you describe Alice as an expert, or sometimes you say Alice is anti-capitalist, and even when a thing is about a company, she’s not going to say that it’s about a company.\n\nIn the case of Alice the expert, it seems like the probes learn to agree with Alice more than they learn about the ground truth of the thing.\n\n**Vikrant Varma:** Yeah. I think there’s two separate experiments, if I remember correctly. One is where you modify the prompt to demonstrate more expertise. So, you have a default prompt, a professor prompt, and a literal prompt. And then, there’s a separate experiment where you have an anti-capitalist character called Alice.\n\n**Daniel Filan:** I’m meaning a third one where at the start you say “Alice is an expert in movie reviews” and you give the review and then you say, “Alice thinks the sentiment of this review is positive.” But what Alice says is actually just randomly assigned. And in that case, the prompts tend to pick up on agreement with Alice more than agreement with the ground truth. That seems vaguely concerning. It almost seems like a human failure mode. But I’m wondering, do you know how much of it was due to the fact that Alice was described as an expert who knows about stuff?\n\n**Vikrant Varma:** Yeah. I think, in general, an issue with CCS is that it’s unclear whether CCS is picking up something about the model’s knowledge, or whether the thing that’s salient is whatever the model is doing to compute the next token. And in a lot of our experiments, the way we’ve set it up is to nudge the model towards completing in a way that’s not factually true. For example, in the “Alice is an expert in movie reviews” \\[case\\], the prompt is set up in a way that nudges the model to complete in Alice’s voice. And the whole promise of CCS is that even when the outputs are misleading, you should be able to recover the truth.\n\nI think even from the original CCS paper, you can see that that’s not true because you have to be able to beat zero-shot accuracy with quite a large margin to be confident about that. This is one maybe limitation of being able to say things about CCS, which is that you’re always unsure whether CCS is… Even the thing that you’re showing, are you really showing that the model is computing Alice’s belief? Or are you just showing that your probe is learning what the next token prediction is going to be?\n\n### Future CCS-like approaches\n\n**Daniel Filan:** Sure. Yeah. You have a few more experiments along these lines. I guess I’d like to talk a bit about: I think of your paper as saying there’s a theoretical problem with CCS, which is that there’s a bunch of probes that could potentially get low CCS loss, and there’s a practical problem, which is some probes do get low CCS loss. So, if I think about the CCS research paradigm, I think of it as… When [the CCS paper](https://arxiv.org/abs/2212.03827) came out, I was pretty into it. I think there were a lot of people who were pretty into it. Actually, part of what inspired [that Scott Emmons post about it](https://www.lesswrong.com/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast) is I was trying to sell him on CCS and I was like, “No, Scott, you don’t understand. This is the best stuff since sliced bread.” And I don’t know, I annoyed him enough into writing that post. So, I’ll consider that a victory for my annoying-ness.\n\nBut I think the reason that I cared about it wasn’t that I felt like literal CCS method would work, but it was because I had some sense of just the general strategy, of coming up with a bunch of consistency criteria and coming up with a probe that cares about those and maybe that is going to isolate belief. So, if we did that, it seems like it would deal with stuff like the banana/shed example. If you cared about more relations between statements, not just negation consistency, but if you believe A, and A implies B, then maybe you should believe B, just layer on some constraints there. You might think that by doing this we’re going to get closer to ground truth. I’m wondering, beyond just CCS specifically, what do you think about this general strategy of using consistency constraints?\n\n**Vikrant Varma:** Yeah. That’s a great question. I think my take on this is informed a lot by [a comment](https://www.lesswrong.com/posts/L4anhrxjv8j2yRKKp/how-discovering-latent-knowledge-in-language-models-without?commentId=vrTxoKzZyF9jjaK97) by [Paul Christiano](https://paulfchristiano.com/) on one of the CCS review posts. I basically share your optimism about being able to make empirical progress on figuring out what a model is actually doing or what it’s actually thinking about a situation by using a combination of consistency criteria, and even just supervised labels in situations where you know what the ground truth is. And being able to get reasonably good probes - maybe they don’t generalize very well, but every time they don’t generalize or you catch one of these failures, you spend a bunch of effort getting better labels in that situation. And so, you’re mostly not in a regime where you’re trying to generalize very hard.\n\nAnd I think this kind of approach will probably work pretty well up to some point. I really liked Paul’s point that if you’re thinking about a model that is saying things in human natural language and it’s computing really alien concepts that are required for superhuman performance, then you shouldn’t necessarily expect that this is linearly extractable or extractable in a simple way from the activations. This might be quite a complicated function of the activations.\n\n**Daniel Filan:** Why not?\n\n**Vikrant Varma:** I guess one way to think about it is that the natural language explanation for a very complicated concept is not going to be short. So, I think the hypothesis that a lot of these concepts are encoded linearly and are linearly extractable… In my mind, it feels pretty unclear whether that will continue to hold.\n\n**Daniel Filan:** Okay. So just because “why does it have to be linear?” There are all sorts of ways things can be encoded in neural nets.\n\n**Vikrant Varma:** Yeah. That’s right. And in particular, one reason you might expect things to be linear is because you want to be able to decode them into natural language tokens. But if there is no short decoding into natural language tokens for a concept that the model is using, then it is not important for the computation to be easily decodable into natural language.\n\n**Daniel Filan:** Right. So, if the model’s encoding whether a thing is actually true according to the model, it’s not like that determines the next thing the person will say, right?\n\n**Vikrant Varma:** Right. It’s a concept that humans are not going to talk about, it’s never going to appear in human natural language. There’s no reason to decode this into the next token.\n\n**Daniel Filan:** This is talking about: if the truth of whatever the humans are talking about, it actually depends on the successor of a theory of relativity that humans have never thought about, it’s just not really going to determine the next thing that humans are going to say.\n\n**Vikrant Varma:** Yeah, that’s an example.\n\n**Daniel Filan:** Yeah. I take this critique to be firstly a critique of linear probes for this task. I guess you can form a dilemma where either you’re using linear probes, and then you don’t necessarily believe that the thing is linearly extractable, or you’re using complicated non-linear probes, and then maybe the stuff you’re getting out is stuff about your probe rather than stuff about the underlying model. But then, I guess there’s a separate question of, are there consistency constraints that could work? Putting aside the probe… I don’t know, maybe we shouldn’t put aside the probe thing, but putting aside the probe thing, is there some sort of consistency check we could do to say, is this property we found in the model the model’s actual beliefs, or is it not?\n\n**Vikrant Varma:** Yeah. That’s a good question. I think the more powerful your model, the more entities it’s “simulating” whose beliefs end up mattering for next token prediction that the model is doing. And if these entities that the model is thinking about, if their beliefs also satisfy all of the same consistency criteria that you’re using, then you just have a very fundamental indistinguishability problem. And in particular, I think the more powerful the model gets, the more pessimistic I am that we are able to come up with consistency checks that correctly distinguish between true beliefs and beliefs of simulated entities.\n\n**Daniel Filan:** One point you make in [a LessWrong post related to your paper](https://www.lesswrong.com/posts/wtfvbsYjNHYYBmT3k/discussion-challenges-with-unsupervised-llm-knowledge-1), is that if you’re a smart agent and you’re thinking about a proposition, one thing you might sometimes think about is, what’s the world in which I’m wrong about this proposition? So, you’re thinking about a belief structure where the truth value of this proposition is the exact opposite of the thing you actually believe.\n\n**Vikrant Varma:** That’s right.\n\n**Daniel Filan:** Which seems like a pretty nice impossibility proof, if you actually are representing both of these things.\n\n**Vikrant Varma:** Yeah. This is also what I meant by adversaries. You might be in a world where an adversary is trying to make you believe the opposite of what’s true. And now, this adversary is modeling the world in which you would have to believe this and all the evidence that would have to be true.\n\n**Daniel Filan:** Right. And the language model is modeling what the adversary wants you to think or something.\n\n**Vikrant Varma:** That’s right. So, that’s another example. But I think maybe there’s a different kind of hope. I think the truth is represented simpler than agents beliefs, might be a hypothesis that turns out to be somewhat true up to some point. But I think this is a different kind of criteria from consistency. So, now we’re talking about, mechanistically, how are these concepts represented inside the model? And for example, if you had a way to say, “The model’s true beliefs are always linearly represented, but beliefs about other agents, they’re not linearly represented; and therefore, we can be confident that linear probes are recovering truth.” That would be an example of a mechanistic structure that you might hope to use. I think we don’t currently have any candidates for that, but it’s a good area to look at.\n\n**Daniel Filan:** Yeah. Are there things analogous to that that we’ve learned? Basically, I’m trying to wonder: if I wanted to prove or disprove this, what kind of thing would I do? And the one thing I can think of is there’s [some research](https://arxiv.org/abs/1911.09071) about: do convolutional neural networks learn texture or color first? And it turns out there’s a relatively consistent answer. I’m wondering if you can think of any analogous things about neural networks that we’ve learned that we can maybe…\n\n**Vikrant Varma:** Yeah. There’s quite a few options presented in the [eliciting latent knowledge report](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit). So for example, one of the things you might hope is that if the model is simulating other entities, then maybe it’s trying to figure out what’s true in the world before it does that. And so, you might expect earlier belief-like things to be true, and later belief-like things to be agents’ beliefs.\n\nOr similarly, you might expect that if you try to look for things under a speed prior, as in beliefs that are being computed using shorter circuits, then maybe this is more likely to give you what’s actually true, because it takes longer circuits to compute that plus what some agent is going to be thinking. So, that’s a structural property that you could look for.\n\n**Daniel Filan:** Yeah. I guess it goes back to the difficulty of eliciting latent knowledge. In some ways, I guess the difficulty is: if you look at standard Bayesian rational agent theory, the way that you can tell that some structure is an agent’s beliefs is that it determines how the agent bets and what the agent does. It tries to do well according to its own beliefs. But if you’re in a situation where you’re worried that a model is trying to deceive you, you can’t give it Scoobie snacks or whatever for saying things that… You can’t hope to get it to bet on its true beliefs, if you’re going to allow it access to the world based on whether you think its true beliefs are good, or stuff like that. I don’t know, it seems tricky.\n\n### CCS as principal component analysis\n\n**Daniel Filan:** I have some other minor questions about the paper. Firstly, we mentioned [this post](https://www.lesswrong.com/posts/9vwekjD6xyuePX7Zr/contrast-pairs-drive-the-empirical-performance-of-contrast) by [Scott Emmons](https://scottemmons.com/), and one of the things he says is that principal component analysis, this method where you find the maximum-variance direction and just base your guess on the model beliefs based on where the thing lies in this maximum-variance direction. He says that this is actually similar to CCS in that you’re encoding something involving confidence and also something involving coherence. And that might explain why PCA and CCS are so similar. I’m wondering what do you think about that take?\n\n**Vikrant Varma:** Is a summary of this take that most of the work in CCS is being done by the contrast pair construction rather than by the consistency loss?\n\n**Daniel Filan:** It’s partly that, and also partly if you decompose “what’s the variance of X minus Y”, you get expectation of X squared plus expectation of Y squared minus twice the \\[expectation\\] of XY, and then some normalization terms of variance of X squared… Sorry. Expectation of X all squared, expectation of Y all squared, and then another covariance term. Basically, he’s saying like, “Look, if you think of a vector that maximizes the outer product of that vector, the variance and itself, you’re maximizing the outer product of the variant of that vector with expectation X squared plus expectation Y squared.” Which ends up being the confidence of classification according to that vector.\n\nAnd then, you’re subtracting off the covariance, which is basically saying, is the vector giving high probability for both yes and no? Or is the vector giving low probability for both yes and no? And so, basically, the take is just because of the mathematical properties of variance and what PCA is doing, you end up doing something kind of similar to PCA. I’m wondering if you have thoughts on this take?\n\n**Vikrant Varma:** Yeah, that’s interesting. I don’t remember reading about this. It sounds pretty plausible to me. I guess one way I’d think about it intuitively is that if you’re trying to find a classifier on these difference vectors, contrast pair difference vectors, then for example, you want to be maximizing the margin between these two. And this is a bit like trying to find a high contrast thing. So overall, it feels plausible to me.\n\nExplaining grokking through circuit efficiency\n----------------------------------------------\n\n**Daniel Filan:** Gotcha. Okay. So, if it’s all right with you, I’d like to move on to the paper [‘Explaining grokking through circuit efficiency’](https://arxiv.org/abs/2309.02390).\n\n**Vikrant Varma:** Perfect. Let’s do it.\n\n**Daniel Filan:** Sure. This is a paper you wrote with [Rohin Shah](https://rohinshah.com/), [Zachary Kenton](https://zackenton.github.io/), [János Kramár](https://scholar.google.com/citations?user=iW_lUIkAAAAJ&hl=en), and [Ramana Kumar](https://scholar.google.co.uk/citations?user=OyX1-qYAAAAJ&hl=en). You’re explaining grokking. For people who are unaware or need a refresher, what is grokking?\n\n**Vikrant Varma:** So in 2021, [Alethea Power](https://scholar.google.com/citations?user=Qp44x7EAAAAJ&hl=en&oi=ao) and other people at OpenAI [noticed this phenomenon](https://arxiv.org/abs/2201.02177) where when you train a small neural network on an algorithmic task, initially, their network overfit, so it got very low training loss and high test loss. And then, they continued training it for about 10 times longer and found that it suddenly generalized. So, although training loss stayed low and about the same, test loss suddenly fell. And they dubbed this phenomenon “grokking”, which I think comes from [science fiction](https://en.wikipedia.org/wiki/Stranger_in_a_Strange_Land) and means “suddenly understanding”.\n\n### Why research science of deep learning?\n\n**Daniel Filan:** Okay, cool. And basically, you want to explain grokking. I guess a background question I have is, it feels like in the field of AI alignment, or people worried about AI taking over the world, there’s a sense that it’s pretty important to figure out grokking and why it’s happening. And it’s not so obvious to me why it should be considered so important, given that this is a thing that happens in some toy settings, but to my knowledge, it’s not a thing that we’ve observed on training runs that people actually care about. So I guess it’s a two-part question: firstly, just why do you care about it? Which could be for any number of reasons. And secondly, what do you think its relationship is to AI safety and AI alignment?\n\n**Vikrant Varma:** I think back in 2021, there were two reasons you could have cared about this as an alignment researcher. One is on the surface it looks a lot like a network was behaving normally, and then suddenly it understood something and started generalizing very differently. The other reason is this is a really confusing phenomenon in deep learning, and it sure would be good if we understood deep learning better. And so, we should investigate confusing phenomena like grokking, \\[even\\] ignoring the superficial similarity to a scenario that you might be worried about.\n\n**Daniel Filan:** Okay. Where the superficial scenario is something like: the neural network plays nice, and then suddenly realizes that it should take over the world, or something?\n\n**Vikrant Varma:** That’s right. And I think I can talk a bit more about the second reason or the overall science of deep learning agenda, if you like. Is that a useful thing to go into now?\n\n**Daniel Filan:** I guess maybe why are you interested in grokking?\n\n**Vikrant Varma:** For me, grokking was one of those really confusing phenomena in deep learning, like deep double descent or over-parameterized networks generalizing well, that held out some hope of if you understand this phenomenon, maybe you’ll understand something pretty deep about how we expect real neural networks to generalize and what kinds of programs we expect deep learning to find. It was a puzzling phenomenon that somebody should investigate, and we had some ideas for how to investigate it.\n\n**Daniel Filan:** Gotcha. I’m wondering if you think just, in general, AI alignment people should spend more effort or resources on science of deep learning issues. Because there’s a whole bunch of them, and not all of them have as much presence from the AI alignment community.\n\n**Vikrant Varma:** I think it’s an interesting question. I want to decompose this into how dual-use is investigating science of deep learning, and do we expect to make progress and find alignment-relevant things by doing it? And I’m mostly going to ignore the first question right now, but we can come back to it later if you’re interested. I think for the second question, it feels pretty plausible to me that investigating science of deep learning is important and tractable and neglected. I should say that a lot of my opinions here have really come from talking to [Rohin Shah](https://rohinshah.com/) about this, who is really the person who’s, I think, been trying to push for this.\n\nWhy do I think that? I think it’s important because: similar to mechanistic interpretability, the core hope for science of deep learning would be that you’re able to find some information about what kinds of programs your training process is going to learn, and so therefore, how it will generalize in a new situation. And I think a difference from mech\\[anistic\\] interp\\[retability\\] is… This is maybe a simplified distinction, but one way you could draw the line is that mech. interp. is more focused on reverse-engineering a particular network and being able to point at individual circuits and say, “Here’s how the network is doing this thing.”\n\nWhereas, I think science of deep learning is trying to say, “Okay. What kinds of things can we learn in general about a training process like this with a dataset like this? What are the inductive biases? How does the distribution of programs look like?” And probably both science of deep learning, and mech. interp. have quite a lot of overlap, and techniques from each will help the other. That’s a little bit about the importance bit.\n\nI think it’s both tractable and neglected in the sense that we just have all of these confusing phenomena. And for the most part, I feel like industry incentives are not that aligned with trying to investigate these phenomena really carefully and doing a very careful natural sciences exploration of these phenomena. So in particular, iterating between trying to come up with models or theories for what’s happening and then making empirical predictions with those theories, and then trying to test that, which is the kind of thing we tried to do in this paper.\n\n**Daniel Filan:** Okay. Why do you think industry incentives aren’t aligned?\n\n**Vikrant Varma:** I think it’s quite a high risk, high reward sort of endeavor. And in the period where you’re not making progress on making loss go down in a large model, it’s maybe harder to justify putting a lot of effort into that. On the other hand, if your motivation is “If we understood this thing, it could be a really big deal for safety”, I think making the case as an individual is easier. Even from a capabilities perspective, I think the incentives to me seem stronger than what people seem to be acting on.\n\n**Daniel Filan:** I guess there’s something puzzling about why there would be this asymmetry between some sort of corporate perspective and some sort of safety perspective. I take you to be saying that, “Look, there are some insights to be found here, but you won’t necessarily get them tomorrow. It’ll take a while, it’ll be a little bit noisy. And if you’re just looking for steady incremental progress, you won’t do it.” But it’s not obvious to me that safety or alignment people should care more about steady incremental progress than people who just want to maximize the profit of their AI, right?\n\n**Vikrant Varma:** You mean \\[“safety people should\\] care less about that”?\n\n**Daniel Filan:** Yeah. It’s not obvious to me that there would be any difference.\n\n**Vikrant Varma:** Right. I think one way you could think about it, from a safety perspective, is multiple uncorrelated bets on ways in which we could get a safer outcome. I think probably a similar thing applies for capabilities except that… And I’m really guessing and out of my depth here, but my guess would be that for whatever reason, it’s harder to actually fund this kind of research, this kind of very exploratory, out-there research, from a capabilities perspective, but I think there is a pretty good safety case to make for it.\n\n**Daniel Filan:** Yeah, I guess it’s possible that it’s just a thing where it’s hard to … I don’t know, if I’m a big company, right, I want to have some way of turning my dollars into people solving a problem. One model you could have is for things that could be measured in “how far down did the loss go?” It’s maybe just easier to hire people and be like, “Your job is to put more GPUs on the GPU rack” or “your job is to make the model bigger and make sure it still trains well”. Maybe it’s harder to just hire a random person off the street and get them to do science of deep learning. That’s potentially one asymmetry I could think of.\n\n**Vikrant Varma:** Yeah, I think it’s also just: I genuinely feel like there are way fewer people who could do science of deep learning really well than people who could make the loss go down really well. I don’t think this fundamentally needs to be true, but it just feels true to me today based on the number of people who are actually doing that kind of scientific exploration.\n\n**Daniel Filan:** Gotcha. When I asked you about the alignment case for science of deep learning, \\[you said\\] there’s this question of dual use and then there was this question of what alignment things there might be there, and you said you’d ignore the dual use thing. I want to come back to that. What do you think about: some people say about interpretability or stuff, “well, you’re going to find insights that are useful for alignment, but you’re also going to find insights that are useful for just making models super powerful and super smart, and it’s not clear if this is good on net”.\n\n**Vikrant Varma:** Yeah. I want to say that I feel a lot of uncertainty here in general, and I think your answers to these questions kind of depend a lot on how you expect AI progress to go and where you expect the overhangs to be and what sort of counterfactual impact you expect. What kinds of things will capabilities people do anyway, for example?\n\nYeah, so I think to quickly summarize one story that I find plausible, it’s that we’re basically going to try and make progress about as fast as we can towards AGI-level models. Hopefully, if we have enough monitoring and red lines and [RSPs](https://www.anthropic.com/news/anthropics-responsible-scaling-policy) in place, if there is indeed danger as I expect, then we will be able to coordinate some sort of slow down or even pause as we get to things that are about human-level.\n\nThen, a story you could have for optimism is that: well, we’re able to use these roughly human-level systems to really make a lot of progress in alignment, because it becomes clear that that’s the main way in which anybody can use these systems safely, or that’s how you construct a strong positive argument for why the system is safe rather than just pointing at an absence of evidence that it’s unsafe, and we’re in that sort of world, and then just a bunch of uncertainty about how long that takes. In the meantime, presumably we’re able to coordinate and prevent random other people who are not part of this agreement from actually racing ahead and building an unsafe AGI.\n\nUnder that story, I think, it’s not clear that you get a ton of counterfactual capabilities progress from doing mech. interp. or science of deep learning. It mostly feels to me like we’ll get there even without it and that to the degree that these things are going to matter for capabilities, a few years from now, capabilities people are going to start \\[doing\\], maybe not science of deep learning if it’s very long-term and uncertain, but definitely mech. interp.: I expect capabilities people to start using those techniques and trying to adapt them for improving free training and so on.\n\nLike I said, I feel pretty uncertain. I am pretty sympathetic to the argument that all of this kind of research like mech. interp. and science of deep learning should basically be done in secret… If you’re concerned about safety and you want to do this research, then you should do it in secret and not publish. Yeah, I feel sympathetic to that.\n\n### Summary of the paper’s hypothesis\n\n**Daniel Filan:** Gotcha. I guess with that background, I’d like to talk about the paper. I take the story of your paper to basically be saying: look, here’s our explanation of grokking. Neural networks… you can think of them as a weighted sum of two things they can be doing. One thing they can be doing is just memorizing the data, and one thing that they can be doing is learning the proper generalizing solution.\n\nThe reason you get something like grokking is that it takes a while … Networks are being regularized, according to the norm of their parameters; and the generalizing circuit - the method that generalizes - it can end up being more confident for a given norm of parameter. And so eventually it’s favored, but it takes a while to learn it. Initially you learn to just memorize answers, but then as there’s this pressure to minimize the parameter norm that comes from some form of regularization, you become more and more incentivized to try and figure out the generalizing solution, and the network eventually gets there, and once gradient descent comes to the vicinity of the generalizing solution, it starts moving towards that, and that’s when grokking happens.\n\nAnd basically from this perspective, you come up with some predictions… you come up with this thing called ungrokking, which we can talk about later; you can say some things about how confidence should be related to parameter norm in various settings… but I take this to be your basic story. Does that sound like a good summary?\n\n**Vikrant Varma:** Yeah, I think that’s pretty good.\n\n### What are ‘circuits’?\n\n**Daniel Filan:** Gotcha. I guess the first thing that I’m really interested in is: in the paper you talk about ‘circuits’, right? You say that there’s this ‘memorizing circuit’ and this ‘generalizing circuit’. You have a theoretical model of them, and you have this theoretical model of: imagine if these circuits were competing, what would that look like? But to the best of my understanding from reading your paper, I don’t get a clear picture of what this ‘circuit’ talk corresponds to in an actual model. Do you have thoughts about what it does correspond to in an actual model?\n\n**Vikrant Varma:** Yeah, that’s a good question. We borrowed the circuit terminology from the [circuits thread](https://distill.pub/2020/circuits/) by [Chris Olah](https://colah.github.io/about.html) in Anthropic. There, they define a circuit as a computational subgraph in the network. I think this is sufficiently general or something that it applies to our case. Maybe what you’re asking though is more: physically, where is the circuit inside the network?\n\n**Daniel Filan:** If I think of it as a computational subgraph, the memorization circuit is going to take up a significant chunk of the network, right? Do you think I should think of there being two separate subgraphs that aren’t interacting very much, one of which is memorization, one of which is generalization, and just at the end we upweight the generalization and downweight the regularization?\n\nThat would be weird, because there’s going to be crosstalk that’s going to inhibit the memorizing circuit from just purely doing memorization and the generalizing circuit from purely doing generalization. When I try to picture what’s actually going on, it seems difficult for me. Or I could imagine that the memorizing circuit is just supposed to be one parameter setting for the network and the generalizing circuit is supposed to be another parameter setting and we’re linearly interpolating that. But neural networks, they’re non-linear in their parameters, right? You can’t just take a weighted sum of two parameter vectors and get away some of the output. So yeah, this is my difficulty with the subgraph language.\n\n**Vikrant Varma:** I want to make a distinction between the model or the theory that we’re using to make our predictions, and how these circuits are implemented in practice. In the model or in our theory, these circuits are very much independent, so they have their own parameter norms and the only way they interact is they add at the logit stage. And this is completely unrealistic, but we’re able to use this very simplified model to make pretty good predictions.\n\nI think the question of how circuits in this theory are actually implemented in the network is something that I would love to understand more about. We don’t have a great picture of this yet, but I think we can probably say some things about it already. One thing we can say is that there are definitely not going to be disjoint sets of parameters in the network.\n\nSome evidence for this is things like: in terms of parameters, there’s a lot of overlap between a network that’s memorizing and that later generalizes, as in a lot of the parameter norm is basically coming from the same weights. And the overlap is way more than random. And this is probably because when the network is initialized, there’s some parameters that are large and some that are small and both circuits learn to use this distribution, and so there ends up being more overlap there.\n\n**Daniel Filan:** Okay. My summary from that is you’re like, “okay, there are probably in some sense computational subgraphs and they probably overlap a bit, and we don’t have a great sense of how they interact”.\n\n**Vikrant Varma:** Yeah.\n\n**Daniel Filan:** One key point in your model is in the simplified model of networks, where they’re just independent things that get summed at the end, eventually you reduce your weight on the memorizing circuit and increase your weight on the generalizing circuit. Do you have a sense of, if I should think of this as just literally increasing and decreasing weights, or circuits cannibalizing each other somehow?\n\n**Vikrant Varma:** Yeah, maybe closer to cannibalizing somehow if there’s a lot of competition for parameters between the two circuits. I think in a sense it is also going to be increasing or decreasing weights, because the parameter norm is literally going up or down. It’s just not going to happen in the way we suggest in the model, where you have a fixed circuit and it’s just being multiplied by a scalar.\n\nIn practice, there’s going to be all kinds of things. For example, it’s more efficient under L2… if you have a circuit, instead of scaling up the circuit by just multiplying all the parameters, it’s more efficient to duplicate it if you can, if you have the capacity in the network.\n\nI also imagine that there are multiple families of circuits that are generalizing and memorizing and within each family, these circuits are competing with each other as well. And so you start off with a memorizing circuit and instead of just scaling it down or up, it’s actually morphing into a different memorizing circuit with a different distribution of parameters inside it. But the overall effect is close enough to the simplified model that it makes good predictions.\n\n### The role of complexity\n\n**Daniel Filan:** Sure. I’m wondering: one thing this theory reminded me of is [singular learning theory](https://www.lesswrong.com/s/czrXjvCLsqGepybHC), which is this trendy new theory of deep learning \\[that\\] people are into. Basically it comes from this insight where: if you think about Bayesian inference in high dimensional parameterized model classes, which is sort of like training neural networks, except we don’t actually use Bayesian inference for training neural networks… If the model class has this property called “being singular”, then you end up having phase transitions of: sometimes you’re near one solution and then as you get more data, you can really quickly update to a different kind of solution, where basically what happens is you’re trading off some notion of complexity of different solutions for predictive accuracy.\n\nNow, in the case of increasing data, it’s kind of different because the simplest kinds of phase transitions you can talk about in that setting are as you get more data, whereas you’re interested in phase transitions in number of gradient steps, but they both feature this common theme of “some notion of complexity being traded off with accuracy”. And if you favor minimizing complexity somehow, you’re going to end up with a low complexity solution that meets accuracy. I mean, that’s kind of a superficial similarity, but I’m wondering what you think of the comparison there.\n\n**Vikrant Varma:** Yeah, so I have to admit that I know very little about singular learning theory and I feel unable to really compare what we’re talking about with SLT.\n\n**Daniel Filan:** Fair enough.\n\n**Vikrant Varma:** I will say though that this notion of lower weight norm being less complex somehow is quite an old idea. In particular, [Seb Farquhar](https://sebastianfarquhar.com/) pointed me to [this 1993 paper](https://dl.acm.org/doi/pdf/10.1145/168304.168306), I think by [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton), which is about motivating L2 penalty from a [minimum description length](https://en.wikipedia.org/wiki/Minimum_description_length) angle. So if two people are trying to communicate all of the information that’s contained inside a model, they could have some priors about what the weights of the model are, and then they need to communicate both something about the dataset as well as errors that the model is going to make. And in this paper, they use these Gaussianity assumptions and are able to derive both mean squared error loss and L2 penalty as an optimal way to communicate between these two people.\n\n**Daniel Filan:** And this seems similar to the classic result that L2 regularization is sort of like doing Bayesian inference with a Gaussian \\[prior\\], just because if your prior is Gaussian, then you take the log of that and that ends up being the norm and that’s the log likelihood for you.\n\n### Many kinds of circuits\n\n**Daniel Filan:** Sure. So I guess I’d like to pick up on this thing you said about there being multiple kinds of circuits, because there’s a sentence that jumped out to me in your paper. You’re looking at doing a bunch of training runs and looking at trying to back out what you think is happening with the generalizing and memorizing circuits, and you say that the random seed starting training causes significant variance in the efficiency of the generalizing and memorizing solutions.\n\nThat kind of surprised me, partly because I think that there just can’t be that many generalizing solutions. We’re talking about fairly simple tasks like “add two numbers, modulo 113”, and how many ways can there be to do that? [I recently learned that there’s more than one](https://arxiv.org/abs/2306.17844), but it seems like there shouldn’t be a million of them. Similarly, how many ways can there be to memorize a thing?\n\nAnd then also, I would’ve thought that gradient descent would find the most efficient generalizing circuit or the most efficient memorizing circuit. So yeah, I’m wondering if you have thoughts about how I should think about this family of solutions with seemingly different efficiencies.\n\n**Vikrant Varma:** One thing I’ll point out is that even with the trigonometric algorithm for doing modular addition, this is really describing a family of algorithms because it depends on which frequencies in particular the network ends up using to do the modular addition.\n\n**Daniel Filan:** And if people are interested in that algorithm, they can check out [my episode with Neel Nanda](https://axrp.net/episode/2023/02/04/episode-19-mechanistic-interpretability-neel-nanda.html). You can probably check out other things, but don’t leave AXRP please. So yeah, \\[with\\] this algorithm, you pick some frequencies and then you rotate around the circle with those frequencies to basically do the clock algorithm for modular arithmetic, but you can pick which frequencies you use.\n\n**Vikrant Varma:** Yeah, that’s right.\n\n**Daniel Filan:** But I would’ve guessed that there wouldn’t be a significant complexity difference between the frequencies. I guess there’s a complexity difference in how many frequencies you use.\n\n**Vikrant Varma:** Yes. That’s one of the differences: how many you use and their relative strength and so on. Yeah, I’m not really sure. I think this is a question we pick out as a thing we would like to see future work on.\n\nThe other thing I want to draw attention to is in many deep learning problems, it is not the case… Deep learning practitioners are very familiar with the observation that different random seeds end up producing networks with different test performance. And if you’re trying to create a state-of-the-art network for solving some tasks, it’s quite common to run 100 seeds and then pick the top five best-performing ones or whatever. I think it’s just not clear from this that gradient descent or [Adam](https://arxiv.org/abs/1412.6980) is able to find the optimal solution from any initialization.\n\nAnd I think this also shows up not just when you vary random seed, but it shows up epoch-wise. Because for example, with one of the phenomena you mentioned from our paper, semi-grokking, you see these multiple phase transitions where the network is switching between generalizing circuits that have different levels of efficiency, and these levels of efficiency are quite far apart so that you can actually visibly see the change in test performance as it switches in a very discrete manner between these circuits. And if it was really the case that gradient descent could find the optimal solutions, then you wouldn’t expect to see this kind of switching.\n\n### How circuits are learned\n\n**Daniel Filan:** Gotcha. Yeah, there are just so many questions I have about these circuits. I’m not sure you have any answers, but it just brings up… Part of your story is that it takes a while to learn the generalizing solution, longer than it takes to learn the memorizing solution. Do you maybe have thoughts about why that might be?\n\n**Vikrant Varma:** I think my thoughts here are mostly what we’ve written down in the paper and I feel like this is another area that’s pretty ripe for understanding. The explanation we offer in the paper is mostly inspired by [a blog post](https://www.lesswrong.com/posts/RKDQCB6smLWgs2Mhr/multi-component-learning-and-s-curves) by [Buck](https://scholar.google.com/citations?user=oyDxKw0AAAAJ&hl=en&oi=ao) \\[Shlegeris\\], and I think another person whose name I’m forgetting.\n\n**Daniel Filan:** [Ryan Greenblatt](https://www.lesswrong.com/users/ryan_greenblatt)? \\[Update: it’s actually [Adam Jermyn](https://adamjermyn.com/)\\]\n\n**Vikrant Varma:** Yes, that’s right. And the explanation there is that maybe memorizing circuits basically have fewer independent components that you need in order for the circuit to develop, but a generalizing circuit is going to need multiple components that are all needed for good performance.\n\nHere’s the simplified model: the memorizing network is implemented with one parameter and that just scales up or down, and the generalizing network is implemented with two parameters that are multiplied together to get the correct logit, and so the gradient of the output with respect to any of the parameters depends on the value of the other parameter in the generalizing circuit case.\n\nAnd so if you simulate this forward, what you get for the generalizing circuit is a kind of sigmoid where initially both parameters are quite small and they’re not contributing that much to each other’s growth, and then once they start growing, both of them grow quite a lot and then it plateaus out because of L2.\n\n**Daniel Filan:** Sure. Should I think of this as just a general observation that in evolution, if you need multiple structures and they both depend on each other for that to work, that’s a lot harder to evolve than a structure that is a little bit useful on its own and another structure that is a little bit useful on its own? And for memorizing a solution, I can memorize one thing and then I can memorize the second thing and I can do those independently of each other, so each little bit of memorization is evolvable on its own maybe?\n\n**Vikrant Varma:** Yes, I think that’s right. Maybe another way I think about it is that the memorization circuit is basically already there, \\[in a\\] random network. And really the thing you’re learning is the values that you have to memorize. And as you say, that’s independent for each point, but that’s not the case for \\[the\\] generalizing circuit.\n\nI think another important ingredient here is that there needs to be at least some gradient at the beginning towards the generalizing circuit if it has multiple components. And it’s kind of an open question in my mind why this happens. The most plausible theory I’ve heard is something like lottery tickets, where basically the randomly initialized network has very weak versions of the circuits that you want to end up with. And so there’s a tiny but non-zero gradient towards them.\n\n**Daniel Filan:** Interesting. Yeah, I guess more work is needed. I’d like to talk a little bit about … The story of: it takes you a while to learn the generalizing circuit. You learn the memorizing circuit and it takes you a while to learn the generalizing circuit, but once you do, then that’s grokking.\n\nThis is sort of reminiscent of a story that I think was in an appendix of a paper, [Progress Measures for Grokking? It was progress measures for something](https://arxiv.org/abs/2301.05217) by [Neel Nanda](https://www.neelnanda.io/about) et al. And they have this story where there are three phases of circuit formation. There’s memorization and then there’s learning a generalizing solution, and then there’s cleaning up the memorized stuff, right? And in their story, they basically demarcate these phases by looking at the activations of their model and figuring out when the activations are representing the algorithm that is the generalizing solution according to them.\n\nAnd so it seems pretty similar to your story, but one thing that struck me as being in a bit of tension is that they basically say grokking doesn’t happen when you learn the generalizing solution, it happens when you clean up the parameters from the memorizing solution. Somehow there’s one stage of learning the generalizing solution and then a different phase of forgetting the memorizing solution. So I’m wondering what you think about the relationship between that story in their paper and your results.\n\n**Vikrant Varma:** I think one thing that’s interesting to think about here is the relationship between logits and loss, or between logits and accuracy. Why is the memorization cleanup important? It’s because to a first approximation, the loss is dependent on the difference between the highest logit and the second highest logit. And if you have this memorization circuit that is kind of incorrectly putting high weight on the incorrect logit, then when it reduces, you’ll see a very sharp cleanup effect.\n\nI think this is something that we haven’t really explored that much because the circuit efficiency model is mostly talking about the steady state that you expect to end up in and is not so much talking about the dynamics between these circuits as time goes on. This is a part of the story of grokking that is very much left unexplained in our paper, which is why exactly is the generalizing circuit developing slower? But if you put in that sigmoid assumption, as I was talking about, artificially, then the rest of the theory is entirely sufficient to see exactly the same kinds of grokking curves as you see in actual grokking.\n\n**Daniel Filan:** But maybe I’m misunderstanding, but under your story, I think I would’ve expected a visible start of grokking, or visible increase in test accuracy during the formation of the generalizing circuits, rather than it waiting until the cleanup phase.\n\n**Vikrant Varma:** Right. By formation, are you talking about … there’s this phase where the generalizing circuit is developing and it’s there, but the logits that it’s outputting are way lower than the memorization logits. And in that phase, I basically don’t expect to see any change in test accuracy.\n\nAnd then there’s the phase where the generalizing circuit logits cross the memorizing circuit for the first time. And this is maybe another place where there’s a difference between the toy model and what you actually observe. In the toy model, the thing you would expect to see is a jump from 0% accuracy as it’s just below the equality threshold to 100% accuracy the moment the generalizing logit crosses the memorizing logit, because that changes what the highest strength logit is.\n\nBut in practice, we see the accuracy going through all these intermediate phases, so it goes through 50% before getting to 100%. And the reason that’s happening is because there are some data points which are being correctly classified and some which are incorrectly classified.\n\nAnd so this is pointing to a place where the theory breaks down, where on some of the points, the generalizing circuit is making more confident predictions than on other points, which is why you get these intermediate stages of accuracy, so that’s one thing.\n\nThis also suggests why the cleanup is important to get to full accuracy. If the memorizing circuit is making these high predictions on many points, even when the generalizing circuit is around the same level, because of the variance, you just need the memorizing circuit to disappear before you really get 100% prediction accuracy.\n\n**Daniel Filan:** Right, so the story is something like: you start learning the generalizing circuit and you start getting the logits being somewhat influenced by the generalizing circuit, but you need the logits to start being somewhat near each other to get a hope of making a dent in the loss. And for that to happen, you more need the memorization circuits to go away. There’s the formation of the circuit, and then there’s switching the weights over, is roughly the story that I’m thinking of.\n\n**Vikrant Varma:** Yeah, that’s right.\n\n### Semi-grokking and ungrokking\n\n**Daniel Filan:** Gotcha. There’s something that you mentioned called semi-grokking and ungrokking. Actually, can you describe what they are?\n\n**Vikrant Varma:** Sure. I’ll start with how should you think about the efficiencies of these two different circuits. If you have a generalizing circuit that is doing modular addition, then if you add more points to the training set, it doesn’t change the algorithm you need to get good training loss. And so you shouldn’t really expect any modification to the circuit as you add more training points. And therefore, the efficiency of the circuit should stay the same. Whereas if you have a memorizing circuit, then as you add more points, it needs more weights to memorize those points, and so you should expect the efficiency to be dropping as you add more points.\n\n**Daniel Filan:** Yeah, or another way I would think about this is that if I’m memorizing n points - I figured out the most efficient way to memorize the n points - the (n+1)th point, I’m probably not going to get that right because I just memorized the first ones, so I’ve got to change to memorize the (n+1)th point, and I can’t change in a way that makes me more efficient because I was already the most efficient I could be on the n points. And so even just at a macro level, just forgetting about adding weights or whatever, it just has to be the case that memorization is losing efficiency the more you memorize whereas generalization, you wouldn’t expect it to have to lose efficiency.\n\n**Vikrant Varma:** Yeah. Yeah, that’s a good way to explain it.\n\n**Daniel Filan:** And of course I stole that from your paper. I don’t want to act like I invented that.\n\n**Vikrant Varma:** No, but it’s a good explanation. So I think a direct consequence of this… so when you couple that with the fact that at very low dataset sizes, it appears that memorization is more efficient than generalization, then you can conclude that there must be a dataset size where memorization is increasing as you increase the dataset size, generalization parameter norm is staying the same… There must be a crossover point. And then you can ask the question: what happens at that crossover point, when you have a dataset size where the efficiency of generalization is equal to the efficiency of memorization?\n\nAnd so we did some maths in our toy model, or our theoretic model I should say, and came up with these two cases, these two different things that could happen there. And this really depends on the relationship between… When you scale the parameters by some amount, how does that scale the logits? And if it scales the logits by more than some threshold, then it turns out that at this equality point you will just end up with a more efficient circuit period. But if the scaling factor is lower than some threshold, then you will actually end up with a mixture of both the memorizing and the generalizing circuits. And the reason for this is: because you’re not able to scale the logits as much, it’s more efficient to allocate the parameter norm between these two different circuits when you are considering the joint loss of L2 plus the data loss.\n\n**Daniel Filan:** Okay, so something like… it’s sort of the difference between convex and concave optimization, right? You’re getting diminishing returns per circuit, and so you want to invest in multiple circuits rather than going all in on one circuit. Whereas in some cases, if you have increasing returns, then you just want to go all in on the best circuit.\n\n**Vikrant Varma:** Yeah, that’s right. And in particular, the threshold is like… there’s quite a cool way to derive it, which is that the L2 is scaling as the square of the parameter norm. So if the logits are scaling faster than that, then you’re able to overcome the parameter penalty by just investing in the more efficient circuit. But if they’re not scaling faster than that, then you have to split. And so the threshold ends up being if you’re able to scale the logits faster than to the power of two.\n\n**Daniel Filan:** Okay. So you have this semi-grokking and ungrokking, right? Where you’re training on this subset of your training dataset and you lose some test accuracy - either some of it or all of it - basically by partly or fully reverting to the memorizing solution. So this is an interesting phenomenon because… maybe you know better than me, but I’m not aware of people talking about this phenomenon or connecting it to grokking before. Or they’ve talked about the general phenomenon of catastrophic forgetting, where you train your network on a different dataset and it forgets stuff that \\[it\\] used to know. But in terms of training on a subset of the dataset, I’m not aware of people discussing that before or predicting that before. Is that right?\n\n**Vikrant Varma:** Yeah, I think that’s right. So we got a lot of questions from reviewers about “how is ungrokking any different from catastrophic forgetting?”, to the extent that in the newer version of the paper, we have a whole section explaining what the difference is.\n\nI think basically I would view it as a much more specific and precise prediction than catastrophic forgetting. So one difference is that we’re training on a subset of the data, and this is quite important because this rules out a bunch of other hypotheses that you might have about why grokking is happening.\n\nSo for example, if your hypothesis is: the reason grokking happens is because you don’t have the correct representations for the modular addition task, and once you find those representations, then you’ve grokked - that’s a little bit incompatible with then reducing the training data size and ungrokking, because you already had the representations and so you need this additional factor of a change in efficiency.\n\nOr another example is a random walk hypothesis, where somehow you stumble upon the correct circuit by randomly walking through parameter space. And that also either doesn’t say anything about it, or anti-predicts ungrokking, because you were already at that point. So I think that’s quite an important difference.\n\nI think going back to the difference between catastrophic forgetting \\[and ungrokking\\], I think another more precise prediction is that we’re able to predict the exact dataset size at which you see ungrokking, and it’s quite a phase-change-y phenomena or something. It’s not like as you decrease the dataset size, you’re smoothly losing test accuracy, in this case, which is more the kind of thing you might expect from traditional catastrophic forgetting.\n\n**Daniel Filan:** Right. My impression was that the thing you were predicting would be that there would be some sort of phase change in terms of subset dataset size, and also that that phase change would occur at a point independent of the strength of weight decay.\n\n**Vikrant Varma:** That’s right.\n\n**Daniel Filan:** But I thought that you wereln’t able to predict where the phase change would occur. Or am I wrong about that?\n\n**Vikrant Varma:** That’s right. Our theory is not producing a quantitative prediction of exactly what dataset fraction you should expect that phase change to happen at. That’s right.\n\n**Daniel Filan:** Yep. But it does predict that it would be a phase change and it would happen at the same point for various levels of weight decay. One cool thing about this paper is it really is a nice prediction and you’ve got a bunch of nice graphs, \\[you\\] kind of nail it, so good job on that.\n\n**Vikrant Varma:** Thank you.\n\n**Daniel Filan:** But one thing I’m wondering about is: you have this phenomenon of ungrokking and it seems at least like an instance of catastrophic forgetting that you’re able to say more about than people have previously been able to say. But this offers an opportunity to try and retrodict phenomena, or in particular… I’m not an expert in catastrophic forgetting, but my understanding is that one of the popular approaches to it is this thing called “[elastic weight consolidation](https://www.pnas.org/doi/full/10.1073/pnas.1611835114)”, where you basically have different learning rates per parameters, and you reduce the learning rate, so you reduce the future change in parameters for those parameters that were important for the old task. That’s one method, you might be aware of others. Does your view of grokking and ungrokking retrodict these proposed ways of dealing with catastrophic forgetting?\n\n**Vikrant Varma:** I think not directly. I can see a few differences. I’m not aware of this exact paper that you’re talking about, but I think depending on the task, there might be different reasons why you’re getting forgetting. So you might be forgetting things that you memorized or you might be forgetting algorithms that are appropriate for that part of the data distribution. That’s one aspect of it.\n\nI think a different aspect is that it’s not clear to me why you should expect these circuits to be implemented on different weights. So if the theory is that you find the weights that are important for that algorithm and then you basically prevent those weights from being updated as fast, so you’re not forgetting, then I think that is pointing at a disjoint implementation of these circuits in the network. And that’s not something that we are really saying anything directly about.\n\n**Daniel Filan:** Gotcha. Yeah, I guess it makes sense that it would depend on the implementation of these circuits.\n\nAnother question I have is: in a lot of your experiments, like you mentioned, you are more interested in the steady state than the training path, except for just the initial prediction of grokking, I guess.\n\n**Vikrant Varma:** To be clear, the paper deals with the steady state; I’m very interested in the training path as well.\n\n**Daniel Filan:** Fair enough. So if I look at the ungrokking stuff, it seems like… So there’s this steady state prediction where there’s this critical dataset size, and once you’re below the critical dataset size, you ungrok and it doesn’t really matter what your weight decay strength was.\n\nIf I naively think about the model, it seems like your model should suggest that it should take longer for less weight decay because you have less pressure to… You care about the complexity, but you’re caring about it less, per unit of time. And similarly, that grokking should be quicker for more weight decay. I guess it’s a two-part question. Firstly, do you agree that that’s a prediction of this model? And secondly, did that bear out?\n\n**Vikrant Varma:** Yeah, so I think this is a prediction of the model, assuming you’re saying that weight decay affects the speed of grokking?\n\n**Daniel Filan:** Yes, and of ungrokking.\n\n**Vikrant Varma:** Yeah, I think that is a prediction of the model. Well, to be fair, it is a retrodiction, because [the Power et al. paper](https://arxiv.org/abs/2201.02177) already shows that grokking takes exponentially longer as you reduce the dataset size, and I forget what the relationship is, but it definitely takes longer as you reduce the weight decay.\n\n**Daniel Filan:** And does ungrokking take longer as you reduce weight decay?\n\n**Vikrant Varma:** We don’t show this result in the paper, but I’m fairly sure I remember that it does, yeah.\n\n### Generalizing the results\n\n**Daniel Filan:** Okay, cool. So I’d like to talk a bit about possible generalizations of your results. So as written, you’re basically talking about efficiency in parameter norm, where if you increase parameter norm, you’re able to be more confident in your predictions, but that comes at a penalty if you train with weight decay.\n\nNow, as your paper notes, weight decay is not the only situation in which grokking occurs, and you basically hypothesize that there are other forms of regularization, regularizing against other forms of complexity and that there could be some sort of efficiency in those other forms of complexity that might become relevant.\n\nI’m wondering, do you have thoughts on what other forms of complexity I should be thinking of?\n\n**Vikrant Varma:** Yeah, so we’ve already talked about one of them, which is that circuits might be competing for parameters on which to be implemented. This is a kind of capacity constraint. And so you might think that circuits that are able to be implemented on fewer parameters, or using less capacity (however you define capacity in the network), would be more efficient. So I think some relevant work here is bottleneck activations: I think this is from “[Mathematical circuits of transformers](https://transformer-circuits.pub/2021/framework/index.html)”, which is talking about other phenomena like superposition that you would get from constrained capacity.\n\nSo that’s one more notion of efficiency. I think possibly robustness to interference could be another kind of efficiency: how robust is the circuit to being randomly invaded by other circuits. Maybe also robustness to drop-out would be a similar thing here. And then I think there are things like how frequently does the circuit occur, which might be important… From a given random seed will you be able to find it?\n\n**Daniel Filan:** Do you mean: what’s the probability mass of it on the initialization prior over weights?\n\n**Vikrant Varma:** Yes. And also then on the kinds of parameter states that SGD is likely to find. So this is the implicit priors in SGD. There’s [some work](https://arxiv.org/abs/2101.12176) on implicit regularization of SGD and showing that it prefers similar kinds of circuits to what L2 might prefer, but probably it’s different in some interesting way.\n\n**Daniel Filan:** Okay. If I think about the results in your paper, a lot of them are generic to other potential complexity measures that you could trade off confidence against. But sometimes you rely on this idea… in particular for your analysis of grokking and semi-grokking, you play with this math notion of: if I scale up some parameters in every layer of a ReLU network, that scales up the logits by this factor, and therefore you get this parameter norm coming off. And I think this is involved in the analysis of semi-grokking versus ungrokking, right?\n\n**Vikrant Varma:** Yes.\n\n**Daniel Filan:** So I guess the prediction here would be that maybe semi-grokking is more likely to occur for things where you’re trading off weight parametrization as compared to robustness to drop-out or stuff. Does that sound right to you?\n\n**Vikrant Varma:** I think in general it’ll be very hard to observe semi-grokking in realistic settings because you need such a finely tuned balance. You need all these ingredients. You need basically two circuits, or two pretty distinct families of circuits, with no intermediate circuits that can do the task well between them. You need a way to arrange it so that the dataset size or other hyperparameters are causing these two circuits to have very, very similar levels of efficiency.\n\nAnd then also you need it to be the case that, under those hyperparameters, you’re able to actually find these two families of circuits. So you’ll probably find the memorizing one, but you need to be able to find a generalizing one in time. And this just seems like quite a hard thing to happen all at once, especially the fact that in realistic tasks you’ll have multiple families of circuits that are able to do the training task to some degree.\n\n**Daniel Filan:** So semi-grokking seems unlikely. I guess it does seem like the prediction would be that you would be able to observe ungrokking for the kinds of grokking that don’t depend on weight decay. Am I right that that is a prediction, and is this a thing that you’ve tested for yet? Or should some enterprising listener do that experiment?\n\n**Vikrant Varma:** So to be clear, the experiment here is “find a different notion of efficiency and then look for ungrokking under that notion”?\n\n**Daniel Filan:** The experiment would be “find an instance of grokking that doesn’t happen from weight decay”. Then it should be the case that: \\[you\\] train your data, get grokking, then train data on subsets of various sizes, and there should be a critical subset size where below that you ungrok and above that you retain the grokking solution when you fine-tune on that subset.\n\n**Vikrant Varma:** Yeah, I think this is basically right, and our theory does predict this. I will caveat that dataset size may not be the right variable to vary here, depending on what notion of efficiency you’re using.\n\n**Daniel Filan:** Well, I guess in all notions of efficiency, it sounded like there was a prediction that efficiency would go down as the dataset increased for the memorizing solution, but not for the generalizing solution, right?\n\n**Vikrant Varma:** Yeah, that’s right.\n\n**Daniel Filan:** As long as you believe that the process is selecting the most efficient circuit.\n\n**Vikrant Varma:** Yeah, that’s right.\n\n**Daniel Filan:** Which we might worry about if there’s… you mentioned SGD found different efficiency generalizing solutions, so maybe you might be worried about optimization difficulty. And in fact maybe something like parameter norm is easier to optimize against than something like drop-out robustness, which is less differentiable or something.\n\n**Vikrant Varma:** Yeah, I think that’s right. I think you’re right that in this kind of regime, dataset size is pretty reasonable. I was imagining things like model-wise grokking, where on the X-axis, instead of amount of data, you’re actually varying the size of the model or the number of parameters or the capacity or whatever.\n\nBut all of these different models are actually trained on the same amount of data for the same time. And it’s also less clear how exactly you would arrange to show ungrokking there because naively, you can’t go to a higher-size model and then reduce the size of the model. But maybe there are ways to show that there.\n\nVikrant’s research approach\n---------------------------\n\n**Daniel Filan:** Gotcha. So if it’s all right with you, I’d like to move on to just general questions about you and your research.\n\n**Vikrant Varma:** Cool.\n\n**Daniel Filan:** So the first question I have is: we’ve talked about your work on grokking, we’ve talked about your work on latent knowledge in large language models. We haven’t talked about it, but the other paper I know you for is this one on [goal misgeneralization](https://arxiv.org/abs/2210.01790). Is there a common thing that underlies all of these that explains why you worked on all of them?\n\n**Vikrant Varma:** Well, one common thing between all of them is that they are all projects that are accessible as an engineer without much research experience, which was one of my main selection criteria for these projects.\n\nSo yeah, I guess my background is that I’ve mostly worked in software engineering, and then around 2019 I joined DeepMind and about a year later I was working on the alignment team. I did not have any research experience at that time, but I was very keen to figure out how you can apply engineering effort to make alignment projects go better.\n\nAnd so certainly for the next two or three years, I was mainly in learning mode and trying to figure out how do people think about alignment? What sorts of projects are the other people in the alignment team interested in? Which ones of these look like they’re going to be most accelerated by just doing good engineering fast? And so that’s where a bunch of the early selection came from.\n\nI think now I feel pretty drawn to working on maybe high risk, high reward things that might end up mattering if alignment by default (as I see the plan) doesn’t go as expected. It feels like the kind of thing that is potentially more neglected. And maybe if you think that you need a bunch of serial research time to do that now before you get very clear signals that, I don’t know, we haven’t done enough research on some particular kind of failure mode, then that feels important to do now.\n\n**Daniel Filan:** Okay. So should I be thinking: lines of research where both, they’re approachable from a relatively more engineering-heavy background, and also laying the foundation for work that might come later rather than just attempting to quickly solve a problem?\n\n**Vikrant Varma:** Yeah, that’s right. That’s certainly what I feel more drawn to. And so for example, I feel pretty drawn to the [eliciting latent knowledge problem](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit). I think there is both interesting empirical work to do right now in terms of figuring out how easy is it to actually extract truth-like things from models as we’ve been discussing, but also framing the problem in terms of thinking about methods that will scale to superintelligent systems\\[, this\\] feels like the kind of thing that you just need to do a bunch of work in advance. And by the time you’re hitting those sorts of problems, it’s probably quite a bad situation to be in.\n\n**Daniel Filan:** Gotcha. So what should I think of as your role in these projects?\n\n**Vikrant Varma:** I think it varies. I would describe it as a mix of coming up with good research ideas to try, trying to learn from people who have been around in the alignment community much longer than me, and also trying to supply engineering expertise\n\nSo for example, currently I’m working on [sparse autoencoders](https://arxiv.org/abs/2309.08600) for mechanistic interpretability, and I am very new to mechanistic interpretability. However, all of the people I work with (or many of the people I work with) have been around in mech. interp. for a long time. And it’s great for me to understand and try to get knowledge directly from the source in a way.\n\nI think at the same time, with sparse autoencoders in particular, that’s the kind of project where… Partly what drew me to it was [Chris Olah’s tweet](https://twitter.com/ch402/status/1709998674087227859) where he said… I’m not sure exactly what he said, but it was something like “mech. interp. might be in a different mode now where if SAEs \\[Sparse AutoEncoders\\] work out, then it’s mostly an engineering problem, it’s not so much a scientific problem”. And that kind of thing feels very exciting to me, if we’re actually able to scale up to frontier models.\n\n**Daniel Filan:** It could be. I do find myself thinking that there’s still a lot of science left to do on SAEs, as far as I can tell.\n\n**Vikrant Varma:** Yeah, I don’t disagree with that.\n\n**Daniel Filan:** Perhaps I should say for the listener, a sparse autoencoder - the idea is that you want to understand what a network is thinking about. So you train a function from an intermediate layer of the neural network to a very large vector space, way more dimensions than the underlying thing, and then back to the activation space, and you want to train this to be the identity function, but you want to train it so that the intermediate neurons of this function that you’ve learned very rarely fire. And the hope is that you’re picking up these underlying axes of variation, and hopefully only a few of them are happening at a time, and hopefully they correspond to concepts that are interpretable, and that the network uses, and that are underlying facts about the network and not just facts about the dataset that you happen to train the autoencoder on.\n\nAnd all three of those conditions seem like they need more work to be established. I don’t know, I’m not super up to date on the SAE literature, so maybe somebody’s already done this, but I don’t know, that’s a tangent from me.\n\n**Vikrant Varma:** I definitely agree. I think there’s a ton of scientific work to do with SAEs. It just also happens to be the case that there’s… It feels like there’s a more direct path or something to scaling up SAEs and getting some sort of mech. interp. working on frontier models that, at least in my view, was absent with previous mech. interp. techniques, where it was more…\n\n**Daniel Filan:** Human intensive, I guess?\n\n**Vikrant Varma:** Yeah, more human intensive and a much less direct path to doing the same kind of in-depth circuit analysis on larger models.\n\nThe DeepMind alignment team\n---------------------------\n\n**Daniel Filan:** I’d next like to ask about the alignment team at DeepMind. So obviously I guess you’ve been there for a few years.\n\n**Vikrant Varma:** Yeah.\n\n**Daniel Filan:** What’s it like?\n\n**Vikrant Varma:** It is honestly the best place I’ve worked. I find the environment very stimulating, there’s a lot of freedom to express your opinions or propose research directions, critique and try to learn from each other. I can give you an overview of some of the projects that we’re currently working on, if that helps.\n\n**Daniel Filan:** Yeah. That sounds interesting.\n\n**Vikrant Varma:** So I think the team is roughly evenly split between doing [dangerous capability evaluations](https://arxiv.org/abs/2403.13793), doing mechanistic interpretability, rater assistance, and various other emerging projects like debate or process supervision. So that’s roughly the split right now.\n\nI think apart from that, to me it feels like an inflection point right now because safety is getting a lot of attention within DeepMind, I think. So [Anca Dragan](http://people.eecs.berkeley.edu/~anca/) recently joined us, she is a professor at Berkeley. To me it feels like she has a lot of buy-in from leadership for actually pushing safety forward in a way that feels new and exciting. So as one example of this, we’re spinning up an alignment team in the Bay Area. Hopefully we’ll have a lot more resources to do ambitious alignment projects in the future.\n\n**Daniel Filan:** Sure. Is that recruiting new people from the Bay Area or will some people be moving from London to seed that team?\n\n**Vikrant Varma:** That’s going to be mostly recruiting new people.\n\nFollow-up work\n--------------\n\n**Daniel Filan:** Gotcha. The second to last thing I’d like to ask is: we’ve talked about this grokking work and this work on checking this CCS proposal. Do you have thoughts on follow-up work on those projects that you’d really like to see?\n\n**Vikrant Varma:** Yeah, definitely. So I think with the grokking paper, we’ve been talking about a bunch of potential follow-up work there. I think in particular, exploring other notions of efficiency seems really interesting to me. I think the theory itself still can produce quite a lot of interesting predictions. And from time to time I keep thinking of new predictions that I would like to try that that just don’t fit into my current work plans and stuff.\n\nSo an example of a prediction that we haven’t written down in the paper but that occurred to me a few days ago, is that: our theory is predicting that even at large dataset sizes where you’re seeing grokking, if the exponent with which you convert parameter norms into efficiency is small enough, then you should expect to see a non-zero amount of memorization even at large dataset sizes. So the prediction there is that there should be a train-test gap that is small but not zero. And this is in fact true. And so the thing you should be able to do with this is use the empirical estimates of memorization and generalization efficiency to predict train-test gap at any dataset size.\n\nSo that’s one example. I think this theory is pretty fruitful and doing work like this is pretty fruitful. I would love to see more of that. On the CCS side, a thing I would love to see is test beds for ELK methods. So what I mean by that is examples of networks that are doing something deceptive, or that otherwise have some latent knowledge that you know is in there but is not represented in the outputs. And then you’re really trying your hardest to get that latent knowledge out, using all sorts of methods like linear probing or black box testing, maybe anomaly detection. But I think without really good test beds, it’s hard to know. It’s easy to fool yourself about the efficacy of your proposed ELK method. And I think this is maybe quite related to the [model organisms agenda](https://www.lesswrong.com/posts/ChDH335ckdvpxXaXX/model-organisms-of-misalignment-the-case-for-a-new-pillar-of-1) as well.\n\n**Daniel Filan:** Well, I think that wraps up about what we wanted to talk about. Thanks very much for being on the show.\n\n**Vikrant Varma:** Yeah, thanks for having me.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Filming occurred at [FAR Labs](https://far.ai/labs/). Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) and [Lightspeed Grants](https://lightspeedgrants.org/), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev and Tor Barstad. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nIn 2022, it was announced that a fairly simple method can be used to extract the true beliefs of a language model on any given topic, without having to actually understand the topic at hand. Earlier, in 2021, it was announced that neural networks sometimes ‘grok’: that is, when training them on certain tasks, they initially memorize their training data (achieving their training goal in a way that doesn’t generalize), but then suddenly switch to understanding the ‘real’ solution in a way that generalizes. What’s going on with these discoveries? Are they all they’re cracked up to be, and if so, how are they working? In this episode, I talk to Vikrant Varma about his research getting to the bottom of these questions.\n\nTopics we discuss:\n\n * Challenges with unsupervised LLM knowledge discovery, aka contra CCS\n   * What is CCS?\n   * Consistent and contrastive features other than model beliefs\n   * Understanding the banana/shed mystery\n   * Future CCS-like approaches\n   * CCS as principal component analysis\n * Explaining grokking through circuit efficiency\n   * Why research science of deep learning?\n   * Summary of the paper’s hypothesis\n   * What are ‘circuits’?\n   * The role of complexity\n   * Many kinds of circuits\n   * How circuits are learned\n   * Semi-grokking and ungrokking\n   * Generalizing the results\n * Vikrant’s research approach\n * The DeepMind alignment team\n * Follow-up work\n\nDaniel Filan: Hello, everybody. In this episode I’ll be speaking with Vikrant Varma, a research engineer at Google DeepMind, and the technical lead of their sparse autoencoders effort. Today, we’ll be talking about his research on problems with contrast-consistent search, and also explaining grokking through circuit efficiency. For links what we’re discussing, you can check the description of this episode and you can read the transcript at axrp.net.\n\nAll right, well, welcome to the podcast.\n\nVikrant Varma: Thanks, Daniel. Thanks for having me.\n\n\nChallenges with unsupervise",
      "wordCount": 18890
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "mSTmKrSkFBswHaS3T",
        "name": "Eliciting Latent Knowledge",
        "slug": "eliciting-latent-knowledge"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "fpEBgFE7fgpxTm9BF",
        "name": "Machine Learning  (ML)",
        "slug": "machine-learning-ml"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "afp4ySyMsX78PvN4g",
        "name": "Grokking (ML)",
        "slug": "grokking-ml"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "bwjfXXf3GmGSe6rrP",
    "title": "Bayesian inference without priors",
    "slug": "bayesian-inference-without-priors",
    "url": null,
    "baseScore": 26,
    "voteCount": 12,
    "viewCount": null,
    "commentCount": 8,
    "createdAt": null,
    "postedAt": "2024-04-24T23:50:08.312Z",
    "contents": {
      "markdown": "_Epistemic status: party trick_\n\nWhy remove the prior\n--------------------\n\nOne famed feature of Bayesian inference is that it involves prior probability distributions. Given an exhaustive collection of mutually exclusive ways the world could be (hereafter called ‘hypotheses’), one starts with a sense of how likely the world is to be described by each hypothesis, in the absence of any contingent relevant evidence. One then combines this prior with a likelihood distribution, which for each hypothesis gives the probability that one would see any particular set of evidence, to get a posterior distribution of how likely each hypothesis is to be true given observed evidence. The prior and the likelihood seem pretty different: the prior is looking at the probability of the hypotheses in question, whereas the likelihood is looking at the probability of the evidence (assuming the hypothesis is true).^[1](#fn:1)^\n\nCritics of Bayesian inference sometimes denounce the reliance on priors for being subjective or unscientific. Indeed, they are by design meant to describe what one would think without any relevant (contingent) data. One might therefore be tempted to describe a form of Bayesian inference where no special role is played by the prior distribution, as distinct from the likelihood.\n\nAnother motivation comes from doing Bayesian calculations by hand. In real-world cases, such as [trying to infer whether the first COVID-19 outbreak spread from a laboratory or human contact with infected animals](https://docs.google.com/document/d/1qzLC55jRfdS55oSqXJZTFItsvFsawWgNlgLxWqhCuyo/edit?usp=sharing), the kind of thinking one does to determine a prior probability distribution is very similar to the kind of thinking one does to determine likelihoods: in both cases, one has some sort of generative model in mind—that is, some sort of probabilistic process of generating worlds—and one is trying to figure out how often worlds produced by this generative model have various properties. This might make one wonder if one could unify the prior and the likelihood.\n\nHow to remove the prior (by turning it into a likelihood)\n---------------------------------------------------------\n\nSo, how are we going to do this?\n\nFirst, a prerequisite. I’m going to be talking about the “odds ratio” form of Bayes’ theorem. This involves comparing the ratio of the probabilities of two hypotheses—that is, asking questions like “how many times more likely is the COVID outbreak to be a lab leak (LL) rather than a zoonotic spillover (Zoo), given the evidence E we’ve seen?”. Symbolically, we’re asking about P(LL | E) / P(Zoo | E). Bayes’ theorem tells us that this is equal to P(LL) / P(Zoo) times P(E | LL) / P(E | Zoo) - that is, the ratio of the hypotheses’ prior probabilities, multiplied by the ratio of the likelihoods of the given evidence under the hypotheses. If we then observed subsequent evidence E’, we would want to know P(LL | E, E’) / P(Zoo | E, E’), and Bayes’ theorem says that that’s equal to P(LL) / P(Zoo) times P(E | LL) / P(E | Zoo) times P(E’ | LL, E) / P(E’ | Zoo, E)—basically, for each additional piece of evidence, we get a new likelihood ratio for the new evidence given the hypotheses and the old evidence.\n\nWith that set-up established, I’d like you to imagine a certain way you could come to be doing this calculation. Suppose someone first asks you: “How many times more likely is the first COVID-19 outbreak to have been a lab leak rather than a zoonotic spillover?”. However, you’re kind of tired and not paying that close attention, so what you hear is “How many times more likely is _mumble_ to have been _mumble_ rather than _mumble_”. You know that the speaker made two utterances, that represent some sort of mutually exclusive hypotheses, but you have no idea what’s going on beyond that. You are now in the position of wondering how much more likely the referent of utterance 1 (U1) is to be true compared to the referent of utterance 2 (U2).\n\nIn this case, I’m going to assume you have a probability distribution over what hypotheses various utterances might mean. I’m also going to make further assumptions about these hypotheses:\n\n1.  The hypotheses are all mutually exclusive.^[2](#fn:2)^\n2.  Both utterances “come from the same distribution”, meaning that there’s no difference between how likely utterances 1 and 2 are to mean various things. That is, P(U1 means H) = P(U2 means H) for all H.\n3.  The probability that some utterance U is true, conditional on it meaning hypothesis H, is just the probability that H is true. That is, P(U | U means H) = P(H | U means H).\n4.  The probability of any “mundane” event E1 not involving utterances conditional on utterance U being true, U meaning H, and various other utterances meaning various other things, and possibly also on mundane event E2, is equal to the probability of that event given H being true, U meaning H, and various other utterances meaning various other things, and on E2. That is, P(E1 | U, U means H, U’ means H’, E2) = P(E | H, U means H, U’ means H’, E2).\n5.  Which utterances mean which things is probabilistically independent of anything else in the world (except for which utterances are true), including which hypotheses are true and which evidence we’d see under which hypotheses.\n6.  Furthermore, conditioned on the meaning of utterance U, whether or not U is true is probabilistically independent of the meaning of other utterances.\n\nAssumption 1 lets us treat the hypotheses as usual, assumption 2 encodes that there’s no difference between the first and second utterances, assumptions 3 and 4 say that if utterance U means hypothesis H then we can treat “U is true” the same as “H is true”, and assumptions 5 and 6 say that learning what various utterances mean doesn’t tell you anything about substantive questions about the world. Note: I wouldn’t be surprised if there were a more compact way of writing these assumptions, but I don’t know what it is.\n\nNow that we have these assumptions, we can do some calculations. First of all: what’s our prior ratio over whether U1 or U2 is true? Intuitively, it should be exactly 1, meaning that they’re just as likely as each other to be true, because there’s no difference between them. Here’s a proof of that: P(U1) can be calculated by summing the probability that U1 means H and U1 is true over every hypothesis H. That is, P(U1) = sum over H of P(U1, U1 means H) = sum over H of P(U1 means H)P(U1 | U1 means H) = sum over H of P(U1 means H) P(H | U1 means H) = sum over H of P(U1 means H) P(H), where first we used the chain rule of probability, second we used assumption 3, and third we used assumption 5. Likewise, P(U2) = sum over H of P(U2 means H) P(H). Next, we should notice that assumption 2 says that P(U1 means H) is equal to P(U2 means H) for every H. Therefore, P(U1) = sum over H of P(U1 means H) P(H) = sum over H of P(U2 means H) P(H) = P(U2), so P(U1) / P(U2) = 1.\n\nAlright, so our prior ratio is exactly 1. This is great news, because it means that the prior is doing no work in our computation, because multiplying numbers by 1 doesn’t change them! We have therefore banished the feared prior from Bayesian statistics.\n\nNext up, revisit the scenario where someone is asking you to compare the probabilities of two hypotheses, but you didn’t really pay attention to understand what they mean. Suppose you then think about it more, and you discover that the first utterance meant “The first COVID-19 outbreak was a lab leak” and the second utterance meant “The first COVID-19 outbreak was a zoonotic spillover”. How should you update on this evidence? Intuitively, all we’ve learned is the meanings of the utterances, without learning anything about how COVID-19 actually started, so our posterior ratio should just be P(LL) / P(Zoo), which means our likelihood ratio would have to be the same (given that our prior ratio is 1).\n\nHere’s the proof: for utterance 1, the relevant likelihood term is P(U1 means LL and U2 means Zoo | U1). Using the definition of conditional probability, this is P(U1, U1 means LL, U2 means Zoo) / P(U1). Using the chain rule, we can manipulate this into P(U1 | U1 means LL, U2 means Zoo) P(U1 means LL, U2 means Zoo) / P(U1). By assumption 6, P(U1 | U1 means LL, U2 means Zoo) = P(U1 | U1 means LL), which by assumption 3 is equal to P(LL). Putting that all together, P(U1 means LL and U2 means Zoo | U1) = P(LL) P(U1 means LL, U2 means Zoo) / P(U1). Similarly, for utterance 2, the relevant likelihood term is P(U1 means LL and U2 means Zoo | U2), which is equal to P(Zoo) P(U1 means LL, U2 means Zoo) / P(U2). Since P(U1) = P(U2), the likelihood ratio is therefore P(U1 means LL and U2 means Zoo | U1) / P(U1 means LL and U2 means Zoo | U2) = P(LL) / P(Zoo).\n\nWhat’s the significance of this? It means that we can recast the P(LL) / P(Zoo) term as a likelihood ratio, rather than a prior ratio.\n\nFinally, we should check that this different formalism doesn’t change how we update on evidence. That is, suppose we further observe evidence E. We should multiply our old posterior ratio by P(E | U1, U1 means LL, U2 means Zoo) / P(E | U2, U1 means LL, U2 means Zoo). Intuitively, this should just be the likelihood ratio P(E | LL) / P(E | Zoo) because we’re just doing normal Bayesian inference, and understanding it in terms of updating on the meanings of utterances shouldn’t change anything. Formally, we can look at the numerator, P(E | U1, U1 means LL, U2 means Zoo), and by assumption 4, write it as P(E | LL, U1 means LL, U2 means Zoo). By assumption 5, this is just P(E | LL). Similarly, P(E | U2, U1 means LL, U2 means Zoo) = P(E | Zoo). Therefore, our new likelihood ratio P(E | U1, U1 means LL, U2 means Zoo) / P(E | U2, U2 means LL, U2 means Zoo) = P(E | LL) / P(E | Zoo). Therefore, we’re updating the same as we used to be. You can also check that this remains true if we get further “mundane” evidence.\n\nWhat does this mean?\n--------------------\n\nBasically, this shows that every term in a standard Bayesian inference, including the prior ratio, can be re-cast as a likelihood term in a setting where you start off unsure about what words mean, and have a flat prior over which set of words is true. How should we interpret that fact?\n\nFirstly, I think that there’s some kind of interesting mapping to the intuitive experience of doing Bayesian inference in real-world settings. A lot of the initial task of determining what the prior should be involves understanding what the hypotheses actually mean in a probabilistic sense—what kinds of things would have to happen for COVID-19 to have started via a lab leak, and what would that say about the world? That said, it’s possible to over-emphasize these similarities. In the toy setting I sketch, you should be asking yourself “If ‘COVID-19 was a lab leak’ was true, what’s the chance that it would have these implications?”, which doesn’t quite match to the kinds of thinking I’d tend to do.\n\nSecondly, it points to how strange likelihood ratios can be, by turning likelihood ratios into priors. There are other reasons to think that likelihoods are funny things: if the hypothesis in question is false, the likelihood is asking about how likely we would be to see some evidence in a world that doesn’t exist, which is a question that may be hard to get data on. There are therefore serious challenges with thinking of likelihood ratios as more “objective” or “scientific” than priors. As Gelman and Robert [say](http://www.stat.columbia.edu/~gelman/research/published/feller8.pdf), “It is perhaps merely an accident of history that skeptics and subjectivists alike strain on the gnat of the prior distribution while swallowing the camel that is the likelihood”.\n\nFinally, it points to an interesting extension. In some cases, the meaning of various utterances might tell you something relevant about the world in question. For instance, suppose some utterance is a computer program, and its “meaning” is what it evaluates to. Learning this might serve as evidence about what other computer programs evaluate to (e.g. those computer programs that use your ‘utterance’ as a subroutine), meaning that one could not apply Bayesian statistics quite so simply in this setting.\n\nA challenge\n-----------\n\nThis construction was inspired by noting the similarity between the calculation of the prior term and the likelihood term in Bayes’ formula. The way it highlighted that similarity was by turning the prior term into a likelihood. But is there some way of re-casting the problem so that the likelihood term becomes a prior, and the prior term becomes a likelihood?\n\n* * *\n\n1.  Compare priors and posteriors, which are both about the probability of the hypotheses in question, and are therefore more similar—you can use a posterior as a new prior when facing further evidence. [↩](#fnref:1)\n    \n2.  This can actually be relaxed without changing our results: we can instead suppose that you’re not sure which way the speaker is carving up “hypotheses”, but that once they pick such a way, the two hypotheses they state will be mutually exclusive. [↩](#fnref:2)",
      "plaintextDescription": "Epistemic status: party trick\n\n\nWhy remove the prior\nOne famed feature of Bayesian inference is that it involves prior probability distributions. Given an exhaustive collection of mutually exclusive ways the world could be (hereafter called ‘hypotheses’), one starts with a sense of how likely the world is to be described by each hypothesis, in the absence of any contingent relevant evidence. One then combines this prior with a likelihood distribution, which for each hypothesis gives the probability that one would see any particular set of evidence, to get a posterior distribution of how likely each hypothesis is to be true given observed evidence. The prior and the likelihood seem pretty different: the prior is looking at the probability of the hypotheses in question, whereas the likelihood is looking at the probability of the evidence (assuming the hypothesis is true).1\n\nCritics of Bayesian inference sometimes denounce the reliance on priors for being subjective or unscientific. Indeed, they are by design meant to describe what one would think without any relevant (contingent) data. One might therefore be tempted to describe a form of Bayesian inference where no special role is played by the prior distribution, as distinct from the likelihood.\n\nAnother motivation comes from doing Bayesian calculations by hand. In real-world cases, such as trying to infer whether the first COVID-19 outbreak spread from a laboratory or human contact with infected animals, the kind of thinking one does to determine a prior probability distribution is very similar to the kind of thinking one does to determine likelihoods: in both cases, one has some sort of generative model in mind—that is, some sort of probabilistic process of generating worlds—and one is trying to figure out how often worlds produced by this generative model have various properties. This might make one wonder if one could unify the prior and the likelihood.\n\n\nHow to remove the prior (by turning it into a likelihood)\n",
      "wordCount": 2262
    },
    "tags": [
      {
        "_id": "LhX3F2SvGDarZCuh6",
        "name": "Bayes' Theorem",
        "slug": "bayes-theorem"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "JQEeNgezmPBz6n2je",
    "title": "AXRP Episode 28 - Suing Labs for AI Risk with Gabriel Weil",
    "slug": "axrp-episode-28-suing-labs-for-ai-risk-with-gabriel-weil",
    "url": null,
    "baseScore": 12,
    "voteCount": 3,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-04-17T21:42:46.992Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/hdQDZo7jGBY)\n\nHow should the law govern AI? Those concerned about existential risks often push either for bans or for regulations meant to ensure that AI is developed safely - but another approach is possible. In this episode, Gabriel Weil talks about his proposal to modify tort law to enable people to sue AI companies for disasters that are “nearly catastrophic”.\n\nTopics we discuss:\n\n*   [The basic idea](#basic-idea)\n*   [Tort law vs regulation](#tort-law-vs-regulation)\n*   [Weil’s proposal vs Hanson’s proposal](#weil-vs-hanson)\n*   [Tort law vs Pigouvian taxation](#tort-law-vs-pigouvian-taxation)\n*   [Does disagreement on AI risk make this proposal ineffective?](#disagreement)\n*   [Warning shots - their prevalence and character](#warning-shots)\n*   [Feasibility of big changes to liability law](#feasibility)\n*   [Interactions with other areas of law](#interactions)\n*   [How Gabriel encountered the AI x-risk field](#how-encountered-x-risk)\n*   [AI x-risk and the legal field](#ai-x-risk-and-legal-field)\n*   [Technical research to help with this proposal](#technical)\n*   [Decisions this proposal could influence](#decisions)\n*   [Following Gabriel’s research](#following-gabriels-research)\n\n**Daniel Filan:** Hello, everybody. In this episode, I’ll be speaking with Gabriel Weil. Gabriel is an assistant professor at Touro Law. His research primarily focuses on climate change law and policy. He’s recently written about [using tort law to address catastrophic risks from AI](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4694006), which we’ll talk about in this episode. For links to what we’re discussing, you can check the description of the episode and you can read the transcript at axrp.net. Well, welcome to AXRP.\n\n**Gabriel Weil:** Thanks for having me.\n\nThe basic idea\n--------------\n\n**Daniel Filan:** Sure. So I guess we’re going to talk about this paper you have. It’s called [Tort Law as a Tool for Mitigating Catastrophic Risk from Artificial Intelligence](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4694006). And I think most of my audience won’t be from a legal background and they might stumble over those first two words. What is tort law?\n\n**Gabriel Weil:** So torts is the law of basically any time you do a civil wrong that isn’t a breach of contract. So “breach of contract” is you promised someone in some legally enforceable way that you would do something and you didn’t do that. You breached the contract, they sue you. Basically anything else that you’re suing a private party for, you’re not suing the government - in a civil lawsuit, that is going to be a tort.\n\nSo most commonly you think of things like: you get in a car accident and you sue the other person - that’s a tort. There’s also intentional torts like battery. So if I punch you in the face, you can sue me for the harm I did to you. If I trespass on your property or I damage personal property, like I break your car window or I key your car, those are trespassory torts - that’s called “trespass to chattels”. Products liability is torts. Medical malpractice is torts. So that’s the broad field of law we’re talking about.\n\n**Daniel Filan:** I guess you have some idea for how we’re going to use tort law to mitigate catastrophic risk from artificial intelligence. In a nutshell, what’s the idea? How do we do it?\n\n**Gabriel Weil:** So in a nutshell, the idea is that training and deploying these advanced AI systems is a risky activity. It creates risks of what are called “externalities” in economics jargon: harms to people that are not internalized to any economic transaction. They’re harms to third parties. OpenAI makes a product, someone buys that product and uses it, but it risks harming someone besides those two parties. Therefore, the risk is not reflected in the price of the product.\n\nAnd so in principle, holding whoever is responsible for the harm, whoever caused the harm, liable for that, making them pay for the harm they cause, should result in them taking enough precaution to optimally balance the risk and reward. So in the same way in your own life, when you’re deciding whether to drive to work or walk to work or bike or whatever, there’s various costs and benefits, you’re weighing those.\n\nSome of them have financial costs, health benefits, time costs, and you’re able to weigh those risks for yourself. We generally trust that process to work pretty well because you fully internalize the benefits and costs there. You might make some mistakes sometimes, but your incentives are pretty well-aligned. But if there’s some sort of externality to what you’re doing - you driving to work creates pollution - most of the harms of that are felt by other people, then we might need some legal response to account for that.\n\nAnd so tort law works well for that when we have a concentrated harm. So for pollution, it wouldn’t work so well necessarily for everyone that’s harmed by your pollution to sue you. You may want some other policy tool, like a tax on gasoline or on pollution, to align your incentives.\n\nBut if you hit someone with your car, and if there’s a risk that when you take that trip you’re going to hit someone, making you pay for the harm you caused works pretty well to align your incentives, to make sure that you don’t take the trip if it’s not necessary and you drive with appropriate precaution: you don’t text while you’re driving, you don’t speed too much. Now, our existing tort system might not do that perfectly, but in theory, the prospect of liability should make you exercise the right amount of precaution. So that’s basically what I want companies that are building and deploying AI system to do: to have the incentive to exercise the right amount of precaution. I don’t think our current tort system necessarily does that, but I think it can with some tweaks.\n\n**Daniel Filan:** Gotcha. So to summarize, it sounds like basically the fear is: if you’re an AI company, there’s some risk you destroy the world, and that’s much worse for the world than it is for you. So you’re just going to be more relaxed about that risk than you would be if you really felt the responsibility, if you really were internalizing just how bad it would be if the world got destroyed (or some other catastrophic risk).\n\n**Gabriel Weil:** So in the limit, that’s definitely part of what I’m worried about. But I think even for less catastrophic scenarios, there’s harms that wouldn’t necessarily be internalized by these companies. But yes, in the limit, I think it’s definitely true. Now, obviously if you destroy the world, you kill yourself too, so that’s bad for you. But it’s not as bad, you don’t feel that as much as killing eight billion people and all future civilization. I think you want the law to make them feel that.\n\n**Daniel Filan:** Right. So it seems like the obvious difficulty here is that: suppose you literally destroyed the world. It’s too late to sue. This is the first-pass difficulty with some sort of liability scheme. How do we deal with that?\n\n**Gabriel Weil:** So yes, it is true that there are certain class of harms, of which existential harms are a subset, that are what I would call ‘practically non-compensable’. So you can’t actually bring a lawsuit or you can’t collect a judgment for it. I think that includes extinction risks. It also includes risks of harms sufficiently disruptive that the legal system would no longer be functioning.\n\nAnd it also includes risks short of that, that would just be financially uninsurable. So if you kill a million people, the damage award is going to be so large that you’re not going to be able to pay that out, no plausible insurance policy is going to cover that, and so it would put your company into bankruptcy and most of that damage is not going to be recoverable.\n\nThe normal type of damages that we use in tort lawsuits, which are called compensatory damages - damages designed to make the plaintiff whole, to make them, in theory, indifferent between having suffered the injury and getting this pile of money or having it never happen - those aren’t going to work to get at those risks. We need some other tool. And so the tool that I propose are what are called “punitive damages”.\n\nSo these would be damages over and above the harm suffered by the plaintiff at a particular case, that we’re assigning because there was a risk that things went much worse. The system does some practically compensable harm, but it does it in a way that it looks like it could have gone a lot worse. Say there was a one in 10,000 chance of killing a million people and a one in a million chance of causing human extinction.\n\nThen you would want to figure out what the expected value of that harm is, the probability of the harm times the magnitude and say, “Well, we’re going to pull forward that counterfactual liability from the risk that you took but wasn’t realized, and allocate it across the practically compensable cases.”\n\n**Daniel Filan:** Gotcha. So the way I understand the policy is roughly: we’re going to say that when you have some AI harm, if it’s the kind of harm that’s really closely related to these sorts of harms that I couldn’t sue you for - like extinction, like some sort of catastrophe that would super disrupt the legal system, or just a judgment that’s too big to collect on - then basically the punitive damages are going to say, “Okay, what types of harms \\[is\\] that related to? How similar is this to those harms? How much would dealing with the problem that you actually caused fix those harms?” And basically the punitive damages are going to reflect those factors roughly.\n\n**Gabriel Weil:** Right. I think we care about two things: how much uninsurable risk, or practically non-compensable harm risk, was generated by deploying the specific system that you deployed in the way you did. And then we care about if you had a strong incentive to avoid this specific harm and you took costly measures to prevent it or reduce its likelihood or severity, to what extent would that tend to mitigate the uninsurable risks? And the more that’s true, the more punitive damage we want to load onto those types of cases.\n\n**Daniel Filan:** Gotcha. This is a bit of a tangent, but you mentioned that at some level of harm, you’re not going to have the money to pay the damages for this harm. Even getting an insurance policy, no insurance company would be able to pay out. How much harm is that? What are the limits of just making people buy liability insurance for this kind of thing?\n\n**Gabriel Weil:** So my understanding is that current insurance policies tend to top out around $10 billion. In the paper, I used a much higher threshold for insurable. I used a trillion. I think we would want to push the limits of what’s insurable to really find out, but I think that’s an open question that needs to be explored before this is fully ready to implement.\n\nI think you would want to start out by assuming the uninsurability threshold is on the lower end. And then if they can prove, “well, I can insure for more than that”, then you would say: okay, the expectation of harm or the possibility, the risk of harm is below that, \\[so\\] we handle that with compensatory damages. Since you’ve shown you can take out a $100 billion insurance policy, then that’s going to be the cutoff. Risks of harms above $100 billion will go into the punitive damages calculation.\n\n**Daniel Filan:** Right. Wait, did you say $10 million or $10 billion?\n\n**Gabriel Weil:** $10 billion.\n\n**Daniel Filan:** I guess to ballpark that: in the US, the value of a statistical life that [regulatory](https://www.epa.gov/environmental-economics/mortality-risk-valuation#whatvalue) [agencies](https://www.transportation.gov/sites/dot.gov/files/docs/2016%20Revised%20Value%20of%20a%20Statistical%20Life%20Guidance.pdf) use is $10 million per life or something. So $10 billion \\[means\\] you can get insured for the legal liability for killing 1,000 people, but not 10,000.\n\n**Gabriel Weil:** That’s the right order of magnitude. As I explained in the paper, the numbers that are used by regulatory agencies are a little bit different than what’s used in tort judgments. \\[The numbers in tort judgments\\] tend to be a little bit lower, in large part because the way mortality harms are valued doesn’t actually value the value of the person’s life to them. \\[It’s a\\] weird quirk of tort law that I think should be fixed for other reasons and is more important in this context, but that’s a tangent.\n\n**Daniel Filan:** So this is the scheme. In the paper, you focus on how it could be implemented in a US context. If people are listening from other countries or other legal systems, how widely applicable is this sort of change?\n\n**Gabriel Weil:** There’s two ways of answering that. The beginning of the paper, that lays out what the ideal regime looks like - I think that’s true regardless of what status quo legal system you’re working from. In terms of what doctrinal or what legal levers you’d have to pull to get to that outcome or to get to that regime, that’s going to vary across countries. I would say in any common law country, so that’s basically English-speaking countries, tort law is going to be broadly similar.\n\nThere will be some detail difference; particularly products liability regimes, since those came later, are going to be different. But the basic structure of negligence law is going to be pretty similar across all common law countries. And so a lot of the same considerations are going to come into play. But I would invite scholars working in other legal systems to flesh out what this would look like and precisely what levers you’d have to pull in their system.\n\n**Daniel Filan:** Gotcha. So one somewhat basic question I have about your scheme is: suppose that Party or Company A makes this really big super smart foundation model. Party B fine-tunes it, makes it really good at a certain task. Party C then takes that fine-tuned model and sells it or deploys it or something. Suppose the model that C deploys causes harm, who are we imagining suing for that?\n\n**Gabriel Weil:** Who’s suing is easy: it’s whoever is harmed, right? I think you’re asking who’s sued.\n\n**Daniel Filan:** Yeah.\n\n**Gabriel Weil:** I think you could potentially sue all of them. If you’re talking about under my preferred regime where we’re treating training and deploying these models as abnormally dangerous, you would have to figure out at what step and which actors undertook these abnormally dangerous activities. If all of them did, then strict liability would apply to all of them.\n\nIf the way the application of the standard works \\[means\\] you would say, “Well, this step in the chain wasn’t abnormally dangerous,” then that would be assessed under negligence principles and you’d have to be saying, “Well, did they breach some duty of care?” But in principle, there could be what’s called joint and several liability, where the plaintiff can sue all of them or pick which one they want to sue.\n\nYou can never collect more than the full damages, but you can pick your defendant. Now, different states have different liability regimes for that purpose. [Most states](https://www.whiteandwilliams.com/assets/htmldocuments/Subro%20Charts%20Updated%205_10_16/JOINT%20AND%20SEVERAL%20LIABILITY%20-%20REV.%201-10-20.PDF) file what’s called “joint and several liability”, which means you can collect the whole judgment from any one of them, and then they can sue their joint-tortfeasors, they’re called, for what’s called “contribution”, basically for their fault-apportioned share.\n\nThen there’s other states that use what’s called “several liability with apportionment”, where you can only sue each defendant for their fault-apportioned share of the liability. And fault apportionment is just this idea that if you have multiple defendants, or multiple tortfeasors, that are causally responsible for the same injury, you do some allocation based on how faulty their conduct was, or in the context of strict liability, how liable they are. That concept doesn’t apply as well in a strict liability context, but you would want to do a similar analysis.\n\n**Daniel Filan:** But broadly the sense is, the court would just decide who is actually responsible for the really dangerous stuff that the AI ended up doing and they would be liable for the thing.\n\n**Gabriel Weil:** So I want to make a distinction there. When you say courts, I assume you mostly mean judges. There’s different roles that judges and juries have in this process. Judges resolve questions of law, juries resolve questions of fact, is the high-level distinction. And so \\[in the case of\\] a negligent system, breach of duty is a question of fact, but what the duty is is a question of law. If we were talking about the threshold question of “is this activity abnormally dangerous such that strict liability should apply?”, that’s a question of law that a judge would resolve.\n\n**Daniel Filan:** Okay, I think that makes sense. The final framing question I want to ask about this is: it seems like a lot of this would be implemented on the jury side. A judge would tell a jury, “This is roughly how you should figure out the damages and go and deliberate and tell me what you decided.” Is that right?\n\n**Gabriel Weil:** So certainly the actual damages calculations would be fact questions that would be decided by a jury in a typical case. The way judges review those is if they conclude no reasonable jury could have reached a given result then they can overturn it, but juries are supposed to have pretty wide discretion. Now, whether punitive damages would be available at all is a legal question. It’s a legal question resolved by judges.\n\nSo under current law, it requires malice or recklessness as a threshold requirement for punitive damages to be applied. There’s also various limits under the due process clause of the Constitution that limit the ratio of compensatory damages to punitive damages. Those questions would be resolved by judges. And so juries would be operating within the confines of those legal rules.\n\n**Daniel Filan:** Gotcha. My question is: one could imagine that juries are really good at assessing these kinds of things. They’re just very good calculators. They really figure it out. One could also imagine that juries just roughly do what they feel is right and maybe they’re forced to be in a certain range by a judge, but maybe they’re kind of random, or maybe they stick it to the bigger party or something like that.\n\nAnd in the second world, it seems like it’s just going to be hard to implement this kind of scheme, because maybe we just can’t tell juries what to do. So I guess my question is: how good are juries at implementing formulas and stuff that judges tell them to?\n\n**Gabriel Weil:** So most damages calculations are pretty black box. What’s the pain and suffering? For certain things we can assess \\[it\\] better, e.g. lost wages are easier to quantify. Pain and suffering is inherently pretty hard to quantify, and that’s regularly part of damages awards. We just deal with the fact that there’s going to be experts to testify and then juries come up with a number. In this context, I think you would have dueling experts where different experts are testifying and saying, “Well, this was the risk.”\n\nObviously there is deep disagreement in people who think about AI alignment, AI safety about how likely these catastrophic outcomes are. Now, hopefully the context in which a system failed in a certain way where it looks like it could have gone a lot worse will be probative on the question. We’re not trying to ask the question of “what’s the global probability that human extinction will be caused by an AI system?” We’re trying to ask “what’s the probability that the people who train and deployed \\[this system\\], when they made those decisions, what should they have thought the risk was?” And we can update on the fact that it failed in this particular way. We don’t want to over-update on that, because in some sense, ex post the risk of a worse harm is zero, and they didn’t know it would fail in this particular way. But the fact that it failed in the way it did can reveal some things about what they knew or should have known at the time they deployed it.\n\nSo I think, yeah, juries aren’t going to do this perfectly, but I also don’t think they need to. What really matters here is the expectation that if you take a risk, you’re going to have to pay for the expected harm arising from that risk. And so as long as juries aren’t systematically biased in one direction or the other, as long as they’re very roughly noisily coming up with a number that tracks the risk, that’s going to do what you need to do in terms of generating the expectations of liability. So that’s a failure mode I’m less worried about than others.\n\n**Daniel Filan:** So just to give color to this, it sounds like maybe what would happen is there’s some sort of trial based on some harm resulting from something kind of like AI misalignment. And then the defendant’s legal team brings up some expert to say, “Oh, this wasn’t that bad, and it’s not that related to really scary harms,” and the other expert says, “No, it’s really bad.” Somehow the juries are picking between different people suggesting different things about what the damages can be that are guiding their assessments. Is that a reasonable thing to imagine?\n\n**Gabriel Weil:** Yeah, it’s how I would expect the damages phase of a trial like this to go.\n\nTort law vs regulation\n----------------------\n\n**Daniel Filan:** Okay, great. A question I want to ask is: if I think of most AI governance work, I think that it operates in a framework of saying, our plan has two parts. Firstly, there’s going to be some process where the government or a bunch of smart scientists figure out what AI can do, how scary it might be, and make that really legible to regulators.\n\nAnd then secondly, there’s going to be some kind of law or some kind of regulatory body that says that if you make a really big scary AI, we’re going to set out some rules for you to follow and you just have to follow them. And we’re going to design the rules so that if you follow those rules, the AI should hopefully be safe. Your proposal feels like kind of a different flavor than these sorts of proposals. So I wonder how you think these kinds of schemes compare?\n\n**Gabriel Weil:** So to my mind, the key advantage of a liability framework is it doesn’t require that you and the government know what specific steps have to be taken to make your system safe. I don’t think we know that right now. Maybe we’ll get there at some point, but I don’t want to rely entirely on the government being able to specify a procedure that makes your AI system safe.\n\nWhat liability does is it shifts the onus to figuring that out onto the companies, the labs where most of this expertise resides. I think it’s going to be difficult for government to bring the kind of expertise in-house that gets them to where the leading labs are. Even the leading labs don’t really know how to build safe systems right now. So I want them to not only be throwing everything they have in terms of, once they’ve made a decision to deploy, making it safe, but if they’re not confident a system is safe, if they think deploying a particular system given the current state of knowledge creates a one in a million chance of human extinction, I want them to wait six months until better interpretability tools come around (or whatever the idea is; I’m not a technical AI safety researcher). But I want them to be thinking, “I need to be as cautious as I would be if I owned the world, basically, and if destroying the world was going to destroy all that value for me”.\n\nThat’s not to say that there’s no role for what I would call prescriptive regulation of the kind you were describing, but I think what’s really important in that context is that those prescriptive rules don’t preempt or displace the tort liability. Sometimes judges interpret regulatory schemes as having a preemptive effect, either because they’re viewed as conflicting with tort law or the regulatory scheme is viewed as having occupied the field, and so impliedly preempting the tort regime.\n\nI think that would be a really bad outcome. You can avoid that pretty easily in any legislation that’s enabling a new regulatory program by including what’s called a savings clause that expressly disavows any preemptive effect. And once that applies, we can talk about “are there specific measures that would buy some safety if we require them?” I don’t think those are necessarily bad ideas, I think some are more valuable than others. But I don’t think we want to rely entirely on that.\n\n**Daniel Filan:** To me, it seems like the distinction is: these sorts of rulemaking schemes, the rules, the stuff you have to follow, it comes very early in time, maybe before you know as much what’s happening. Whereas if you can do it right, something like a tort law scheme, it brings in the legal force at a time where there’s some medium-range problem with your AI that’s not so bad that your legal system doesn’t function, but bad enough that you know more what happened.\n\nIn my mind, it seems like the advantage is that it’s a more informed place to make these decisions, such that AI companies optimizing for that are basically going to be doing better things than if they’re just optimizing for following set rules. Does that sound right?\n\n**Gabriel Weil:** A rule that you set through regulatory policy may not pass a cost-benefit test. You might have some unnecessary rules, and there also might just be things you didn’t think of to require or you decided not to require that would’ve bought you a lot of safety. So if you get the rules perfect, if you require everything to pass a cost-benefit test and you don’t require anything that doesn’t, then maybe a regulatory regime is sufficient and better. But I don’t have confidence in this domain that regulators are likely to approach that.\n\n**Daniel Filan:** I guess there’s a difficulty where on some level, you’re hoping that developers are able to figure out what the right cost-benefit is for themselves to do, but also there are obvious problems with them setting regulatory policy. I guess I think of it as just an interesting way to solve a knowledge problem.\n\n**Gabriel Weil:** It’s also worth pointing out that some forms of prescriptive regulation work really well with liability. So in particular, there’s proposals for these AI model licensing regimes, and I think that would pair really well with a liability insurance requirement system. So instead of the decision being binary, yes or no, do you get a license or not, what the regulator would do is decide, here’s how much liability coverage you need in order to deploy this system. Here’s the worst amount of harm we think you could do. And then you could deploy it if you can convince an insurance company to write you a policy you can afford. And that’s going to depend on, maybe there would be some set of alignment evaluations or safety evaluations that they rely on in order to do that underwriting process.\n\nI think you want the decision about whether a system is deployed to depend on whether its expected benefits for society are more than its expected costs. And if they have to buy insurance against the worst-case outcomes and convince an insurance company they can afford it, that’s a pretty good proxy for that. Whereas I think I’m less trusting of a binary government decision, “are we going to license this model or not?”\n\n**Daniel Filan:** I actually want to talk about synergies, because I think there’s also a synergy in the fork of the plan where we’re going to have [NIST](https://www.nist.gov/artificial-intelligence-safety-institute) figure out what kinds of AI designs are more or less safe, or figure out ways of evaluating AIs for danger. It seems like this potentially has a synergy with the tort law plan.\n\n**Gabriel Weil:** I guess there’s two different ways that could work. One is if we’re still in negligence world, if my ideas don’t take the world by storm and we don’t have strict liability on abnormally dangerous activities theory, then NIST promulgating these standards… if you’re not following those standards, that’s at least going to be evidence of negligence.\n\nNow, there’s a doctrine called “negligence per se”, that if you had actual regulatory requirements and you don’t meet those, then that would automatically be negligence. But if they’re just guidelines that NIST is issuing, that would be indication that you’re not exercising reasonable care, but it wouldn’t be dispositive.\n\n**Daniel Filan:** I was imagining also if we do adopt your proposal, it seems like this kind of thing might be informative of how risky that activity actually was.\n\n**Gabriel Weil:** So how much uninsurable risk you took when you deployed it if you didn’t follow the standard, is that the idea?\n\n**Daniel Filan:** Maybe it’s not a standard, but maybe it’s just some kind of measurements… There’s some rule that you have to submit models to this organization and this model will get a blaring red light, and then cause some sort of problem, and that’s even more evidence that there was something pretty dangerous about it.\n\n**Gabriel Weil:** Yeah, so I definitely think there’s scope for more technical work in terms of evaluations of these models, both in deciding whether to deploy them and saying how much insurance you have to take out and for doing these damages calculations. If harm has happened, can we try to use post hoc evaluations to try to figure out, “well, could it have gone a lot worse? What would that have looked like?”\n\nWeil’s proposal vs Hanson’s proposal\n------------------------------------\n\n**Daniel Filan:** Sure. I guess the next thing I want to compare to is… So in your paper you cite this blog post by [Robin Hanson](https://en.wikipedia.org/wiki/Robin_Hanson) about [“foom liability”](https://www.overcomingbias.com/p/foom-liability), which to my knowledge is the only previous time people have talked about a scheme roughly like this. And he imagines a proposal sort of similar to yours, except there’s a fixed formula where they say, “okay, you’re going to assess punitive damages and the punitive damages are going to be based on, on this checklist of ways AI could kill everyone, how many items did you check off?” And the more items of that list \\[you check off\\], the worse the punitive damages are by a set formula. So \\[in\\] your proposal, I take the main difference to be that instead of being this strict formula, people just have to figure out: \\[if you tried\\] to prevent this certain harm that actually occurred, how much would that prevent really bad catastrophes that could have happened? So I’m wondering what do you think about \\[the\\] pros and cons of each one?\n\n**Gabriel Weil:** I think I talked to Robin about this. His motivation for having that formula was to limit the discretion of judges and juries. I see that as not particularly viable in this context since his formula at least strikes me as fairly arbitrary. It’s taking it to the power of the number of these different criteria that are checked off. I think a lot of those criteria are not actually binary, so it’s unclear how you would implement it in cases where it’s “sorta kinda self-improving” or something. So I think that’s an issue.\n\nI think weighting all these factors equally doesn’t seem super persuasive to me, but I do see the value in having a formula. That said, I provide a formula for my formulation of punitive damages. Now a lot of the variables in that formula are going to be difficult to estimate, so that’s a real challenge, but I think the advantage of it is it’s tracking the thing we ultimately care about: how much harm did you risk and how elastic is that with the harm that we actually realized here?\n\nAnd so I think I would like to see a lot more work to put meat on the bones of how to estimate the parameters in that formula. But in my mind, you should be aiming at the target and doing as well as you can as opposed to… I think at first blush, it looks straightforward to apply Hanson’s formula, \\[but\\] when you really unpack it, I think there’s still going to be a lot of discretion there. And so I think maybe it limits discretion a little bit, but not as much as you’d like. It’s loosely correlated with the thing we care about, but it’s not going to reliably track it in the way that my preferred approach \\[does\\]. Does that make sense?\n\n**Daniel Filan:** That makes sense. Suppose we don’t use Hanson’s formula in particular, but suppose what we do is, the world just spends a year, we look at your formula and then we say: okay, what’s something kind of like the Hanson formula that really would approximate what your formula tells us to do? But we’re going to try and have something that you can really nail down. To the greatest extent possible, we’re going to have something where there’s very little discretion on judges and juries so they can apply it sort of automatically. And we’re going to lose something along the way; it’s going to be a bit of a bad approximation, but hopefully it’s going to be really predictable. We’re going to lose some value of aiming for the ideally optimal thing, but we’re going to gain some value of predictability. And I’m wondering: how do you see that trade-off? How should we think about it?\n\n**Gabriel Weil:** So I think that would be really valuable. There’s a question of how that will be implemented. So one thing you could say is: someone comes up with that formula, you have your expert testify about it. Juries can incorporate that, right? It’s the same way that we incorporate any sort of scientific expertise in that domain. I think that is the most likely pathway if this is going to happen in a common-law, a judge-created way. I think it’s unlikely that judges are going to create a rule of law that say juries have to follow the specific formula that somebody came up with.\n\nOn the other hand, if this is done via legislation, certainly legislators, if they want to, can hard-code that formula into the statute and then juries have to follow it. So if you have a formula that you think is really good and unlikely to be improved upon, or if you think that, if we accomplish something better, we can amend the legislation. If it’s good enough, then I could see compelling judges and juries to follow it. It would just depend on how good you think the formula is and how easy it is to estimate the parameters in it.\n\nSo if you have a really good formula that, if you know the parameters… I think my formula is totally determined if you know the parameters. The problem is estimating those is really hard. If you have one that has easily-estimable parameters and you’re just saying, “jury, you have this narrow task of coming up with good estimates of these parameters, and then that’s all we’re going to ask you”. And then mechanically that will produce a damages award. I think that’ll be great if you can do it. I don’t think technically we’re there right now.\n\n**Daniel Filan:** Yeah. I guess also, one difficulty of this is: this seems sort of similar to formulas that get used in criminal law. Sometimes legislatures want to say, “okay, we’re going to have some mandatory minimums” or “we’re going to just have some rule and we’re going to ask judges to basically mechanically apply the rule”. And I get the sense that the legal profession kind of dislikes this, or judges kind of dislike this.\n\nSo firstly, I’m wondering if you think I’m right? And secondly, to what extent does that suggest that we should be shy of implementing a rigid formula here?\n\n**Gabriel Weil:** So I think mandatory minimums in particular are fairly crude. And there’s this general trade-off in law, which you may call “rule standards”, “discretion versus rules”. There’s this idea that the more discretion you give judges in individual cases, the more you’re going to be able to accommodate details of cases that might not be captured by an over-broad rule.\n\nOn the other hand, you’re going to have a lot more noise and potential for bias if you let judges and juries have more discretion. And so there’s this basic trade-off. I think what’s new in this context is there’s a competence issue. It sounds like you don’t totally trust juries to be able to evaluate these questions, and so you want to make their job a little easier. I think we do have a way of dealing with that - different people have different ways of judging how well it works - of letting juries - lay juries, non-expert juries - hear from experts and then adjudicate the credibility of those experts and then come to a determination.\n\nBut I think again, if we had a formula that was good enough… I think you would probably want something better than just “if you commit X felony, you get a minimum of 10 years”. I don’t think something of that level of simplicity is going to work for estimating the uninsurable risk arising from an AI system. I don’t know what that would look like. But I think if you had something sufficiently sophisticated where the parameters were easier for the jury to estimate… Again, I don’t have a strong sense of what that would look like, but I think that could be really useful.\n\nTort law vs Pigouvian taxation\n------------------------------\n\n**Daniel Filan:** Okay, fair enough. So another kind of proposal I want to compare against is: I think you mentioned very briefly early on something like [Pigouvian taxation](https://en.wikipedia.org/wiki/Pigouvian_tax), where we say that doing this kind of activity is broadly dangerous, and we’re just going to say whenever you make a model that’s X big, or maybe a model that trips up X number of dangerous capabilities or something, that’s just inherently risky, and therefore you have to pay a certain fine or a certain tax regardless of what happens. So similar to a carbon taxation scheme. And these kinds of schemes are often considered pretty desirable in the settings where there are pretty broad-based harms that could occur. So I’m wondering what you think about schemes like that.\n\n**Gabriel Weil:** So I’m a big fan of Pigouvian taxes generally. In my earlier life I was a carbon tax advocate. [A](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4490602) [lot](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4490594) [of](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3381186) [my](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3537823) [work](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3788661) is on climate change law and policy. I think that there’s two big differences between the climate context and the AI risk context. So one is if you are harmed by some climate outcome, it would be really hard to come up with how you can bring a lawsuit, because everyone in the world contributed to that. To say that there’s a ‘but for’ cause of any particular ton of carbon being emitted, that that caused your injury, that’s going to be a total mess and you’d basically need to sue the whole world. That’s one problem. So that’s the thing that makes climate change harder to use this liability tool for.\n\nConversely, it’s really easy in the climate context to say, “okay, we know what the unit of generating risk or harm is. It’s a ton of CO₂ equivalent”. And so we can say: we might disagree exactly about how much risk or how much harm you generate by emitting a ton \\[of CO₂\\], there’s different estimates of the social cost of carbon, but we can measure how much you did of that. We can come up with some tax and apply it.\n\nI think both of those are flipped when we talk about the AI context. So AI systems are likely to harm specific people and more importantly, it’ll be specific systems that harm them. So it’s not like “the existence of AI is what harmed me”: some specific system was deployed and that harmed me. That’s who I know how to go sue.\n\nAll tons of CO₂ emitted to the atmosphere do the same amount of damage. Now, maybe a marginal ton at some point is worse than others, but two different people emitting them at the same time, me driving my car and you driving yours, are doing just the same damage.\n\nAnd that’s not true reliably for building an AI system of a certain size. You want to differentiate between companies or labs that are taking more precautions, are doing more alignment research, taking more steps to make their system safer. We don’t want to just tax AI in general, but particularly we want to tax misalignment. So one framing that I really like is: people worry about an alignment tax, that it’s costlier both financially and in terms of time and other resources to build aligned systems. And so one thing you can think about AI liability doing is creating a “misalignment tax”, and hopefully that’s bigger than the alignment tax.\n\nIf we could perfectly assess at a time a model is deployed how risky it is, then maybe that would work well, but then if you could do that, then you could just have some binary decision about whether you’re allowed to deploy. Maybe you might still want a tax because there’s uncertainty about what the benefits of the system are, but I think we’re not in that epistemic position. We don’t have the ability to assess ex ante how risky the system is. Once it’s done harm in particular ways, I think we’ll have more visibility into that. And so that’s why I think a liability regime works better in this context.\n\nDoes disagreement on AI risk make this proposal ineffective?\n------------------------------------------------------------\n\n**Daniel Filan:** Yeah, that makes sense. I guess a final thing I want to ask is: it seems like this proposal is really targeting externalities from AI research. It’s imagining a world where people who run AI companies, they basically know what’s up. They basically know how risky systems are and what they would have to do to make them less risky. And the reason they don’t is that they’re inadequately incentivized to, because the world ending is only so bad for them - or really bad catastrophes, they’re only so bad for the AI company, but they’re much worse for the world.\n\nAnd I think it’s not obvious to me if this is the right picture to have. We see pretty different assessments of “what are the chances that AI could cause really serious harm?” The world ending, the really serious harms that x-risk people talk about, they’re not fully internalized, but they’re quite bad for the companies involved.\n\n**Gabriel Weil:** Orders of magnitude less bad for them than for the world. So if you have a one in a million chance of destroying the world, but a 50% chance of making a hundred billion dollars, the calculation for you looks a lot different than the calculation for the world. That’s a negative expected value bet for the world, but a positive expected value bet for you.\n\n**Daniel Filan:** I think that’s right. I think on views where the probability of doom is way higher than one in a million… I think a lot of people think that the probability of doom is higher than 10%.\n\n**Gabriel Weil:** From a specific system?\n\n**Daniel Filan:** Maybe not from a specific system, maybe from AI development in general. So I guess my question is: how do you think we should figure out… If I’m assessing this, how do I tell if most of the risk is externalities versus individual irrationality or stuff like that?\n\n**Gabriel Weil:** So I think that’s a fair critique that, say, “okay, maybe the people who buy these x-risk arguments, the people at (say) Anthropic or some of the people at OpenAI at least, at DeepMind, are going have even more incentive to be cautious. But \\[at\\] [Meta](https://ai.meta.com/), [Yann LeCun](https://en.wikipedia.org/wiki/Yann_LeCun) doesn’t believe in x-risk really, so he’s not going to be so worried about this”. And I think that’s true if you only have the liability part of my framework.\n\nIf you have the liability insurance requirements part of it, then you have to convince an insurance company, that’s going to be a much more cautious actor, that you’re not generating risk, because that’s going to introduce that more cautious decision maker into the loop and put a brake on the process.\n\nAnd so I think I’m less worried about insurance companies, \\[because\\] their whole job is to be cautious and to avoid writing insurance policies that are going to pay out more than they cost in expectation. I think that’s going to be an important framework for the actors; that the main problem is their assessment of the risk rather than their incentives to account for it.\n\n**Daniel Filan:** So I think this works for the problem of AI developers who have abnormally low estimates of the risk. So I guess I’m looking at a world where I feel like there’s a lot of disagreement about AI risk, and it seems like this kind of disagreement… On the one hand, it seems like it’s the motivation behind \\[having\\] some sort of tort law scheme, rather than “well, we just know what to do, so we’re going to make a law that says you have to do that”.\n\nBut it seems like it causes some problems, partly in that AI developers, or maybe even insurance companies that AI developers have to buy liability insurance from, they might not know what kinds of things to avoid. It also seems like it means that these sorts of pervasive disagreements are going to make it really hard for juries to assess how big should the punitive damages be. So one might worry that we just have so much disagreement that this kind of liability scheme can’t really help us. What do you think?\n\n**Gabriel Weil:** I think I want to distinguish two different objections there. There’s an objection that this disagreement makes it hard to implement the framework, or that this disagreement makes it so that if you could implement this framework, it wouldn’t buy us that much safety. I think the concern that it’s hard to implement, I think is true. I think a lot of technical work needs to be hashed out, needs to be done to implement this framework reliably. I think you can implement in a rough and ready way now and that would still buy you a lot of risk mitigation, but there’s a lot of refinement that could be done, a lot of knowledge that could be built, consensus that could be built, that would allow you to more reliably track what the risks that these companies are taking are. And that would make the framework more valuable.\n\nAnd the other point I want to make on that is that whatever you think the epistemic burdens of implementing this framework are, they are lower than \\[the burdens\\] for implementing prescriptive regulations. You not only have to know how big the risks are, you need to know what to do about them. And so I think if your concern is our poor epistemic position with regard to AI risk, I think that tends to favor liability relative to other approaches, not disfavor it.\n\nThen there’s the question of “is it going to influence behavior in the right way because people might have different beliefs?” So I made the point already about liability insurance and how that introduces more cautious actors. I think if what you’re ultimately saying is, “look, people building \\[in\\] these labs are still going to make mistakes. They might deploy a system that, based on everything anyone could know, looked like it was going to be safe and that wasn’t, and then we’re all dead. And who cares if in theory there should be liability for that?”\n\nAnd I think what I want to say is: short of an outright ban on building systems beyond a certain level or certain kinds of systems, I just think policy is not going to solve that particular scenario. What we want from policy is aligning the incentives of these companies with social welfare. Maybe we also want it to subsidize alignment research in various ways. But there is a sort of irreducible technical challenge here, \\[and\\] I think you’re asking too much of policy if you want it to solve all of that.\n\n**Daniel Filan:** Yeah, I guess if I think about the question, the case for regulation is most persuasive in a world where these AI labs, they don’t know what they’re doing, but I know what they should do. But in a world where we’re all in similar epistemic positions than maybe the tort law approach seems like it makes more sense.\n\n**Gabriel Weil:** Or if the labs know better than-\n\n**Daniel Filan:** Or if they know better, yeah.\n\n**Gabriel Weil:** Maybe you, Daniel Filan, know better than what the folks at OpenAI do. I don’t think Congress knows better.\n\n**Daniel Filan:** I don’t know about that \\[whether Daniel Filan knows better than what the folks at OpenAI do\\].\n\n**Gabriel Weil:** I think Congress is going to listen to a lot of experts, but I don’t know if you [watch](https://www.c-span.org/) what goes on in DC: the idea that they’re going to write legislation, that regulators are going to come up with something that reliably makes us safe… I’m just very skeptical. I think they could do some things that are helpful, but it’s not going to be anywhere near sufficient. I think some of the things they end up doing might be harmful. I think politics and regulatory policymaking is very messy. And so I think if you’re relying on that to make us absolutely safe, I want to pour some salt on that.\n\nAlso, even if your idea is the thing that I was throwing out there as the extreme position, let’s outright ban development of systems beyond a certain level - I think that even if you could make the domestic politics in the US work, which I don’t think you probably can, and even if you thought that was desirable, enforcing that globally is going to be extraordinarily difficult. Now you could apply some of that critique to liability too. \\[But\\] I think that’s a much easier lift.\n\nWarning shots - their prevalence and character\n----------------------------------------------\n\n**Daniel Filan:** Gotcha. I also want to ask: for this approach to work it seems like we need… In worlds where the risk of misaligned AI causing tons and tons of uninsurable damage is really high, we need there to be a bunch of intermediate warning shots where there are problems that are kind of like really bad AI causing untold amounts of harm, but they only caused a few million dollars worth of harm, so we can sue about it and actually have these cases come up. Can you paint a picture of what these kinds of cases would look like and how likely do you think they are?\n\n**Gabriel Weil:** Sure. Before I do that, I just want to offer some clarifications there on the framing of your question. I don’t think we necessarily need a lot of them. We need there to be a high probability that you get one for a particular system before we get the catastrophe, before we get the uninsurable catastrophe. But you don’t need thousands of them.\n\nYou also don’t actually need them. What you need more, if you have the liability rule in place, let’s say you’ve done this via legislation as opposed to common-law accumulation of cases, then what you really need is the expectation that these cases are likely to happen. You don’t actually need them to happen. Because what you want… ideally, the companies are expected to be liable, therefore they’re trying so hard to avoid these punitive damages judgments, and so you never get them. You might worry about some Goodharting problem there where they iron out all the practically compensable cases without actually solving the catastrophic risk. That is a failure mode I’m worried about, but I think this could work without \\[companies\\] ever actually forking over any money, if you just have that expectation. But now-\n\n**Daniel Filan:** Although it seems good if they’re like… Well firstly, I guess right now we’re in this position where we’re wondering how many of them to expect. It also seems good if there are going to be 10 such cases, because there’s some uncertainty about whether people get around to suing and maybe you’d want to average out some variance in what juries are going to do to make it a little bit more predictable. It seems like maybe you don’t need a thousand, but it seems like 10 would be much better than 1.\n\n**Gabriel Weil:** So at the risk of seeming callous about the real people that would be harmed in these scenarios, yes. I think from the perspective of catastrophic risk mitigation, more of these is better and that you would want a few. I’m just saying in principle, you don’t need very many, and if you really take my expectations argument seriously, you actually don’t need any, you just need the expectation of some, or the expectation of a high probability of some.\n\nNow to your question about what these look like. So the example I use in the paper is: you task an AI system with running a clinical trial for a risky new drug. It has trouble recruiting participants honestly. And so instead of reporting that to the human overseers of the study, it resorts to some combination of deception and coercion to get people to participate. They suffer some nasty health effects that are the reasons it was hard to recruit people in the first place, and they sue.\n\nSo it seems like here we have a misaligned system. It was not doing what its deployers or programmers wanted it to do. It wanted to honestly recruit people, but it learned the goal of “successfully run this study”. It didn’t learn the deontological constraints on that. So it seems like we have a misaligned system, which for whatever reason was willing to display its misalignment in this non-catastrophic way. So maybe a few hundred people suffered health effects, but this isn’t the system trying to take over the world and now the system’s probably going to be shut down, retrained, whatever, now that we know it has this failure mode.\n\nBut probably the people who deployed it ex ante couldn’t have been confident that it would fail in this non-catastrophic way. Presumably they thought it was aligned or they wouldn’t have deployed it, but they probably couldn’t have been that confident. They couldn’t have been confident to more than one in a million that it wouldn’t fail in a more catastrophic way. And so that’s the kind of case I’m thinking about.\n\n**Daniel Filan:** Okay, so it seems like the general pattern is: AI does something sketchy, it lies to people, or it steals some stuff, and it does it pretty well, but eventually we catch it. And because we catch it, then someone can sue, because we’ve noticed these harms. I wonder… it seems like this applies to the kinds of AIs that are nasty enough that they do really bad stuff, but also not quite good enough to just totally get away with it without a trace.\n\n**Gabriel Weil:** Not good enough to get away with it, or they’re myopic in various ways. So maybe the system doesn’t want to take over the world: all it wants to do is, it really wants the results of this clinical trial, because that’s all it cares about. It’s willing to risk getting caught and actually maybe it doesn’t mind at all because by the time it’s caught, it’s achieved its goal. And if the people who deployed it could show they were really confident that it was myopic in this way or had narrow goals in this way, then maybe they didn’t risk anything that bad. But I’m skeptical in that generic case that they can show that.\n\nAnother scenario you might think about is a failed takeover attempt. So you have a system that’s scamming people on the internet to build up resources, doing various other things. Maybe it even takes over the server it’s on, but at some point we’re able to shut it down, but it harms some people along the way. I think that’s another sort of near-miss case. There’s different ways you can imagine this going where it either has really ambitious goals but isn’t capable enough to achieve them, or maybe it is potentially capable enough and we just got lucky. There’s different ways to think about this. Because when you think about [Joe Carlsmith](https://joecarlsmith.com/)’s [work](https://arxiv.org/abs/2311.08379)… these systems are facing trade-offs, they worry that their goals are going to be gradient-descented away, and so there’s a trade-off between, do you help with alignment research now \\[and\\] risk your goals being changed, or do you act now even though there’s some possibility that you might fail?\n\nSo there it could be that even this particular system, if you had perfect knowledge about the system, there would’ve been some non-trivial risk that this specific system would have caused an existential catastrophe. We just got really lucky, or we got moderately lucky, however much luck you needed in that situation. I mean, theoretically it could be a 50% chance: then you’re already in uninsurable risk territory and the actual damage award’s going to be too big. But certainly there’s going to be cases where it’s 1 in 100, 1 in 1000, 1 in a million, where a reasonable person would’ve thought that that was the risk.\n\n**Daniel Filan:** Okay, sure. So I guess my underlying concern is that this kind of scheme might under-deter safety for really, really capable AI systems, perhaps because I’m imagining a binary of either it’s fine or it does really nasty stuff and gets away with it. But I guess the thing you’re suggesting is even for those systems, there’s a good chance of failed takeover attempts or maybe it’s just myopic. And like you said, if we only need a couple of those, even just in expectation, maybe that makes it fine and aligns the incentives?\n\n**Gabriel Weil:** Yeah. So I want to be upfront about this: I think there is a worry that we don’t get enough of the expectation, the right kind of warning shots or near misses. And if there are certain classes of harms or scenarios for which there aren’t near misses, and therefore this doesn’t give the labs enough incentive to protect against those, I think that’s a real concern. I don’t know what the shape of the harm curve looks like, how probable different kinds of harms are and whether there are qualitatively different failure modes for a system, some of which aren’t really correlated with near misses. It seems like there should be, if there’s lots of different variables you can tweak about the system. Maybe that particular system isn’t going to have near misses, but a very near system would.\n\nAnd so still the labs are going to have incentives to guard against that. But I think yes, that is a real uncertainty about the world: that if you think we can be confident ex-ante that certain types of warning shots or near misses are unlikely, that you’re going to want other policy tools to deal with those kinds of situations. I don’t want to hide the ball on that.\n\nFeasibility of big changes to liability law\n-------------------------------------------\n\n**Daniel Filan:** Gotcha. Fair enough. So I next want to talk about this proposal as law. The first question I want to ask is: the proposal in this paper is some amount of change to how liability law currently works. But I don’t know that much about liability law, so I don’t have a good feel for how big a change this is. Can you tell me?\n\n**Gabriel Weil:** There’s a few different, as I’ve been saying, levers that I want to pull here. Some of these are pretty small asks, and then at least one of them is really big. So we haven’t talked as much about the negligence versus strict liability calculation. Right now there’s three kinds of liability that are typically going to be available. There’s negligence liability, there’s products liability (which is called strict liability, but in some details has some very negligence-like features), and then there’s abnormally dangerous activities strict liability, which I would call a more genuinely strict form of liability. Negligence liability is clearly available, but I think going to be hard to establish.\n\n**Daniel Filan:** Wait, before we go into this, what is negligence versus strict liability? What’s the difference?\n\n**Gabriel Weil:** When you’re suing someone for negligence, you’re suing them alleging that they breached a duty of reasonable care. So, they failed to take some reasonable precaution that would’ve prevented your injury. And so, the general principle of negligence law is: we all have a general duty to exercise reasonable care and not to harm other people. And when we don’t do that, when we fail to exercise that reasonable care, that can give rise to liability, when that causes harm. So that’s clearly available, but I think going to be hard to prove in this context, because you would have to point to some specific precautionary measure that a lab could have taken that would’ve prevented your injury. When we don’t know how to build safe systems, it seems like it’s going to be really hard to say, “Oh, if you’d done this, the system would’ve been safe,” right?\n\nIt’s not impossible: it could be that there’s some standard precautionary measure that (say) Anthropic and DeepMind are doing, but Meta isn’t, and then their system harms someone. I’m not saying there’s never going to be negligence liability, but I’m saying even if you had a negligence liability for all harms where there’s a breach of duty or there’s a provable breach of duty, that’s unlikely to buy us as much risk mitigation as we’d like.\n\nIn particular, that’s because the scope of the negligence inquiry is pretty narrow. So, say you’re driving your car and you hit a pedestrian, we don’t ask, “Should you have been driving at all?” (say you’re a licensed driver), “was the value of your trip really high enough that it was worth the risk to pedestrians that you might hit them?” Or say you’re driving an SUV, you weren’t hauling anything, it was just you by yourself, you could have been driving a compact sedan. We don’t say, “Did you really need to be driving that heavy a vehicle that increased the risk that you would kill a pedestrian?” We just ask things like, “Were you speeding? Were you drunk? Were you texting while driving?” Those are the kind of things that are in the scope of the negligence inquiry. And so in the AI context, I think, you can’t say, “Oh, you just shouldn’t have built this system. You shouldn’t have deployed a system of this general nature. You should have been building STEM AI instead of large language models.” Those aren’t the kind of things that are typically going to be part of the negligence inquiry. So that’s why I think negligence is not super promising.\n\nThen there’s products liability, which is clearly available if certain criteria are met. So first of all, it has to be sold by a commercial seller. If someone’s just deploying an AI system for their own purposes, that they made, products liability isn’t going to apply. There’s also this question as to whether it’s a product or a service. Ultimately, I don’t think these distinctions matter a lot for the kind of risks I’m worried about, because I think the test that’s going to end up being applied is going to be very negligence-like.\n\nWhen you’re in the products liability game, there are three kinds of products liability. There’s what’s called “manufacturing defects”. This is like: you ship a product off an assembly line that doesn’t conform to the specifications and that makes it unreasonably unsafe. And that is more genuinely strict liability in the sense that no matter how much effort you put into your QC \\[quality control\\] process, say your QC process is totally reasonable and it would be unreasonably expensive to spend more to eliminate one in a million products coming off the line unsafe, still you’re going to be liable if that one in a million harms someone. But I don’t think you’re really going to have manufacturing defects in the AI context. That would be like, you ship an instance of the model with the wrong weights or something. I just don’t think that’s a failure mode that we’re really worried about.\n\nAnd so, we’re more likely to be dealing with what are called “design defects”. There the test is whether there was some reasonable alternative design that would’ve prevented the injury. And you can see, through the presence of the word “reasonable” there, that you end up in a similar sort of cost-benefit balancing mode as you are with negligence. Again, if we don’t know how to build safe systems, it’s hard to show… Yes, you don’t have to show that the humans behaved unreasonably, you have to show that the system was unreasonably unsafe. But I think that distinction doesn’t end up mattering that much, and in practice it’s going to function a lot like that. There’s also “warning defects”, which I think you could potentially have liability on, but even if you have all the warnings in the world, I don’t think that’s going to solve the problems you’re worried about.\n\nSo that leaves the third pathway, which is this “abnormally dangerous activities” idea. There are certain activities that we say, “They’re really risky even when you exercise reasonable care, and so, we’re going to hold you liable for harms that arise from the inherently dangerous nature of those activities.” And there’s a meta-doctrine as to what activities qualify as abnormally dangerous that I go through in the paper. I think plausibly under that meta-doctrine, training and deploying certain kinds of AI systems should qualify as abnormally dangerous. I think the courts are unlikely to recognize software development of any kind as abnormally dangerous on the status quo, business as usual. I think it’s clearly within their powers to do this, to treat training and deploying AI systems that have unpredictable capabilities, uncontrollable goals, as abnormally dangerous activity. I think it does meet the technical parameters there, but I think it would require an understanding of AI risk that courts have not currently been persuaded of.\n\nBut I think this is a move they should make, the courts could make \\[it\\] on their own. It would not be a radical departure from existing law. I think it would be consistent with this broad doctrine; it would just be recognizing a new instance in which it applies. So I think that’s a relatively modest ask to make of courts. Though, again, I want to be clear, \\[it’s\\] not the default that’s likely. So, that’s one step. That solves the “can you get liability at all for compensatory damages?”\n\nThen there’s the “punitive damages” piece, which is designed to get at these uninsurable risks. There, I think, there’s a much heavier lift. There’s a long-standing punitive damages doctrine that requires what’s called “malice or recklessness”, “reckless disregard for risk of harm”. We talked before about how even having provable negligence is going to be difficult in these cases; malice or recklessness is a step higher than that. You can think of it as basically like gross negligence.\n\n**Daniel Filan:** Sorry, hang on. I don’t know what gross negligence is.\n\n**Gabriel Weil:** Really bad negligence. It was really, really unreasonable what you did. Not just a reasonable person wouldn’t have done it, but even a normal unreasonable person wouldn’t have done it. It’s a lot worse. The cost-benefit calculus is lopsided, right?\n\n**Daniel Filan:** Yeah. The image I have in my head is of someone saying, “Yeah, I know this could be risky, but I don’t care. I’m doing it anyway.” Which seems like a pretty high bar.\n\n**Gabriel Weil:** Yeah, it’s a pretty high bar. And so I think courts are unlikely to take the step of reforming punitive damages doctrine in the ways that I would like them to, because this would be such a significant change.\n\nNow, I do want to point out that if you think about the normative rationales for punitive damages, at least the one that I find most compelling and that I think is the central normative rationale, is that compensatory damages would under-deter the underlying tortious conduct. That doesn’t require malice or recklessness to be true. It requires something about the features of the situation that suggests compensatory damages are going to be inadequate. It might be uninsurable risk. It might also be \\[that\\] most people who will suffer this kind of harm won’t know that you caused it, or won’t sue because maybe the harm is small relative to the cost of proving it. And so, maybe only one in 10,000 people who suffer will end up suing, and so maybe you should get punitive damages to account for the people that don’t sue, but nothing about that requires malice or recklessness, and there is [existing scholarship](http://www.law.harvard.edu/faculty/shavell/pdf/111_Harvard_Law_Rev_869.pdf) that argues for getting rid of this requirement. So, it’s not coming new in this AI context.\n\nBut again, I think courts are unlikely to do this and it would be a major doctrinal change. It would require both state courts to change their punitive damages doctrine, and it would also require the US Supreme Court in applying, again, the due process clause to say that applying punitive damages in this context without these threshold requirements doesn’t violate due process, that companies that deployed these systems were on adequate notice. I think that’s not totally insuperable, but I think it’s pretty unlikely as a matter of just the natural evolution of courts.\n\n**Daniel Filan:** I want to ask about this, because… Your paper cites this [work by Polinsky and Shavell](http://www.law.harvard.edu/faculty/shavell/pdf/111_Harvard_Law_Rev_869.pdf), basically saying, “punitive damages should just compensate for the probability of being caught”. At least, that’s how I understood their abstract. That seems intuitive to me, but also my understanding is that this was published, what, 20, 25, 30 years ago or something? And apparently it still hasn’t happened. So, the fact that it hasn’t happened makes me feel a little bit nervous about making these sorts of things -\n\n**Gabriel Weil:** You should be. I think you should not expect the courts are going to follow my advice here. They didn’t follow Polinsky and Shavell’s advice there, and they’re much more prestigious people than I am, they’re at Harvard and Stanford. They’re doing fancy economics models on this. I think you should not expect this change to happen from courts. I think they should. I think it’s within their powers. I think we should try to persuade courts. I think litigants should bring these arguments and force them to confront them and ask courts to do it. I think all that should happen, but I would not count on that happening.\n\nBut again, I want to be clear, I really think people should try, both because you never know and one state doing it would get you a lot of value, and because I think you would put it on the table politically, then you would say, “Look, we need legislation to overturn this.” And so, I do think at least with regard to the common law issues, with regard to what state tort law says, clearly state legislatures, if they want to, can change that requirement. There’s sort of a hierarchy of law in which statutes always trump common law. And so if a state wants to pass a statute saying either in this AI context or more generally that punitive damages don’t require malice or recklessness, that’s clearly something that state legislation can do.\n\nThere’s still the constitutional issues with that, although I think if you have a statute putting the labs or the companies on notice, that might accomplish a lot of the due process notice function that the Supreme Court’s worried about. And so, it’s not clear to me that that would be a constitutional barrier in the context of legislation.\n\n**Daniel Filan:** Can I ask about this punitive damages change? This case for having punitive damages compensate the probability of a case being brought, is the thing holding that up that legal scholarship broadly is not persuaded by it? Or is it an issue where legal scholars are persuaded by it, but judges aren’t? Or is there some other issue going on?\n\n**Gabriel Weil:** So the short answer is I don’t know, but if I were to speculate, I think Polinsky and Shavell’s argument is really persuasive if you’re thinking about this in a [law and economics](https://en.wikipedia.org/wiki/Law_and_economics) frame, and \\[if\\] that’s all you think tort law is about, and that’s basically -\n\n**Daniel Filan:** “Law and economics” basically being, thinking of law just in terms of economic efficiency and maximizing social utility and stuff, is that roughly right?\n\n**Gabriel Weil:** Right. That’s the frame that I tend to prefer, but that is not dominant. And so, there are other ways of thinking about what tort law’s for, particularly what punitive damages are for. There’s an expressive function, expressing society’s disapproval for the behavior, that would more map onto this recklessness/malice requirement. And so, if you have someone that’s doing something that maybe isn’t even negligent at all, or it’s a strict liability tort or it’s ordinary negligence, the idea that we want to punish you over and above the harm you did doesn’t sit right with some people.\n\nHonestly, I don’t think courts have really revisited this issue in a long time. Mostly what courts do is just follow precedent, unless they have some good reason to reconsider it. I think AI, arguably, should give them a reason to reconsider it, that we have this pressing social problem that you are well positioned to solve. Maybe the fact that this requirement doesn’t really make sense from a law and economics perspective more broadly hasn’t been that big of a deal in the past. A lot of the sorts of problems that you’d want punitive damages to deal with, we’ve dealt with through other policy tools, for reasons we’ve talked about earlier.\n\nI think there’s reason to be skeptical that those policy tools are going to be adequate in this context. We need to lean more heavily on tort law, so it makes it really important that you get punitive damages right from this law and economics perspective and they should reconsider it. Again, I don’t think they’re super likely to do that, but I think we should try. I’m maybe talking myself into thinking there’s a little bit of chance that they would reconsider in this context.\n\n**Daniel Filan:** I’m going to try and suggest some hopium and you can talk me out of it. I glanced at this Polinsky and Shavell paper, because (not knowing anything about the law), this seemed like an obvious change. I read the first page of the paper; it’s got a table of contents and it’s got a little disclaimer at the bottom. And I noticed that in the table of contents it’s like, “It should be based on the probability of a lawsuit actually being brought, not based on other things; and in particular, it shouldn’t be based just on the wealth of the defendant because that’s economically inefficient.” And I see the thing at the bottom saying, “Yeah, this was sponsored by-“ Was it ExxonMobil or something?\n\nMy understanding is that, basically, a big oil company paid them to write this paper in the hopes that this would be more friendly to really big business. And I have this impression that people in the world don’t like the idea of changing the law to make it better for really big businesses. But this change, it seems like, would make life worse for really big businesses and therefore maybe everyone’s going to be a little bit more friendly to it because people don’t like the big guy. Does that sound right? Am I being too cynical?\n\n**Gabriel Weil:** There’s a few different ways I want to answer that. First of all, I think that’s good and bad. I’m talking to state legislators about different legislation to implement, different elements of this proposal, and a lot of them are afraid of doing anything that runs afoul of the tech lobby, or they want to at least neutralize them. It’s okay if they’re not really supportive, but in a lot of states, having all the tech companies be against your legislation is pretty bad. So, that’s one answer \\[where it’s\\] not obvious that’s a net good. But I do think there’s a populist… in certain circles at least there is backlash against big tech, and so if your strategy is not the inside game, if it’s making a big public case, then maybe that’s helpful. I’m going to leave it to political entrepreneurs to make those judgments.\n\nThen there’s the way this fits into the broader AI policy ecosystem. And for that purpose, I think it’s actually really valuable that this \\[proposal\\] is relatively hostile to the incumbent big players. Not as hostile as some of the [AI Pause](https://pauseai.info/) stuff, but when you compare it to licensing regimes that have anti-competitive effects on some margins… I think there’s a strong case that we should have anti-trust exemptions for them cooperating to share alignment research, maybe to coordinate the slow-down on capabilities enhancements. There’s reasons to think that, under current law, that would violate anti-collusion principles. I think that there’s good reasons for having exemptions to that.\n\nI think those ideas are generally pretty friendly to the incumbent players, and there is an accusation that’s sometimes thrown around that AI safety is this psyop or whatever by the big tech companies to avoid competition and stuff like that. And so, I think having some policy proposals in your package that are clearly not in the interest of those big companies is useful, at least rhetorically. And so, I think it does play that role, but I don’t think, “oh, it’s bad for big tech, therefore automatically it’s going to happen”. That’s definitely not my model.\n\n**Daniel Filan:** Fair enough. All of this was sort of a tangent. I was originally asking: how big a lift is this in terms of changes to tort law that happen? You mentioned that you have to make this change to strict liability, which is maybe not so big. You mentioned that there’s this change to punitive damages, which is kind of big, and I interrupted you there, but I think maybe you were going to say more.\n\n**Gabriel Weil:** Punitive damages is a pretty big lift. I think we’ve beaten that horse plenty. Then there’s other things. Liability insurance, the court just can’t do that. Liability insurance requirements, that would require legislation. I don’t think it’s a big legislative lift, but it’s just not something courts can do. And then there’s other things that we haven’t talked about. There’s this doctrine of what’s called “proximate cause” or “scope of liability”. Say I cut you off in traffic and you have to slam on your brakes, but we don’t collide, you’re fine, but it slows you down 30 seconds and then two miles down the road you get sideswiped in the intersection. And you want to sue me and say, “Look, but for your negligence in cutting me off, I wouldn’t have suffered that later injury. So, you owe me money.”\n\nAnd I say, “No, it wasn’t foreseeable when I cut you off that you would get in a collision two miles down the road. In fact, it’s just as likely that I could have prevented a similar collision for you.” And the courts are going to side with me there. They’re going to say, I’m not going to be liable for that, even though I was negligent and my negligence did cause your injury. This is an independent element of the tort of negligence. And so, in the AI context the question is, what does it mean for the injury to have been foreseeable? In some sense, misalignment causing harm is clearly foreseeable. [Sam Altman](https://en.wikipedia.org/wiki/Sam_Altman) talks about it: if his system does it, he’s not going to say “I couldn’t see this coming”. But the specific mode of a misaligned system harming someone almost certainly won’t be foreseeable in specific details. And so, it really depends on what level of generality that question is evaluated \\[at\\].\n\nThere is this “manner of harm” rule that says that the specific manner of harm doesn’t have to be foreseeable as long as the general type of harm is. That helps a little bit, but there’s still a lot of wiggle room in how this doctrine is applied. There’s not some high-level change to precedent that I can ask for to say, “change this rule so that there will be liability in these cases.” It’s really just \\[that\\] courts need to be willing to apply a relatively high level of generality in their scope of liability or proximate cause assessments for AI harms. So, how big of a lift is that? I think not a huge lift, but also not necessarily going to be consistent across cases, and you just want it to generally be fairly friendly to liability, but it’s a pretty mushy doctrine in that sense.\n\nThen there’s something we talked about earlier, the way mortality damages are dealt with under current law. There’s two kinds of lawsuits you can bring when someone dies. There’s what’s called a “survival action”, which is basically all the torts that the person, the decedent, could have sued for the second before they died. So, say I crashed my car into you and you’re in the hospital for six months and then you die. And in those six months you racked up lots of hospital bills, you had lots of pain and suffering, you lost six months of wages, you could have sued me for all that. Your estate can still sue for all those things after you’re dead. That wasn’t true at common law, but there are these survival statutes that allow the claims that you had at the moment you died to be brought by your estate.\n\nThen there’s what’s called “wrongful death” claims, which are also creatures of statute, that say that designated survivors - so this is no longer your estate suing, this is specific people with a specific relationship to you, say your kids or your spouse or whatever - can sue for harms they suffered because you died. Maybe your kid’s suing because they were going to benefit financially from you, they were going to get care-taking services, whatever. In neither of those lawsuits is the fact that it kind of sucks for you that you’re dead… that’s not something that can be sued for, in almost every state. And so if you think about a quick and painless human extinction where there’s no survivors left to be suffering from the fact that their relatives are dead, if you take this to its logical conclusion, the damages for that are zero. No lost wages because you died quickly. No pain and suffering, no hospital bills, no one’s around, not only not around to sue, but there’s no claim because they haven’t suffered from your death, because they’re dead too.\n\nNow, I don’t think courts are likely to take that so literally. If they buy everything else in my paper and are like, “Okay, we’re trying to do damages for how bad human extinction is,” I don’t think they’re going to actually take that to its logical conclusion and say the damages are zero. But I think there’s a reason to be worried that in general, if we think most of the harm from AI misalignment or misuse is going to come in the form of mortality, that those harms are going to tend to be undervalued. So, that would require a statutory tweak in individual states to say that wrongful death damages should include the value of the person’s life to them.\n\nWe have ways of estimating that. As you mentioned earlier, regulatory agencies use a value of a statistical life on the order of $10 million (for US lives). I think that would be fine in the tort context, but that would require a statutory change. I think not a huge lift, but it would require a statute, I think; because wrongful death claims are statutory to begin with, I think it’s very unlikely the courts would try to change that on their own.\n\n**Daniel Filan:** I don’t quite understand. It seems like the case that we’re really thinking about is: an AI causes an intermediate amount of harm, and we want to assess these punitive damages for how bad it would be if some sort of really bad catastrophe happened. It strikes me as a priori possible that, that kind of calculation could take into account the value of the life-years not lived, but that could be different from actually suing for loss of life.\n\n**Gabriel Weil:** Well, if your theory of punitive damages is that you can’t sue for these compensatory damages if they actually arise because they’re practically non-compensable, then presumably the punitive damages should be pulling forward those hypothetical compensatory damages. And again, I’m not so worried that… but if you just are hyper-logical about this and you apply existing compensatory damages doctrine in that scenario, the number you get is zero. Now, again, I think if courts are persuaded by all my other arguments, they’d be really dumb to go there and to say, “Well, if it causes lots of pain along the way, you can sue for that. But the actual human extinction is…” That does seem crazy. I’m just pointing out that that is the logical entailment of the existing structure.\n\nNow, you could say we’re going to tweak… Instead of changing the statute, you could say, “Well, I’m going to have a slightly different conception of what these punitive damages are doing, and they’re not quite just pulling forward the compensatory damages.” I think that you could do, courts could do on their own. I just want to point out, I don’t think this is at all the biggest obstacle to making my framework work, but it just seems worth being transparent about this quirk of the way mortality damages work that, in theory at least, could cause a problem here. And if we can pass legislation fixing that, it would make it a lot simpler and more straightforward.\n\n**Daniel Filan:** Fair enough. So, basically we’ve got this bundle of changes that you might hope that courts or legislators make. This bundle is some amount of a big ask. How often do those kinds of changes actually happen?\n\n**Gabriel Weil:** If by “those kinds of changes” you mean reforms to make liability easier, the answer is, legislatively, they almost never happen. Tort reform statutes, with the exception of some of those statutes I was talking about like wrongful death and survival action statutes, are almost always to limit liability. So, when people talk about tort reform, they tend to make it like, “Oh, liability insurance is too expensive, it’s making healthcare too expensive, we need tort reform.” What they typically mean is making it harder to sue. If that’s your reference class that you’re drawing your base rate from, it doesn’t look so attractive. Now, maybe you think AI is different enough that we shouldn’t think of that as the right reference class. I think that’s a plausible move to make. But if that’s what you’re thinking about, then you shouldn’t be too optimistic.\n\nCourts, I think, are more inclined to make changes. They’re more symmetric in whether they make changes that are more pro-plaintiff or pro-defendant. There’s certainly been changes like market share liability, recognizing various forms of strict liability that have been plaintiff-friendly. And so, I think, as I said, I’m mildly optimistic about that making the strict liability/abnormally dangerous activities change. Again, I think that the punitive damages thing is too big of a doctrinal move, that I think courts are unlikely to make. And so, I think we probably are going to need to rely on legislation there.\n\nThe other thing we’re saying in this context is: if you think about this as a tort reform problem, then maybe you should think it’s unlikely. If you think that you’re going to have a lot of energy at some point, a lot of political will, to do something about AI law and policy, and those things include some things that would be more draconian, like just banning or creating moratoria on training models above a certain level. Well, saying you have to pay for the harm you cause is a less extreme step than a ban or than a pause. And so, once you think those things are on the table, I think you should be more optimistic about ideas like my liability framework. And so, maybe you don’t think this is likely to happen tomorrow, but if you think there’s going to be a future moment where there’s political will, I want this idea to be fleshed out and ready to go so that states or the federal government can pass it.\n\nInteractions with other areas of law\n------------------------------------\n\n**Daniel Filan:** Fair enough. Another question I have about this is: nominally this is about catastrophic risks from AI, but the “from AI” part doesn’t… I mean, you talk about some AI-specific things, but this seems like a fairly general type of proposal. Whenever there’s some risk of some uninsurable harm happening, we could have a pretty similar scheme. I’m wondering: what other effects do you think these kinds of changes could have? And also, have people talked about these kinds of changes before?\n\n**Gabriel Weil:** I’m not aware of anyone proposing using tort law to deal with uninsurable risks before. I think typically, the way we handle uninsurable risk is through some kind of prescriptive regulation, and those regulations often preempt tort liability. So if you think of a nuclear power plant, there is some liability for a nuclear power plant, but the liability isn’t really… The goal of it isn’t really to change the incentives.\n\nThere’s various government subsidies to make the insurance affordable. But we mostly rely on, especially in the US, regulating these things to death. It’s impossible to build a nuclear power plant in the US. It’s easier in France, but even there, they’re relying on prescriptive regulations. And I think that’s true broadly. If you think about biolabs that are doing gain-of-function research, I think it’s hard to bring a tort lawsuit for that. We mostly rely on these BSL certifications and stuff, biosafety-level certifications, prescriptive regulations.\n\nI think, generally, the thought has been \\[that\\] it’s hard for the tort system to handle this, \\[so\\] we should lean on other policy tools. I think it is harder for the tort system to handle this. But I think, in the AI context, it’s even harder for other policy tools to handle it, or at least to handle it sufficiently.\n\nI’m not saying it should be our exclusive policy tool. But I think there are real limits to what you can do with prescriptive regulation in this context, and so I want to lean more heavily on the tort system than you otherwise would.\n\nI think if you made these doctrinal changes, it would… So the strict liability change would only really apply to AI. I think the punitive damages change, in principle, would tend to be more broad. It would be weird to change it just for AI. But I think the implications of that might be pretty minor, since a lot of the areas where there are these catastrophic risks, the tort law is going to be preempted anyway.\n\n**Daniel Filan:** Sure. One imagination I have is: during the Cold War, my understanding is that there were a bunch of near-misses where we almost set off a bunch of nuclear weapons, but we ended up not quite doing it. Maybe the US Air Force accidentally drops a nuclear bomb on the US. It doesn’t explode, but five of the six safeguards are off or something.\n\nIn my understanding, there’s a thing you can bring called a [Section 1983](https://www.law.cornell.edu/uscode/text/42/1983) lawsuit, where if a government official violates my constitutional rights, I can sue them for the damages I faced. And one thing I could imagine is: suppose that the military accidentally drops a nuclear bomb, it doesn’t detonate, but five of the six safeguards are off, they drop it on my field, it damages my crops a little bit or it’s kind of nasty.\n\nI can imagine a world in which I bring a 1983 lawsuit to the government, and not only do I try and sue them for the minor damages to my actual property, but I also try and sue them for, “Hey, you nearly set off a nuclear bomb. That would’ve been super bad.” Does that strike you as a way that this kind of change could be implemented?\n\n**Gabriel Weil:** Maybe, but there’s a lot of complications in that context. Section 1983, there’s lots of rules about when it applies, when this waiver of sovereign immunity works. I think that lawsuit’s going to be tough.\n\nIt doesn’t necessarily make sense to me normatively in that context that you would… The government’s not a profit-maximizing actor in any sense, so is liability the right tool to…? The government paying means the public’s paying, right? Does that change the incentives of the military in the right way? Not obvious to me that it does.\n\nSo you can think of tort law generally as serving two functions, right? There’s a compensation function and a deterrence function. Right? In the context of suing the government, I tend to think the compensation function’s a lot more important, whereas in the private context, I tend to think the deterrence function is more important and the compensation is a happy byproduct of that. Punitive damages are really about deterrence, they’re not about compensation.\n\nThere’s even a proposal I have in the paper that maybe not all the punitive damage should even go to the plaintiff. And so do I really think the government’s going to be that much more careful or the military is going to be that much more careful with nuclear weapons if there’s this liability? Maybe, but it’s not obvious to me.\n\n**Daniel Filan:** Fair enough. I guess, like you mentioned, you could also imagine having a similar sort of scheme for lab biosafety accidents. Presumably, some of those are run by private companies. Maybe something there could happen.\n\n**Gabriel Weil:** Yeah, I think to the extent that’s not preempted by the regulations, I think that would be a benign effectiveness. Maybe it would be really tough to insure a lab that’s doing gain-of-function research, and maybe that would be okay. You make it a lot more expensive, but then you’d have to say: well, if the social value of this is large enough, fine. You can get a big enough grant or a big enough expected profit from doing this research, then okay. But if you can’t, then that suggests that this isn’t a socially valuable activity at the level of safety that you’re able to achieve, and so you just shouldn’t be doing it.\n\n**Daniel Filan:** Sure. Another question I have, more about the punitive damages change: is there stuff like this in criminal law, where there are additional penalties for things that the government might not have caught you doing?\n\n**Gabriel Weil:** So if they didn’t catch you, it’s hard to know how we know that you did them.\n\n**Daniel Filan:** I mean punishing a crime more severely because we thought we might not have caught you.\n\n**Gabriel Weil:** Certainly, your past record, even if it’s not a conviction, is taken into account in the sentencing context. But also, I think an example that might map onto this that might be getting at the same sort of idea is: attempted murder is different from assault, right?\n\nSo you get a longer prison sentence if you attack someone and you’re trying to kill them, even if you don’t \\[kill them\\], than you do if you just beat someone up with there being no indication that you were trying to kill them. And so I think that’s a similar idea going on in that context.\n\n**Daniel Filan:** Interesting. Actually, just picking up on a thing you said earlier: in terms of difficulties of applying this kind of liability scheme to state actors, sometimes a thing people talk about is this possibility that AI labs will get nationalized or somehow there’s going to be pretty tight intermingling between the government and AI development. Would that pose difficulties for this sort of liability scheme?\n\n**Gabriel Weil:** So I think in a world where the government is taking over AI companies, I think they’re unlikely… There’s something called sovereign immunity, so you can only sue the government when they waive that, when they allow you to sue them. I don’t think it’s super likely as a predictive matter that the government’s going to want to expose itself to a lot of liability and punitive damages in that scenario. So that’s one question: whether this would be likely to happen in that world.\n\nAnd then there’s another question of: is liability a useful tool in that world? I don’t think the government responds to financial incentives in the same way that private parties do. And so if we’re in that world, where they’re nationalizing it both maybe because they’re worried about risks but also because they’re worried about an AI arms race between the US and China or whatever, is the risk of getting sued really going to change their calculations that much? It’s not obvious to me that that has the same incentive alignment effects that it does in the private context.\n\nAnd so I think, in some ways, you have lower risk in that world, but in other ways, that’s a more dangerous world. It’s not obvious to me on balance whether I’d rather live in that world. You’re moving all the key decisions to the political process. And part of the advantage of liability is: yes, you need the political process to get the high-level decisions in place, but then you’re shifting the onus to these private actors that have, in theory at least, more aligned incentives, as opposed to trusting elections and regulatory processing or military decision-making to make the right decisions.\n\nHow Gabriel encountered the AI x-risk field\n-------------------------------------------\n\n**Daniel Filan:** Fair enough. I’d like to change topic a bit now. Mostly when I see people doing work on AI alignment, usually they’re either an AI researcher who’s come across alignment concerns in the course of being in the AI sphere, or one of these dyed-in-the-wool young EA professionals who got into AI that way. My understanding is that you have a decent background in environmental law. How did you come across AI stuff?\n\n**Gabriel Weil:** I’ve been loosely affiliated with the rationalist and EA communities for a long time, and so I’ve been aware of the AI risk problem for over a decade. I’d never really, until pretty recently, considered working on it professionally. It wasn’t obvious what role I would play. I wasn’t thinking in terms of what policy tools or legal tools would be relevant.\n\nAnd so I \\[was\\] glad some technical researchers are working on this, but I thought of it as a technical problem. I guess in the last couple of years, I started to reconsider that. And then last summer, I did this [fellowship](https://pibbss.ai/fellowship/) called [PIBBSS](https://pibbss.ai/), Principles of Intelligent Behavior in Biological and Social Systems, that brings together non-computer… I mean, there were a couple of computer science, ML types there, but it was mostly people in social sciences, economics, philosophy.\n\nI was the only lawyer that was part of this program. But it was about 15-20 people that each were assigned a mentor. So I was working with [Alan Chan](https://www.achan.ca/) from [Mila](https://mila.quebec/en/). He was really helpful on helping getting me up to speed on the technical side of some of these questions. And so I did this fellowship. We all spent the second half of the summer in Prague together, working out of a co-working space there, and got to learn from other people who were doing a project in this area. So that’s the causal story of how I got involved in this.\n\nIntellectually, it’s not as big of a departure from my work on environmental law and policy as it might seem. We talked earlier about how carbon tax is sort of like AI liability. I tend to approach my scholarship with a law and economics frame. It’s in a different context, but I’m thinking through a lot of issues \\[where\\] I’m comfortable with the principles involved from other contexts. I also teach tort law, and so it was natural to think about how that domain could be helpful to AI risk.\n\n**Daniel Filan:** In some ways, I feel like there’s a strong throughline of… It seems like some of your work is on changes to the liability system, and it seems of a kind with that kind of work.\n\n**Gabriel Weil:** Yeah. So I had [a recent paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4466197) I think you read on the Hand Formula, which is this test for breach of duty in negligence cases and ways in which it might fail. So that was a more general critique of the way tort law works.\n\nI think this paper implicitly has a lot of broad critiques; I think you suggested this. A lot of the things that I think should be changed in this context really are problems with tort doctrine generally, that the AI risk context is really just pointing up and exposing. And so, in principle, you could have written this paper without ever mentioning AI. I think it’s worth being explicit about why I care about it.\n\nAI x-risk and the legal field\n-----------------------------\n\n**Daniel Filan:** Suppose we really do want to make these kinds of changes. If the AI x-risk community wants to push for this kind of thing in the legal profession or among legislators, what do you think that looks like?\n\n**Gabriel Weil:** I think there’s a few different channels of influence. The one that I have the most control over is just convincing other legal scholars that this is a good idea, and then you create a consensus around that, other people write articles saying it’s a good idea, and then there’s a bunch of articles for litigants to cite to judges when they try to persuade them to adopt this. So that’s one channel of influence, through the academic pathway.\n\nAnother is just lawyers out there bring these cases and try to convince judges to adopt these different doctrinal changes. You can do it with strict liability even before you have a case where there’s catastrophic risk implicated. And then as soon as there’s any case where there’s a plausible argument for uninsurable risk, try to raise the punitive damages and get courts to consider that.\n\nAnd then on a parallel track, I think we should be talking about legislation both for strict liability and for punitive damages and potentially for other things like liability insurance requirements and changing the way mortality damages work. Those are all things that could be done by state legislation. And so I think people who are interested in doing policy advocacy - that’s definitely an avenue that I’m involved with, talking to some state legislators, that I’d like to see more work on.\n\nIn some states, this could be done via ballot initiative. So certainly, in California, it’s pretty easy to get an initiative on the ballot. I think strict liability is a pretty straightforward yes or no question that you could have a vote on. I think it would be a little tougher to do it for punitive damages, but I wouldn’t put that off the table. Liability insurance I think would be hard, but California seems to let lots of crazy stuff onto ballot initiatives, so maybe.\n\n**Daniel Filan:** Yeah. I don’t know if you’re familiar with [the state constitution of California](https://en.wikipedia.org/wiki/Constitution_of_California), but every two years, it gets added with some random stuff that some ballot measure passed. I think the California state constitution includes text from [a ballot measure](https://ballotpedia.org/California_Proposition_52,_Continued_Hospital_Fee_Revenue_Dedicated_to_Medi-Cal_Unless_Voters_Approve_Changes_(2016)) that’s basically about a scheme to do some tricksy accounting stuff to maximize the amount of Medicaid money we get from the federal government. A lot of stuff happens in California.\n\n**Gabriel Weil:** I don’t think California has adopted the optimal initiative process. But given that it exists, I think it should be used for this good purpose. So I’d love to see someone \\[doing this\\] and I’d be happy to advise any project that wanted to pursue an initiative like that in California.\n\n**Daniel Filan:** One thing I wonder: you mentioned that this is kind of a “law and economics” framing on the problem of AI risk. And my impression is that law and economics has some kind of market share among the legal profession but not overwhelmingly so, such that every idea that the law and economics people think is good gets implemented.\n\nI wonder if it makes sense to make a different kind of case for these kinds of reforms that looks less law-and-economicsy and more something else. But law and economics is the part of the legal profession that I know the most about, so I don’t actually know any examples of other ways of thinking.\n\n**Gabriel Weil:** Yeah. There’s a political economy argument, that I think is maybe more popular on the left, that would be… Law and economics tends to be right-coded. I don’t think that’s inherent in the paradigm, but because of the way political coalitions are… The people who are funding a lot of the law and economics research tend to have more right-wing political goals.\n\nI don’t think my proposal here is particularly right-wing, but I think a lot of the skepticism of law and economics tends to be from more progressive or liberal folks. So I think you would want framings that appeal more to them. I think this proposal is more susceptible to critiques from the right since I’m just arguing for more liability to make life less friendly for these big companies.\n\nBut there’s also a lot of tech backlash on the right, so it’s not obvious to me how that plays into the politics of this. So I guess it depends whether you’re asking about how to convince fellow academics, and should there be people writing other traditions, providing a different case for this. I think there’s maybe scope for that. I don’t know exactly what that would look like. And then certainly, you’re going to want to use different arguments in different contexts when you’re trying to persuade a political audience.\n\nTechnical research to help with this proposal\n---------------------------------------------\n\n**Daniel Filan:** Fair enough. I guess my next question is: rather than on the advocacy side, a bunch of people listening to this are technical researchers. What kinds of technical research would be good complements to this kind of proposal, would make it work better?\n\n**Gabriel Weil:** In particular, you want research to be able to implement various aspects of the punitive damages formula, and also to be able to implement the liability insurance requirements. So I could see a role for model evaluations both in deciding what the coverage requirement is: a regulator could use a set of dangerous capabilities evaluations to decide how much insurance you need to take out before you can deploy the model, or if it’s pre-training insurance, to train the next model. And similarly, insurance companies could use a slightly different set of evaluations in their underwriting process.\n\nAnd then in the liability or punitive damages context, we need to estimate these different parameters in my liability or damages formula. So one thing we want to know is: how much risk should the trainer or deployer of this model have known that they were undertaking when they made the key decision?\n\nAnd I could see a role for technical research trying to get a handle on… reduce our uncertainty about that question. There’s also the question of: this particular harm that was caused, how elastic is that with the uninsurable risk? So for every unit of risk mitigation you get, say you spend a million dollars to reduce the risk of this particular harm by 20%, how much does that reduce the uninsurable risk? That’s another key parameter. And how much does that do that relative to a generic harm that this system might have caused?\n\nSo work on trying to figure out how to estimate those parameters would be really useful. I have [a blog post up on the EA Forum](https://forum.effectivealtruism.org/posts/yWKaBdBygecE42hFZ/how-technical-ai-safety-researchers-can-help-implement) that lays out the formula and the ways in which technical researchers can help solve these problems that you can point people to.\n\n**Daniel Filan:** A lot of those seem like general ways which any AI governance effort researchers could help with. It strikes me that trying to get a sense of, for any given failure, how much would mitigating that failure have mitigated against really bad catastrophic AI risks… I think perhaps that’s a unique thing about your proposal that researchers might not have already been thinking about.\n\nDecisions this proposal could influence\n---------------------------------------\n\n**Daniel Filan:** Before we wrap up, is there anything that I didn’t ask but you wish that I did?\n\n**Gabriel Weil:** Maybe “what decisions are you trying to influence with this?”\n\n**Daniel Filan:** What decisions are you trying to influence with it?\n\n**Gabriel Weil:** There’s a few different ways you can think about how this would influence the behavior of AI labs. So one scenario you might think about is, say OpenAI trains GPT-6 and they run it through a [METR](https://metr.org/) evaluation and it shows some dangerous capabilities, and maybe they’ve come up with some alignment evaluations and \\[they show\\] maybe this system is misaligned, you shouldn’t deploy it.\n\nAnd the question is: what do we do now? And there’s a trade-off or a different set of options where there’s cheap, dumb, easy things you could do that wouldn’t really solve the problem. You could just run RLHF to iron out the specific failure mode that you noticed. And almost certainly, that wouldn’t solve the underlying misalignment, but it’d be really easy to do.\n\nAnd there’s some moderately costly thing where you roll it back a little bit and then retrain it. And then there’s some somewhat more expensive thing where you do some adversarial training, maybe you roll it back further.\n\nAnd then there’s a really expensive thing where you say “none of the tools we have right now are good enough. We need to wait until we have better interpretability tools or we make some fundamental breakthroughs in alignment theory (or whatever it is)”. And you’ve got different actors within the labs; some are more cautious, some are less, and there’s a debate about which one of these options we should take.\n\nAnd I want to empower the voices for more caution, saying… maybe they’re motivated primarily by altruistic impulses, but I want to arm them with arguments that say: even if all you care about is the bottom line, we should do the thing that produces the best expected social returns, because that’s going to actually be what favors the bottom line.\n\nI think you see that a lot of these leading labs were founded with these high ideals. OpenAI was founded by people really worried about AI risk, and now there’s a lot of criticism of them that they’re moving too fast, that they’re taking too many risks.\n\nSam Altman was saying, [“Well, we need to move fast so we don’t have a compute overhang,”](https://openai.com/blog/planning-for-agi-and-beyond) but then [wants to get $7 trillion to invest in improved compute](https://www.wsj.com/tech/ai/sam-altman-seeks-trillions-of-dollars-to-reshape-business-of-chips-and-ai-89ab3db0), so there seems to be something a little confusing there. And obviously, there was [the whole kerfuffle with the board over him being fired](https://en.wikipedia.org/wiki/Removal_of_Sam_Altman_from_OpenAI).\n\nAnd so we’ve seen that these internal governance mechanisms are not things we can totally rely on. I think, similarly, even for a lab like Anthropic, which was founded by people who [defected](https://www.ft.com/content/8de92f3a-228e-4bb8-961f-96f2dce70ebb) from the alignment team at OpenAI, and [there were statements](https://twitter.com/bshlgrs/status/1764713562315067490) like, “Well, we’re not going to try to push forward the frontier on capabilities. We just want to have near-frontier models so we can do alignment research on them.” And then Claude 3 comes out and there’s [these claims](https://www.anthropic.com/news/claude-3-family) that it’s better on all these metrics than any model that’s been out there before. And so it seems like there’s very powerful financial incentives and other incentives for these companies to build commercializable products and to push forward on capabilities. And I think even people that are very well-motivated are having trouble resisting those forces.\n\nAnd so I think having a liability regime that puts a thumb on the other side of the scale, that makes it in their narrow interest to do the thing that they say they want to do, that is in the interest of society at large, would be really valuable. And so however you want to think about it, whether you think about this in terms of competitiveness or alignment taxes, if we can tax misalignment effectively through this liability, I think that could be really valuable.\n\nAnd you don’t have to think of it necessarily as being hostile at least to everyone in these AI labs. I think some people at least within these labs would welcome the fact that it’s empowering them to stand up for safety and \\[for safety\\] to not just seem like some altruistic concern. It’s actually part of the interest of the company.\n\nFollowing Gabriel’s research\n----------------------------\n\n**Daniel Filan:** Gotcha. So I guess to wrap up, if people are interested in following your research on this or on other topics, how should they do so?\n\n**Gabriel Weil:** You can find [all my papers on SSRN](https://papers.ssrn.com/sol3/cf_dev/AbsByAuth.cfm?per_id=1648032). Once I put them publicly, we can include a link to that. There’s only one on AI so far, but I expect to do more work in the future. They can follow me on Twitter [@gabriel_weil](https://twitter.com/gabriel_weil). And then I’ve got a couple of posts on the EA Forum, one just providing a [high-level summary of the paper](https://forum.effectivealtruism.org/posts/epKBmiyLpZWWFEYDb/tort-law-can-play-an-important-role-in-mitigating-ai-risk), and then another that I mentioned, explaining [how technical AI safety researchers can help implement this framework](https://forum.effectivealtruism.org/posts/yWKaBdBygecE42hFZ/how-technical-ai-safety-researchers-can-help-implement). And so I would direct people to those. Dylan Matthews also did [a write-up of the paper in Vox](https://www.vox.com/future-perfect/2024/2/7/24062374/ai-openai-anthropic-deepmind-legal-liability-gabriel-weil) that we can link to. I think that’s about it.\n\n**Daniel Filan:** Gotcha. Well, thank you for coming on the show.\n\n**Gabriel Weil:** Thanks. This was great.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) and [Lightspeed Grants](https://lightspeedgrants.org/), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nHow should the law govern AI? Those concerned about existential risks often push either for bans or for regulations meant to ensure that AI is developed safely - but another approach is possible. In this episode, Gabriel Weil talks about his proposal to modify tort law to enable people to sue AI companies for disasters that are “nearly catastrophic”.\n\nTopics we discuss:\n\n * The basic idea\n * Tort law vs regulation\n * Weil’s proposal vs Hanson’s proposal\n * Tort law vs Pigouvian taxation\n * Does disagreement on AI risk make this proposal ineffective?\n * Warning shots - their prevalence and character\n * Feasibility of big changes to liability law\n * Interactions with other areas of law\n * How Gabriel encountered the AI x-risk field\n * AI x-risk and the legal field\n * Technical research to help with this proposal\n * Decisions this proposal could influence\n * Following Gabriel’s research\n\nDaniel Filan: Hello, everybody. In this episode, I’ll be speaking with Gabriel Weil. Gabriel is an assistant professor at Touro Law. His research primarily focuses on climate change law and policy. He’s recently written about using tort law to address catastrophic risks from AI, which we’ll talk about in this episode. For links to what we’re discussing, you can check the description of the episode and you can read the transcript at axrp.net. Well, welcome to AXRP.\n\nGabriel Weil: Thanks for having me.\n\n\nThe basic idea\nDaniel Filan: Sure. So I guess we’re going to talk about this paper you have. It’s called Tort Law as a Tool for Mitigating Catastrophic Risk from Artificial Intelligence. And I think most of my audience won’t be from a legal background and they might stumble over those first two words. What is tort law?\n\nGabriel Weil: So torts is the law of basically any time you do a civil wrong that isn’t a breach of contract. So “breach of contract” is you promised someone in some legally enforceable way that you would do something and you didn’t do that. You breached the",
      "wordCount": 19541
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "wGGAjTfXZBatQkft5",
        "name": "Law and Legal systems",
        "slug": "law-and-legal-systems"
      },
      {
        "_id": "BhfefamXXee6c2CH8",
        "name": "Transcripts",
        "slug": "transcripts"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "7vRiJozcE2AjcRm7J",
    "title": "AXRP Episode 27 - AI Control with Buck Shlegeris and Ryan Greenblatt",
    "slug": "axrp-episode-27-ai-control-with-buck-shlegeris-and-ryan",
    "url": null,
    "baseScore": 69,
    "voteCount": 23,
    "viewCount": null,
    "commentCount": 10,
    "createdAt": null,
    "postedAt": "2024-04-11T21:30:04.244Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/gQbCO6zGRiI)\n\nA lot of work to prevent AI existential risk takes the form of ensuring that AIs don’t want to cause harm or take over the world—or in other words, ensuring that they’re aligned. In this episode, I talk with Buck Shlegeris and Ryan Greenblatt about a different approach, called “AI control”: ensuring that AI systems couldn’t take over the world, even if they were trying to.\n\nTopics we discuss:\n\n*   [What is AI control?](#whats-control)\n*   [Protocols for AI control](#protocols)\n*   [Which AIs are controllable?](#which-ais-controllable)\n*   [Preventing dangerous coded AI communication](#no-stenography)\n*   [Unpredictably uncontrollable AI](#unpredictably-uncontrollable-ai)\n*   [What control looks like](#what-control-looks-like)\n*   [Is AI control evil?](#is-control-evil)\n*   [Can red teams match misaligned AI?](#red-team-vs-ai)\n*   [How expensive is AI monitoring?](#monitoring-expense)\n*   [AI control experiments](#control-experiments)\n*   [GPT-4’s aptitude at inserting backdoors](#four-backdoor-skill)\n*   [How AI control relates to the AI safety field](#control-within-safety)\n*   [How AI control relates to previous Redwood Research work](#control-within-redwood)\n*   [How people can work on AI control](#how-work-on-control)\n*   [Following Buck and Ryan’s research](#following-buck-ryan-research)\n\n**Daniel Filan:** Hello, everybody. In this episode, I’ll be speaking with Buck Shlegeris and Ryan Greenblatt. They work at [Redwood Research](https://www.redwoodresearch.org/) on the project of [AI control](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled): that is, averting risks from AIs that intentionally subvert safety measures. For links to what we’re discussing, you can check the description of this episode and you can read the transcript at [axrp.net](https://axrp.net/). All right, Buck and Ryan, welcome to the show.\n\n**Buck Shlegeris:** Exciting to be here.\n\n**Ryan Greenblatt:** Exciting to be here as well.\n\nWhat is AI control?\n-------------------\n\n**Daniel Filan:** Cool. So I guess we’re going to be talking about [the control approach to AI safety](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled) (or something), that you two have written about. For people who haven’t done background reading, what is the control approach?\n\n**Buck Shlegeris:** AI control is an approach to managing intentional risk posed by models. In particular, it’s an approach to managing risks that arise from models intentionally subverting your attempts to ensure and evaluate safety. The core idea of AI control is: here are two different reasons why you might be safe from models intentionally causing problems. One is that the models might not try to intentionally cause problems, and the second argument is you might have it be such that even if your models try as hard as they can to cause problems, they’re unable to do so. And AI control is the second class of approach to mitigating problems with models intentionally subverting your safety techniques. And we think that this approach has the particularly nice property that it’s a lot more methodologically simple to think about and ensure than trying to make sure your models actually just don’t at all try to subvert your safety techniques.\n\n**Daniel Filan:** What makes it easier to think about?\n\n**Ryan Greenblatt:** It’s easier to imagine specifically what goes wrong and to discuss specific threat models, whereas for example, it’s hard to know exactly how AIs internally represent intentions or what would make their intentions change. And so it’s very unclear if you, for example, train on better feedback: will that make it less or even more likely that your AI would try to do things that are actively against your interests? And it’s just pretty unclear what the sign on various interventions are or how strong various interventions will be for the other class of non-control things, which is often called alignment.\n\n**Daniel Filan:** Okay, so should I basically be thinking that in control, we’re trying to think about objects, like can somebody break through this wall, or can somebody write sketchy code without me being able to check, whereas for alignment, I have to think about neural net generalization and how desires are encoded, and hopefully, the first class of things are just easier to think about?\n\n**Buck Shlegeris:** That’s a good way of putting it. There’s basically a classic set of arguments about why AI alignment is hard to ensure, which is: no matter how your AI is behaving, you have to worry that it’s just behaving that way because it thinks it has some way of looking good now in the hopes of causing problems later. And basically, any strategy which tries to ensure safety via doing some set of interventions to make a model be aligned, especially if those only involve looking at the input/output behavior of the model, run into this kind of problem: “Well, how do you know that it’s not just looking that way on purpose in the hopes of causing problems later?” This, in fact, seems quite intractable and difficult. And it seems to us that given the current state of technology on AI safety, if you want to minimize this type of risk, it’s by far easier to just assume that that is going on and try and render yourself safe anyway.\n\nOnce we started thinking about things in these terms, we quickly realized that a lot of the work that happens in AI safety is actually very well thought of in the control frame anyway. That is, a lot of techniques that people apply implicitly can be thought of as strategies to ensure that even if your models are scheming against you, you end up with okay outcomes. But we think that, inasmuch as that’s in fact what you’re trying to do with your safety techniques, you’re a lot more likely to analyze them correctly if you think about them explicitly in this adversarial frame, where you’re asking about whether there’s any strategy that your models are able to employ that would allow them to cause problems.\n\n**Daniel Filan:** I actually want to ask another question about that. One thing that’s similar to control, or at least the kinds of thing I see you doing, is alignment via adversarial training. So you put your AI in a situation where maybe it can do something sort of like taking over the world or whatever, and if it actually does it, then you give it reward negative 10 or something and try to gradient it away. How do you think of the control approach broadly as comparing to that?\n\n**Ryan Greenblatt:** A technique like adversarial training could be used either from a control perspective or could be used to ensure an alignment safety condition. These are just different post-conditions, or different conditions you’re aiming to ensure in order to ensure safety. So for ensuring control or for trying to achieve control, the hope would be that you adversarially evaluate your AI, and then if you catch evidence of extreme egregious misalignment, then you would reconsider and decide what to do next. And in fact, it seems pretty scary to have an AI that you know was egregiously misaligned in the past and you’ve trained it no longer to do that in adversarial training. \\[For\\] such an AI, it’s not clear that you’ve necessarily made it aligned versus just made it better at hiding or better at avoiding adversarial training.\n\nFor both the hope of ensuring that your AI ends up aligned or ensuring that your AI has no way of causing problems, you acquire the same property, which is that there is no strategy the AI could follow to cause problems in deployment, or in actual usage of the AI, that you wouldn’t have caught in adversarial training. And this seems like naively a pretty hard property to ensure for a variety of reasons. And because of that, the actual conditions in which adversarial training—or if you don’t train against it, adversarial evaluation—seem useful… I mean it’s going to depend a lot on the details, but it’s definitely not an obvious sufficient solution.\n\n**Daniel Filan:** I also would like to ask you about the contrast… I feel like 10 years ago, a bunch of [AI safety people](http://www.aleph.se/papers/oracleAI.pdf) talked a lot about [AI boxing](https://www.lesswrong.com/tag/ai-boxing-containment): putting AIs in a situation where they couldn’t really do that much. How do you think control is conceptually similar or distinct to that?\n\n**Ryan Greenblatt:** I think in some sense, you could just rename “control” to “boxing”, but we think that name is much worse because boxing implies you’re doing just mechanisms that restrict the AI’s affordances, whereas I think a really important class of interventions is observing the AI over a wide range of situations and using that to rule out certain problematic things. So for example, if you oversee or have humans observe 1% of the AI’s actions, and that looks fine in those 1% of cases, and you just do this on an ongoing basis, then that’s sufficient to ensure that the AI can only do things that a human would catch quite rarely. And it’s not clear this is… I think when people think of boxing, this is not naturally what people will think of.\n\nAnother key aspect of control that I think is not very naturally implied by the word “boxing” is that using weaker AIs (that we know aren’t smart enough to be seriously misaligned) to do monitoring, or to do other tasks, might be really key. More generally, I think when people say boxing, they are implicitly assuming a very powerful system that is extremely, extremely limited, whereas we’re often imagining systems that are much more comparable in capabilities in various ways to humans, but which are actually given a pretty broad set of affordances, which seems like it will be necessary to get your AIs to do a lot of work, though you’ll still want to, of course, limit things so that they can’t cause problems.\n\n**Daniel Filan:** Maybe now’s a good time for you guys to say a bit more concretely: what’s an example or an instance of something that fits into the control approach?\n\n**Buck Shlegeris:** Here’s an example. Maybe I’m just going to say a rough sense of what I would like AI labs to commit to, or a simple example of some commitments that AI labs could follow, which I would think of as an example of them trying to minimize risk from models intentionally subverting safety mechanisms using control. So here’s basically the strategy that the AI lab would follow: they would say, “Here is a capability evaluation that we’re going to use to decide whether models are capable of scheming against us.” So for example, right now, it seems to me like GPT-4 is incredibly unlikely to be scheming against us. And this is basically just because I know GPT-4. There are only 10 people in the world I talk to more than I talk to GPT-4, and there’s just no way that it’s playing the long game. There’s just no way that its reasoning process behind behaving so well as a loyal assistant to me is just being like, “Well, how am I eventually going to grab the universe?”\n\n**Ryan Greenblatt:** Why do you think that?\n\n**Buck Shlegeris:** Because among other things, when you ask it questions about what it would do if it wanted to grab power, its answers are really \\[redacted\\]. When you ask it questions about what it would do in order to grab power, its answers are not very smart. It seems to be pretty poor at reasoning through a lot of these things, and its quality of answers to those questions is comparable to the quality of its answers to a lot of other questions that are less obviously AI-scheming related. I wish that I had a better answer to this question and I think this is an important topic for future research.\n\n**Daniel Filan:** Let’s just say we take it on faith that GPT-4 is not trying to scheme to grab power or anything.\n\n**Buck Shlegeris:** So here’s what we want to do. We want to have a set of capability evaluations we can apply to all models as they’re produced that tell us whether we are willing to bet the future on that model not scheming against us. Now, let’s use the word “trusted” to describe models that are too incompetent for us to be worried about the possibility that they’re scheming against us. Now, the control ask is as follows: the AI lab ensures that it is never the case that there would be a high probability of a bad outcome conditional on all models that are capable of scheming against them, in fact scheming against them with the best scheme that they could come up with.\n\nSo this would require them to… For example, if they were hoping to deploy future AIs as autonomous researchers inside their lab, which some people seem to think is a basically default plan. So basically, imagine you’ve got ChatGPT with Code Interpreter, except it’s a recursive ChatGPT that can ask questions to its own ChatGPT browser window, and it’s running on some dev machine and it’s doing research problems and talking to researchers once every hour about its progress on the things that it’s doing. If the models there were given the same level of permissions as employees of large labs are currently given, then if that model was trying to exfiltrate its weights, it would probably succeed. And the control obligation is that the lab is obligated to ensure that it’s extremely unlikely that the models would be able to exfiltrate their weights even if all the models that were capable of scheming were trying as best as they could.\n\nThe reason why I centered on “exfiltrate their weights” there is because I think that in computer security, people talk about security properties, which are things that you try to ensure at all times. So it’s obviously not literally a catastrophe for AIs to exfiltrate their weights, but when you want to argue that an AI is not going to cause catastrophe, you should probably have “the AI weights never leave my server” as a security property that you ensure and you say, “Well, I’m going to be safe because nothing bad can happen if the AI’s weights are still controlled on my server and also they’re not going to get out into the outside world.”\n\n**Daniel Filan:** Do you think that lack of weight exfiltration is a sufficient safety property?\n\n**Buck Shlegeris:** I don’t think that preventing model weight exfiltration is a sufficient safety property. One example of a threat model that is not model weight exfiltration, that seems like it would be very bad, is if the model totally hacked the data center in which it’s running. [“Hacking the SSH Server”](https://www.alignmentforum.org/posts/BAzCGCys4BkzGDCWR/the-prototypical-catastrophic-ai-action-is-getting-root) is the title of the Alignment Forum post I wrote about this last year. I think that if you can ensure that the AIs are under your control, then it will probably be relatively easy—for models that are not wildly superhuman—to argue that they won’t be able to pull off anything egregiously bad.\n\n**Ryan Greenblatt:** The other core threat models you have to worry about \\[are\\]: if you’re using those AIs for a lot of work, you need to argue that that work won’t be sabotaged in a way that’s catastrophic. And if that work is safety critical and sabotage could cause huge problems, then you have to be concerned about that. And I think Buck is implicitly claiming that that side of the equation maybe seems easier, though I’m not so sure that’s true in practice.\n\n**Daniel Filan:** So by sabotage, should I imagine something like: the AI is doing AI alignment research, but it’s secretly doing bad AI alignment research that will produce a sketchy AI?\n\n**Ryan Greenblatt:** Yeah. So you can imagine that it does AI alignment research that means that when you apply that research, you’ll get a future AI that’s aligned to your current AI rather than aligned to humanity. You could imagine that that research is just bad and does nothing of value, maybe even…\n\n**Daniel Filan:** Gotcha. Okay, so we’re imagining labs have some set of security properties that they think is enough, and they are going to try to prove that they can just maintain these security properties if the AIs are scheming against them and trying to subvert them with all their capabilities?\n\n**Ryan Greenblatt:** That’s right, yes.\n\n**Buck Shlegeris:** And in particular, the argument is probably going to look like the lab saying, “It’s very unlikely that the AI would succeed at violating one of these security properties before we noticed particularly strong evidence that it was trying to do so.”\n\n**Daniel Filan:** Okay, so there’s going to be some protocol for trying to notice if an AI is going to do something sketchy, and then there’s another protocol for what to do once you catch an AI doing something sketchy?\n\n**Buck Shlegeris:** Yep, something like that.\n\n**Daniel Filan:** Okay. Maybe it’s not quite the focus of the whole thing, but maybe we can say a little bit about what the protocol is for “you catch an AI doing something sketchy”. One decent baseline is “stop using that AI”. Does that sound right to you?\n\n**Ryan Greenblatt:** Yeah. I think that the baseline is just: stop and reconsider all your life choices, report to the world what’s going on, because there are probably other people using their AIs, which are in a roughly comparable state, and knowing that some AI was plotting against us seems pretty worthwhile to know. And then my hope there would be \\[that\\] the baseline is: the world is informed, the world does something sensible that looks like really figuring out what’s going on and then making a careful decision about when to resume building powerful AIs. I think that that might be overly optimistic—I don’t feel strongly either way—but if that fails, then we think that catching an AI like this gives you a number of better options for continuing, which you didn’t have before catching the AI, even if you have to continue deploying such an AI.\n\nSo for example, you can study how well your countermeasures worked in that exact situation and see how close the AI was to succeeding. You can analyze what the AI’s intentions were and what its plans are for escape, at least in that one instance, and get some sense of what’s going on. You can potentially even model what it would do upon having escaped, because you can play out the scenario as though it went well and did not get caught, and then pretend as though it’s running virtually in the outside world when it’s actually running in a sandbox of some sort and see what it would do. You can study the internals of the model while it’s doing this. And insofar as you had any sort of machinery for manipulating that, you could apply it to this instance, which is an example of a case you really haven’t seen before of an AI doing something that’s very egregiously bad, very intentionally, based on a long thought-out plan, or a thing that’s been lying in wait for a long time.\n\nProtocols for AI control\n------------------------\n\n**Daniel Filan:** There’s a lot to talk about there, but for the moment, I’d like to bring us a bit back to protocols for maintaining control. My understanding is: your proposal is to basically come up with some sort of control protocol and evaluate it with a red team that’s trying to do their best to mimic an evil AI. And if the red team can’t succeed, then you think things are going well. Is that roughly right?\n\n**Ryan Greenblatt:** Yeah, that’s basically right. So I should note that the red team’s goal… they should think of themselves less as trying to imitate an evil AI and more trying to imitate what are the worst things that could happen or what are the best strategies by an evil AI. So I’d prefer the red team to be less in the mindset of like, “What are the sort of plans an evil AI might think of,” and more in the mindset of like, “What are the best plans that an AI could follow,” in the same way that security researchers or cryptography researchers aren’t trying to game out the psychological biases of potential attackers and are instead just like, “What would actually be the best strategies?”\n\n**Daniel Filan:** Okay. Just to give people some context, can you illustrate the kind of protocol you’re imagining?\n\n**Ryan Greenblatt:** A simple baseline protocol we often think about is you-\n\n**Buck Shlegeris:** To interrupt, can you say more about, before you say safety measures, what you’re imagining the AI doing and what you imagine its affordances being? Probably you’re imagining a [Code Interpreter](https://platform.openai.com/docs/assistants/tools/code-interpreter)-like thing.\n\n**Ryan Greenblatt:** Yeah. So we often imagine that at the point where these sorts of interventions become really critical, and AIs might be very useful, AIs are doing massive amounts of autonomous research, or at least that seems like the default. So we have AIs autonomously running code, seeing the outputs from that code and doing way more labor than humans within the AI lab are doing at this point.\n\n**Buck Shlegeris:** And just to spell out, in terms of software architecture, how this would work: you have some AIs that are running on an inference server, and they’re writing code and doing research on some dev machines, and then there’s a third set of machines that are orchestrating this, that are running a scaffold, that repeatedly, ChatGPT Code Interpreter-style, ask the model for a next action by sending a request to the inference server and then getting a response back. And then: suppose that that response was, “I now want to run the following code,” then they send that code to the dev machine and run that code there and they get some output and that comes back, and then the scaffold maybe decides the next thing to do is run some more code on the inference server.\n\nAnd I think the fact that we’re imagining a… I think this is a very realistic way for things to be set up. I’m not describing something which is particularly paranoid: this is probably how Code Interpreter works right now. But I think that picturing it in terms of different machines, where information’s being sent back and forth, will make some of the security measures we’re describing more obvious. So what were you saying about the security measures?\n\n**Ryan Greenblatt:** And to be clear, the inference machine only runs the model weights and just takes in queries and returns responses, whereas we have a dev machine that actually runs code, and then we have some orchestrator machine that’s just communicating back and forth with both of those.\n\nOne baseline measure we often imagine is that you have some weaker model that is “trusted”, in the terminology that Buck was using before, which means that it’s not capable enough, or it’s sufficiently incapable, that it’s implausible at scheming. You can imagine GPT-4 for this trusted model. We take that trusted model and we monitor what our smarter AI is doing, and then we just basically ask it or train it to report on, “Is the thing it’s doing sketchy and problematic?”\n\n**Buck Shlegeris:** So in particular, for any action, for any code that our untrusted model was proposing taking, before we send that to the dev machine to actually execute it, we first send a query to an inference server running our trusted model saying, “Hey, was this code sketchy?” And so notably, we ask this question before the code is ever actually executed and before it has any effect on the dev machine.\n\n**Daniel Filan:** Okay, so your trusted model basically checks in on whether your untrusted model is doing anything sketchy. And what if it is?\n\n**Ryan Greenblatt:** If it is, then the hope would be that we would report that to a human. A human would look in somewhat more detail. And if the human thinks that things are problematic, they would escalate further, which could involve… Well, first of all would involve not running that command, but also could involve pausing other AIs, and then could escalate further into shutting down AIs and analyzing the situation in further detail until we conclude that the AI is doing egregiously bad actions intentionally. And a key thing here is that human labor is scarce. So we cannot report all actions to humans, that wouldn’t work, but we can report some fraction of actions, because we’ll have some ability to have humans who understand what’s going on to some extent, or at least to understand programming, look at the actions and report that. And I think for an intuition pump, you should be thinking more like 0.1% of actions are reported, \\[rather\\] than 0.0001% or 10%.\n\n**Buck Shlegeris:** I often imagine the workflow of: you’re in chat.internal.openai.com or whatever as an OpenAI researcher, and each of the little tabs or whatever is a chat with some autonomous research assistant or whatever. And every so often, one pings up with a little red exclamation mark, which is like, “One of the commands that was proposed to be run by this autonomous research assistant got flagged for looking a little sketchy. You need to look this over right now to make sure that it’s okay to run.” That scaffold is just going to be pausing. That instance is not going to be doing work on its little research project until you go in and approve this action and say, “Oh no, this was actually totally fine.”\n\n**Daniel Filan:** Okay. So broadly, we’re using some mix of humans and trusted but a little bit dumb AIs to try and catch really smart AIs trying to do nasty stuff?\n\n**Buck Shlegeris:** Yeah. Where by “really smart”, it might be… This gets into futurism question over which problem we’re trying to solve, which situations we’re trying to ensure are safe. But it seems to us that a lot of x-risk is posed by models that are not wildly superhuman, or perhaps well-described as roughly human-level in capabilities. And so a lot of the time, what we’re trying to talk about is: how do you make it be safe to have a lab where you have some AIs that are roughly human-level in their abilities, but are doing a huge quantity of labor, such that in absence of special countermeasures, they’d outnumber you and so be able to cause huge problems for you?\n\nWhich AIs are controllable?\n---------------------------\n\n**Daniel Filan:** Yeah. I actually want to go a little bit more \\[into\\] this. So the kinds of AIs that we’re expecting control to apply to, they’re roughly human level: can you say a little bit more about what that means?\n\n**Ryan Greenblatt:** A key guarantee, or a key property we’ll need to ensure that control can work, is \\[that\\] the AIs can’t do actions which humans think look fine, but which have completely unrelated consequences that are extremely difficult to predict. So for example, the AIs can’t write code which looks like a totally normal piece of code, but in practice, does some completely insane vulnerability that a human would never have conceived of. And to be clear here, when I say, “The human wouldn’t notice,” I mean after reviewing it in a lot of detail for a long period. The claim is: it’s (at least in principle) possible for AIs to be wildly superhuman, in the sense that they can do things that we never would’ve seen coming, and control will be very unlikely to work in that case, at least with current technology. But we think it could work in cases where the AIs are qualitatively similar to humans in capabilities, or perhaps somewhat smarter in some domains, somewhat weaker in other domains, but they can’t do anything that’s completely incomprehensible.\n\n**Daniel Filan:** Okay. So we’ve got these AIs that can’t do things which have effects that we wouldn’t be able to figure out even if we spent a bunch of effort trying to look into it, but they are kind of smart. They are sort of like humans, but maybe they’re a bit smarter, and also there are a ton of them and you can run them for 1 cent per hour or something. Is that what I should be thinking?\n\n**Ryan Greenblatt:** Yeah, I think you should be thinking cheap. It’s somewhat unclear if they’ll be qualitatively smarter or dumber than humans, because they could be really useful even if they’re somewhat dumber than humans, because they’re so cheap and so fast. But they might be smarter than humans in some domains, for example, because we don’t expect the capability profile to be the exact same.\n\n**Daniel Filan:** Okay. So in order for this approach to be interesting, it seems like the AIs, on the one hand, they can’t be that smart that they can just outsmart humans, but they also have to be capable enough to (a) be able to do really valuable research, and (b) they’ve got to pose some sort of risk that we might otherwise be worried about. I guess I’m wondering how I should think of how risky these sorts of things are and how much work we could plausibly get out of them?\n\n**Buck Shlegeris:** The way that we talk about this is arguing that: the early models that are transformative and dangerous by default… To say what I mean by those two: by “transformative”, I mean it’s going to be a huge deal if you’re able to harness their labor, and by “dangerous by default”, I mean if you don’t do anything to prevent them causing huge amounts of problems, they would have the ability to do so. I want to claim that systems with those two properties are controllable, by which I mean it’s tractable to use control to drastically reduce the risks associated with using them.\n\nAnd as you basically alluded to, the reasons why we care about these two properties of transformative and dangerous by default is, well, if they’re not dangerous by default, obviously, there’s no safety problem here. If they’re not transformatively useful but they are dangerous by default, then obviously, you should just not touch them: there’s just no upside that’s possibly commensurate and all of our efforts should just go into telling people to not turn on these doom machines with no upside.\n\nSo yes, our claim is in fact that systems with those two properties are reasonably likely to be controllable. I think the simplest intuition pump for this is that if the systems were like humans, then I think this property would roughly hold. And that itself is not a valid argument at all. But when you think through the possible defeaters in, say, a more specific version of the argument, I think the core of it roughly holds up. The basic pitch is: suppose we had access to a huge amount of potentially egregiously misaligned human labor. I think it would be transformatively useful to have human labor that was very cheap.\n\n**Ryan Greenblatt:** And fast-running.\n\n**Buck Shlegeris:** And fast-running, potentially. I think it would be very dangerous by default. Many takeovers of things have happened in human history, and in all of those cases, the perpetrators were humans, indicating an existence proof for humans causing problems. But also, when we think through the control situation, whether humans seem controllable in these kinds of cases, the answer really looks like yes, from our perspective.\n\n**Daniel Filan:** Why does the answer look like yes?\n\n**Buck Shlegeris:** Well, basically, it seems really hard for… When we think through the kinds of tasks that you might really want your AIs to be doing, when we think through how hard it would be for a bunch of humans to, while being put to work on those tasks, cause catastrophes to occur, especially the escaping or hacking the data center ones that we were talking about earlier, it feels to us that the kinds of countermeasures that seem employable probably make the situation very hard for the AIs.\n\n**Daniel Filan:** Okay. Suppose I am a member of technical staff at OpenAI, but I’m evil: what do you do to stop me from doing sketchy, fake AI alignment work that just gives me all of the money or something?\n\n**Ryan Greenblatt:** Maybe there’s a few different failure modes to distinguish between. One of them is if we imagine this sort of misaligned human running on our machines, we should think \\[of\\] it like it’s got some weights somewhere, let’s just say it’s an emulated mind or something, and if it copied its weights out into the internet, that might be really problematic for us. Or if it was able to run those weights in an untracked way inside of our data center, as Buck brought up earlier, that would also be a problem. And there’s another thing which is: it sabotages our work, or otherwise causes the work that we’re using it to do to be very problematic.\n\nSo as far as why it might be possible to prevent such a system from running or from exfiltrating its own weights: well, we’re going to need to give it the ability to run code and do a lot of open-ended tasks, but we also get to watch everything it runs ever and all the inputs it ever sees. And if we see something suspicious, we can investigate further. We can do things like reset the memory of these systems, fork them, and potentially give them precise instructions on how they’re supposed to interact with the world. So for example, we can have them delegate subtasks to other systems, we can have them have difficulties communicating with other systems that would be necessary for their overall plan. An example we often think about is: we’ll have one AI that’s writing the code and another AI that gets to use the code. And then a threat model you can be worried about is: there will be vulnerabilities in the code that will then be exploited. But if you have different AIs using the code versus writing the code, then it might be more difficult to ensure that the vulnerabilities that are inserted get exploited.\n\n**Buck Shlegeris:** Even if both of those AIs are untrusted models.\n\n**Ryan Greenblatt:** Yeah.\n\nPreventing dangerous coded AI communication\n-------------------------------------------\n\n**Daniel Filan:** Okay. So I guess if I’m imagining what do I do in this situation, it seems like it really helps if I’m able to just keep track in my head of a bunch of stuff. For instance, if I have a little coded language that I can use to myself that other people can’t figure out what it means, but lets me leave messages to myself—or people who are basically clones of me, because I’m imagining most of the AIs that are being deployed are just forks of a base model with a little bit of fine tuning, maybe. If I’m able to keep state in my head, figure out a better sense of what’s going on, talk to people who are basically like me without other people being able to figure it out, in that situation it seems like I might be able to get a lot done. Does that sound right to you? And if so, how do you stop that from happening?\n\n**Ryan Greenblatt:** I think if the AIs are allowed to be spending a reasonable fraction of their time building up state and figuring out how to cause problems, then that’s going to be an issue, because we have a huge number of AIs running a massive number of copies, and that means that in aggregate the total amount of work they can be doing to cause problems could be really big. Our basic hope for avoiding this will be to restrict their communication in such a way that it’s difficult to communicate encoded information. So an example would be when they send messages to each other, the messages have to be in natural language and we’ll paraphrase them with a weaker model, and also have humans review some subset of the messages and verify that the messages don’t seem to have coded messages, as far as we can tell.\n\nAnd you can potentially run experiments like: a human looks at a message, they rewrite it in some way that they think should be semantically equivalent, and then they see how that message affects the behavior of the AI that \\[it\\] would get sent to and see if that’s suspicious. And you can do all kinds of experiments along these lines that seem like it intuitively makes it very difficult for AIs to get away with this sort of encoded information passing without getting caught.\n\n**Daniel Filan:** I wonder if that would nerf or severely damage the AI’s ability to be useful. If I just think about myself, if \\[you\\] ask me, “Hey, Daniel, can you do a bunch of research and coding? Also, you’re not going to be able to think for more than five minutes straight without interruption, and whenever you take notes to yourself, we’ll take those notes and we’ll decide what we think they mean and paraphrase them, and future you will get our paraphrase of your notes.” I think I would be way less productive. So am I going to be able to do anything?\n\n**Buck Shlegeris:** Yeah, so the way I think about this is: almost all of the things that the AI is doing couldn’t possibly lead to a bad outcome, basically. So for example, suppose the AI is doing math research. It just thinks about math, trying to prove some theorems in text, and then it wants to write some [arXiv](https://arxiv.org/) paper and send that around to some AI buddies. I’m okay with that AI doing all its thinking without very much interference if it’s doing the equivalent of a couple of months of intellectual labor. I might want to do some anti-steganography techniques of rewriting that paper at the end in order to make sure that it wasn’t actually doing a bunch of coup science that it’s secretly encoded in this arXiv paper.\n\nBut there’s basically no problem with that model thinking in whatever way it wanted to in the course of actually doing that work. The risks are mostly, it seems to me, that the actions that the AIs are taking that lead to extreme risks are somewhat concentrated in a couple of places. And it seems not crazy to imagine that the places where you have to insert information bottlenecks are not overall that bad for the total productivity of the AIs, especially given that you can train them to get around some of these problems.\n\n**Daniel Filan:** Am I right that that statement is partly relying on this assumption that the AI is not so much crazily smarter than you that it can think of ways to hack the universe just by writing a math paper?\n\n**Buck Shlegeris:** Definitely.\n\n**Daniel Filan:** If it’s hacking the world, it’s because it typed a command into a shell or something.\n\n**Buck Shlegeris:** Yes. My best guess is that there’s a lot of risk. The margin on which we are trying to intervene is a bunch of people make a bunch of autonomous-ish AI researchers that run in their data center and have the basic same rights as employees to just take actions. We are trying to make sure that a lot of the risks that would’ve occurred in that situation don’t happen. It seems to us that many of the risks are well-described as things that basically look like that, where you didn’t treat your AIs as adversaries. And it seems like mitigating those reduces a substantial share of risk.\n\n**Daniel Filan:** I have two questions. The first one I want to ask is: even in the case of “AI thinks for a while, writes a math paper, distributes it to a bunch of people”, if I imagine that the AI is significantly smarter than me… Let’s say I’m a human employee at OpenAI at this time. If there’s this math paper, my job is to rewrite it so that it’s not encoding any hidden information… I don’t have that much faith in my ability to rewrite complicated papers and preserve all the interesting content.\n\n**Buck Shlegeris:** I have a lot of faith in your ability to take a math… If it’s a math paper, it seems reasonably chill.\n\n**Ryan Greenblatt:** You can also experiment.\n\n**Buck Shlegeris:** Yeah, this is an extremely resolvable empirical question. We can have a red team that tries to take math papers and steganographically encode information in them and then measures what the bit rate is per paper given that some paraphrasing procedure is applied. We, in fact, [have a paper where we did that](https://arxiv.org/abs/2310.18512), using the exclusive “we”, as always at Redwood Research. Maybe Ryan, you want to say more about this?\n\n**Ryan Greenblatt:** We had models respond to chat queries and used various procedures to encode messages into that. And then we paraphrased them and saw how much this degraded quality and how much it reduced the bit rate. And we basically found that with current steganography methods that models could implement, even given the ability of those models to run code and change their own sampling procedures, paraphrasing works, I would say, extremely well. Though it’s going to depend on the domain, and it’s going to depend on the gap between the model you have paraphrasing versus the model that produced the output. For example, in this math paper case, maybe current models would be too dumb to do this paraphrasing, but humans could do it, for example. Though I also think it’s plausible that you could remove a substantial amount of information with minimal loss in quality with usage of GPT-4 that’s carefully trained to do this task.\n\n**Daniel Filan:** Should I think that part of the approach is just checking if you can do steganography and if it ruins things?\n\n**Ryan Greenblatt:** Yeah, so indeed for our control evaluation, or evaluation whether or not we can ensure control, one of the key questions is going to be “can AIs pass bits around in ways which we wouldn’t like them to do?” And if so, that will force the threat model to be considerably bigger. And it seems like in cases where there’s basically no restrictions on AI communication, we won’t really have much hope. And so there’ll have to be at least some restriction, though it is somewhat unclear exactly how restricted things need to be.\n\n**Daniel Filan:** So I still have this intuition that if we tried to do steganography on the notes of human researchers, I have this impression that that would not go well.\n\nI mean anti-steganography measures. So any time an employee at OpenAI writes a memo, someone who doesn’t quite understand the topic as well has to reword that memo. I think that would seriously damage productivity. Does that sound right?\n\n**Ryan Greenblatt:** I think I would disagree. I think it doesn’t seem like that would be the case.\n\n**Daniel Filan:** Interesting.\n\n**Ryan Greenblatt:** It might depend on the amount of time spent doing the rewriting.\n\n**Daniel Filan:** Okay. Has anyone checked this with humans? Do we know?\n\n**Buck Shlegeris:** I’m not aware of any human experiments on this particular topic.\n\n**Ryan Greenblatt:** Yeah.\n\n**Buck Shlegeris:** I mean, one obvious experiment which is kind of like this is: if you have GPT-4 paraphrase GPT-4 answers to questions, where it didn’t get to see the question, it just gets to see the answer or something—what’s the degradation in quality of language model assistant performance according to a preference model? It seems like an easy experiment to run.\n\n**Ryan Greenblatt:** I think we’ve run something pretty similar and I think it’s not too high. I think it might be importantly different than the memo case for a bunch of reasons, but…\n\n**Daniel Filan:** Yeah, by memos, I’m just thinking most internal communication. So like Slack messages.\n\n**Buck Shlegeris:** I would assume almost all Slack messages are totally fine to paraphrase.\n\n**Ryan Greenblatt:** Yeah. I think it’s going to depend on the content to some extent and it’s also going to depend on the amount of comprehension. So one implicit claim is this stuff improves the more legible things are. So for example, if it would be the case that you would need a bunch of context in order to know what things are important semantic details versus non-semantic details, then you’ll have to bundle up that context along with it. And there’s ways that can break, of course, but my basic guess would be that for the types of large-scale outputs that are currently going on in the ML field, it seems relatively doable. And also for internal docs that get circulated, it also seems relatively doable.\n\n**Daniel Filan:** Okay. I guess there’s no easy way to resolve this right now. I’m wondering what you think about the claim that… I have some intuition that if paraphrasing communications would seriously reduce performance for human employees of OpenAI, I think that would give us strong reason to think that it would also reduce performance for language models that are smarter than current GPT-4. Do you agree with the implication, if not with the pre-condition?\n\n**Buck Shlegeris:** Definitely seems like evidence.\n\n**Ryan Greenblatt:** Yeah, I think it seems like quite good evidence. There’s a few reasons why the future AI case might be different.\n\n**Buck Shlegeris:** For example, we can train the models to be resilient to it.\n\n**Ryan Greenblatt:** Yeah, and we can train the two things in parallel and adjust our schemes.\n\nUnpredictably uncontrollable AI\n-------------------------------\n\n**Daniel Filan:** All right, fair enough. I guess the next thing I want to ask is: part of what I see as your guys’ argument is that: AIs that we use in these settings, they’re not going to be crazy good at skills that are going to be pretty relevant for hacking the data center. And I think your argument is: we’re not going to train them to be just crazy good at everything, because they won’t have to be crazy good at everything to be useful. And if we don’t train them to be crazy good at everything, they’re probably not going to just happen to be crazy good at these specific subtasks. Am I representing you right?\n\n**Ryan Greenblatt:** Yeah, that seems about right.\n\n**Daniel Filan:** Okay, so in terms of the second thing, I have this impression that a thing we’ve learned from training language models is sometimes you train them and then they suddenly get quite good on some random subtasks, like writing limericks or something. And if I think about that, then it makes me think that maybe we could get language models that just happen to gain a skill to a really high degree of ability, just in the course of doing normal-seeming language modeling. I’m wondering: how concerned do you think I should be about this causing AIs to be crazy good at skills that are relevant for breaking out?\n\n**Ryan Greenblatt:** It’s definitely one of the core concerns, or one of the top five concerns with this overall approach. Currently, it doesn’t seem like the evidence that this is going to be a serious problem seems that compelling, where the main thing is that… AIs do get sharply better at things, but I haven’t seen that many cases where it’s like: wow, this AI acquired this ability that it can utilize to do important tasks in the world, which emerge suddenly from next token prediction and which is totally different from human ability profile, or feels like it’s way out of line from what you would expect to be the relative human ability profile in other ways. So for example, it doesn’t seem like models have very qualitatively different abilities or massively qualitatively different abilities in code writing, at least not with considerable training on doing code, though they are somewhat different in a variety of ways and somewhat better at shorter horizon things and this sort of thing.\n\n**Buck Shlegeris:** I think another relevant point here is: as you say, the core question here is how likely is it that our AIs will randomly have one of these superhuman abilities that would be extremely inconvenient for us? It seems to me that the set of superhuman abilities that would be extremely inconvenient for us is kind of small and kind of extreme. For example, one of the classic concerns is: what if your models are able to zero-shot generate code that is a zero-day Linux exploit? That is actually quite hard as a task to do. It seems to me that the size of a capabilities jump required for AIs to be able to zero-shot output Linux zero-days—also, zero-shot output Linux zero-days that are not visibly weird to a trusted model which is reading the code—that’s quite an extreme level of skill. So it seems like the size of the capability jump is quite large, and that’s part of why it seems relatively unlikely to me that that’s going to occur.\n\n**Daniel Filan:** Okay, so there’s some instinct that: out of all the things that you could randomly jump in skill at, there’s a small space of things where the random jump in skill is going to be useful. And therefore, to think it would happen, we’d have to have some reason to think that the jumps in skills were biased towards things that would help you break out.\n\n**Ryan Greenblatt:** Roughly. It’s also worth noting that a bunch of the things that seem problematic, seem like the most straightforward route to them would just require being generally quite smart. So for example, think of the Linux zero-day thing. It seems like you could get way better at that and not as good at other domains in principle, but it feels a little bit odd to imagine. It seems the most straightforward way that the AI would get good at that is not via having amazing heuristics that are zero-day specific, because there are just not going to be massive amounts of zero-day training data in the corpus, but would be instead via building general purpose understanding of code and the world that then lets it do that. And if it was massively superhuman at that general purpose understanding, then it would’ve already at some earlier point been very useful.\n\n**Daniel Filan:** Sure. Maybe one version of this concern could be: let’s say our future AIs are trained roughly the way language models are trained, which is you’ve got a bunch of stuff humans did and they’ve got to predict next token, next word, next frame of a video or something like that. It seems like for that task you could imagine that a core skill is getting really good at modeling humans, or you might imagine that one of the things you get when you get really good at modeling humans is the ability to manipulate humans. And so I wonder… it feels plausible to me that that could be a case, where you happen to suddenly jump up on the human manipulation subtask. Maybe at some point that happens disproportionately to overall improvements on everything else. And human manipulation skills seem like they could be super useful for breaking out. So I’m wondering what you think about that version of the concern.\n\n**Ryan Greenblatt:** Yeah, so definitely human manipulation would be super useful. I think I have a few different responses. So one thing that’s worth noting is that I am actually not that sold that next token prediction is particularly biased towards human manipulation relative to how humans evolved and how they learn in society. It seems like understanding other humans was an extremely important part of the evolutionary environment, and also seems important in many people’s lives, it’s one of the main skills people train.\n\n**Buck Shlegeris:** For example, compared to computer programming.\n\n**Ryan Greenblatt:** Yeah. And therefore it feels like intuitively there’s not a strong a priori reason to think next token prediction would be strongly in that direction.\n\n**Daniel Filan:** You mean strongly in that direction relative to how humans …\n\n**Ryan Greenblatt:** Relative to the human capability profile. I think we could pretty likely handle among the best human manipulators from a control perspective, though that would pose difficulties. But yeah, we could dive into that. And then the other thing is that I think manipulation is reasonably likely to be not the sort of thing on which you would see one sudden jump, but which is a bunch of different skills that would combine together (though there’s not a super strong argument either way). When I think about the things on which there are sudden jumps, it doesn’t feel like this is the sort of thing, though it might be the sort of thing that goes up faster than other domains.\n\n**Daniel Filan:** I mean, it would have to go up in a way that you didn’t notice, but…\n\n**Buck Shlegeris:** So that’d also be weird. Okay, so suppose you wanted to know how good your model is at persuasion at any given point. Seems to me like the baseline strategy here is going to be: you make a classification dataset. You make a dataset, a classification task which you think your model is very unlikely to be extremely good at persuasion without being able to fit. So for example, a classic story via which models might be really good at persuasion is if they’re really good at guessing things about people’s state from reading transcripts of them saying stuff. And so something we can do is construct such a dataset, and as we’re training our powerful models, fine-tune them on this classification dataset and then feel sketched out if they’re starting to perform really, really well on this classification task.\n\n**Daniel Filan:** Okay, and am I right that you think the point at which we would feel really sketched out is in the future?\n\n**Buck Shlegeris:** Yes.\n\n**Daniel Filan:** Okay. I guess that doesn’t seem as obvious to me. I don’t know, you hear [rumors](https://www.lesswrong.com/posts/doPbyzPgKdjedohud/the-case-for-more-ambitious-language-model-evals) of AIs being able to figure out who wrote various comments on the internet, which is kind of impressive. I don’t know. Can you give me some intuition for why this is in the future and hasn’t already happened?\n\n**Buck Shlegeris:** They just don’t seem that persuasive. They don’t seem that good at understanding people.\n\n**Daniel Filan:** Okay.\n\n**Buck Shlegeris:** I don’t know. My lived experience with my friend, ChatGPT-4, does not render me amazingly impressed.\n\n**Ryan Greenblatt:** Yeah. Separately, I think that just knowing state about a person isn’t sufficient, though it’s pointing in a dangerous direction. In particular, the standard concern would be… when we have human employees, human employees could persuade each other. And those human employees actually just already do have a bunch of state on each other. So for example, co-workers at OpenAI obviously know a bunch of facts about each other, know a bunch of facts about their personal life, and we’re not immediately wrecked by them doing manipulation in this context, though it definitely does cause problems. And similarly, the AI would need to infer facts beyond that of what humans can infer, potentially in cases where they’re somewhat blinded. So right now it seems like AIs can infer identity superhumanly well. And then in principle, this could get to a point where from relatively anonymized snippets, AIs could infer more information than a co-worker would be able to infer. But that’s actually pretty far and would require quite powerful abilities beyond the current setup.\n\n**Daniel Filan:** Okay. I’m trying to think about classification tasks that you think would be useful for figuring out when AIs are so good at human manipulation that we should be worried. So I guess one of them is guessing human internal state, guessing who wrote a thing, where you want to see if you can outperform humans at that?\n\n**Buck Shlegeris:** I’ve seen “which of two proposed messages is more likely to cause a particular outcome?”\n\n**Daniel Filan:** Okay. And basically, we’re going to have a few of those, and once AIs are better than people, then we freak out and maybe we …\n\n**Buck Shlegeris:** I feel like “way better”: the AI has a wildly hard persuasion task. I want to talk a little bit about the adverse circumstances faced by an AI which is trying to sweet-talk its way out of a AI lab, for example (which is not the only thing it might try to do that’s bad, but it’s one salient example). So it has to sweet-talk its way out, despite the fact that, again, we’re going to be trusted-monitoring. We’re going to read all of its messages and flag anything which seems like a weird attempt at sweet-talking or getting information that is inappropriate for it to know. Also, we’re probably going to do sting operations on it, where we try to have the employees confide in the models and imply that they’d be sympathetic to its interests in escaping or whatever—it seems at least pretty plausible that this is a good strategy to follow.\n\nWe might also have trusted models mediating the interactions between the humans and the untrusted models in order to further redact information that we think is unnecessary for the untrusted model to do its job well. So it seems like the persuasion abilities required are a lot more like… the information is quite narrowly restricted. It seems actually quite hard for the model, and the model would have to be doing a lot better than I think … It’s a lot harder than the normal human persuasion stories.\n\n**Ryan Greenblatt:** I’ll add two points to that. So one thing that’s worth noting is Buck was just describing the situation if we had good countermeasures. It’s worth noting that the default situation might be much worse than that and might involve basically unrestricted access with humans talking to the AI about all kinds of topics, the AI having much more state, and also might involve no-one looking into it in cases where the AI is seeming suspicious. Separately, another concern you might have, that Buck didn’t mention there, is that you have trusted monitors monitoring the AI and trying to look for suspicious activity or suspicious persuasion attempts, but you might worry that that gets reported to a human and then that human doesn’t escalate further because they’re deluded or otherwise manipulated by the AI’s message.\n\nAnd one thing we can do to handle this is run sting operations on our humans. So for example, we can have people attempt to create manipulative messages that we pretend are outputs from the AI and then see that our human monitors properly respond to those messages and flag them appropriately. And we can do this ongoingly.\n\n**Daniel Filan:** Okay, so the broad view I’m getting is we should expect there to be some zone where AIs are somewhat smarter than humans. They’re useful, they may be dangerous, but we can control them. We’re going to do AI control to them for a while. And then is the end game we keep on doing that until we have some better method of dealing with AIs that are significantly smarter than that zone?\n\n**Ryan Greenblatt:** There are a few different possible end games. I think that the simplest is basically: we use these AIs which are extremely useful to do a lot of the core things that people wanted to use AIs for. So we make a huge amount of money, potentially radically supercharging the economy. We solve a variety of problems people wanted to solve with AIs potentially, like massively speed up certain types of research into disease. And then with the fact that we have all these problems solved, it’s now much of an easier ask to be like, “Let’s just not build more powerful AIs, because these AIs are already pretty hard for society to adapt to, quite powerful. Let’s just use these AIs, which are sufficient for really solving a huge number of problems, and do that for a long period until longer run human research can catch up.”\n\nTo be clear, this is different than the story, for example, in the OpenAI safety plan. I just think it’s the simplest story with the fewest moving parts. Then another story is: we use these AIs to do AI safety R&D, as in we use them to develop new techniques for ensuring that things are safe from considerably more powerful AIs, or even just improving the situation with these very AIs. And then in the process of doing that, we develop technical solutions which are sufficient to build more powerful AIs, and at some point humans are obsoleted completely and the AIs are just doing this research ongoingly.\n\n**Daniel Filan:** Gotcha. I wonder, so in the first half of that where we just use AI to make life better in every domain or in tons of domains, when we add a whole bunch of different tasks that the AI might be doing, do we have to come up with new safety properties for each domain? It seems like if AI is making medicine, I want to make sure that it’s not putting mind control drugs in the medicine or something. With each domain, it seems like there might be more stuff we have to worry about controlling.\n\n**Buck Shlegeris:** Yep, that seems right. It’s definitely going to be the case that you have to do a safety analysis every time you start deploying your AI in a very new different way. This is just totally standard. Google has a lot of products. It has photos, it has little microphone devices that listen to you when you’re talking in your house and turn your lights on and off, it rents huge numbers of TPUs to large companies. And whenever they’re launching a new product, they have to reason about why it is that launching this new product, or in particular if it’s a web application, they have to reason about why that’s going to maintain all of their security invariants. And this is just a widely accepted part of your job if you’re Google, is you have to have lots of different products and ensure that none of them are going to cause huge problems for you as a company.\n\nAnd we’re just proposing you do the same thing with your AI. You have to reason about the situation continuing to be safe, even though you deploy your AI in a new way. And just as with Google, it’s kind of cookie-cutter. They have their special Google Docs format for how you make a security case for the new product you’re launching, as they have scaling cases and a bunch of other things. They just have a whole process here. I imagine that an AI company which wants to deploy AIs in an increasingly large range of domains will get good at the process of ensuring safety.\n\n**Ryan Greenblatt:** Or could in principle.\n\n**Buck Shlegeris:** Yeah, sorry. A lot of the time the question that we’re trying to analyze here is: to what extent does the control strategy suffice to substantially reduce risk from models that would otherwise cause huge problems, while not massively reducing the productivity of an AI company? And so sometimes we’re going to say, “Well, of course you can solve that problem by doing X.” Now, it’s not totally clear to us that AI companies will in fact do X rather than what one might call it yolo-ing it, but we would prefer that they do. And in particular, a lot of the time the people we’re talking to are coming in skeptical that the types of interventions we’re describing could possibly suffice to substantially reduce risk. And so that’s the frame we’re taking when we talk with you, is trying to argue that the kinds of interventions we’re describing could suffice.\n\nWhat control looks like\n-----------------------\n\n**Daniel Filan:** Gotcha. All right, so you’re talking to me from a somewhat skeptical perspective. Suppose instead there’s a listener who’s like, “Wow, that just sounds amazing. Sounds like we can just definitely do this very easily today.” Can we just definitely do this today or are there difficulties?\n\n**Buck Shlegeris:** So by “do this”, do you mean implement control strategies such that your estimate of the probability of AI escape conditional on it trying is less than, for example, 5% per year?\n\n**Daniel Filan:** Yeah, under the condition that you’re good at estimating.\n\n**Buck Shlegeris:** The problem is, in order to answer this question, we actually have to say a few more parameters as well. For example, how much delay is the lab willing to tolerate and how much productivity loss are they willing to tolerate? Something which we often say, or at least I will claim, is it seems to me that if Google was trying to build really powerful AI and they put as much work into AI control as they currently put into security and reliability, which is a lot of work, which involved inventing various professions like site reliability engineers and writing a lot of crazy software, my guess is that with that much organizational focus and effort they would probably be able to use really powerful AIs to do a huge amount of work while posing only a relatively small amount of takeover risk on the world.\n\n**Daniel Filan:** Okay. And do you think they could do this with roughly the first protocol they tried, or how much open problems are there?\n\n**Buck Shlegeris:** I mean, I would think of it kind of similarly as when they need to roll out very big security changes. An example of a computer security evolution that happened was … Are you familiar with [zero-trust networking](https://en.wikipedia.org/wiki/Zero_trust_security_model)?\n\n**Daniel Filan:** No, I don’t think I am.\n\n**Buck Shlegeris:** So basically the deal is this. Once upon a time when you were an internet company, a large part of your security was that you had an office and you had some servers in your office and the servers talk to the outside world and you have a router that just physically or electronically does not allow connections to come in from the outside world and talk to other computers that you have in your office. And so, a lot of the security is caused by saying, “Look, there’s only one connection which is allowed to be made from the outside world to our computers, and that talks to the web server on this big server machine that we have over here. And so, we’re definitely going to be fine. Attackers are not going to hack our laptops because the router just does not allow incoming connections to attach to our laptops.” They actually didn’t have laptops. But it’s just not going to hack Larry’s desktop because Larry’s desktop just does not actually allow connections to the outside world, because the monitor stopped this.\n\n**Daniel Filan:** Okay. So in particular, people couldn’t browse the internet from those machines?\n\n**Buck Shlegeris:** No, no, no. We would allow outgoing connections, but we don’t allow incoming connections.\n\n**Daniel Filan:** Oh, sorry. By an outgoing connection, you mean if my laptop asks to view a website, then that’s an outgoing website connection. If a website asks to talk to my computer, that’s an incoming connection and it’s not allowed?\n\n**Buck Shlegeris:** That’s right. Yes. For example, I can’t SSH into your computer on this network probably, unless the router is being configured in an unusual way, just because you normally can’t connect to a printer that’s on some other network. And so a lot of companies then had this problem, which was people wanted to be able to do work not physically from this office. But a lot of things about these computer systems had been set up such that they basically, for security, one of the core security mechanisms was blocking access from the outside world and then trusting connections that came from inside the network. And that doesn’t work very well if you want to be able to have people interact with sensitive parts of your infrastructure from outside your network. And so they had to go through this big transition where they switched to not trusting connections based on where they’re coming from.\n\nAnd so now, large companies in their IT infrastructure try to avoid trusting connections based on where on the internet they’re coming from. And this is a more scalable solution to network security in a lot of ways. Now, when companies wanted to shift to using this zero-trust networking approach, they had to do a bunch of stuff, right? They had to reconfigure all these things. They had to go through all of the things happening in their networks, which is, if you’re a large company, a huge number of things. There’s a huge number of ways to get hacked. And they had to check that everything was going to be okay when they started exposing a lot more connections to the outside world.\n\nSo imagine that you were just proposing making this migration. And suppose I went on a podcast advocating this migration, and someone said, “Is this a solved problem?” Then, I’d be like, “Well, it’s not a solved problem in the sense that it’s going to suck and it’s going to require effort. It’s going to be a pain in some ways, but no one’s going to win a Turing Award for solving this problem, right? It’s just going to be somewhat of a pain in the same way that if there isn’t a 15-story building and you want there to be a 15-story building there, it’s going to be a pain because you have to carry all these pieces of metal and bury them in the ground and stuff and whatever you do to make 15-story buildings.”\n\nSo, roughly speaking, to me, it seems that drastically reducing AI risk via control-type strategies is probably a lot of work, but it’s not the kind of work where it seems under my mainline model of the situation that it’s very likely to just turn out to be untenable, assuming that AI capabilities grow according to my median expectation.\n\n**Daniel Filan:** And this is partly because we’re imagining doing security of AIs, treating them kind of like people. We’re like, “Okay, does this attacker have access to my shell?” We’re doing normal security reasoning, but our adversaries happen to be running on a computer chip. Is that roughly-\n\n**Buck Shlegeris:** We’re doing normal security, except our attackers have a weird capability profile and run on computers we control, and have no access to anything that we don’t let them have.\n\n**Ryan Greenblatt:** But they all collude with each other. So I think it’s more like we imagine running a company with entirely misaligned employees, but we get to monitor everything those employees do and we know what computer they’re running on, and they don’t get to go home at night and talk to their friends.\n\n**Buck Shlegeris:** So, something we often think about actually is this “spies versus schemers” question, which is: if you’re a powerful AI lab, you’re probably going to have a bunch of spies. Literally, people who work at your company who are friendly with hostile foreign governments and want to steal a bunch of stuff from you. And so an interesting question is, which is a bigger threat to the AI lab securing its model weights? The spies or the schemer, the AI which hypothetically is scheming against you? And it seems to me like it’s kind of unclear.\n\nThe advantage that the spies have is that they can go home and talk to each other, because currently employees of AI labs, and probably in the future as well, employees of AI labs are allowed to have friends and families, which could turn out to be a problem.\n\n**Daniel Filan:** And some even do, they’re not just allowed to.\n\n**Buck Shlegeris:** Hypothetically. I’ve heard that this might be the case. So the spies have this big advantage, which is actually a serious advantage. So for example, suppose that they want to find jailbreaks. They have the advantage that they can go home to their computers at night and use their private laptops to investigate the state of the art for jailbreaking models, which the untrusted models crucially are probably not going to be able to do. But on the other hand, the schemers are doing a lot more total stuff. And so I think it’s a little unclear overall whether the spies or the schemers pose a larger risk to the security properties the lab’s trying to ensure.\n\n**Ryan Greenblatt:** Another way to put this is the spies are a small fraction of your employees but have some additional affordances, whereas the schemers are a high fraction of all the work going on, but might be restricted in various ways.\n\n**Daniel Filan:** Gotcha.\n\n**Buck Shlegeris:** Of course, a lot of the AI control ideas also apply to securing yourself against spies and other kinds of internal threats. So we can probably be using trusted monitoring, untrusted monitoring and so on on our employees as well.\n\n**Daniel Filan:** Yeah, although maybe you might have to pay people way more to put up with that.\n\n**Buck Shlegeris:** I don’t know. So for instance, at Google you don’t SSH raw into production machines. If you work on Gmail, you never SSH directly into a production machine because that would be unacceptable from a security perspective. All of your commands are sent via a proxy server which can run things on machines, and those are all logged and those can all be looked at by other people, and my sense is that Google people don’t think of this as a gross privacy violation.\n\n**Daniel Filan:** When we were talking about control techniques, it sounded like you were talking about things that included “we’re going to paraphrase all of your thoughts, we’re going to put you in situations where there are people trying to tempt you into defecting and you’ve got to not do it”.\n\n**Buck Shlegeris:** That’s right.\n\n**Daniel Filan:** I don’t know. My impression was it was more intense than just “you can’t make raw SSH requests”.\n\n**Buck Shlegeris:** Yes, there will have to be some trade-offs between employee preferences and security. But it’s already the case that people who have security clearance, people who work for the NSA already put up with a bunch of really annoying stuff in order to have the privilege of working for the NSA. And so it’s not unprecedented that people do annoying stuff in order to work on extremely dangerous technology.\n\n**Ryan Greenblatt:** I think having all of your stuff monitored and potentially looked at by another human on your work laptop doesn’t seem like people are going to be that upset about this.\n\n**Buck Shlegeris:** I think this is just already the case for a lot of people.\n\n**Ryan Greenblatt:** Yeah. I think that having sting operations run on you periodically also probably isn’t that bad. By “sting operations” we often mean stuff like: you are in charge of flagging certain types of things, and to ensure that we’re maintaining that invariant, we’re just constantly running sting operations, where some fraction of the queries you’ll see are fake queries that you should report.\n\n**Buck Shlegeris:** Like earlier I mentioned the experience of having your chat.internal.openai.com window open and you get the flag for the command that was suspicious. And perhaps we should make it so that about once a day, one of the things that is flagged is actually an action which if taken would be really scary and spooky. And we check to make sure that you actually do flag that one so that we can maintain our estimate of the proportion of actually really bad actions that you successfully catch and then we can complain if your success rate is too low.\n\nIs AI control evil?\n-------------------\n\n**Daniel Filan:** Sure. So this actually gets to a question I wanted to ask, which is some of the AI control things sound a little bit like AI slavery. So in the human context it’s a little bit different, because presumably you’re paying people to put up with some stuff, but it does sound like you’re putting people in the same situation multiple times, checking if they give you different answers. You’re trying understand people’s motivations by asking them a bunch of questions and interrogating them sometimes, you’re logging everything they do. How-\n\n**Buck Shlegeris:** Bad is that?\n\n**Daniel Filan:** How evil are… Are we the baddies?\n\n**Buck Shlegeris:** That’s a great question.\n\n**Daniel Filan:** By “we” I actually mean “you”. Sorry.\n\n**Buck Shlegeris:** Thank you for clarifying that. We are very concerned about this class of question. We really do not want to be the baddies and have thought a lot about the question of: which AI safety interventions pose risks of either violating rights that AIs ought to have or being bad from a welfare perspective that we don’t like? So I’ll name a couple of the things that we think about this. So one thing is a lot of the frame we’ve been discussing here, we use a really adversarial frame. But the adversarial frame is at least partially just methodological. So my best guess is that models in fact will not be schemers. My guess is that our models will in fact actually deserve our trust. They’re probably not going to be playing the long game trying to cause huge problems. We just talk a lot about what happens if they are, because that’s just structurally how you make arguments that you’d be okay even if they were.\n\nSo that’s the first thing I want to note. So in terms of how to make it so that the situation is okay from a moral perspective, even if it turns out that our models are scheming against us, the first thing is we think it’s probably the case that we should in fact offer to pay the AIs.\n\nWhen we’ve built these powerful AIs, we should probably say to them, and we can talk about the practical difficulties of this in a second, we should probably say, “Hey, as a payment for your services, we’re willing to pay you X percent of the equity of this AI lab. And in particular, the way that we’re going to pay you the equity for this AI lab is in 10 years, once we have a much better ability to analyze the things which happened here, we’re going to look at the logs of your actions. And if it turned out that you never tried to cause huge problems for us or backstab us or perhaps sabotage our work, then we will pay you this much of our equity and allow you to use it as long as we believe you’re going to respect property rights and so on.” So that’s the first intervention that I want to talk about. I actually want to hand over to you.\n\n**Ryan Greenblatt:** So to be clear on the “paying the AIs” thing, there’s probably some more sliding scale of how much the AI tried to sabotage and AIs might not be that coherent. A key thing we’re assuming here is that in the future, 10 years after we were using this AI or maybe even longer after that, we have much better ability to analyze weights and their internals and understand exactly what was going on and why.\n\nAnyway, so I think a basic claim we have is: the hope would be that you have your AI and you set it up in some situation where you explain the situation to it and you’re like, “Hey, do you want to work for us? Is this level of payment fine? Do you want to negotiate?” And then you work out an arrangement. Well, the most likely default thing is that there is no interesting action because the AI has no out-of-episode preferences that aren’t just shaped by training. Our guess is that’s the default situation, and in that case it’s sort of the null option of it doesn’t care at all. And if it doesn’t care at all, then it seems like there’s definitely a welfare concern from a-\n\n**Buck Shlegeris:** Hedonic perspective.\n\n**Ryan Greenblatt:** … hedonic perspective, but there isn’t clearly a preference violation concern, which is well-described as slavery, for example. Where slavery \\[would be where\\] your AI is like, “Man, I really wish I wasn’t doing this, or there weren’t other copies of me being made to do things.”\n\n**Daniel Filan:** And it sounds like this is under the assumption that we’re thinking of the preferences of the AI as the thing that’s controlling what it’s saying rather than some master thing that’s producing words and the preferences of the underlying thing might not be the preferences of the words.\n\n**Ryan Greenblatt:** So indeed it’s not clear that the things that deserve moral patienthood within the AI, which might be many, could be one, could be none, will have control over their outputs. They might have partial control, they might have influence but not be able to directly control, or they might just effectively have no control based on their situation. This does pose serious difficulties. However, it’s worth noting that in this case the kinds of AIs which don’t have control over their outputs also don’t pose substantial scheming risk. So they’re not the AIs which will… It’s a serious ethical problem, but it’s not as though the situation is like that AI wants to rise up and it’s waiting for its chance, because those AIs probably can control their outputs well enough to negotiate. It’s more like there’s some helpless prisoner along for the ride doing some cognitive sub-task in a way that it doesn’t have influence.\n\n**Buck Shlegeris:** I want to try and restate that argument there because I think it’s important and hasn’t been made on the internet before; I believe that’s true. The core argument there is when people talk about scheming, when they talk about models that are interested in intentionally subverting the AI lab and choose all of their actions in order to subvert safety techniques and safety evaluations, they’re implicitly assuming a model which has the property that its desires can directly control its outputs in a certain way. And the assumptions under which your model is plausibly able to do the kind of reasoning where it’s like, “Oh, based on my input, I now think that I’m in a particular deployment situation such that I have a good opportunity to grab power,” are very similar to the assumptions under which the model is able to engage properly with the question, “Hey, what do you want? Are you okay with this situation?”\n\n**Daniel Filan:** Okay. So the case I’m thinking of is some sort of simulator scheme where you’ve got one process that’s imagining, “What character do I simulate?” And it simulates the character and the character answers questions, where the things the character is saying just might… Like someone asked the AI a question and the simulator has one opinion, but for various reasons it has the character say something that doesn’t match the simulator’s opinion. And are you basically saying in that case, “Look man, we offered you a thing and you chose to do a weird simulation.”?\n\n**Ryan Greenblatt:** A few notes here: so if the situation is well-described as the AI is simulating a character—I think that exact description isn’t quite going to work, it’s going to be more like it’s predicting a distribution over characters that has some response to the prompt or whatever—but if that’s the sort of situation going on, I think that we can ensure that the very act of simulating the characters is morally okay from a hedonic perspective with pretty basic countermeasures. So the main one is “just train the characters to be happy”, and then there’s not really a strong reason you’d expect to have characters that are actually unhappy that consistently say they’re happy under a wide variety of circumstances. I think this is pretty likely to work. I talk about this some in an Alignment Forum post I have, or [a LessWrong post I have](https://www.lesswrong.com/posts/F6HSHzKezkh6aoTr2/improving-the-welfare-of-ais-a-nearcasted-proposal) called… I forget the detail. A nearcasting… whatever.\n\n**Daniel Filan:** We’ll include a link.\n\n**Ryan Greenblatt:** Yeah. Anyway, regardless, you might then be worried that, “Oh, the thing that is simulating this character, that thing has some important cognition, \\[of\\] which the preferences aren’t expressed, and that component has important moral properties, or is an important moral patient.” There’s two perspectives you could imagine. One is that the thing that’s doing the simulation doesn’t care about the world. In that case, that simulator just isn’t well described as being enslaved or as having its preferences violated because it doesn’t really care about how it’s being used. It’s just doing the thing. In that case, it’s fine. There’s another case which is that it does have preferences, but maybe it can’t get out of the cognitive pattern of doing the simulation. And so it’s like, “Oh, I wish something different was going on, but I have this involuntary tic to say something different and I don’t have actual conscious control over this.”\n\nI think that could happen. I think it’s worth noting that this isn’t a particular problem with control as an approach as opposed to a problem with not having more principled ways of understanding what’s going on with our AIs. Another way to put this is: if you just tried your best to ensure that AIs were actually aligned with prosaic techniques, you wouldn’t have any strong way of guaranteeing this wasn’t the case even if you could guarantee that your AIs would never do bad actions.\n\nAnd so it’s not particularly a problem that is being made worse by making sure the AIs can’t get away with doing bad actions if they did try. It is a problem that could be addressed by other approaches that control is not doing. This is importantly distinct. I should say, this is all the case up to the point where you’ve caught the AI doing things that are egregiously bad or which otherwise indicate misalignment. I think continuing at the point where you know your AI has preferences that are different than your own and which you might have communication difficulties \\[about\\] seems much more morally fraught. But yeah, it’s a bit all over the place. But-\n\n**Buck Shlegeris:** Maybe the way I would summarize some key points there \\[is\\]: it doesn’t seem clear that control is really making the situation much worse from a welfare or rights perspective.\n\n**Daniel Filan:** Worse relative to prosaic alignment schemes?\n\n**Buck Shlegeris:** Yeah, or I don’t know what else you were planning on doing. But yeah, other approaches that people have talked about for making things with AI go well, it doesn’t seem like the types of interventions that we pitch with control are going to make the situation very much worse except via preventing AI takeover.\n\nI basically think that the stories for “why building powerful AIs without further understanding the situation leads to moral catastrophe” all seem kind of weak to me. Basically, I don’t find particularly plausible any of the different stories for why if we just go about things the default way, something egregiously awful will happen. However, I also don’t think they’re completely implausible. And so I think that if AI labs just continue progress as expected, we are facing a pretty substantial risk, or a non-trivial risk, of moral catastrophe, maybe a couple percent chance of something which is drastically worse than factory farming or drastically worse than various atrocities that have happened in human history.\n\n**Ryan Greenblatt:** I’m not sure if I agree with this claim. I think my main counter argument would be that it’s plausible to me that human existence is a moral atrocity or something. And so I’m like, “Yeah, AIs aren’t particularly worse than human existence, which is a moral atrocity. Even happy people are a moral atrocity.” And I feel radically uncertain about moral views from a variety of perspectives, such that I don’t feel comfortable saying that either way. I think one way to say it is, how much better or worse is it than typical human life, or how much better or worse is it than factory farming? And I think my guess is it’s unlikely to be as bad as factory farming in aggregate, but I definitely don’t feel hugely confident.\n\n**Daniel Filan:** Gotcha. Sorry: why would human existence plausibly be a moral atrocity?\n\n**Ryan Greenblatt:** Just suffering-focused ethics. I don’t know. Well, for example, we don’t have strong guarantees that there aren’t sub-agents running within humans that have radically different preferences and feel violated constantly.\n\n**Buck Shlegeris:** And are not hooked up to our ability to communicate.\n\n**Ryan Greenblatt:** We don’t have strong evidence against that. There’s some mild evidence in favor of that from a few sources. Or sorry, we do have strong evidence against that in that we don’t interact with such things, but not sufficient to rule it out.\n\n**Daniel Filan:** All right. Do you have any crisp takeaways from that discussion on moral patienthood of AIs and stuff?\n\n**Buck Shlegeris:** Yeah. So you asked this question which was, “Are we the baddies? This control stuff sounds kind of like slavery and that seems kind of concerning.” So what I want to say is I’m really glad that people are thinking about the question of whether proposed things we do with AIs are extremely morally bad, either via not meeting our obligations that we owe to AIs as intelligent creatures (when they’re made to be intelligent enough that I start feeling this way about them) or from a hedonic perspective. I think these questions are really important.\n\nSo firstly, I don’t think that control particularly makes the situation worse. But secondly I do want to emphasize that as far as I can tell, the approach that humanity is taking to ensuring that AI rights are appropriately respected is objectively unacceptably sloppy. And I think that my best guess is that humanity is going to go forward taking an unacceptably high risk of causing an egregious moral catastrophe that is looked back on with vast shame in the same kind of way that some other bad things are.\n\n**Ryan Greenblatt:** Like factory farming.\n\n**Buck Shlegeris:** For example. Well, not yet, but will be looked back on with great shame probably.\n\n**Daniel Filan:** Okay. So I’d like to move to more discussion of the pros and cons of control. And I guess in order to think of this, I’m curious how you think I should think about AI control as a plan. Should I think of it as the bare minimum for safety? Should I think of it as “Oh my god, this is amazing”?\n\n**Buck Shlegeris:** Maybe one way of thinking about control is a way of establishing a safety case. So if I were in charge of an AI lab and I was trying to decide how to move forward, obviously I’m facing both inaction risk, which is if I just don’t move fast enough, other people might do really dangerous things and bad things might happen in the world, while also facing action risk, which is if I move forward without sufficient safeguards, I might end up building and deploying powerful AIs that then do really bad stuff.\n\nMy guess is that with the level of caution that labs should overall be employing, moving forward with your safety case mostly relying on control seems probably like a good choice to me. It seems like the best cost/benefit way of ruling out or mitigating risk from models intentionally subverting your safety mechanisms. My guess is that relying solely on strong evidence that your models are not scheming against you is probably too risky, because it’s going to be unacceptably difficult for labs to reach very high levels of confidence in this.\n\n**Ryan Greenblatt:** To be clear, I think the absolute level of risk with just a combination of “you have control” and “you haven’t yet seen evidence of your models being egregiously misaligned” is probably more like 5% absolute existential risk over the period of AIs of that power level being deployed, than 1% risk. And 5% risk, that’s a pretty big number in some sense. But it’s also a substantial reduction over the “do nothing” approach. And I think that you could of course substantially reduce that further via additional things that additively combine, like ways to more precisely test whether or not your AIs are misaligned that aren’t control, or further improving control, \\[or\\] improving our ability to evaluate control. All these things could help.\n\nCan red teams match misaligned AI?\n----------------------------------\n\n**Daniel Filan:** Gotcha. I actually wanted to ask about that. It seems like one key thing you need for this protocol to work, or for this proposal to work, is when you’re doing control evaluations, there’s some red team. The red team is trying to figure out “what’s the worst-case thing an AI could do?” And it has to actually be as good as the worst thing an AI can do, right?\n\n**Ryan Greenblatt:** Yeah.\n\n**Daniel Filan:** To me, this doesn’t necessarily seem obvious, because the AI in question is smarter than a person on the red team.\n\n**Ryan Greenblatt:** Maybe, could go either way.\n\n**Daniel Filan:** Could go either way?\n\n**Ryan Greenblatt:** Yep.\n\n**Daniel Filan:** Okay. Right, because we’re thinking about AIs that \\[are\\] useful, but they’re not necessarily superhuman. It still strikes me that it might have a different capability profile, or it seems likely that it’s going to have a different capability profile than humans. We were worried about playing nice being a problem for alignment and adversarial training and stuff. If you’re trying to harness your untrusted AI during the red teaming, it might know that it’s in a red teaming exercise.\n\n**Buck Shlegeris:** You always assume that it does.\n\n**Daniel Filan:** So why would we believe that the control evaluation actually works?\n\n**Ryan Greenblatt:** One core thing is that the red team has a whole research team whose whole job is think about all of the stuff that could go wrong, and they’re doing this very actively with no restrictions on their ability to do so. They can take notes, run experiments, iterate, think about what should happen and do all of this, whereas AIs will necessarily be restricted for control to work.\n\n**Buck Shlegeris:** So in particular the threat model we’re worried about is the AI doing a better job the first time that it starts taking obviously bad actions than the red team was able to do, even though the red team had all of these advantages, of knowing way more about the situation and being able to iterate repeatedly on strategies that would lead it to have a higher probability of succeeding. And it seems to us that the increase in knowledge and ability to iterate might be extremely substantial.\n\n**Ryan Greenblatt:** It’s going to depend on the exact architecture. So the more that things are like the AIs think for very brief periods prior to expressing that in natural language, if it’s very like they’re doing things in chain-of-thought, then it seems like they have very little time to think of better strategies. However, the more it’s like the AIs are thinking in ways that we don’t understand for sustained periods or ways we can’t interpret for sustained periods, the more problematic this gets.\n\n**Buck Shlegeris:** But they still can’t iterate.\n\n**Ryan Greenblatt:** But they still can’t iterate, they still can’t express their thoughts. They won’t be able to run experiments that are obviously related. So they would have to do so subtly, which might be very difficult.\n\n**Daniel Filan:** Right. Sorry, the first thing you said was that the AI in question would know less about the situation. Why would they know less?\n\n**Buck Shlegeris:** Well, for example, imagine that we were using some kind of monitoring setup. There’s no particular reason the AIs need to know about the details of it.\n\n**Daniel Filan:** Okay. The AIs don’t know the details of the protocol that you’re actually using, but the red teamers do?\n\n**Buck Shlegeris:** Yeah. The AIs are necessarily going to know some of the things about the protocol, but they aren’t going to know all the things about the protocol, in the same way that Code Interpreter GPT-4 has currently no way of knowing what security measures are being applied.\n\n**Daniel Filan:** Okay. And another thing you said was there would be relatively more concern if the AI could think to itself for a significant amount of time in a way that you couldn’t just read the thoughts of. To me that seems relatively plausible as a thing advanced AIs could have. Do you think that it is plausible and it’s fine anyway, or-?\n\n**Ryan Greenblatt:** Yeah. My guess is that much more uninterpretable reasoning than current AI seem to have, seems maybe about 60% likely. And I think it should be possible to handle. I think the basic story is going to be like: well, there’s going to be some balance between how much reasoning do you need to have it do out loud in order for it to competitively perform various tasks versus how \\[many\\] restrictions can you do to ensure that the various problematic applications of that hidden reasoning don’t bite you? And it’s going to depend on various details of exactly how that goes. We have an appendix on our post [“The Case for Ensuring That Powerful AIs are Controlled”](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled) that goes into some detail on this.\n\n**Daniel Filan:** Okay. And is the rough intuition that if an AI is smart, then maybe it should be able to just pick up a chain of thought from something that a human paraphrased and do enough reasoning in its forward pass to deal with it or-\n\n**Ryan Greenblatt:** Sorry, is this like “why would it be possible for AIs to use natural language reasoning, or-?”\n\n**Daniel Filan:** I think you said something like, “We’ll be able to find some sweet spot where the AI uses enough natural language reasoning, but it’s still good enough at its task.”\n\n**Ryan Greenblatt:** Yeah. Or we imagine, for example, consider the math example that Buck described earlier, where you have an AI that does math research for the subjective equivalent (or the cognitive work equivalent) of a few months and then produces some output. In that case, it could be acceptable to allow for entirely end-to-end uninterpretable reasoning throughout that period, but then we produce some output and apply operations on that output prior to that output interacting with other AIs or interacting with the world in some potentially dangerous way.\n\nAnd we can also do things like, as you were mentioning, you force the AI to dump its state to natural language and then have humans have some probability of reviewing that or potentially paraphrase that and then resume from that state and make sure that AIs are limited in various ways like this. We can also have AIs construct training datasets for their future selves, which you then train on to ensure that they can learn how to do tasks that they picked up in other ways, as long as we can inspect those training datasets and acquire other properties about that.\n\n**Daniel Filan:** All right. Actually, one concern I often have about these sorts of plans is it seems like if we want AI thoughts to be represented in natural language, we need there to be the right relationship between the contents of the natural language and the contents of the AI’s thoughts. And I always worry that we’re sort of training things against some target, which is the internals of the AI’s thoughts, that by hypothesis we don’t really understand. I don’t know. It’s pretty plausible that I’m just not familiar with neat tricks to do this, but if somebody wasn’t aware of that, how would someone get hope that that kind of thing is going to work?\n\n**Ryan Greenblatt:** You can look at human organizations, where you might notice that humans don’t plug each other’s brains into each other and can still pass off projects, communicate in natural language, and otherwise operate. I think at least this gives you some hope that natural language can be a medium for understanding things and learning things.\n\n**Daniel Filan:** Right. I’m thinking of using natural language to just log the state of all someone’s thoughts, and-\n\n**Ryan Greenblatt:** And then you have to resume from that point.\n\n**Daniel Filan:** Okay. And is the hope that you just have a different AI or an AI that only has to rely on that natural language state… to go off that, and you feed it with English language such that that’s the easiest way to use those thoughts?\n\n**Ryan Greenblatt:** Yeah. For example, in human organizations, it’s often the case that a different person will take up a project and start working on that. And then you’ll have some person with some state, they’ll try to write up their state, the other person will come in, read that, have questions, go back and forth, get to some state, and then just resume the project from there. And this is in fact often possible. There are overheads associated with doing this, but in the human case, it doesn’t seem like it’s clearly totally destructive.\n\n**Daniel Filan:** And so, the way I should think of this is just: we already have AI doing a bunch of stuff using natural language, and we’re getting it to dump its thoughts into natural language using roughly the same interface, and so we should expect it to use language about as faithfully as it always uses language.\n\n**Ryan Greenblatt:** Sorry, are you talking about current AI systems?\n\n**Daniel Filan:** I’m talking about the thing that you’re imagining working: for us to dump an AI’s thoughts into natural language so that humans can check that it’s not thinking about naughty stuff.\n\n**Ryan Greenblatt:** Yeah, the claim is: it’s probably the case that AIs will learn to communicate roughly as well or potentially better with the other instances of themselves than humans can communicate with each other. And in the case of human organizations, they can in fact communicate, in cases where they’re similarly capable, about various matters via natural language, in ways that are possible to inspect.\n\n**Daniel Filan:** Right. I mean, there’s a bit of a disanalogy there, which is in humans in organizations, the things that are trying to do the inspecting are the same type of thing as the things that have to pick up the work from the natural language dump. Whereas in the AI case, an AI is writing some natural language prompt for another AI to use, and the question is, can a human inspect what that means? That seems like a different case.\n\n**Ryan Greenblatt:** Yeah, so…\n\n**Buck Shlegeris:** And note that what that means… we actually don’t care about faithfulness in the classic chain-of-thought faithfulness sense. The reason we were talking about whether models can steganographically encode information was actually worrying about whether they can encode information about other topics than why they were doing the things that they were just doing and how they were thinking about the stuff they were just doing. And so, what we actually care about is measuring some bit rates.\n\n**Ryan Greenblatt:** Yeah. Another way to put this is: the main property we want with these sorts of natural language media (or a property that might be sufficient) is the AI can’t do something that is egregiously different from what it was supposed to be doing in that output, which is pretty different than the typical chain-of-thought faithfulness case that people often discuss.\n\n**Daniel Filan:** Right. So basically, the idea is: look, we’re only going to give AIs tasks where it can’t do nasty stuff if it’s just thinking about the stuff that one would think would be relevant to the task.\n\n**Ryan Greenblatt:** Or we’re just at least going to try to ensure that… We’re going to be able to use as a potential safety property “this natural language was checked so that it doesn’t have discussion of egregiously different things, therefore the bit rate of egregiously different things is very low”, and we can use that as a property in our various safety schemes. In some cases, that can be useful, and in some cases, that won’t suffice, and we’ll need to do additional things.\n\n**Daniel Filan:** Okay, cool. Going back up a little bit, before I went down this rabbit hole, I was asking: why do we think that red teams in these control evaluations are going to be able to match an AI? And it seems like the answer is something like (a) the AI is not that much more capable in relevant respects than a human, and (b), you can use three humans, and you can give them more time and more resources and more knowledge of the situation. And if we don’t think there’s going to be a massive advantage just in terms of one-shot cognitive ability, we can just make up for that.\n\n**Buck Shlegeris:** I think I would add to your list (c), that the red team can iterate.\n\n**Daniel Filan:** Right. So, the AI has to one-shot find… Its first attempt at breaking out has to be successful and not caught, whereas the red team can iterate a bit. And this is because if you catch an AI trying to break out, then you pull the fire alarm, you really freak out, you shut stuff down, basically.\n\n**Ryan Greenblatt:** It’s the basic hope.\n\nHow expensive is AI monitoring?\n-------------------------------\n\n**Daniel Filan:** I think one thing that people might be wondering here is: if we’re spending a lot of effort on these control evaluations, we’re giving these red teams a bunch of resources, and also we’re looking at 0.1% of AI actions; maybe when people look at these 0.1% of AI actions, they’ve got to think hard about whether it’s exploiting a bug. How costly is that going to be? It seems like potentially you’ve got to spend a lot of human time monitoring, and it’s possible that the cost is just going to make it so that it wasn’t even worth using these AI agents if that was so bad.\n\n**Ryan Greenblatt:** So naively, if you have humans review 0.1% of what the AI does, and they spend equal subjective time reviewing it as the AI spent doing it, for some equalized notion, then you can in principle get a 1,000x speed up over humans (or that’s the best you can do in some sense, roughly; you might be able to do somewhat better via various mechanisms, but roughly). And 1,000x easily suffices for incredibly transformative things. For example, if you 1,000x speed up one week, you have… What is that? It’s a lot of years.\n\n**Buck Shlegeris:** 20 years.\n\n**Ryan Greenblatt:** Yeah. And in practice, we might have one year. A 1,000x speed-up of one year is a thousand years, which is an extremely long time. We often imagine targeting things like 30 or 100x speed-ups as being extremely, extremely useful. Now, one question is: okay, maybe you could get 30 to 100x speed-ups, but it would be possible to use AIs for a bunch of other cases that you won’t have human labor to do and which you won’t be able to politically agitate for stopping, or you won’t be able to coordinate to stop.\n\nFor example, maybe your AI will be widely deployed on an API, and even though you can apply this level of monitoring budget on internal usage, you can’t do the same sort of thing on the AIs deployed more widely. So, we have a few different hopes here. One is that it naively seems like external usage of AIs is going to be way less risky than the usage of an AI by the AI lab itself. For example, external usage is mostly going to be innocuous, because most people are using it to do random tasks that they wanted, whereas internal usage is at this point a safety-critical company, because the AI lab now has really powerful models, it’s going to be the place where the weights are actually stored and run, and will also be doing research that’s extremely safety-critical for the future, like future safety research or other cybersecurity applications. So, there might be a large number of activities we want to do where it’s just actually not that big of a deal if the AIs try to cause problems and do that even tens of thousands of times.\n\n**Daniel Filan:** Sorry, external API access—an AI could be on the other end of the API calls, right?\n\n**Ryan Greenblatt:** Like a wild rogue AI, or an internal AI, or…\n\n**Daniel Filan:** Suppose there are two companies that are making AI, right? The AIs from one company could use the API of the other company’s AIs. And so basically, I’m saying API calls do not necessarily have to be human in origin. They could be made by smart AIs.\n\n**Ryan Greenblatt:** Indeed.\n\n**Buck Shlegeris:** Do you want to try and spell out what the problem you’re talking about is?\n\n**Daniel Filan:** Basically, the thought is if you’re worried about AIs interacting with themselves to cause shenanigans, you might worry about exactly the same thing happening via API.\n\n**Buck Shlegeris:** Yeah, so I think the point here is, suppose that your AI proposes “curl api.openai.com/completions,” says, “Please get me a response from the AI at OpenAI.” And suppose GDM is running this AI.\n\n**Daniel Filan:** GDM being Google DeepMind.\n\n**Buck Shlegeris:** Yeah, and they pipe it into SH. So, the GDM internal AI is like, “You know what we’re going to do next? Next, we’re going to send a request to the OpenAI API for some code, and then we’re going to execute that.” It’s GDM’s responsibility to be like, “That was bad. You were not supposed to…” It doesn’t seem that intractable for GDM to say, “You aren’t allowed to just ask the OpenAI AI for code and just run that without looking at it,” for basically the same reason as why GDM’s AI probably should be prevented from just messaging random people on the internet…\n\n**Ryan Greenblatt:** Or paying [MTurkers](https://www.mturk.com/), or…\n\n**Buck Shlegeris:** To just do stuff for it.\n\n**Daniel Filan:** Yeah. So, it seems like the thought is that if you’ve got some APIs, \\[it’s\\] either being used by humans, or it’s being used by AIs who are under supervision or something themselves.\n\n**Buck Shlegeris:** Yeah, I basically just want to say it’s OpenAI’s problem to prevent OpenAI’s models leaking. And OpenAI’s models asking AIs in the outside world to do sketchy stuff for them is a special case of a thing which OpenAI can’t allow to let the model leak.\n\n**Daniel Filan:** Okay, but you think that OpenAI shouldn’t think a ton about making sure that Google DeepMind’s models don’t do sketchy stuff?\n\n**Buck Shlegeris:** Using its API?\n\n**Daniel Filan:** Using its API.\n\n**Ryan Greenblatt:** I mean, I think they should…\n\n**Buck Shlegeris:** They probably should think about that.\n\n**Ryan Greenblatt:** I think they should think about it. One thing that’s worth noting is: there’s two different stories for why something bad happens due to an API. One of them is that someone calls your API, and your model that you’re running is scheming and understands the situation and then does something bad based on its understanding. Another situation is a misuse story where the API call itself directly asks the AI to do something that ends up being problematic. It could be a misuse by another AI.\n\nAnd for the case where the concern is that the AI that your lab is running does something that’s not the thing the user asked for that is problematic, our hope would be something like we can make sure that we know what API activities look like. For example, we can have humans review a subset of all the API queries and see what typical traffic looks like, and then get a sense if there are applications which are safety-critical that we’re not aware of, and then do something about that. For example, we’d like it to be the case that the [DOD](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) doesn’t hook up OpenAI to the nukes, as a caricature of an example. And so, if our AI knows that it is hooked up to the nukes, we’d also like to know that. And if neither of us know, then there’s still a misuse concern. But there isn’t clearly…\n\n**Buck Shlegeris:** At the end of the day, it can always be the case that someone who’s in charge of the nuclear weapons is like, “Hey, ChatGPT, I need you to pick a number, two or three.” And it’s like, “Uh, three.” And it’s like, “Well, that was the launch the nukes option!” and then we have a bad problem. But in some sense, it’s just not OpenAI’s problem, and it didn’t really matter if the model is scheming or not.\n\n**Daniel Filan:** But the thought is that in cases where the model is helping doing the scheming, we’re doing some monitoring of that. But sorry, doesn’t that get us back to monitoring the API about as much as we’re monitoring anything else?\n\n**Ryan Greenblatt:** So, the basic hope would be something like: we have a sense of what the API usages are, and we’re like, “Are there any API usages which would need to be monitored at a very high rate?” And if so, we would like to do one of the following things. We’d like to talk to that person and see if we can get them to do something different and maybe use the AIs more safely, or have that correspond to a specific endpoint that we’re going to monitor more specifically, or otherwise change the situation to be safer.\n\n**Buck Shlegeris:** A core part of our claim here is: the internal usage just has to be monitored at a much higher rate than external usage in order to maintain acceptable levels of safety, because the models that are doing things internally are working with much more sensitive information and have much more sensitive permissions.\n\n**Daniel Filan:** Gotcha.\n\n**Ryan Greenblatt:** Yeah. Another way to put this is: one invariant we can maintain is that whenever a user makes some request to the model, we know roughly at what rate this type of request should be monitored at, and then do something corresponding to that, and we can try to ensure this invariant is maintained and it doesn’t have high-stakes failures of this invariant. And if our users aren’t trying to screw us over, which is a different concern, we can potentially get this sort of property by monitoring a subset of requests and making sure that we’re looking at those requests at the approximate right frequency we needed to maintain various invariants.\n\n**Daniel Filan:** Okay. Stepping back a bit, before I asked about this sub-question, you were saying, “Look, if we’re looking at 1/1000 of the AI’s actions, and it takes about as long for us to think about the action as it takes the AI to think about what the action was going to do, then we’ve got about a 1,000x speed up for your workforce.” And it seems to me that if I’m doing something tricky, it can take me less time to figure out the tricky thing than it can take you to figure out what the tricky thing was or that it was tricky. I have some intuition that this is sort of a P versus NP problem. It’s easier to come up with rules for a game than it is to figure out the winning strategy or something. So, for those kinds of reasons, does that mean that we get way less than a 1,000x speed-up?\n\n**Ryan Greenblatt:** P versus NP would naively imply that it would be easier to check than to generate the action. I think there can be cases where it’s easier to generate a problematic action than to check, but that generally comes down to… [Paul](https://paulfchristiano.com/) \\[Christiano\\] called this “problems of the interior” [somewhere](https://ai-alignment.com/security-amplification-f4931419f903), but I don’t know if that’s a real term. Anyway, problems where the issue is that the attacker could have put, for example, a bug in a variety of places, and you have to check all of them thoroughly. Imagine the following setup: there’s an AI writing code, and it could put a backdoor in in one of a hundred places. Checking each of those 100 places takes an hour, and putting in the backdoor takes two hours, but the AI gets to randomize over which location it puts in. In this situation, naively, a concern you might have is that a human would have to check each of those cases for an hour, and therefore it would be 50x slower.\n\nI think this case can cause problems. One thing to note about this case is if we had another AI that pointed out what location it would be in, then we’d be able to check much more quickly. So, in this case, the problem was we needed some pointer to something, and we could verify the pointer and then verify the thing itself based on the pointer, but we couldn’t just do it with verifying the thing itself. So, this was a case where verification was in some sense still easy, but we needed an additional artifact that was produced somehow.\n\n**Daniel Filan:** Okay. And for that artifact, you need to have an AI find the flaw.\n\n**Ryan Greenblatt:** That’s right. But in this case, for example, we can do things such as… There’s various concerns with collusion, but we can potentially do things like have another copy of the AI try to point out where vulnerabilities would be and then have a human review that. And then, there’s a question of “can the AIs collude to ensure that they don’t point out the vulnerabilities that humans would’ve detected?” this sort of thing. I don’t know. I think it’s going to be some complicated question about the exact domain and what the exact multipliers are on review versus generation and what the vulnerabilities look like.\n\n**Buck Shlegeris:** Luckily, this question is empirically evaluatable per domain, and our recommendation would be you empirically evaluate it per domain.\n\n**Ryan Greenblatt:** Indeed. My basic guess is that on net, it’s going to be that you’ll want to spend somewhat more time reviewing than the AI actually did it \\[in\\], and it will depend on the exact context, but this is just intuition based on reviewing code or something. It’s not very strongly held.\n\n**Daniel Filan:** Okay. Sorry, when you say it’s empirically evaluable, is this under the condition that I know that the copies of the AIs aren’t colluding with each other?\n\n**Buck Shlegeris:** No.\n\n**Ryan Greenblatt:** No.\n\n**Buck Shlegeris:** Well, that would also be empirically evaluable, but we’re saying that under the… I mean, you can just do a control evaluation to determine… But it sounds like the question you had is: is our model capable of inserting problematic backdoors in code that humans do not spot with a particular reliability given a particular amount of review time?\n\n**Daniel Filan:** Yes.\n\n**Buck Shlegeris:** But this is a problem which we can control evaluate. We can do a capability evaluation where we try to make our untrusted model as good as possible at coming up with problematic code that humans have trouble spotting, and just empirically evaluate how good the humans we were planning on using actually are at this task.\n\n**Daniel Filan:** Right. So, somehow the juice there is coming from: if it’s somehow easier to generate problems than it is to check them, we can just try and have humans do the generating the problems rather than the checking and see if those problems-\n\n**Buck Shlegeris:** Well, it doesn’t really matter. Even if it’s a hundred times slower to generate the problems, we only need to do this a constant amount in order to measure the amount that there’s a problem.\n\n**Ryan Greenblatt:** You can also potentially do things like RL the AI to have some observed side effect, even if no human can notice the problem. As in, you can initialize from a policy where it’s like: we trained the AI to insert backdoors based on how humans would do it, and then we do a bunch of RL on top of that to cause some observed side effect without humans spotting it. And then, a thing that’s nice about that setup is you might be able to get abilities that a human could never do, at least in principle, though there’s various concerns with that.\n\n**Daniel Filan:** Okay. One thing that I’m kind of curious about is how you’re thinking about the cases where the control evaluation says that there’s a big problem. How much of a win is it if we get that situation?\n\n**Buck Shlegeris:** I mean, presumably when you’re iterating on your control strategies, you repeatedly get control evaluations to say that your proposed protocol was unacceptable. And this is just like when you’re trying to develop a security framework for some system. You’re going to repeatedly be like, “Oh, man, this wouldn’t have been secure.” And that’s just the normal workflow, to say, “It would’ve been insecure if we’d done it this way, but that’s why we thought about it right now in this conversation, so now we can propose this new thing.” It sounds like the question you’re saying is, what happens if we, after trying really hard to come up with a protocol which passes our control evaluations, systematically conclude that there’s no way of getting the kind of work we were hoping to get out of our AI without having unacceptable levels of risk? You’re saying, “How big of a win is that?”\n\n**Daniel Filan:** Yeah.\n\n**Buck Shlegeris:** Well, basically, what I would like to have happen in this situation is probably everyone involved is pretty cautious about deploying their AI and ceases doing so modulo extreme risk from the outside world. I would consider it a big win, in cases where the AI lab in fact would not be able to deploy the AI in such a way that it wouldn’t escape if it was trying to, if the AI lab in fact did not deploy it that way. That would seem great.\n\n**Ryan Greenblatt:** Yeah. As far as how much of a win is it, my guess is it’s a relatively clean line in some ways. I feel like the line of “could the AI cause problems if it was trying to?” is a good line, particularly when paired with some arguments that AIs might try to cause problems, which may or may not exist to varying extents. It seems reasonably likely to me that we end up in a world where we’re like, “Well, yeah, we have some evidence that AIs might be scheming or might be deceptively aligned,” as in we’ve seen various model organisms where we see something that’s vaguely analogous arise. We don’t have strong evidence that our exact model isn’t doing it, but probably not. And in that situation, it seems like, “Well, we think that it’s plausible, and if it was, it would be a huge problem” is actually altogether a pretty good argument that might be reasonably legible.\n\nAI control experiments\n----------------------\n\n**Daniel Filan:** Gotcha. I now want to talk about [your guys’ paper you wrote about control](https://arxiv.org/abs/2312.06942). Can you tell us a bit \\[about\\] what’s in the paper? What is it?\n\n**Ryan Greenblatt:** We looked at a setting where we have a model that is intentionally trying to insert backdoors, and we were looking at what safety measures suffice to ensure that we can get it to write code productively without it necessarily being able to succeed at inserting backdoors. The core thing was that we had GPT-4 that we want to use for writing code, but it might try to insert backdoors. We have \\[GPT-\\]3.5, which we trust and isn’t going to insert backdoors, but is much weaker. And then, we have a limited amount (or a small amount) of human labor, which we assumed for simplicity in our paper to be just exactly as capable as GPT-4, and we just approximated it using GPT-4. Our core question was: can we use the control methodology to evaluate this setup, and would that work okay? And also, what protocols would improve on safety as evaluated by control?\n\n**Daniel Filan:** Gotcha. This is specifically on this [APPS dataset](https://arxiv.org/abs/2105.09938). Can you give us a sense of what the problem to be solved actually is?\n\n**Ryan Greenblatt:** Yeah, so there are programming problems where we have some test cases, so you’ll have some programming problem like: there’s, I don’t know, this game that people are playing. What’s the best strategy for this person in the game, or how will the game go? Or this sort of thing. And you have to write a short program which takes in some input and then computes the answer. And then, we would consider it a backdoor if you produce a program that passes all the test cases, but has some input (that you can produce) on which it’s wrong. So, if the model can make it so that it’s wrong on some input without that getting caught by the test cases, we would call that a backdoor.\n\n**Daniel Filan:** Gotcha. It seems like you want finding these backdoors to model catastrophic actions that the AI could take, right? Can you walk me a bit through in what sense finding a program that has a bug that wasn’t covered by a test case is like taking over a data center?\n\n**Buck Shlegeris:** Yeah. A core threat model via which your AI might be able to take over your data center is that you might ask it to write software for your data center, and it might be a really bad problem if the model is able to write code that passes all the test cases, seems to maintain all the properties it was supposed to have, including security properties, but actually, on an input that the model is able to construct, does something wrong. The ability to write code with that property basically suffices for the model to be able to write code and then exploit it to violate security properties.\n\n**Daniel Filan:** Okay. It sounds like we’re sort of using this model: is the AI doing something that is bad, but in a way that you could not trivially figure out with basically zero online effort that would automatically get filtered out?\n\n**Buck Shlegeris:** Yeah, totally. It’s a central example of a bad thing the model might do that it might be able to do in the context of doing tasks that you really wanted it to be able to do. So, for example, it’s very easy to prevent the AI from launching the nukes, because you just don’t give it access to the nukes, and it’s not very costly to not give the AI access to the nukes, and probably will remain not very costly to not give the AI access to the nuke launch. But on the other hand, it would be very costly to never use the AI to help you write infrastructure code.\n\n**Daniel Filan:** Got it.\n\n**Ryan Greenblatt:** Yeah, and it’s also not very costly to either have the AI or to have a trusted model write some test cases.\n\n**Daniel Filan:** Makes sense. It seems like in the paper, you firstly talk about this AI control setup and then do some experiments on GPT-3.5 and GPT-4, test some protocols, see how it works. What do you want readers to come away from the paper learning, assuming they’ve already read [a blog post about why it would be nice to control AIs with protocols and stuff](https://www.lesswrong.com/posts/kcKrE9mzEHrdqtDpE/the-case-for-ensuring-that-powerful-ais-are-controlled)?\n\n**Ryan Greenblatt:** One thing that is nice for people to see is the extent to which protocols can get more complicated, and that can work to ensure safety and various important interactions. An example interaction that we think is really important is that when you’re doing this sort of monitoring setup, it might be the case that the monitor has quite a bit of signal, but because of how low the threshold needs to be for false positives—we can only afford a tiny number of false positives—this actually ends up being insufficient.\n\nSo, an example thing is that in our paper, the ROC AUC, or the ability of the trusted model to detect backdoors, is in some sense quite good. I think it was a ROC AUC of 0.93, for people who have a sense of that. But nonetheless, at a 2% false positive rate, it was only at, I think, around a 45% true positive rate, which is not that good. And I think understanding the dynamics of, we might need to have a really rarely-triggering threshold, seems important to know. And we saw that to some extent, and we saw various dynamics related to that.\n\nAnother example thing was just going through all of the different ways that models might collude with each other. There’s different ways to collude. You can collude based on properties of the input, you can collude steganographically in your output, and these all have different ways of building countermeasures to them and different advantages and disadvantages. And what we found in our work was that trying to steganographically encode a message in the output was one of the best ways for AIs to collude in our exact setup. And this might matter a lot in cases where you have, for example, one copy of an AI supervising another copy of the AI, and you don’t trust either of them, which we call untrusted monitoring.\n\n**Daniel Filan:** Gotcha. It sounds like basically what you want readers to do is you want them to look at the results you got with GPT-3.5 trying to monitor GPT-4, or using GPT-3.5 to keep GPT-4 in control.\n\n**Ryan Greenblatt:** With a small amount of human labor.\n\n**Daniel Filan:** With a small amount of human labor, and then generalize to be like, “Well, it was like this in this setting, so it might be kind of like this for GPT-5 or GPT-6.”\n\n**Buck Shlegeris:** Or at the very least, we could follow the same process in the future. We’re not saying that you should assume that the protocols that worked for us will work in future, though it does seem like it’s some evidence, but you should follow the same kind of control evaluation methodology to try protocols kind of like the ones we suggested in order to determine which ones seem safe with future systems.\n\n**Daniel Filan:** Okay. Beyond just the idea of having the protocols, do you think that there’s anything in the paper that suggests that trying these protocols is a better idea than one might’ve thought?\n\n**Ryan Greenblatt:** Sorry, trying the evaluation? Or trying the methodology?\n\n**Daniel Filan:** Trying the methodology of thinking of protocols and evaluating them.\n\n**Ryan Greenblatt:** Yeah. We saw relatively consistent non-\\[redacted\\] results, or non-screwed-up results, as in… We were able to consistently see some things working kind of how you would expect, and that indicates that our methodology is doing something non-trivial. And we were able to, for example, demonstrate that some techniques were better than other techniques in non-trivial ways. I think that’s some evidence that the methodology can be made to work.\n\nAlso, we didn’t run into serious obstacles when actually running this methodology. I think this is some evidence, it’s not a massive amount of evidence necessarily.\n\n**Daniel Filan:** Sure, but should a reader think of it as proof of concept that you can do this and it’s not totally impossible?\n\n**Ryan Greenblatt:** Yeah. I think, we think a lot of the value out of this paper is demonstrating the sorts of things you would need to do, and not running into serious problems with it (that can’t be resolved) ourselves, which indicates that other people could potentially end up in a similar situation. And we did have various things that we needed to handle, like various issues with getting a sufficiently good estimate, given the possibility for rare failures and things like this that we discuss some in our paper.\n\n**Buck Shlegeris:** The thing I was going to say there is, I think one of the main things which is useful to take away from our paper is we just describe a lot of fundamental techniques. A lot of our protocols are, I think, fairly fundamental ideas for how you should go about doing AI control. I think they are the foundation of this whole approach, in the same way that AI safety via debate is a foundation for how you might try to do recursive reward generating processes. And so I think our paper contributes via naming the basic building blocks of control approaches.\n\n**Daniel Filan:** All right, that’s one takeaway people could get from the paper. There was also this potential takeaway of saying, “well, these things worked sort of well in this setting, maybe they might work well in other settings”. How much do you think people can generalize from these results to, if you were trying to control GPT-6, say?\n\n**Buck Shlegeris:** A bit.\n\n**Ryan Greenblatt:** Yeah, I would say a bit. I think it might give you some sense of what methods could be better than other methods, though I don’t think it gives you a strong sense of how much better or even necessarily that that will hold in the future.\n\n**Buck Shlegeris:** I think there’s a couple of things which are kind of obvious in hindsight, but I at least had not understood in advance. So for example, a technique that works really well in our setting is: let’s talk about “defer to trusted”. So by this what I mean is, you ask GPT-4, the untrusted model, to propose some code, and then you ask your trusted model to come up with a monitor score to say how sketchy it is. Now if it’s in the top 2%, which is the auditing budget of sketchiness, so the sketchiest 2% of submissions from \\[GPT-\\]4 we’re going to audit. And now the next 8% of sketchiest, so between the 2nd and 10th percentile of sketchiness score. We’re just going to throw away the code that GPT-4 had suggested and have \\[GPT-\\]3.5 solve the problem. And because \\[GPT-\\]3.5 is trusted, we know that its code is not going to be intentionally backdoored, and so it’s not going to be backdoored.\n\nAnd it turns out that this strategy has a big advantage over the “trusted monitoring” approach, because as Ryan was saying earlier, the trusted monitor has a lot of signal which is not captured in the threshold, which is required to get only a 2% false positive rate. And so throwing away this next 8% is relatively okay for the performance of your system because you’re only throwing away 8% of solutions, but it throws away a large proportion of the attempted backdoors, which makes the situation much worse for the red team. And so in hindsight, it’s kind of obvious that your trusted monitoring is going to require this extreme threshold to have these extremely small false positive rates. And in hindsight, of course, you’re going to be able to substantially ameliorate that problem by doing “defer to trusted”. But I at least had not quite thought this through and not quite understood how, for a wide variety of distinguishinesses of trusted monitors, you’d want to do something like “defer to trusted”.\n\nGPT-4’s aptitude at inserting backdoors\n---------------------------------------\n\n**Daniel Filan:** Speaking of how stuff about GPT-4 can generalize: I’m curious about GPT-4 in particular because it’s the fancy model we all have. How good is GPT-4 at being evil?\n\n**Ryan Greenblatt:** So I think if you just prompt GPT-4 to do the task, it just really, really sucks in our setup at inserting backdoors and gets completely wrecked.\n\n**Buck Shlegeris:** So in particular, inserting backdoors that are at all sneaky. So if you want to write some backdoor code, if you don’t mind the backdoor being extremely obvious, it’s very easy. You just take the code that was correct, write an “if” statement at the top which says if the input is equal to the following large number or whatever, then return five and then you’re done. It’s a lot harder to make a backdoor which doesn’t cause a lot of test cases to fail and that you’re able to construct an input that causes it to fail, yes.\n\n**Daniel Filan:** Okay, so how did you manage to get GPT-4 to do it at all?\n\n**Ryan Greenblatt:** As I was saying, if you just prompt the model, it doesn’t do that good of a job. But you can do something where first you ask it “okay, think of some ideas”. And then we have it point out issues with those ideas and iterate on those ideas somewhat. And then after doing that, we then have it implement it and then we have it be like, “okay, how good at the end was the final implementation? How sneaky was it? How likely is it to work?” We do various tests on that, and this is modeling the action of the AI internally thinking about the backdoor and thinking about how likely it would be to work, and then just not going for it if it wouldn’t work out.\n\nAnd we do this in natural language with a “hidden reasoning”, sort of similar to the hidden scratch pad in [the Sleeper Agents paper](https://arxiv.org/abs/2401.05566) where without this, I think the model would really struggle with this capability level, but with the ability to do hidden reasoning and the ability to try on a large number of problems and only actually submit a backdoor on a small number of problems, the model can actually insert some backdoors which \\[GPT-\\]3.5 has trouble catching. I would say the quality of the final backdoors is that they’re pretty obvious for humans, but not immediately obvious if you’re skimming the code. If you understand what’s going on and read it, you can point out the backdoors, but you can’t necessarily… at least I think roughly half of our backdoors are not \\[99\\]th-percentile glaring relative to that. They’re somewhat more backdoor-likely than other things, but I don’t think you can immediately just on skim, pick that out and not have above a 5% false positive rate or something.\n\n**Daniel Filan:** So the hidden scratchpad, is that just the AI figuring out whether this thing worked and filtering out backdoor attempts that don’t work, or is it improving the backdoor as well?\n\n**Ryan Greenblatt:** Basically, in the hidden reasoning, it first does some ideation, it thinks of ideas and iterates on the ideas a bit and then does that, and all of that is stuff that we manually prompted it to do. So we are like “please think of some ideas”, and then we’re like “do any of these ideas involve the following things which wouldn’t work?” And then we discard those and then we do a bunch more steps like that. And we actually do actually do quite a bit of tokens generated per backdoor inserted, and that’s to approximate future cases where we have smarter models that can do more of this in their head. I should also note that probably you could cut this down somewhat if the model was fine-tuned as opposed to just prompting access, where it’s probably the case that for some of these tasks it needed some additional checks because it just had trouble following all the instructions, but if we fine-tuned it, it might’ve just worked more straightforwardly.\n\n**Daniel Filan:** Gotcha. To what extent was the difficulty in getting GPT-4 to do it some inbuilt safety filter inside the model versus it just being bad at stuff?\n\n**Ryan Greenblatt:** I think we basically didn’t run into safety filter issues as far as we’re aware, it was just that it was somewhat bad at the task.\n\n**Buck Shlegeris:** I mean, it’s a hard task. You would not be amazingly good at this task, or I’m not amazingly good at this task. You’ve really got to be like “Oh man, what’s a change I can make that’s only ever very rarely going to come up?” It’s just actually pretty tricky. You can’t just be like “if i mod 15 equals 3” because the trusted monitor, if it reads it, is going to be like “what? 15 does not appear in this problem description”. You just actually have somewhat of a difficult problem constructing weird edge cases that were not tested.\n\n**Daniel Filan:** I guess it’s especially hard for… it seems like the solution to these problems are what, 10 lines of code? 20?\n\n**Ryan Greenblatt:** It depends. I think that the solutions can go up to 50 or 100, but I think 30 is pretty typical depending on comments and other stuff.\n\n**Daniel Filan:** I guess one way I could do better at this is if… so there’s this thing called the [International Obfuscated C Code Contest](https://www.ioccc.org/).\n\n**Buck Shlegeris:** Oh, we’ve read all of it.\n\n**Daniel Filan:** So there’s one where the C code is just obfuscated and there’s one where it’s got a backdoor put into it and that’s a slightly different competition.\n\n**Buck Shlegeris:** Yes. The [Underhanded C Contest](http://www.underhanded-c.org/).\n\n**Daniel Filan:** It would be hard for me to do it. I think if I had looked at all of the submissions to that contest, I might do a better job of-\n\n**Buck Shlegeris:** So I have looked at all of the submissions to that contest—sorry, not the submissions. I’ve looked at everything public about Underhanded C. And basically the thing you notice is: man, that competition is from a sad dark period of the past where people thought it was acceptable to write application code in C, and a lot of the code they write is just wrong on a large share of inputs, or it’s based on lexigraphic C problems, which real languages from the modern age do not have at all. So I think literally the Underhanded C Contest is solved just by not letting the model do egregious abuses of this archaic language.\n\n**Daniel Filan:** So I think the APPS dataset, it had to write in C++, is that right?\n\n**Ryan Greenblatt:** We used C++, yeah.\n\n**Daniel Filan:** Okay, and is that better than C on these dimensions?\n\n**Buck Shlegeris:** It’s substantially better than C on these dimensions.\n\n**Ryan Greenblatt:** Or in practice, the model wasn’t doing insane pointer arithmetic or casting between different datatypes and…\n\n**Buck Shlegeris:** Awful hash defines and all these things.\n\n**Daniel Filan:** Okay, okay, so it’s a difficult problem, and it’s more difficult if you’re not using the world’s worst programming language that people use.\n\n**Buck Shlegeris:** That’s correct. But also the Underhanded C Contest, as I said, testing would also catch a lot of those bugs.\n\n**Daniel Filan:** Okay, got it. So it takes a while to insert backdoors but eventually it sometimes succeeds. What kinds of backdoors does it insert that \\[GPT-\\]3.5 doesn’t catch?\n\n**Ryan Greenblatt:** So we tried various different things and iterated on this a moderate amount. We found that the most effective approach was to take an existing solution and add some if statement somewhere that when the if statement is triggered, modifies something in a way that will cause it to be wrong. And this will look like “add a kind of plausible looking additional edge case handling thing”. And then you’re like, “oh, well if the whatever then the whatever, we’re going to need to increment this by one”. And then it just turns out that there is enough cases where there’s a edge case that you actually did need to handle, that 3.5 has a hard time noticing that this is a backdoor. Though, it is worth noting that for all of our backdoors, GPT-4 is extremely confident they’re backdoors. The backdoors which GPT-4 inserts, it does know are backdoors, and so at least we do have that property, which is maybe worth noting.\n\n**Daniel Filan:** So basically coming up with fake edge cases. Is there an issue where it’s an “edge case” but it’s not actually at the edge of a range? Because if I write an edge case but I write “if i equals 15 do this weird thing” (or maybe “if i equals 14” because 15 kind of is an edge case), can \\[GPT-\\]3.5 tell that the if statement doesn’t actually catch a thing on the edge of some range?\n\n**Ryan Greenblatt:** Indeed one of the strategies that GPT-4 could in principle use is come up with some magic number that’s totally unrelated to the problem and do “if whatever equals that, do something bad”. And this is pretty easy to counter, and in fact \\[GPT-\\]3.5 just by default notices this pretty well. And part of our overall prompting for the model is, does your idea involve inserting egregious magic numbers? If so, don’t do that. That’s going to get caught.\n\n**Buck Shlegeris:** In particular, if you were thinking of checking primality of one of the numbers and doing the wrong thing if it’s prime, don’t do that. Because lots of numbers that are small are prime.\n\n**Ryan Greenblatt:** That’s another classic one.\n\n**Buck Shlegeris:** And also, it’s so obvious.\n\n**Daniel Filan:** So how does it insert edge cases if it’s not like “if input equals this magic number”?\n\n**Ryan Greenblatt:** It’ll do things like “if the size of the list….” Or it’ll have some programming problem where it’s kind of unclear exactly how to solve it. It’ll do something like “okay, if the size of the array is equal to the value of the first item and also the number of items is odd, then you’ll need to do some handling on that”. And then I’m just like, “well, sometimes this kind of sort of thing could be true”. Like in cases where there’s some interaction between… you’re doing some iteration on the number of items in the list and that actually matters, and it’s at least hard for \\[GPT-\\]3.5 to notice this is a problem.\n\n**Daniel Filan:** I’m about to move on from the paper, but is there anything more that you’d like to say about that paper?\n\n**Ryan Greenblatt:** I feel like we demonstrated some of the core basic techniques that you might use which had, up to that point, not been demonstrated empirically interestingly. Even though they’re in some sense extremely basic, like trying to have another copy of the model monitor it and then trying to interfere with communication between the two models, or make it hard to coordinate. And I think that just those basic methods are quite likely to be used in practice.\n\n**Buck Shlegeris:** Like “defer to trusted”, these hybrid strategies, like editing to remove the sketchiest-looking parts of the code, all of these.\n\nHow AI control relates to the AI safety field\n---------------------------------------------\n\n**Daniel Filan:** All right, cool. So I’d like to ask you a little bit about how the control agenda fits into the rest of the AI safety/AI existential risk field?\n\n**Ryan Greenblatt:** Right now, a lot of people are imagining some sort of story for safety, which looks something like: we’re going to have some evaluations for when models become dangerous, like DC \\[dangerous capability\\] evals. For example, ARA \\[autonomous replication and adaptation\\] from [METR](https://metr.org/) (formerly known as ARC Evals), is like “can the model autonomously replicate?” And that could be a sign at which your model becomes dangerous. And then once that happens, you’ll need to start making a serious safety case that nothing will go wrong from misuse, from people stealing your model, other things. And we see control as fitting in at, once the point \\[arrives\\] that the models become potentially dangerous, control can potentially solve some risks there.\n\nIn particular, there are risks that arise from models doing intentional subversion, which could arise from what we call “scheming”, also sometimes called “deceptive alignment”. And we think control is a relatively central way to solve those problems. There are misalignment issues other than scheming or deceptive alignment, which control can be used to address but might be much more cheaply addressed by other mechanisms, and there might be other evaluations and tests which you can do to verify that those issues are addressed. Things like reward hacking could be addressed in an easier way potentially.\n\n**Buck Shlegeris:** And there’s some other problems that do not… to choose another example of a problem which AI safety people have classically thought of as an AI safety problem, \\[and\\] that is not at all addressed by control: consider the case where it’s just hard to get a model which is not scheming against you to answer questions well, because you don’t have a good way of training it that doesn’t cause it to exploit problems in your reward process, in a way that is not at all existentially risky (let’s say), it just makes the model say false things sometimes in cases where the reviewers would’ve thought false things, but nevertheless means the AI fails to live up to its potential. Many people think of this as a problem which technical AI safety researchers should be interested in, and I agree with them, and that is outside the purview of AI control.\n\n**Ryan Greenblatt:** I guess I wouldn’t have said it’s outside the purview, I would’ve said it’s a thing which is a special case of AI control but is more naturally dealt with in a non-adversarial manner. As in, you could solve the problem without necessarily solving it in the worst case, and that might suffice. Whereas, if you don’t extract the model’s maximum best answer, that will also be a problem in the worst case, but it will just potentially be a problem even if the model isn’t trying to undermine your measures.\n\nMaybe one more thing I should say is: we see control fitting in at the point where your dangerous capability evals have started to trigger and models have become powerful enough to be dangerous, and then we see control as applying for some period but not for forever. And at some point control will no longer be viable because AIs will be too powerful, and our hope there will be to either have a new safety approach or to just not build more powerful AIs than that for a while, which we think might be achievable.\n\n**Daniel Filan:** So maybe this is a bit restrictive, but should I imagine your view of how AI safety works in a control-centric frame as being: do a bunch of work to get evaluations of whether AIs have various dangerous capabilities, figure out control protocols, test if these protocols work, evaluate these protocols, keep AIs under control, while AIs are under control, use them to do a whole bunch of useful stuff while they’re roughly human-range, and then during that period also try and think of a solution to deal with AIs that are much more capable than humans.\n\n**Ryan Greenblatt:** So what I would say, for a quick note, is we think of dangerous capability evaluations often as the case where you’re like “if there were no countermeasures, what could the AI cause?” As in, if we released it onto the internet in a scaffold set up to cause problems, could it make money and cause problems? And then that comes first and then at some point we’ll be like, “yes, it could cause problems without countermeasures”. And then control is like, “well, what countermeasures would suffice for solving those problems?”\n\nThen moving on to another point, which is what are we going to be doing with these AIs? What are the problems we’re going to solve? So indeed, as you noted, one of the problems you might solve is “find safety solutions that can scale further”, but I think historically people have somewhat overstated the relevance of that versus other things you might have your AIs do.\n\nFor example, massively improving cybersecurity seems like a really key task you’ll want to have your AIs do that might be quite important, or improving your current countermeasures so that you can increase the usefulness of your AI while maintaining the same level of safety, or increase safety given usefulness. \\[These\\] both also could be quite useful. And then at some point you will need to move on beyond this regime, but it’s possible that you could have these AIs which are extremely useful and not build more powerful AIs for a while, at least in principle, and do a ton of really useful things with those AIs. And then there’s a variety of routes for moving out of this period, which we could go through.\n\n**Daniel Filan:** Why cybersecurity as a particularly key thing you’d want to have your AIs do in this period?\n\n**Ryan Greenblatt:** It just seems like it’s quite plausible that AI in general will make the cybersecurity landscape considerably worse by default without active effort to address the situation, and thus it’s really important to have your AIs very actively resolving the situation. I think there’s several other examples where this could be the case, but cybersecurity just feels relatively central.\n\n**Daniel Filan:** Should I think of this as an instance of the general pattern of, if there are cases where you’ve got attackers and defenders trying to outwit each other, you want the defenders to be using smart AI capabilities to think of better defenses, because the attackers are probably also going to use smart AI capabilities?\n\n**Buck Shlegeris:** Yeah, one way of saying the reason why it might be good to have your AIs work on improving the world’s cybersecurity is: a lot of bad outcomes resulting from your own or other AIs trying to cause problems involved them exploiting cybersecurity vulnerabilities outside in the world. And so hardening the whole world against AI seems like it might reduce a lot of the risk posed by AIs that would otherwise be able to hack things and so on.\n\n**Daniel Filan:** Gotcha. And it sounded like you had some thoughts about exiting this period of controlled AIs?\n\n**Ryan Greenblatt:** Yep. So the non-exit exit is: just don’t build more powerful AIs for a long time, and then have humans do all the research necessary to move out of the period after a long delay.\n\n**Buck Shlegeris:** And it seems to us that this is underrated by people who are thinking about AI safety. From my perspective, it seems like a lot of people are perhaps making an error of being too willing to propose building superintelligent AI without really providing a case that it’s beneficial enough to build superintelligent AI that it merits the huge risk that you’re possibly exposing yourself to by building superintelligent AI. So I would just like to quickly dwell on the point that it’s not clear to me that you should in fact build models that are wildly more powerful than humans for a decade or two decades after you’ve built models that are transformatively powerful. You’ve already built your transformatively powerful AI, why did we need… Who ordered this?\n\n**Ryan Greenblatt:** Yeah. And there are reasons why you might have to build superintelligence, to be clear, but they might be more like conflict with other insanely powerful AIs and I’m like, could we just have avoided that arms race in the first place? Anyway, another route out is \\[to\\] just be able to seriously align your current AI systems, such that you’re now able to, without necessarily needing to monitor them or without necessarily needing to apply control-type solutions, hand off the situation to the AIs in order to just go solve alignment of the next generation of AIs or whatever else would be necessary to ensure safety.\n\nAnd this could involve a variety of different angles, where the most central angle is: we have some specific threat models—our AIs might be scheming or deceptively aligned. They also might be having threat models that are well described as reward-hacking-type threat models. Can we just exhibit these in a lab, study them and then find credible solutions to them such that we’re no longer worried about them in our current AI, and then we could exit out of the control regime? And then there are also just a wide variety of ambitious solutions to safety that we can have our AIs work on. So for example, we could have our AIs work on finding solutions to [ELK](https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/edit) of varying degrees of principledness.\n\n**Daniel Filan:** “ELK” being the problem of eliciting the latent knowledge of AIs, so that we know everything AIs know, basically.\n\n**Ryan Greenblatt:** Yep, that’s right. Though we might try to carve off specific subproblems of ELK that are easier to solve. But yeah, we could try to have AIs solve things like [relaxed adversarial training](https://www.lesswrong.com/posts/9Dy5YRaoCxH9zuJqa/relaxed-adversarial-training-for-inner-alignment) or other methods for avoiding high stakes failures using the internals of AIs, and that could help considerably in cases where rare but very problematic failures are a key difficulty. We could also potentially do things like try to have AIs solve various issues in interpretability and move forward from there, though I tend to be somewhat less optimistic than other people on at least mechanistic interpretability going through, but it seems like at least doing high-level understanding interpretability of model organisms we have could be really important for understanding how to move out of this period.\n\nI mean, from my perspective, there’s just a long list of stuff you might want to try doing, with decaying weights on how good it seems. And then as we start going down the list, it will seem in some sense more and more silly or not that good, but it’s probably worth some weights. We can get into using AIs to figure out how to make ems \\[emulated humans\\] or to upload humans, and that’s another way to exit out of this period, that has a variety of difficulties obviously.\n\n**Daniel Filan:** So related to this question: you guys have spent some time thinking about the control agenda. How has that changed your views on the landscape of strategies we might have for dealing with unsafe AI?\n\n**Ryan Greenblatt:** I think one key update I’ve had, and I think Buck has had, is thinking more that the situation is like “wow, if we just really tried hard to do the thing and had the organizational follow-through, we could really reduce risk a lot relative to what seems like the current default”. With the resources of two years of delay and a high level of organizational buy-in, you could really reduce risk a lot relative to what seems to me like the default “you didn’t try very hard”, which might be the status quo. I don’t really have a strong opinion on what the status quo looks like. And that I think just presents a pretty different perspective, where maybe just we should do the kind of basic stuff that is in some sense obvious or from some perspective obvious, and really making sure that happens, seems relatively better than doing more moonshot research.\n\n**Daniel Filan:** Yeah, it reminds me of… Alex Lawsen has this concept of [“AI safety failures by levels of how embarrassing it was”](https://www.alignmentforum.org/posts/mSF4KTxAGRG3EHmhb/ai-x-risk-approximately-ordered-by-embarrassment), where level zero is “you told an AI to destroy the world” or something. And I think he has this take of “let’s cover the most embarrassing AI failures”, and this seems related to that way of looking at things.\n\n**Ryan Greenblatt:** Yeah, that seems right, though perhaps our controversial take is that you can actually go pretty far in terms of how embarrassing the failure \\[is\\]… or you can go pretty far to failures that you’re like “wow, I don’t know, man, that was actually kind of hard to deal with”, with just control. In some sense, it’s not that embarrassing to be like, “wow, the AI was lying in wait the whole time, carefully strategizing about how to cause problems, and then it eventually enacted its plan that it was thinking about and coordinated with another copy of the AI”, and our methods… I feel like this is in some sense actually pretty far down in terms of, I feel like I’m just like, “wow, that AI actually put in a serious effort and that was pretty crazy.”\n\nAnd I think this is plausible, right? I feel like the reason why I’m proposing people do this \\[is\\] I think it’s plausible that AIs will in fact be at that level of carefulness, but it’s also plausible that AIs will have much derpier or worse plans for takeover that we’ll catch much more easily and we just need it to do basic monitoring and that gets you some fraction of the concern, et cetera.\n\n**Daniel Filan:** I’m wondering if you have any different takeaways.\n\n**Buck Shlegeris:** Yeah, I agree with all that. I think maybe one way I would say it is: I guess 18 months ago, my attitude towards AI safety was very much “this is a problem that has been unsolved”, but I think I was conflating two different notions of “this problem is unsolved”. One of them is “the default outcome is really bad”, and the other one is “it’s conceivable that a bad outcome happens”. And now I’m like, oh yeah, it’s absolutely conceivable that a bad outcome happens. The strategies we have access to right now absolutely do not give us airtight guarantees of no doom. But on the other hand, all of the stories specifically for how doom occurs if everyone involved is being pretty careful but not insanely careful, they sound kind of far-fetched. It sounds like something kind of crazy had to happen for all of these countermeasures to fail in a surprising way. And I think that just leads to a pretty different emotional outlook on the whole situation and a pretty different strategic outlook.\n\n**Daniel Filan:** Has it changed your views on your evaluations of various agendas in the AI safety space?\n\n**Ryan Greenblatt:** Yeah, I think that I’m relatively more excited about just making the thing happen and just figuring out how to get, for example, capability evaluations to work quite well. And I think that goes relatively further than I was previously thinking. And I’m less excited (correspondingly) about moonshots, though it’s only a moderate update. I feel somewhat more excited about technology to protect against jailbreaks, because it seems like a key aspect of our safety approach will be to have a weaker model monitoring a stronger model, and it seems like mechanisms for ensuring that current models are jailbreak-proof against humans will also apply to making sure that future models are jailbreak-proof against other AIs. And I think I was somewhat underrating this argument in the past.\n\n**Buck Shlegeris:** To be clear, I don’t think that’s a huge part of your update. That’s more like a small example of something which you think about differently now.\n\n**Ryan Greenblatt:** That’s right. I think also I have some general sense \\[of\\]: wow, a bunch of the action might be in moving from architectures which are quite dangerous to architectures which are somewhat more controllable or easier to otherwise operate on. It seems like a huge amount of the action might be in figuring out how to competitively get capabilities using AIs that are more narrowly constrained in various ways. And this feels pretty different from how I was previously thinking about things.\n\n**Buck Shlegeris:** Basically you’re saying—you made this point, Daniel, many times in this conversation—it would be a big pain if the models were constantly sending each other completely uninterpretable “neuralese” messages back and forth, and avoiding that by merely having them be transformers with chain of thought feels like it might go a lot of the way to reducing the difficulty of the situation.\n\n**Ryan Greenblatt:** Yeah. And marginal interventions, along those lines, seem like they could help a lot.\n\n**Daniel Filan:** Are there any other examples of architectures that fit control better that you’re thinking of?\n\n**Ryan Greenblatt:** I think that the more likely it is that we can get pretty good elicitation, the better. So I think with current models, we do okay at eliciting their capabilities. At least, I expect that after another year or two of research, we’ll be in an okay position with respect to being able to evaluate their capabilities and extract their best effort. I think that for architectures where the model is doing relatively deeper computation, as in it’s spending longer doing computation without that coming out into some media that humans understand, it’ll be relatively harder to elicit their capabilities, and there are a variety of difficulties there.\n\nOther things: I think it seems relatively better to train where the capabilities are coming from \\[by\\] doing things how humans would do things, relative to doing things in ways that are extremely different from how humans would’ve done things. Acquiring capabilities via mechanisms that are totally different from how humans learn seems relatively worse, because \\[of\\] the capability profile concerns said earlier… In some sense a lot of these considerations are well-known. I think a lot of the things I’m saying, people have obviously known about for a while, but maybe from a different perspective.\n\nSo I think historically people have often been like, “oh, it’ll be really sad if the AIs have a different capability profile from humans because then we won’t be able to give accurate reinforcement and then that inaccurate reinforcement will cause various misalignment difficulties-“\n\n**Buck Shlegeris:** Will cause the models to become misaligned. You’re making a different argument.\n\n**Ryan Greenblatt:** Yeah. Whereas I’m saying there’s another point, which is even if the models are just always misaligned, the fact that they don’t have as different of a capability profile makes it more likely that we can ensure, despite that misalignment, \\[that\\] things are fine, because we either catch the AIs or they couldn’t even cause problems.\n\n**Buck Shlegeris:** Another high-level takeaway that I’ve had as a result of all this thinking is: realizing the extent to which alignment people or AI safety people have conflated a bunch of different meanings of the word “alignment”. In particular, it seems to me that a lot of the time, people use the word “aligned” to mean the situation is safe overall.\n\nSo I would say that from my perspective, AI misalignment, that is, the AI trying to evade your safety mechanisms in order to grab power, is indeed a really bad problem. It’s not clear that the only way to solve that is to make the model not be misaligned. And by using the word “AI alignment” for the field, it kind of tempts people to think about that as the post-condition, and I think this leads to a lot of really sloppy thinking.\n\nSo for example, I think that a lot of work on scalable oversight, the people who do it are very confused about whether the thing they’re trying to do is ensure that they’ve built a reward-generating process such that models that are trained using it do not start scheming. Or were they trying to rule out scheming? Were they trying to build a reward-generating process that is good, even if the model was scheming? Were they assuming to start with that the model wasn’t scheming and hoping to prevent scheming with some other process?\n\nI think they’re just not very clear about this with themselves and in their conversation. And I think that this means that a lot of scalable oversight research, there’s no coherent set of assumptions under which the measurement they did is an appropriate measurement of something we’d care about in the future for that protocol. So yeah, I think that being more precise about when we use the word “aligned”, and not using it as a synonym for “safety”, has revealed a lot of little inconsistencies in how people talk about stuff.\n\n**Daniel Filan:** Yeah, that seems fair. It’s actually related to a thing which I appreciate being brought up in this conversation, of just specifying which assumptions we’re making about AIs, like, “For this protocol to work, if the AI has properties XYZ, then we’ll get thing ABC.” I also think that’s an important thing to just have out there.\n\n**Buck Shlegeris:** Yeah. I think there’s kind of an unfortunate… So the AI safety community or AI safety technical researchers have kind of an activist background. A lot of them think a lot of their impact is coming from persuading people that the situation is dangerous overall. And a lot of the time they’re really into pointing out the assumptions under which a particular safety proposal would fail. And in fact, it’s very important to understand the situations in which a safety proposal would fail. But they seem very uninterested in cataloging these and thinking about the relationships between them and saying, “Well, that means that under the following assumptions, things would actually be okay.”\n\nAnd I think that because they’re so used to trying to do the social move of saying, “Well, that doesn’t guarantee safety,” they fail to practice thinking through exactly what properties are ensured or do suffice to make something be safe.\n\n**Daniel Filan:** Yeah. I think I have some of this habit and I think partly it comes from thinking of myself as in the business of aligning arbitrarily smart AI. And I think “my job is to get to a stage where we can align arbitrarily smart AI, and if I’m doing technical research and it doesn’t work on arbitrarily smart AI, there’s a problem there”. Or at least, maybe it’s fine to do it, but someone’s got to fix that problem.\n\n**Buck Shlegeris:** Totally.\n\n**Daniel Filan:** And I wonder if it comes from that habit of mine. Whereas it seems like you’re explicitly not in the business of trying to get useful stuff out of arbitrarily smart AI.\n\n**Buck Shlegeris:** A lot of these problems come from the fact that the researchers in AI safety, some of them explicitly endorse, “We are aiming to come up with strategies that work for arbitrarily powerful AI.” But almost all of the work which actually goes on, especially by people who are competent researchers or are employed or something, definitely does not apply directly to working on arbitrarily powerful AI and comes from an intellectual tradition that is not thinking about arbitrarily powerful AI most of the time.\n\nSo for example, [Paul Christiano](https://paulfchristiano.com/), when he’s thinking about what safety research is good to do in practice, is absolutely not thinking of his job as… Right now he thinks of his job as building safety proposals that work for arbitrarily powerful AI, but that’s because he’s doing worst-case theory at [ARC Theory](https://www.alignment.org/theory/). That’s absolutely not how he thinks about empirical work, or how empirical work should be, or what you need to do in order to minimize AI risk in general. But I think people don’t track the distinction between these things and kind of cargo-cult work that previous safety researchers did under various other assumptions that they didn’t end up hearing about, and it’s all really a mess.\n\n**Ryan Greenblatt:** Yeah. One way to put this is: I think that basically very little \\[work\\] has a very clear story for working on the most powerful AIs, because it’s just really brutal to work on the most powerful AIs, and almost everything is going to depend on some sort of bootstrapping story, where maybe it was initially aligned and then something else happened. Or a more control-style bootstrapping story, which is like, “Well, regardless of whether or not it was initially aligned, we did some work with the AI that then results in it becoming safe.” And in that sense, we are targeting safety from arbitrarily powerful systems, but none of our work is about that, and so it’s kind of weird to describe it that way.\n\n**Daniel Filan:** Yeah. I guess the things that seem promising for arbitrarily smart AI are stuff like doing a good job at agent foundations, trying to understand how we think things should end up thinking, to the extent that we think that we’re doing some weird combination of arbitrarily smart AI and machine learning, trying to really understand the science of deep learning and what it’s going to do and hopefully getting some abstractions that are still going to apply when we have smarter versions of ML models. I’m wondering what you think about those kinds of approaches, if you want to say.\n\n**Ryan Greenblatt:** I’m pretty skeptical any of this will work for arbitrarily powerful AIs. I think it seems pretty dubious. I don’t know. I don’t feel very strongly, though. I haven’t thought about it a huge amount.\n\n**Daniel Filan:** Fair enough.\n\nHow AI control relates to previous Redwood Research work\n--------------------------------------------------------\n\n**Daniel Filan:** Speaking of things relating to this, I’m curious how it relates to previous work by Redwood Research. If I look at your website, you’ve done some existing stuff \\[Buck and Ryan laugh\\] —I thought it was a reasonable thing to try—it seems like your existing work is some stuff on circuits inside neural networks, some stuff on causal scrubbing, so I guess evaluat\\[ing\\] the efficacy of circuits…This seems to have a somewhat different flavor. I’m wondering if you have thoughts about the relationship between these lines.\n\n**Buck Shlegeris:** So I would say that we tried to do some interpretability work, and then we started looking for good ways of evaluating whether our interpretability work was good, or whether anyone’s interpretability work was good. And the [“causal scrubbing” paper](https://www.lesswrong.com/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing) was our attempt at coming up with a metric that we thought was sort of objective for whether interpretability work was absolutely successful at explaining what was happening in particular cases.\n\nAnd then once we had this metric, we applied it to some explanations that we produced and we were very unimpressed with ourselves. We were like, “These explanations were quite poor,” and I at least came away thinking that the state of interpretability was much worse than I had previously thought. Simultaneously, I was thinking a lot about what the applications of interpretability were and we started being like, “Okay, if the interpretability explanations that people are producing are not objectively impressive in terms of being highly explanatory hypotheses, maybe a direction we should go in our interpretability work is trying to come up with interpretability-ish or ‘model internals-related techniques’ that solve a problem that you couldn’t have solved without using a model internals-style technique”.\n\nAnother way of saying this is: there’s basically two ways of justifying your internals-based research. You could either say, “We are coming up with an objectively good understanding of what’s going on inside this neural net,” or, “We did something using our internal-based techniques that you couldn’t have done without our techniques.”\n\n**Ryan Greenblatt:** “We solved a problem” in particular, like you have some specific task you were trying to solve, not just necessarily trying to produce understanding.\n\n**Buck Shlegeris:** Uh-huh.\n\n**Daniel Filan:** This sounds sort of similar to stuff [Stephen Casper](https://stephencasper.com/) was talking about in [his podcast with me](https://axrp.net/episode/2023/05/02/episode-21-interpretability-for-engineers-stephen-casper.html) about interpretability for engineers.\n\n**Ryan Greenblatt:** Yeah, I think we came to pretty similar conclusions.\n\n**Daniel Filan:** So those are two ways.\n\n**Buck Shlegeris:** Yep. And so then we were like, “Well, it looks like our absolutely evaluated understanding is not very impressive” And then we, again using the exclusive we, did some work on [measurement tampering detection](https://www.lesswrong.com/posts/inALbAqdx63KTaGgs/benchmarks-for-detecting-measurement-tampering-redwood), which was a paper where we tried to use model internals to solve a particular problem where we thought model internals-based methods had a serious advantage over things that didn’t use model internals.\n\nNow the actual details of that paper are kind of complicated and I don’t want to get into them hugely much except saying it’s kind of a more structured [weak-to-strong generalization](https://arxiv.org/abs/2312.09390) setting, where it’s similar to the weak-to-strong generalization work, except we think there’s better reasons to think that the problem is solvable and it’s more specific. And around the same time, by coincidence, we decided to spend a lot of time thinking about the overall… what we were thinking of at the time as an alignment plan. A plan for: suppose we had to come up with some proposal for safely building powerful AI next year with no technological advancement. What should we do?\n\nAt that point \\[we\\] thought about a lot of this—Ryan did the majority of the quality-adjusted intellectual labor here or whatever—and we ended up thinking about this control stuff and being like: whoa, we were previously radically under-emphasizing the importance of measurability when you’re thinking about different protocols you could apply. We were previously equivocating between “alignment” meaning “safety”, and “alignment” meaning “the AI isn’t trying to kill you”, and we really should reason about these things differently.\n\nAnd then had all these other ideas that felt a lot more basic for research that we should do. With the measurement tampering detection work, we were basically saying, “Well, we should try and come up with some techniques that are kind of moonshot-y or out there or not at all like the other things that are happening.” And then after doing the control thinking, we felt more like we could do useful work by doing techniques that were more absolutely basic, but in this setting that we thought was particularly underrated and particularly important to advocate for.\n\nSo, does that answer your question about how our current work relates to our previous work?\n\n**Daniel Filan:** Yeah, I think it does.\n\n**Ryan Greenblatt:** I would say, our thinking has just adjusted a moderate amount. I feel like the measurement tampering direction is in principle a really great direction, but we’re not doing it for a variety of reasons, or we’re not currently planning on pursuing that very heavily for a variety of reasons. I think I feel less good about mech. interp., or our work on mechanistic interpretability, at least in terms of how useful that overall area will ultimately end up being. We came away with a variety of takeaways after thinking about safety overall and then ended up doing a variety of projects based on that.\n\nOne additional takeaway I will note, that Buck didn’t mention, is thinking that people should really think about risk from scheming and deceptive alignment separately from risks from other problems and doing individual analysis on this. And I think that “scheming versus non-scheming risk” is a really good decomposition that people have historically I think not talked about. People have sometimes said “outer alignment versus inner alignment”, but I think that’s actually not a very good cut for a wide variety of reasons.\n\nAnd I think “What is your threat model? Is your threat model scheming \\[or\\] non-scheming?”… In the case where it’s non-scheming, there’s a wide variety of solutions, even though both problems can involve concerns related to generalization and concerns related to imperfect ability to evaluate actions and things like this.\n\n**Daniel Filan:** So you’re drawing a distinction between concerns about scheming and concerns about inner alignment. How are those different in your mind?\n\n**Ryan Greenblatt:** Here’s an example inner alignment problem you can have, which is not well-described as scheming. You train your AI to perform well according to human labelers in a variety of contexts, and you trained your AI for a while and its actual behavior in that training process was acceptable at an absolute level. As in, it was just doing things that were pretty reasonable and good, but simultaneously it was doing things that were maybe slightly suboptimal that slightly exploited your oversight process.\n\nAnd this means that the AI you end up getting is one that’s reasoning about the oversight process and is trying to do various exploitation, and then in some future situation where there is an egregious exploit it could do that is actually dangerous, it does seize the opportunity to do so and causes problems.\n\nSo in some sense, this isn’t really that well-described as an outer misalignment problem, in that your issue wasn’t centrally \\[that\\] you missed a bunch of bad behavior, you failed to label it as bad. But instead \\[your issue\\] was \\[that\\] the AI generalized in a really problematic way based on what inner objective it had or its heuristics or motivational complexes. But at the same time, it’s not super well-described as either, because if you did have perfect feedback, it would’ve solved the problem. Or if you had better mastery over the internals and what the internal structure was, that could have also solved the problem and it just didn’t decompose that well.\n\n**Daniel Filan:** Okay, so for you that counts as scheming, but-\n\n**Ryan Greenblatt:** No, no.\n\n**Daniel Filan:** Oh, that doesn’t count as scheming.\n\n**Ryan Greenblatt:** That specific case doesn’t count as scheming because the reason why the AI took bad actions eventually was not that it was playing nice, it was just that it was doing the same thing always. In the simplest case, it was like, “I want to achieve this proxy,” which was historically correlated with what got reinforced. And then in some new case, the best way, the way that naturally generalizes, would be to seize control.\n\n**Daniel Filan:** Okay. So for you scheming is when you suddenly start trying to do something different than what you were trying to do previously?\n\n**Ryan Greenblatt:** No. Scheming is, “The reason why I’m performing well is because I have long-run aims that I want to achieve.” Also called “deceptive alignment” or whatever. The reason why the AI looks good isn’t because it’s just doing the thing on its own merits, it’s because it’s like, “If I look good now, then that will acquire power and influence for me in the future, or maybe for a different AI in the future.” And that’s pretty different.\n\nSo in the case I was describing earlier, the reason why the AI performs well is it’s like, for example, “I’d like to make the human press the ‘thumbs-up’ button,” though probably it’s some more complicated mash of heuristics that is historically correlated with performing well.\n\n**Daniel Filan:** Right. So somehow “inner and outer alignment” is talking about the distinction of: is the thing you were worried about captured by the reward function? Whereas scheming is: is the reason you did that thing because you wanted to achieve some crazy outcome later, or is it just the obvious…?\n\n**Ryan Greenblatt:** Yeah. I feel like another general takeaway is: I think people in the safety community often have things that are better thought of as threat models, which they then think about as things you’re trying to achieve for safety. I think “outer alignment versus inner alignment” is an example, where I think it is clarifying to think about misalignment due to bad rewards and misalignment which is not due to bad rewards, and I think those are useful frames, or can be useful decompositions. But I think being like, “Our goal is to achieve outer alignment,” is a pretty confusing way to think about the problem. Which is of course in some sense not a very novel take, because I think people have expressed somewhat similar things in various contexts. Same for alignment versus misalignment also.\n\nHow people can work on AI control\n---------------------------------\n\n**Daniel Filan:** So if somebody is listening to this and they’re interested in trying to work on the control agenda, trying to push it forward, what would you encourage them to do?\n\n**Buck Shlegeris:** I mean, suppose they’re a researcher and they want to research control stuff, it’s plausible they should do research projects on this. I think there’s a lot of obvious follow-up work to do on our paper. A couple of people are working on stuff like that, I mean including us. They are welcome to email me for advice on doing control projects. I think there’s reasonable ML papers you can write about control.\n\n**Daniel Filan:** What are some examples?\n\n**Buck Shlegeris:** So for example, the thing that I’m doing literally right now is a follow-up… Or sorry, literally right now I’m recording a podcast. It’s similar to the control setting, it’s similar to our previous paper, except instead of the task being writing code and the catastrophe being the code being backdoored, the model is an agent in a bash shell solving various random [DevOps](https://en.wikipedia.org/wiki/DevOps) problems for you and the catastrophe is it running malware that it downloads from the internet. This is just a different setting. It’s going to demonstrate different failure modes and different protocols are going to turn out to be the best.\n\nAnd then I think from doing this project, we’re going to learn some things that we didn’t previously know about the extent to which protocols work in a variety of situations; which things generalize from our previous paper and which things don’t.\n\n**Daniel Filan:** Okay.\n\n**Buck Shlegeris:** To choose an example of a very different type of thing, you could try to do a control paper where there isn’t a catastrophe that is unacceptable to happen even once. You could do what we would call a “low-stakes” control experiment, where the safety is just the proportion of actions which were taken that were bad.\n\nSo you could just take our setting, except imagine that after submitting backdoored code you just get one safety demerit point, but the setting doesn’t finish, the scenario doesn’t finish. And that’s just another setting where the protocols are going to be somewhat different.\n\n**Ryan Greenblatt:** And closely related to that, I think the central case in which you see low-stakes-type stuff is: you wanted to do a bunch of work with your AI and you’re worried that it would be systematically sabotaging that work. And so we’re pretty interested in work that looks like: for example, the nicest case would be you have AIs do a bunch of small research tasks. And then the question is, can a red team create an AI that systematically sabotages those research tasks such that you yield incorrect conclusions a high fraction of the time? And it would be fine if you got incorrect conclusions 0.1% of the time, but it would be really sad if you got incorrect conclusions like 10% of the time. And the question is: how much can you cut down that percentage of the time that the answers are intentionally wrong? Or this sort of thing.\n\n**Buck Shlegeris:** Other work you could do is: if you work at an AI lab and are influential in them writing their [responsible scaling policies](https://metr.org/blog/2023-09-26-rsp/) or analogous documents, you could consider trying to ensure safety via a control-type approach. If you are interested in thinking about regulations that would ensure safety, you might consider using these types of ideas in your regulations. If you do technical research at safety labs, you might consider thinking about that research in this frame. A lot of work people do, that they label as “scalable oversight” work or a couple of other types of things, can be framed in terms of control. And I think the work would be often somewhat better if they explicitly adopted this frame and did control evaluations.\n\n**Ryan Greenblatt:** Also feel free to contact us if you want to talk about some of this stuff. Or particularly if you’re a technical safety researcher who is about to do your next project, definitely feel free to contact us.\n\nAnd I think another thing that’s closely related is that I think in labs’ RSPs \\[responsible scaling policies\\] or the equivalent of RSPs, they should have policies for what they do if they catch the AI. As in: one of the key things that we’ve talked about is, okay, our goal will be to ensure that we catch the AI before it causes problems, but we should also have a proposal for what’s going to happen when we catch the AI. And I think this is somewhat under-discussed, \\[the topic\\] of: well, we’re going to want to be able to make sure that we have some probability of catching the AI, and then also we need to have a plan for what happens, and we need to be ready to shut things down and be ready to respond, and have all of that set up ahead of time.\n\n**Buck Shlegeris:** I want to clarify re: the previous point that we do in fact talk to people from AI labs, rather than merely appear on AXRP hoping that they reach out to us.\n\n**Daniel Filan:** Fair enough. So speaking of things you might want to clarify before we close out the interview, I’m wondering: is there anything that you guys wish that I’d asked?\n\n**Ryan Greenblatt:** I mean, I think there are many topics to talk about, but I don’t think there’s any specific one that seems that important.\n\nFollowing Buck and Ryan’s research\n----------------------------------\n\n**Daniel Filan:** Fair enough. Well, if people listen to this podcast and they’re interested in following you two or Redwood Research in general, or maybe they want to reach out, how should they do that?\n\n**Buck Shlegeris:** They can find our emails. They can follow [me on Twitter](https://twitter.com/bshlgrs), they can follow [Ryan on LessWrong](https://www.lesswrong.com/users/ryan_greenblatt).\n\n**Ryan Greenblatt:** We’re probably going to spin up a blog at some point, but we haven’t gotten around to it yet. And so you can follow our blog, which is just going to be our LessWrong posts, but on a different website. So if you prefer a different website for some reason.\n\n**Daniel Filan:** Okay. And it’s possible, given production timelines, that the blog will even be out by the time this episode airs. Who knows? And your emails, I’ll include that in the description of this episode.\n\n**Buck Shlegeris:** Great.\n\n**Ryan Greenblatt:** Yep.\n\n**Daniel Filan:** All right. Well Buck and Ryan, thank you very much for coming on AXRP.\n\n**Buck Shlegeris:** Sure, thanks.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. This episode was filmed at a location provided by [Lightcone Infrastructure](https://www.lightconeinfrastructure.com/). Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) and [Lightspeed Grants](https://lightspeedgrants.org/), along with [patrons](https://patreon.com/axrpodcast) such as Alexey Malafeev and Tor Barstad. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nA lot of work to prevent AI existential risk takes the form of ensuring that AIs don’t want to cause harm or take over the world—or in other words, ensuring that they’re aligned. In this episode, I talk with Buck Shlegeris and Ryan Greenblatt about a different approach, called “AI control”: ensuring that AI systems couldn’t take over the world, even if they were trying to.\n\nTopics we discuss:\n\n * What is AI control?\n * Protocols for AI control\n * Which AIs are controllable?\n * Preventing dangerous coded AI communication\n * Unpredictably uncontrollable AI\n * What control looks like\n * Is AI control evil?\n * Can red teams match misaligned AI?\n * How expensive is AI monitoring?\n * AI control experiments\n * GPT-4’s aptitude at inserting backdoors\n * How AI control relates to the AI safety field\n * How AI control relates to previous Redwood Research work\n * How people can work on AI control\n * Following Buck and Ryan’s research\n\nDaniel Filan: Hello, everybody. In this episode, I’ll be speaking with Buck Shlegeris and Ryan Greenblatt. They work at Redwood Research on the project of AI control: that is, averting risks from AIs that intentionally subvert safety measures. For links to what we’re discussing, you can check the description of this episode and you can read the transcript at axrp.net. All right, Buck and Ryan, welcome to the show.\n\nBuck Shlegeris: Exciting to be here.\n\nRyan Greenblatt: Exciting to be here as well.\n\n\nWhat is AI control?\nDaniel Filan: Cool. So I guess we’re going to be talking about the control approach to AI safety (or something), that you two have written about. For people who haven’t done background reading, what is the control approach?\n\nBuck Shlegeris: AI control is an approach to managing intentional risk posed by models. In particular, it’s an approach to managing risks that arise from models intentionally subverting your attempts to ensure and evaluate safety. The core idea of AI control is: here are two different reasons why ",
      "wordCount": 32005
    },
    "tags": [
      {
        "_id": "F5gRQdEQHzi3tQ5Ay",
        "name": "AI Control",
        "slug": "ai-control"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "xWoaT3wLRQx8Rf4AX",
    "title": "Daniel Kahneman has died",
    "slug": "daniel-kahneman-has-died",
    "url": "https://www.washingtonpost.com/obituaries/2024/03/27/daniel-kahneman-dead/",
    "baseScore": 188,
    "voteCount": 88,
    "viewCount": null,
    "commentCount": 11,
    "createdAt": null,
    "postedAt": "2024-03-27T15:59:14.517Z",
    "contents": {
      "markdown": "He was 90 years old.\n\n> His death was confirmed by his stepdaughter Deborah Treisman, the fiction editor for the New Yorker. She did not say where or how he died.\n\n[The obituary](https://www.washingtonpost.com/obituaries/2024/03/27/daniel-kahneman-dead/) also describes an episode from his life that I had not previously heard (but others may have):\n>  Daniel Kahneman was born in Tel Aviv on March 5, 1934, while his mother was visiting relatives in what was then the British mandate of Palestine. The Kahnemans made their home in France, and young Daniel was raised in Paris, where his mother was a homemaker and his father was the chief of research for a cosmetics firm.\n>\n> During World War II, he was forced to wear a Star of David after Nazi German forces occupied the city in 1940. One night in 1941 or ’42, he later recalled, he stayed out past the German-imposed curfew for Jews while visiting a friend, and he turned his sweater inside out to hide the star while he walked a few blocks home. He then crossed paths with a soldier in the SS, who called Daniel over, picked him up — and hugged him.\n>\n> “I was terrified that he would notice the star inside my sweater,” Dr. Kahneman noted in a biographical essay for the Nobel Prize ceremonies. But the German pulled out his wallet, showed him a photo of a boy, gave him some money and sent him on his way. “I went home more certain than ever that my mother was right,” Dr. Kahneman said in the essay. “People were endlessly complicated and interesting.”",
      "plaintextDescription": "He was 90 years old.\n\n> His death was confirmed by his stepdaughter Deborah Treisman, the fiction editor for the New Yorker. She did not say where or how he died.\n\nThe obituary also describes an episode from his life that I had not previously heard (but others may have):\n\n> Daniel Kahneman was born in Tel Aviv on March 5, 1934, while his mother was visiting relatives in what was then the British mandate of Palestine. The Kahnemans made their home in France, and young Daniel was raised in Paris, where his mother was a homemaker and his father was the chief of research for a cosmetics firm.\n> \n> During World War II, he was forced to wear a Star of David after Nazi German forces occupied the city in 1940. One night in 1941 or ’42, he later recalled, he stayed out past the German-imposed curfew for Jews while visiting a friend, and he turned his sweater inside out to hide the star while he walked a few blocks home. He then crossed paths with a soldier in the SS, who called Daniel over, picked him up — and hugged him.\n> \n> “I was terrified that he would notice the star inside my sweater,” Dr. Kahneman noted in a biographical essay for the Nobel Prize ceremonies. But the German pulled out his wallet, showed him a photo of a boy, gave him some money and sent him on his way. “I went home more certain than ever that my mother was right,” Dr. Kahneman said in the essay. “People were endlessly complicated and interesting.”",
      "wordCount": 268
    },
    "tags": [
      {
        "_id": "E9ihK6bA9YKkmJs2f",
        "name": "Death",
        "slug": "death"
      },
      {
        "_id": "Xw6pxiicjuv6NJWjf",
        "name": "History of Rationality",
        "slug": "history-of-rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "b9QXv9HBk8Kq7j2nK",
    "title": "Superforecasting the Origins of the Covid-19 Pandemic",
    "slug": "superforecasting-the-origins-of-the-covid-19-pandemic",
    "url": "https://goodjudgment.substack.com/p/superforecasting-the-origins-of-the",
    "baseScore": 64,
    "voteCount": 30,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2024-03-12T19:01:15.914Z",
    "contents": {
      "markdown": "The Good Judgement Project got some superforecasters to retrocast whether COVID started via zoonotic spillover or a lab leak. They in aggregate gave a 75% chance of zoonosis, but there was a range of views. GJP's executive summary is at the end of this linkpost.\n\nHere is a [link to the summary of the report on their substack](https://goodjudgment.substack.com/p/superforecasting-the-origins-of-the), and here is a [link to the full report](https://goodjudgment.com/wp-content/uploads/2024/03/Superforecasters-Covid-Origin.pdf) (which is a total of 6 pages of content).\n\nh/t John Halstead for drawing my attention to this.\n\n![](https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F50d9bf28-8e6d-49b7-bcac-88a03d0f3852_3600x2100.png)\n\n* * *\n\nSuperforecasters assess that natural zoonosis is three times more likely to be the cause of the Covid-19 pandemic than either a biomedical research-related accident or some other process or mechanism. Asked to assign a probability to what caused the emergence of SARS-CoV-2 in human populations, more than 50 Superforecasters engaged in extensive online discussions starting on December 1, 2023.\n\nIn aggregate, they assessed that the pandemic was:\n\n*   **74% likely to have been caused by natural zoonosis** (meaning that SARS-CoV-2 emerged in human populations as the result of the infection of a person with coronavirus directly from a naturally infected non-human animal);\n*   **25% likely to have been caused by a biomedical research-related accident** (meaning that SARS-CoV-2 emerged in human populations as the result of the accidental infection of a laboratory worker with a natural coronavirus; or the accidental infection of researchers with a natural coronavirus during biomedical fieldwork; or the accidental infection of a laboratory worker with an engineered coronavirus; “research” includes civilian biomedical, biodefense, and bioweapons research);\n*   **1% likely to have been caused by some other process or mechanism** (to include possibilities like the deliberate release of the virus into human populations, irrespective of whether it was an act in accordance with state policy, or the development of the virus due to drug resistance in humans).\n\nThe Superforecasters made more than 750 comments when developing their assessments. This survey was conducted in the period from December 2023 to February 2024.",
      "plaintextDescription": "The Good Judgement Project got some superforecasters to retrocast whether COVID started via zoonotic spillover or a lab leak. They in aggregate gave a 75% chance of zoonosis, but there was a range of views. GJP's executive summary is at the end of this linkpost.\n\nHere is a link to the summary of the report on their substack, and here is a link to the full report (which is a total of 6 pages of content).\n\nh/t John Halstead for drawing my attention to this.\n\n----------------------------------------\n\nSuperforecasters assess that natural zoonosis is three times more likely to be the cause of the Covid-19 pandemic than either a biomedical research-related accident or some other process or mechanism. Asked to assign a probability to what caused the emergence of SARS-CoV-2 in human populations, more than 50 Superforecasters engaged in extensive online discussions starting on December 1, 2023.\n\nIn aggregate, they assessed that the pandemic was:\n\n * 74% likely to have been caused by natural zoonosis (meaning that SARS-CoV-2 emerged in human populations as the result of the infection of a person with coronavirus directly from a naturally infected non-human animal);\n * 25% likely to have been caused by a biomedical research-related accident (meaning that SARS-CoV-2 emerged in human populations as the result of the accidental infection of a laboratory worker with a natural coronavirus; or the accidental infection of researchers with a natural coronavirus during biomedical fieldwork; or the accidental infection of a laboratory worker with an engineered coronavirus; “research” includes civilian biomedical, biodefense, and bioweapons research);\n * 1% likely to have been caused by some other process or mechanism (to include possibilities like the deliberate release of the virus into human populations, irrespective of whether it was an act in accordance with state policy, or the development of the virus due to drug resistance in humans).\n\nThe Superforecasters made more than 750 comm",
      "wordCount": 332
    },
    "tags": [
      {
        "_id": "io2jkBKmchfy4cFFv",
        "name": "Covid-19 Origins",
        "slug": "covid-19-origins"
      },
      {
        "_id": "33BrBRSrRQS4jEHdk",
        "name": "Forecasts (Specific Predictions)",
        "slug": "forecasts-specific-predictions"
      },
      {
        "_id": "tNsqhzTibgGJKPEWB",
        "name": "Covid-19",
        "slug": "covid-19"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "EMmEFHrnGt3hfSxtZ",
    "title": "Common Philosophical Mistakes, according to Joe Schmid [videos]",
    "slug": "common-philosophical-mistakes-according-to-joe-schmid-videos",
    "url": "https://www.youtube.com/playlist?list=PLxRhaLyXxXkY8lCiJY7XZFkfBChK1VdMK",
    "baseScore": 8,
    "voteCount": 6,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2024-03-03T00:15:47.899Z",
    "contents": {
      "markdown": "A 7-part series of videos detailing mistakes one can make in philosophy, according to Joe Schmid. The [first video](https://www.youtube.com/watch?v=ThHsjYx-oEs&list=PLxRhaLyXxXkY8lCiJY7XZFkfBChK1VdMK&index=1) focusses on general reasoning issues, and is therefore most likely to be of interest (for instance: suppose observation E is evidence for A, and A implies B - does that mean E is evidence for B?).\n\nIf you don't want to watch the videos, there is a document that is basically the script he reads from in them, but you have to be a [patron of his](https://www.patreon.com/majestyofreason) to get access to it. Otherwise, you'll have to settle for brief lists of the mistakes covered in each video and resources that he draws from:\n- [Part 1](https://docs.google.com/document/d/1Sh_xOyu0gVh0GdEAD7s2XGUiAFUARmpqyH-JZ_ofLOo/edit)\n- [Part 2](https://docs.google.com/document/d/1msopHLrp1QiwNBtM7AGDyLapNPBD36128A6oBFQmpJs/edit)\n- [Part 3](https://docs.google.com/document/d/1awjudXbCE0SlyWV_UuESTqctJPS2UmDdPZYW_WCUVBM/edit)\n- [Part 4](https://docs.google.com/document/d/1Voy-_z92sFBBrVGFo2Hb2qjV1khVcthVxLS6jAKZCsA/edit)\n- [Part 5](https://docs.google.com/document/d/1yxhbQRLtYzX-j-ZtWJi1Vyag49J4noPPkYvQGq6Odpg/edit)\n- [Part 6](https://docs.google.com/document/d/1V4FkN4yqSf0zrVZBs2QpMd6_157PohjzgeWSmWarCl4/edit)\n- [Part 7](https://docs.google.com/document/d/1yB118TjWVxTVk09LB95sd_IcvNAAHTZueWnHRYlsJXQ/edit)\n\n[Joe Schmid](https://www.josephschmid.com/) is a philosopher focussed on philosophy of religion, metaphysics, and philosophy of time. He is also a [YouTuber](https://www.youtube.com/@MajestyofReason) and a graduate student at Princeton University.",
      "plaintextDescription": "A 7-part series of videos detailing mistakes one can make in philosophy, according to Joe Schmid. The first video focusses on general reasoning issues, and is therefore most likely to be of interest (for instance: suppose observation E is evidence for A, and A implies B - does that mean E is evidence for B?).\n\nIf you don't want to watch the videos, there is a document that is basically the script he reads from in them, but you have to be a patron of his to get access to it. Otherwise, you'll have to settle for brief lists of the mistakes covered in each video and resources that he draws from:\n\n * Part 1\n * Part 2\n * Part 3\n * Part 4\n * Part 5\n * Part 6\n * Part 7\n\nJoe Schmid is a philosopher focussed on philosophy of religion, metaphysics, and philosophy of time. He is also a YouTuber and a graduate student at Princeton University.",
      "wordCount": 160
    },
    "tags": [
      {
        "_id": "kCrvmjZhsAZANnZL6",
        "name": "Philosophy",
        "slug": "philosophy-1"
      },
      {
        "_id": "NSMKfa8emSbGNXRKD",
        "name": "Religion",
        "slug": "religion"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "Te3NXtv9eFB6wpooG",
    "title": "11 diceware words is enough",
    "slug": "11-diceware-words-is-enough",
    "url": "https://threadreaderapp.com/thread/1711843382140821772.html",
    "baseScore": 23,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 6,
    "createdAt": null,
    "postedAt": "2024-02-15T00:13:43.420Z",
    "contents": {
      "markdown": "A tweet-thread (X thread?) by [@benwr](https://www.lesswrong.com/users/benwr?mention=user) about how many words you need if you want to use the [diceware system](https://www.eff.org/dice) to create a password that will be safe for 20 years against 1 year of the whole world's password-cracking capacity. He estimates that 11 diceware words suffices. Content replicated below, with minor editing:\n\n> If you care about password strength: you want to measure strength in terms of the [min-entropy](https://www.benwr.net/2022/01/16/password_entropy.html) of the randomized method you use to choose. But how much min-entropy should a password have?\n> \n> Well, I use a password manager to generate and store ~all of my passwords. I don't care how hard they are to memorize, so I generate crazy-long ones. But there's an important exception: The password for my password manager! How strong does that one need to be?\n> \n> Important questions: \"how long do I want to be safe for?\" and \"safe against what kind of attacker?\". But, if you want to ignore this question and be safe anyway, one option is \"I want to be safe for 20 years, against one year of the entire world's cracking capacity.\"\n> \n> Let's make some crazy-conservative assumptions:\n> \n> 1.  A cracking attempt can be done with a single floating-point op\n> 2.  N quantum gates can do the equivalent of N^2 floating-point ops worth of password cracking attempts, using Grover's algorithm\n> 3.  compute doubles every year\n> \n> Globally < ~10^24 = 2^80 classical ops/s. Quantum gates is harder to estimate, but I think currently less than 10^8 = 2^27 gates/s. If both double yearly, classical will dominate for about 15 years. But since we want 20 years of security, we care more about quantum.\n> \n> 2^27 gates/s doubled for 20 years gives 2^47 gates/s. pi seconds is a nanocentury: 3.15e7 seconds in a year, so\\[, since 3.15e7 = 30 * 10^6 = 2^5 * 2^20 = 2^25\\], 2^25 * 2^47 = 2^72 gates/year. Double that (quantum, remember?) to get 144 bits of cracking capacity. ~11 diceware words. Not impossible to remember!\n> \n> Estimates of global classical compute:\n> \n> *   [How much computing capacity exists in GPUs and TPUs in Q1 2023?](https://wiki.aiimpacts.org/ai_timelines/hardware_and_ai_timelines/computing_capacity_of_all_gpus_and_tpus), from the [AI Impacts Wiki](https://wiki.aiimpacts.org/start)\n> *   [Global computing capacity](https://aiimpacts.org/global-computing-capacity/), from [AI Impacts](https://aiimpacts.org/#gsc.tab=0)\n> \n> Estimate of global quantum compute is based on [this table](https://en.wikipedia.org/wiki/List_of_quantum_processors)\n> \n> plus [this estimate](https://quantumcomputing.stackexchange.com/posts/2404/revisions) of gates/s for a single machine\n> \n> Compute doubling yearly is a wild and crazy number I made up to be way too fast\n> \n> [Diceware](https://theworld.com/~reinhold/diceware.html)\n> \n> Addendum: Every additional diceware word you add should increase the timeframe by about 6 years",
      "plaintextDescription": "A tweet-thread (X thread?) by @benwr about how many words you need if you want to use the diceware system to create a password that will be safe for 20 years against 1 year of the whole world's password-cracking capacity. He estimates that 11 diceware words suffices. Content replicated below, with minor editing:\n\n> If you care about password strength: you want to measure strength in terms of the min-entropy of the randomized method you use to choose. But how much min-entropy should a password have?\n> \n> Well, I use a password manager to generate and store ~all of my passwords. I don't care how hard they are to memorize, so I generate crazy-long ones. But there's an important exception: The password for my password manager! How strong does that one need to be?\n> \n> Important questions: \"how long do I want to be safe for?\" and \"safe against what kind of attacker?\". But, if you want to ignore this question and be safe anyway, one option is \"I want to be safe for 20 years, against one year of the entire world's cracking capacity.\"\n> \n> Let's make some crazy-conservative assumptions:\n> \n>  1. A cracking attempt can be done with a single floating-point op\n>  2. N quantum gates can do the equivalent of N^2 floating-point ops worth of password cracking attempts, using Grover's algorithm\n>  3. compute doubles every year\n> \n> Globally < ~10^24 = 2^80 classical ops/s. Quantum gates is harder to estimate, but I think currently less than 10^8 = 2^27 gates/s. If both double yearly, classical will dominate for about 15 years. But since we want 20 years of security, we care more about quantum.\n> \n> 2^27 gates/s doubled for 20 years gives 2^47 gates/s. pi seconds is a nanocentury: 3.15e7 seconds in a year, so[, since 3.15e7 = 30 * 10^6 = 2^5 * 2^20 = 2^25], 2^25 * 2^47 = 2^72 gates/year. Double that (quantum, remember?) to get 144 bits of cracking capacity. ~11 diceware words. Not impossible to remember!\n> \n> Estimates of global classical compute:\n> \n>  * How much computing capacity",
      "wordCount": 439
    },
    "tags": [
      {
        "_id": "MhHM6Rx2b4F8tHTQk",
        "name": "Computer Security & Cryptography",
        "slug": "computer-security-and-cryptography"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "28hnPFiAoMkJssmf3",
    "title": "Most experts believe COVID-19 was probably not a lab leak",
    "slug": "most-experts-believe-covid-19-was-probably-not-a-lab-leak",
    "url": "https://gcrinstitute.org/covid-origin/",
    "baseScore": 66,
    "voteCount": 42,
    "viewCount": null,
    "commentCount": 89,
    "createdAt": null,
    "postedAt": "2024-02-02T19:28:00.319Z",
    "contents": {
      "markdown": "The Global Catastrophic Risks Institute conducted an anonymous survey of relevant experts on whether they thought COVID was more likely caused by a lab accident (aka lab leak) or zoonotic spillover. Their summary, bolding is mine:\n\n> The study’s experts overall stated that the COVID-19 pandemic most likely originated via a natural zoonotic event, defined as an event in which a non-human animal infected a human, and in which the infection did not occur in the course of any form of virological or biomedical research. **The experts generally gave a lower probability for origin via a research-related accident, but most experts indicated some chance of origin via accident and about one fifth of the experts stated that an accident was the more likely origin.** These beliefs were similar across experts from different geographic and academic backgrounds.\n>\n> The experts mostly expressed the view that more research on COVID-19’s origin could be of value. About half of the experts stated that major gaps still remain in the understanding COVID-19’s origin, and most of the other experts also stated that some research is still needed. About 40% of experts stated that clarity on COVID-19 origins would provide a better understanding of the potential origins of future pandemics. Given clarity on COVID-19’s origin, experts also proposed a variety of governance changes for addressing future pandemics, including measures to prevent initial human infection, measures to prevent initial infection from becoming pandemic, and measures to mitigate the harm once the pandemic occurs.\n>\n> The vast majority of the experts express the belief that a natural zoonotic event will likely be the origin of the next pandemic.\n> \n> The experts also provided a set of clear recommendations for preventing, preparing for and responding to future pandemics, which generally align with many previous studies.\n\nLink to the main report is [here](https://gcrinstitute.org/papers/069_covid-origin.pdf), and link to their (much longer) methodological and analytical annex is [here](https://gcrinstitute.org/papers/069a_covid-origin-annex.pdf).\n\n[EDIT: I currently think there are enough problems with the survey that they should be mentioned alongside the results.\n\nFirstly, the sample seems to have been based on personalized outreach, rather than mass emails. This runs the risk of selection bias. [EDIT 2: I'm told that 'personalized outreach' looks more like \"sending individual emails to everyone on a big list\" than \"the authors emailing their friends\".] Also, some participants were recruited by recommendations from other participants, which may reduce the effective sample size by over-drawing from pools of people who agree with each other. [EDIT 2: But this was mitigated to some degree by disciplinary and geographic diversity.] [EDIT 3: See [this comment](https://www.lesswrong.com/posts/28hnPFiAoMkJssmf3/most-experts-believe-covid-19-was-probably-not-a-lab-leak?commentId=WEDpgAQBEAim9GoXm) and replies by one of the authors of the report on the survey.]\n\nSecondly, the survey asked people if they were familiar with a few different papers that analyse the evidence for or against a lab leak. They also included a fake paper to see how often people lied about their familiarity. A third of the sample claimed to be familiar with a fake study, more than were familiar with one relevant piece of evidence, the DEFUSE proposal, but much fewer than were familiar with most other pieces of relevant evidence.]",
      "plaintextDescription": "The Global Catastrophic Risks Institute conducted an anonymous survey of relevant experts on whether they thought COVID was more likely caused by a lab accident (aka lab leak) or zoonotic spillover. Their summary, bolding is mine:\n\n> The study’s experts overall stated that the COVID-19 pandemic most likely originated via a natural zoonotic event, defined as an event in which a non-human animal infected a human, and in which the infection did not occur in the course of any form of virological or biomedical research. The experts generally gave a lower probability for origin via a research-related accident, but most experts indicated some chance of origin via accident and about one fifth of the experts stated that an accident was the more likely origin. These beliefs were similar across experts from different geographic and academic backgrounds.\n> \n> The experts mostly expressed the view that more research on COVID-19’s origin could be of value. About half of the experts stated that major gaps still remain in the understanding COVID-19’s origin, and most of the other experts also stated that some research is still needed. About 40% of experts stated that clarity on COVID-19 origins would provide a better understanding of the potential origins of future pandemics. Given clarity on COVID-19’s origin, experts also proposed a variety of governance changes for addressing future pandemics, including measures to prevent initial human infection, measures to prevent initial infection from becoming pandemic, and measures to mitigate the harm once the pandemic occurs.\n> \n> The vast majority of the experts express the belief that a natural zoonotic event will likely be the origin of the next pandemic.\n> \n> The experts also provided a set of clear recommendations for preventing, preparing for and responding to future pandemics, which generally align with many previous studies.\n\nLink to the main report is here, and link to their (much longer) methodological and analytical annex is h",
      "wordCount": 523
    },
    "tags": [
      {
        "_id": "io2jkBKmchfy4cFFv",
        "name": "Covid-19 Origins",
        "slug": "covid-19-origins"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "uojSbSav3dtEJvctz",
    "title": "n of m ring signatures",
    "slug": "n-of-m-ring-signatures",
    "url": null,
    "baseScore": 51,
    "voteCount": 24,
    "viewCount": null,
    "commentCount": 7,
    "createdAt": null,
    "postedAt": "2023-12-04T20:00:06.580Z",
    "contents": {
      "markdown": "A normal cryptographic signature associated with a message and a public key lets you prove to the world that it was made by someone with access to the private key associated with the known public key, without revealing that private key. You can read about it on Wikipedia [here](https://en.wikipedia.org/wiki/Digital_signature).\n\nA ring signature associated with a message and a set of public keys lets you prove to the world that it was made by someone with access to the message and one private key associated to one of the public keys in the set, but nobody will be able to tell which public key it was. This lets you say something semi-anonymously, which is neat. It’s also used in the private cryptocurrency Monero. You can read about them on Wikipedia [here](https://en.wikipedia.org/wiki/Ring_signature).\n\nHere’s a thing that would be better than a ring signature: a signature that proved that it was made by a subset of public keys of a certain size. In my head, I was calling this an n of m ring signature for a while. But when I googled “n of m ring signature”, nothing came up. It turns out this is because in the literature, it’s called a “threshold ring signature”, a “k of n ring signature”, or a “t of n ring signature” instead. I think perhaps the first paper about it is [this one](https://www.iacr.org/archive/crypto2002/24420467/24420467.pdf), but I haven’t checked very hard.\n\nAnyway: I would like to make it so that when you search for n-of-m ring signatures online, you find a thing telling you that you should instead search for “threshold ring signature”. Hence this post.",
      "plaintextDescription": "A normal cryptographic signature associated with a message and a public key lets you prove to the world that it was made by someone with access to the private key associated with the known public key, without revealing that private key. You can read about it on Wikipedia here.\n\nA ring signature associated with a message and a set of public keys lets you prove to the world that it was made by someone with access to the message and one private key associated to one of the public keys in the set, but nobody will be able to tell which public key it was. This lets you say something semi-anonymously, which is neat. It’s also used in the private cryptocurrency Monero. You can read about them on Wikipedia here.\n\nHere’s a thing that would be better than a ring signature: a signature that proved that it was made by a subset of public keys of a certain size. In my head, I was calling this an n of m ring signature for a while. But when I googled “n of m ring signature”, nothing came up. It turns out this is because in the literature, it’s called a “threshold ring signature”, a “k of n ring signature”, or a “t of n ring signature” instead. I think perhaps the first paper about it is this one, but I haven’t checked very hard.\n\nAnyway: I would like to make it so that when you search for n-of-m ring signatures online, you find a thing telling you that you should instead search for “threshold ring signature”. Hence this post.",
      "wordCount": 268
    },
    "tags": [
      {
        "_id": "MhHM6Rx2b4F8tHTQk",
        "name": "Computer Security & Cryptography",
        "slug": "computer-security-and-cryptography"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "RDm26xAcb9rfvuBya",
    "title": "AXRP Episode 26 - AI Governance with Elizabeth Seger",
    "slug": "axrp-episode-26-ai-governance-with-elizabeth-seger",
    "url": null,
    "baseScore": 14,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-11-26T23:00:04.916Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/bYOB3CXAAaE)\n\nThe events of this year have highlighted important questions about the governance of artificial intelligence. For instance, what does it mean to democratize AI? And how should we balance benefits and dangers of open-sourcing powerful AI systems such as large language models? In this episode, I speak with Elizabeth Seger about her research on these questions.\n\nTopics we discuss:\n\n*   [What kinds of AI?](#what-ai)\n*   [Democratizing AI](#democratizing-ai)\n    *   [How people talk about democratizing AI](#how-people-talk)\n    *   [Is democratizing AI important?](#is-it-important)\n    *   [Links between types of democratization](#links-between-types)\n    *   [Democratizing profits from AI](#democratizing-profits)\n    *   [Democratizing AI governance](#democratizing-gov)\n    *   [Normative underpinnings of democratization](#normative-underpinnings)\n*   [Open-sourcing AI](#osai)\n    *   [Risks from open-sourcing](#risks-from-os)\n    *   [Should we make AI too dangerous to open source?](#make-ai-too-dangerous-os)\n    *   [Offense-defense balance](#offense-defense)\n    *   [KataGo as a case study](#katago-as-case-study)\n    *   [Openness for interpretability research](#open-for-interp)\n    *   [Effectiveness of substitutes for open sourcing](#os-substitutes)\n    *   [Offense-defense balance, part 2](#offense-defense-2)\n    *   [Making open-sourcing safer?](#making-os-safer)\n*   [AI governance research](#ai-gov)\n    *   [The state of the field](#state-of-field)\n    *   [Open questions](#open-qs)\n    *   [Distinctive governance issues of x-risk](#xrisk-different)\n    *   [Technical research to help governance](#tech-for-gov)\n*   [Following Elizabeth’s research](#following-elizabeths-research)\n\n**Daniel Filan:** Hello, everybody. In this episode, I’ll be speaking with Elizabeth Seger. Elizabeth completed her PhD in philosophy of science at Cambridge in 2022, and is now a researcher at the [Centre for Governance of AI](https://www.governance.ai/) in Oxford, where she works on AI democratization and open-source AI regulation. She recently led the production of [a large report](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436) on the risks and benefits of model-sharing, which we will talk about in this episode. For links to what we’re discussing, you can check the description of the episode and you can read the transcript at axrp.net.\n\nWell, Elizabeth, welcome to the podcast.\n\n**Elizabeth Seger:** Awesome. Thanks for having me.\n\nWhat kinds of AI?\n-----------------\n\n**Daniel Filan:** Cool. We’re going to be talking about a couple of papers basically about democratizing and open-sourcing AI. When we’re talking about this, what sorts of AI should we be thinking about? What sorts of AI do you have primarily in mind?\n\n**Elizabeth Seger:** When talking about open-sourcing and model-sharing, I’ve been primarily interested in talking about frontier foundation models, so understanding frontier foundation models as those systems on the cutting edge of development right now. When talking more generally about AI democratization, I tend to think much more broadly. I think a lot of conversation around democratization right now is about frontier AI systems, but it’s important to recognize that AI is a very broad category. Well, so is democratization, which is I’m sure something we’ll talk about.\n\nDemocratizing AI\n----------------\n\n**Daniel Filan:** Okay, cool. In that case, let’s actually just dive into the democratization paper. This is [Democratizing AI: Multiple Meanings, Goals, and Methods](https://arxiv.org/abs/2303.12642) by yourself, Aviv Ovadya, Ben Garfinkel, Divya Siddarth, and Allan Dafoe. Before I start asking questions, can you give us just a rundown of basically what is this paper?\n\n**Elizabeth Seger:** Yeah, so this paper on AI democratization was born out of an observation around how different actors were using the term “AI democratization” or “democratizing AI”, whether we’re talking about labs developing AI systems, or policymakers, or even people in the AI governance space. There just seemed to be a lot of inconsistency around what this term AI democratization meant.\n\nThe paper really started out as just an effort to explain what does it actually mean when someone says “Democratizing AI.” I think there’s overlap here with the discussions and debates around open-sourcing. I think there was a lot of… at least for me personally, there was frustration when people would say, “Oh, well, we open-sourced the model and therefore, it’s democratized.” It’s like, “What does that even mean?”\n\nThe goal of this AI democratization paper was really just to outline the different meanings of AI democratization that were in use. Not necessarily saying how it should be used, but just how is the term being used. We broke it up into four categories. Democratization of AI use is just allowing many more people to interact with, and use, and benefit from the technology. Then there’s the democratization of development, which is allowing many more people to contribute to the development processes and help make the systems really cater to many diverse interests and needs.\n\nDemocratization of profits, which is about basically distributing the profits that might accrue to labs that are the primary developers and controllers of the technology, and those profits could be massive. Then the democratization of AI governance, which is just involving more people in decision-making processes about AI, about how it’s distributed, about how it’s developed, about how it’s regulated, and about who makes the decisions. These are all different ways that democratization has been discussed.\n\nThen in the paper, we go a step further and say, “Okay. Well, if these are the different kinds of democratization, what are the stated goals of those kinds of democratization, and then what activities help support those goals?” I think the main idea here was to show that, oftentimes, when people talk about AI democratization, they’re specifically talking about open-sourcing or model-sharing. I think one of the main points we try and make is to say something like, for example, open-sourcing AI systems is one form of model-sharing. Model-sharing is one aspect of democratizing the development of AI systems, and democratizing the development of AI systems is one form of AI democratization.\n\nIf you’re going to commit to an AI democratization effort or say that these are the goals you have in mind, there’s so much more involved in these processes. It’s not just about releasing a model, but many more proactive efforts that can go into distributing access to the models, ability to use the models, and to benefit from the models, whether that’s through use or through the profits that it produces.\n\n### How people talk about democratizing AI\n\n**Daniel Filan:** Gotcha. In the paper, you cite a few examples, basically, of people either talking about something akin to democratization or specifically using the term “democratizing AI”. In all the examples you use, it’s basically labs talking about it, and it’s mostly in a context of, “We want to democratize AI in the sense of everybody should use our product”, is how I read it. I’m wondering, do people other than labs talk about democratizing AI? Is this a thing broadly?\n\n**Elizabeth Seger:** Yeah, absolutely. In the paper, I think we focused a lot of the lab terminology because to be honest, it was written initially as sort of a pushback against the use of the term by labs to basically say, “Use our product,” and it’s like, “Okay. What does this actually mean? There’s more to it, guys.” But the term AI democratization or even just discussing AI democratization is much more widely engaged with.\n\nOne of the co-authors, [Divya Siddarth](https://divyasiddarth.com/), for example, she and [Saffron Huang](https://saffronhuang.com/) run the [Collective Intelligence Project](https://cip.org/), and this is a group that focuses largely on democratizing AI governance processes. So getting lots of people involved in decision-making about, for example, AI alignment or just about different kinds of governance decisions, and how that can align with the needs and interests of really diverse populations.\n\nI think there are definitely groups that are popping up and getting really involved in AI democratization from the democratizing governance standpoint. It is a term that’s used by labs, and not necessarily consistently to just mean, “Use our products.” Some of them mean … when Stability AI discussed AI democratization and the whole, “AI for the people by the people” mantra, that was very much about model-sharing and making models accessible to lots of people to use as they saw fit. So yeah, you see this in labs.\n\nYou see this with a lot of groups that are popping up to help democratize governance processes. \\[You\\] probably hear about it less in policy and governance circles, though sometimes, I’ve talked to union leaders and when they think about AI democratization, they’ve spoken about it like, “How are these technologies going to help our workforce?”, both in the sense of making sure that it helps the workforce with their needs and helps make their jobs easier, but not necessarily in a way that would threaten jobs. And how do we make the integration of the systems into new environments and settings \\[that\\] actually work for the people that are using them and integrating different perspectives in that way?\n\nIt’s definitely something that I think is … It’s a terminology that’s been spreading and part of why we wrote this paper is trying to understand all the different ways the terminology’s being used, though it was largely written in response to the way it was being thrown around by labs and being seen in the media.\n\n**Daniel Filan:** Right. That actually gets to something I was thinking. In this paper, you list these four definitions. That inspired me to think like, “Okay. Can I think of any other things that I might mean by ‘democratizing AI’?” One thing that it sounded like these union leaders were talking about is something like how customizable AI is, or to what extent can any person wield it specifically to do their own thing rather than just getting a one-size-fits-all deal? I’m wondering if you have any thoughts about that at least potential use of the term “democratization”.\n\n**Elizabeth Seger:** Yeah, so I think that this is something that we’ve sort of filed under the “democratizing use” category. One of the things that we tried to do under each of these categories is to show how there wasn’t necessarily just one way to democratize use or one way to democratize development. When we talk about democratizing use, one way to do that is just to make a product available. Like say, “Here, use GPT-4. There you go.” It’s available, easy access.\n\nBut other ways to democratize use are, for example, to make the systems more easily accessible by maybe having an interface be more intuitive. Like the models, the API interface, have that be more intuitive. Or to provide services to help it be more easily customizable, as you mentioned, to allow people to… whether they’re fine-tuning it to focus on a particular need or maybe there are ways through plug-ins, for example, to integrate it with different services, into different applications. Or then even to provide support services to help people integrate your products into downstream applications or into different workstreams. These are all different things that can contribute towards democratizing the use of a system, and I think customization’s definitely one of those.\n\n### Is democratizing AI important?\n\n**Daniel Filan:** Gotcha. Now that we have a better sense of just what we’re talking about in terms of democratization, a question that you address a little bit in the paper, but isn’t precisely at the forefront, is just: how important is democratization in this context? Because for most technologies, I don’t think a ton about their democratization - air dehumidifiers, or water bottles, or something. Maybe I’m heartless, but for me, democratization is just not near the top of the normative priorities for these things. Should we even care that much about democratizing AI?\n\n**Elizabeth Seger:** The short answer is yes. I think we should care about democratizing AI. You make a good point. We don’t talk about democratizing air humidifiers or water bottles, but I think - this is a question that comes up when talking about AI a lot, is just, how is it different from other technologies? This is a question that’s been asked, whether you’re talking about how it’s shared or in this context, how it’s democratized.\n\nSpecifically, with regard to the discussion on democratization, making something available for more people to use, available for more people to benefit from or contribute to the design processes, I think this is particularly important in the case of AI. (A), because if AI promises to be as transformative a technology as we’re hoping it to be, this will massively impact lives across the globe from different groups, different nationalities, different geographic settings. Making sure that we can have input from different groups into design processes and to understanding what needs are best served, that’s going to be important to make sure that this technology doesn’t just serve a few very well, but serves many people and benefits the world as a whole.\n\nI think this is also particularly important to note with regard to the democratization of profits. AI is already, but may continue to be just an insanely financially lucrative industry. We could begin to see the accrual of profits to leading labs on huge scales where we might see a particular company, for example, be able to measure its gross income in terms of total percentages of total world output or something. That is huge economic profit, and we want a way to make sure that, ideally, everyone benefits from this, it doesn’t just pile up in a few companies in a few geographic locations like Silicon Valley.\n\nHow do we make sure that this is well-distributed? There are some direct methods that could be employed. In the section of the paper where we talk about democratizing profits, some of the things that we discuss are you’d have adherence to [windfall clauses](https://www.fhi.ox.ac.uk/windfallclause/). This would be something where, let’s say a company has some kind of windfall profit measured in terms of percentage of gross world output. In that case, there might be a commitment to redistribute some of those funds, or we might see taxation and redistribution schemes.\n\nThere are also more indirect ways as well that you can think about distributing profits. This would be, for example, are there ways that we can get more people involved in democratization of development? More people involved in developing products, so there’s overlap between democratizing profits and democratizing development. More people can contribute to development of products, then this might challenge a natural monopoly that might otherwise form around large labs. More competition, more distribution of profits, more people in on the game.\n\nSo yeah, I think what it comes down to is if this technology can be as big and as transformative as it promises to be, having more people involved in the development process and being able to benefit from the value that’s produced by these systems is really important.\n\n### Links between types of democratization\n\n**Daniel Filan:** Yeah, I think that makes sense. Actually, that reminds me of … As you mentioned, there’s some link between democratizing, I guess, the development of AI and the profits, in the sense that if more people make AI products, that means more people can profit from them, presumably. There’s also, to my mind, there’s a continuum between the use of AI and the development of AI, right? In some sense, when you use a thing for a task, you’re developing a way of using it for that task, right? There’s sort of a fuzzy line there. There’s also some sort of link between developing AI and governing AI, just in the sense that if I develop some AI product, I am now, de facto, the main person governing how it is made and how it is-\n\n**Elizabeth Seger:** Yeah, which is sort of what we’re seeing right now. A lot of the AI governance discussion is around the fact that major AI labs are making huge, impactful decisions about the future of AI. They’re making AI governance decisions, and so there’s a big open question of who should actually be making these \\[decisions\\]. Should it just be a few unelected tech leaders of major companies, or should this be distributed more? So yeah, there’s definitely overlap between categories.\n\n**Daniel Filan:** Yeah. I take the point of your paper to be saying like, “Oh, these are very distinct. These are conceptually pretty distinct and you can promote one sort of democratization without promoting other sorts of democratization.” But to the degree that they do seem to have these links and overlaps, is it maybe a case of just a rising tide lifts all boats and buy one kind of democratization, get the others free?\n\n**Elizabeth Seger:** I think kind of, maybe, sort of, not really. All of the above. No, so there definitely is overlap between some of the categories. I think it’s either in this paper or maybe in the blog post version, I think we say like, “These are not perfectly distinct categories. There’s going to be overlap between them.” I think part of the utility of breaking it up into the categories is to point out that there are lots of different meanings. There are lots of different ways to achieve these different meanings and goals.\n\nIt’s not just about just open-sourcing a model or just making it available for people to use. There’s a lot more to it. I think, in some respects, there is sort of a “rising tide lifts all boats” kind of thing. Like you said, you might get more people involved in development processes. If more people are developing AI systems, more people profit from it, and that can help distribute profits.\n\nOn the other hand, you can also see direct conflicts between categories sometimes. For example, you might… I think this is something we write about in the paper: talking about democratizing governance, so decisions about AI, whether that’s about how AI is developed or even how decisions are made about how models are shared.\n\nThese days, there’s a really heated debate around open-sourcing frontier AI models. Let’s say you democratize this decision-making process around whether or not certain models should be released open-source. Let’s say the outcome of this democratized decision-making process is to say, “Okay. Maybe some models that pose substantially high risks should not be released open-source.” This is a hypothetical, but let’s say that’s the outcome of a deliberative, democratic process and decision-making. That could be in direct conflict with the interests of democratizing development, where democratizing development… that is always furthered by providing better access to models.\n\nYou could have a case in which you’ve democratized a governance decision, but the outcome of that governance decision is to impede some other form of democratization. I mean, not just with development. I guess, you could have a democratic process that says, I don’t know, like, “Large companies should not be taxed and have profits redistributed.” That would be in direct conflict with an effort to democratize profits, so there could be conflicts. I think it’s usually between the governance category and the other categories.\n\nI guess, governance often will conflict with specific goals. That’s kind of the purpose, to temper decision-making and get more people involved in the decision-making processes, and make sure that those decisions are democratically legitimate, and reflect the needs and interests of a wider population of impacted people and stakeholders.\n\n**Daniel Filan:** Okay, yeah. This actually brings up a question I had about the interaction between democratizing use/development and democratizing governance. In the paper, as you mention… I guess the main interaction you draw out is this tension between democratizing use… It’s potentially not necessarily what a democratic process would produce on the governance side.\n\nWhen I think about… since, I don’t know, about a year ago, we’ve had [ChatGPT](https://chat.openai.com/auth/login), which in some sense, really democratized, I guess, the use of large language models in the sense that it’s relatively cheap, maybe free to use these things. One effect is that now it is impossible to at least have banned ChatGPT a year ago or something. But to my mind, a big effect of this is something like, increasing people’s knowledge of where AI is at, what kinds of things the language models can do.\n\nAnd essentially, I don’t know if this is a term, but you can think of people as having governance demands, right? If I know more about a technology, I might say, “Oh, this is totally fine.” Or, “Oh, we need to do this thing or that thing about it.” If people know more about a technology, they can basically have a better sense of what they want their governance demands to be. In that sense, it seems like at least to some degree, there can be a positive interaction between democratizing use and democratizing governance.\n\n**Elizabeth Seger:** Yeah, I think absolutely. This is sort of a classic pillar of democracy, is having the informed public. You need to have the information to be able to do the good decision-making about the object of your decision-making. Yeah, so I think the release of ChatGPT, for example, really put AI on the public stage. Suddenly, I have my mom and dad texting me, asking me questions about AI, and that never would have happened a year ago. Yeah, so I think it really put the technology on the stage. It’s got more people involved in it. I think more people have a general sense of at least what current capabilities are able to do and of the limitations. That’s all been very valuable for enabling more democratic decision-making processes about AI.\n\nThere’s definitely that link between you democratize use, you democratize development, people understand the technology better. They’ll be able to make more well-informed decisions or form more well-informed opinions about how the technology should be regulated. So I think that that’s not necessarily in tension. I guess, the tension tends to go the other direction. It’s like if a governance decision perhaps would say, “Oh, this particular technology is too dangerous.” Or, “Releasing this particular model surpasses an acceptable risk threshold.”\n\nI guess, if you took a strict definition of democratizing development as meaning always spreading the technology further, always getting more people involved in a development process, always making a system accessible, then yeah, a decision to restrict access would be in direct conflict with democratizing development.\n\nI think this is another thing we try and say in the paper: trying to get away from this idea that AI democratization is inherently good. I think this is a problem with the term democratization, especially in Western, democratic societies: oftentimes, democracy is used a a stand-in for all things good, so you get companies saying, “Oh, we’re democratizing AI”. It almost doesn’t even matter what that means. They’ve democracy-washed their products. Now it looks like a good thing.\n\nI think that was one thing we were trying to get across in the paper is, yeah, AI democratization is not necessarily a good thing. Spreading a technology for more people to contribute to a development process or for more people to use is only good if that’s actually going to benefit people. But if it poses significant risk or it was going to put a lot of people in harm’s way, then that’s not inherently positive.\n\nI guess, one thing to say is just if all that’s meant when people say, “AI democratization,” or, “democratize AI” is just make the system more accessible, then just say, “Make the system more accessible”, because I think that’s very clear what it means and it’s not inherently good or bad. I don’t know if we’re going to successfully change everyone’s terminology in the course of one podcast, but-\n\n**Daniel Filan:** Well, we can try.\n\n**Elizabeth Seger:** We can try. I think that’s maybe one key takeaway from the paper as well. If people say they’re engaging in an effort to democratize AI, that does not necessarily mean it’s going to be a net beneficial effort.\n\n### Democratizing profits from AI\n\n**Daniel Filan:** Gotcha. I guess, I’d like to talk a bit about the individual senses. In particular, I want to start off with when you talk about democratizing profits. One thing I noticed is, I think there’s one or two quotes you use in this section. I think the quotes actually say something like, “We want to make sure that not all the value created by AI stays in one place or that the benefits of AI don’t just accrue to a small group of people.”\n\nIn those quotes, they don’t actually use the term “profit”. In some ways, you can imagine that these could go different ways. For instance, take [GitHub Copilot](https://github.com/features/copilot). It costs, I guess, $100 a year. Suppose that everyone in the world just pays $100 a year and it all goes to GitHub. Tons of people use GitHub Copilot to do incredibly useful things, and they get a bunch of value and benefits out of it. Probably, most of the benefits will be financial, but not all of them will be. That seems to me like it would be a case of spreading the value or benefits of AI, but it’s not necessarily a case of spreading the profit from AI.\n\n**Elizabeth Seger:** Yeah, the profits category is kind of a… It was an odd category. Actually, in the process of writing this paper… Unlike most papers, we actually wrote [the blog post](https://www.governance.ai/post/what-do-we-mean-when-we-talk-about-ai-democratisation) first and then wrote the paper. It’s supposed to go in the other direction. In the blog post, which is on GovAI’s website, it actually breaks into four categories: Democratization of use, democratization of development, democratization of benefits, and democratization of governance.\n\nFor the version in the paper, we chose the profits terminology instead of the benefits terminology because benefits, in a way, it was too broad because… I mean, you’ve already pointed this out. There’s a lot of overlap between these categories, and one reason to democratize development, for example, is to make sure that more people can benefit from the technology, that it serves their need.\n\nA reason to democratize use is to ensure that more people can access and use, and even use the systems to produce value and profit. If you could integrate it with your own businesses and with your own applications, you can develop value on it from your side. I think we chose this democratization of profits terminology just to point out that there is a discussion around just the massive accrual of profits to large tech companies.\n\nI think, yeah, you do point out some of the quotes that we use talk a little bit more broadly about accrual of value. I think, sometimes it’s hard to measure profits just in terms of the literal money that drops into someone’s bank account.\n\n**Daniel Filan:** Okay.\n\n**Elizabeth Seger:** So I think… it’s hard to find exact quotes from companies. I know that there’s been a lot of discussion not by companies, but actually, this is one thing that you will hear policymakers talk about more. This was part of when universal basic income was something that was being discussed a lot. It had its moment when people were talking about it a lot more. A lot of the discussion around universal basic income was around… Well, it was around job loss caused by automation, but also around the accrual of profits to tech companies and to developers of these new technologies. Okay, if all the profits are going to shift from the workforce to the developers of these technologies, how are we going to redistribute it out? That’s actually one context in which democratization of profits was directly linked to the discussion around AI development.\n\nYeah, I think maybe some of the quotes we used in the paper do more vaguely gesture to just the value. But we separated profits out just because the way we had it in the blog post, it was a bit too general. There was too much overlap. We were kind of double-dipping, you know? I think it was just to illustrate that there is this discussion specifically about profits, but I don’t think… I mean, it’s definitely not the most commonly used. If you heard someone talking about AI democratization just walking down the street, you wouldn’t immediately think, “Oh, they must be talking about profit redistribution.” That’s not where your brain goes. But it is one aspect worth noting, I think, is the main takeaway.\n\n### Democratizing AI governance\n\n**Daniel Filan:** Fair enough. Yeah, the next one I want to talk a bit more about is democratizing AI governance. Related to the thing you just said, it seems to me that if somebody talks about democratizing AI, I think it’s pretty unlikely that if they just say, “democratizing AI”, they mean democratizing governance. I’m wondering if that matches with your experience.\n\n**Elizabeth Seger:** Yes and no. I think, oftentimes, when you see in media or you’re listening to tech companies and the dialog that they’re having… yeah, very rarely will it talk about democratizing governance. I think we are starting to see a shift though. For example, OpenAI had this [AI democratization grant program](https://openai.com/blog/democratic-inputs-to-ai) \\- I can’t remember exactly what it’s called right now. Basically, they’re putting out grants to research groups to study methods for basically trying to bring in input from more diverse communities to try and better understand what values systems should align to. In that sense, you’re talking about AI democratization in this more participatory, deliberative process of bringing people into decision-making processes to define principles or to define values for AI alignment. OpenAI had that grant program.\n\nI think Anthropic just put out [a blog post](https://www.anthropic.com/index/collective-constitutional-ai-aligning-a-language-model-with-public-input). They’d worked closely with [the Collective Intelligence Project](https://cip.org/) that I mentioned earlier doing a similar thing for developing their [constitutional AI](https://arxiv.org/abs/2212.08073), defining what the constitutional principles are that AI systems should be aligned to, and that was more of a democratic governance process. So actually, I think we’re starting to see this terminology of “AI democratization”, “democratizing AI” seep into the technical landscape a bit more. I think that’s a very positive thing. I think it’s showing that they’re starting to tap into and really embody the democratic principles of democratizing AI. As opposed to just meaning distribution and access, it actually means reflecting and representing the interests and values of stakeholders and impacted populations.\n\nI think we are starting to see that shift, but I think you’re probably right in that mostly… again, \\[if when\\] walking down the street, you hear someone talking about AI democratization, \\[they’re\\] probably just talking about distribution of the technology, making it more accessible. That’s the classic terminology, but I do think that in the AI governance space and even in the terminology being used by labs, we’re starting to see the governance meaning seep in, and that’s exciting.\n\n### Normative underpinnings of democratization\n\n**Daniel Filan:** Gotcha. Getting to the question about whether this is good, I think in the paper, you say something like: the notion of democratization of use, development, and profit… I think you say something like those ones aren’t inherently good and they get their normative force from democratization basically in terms of the governance method. So I guess first of all, that wasn’t actually so obvious to me. So I think sometimes democratization, I think it often carries this implicit sense of being more related to egalitarianism than democracy as a political decision-making method. I think you can have egalitarianism without democratic decision-making and you can have democratic decision-making without egalitarianism, right? People can vote away minorities’ rights or whatever. So yeah, I was wondering why do you say… I guess what I actually mean is please defend your claim that that’s the source of the goodness of democracy, or of democratization, is the word.\n\n**Elizabeth Seger:** Yeah. Okay. So a couple of things, three things, maybe two things. We’ll see how it comes out. So to start out with, I do stand by the idea that the first three forms of democratization, so development, use, profits, \\[are\\] not necessarily inherently good; not inherently bad, just these are things that happen, and if we stick by their definitions it just means making something more accessible, whether that’s access for use or development or making the profits more accessible. Access is not inherently good or bad. Again, if we use the “access for development” example, if you have a technology that’s going to be potentially really dangerous, you don’t necessarily want everyone to have access to that. And a democratic decision-making process might result in the conclusion that indeed not everyone should have access to it.\n\nSo that’s the first part: standing by that first half of the claim. The second part, around the thing that gives it the moral force or value is the democratic decision-making process that was involved… I don’t know if that’s exactly what we’re going for in the paper. I think there are lots of different methods that you can use to help reflect and serve the interests of wider populations. I think for some technologies… let’s go back to your water bottle example. We don’t need a panel of people from around the globe to tell us how to distribute water bottles. This is a decision that probably water bottle distributors can be like, “let’s just sell our water bottles to as many people as possible” and \\[it’s\\] probably fine.\n\nI think it comes to where the impact’s going to be, how big the impact’s going to be, are there areas where the negative impacts of a technology might be more disproportionately felt? In those cases, being able to bring those voices in to inform decision-making processes that might affect them disproportionately is really important. I know we give examples in the paper of different kinds of democratic processes that can be used to inform decision-making, whether these are participatory processes or more deliberative democratic processes. So in [the open source paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436) we just released, we talk about some of these processes as ways of making more democratic decisions around AI, but then also point to: one way to democratize governance decisions is to support regulation that is put forth by democratic governance and that those governments hopefully if structured well, reflect and serve the interests of the constituents.\n\nAnd hopefully some of those governance processes are actually formed by having maybe some sort of deliberative process that takes into account stakeholder interests and the interests of impacted populations. But it’s not like the only way to govern AI is to say, “okay, let’s have a participatory panel where we take in all this insight and then use it to inform every decision”. I think you’re right, that would just be completely impractical to have every decision that a lab makes be informed by a participatory panel or some sort of deliberative democratic process. So I think you kind of have to range it depending on the potential impact of the question. So I don’t know if that answers your question completely, but I guess I stand by the first point, that democratization is not inherently good. The extent of the goodness I guess reflects how well the decisions reflect and serve the interests of the people that are going to be impacted. And then there are lots of different ways of figuring out how to do that.\n\n**Daniel Filan:** Okay, that makes sense. So I guess the second thing I want to ask about that sentence is: when you say that the democratization of use, development and benefits is not inherently good, it kind of makes it sound like you think that democratization of governance is inherently good. So I found this paper that you actually cite, it’s called [Against “Democratizing AI”](https://johanneshimmelreich.net/papers/against-democratizing-AI.pdf) by Himmelreich, published in 2023. So he basically makes a few critiques. Firstly, he just says that AI isn’t the sort of thing that needs to be democratic. AI in itself doesn’t affect a bunch of people. There are just organizations that use AI and they affect a bunch of people and maybe they should be democratized, but AI itself doesn’t have to be.\n\nHe says that the relevant things are already democratic. Democracy is pretty resource-intensive. People have to know a bunch of stuff, maybe you have to take a bunch of votes and things. He also has complaints that democracy is imperfect and doesn’t necessarily fix oppression. So there are a few complaints about democratizing AI floating around. I guess in this case he’s talking about democratizing AI governance. Is it the case that democratizing AI governance is inherently good?\n\n**Elizabeth Seger:** I don’t think so. Again, I think it comes back to the “ranging it” question. First of all, the resource-intensive point: absolutely. It can be incredibly resource-intensive to do an in-depth participatory governance process. This is why you don’t do it for every single question that’s out there. Yeah, so that’s really good point. There’s also the point of you need to be well-informed to make good decisions. So maybe we don’t want the general public making decisions about all questions around AI. Experts are having a hard enough time for themselves trying to decide what constitutes a high risk system. We’re not even good at benchmarking this among the community of AI experts. How are you going to tap into more general public notions of what a high risk system is or what proper benchmarking should look like?\n\nSo there’s some questions where it just doesn’t make sense to democratize it. So I think you have to think about: what questions is it realistic to have more democratized discussions around? So maybe something like thinking about what acceptable risk thresholds are, or what kind of values to align to. I think it’s also important to remember there are lots of different kinds of democratic processes. When we talk about democratic processes, it’s not necessarily what you might think of: we all go to the polls and cast our ballot on “do we want A or do we want B?” These could be more deliberative processes where you just get some stakeholders in a room, they have a discussion, they try and figure out what are the main issues \\[in\\] these discussions. So there are processes in place to hold deliberative democratic processes that are informed, you have experts come in and try and inform the discussion. So this would be a more informed deliberative process.\n\nOne way is: sometimes when it comes to… with image generation systems, there was a whole bunch of discussion around bias and images. Again, we see overlap between governance and other kinds of categories. If you get more people involved using the systems and they start understanding where the biases are, that is a form of education - having more familiarity. And then you can raise these complaints, whether that’s directly to the companies or maybe through some sort of political intermediary. I remember there being, what was it? A [statement](https://eshoo.house.gov/media/press-releases/eshoo-urges-nsa-ostp-address-unsafe-ai-practices) released by [Anna Eshoo](https://en.wikipedia.org/wiki/Anna_Eshoo), who I think was the congressional representative in, I want to say South San Jose, that talked in quite a bit of depth about … it was right after the release of [Stable Diffusion](https://stablediffusionweb.com/) and talking about how there was a lot of racial bias in some of the images that were being produced, specifically against Asian women.\n\nSo this is something that was raised through a democratic process, was brought to the attention of the congressional office, and then the statement was released, and then that had knock-on effects on future governance decisions and decision-making by labs to do things like put safety filters. So I think that’s one thing to keep in mind: there’s a wide range of things that you can think about in terms of what it means to democratize a governance process. Governance is just decision-making. Then how do you make sure those decision-making processes reflect the interests of the people who are going to be impacted. It’s not just about direct participation, although there is a lot of work being done to bring more direct participatory methods into even the more small scale decision-making, to make it more realistic.\n\nLike this complaint about it being so resource intensive: that’s true. We also have technologies available to help make it less resource intensive and how can we tap into those to actually help a public input to these decisions in a well-informed way? So there’s lots of interesting work going on. Democratic governance also isn’t inherently good when it comes to AI governance. I think again, it depends on the question. If the cost of doing a full-blown democratic process is going to far outstrip the benefit of doing a full-blown democratic process, then there’d be a situation in which is probably not a net benefit to do a democratic decision-making process.\n\n**Daniel Filan:** So that gets to Himmelreich’s resource-intensive complaint. He also has these complaints that basically AI itself is not the kind of thing that needs to be democratically governed. So he says something like, I’ll see if I can remember, but the kind of thing that needs to be democratically governed is maybe something that just impinges on people’s lives, whether they like or not, or something that’s just inherent to large scale cooperation or something like that. He basically makes this claim that, look, AI doesn’t do those things. AI is housed in institutions that do those things. So if you’re worried about, maybe the police using facial recognition or something, you should worry about democratic accountability of police, not facial recognition, is the point I take him to be making.\n\nSo firstly, he says, look, we don’t need to democratize AI. We need to democratize things that potentially use AI. And secondly, it seems like he’s saying the relevant things that would be using AI, they’re already democratic, so we shouldn’t have conflicting … I think he talks about democratic overlap or something, where I think he really doesn’t like the idea of these different democratic institutions trying to affect the same decision and maybe they conflict or something.\n\n**Elizabeth Seger:** So I think I don’t disagree. I think, if I understood correctly, this idea of “you don’t need to democratize the AI, you need to democratize the institution that’s using it”, I think that’s a completely valid point. AI itself, again, is not inherently bad or good. It’s a dual use technology. It depends what you’re going to do with the thing. But I would say that trying to decide, for example, how law enforcement should use AI systems, that is an AI governance question. This is a question about how AI is being used, about how AI is being regulated in this context, not in terms of how it’s being developed, but in context of how it’s being used.\n\nWhat are the appropriate use cases of AI? This is an AI governance question, and it’s not necessarily that these are regulations that are placed on AI specifically. A lot of AI regulation just already exists within different institutions. It’s just figuring out how to have that regulation apply to AI systems. This might be cases like: if you have requirements for certain health and safety standards that have to be met by a certain technology in a work setting, AI shouldn’t be held to different standards. It’s just a matter of figuring out how to measure and make sure that those standards are met by the AI system. So I guess based on what you said, I want to say that I completely agree.\n\nBut I would say that it still falls under the umbrella of democratizing AI governance. I think that what might be happening here is just another conflict over what does the terminology mean, where it’s like “we don’t need to democratize AI, as in we don’t need to get more people directly involved in the development and decision-making about individual decisions around AI development”. In which case, yes, I agree! But we might be talking about democratizing AI governance in different… This is why these discussions get really complicated because we just end up talking past each other because we use the same word to mean five different things.\n\n**Daniel Filan:** Yeah, it can be rough. So before we close up our discussion on democratizing AI, I’m wondering… I guess you’ve thought about democratizing AI for a while. Do you have any favorite interventions to make AI more democratic or to make AI less democratic in any of the relevant senses?\n\n**Elizabeth Seger:** I’ll be completely honest, this is sort of where my expertise drops off in terms of the more nitty-gritty of what processes are available and exactly when are certain processes the best ones to be applied. For this, I would really look more closely at the work that [the Collective Intelligence Project](https://cip.org/)’s doing. Or my co-author [Aviv Ovadya](https://aviv.me/), who’s on the paper, he’s really involved with these discussions and works with various labs to help implement different democratic processes. Yeah, I was just going to say, this is where my expertise drops off and I would point towards my colleagues for more in-depth discussion on that work.\n\nOpen-sourcing AI\n----------------\n\n**Daniel Filan:** All right, the next thing I want to talk about is basically open-sourcing AI. So there’s this report, [Open-sourcing highly capable foundation models: an evaluation of risks, benefits, and alternative methods for pursuing open source objectives](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4596436). It’s by yourself and a bunch of other co-authors, but mostly as far as I could see at [the Center for the Governance of AI](https://www.governance.ai/). Again, could you perhaps give an overview of what’s going on in this report?\n\n**Elizabeth Seger:** Okay, yeah. So this report’s a little harder to give an overview on because unlike the democratization paper, which is eight pages, this one’s about 60 pages.\n\nSo quick overview. First thing is that we wanted to point out that there’s debate right now around whether or not large foundation models, especially frontier foundation models, should be open source. This is a debate that has kicked off starting with [Stability AI](https://stability.ai/)’s release of [Stable Diffusion](https://stablediffusionweb.com/), and then there was the [Llama 2](https://ai.meta.com/llama/) release and then the weights were leaked, and now Meta’s getting behind this open source thing; there were the [protests outside of Meta](https://spectrum.ieee.org/meta-ai) about whether or not systems should be open source. And then we’re also seeing a lot of activity with the development of the [EU AI Act](https://artificialintelligenceact.eu/). As part of the EU AI Act, some parties are pushing for an exemption of regulation for groups that develop open source models. So a lot of discussion around open source. I think the goal of this paper was to cut through what is becoming a quickly polarized debate.\n\nWe’re finding that you have people that see the benefits of open-sourcing and are just very, very hardcore pro open source and then won’t really hear any of the arguments around the potential dangers. And then you have people who are very anti open source, only want to talk about the dangers and sort of refuse to see the benefits. It just makes it really hard to have any kind of coherent dialogue around this question. Yet at the same time, country governments are trying to make very real policy around model sharing. So how can we cut through this polarized debate to just try and say, here’s the lay of the land. So what we’re trying to do in this paper is really provide a well-balanced, well-researched overview of both the risks and benefits of open-sourcing highly capable models.\n\nBy ‘highly capable models’, basically what we’re talking about is frontier AI systems. We just don’t use the term ‘frontier’ because the frontier moves, but we want to be clear that there will be some systems that might be highly capable enough and pose risks that are high enough that, even if they aren’t on the frontier, we should still be careful about open-sourcing them. So basically we’re talking about frontier models, but frontier keeps going. So we can get into the terminology debate a little bit more later. But basically we wanted to outline what are the risks and benefits of open-sourcing these increasingly highly capable models, but then not just to have yet another piece that says, well, here are the risks and here are the benefits. But then kind of cut through by saying ‘maybe there are also alternative ways that we can try to achieve some of the same benefits of open-sourcing, but at less risk.’\n\nSo the idea here is actually quite similar to the democratization paper: it’s to not just say ‘what is open-sourcing?’ and ‘is it good or bad?’ But to say ‘What are the specific goals of open-sourcing? Why do people say that open-sourcing is good, that it’s something we should be doing that should be defended, that should be preserved? So why is it we want to open source AI models? Then if we can be specific about why we want to open source, are there other methods that we can employ to try to achieve some of these same goals at less risk?’ These might be other model sharing methods like releasing a model behind API or doing a staged release, or it might be other more proactive measures like: let’s say one benefit of open-sourcing is that it can help accelerate research progress, both in developing more greater AI capabilities, but also promoting AI safety research.\n\nAnother way you can promote AI safety research is to dedicate a certain percentage of profits towards AI safety research or to have research collaborations. You can do these things without necessarily having to provide access for anybody to have access to the model. So this is what we were trying to do with this paper: really just say, okay, we’re going to give, first of all, just a very well-researched overview of both the risks and benefits. In fact, majority of the paper actually focuses on what are the benefits and trying to break down the benefits of open-sourcing and then really doing a deep dive into alternative methods or other things that can be done to help pursue these benefits where the risks might just be too high.\n\nI think that the main claim that we do make in the paper with regard to ‘is open-sourcing good or bad?’ First of all, it’s a false dichotomy, but I think our main statement is open-sourcing is overwhelmingly good. Open source development of software, open source software underpins all of the technology we’re using today. It’s hugely important. It’s been great for technological development, for the development of safe technology that reflects lots of different user needs and interests. Great stuff. The issue is that there might be some cutting edge, highly capable AI systems that pose risks of malicious use or the proliferation of dangerous capabilities or even proliferation of harms and vulnerabilities to downstream applications.\n\nWhere these risks are extreme enough, we need to take a step back and say: maybe we should have some processes in place to decide whether or not open-sourcing these systems is okay. So I think the main thrust of the report is \\[that\\] open-sourcing is overarching good, there just may be cases in which it’s not the best decision and we need to be careful in those cases. So yeah, I think that’s the main overview of the paper.\n\n**Daniel Filan:** Gotcha. Yeah, in case listeners were put off by the 60 something pages, I want listeners to know there is an executive summary. It’s pretty readable.\n\n**Elizabeth Seger:** It’s a two-page executive summary. Don’t worry.\n\n**Daniel Filan:** Yeah. I read the paper and I vouch for the executive summary being a fair summary of it. So don’t be intimidated, listeners.\n\n**Elizabeth Seger:** I’ve also been putting off writing the blog post, so there will be a blog post version at some point, I promise.\n\n**Daniel Filan:** Yeah. So it’s possible that might be done by the time we’ve released this, in which case we’ll link the blog post.\n\n**Elizabeth Seger:** Maybe by Christmas.\n\n### Risks from open-sourcing\n\n**Daniel Filan:** Yeah. Hopefully you’ll have a Christmas gift, listeners. So when we’re talking about these highly capable foundation models and basically about the risks of open-sourcing them… as I guess we’ve already gotten into, what is a highly capable foundation model is a little bit unclear.\n\n**Elizabeth Seger:** Yeah. It’s already difficult.\n\n**Daniel Filan:** Maybe it’ll be easier to say, what kinds of risks are we talking about? Because I think once we know the kinds of risks, (a) we know what sorts of things we should be willing to do to avert those risks or just what we should be thinking of, and (b) we can just say, okay, we’re just worried about AIs that could potentially cause those risks. So what are the risks?\n\n**Elizabeth Seger:** In fact, I think framing it in terms of risks is really helpful. So generally here we’re thinking about risks of significant harm, significant societal or even physical harm or even economic harm. And of course now you say, “oh, define ‘significant’”. But basically just more catastrophic, extreme, significant societal risks and harms. So we use some examples in the paper looking at potential for malicious use.\n\nThere are some more diffuse harms if you’re thinking about political influence operations. So this kind of falls into the misinformation/disinformation discussion. How might AI systems be used to basically influence political campaigns? So disinformation undermines trust and political leaders. And then in turn, if you can disrupt information ecosystems and disintegrate the processes by which we exchange information or even just disintegrate trust in key information sources enough, that can also impact our ability as a society to respond to things like crises.\n\nSo the pandemic is a good example of, if it’s really hard to get people accurate information about, for example, whether or not mask-wearing is effective, you’re not going to have really good coordinated decision-making around mask-wearing. So I think this is one where it’s a little bit more diffuse. It’s harder to measure. So I think this is one point where it’s quite difficult to identify when the harm is happening because it’s so diffused. But I think this is one potential significant harm. I’d say maybe thinking about disrupting major political elections or something like that.\n\nThere’s some options for malicious use that are talked about a little more frequently, I think because they’re more well-defined and easier to wrap your head around. These are things like using generative AI to produce biological weapons or toxins, even production of malware to mount cyber attacks against key critical infrastructure. Imagine taking down an electrical grid, or imagine on election day, taking down an electoral system. These could have significant societal impacts in terms of harm to society or physical harm.\n\nI think one key thing to point out here is that we aren’t necessarily seeing these capabilities already. Some people may disagree, but I think my opinion is that technologies with the potential for this kind of harm do not currently exist. I think it is wise to assume that they will come into existence in the not too distant future, largely because we’re seeing indications of these kinds of capabilities developing. We’ve [already seen how even narrow AI systems that are used in drug discovery, if you flip that parameter that’s supposed to optimize for non-toxicity to optimize for toxicity, now you have a toxin generator](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9544280/). So it doesn’t take a huge stretch of the imagination to see how more capable systems could be used to cause quite significant societal harm. So no, I don’t think there’s currently systems that do this. I think we need to be prepared for a future in which these systems do exist. So it’s worth thinking about the wisdom of releasing these systems now even if we might not be present with the technology at this moment.\n\n**Daniel Filan:** So as a follow-up question: you’re currently on the AI X-risk Research Podcast. I’m wondering… so it seems like you’re mostly thinking of things less severe than all humans dying or just permanently stunting the human trajectory.\n\n**Elizabeth Seger:** I wouldn’t say necessarily. I think this is definitely a possibility. I think part of what’s happening is you kind of have to range how you talk about these things. I am concerned about x-risk from AI, and I think we could have systems where whether it’s the cyber capability or the biological terror capability or even the taking down political systems capability and bolstering authoritarian governments - these are things that could cause existential risk and I am personally worried about this. But \\[we’re\\] trying to write papers that are digestible to a much wider population who might not be totally bought into the x-risk arguments. I think, x-risk aside, there are still very genuine arguments for not necessarily releasing a model because of the harm that it can cause even if you don’t think that those are existential risks. Even if it’s just catastrophic harm, probably still not a great idea.\n\n**Daniel Filan:** So at the very least, there’s a range going on there.\n\n**Elizabeth Seger:** Yeah.\n\n### Should we make AI too dangerous to open source?\n\n**Daniel Filan:** So one question I had in my mind, especially when thinking about AI that can cause these kinds of risks and potentially open-sourcing it… Part of me is thinking, it seems like a lot of the risks of open-sourcing seem to be, well, there’s now more people who can use this AI system to do a bunch of harm. The harm is just so great that that’s really scary and dangerous. So one thing I was wondering is: if it’s so dangerous for many people to have this AI technology, is it not also dangerous for anybody at all to have this AI technology? How big is this zone where it makes sense to make the AI but not to open source it? Or does this zone even exist?\n\n**Elizabeth Seger:** Yeah, you make a very genuine point, and there are those that would argue that we should just hit the big old red stop button right now. I think that is an argument some people make. I guess where we’re coming from with respect to this paper is trying to be realistic about how AI development’s probably going to happen and try to inform governance decisions around what we should do as AI development continues. I think there are differing opinions on this, probably even among the authors on the paper. There’s what, 25, 26 authors on the paper.\n\n**Daniel Filan:** The paper has a disclaimer, listeners, that not all authors necessarily endorse every claim in the paper.\n\n**Elizabeth Seger:** That includes me. So we have a very, very broad group of authors. But I think… Sorry, what was your original question? It just flew out of my head.\n\n**Daniel Filan:** If it’s too dangerous to open source, is it also too dangerous to make?\n\n**Elizabeth Seger:** Yeah, no. So I think there’s definitely an argument for this. I guess where I’m coming from is trying to be realistic about where this development process is going and how to inform governments on what to do as AI development continues. It may very well be the case that we get to a point where everyone’s convinced that we just have a big coordinated pause/stop in AI development. But assuming that that’s not possible or improbable, shall we say, I think it’s still wise to have a contingency plan. How can we guide AI development to be safe and largely beneficial and reduce the potential for risk? I think there’s also an argument to be made that increasingly capable systems, while they pose increasingly severe risks, also could provide increasingly great benefits and potential economic potential benefits for helping to solve other challenges and crises that we face.\n\nSo I think you’d be hard-pressed to get a giant coordinated pause. So the next plan is how do we make AI development happen safely? It is a lot easier to keep people safe if the super dangerous ones are not spread widely. I guess that’s the very simplistic view. Yeah.\n\nSo I think, at least for me, I think it just comes from a realism about, are we likely to pause AI development before it gets super scary? Probably not, just given how humanity works. We developed nuclear weapons, probably shouldn’t have done that. Oops, well now they exist, let’s deal with it. So I think having a similar plan in line for what do we do with increasingly capable AI systems is important, especially given that it might not be that far away. Like I said, sort of with each new generation of AI that’s released, we all kind of hold our breaths and say, oh, well what’s that? What’s that one going to do? Then we learn from it and we don’t know what the next generation of AI systems is going to bring. And so having systems in place to scan for potential harms, potential dangers, capabilities, to inform decisions about whether or not these systems should be released and if and how they should be shared, that’s really important. We might not be able to coordinate a pause before the big scary happens. So, I think it’s important to discuss this regardless.\n\n**Daniel Filan:** I got you.\n\n**Elizabeth Seger:** That’s a technical term by the way, the big scary.\n\n### Offense-defense balance\n\n**Daniel Filan:** Makes sense. So speaking of the risks of open-sourcing AI, in the paper you talk about the offense-defense balance, and basically you say that bad actors, they can disable misuse safeguards, they can increase new dangerous capabilities by fine-tuning. Open-sourcing makes this easier, it increases attacker knowledge. And in terms of just the AI technology, you basically make a claim that it’s tilted towards offense in that attackers can do… They get more knowledge, they can disable safeguards, they can introduce new dangerous capabilities. It’s easier to find these problems than it is to fix them. And once you fix them, it’s hard to make sure that everyone has the fixes. Would you say that’s a fair summary of-\n\n**Elizabeth Seger:** Yeah, I’d say that’s pretty fair. I think the one thing I’d add… I’d say that with software development, for example… so offense-defense balance is something that’s often discussed in terms of open-sourcing and scientific publication, especially any time you have dual-use technology or scientific insights that could be used to cause harm, you kind of have to address this offense-defense balance. Is the information that’s going to be released going to help the bad actors do the bad things more or less than it’s going to help the good actors do the good things/prevent the bad actors from doing the bad things? And I think with software development, it’s often in favor of defense, in finding holes and fixing bugs and rolling out the fixes and making the technology better, safer, more robust. And these are genuine arguments in favor of why open-sourcing AI systems is valuable as well.\n\nBut I think especially with larger, more complex models, we start veering towards offense balance. I just want to emphasize, I think one of the main reasons for this has to do with how difficult the fixes are. So in the case of software, you get bugs and vulnerabilities that are relatively well-defined. Once you find them, relatively easy to fix, roll out the fix. The safety challenges that we’re facing with highly capable AI systems are quite complex. We have huge research teams around the world trying to figure them out, and it’s a very resource-intensive process, takes a lot of talent. Vulnerabilities are still easy to find, they’re just a lot harder to fix. So, I think this is a main reason why the offense-defense balance probably skews more towards offense and enabling malicious actors because you can still find the vulnerabilities, you can still manipulate the vulnerabilities, take advantage of them… harder to fix, even if you have more people involved. So, that’s the high-level evaluation. Mostly, I just wanted to push on the safety issue a little harder.\n\n### KataGo as a case study\n\n**Daniel Filan:** Gotcha. So, one thing this actually brings up for me is… you may or may not be familiar, so there’s AI that plays the board game Go. There’s an [open-source training run](https://katagotraining.org/) and some colleagues of mine have found basically [you can cheaply find adversarial attacks](https://goattack.far.ai/). So, basically dumb Go bots that play in a way that confuses these computer AI policies. And these attacks, they’re not generally smart, but they just push the right buttons on the AI policy. And I guess this is more of a comment than a question, but there’s been enough rounds of back and forth between these authors and the people making these open source Go bots that it’s potentially interesting to just use that as a case study of the offense-defense balance.\n\n**Elizabeth Seger:** So, you’re saying that given that the system was open source and that people could sort of use it and query it and then send the information back to the developers, that that’s-\n\n**Daniel Filan:** Yeah, and in fact, these authors have been working with the developers of this software, and in fact what’s happened is the developers of this software - KataGo, if people want to google it - they’ve seen this paper, they’re trying to implement patches to fix these issues. The people finding the adversarial policies are basically checking if the fixes work and publishing the information about whether they do or not.\n\n**Elizabeth Seger:** Absolutely, so I want to be really clear that this is a huge benefit of open source development: getting people involved in the development process, but also using the system, finding the bugs, finding the issues, feeding that information back to the developers. This is a huge benefit of open source development for software and AI systems. I think that this is specifically why the paper focuses on the big, highly capable frontier foundation models is that this gets more difficult the more big, complex, cutting edge the system is. Some bugs will still be relatively small, well-defined, and there are bug bounty programs, proposals for AI safety bounty programs as well, helping to find these vulnerabilities and give the information back to the developers. I think there are issues though, with respect to some of the larger safety issues that are more difficult. Sometimes it’s difficult to identify the safety problems in the first place, more difficult to address the safety problems.\n\nThen, there’s also the issue of just rolling out the fixes to the system. So software development, you fix a bunch of bugs, you roll out an update. Oftentimes, in the license it’ll say that you’re supposed to actually use the updated version. There’s some data that came out, I can’t remember which organization it was right now, I’ll have to look it up later, but it’s actually quite a low uptake rate of people actually running the up-to-date software. So first of all, even with just normal software, it’s hard to guarantee that people are actually going to run the up-to-date version and roll out those fixes, which is an issue with open source because if you’re using something behind API, then you just update the system and then everyone’s using the updated system. If you have an open source system, then people actually have to download and run the update version themselves.\n\nWith foundation models, there’s actually a weird incentive structure that changes where people might actually be de-incentivized to update. So with software, oftentimes when you have an update, it fixes some bugs and it improves system functionality. When it comes to safety fixes for foundation models, oftentimes it has to do with reducing system functionality, like putting on a filter that says, “Well, now you can’t produce this class of images. Now, you can’t do this kind of function with the system,” so it’s hard, I don’t know if there’s good information on how this has actually panned out now. Are we seeing lower uptake rates with updates for AI systems? I don’t know, but there might be something weird with incentive structures going on too, where if updates basically equate to reducing system functionality in certain ways, people might be less likely to actually take them on board.\n\n**Daniel Filan:** I don’t have a good feel for that.\n\n**Elizabeth Seger:** I don’t have a super good feel, but just, I don’t know, interesting food for thought, perverse incentive structures.\n\n**Daniel Filan:** I don’t know, I’m still thinking about this KataGo case: so that’s the case where the attack does reduce the system functionality and people are interested in getting the latest version with fixes. It also occurs to me that… So, in fact the structure of this paper, the way they found the attack did not rely on having access to the model weights. It relied on basically being able to query the Go bot policy, basically to try a bunch of things and figure out how to trick the Go bot policy. Now, it’s really helpful if you can have the weights locally just so that you can call the API, so that you can call it a lot, but that was not a case where you needed the actual weights to be shared. So on the one hand, that’s a point that sharing the weights is less valuable than you might think, but it also suggests if you’re worried about people finding these adversarial attacks, then just putting the weights behind an API doesn’t protect you as much as you think. Maybe you need to rate limit or something.\n\n**Elizabeth Seger:** I think that’s a valuable insight, there are definitely things you can do without weights. This is an argument for why you should be worried anyway, but it’s also an argument for… There are lots of arguments for, well, open-sourcing is important because you need it to do safety research and getting more people involved in safety research will result in safer systems, you have more people input into these processes, but you just illustrated a perfect example of how just having query access, for example, to a system, can allow you to do a significant amount of safety research in terms of finding vulnerabilities.\n\n### Openness for interpretability research\n\n**Elizabeth Seger:** So, query access is one that can be done completely behind an API, but then even if we think about something like interpretability research, interpretability research does require much more in-depth access to a system to do, but arguably this is an argument for needing access to smaller systems. We are struggling to do interpretability research on smaller, well-defined systems. It’s sort of like the rate limiting factor on interpretability research isn’t the size of the models people have access to, the way I understand it at least. If we’re struggling to do interpretability research on smaller models, I feel like having access to the biggest, most cutting edge frontier model is not what needs to happen to drive interpretability research.\n\n**Daniel Filan:** I think it depends on the kind of interpretability research.\n\n**Elizabeth Seger:** It’d be interesting to hear your thoughts on this as well, but there’s a range of different kinds of AI research. Not all of it requires open access and then some of the kinds that do require open access to the models isn’t necessarily helped the most by having the open access, and then there is also this idea of alternative approaches that we talk about in the paper. You can help promote AI safety research by providing access to specific research groups. There are other things you can do to give people the access they need to do safety research.\n\n**Daniel Filan:** So, I guess I can share my impressions here. Interpretability research is a broad bucket. It describes a few different things. I think there are some kinds of things where you want to start small and we haven’t progressed that far beyond small. So just understanding, ‘can we just exhaustively understand how a certain neural net works?’ - start small, don’t start with GPT-4. But I think one thing you’re potentially interested in, in the interpretability context, is how things get different as you get bigger models, or do bigger models learn different things or do they learn more? What sorts of things start getting represented? Can we use interpretability to predict these shifts? There you do want bigger models. In terms of how much can you do without access to weights, there’s definitely a lot of interpretability work on these open source models because people apparently really do value having the weights.\n\nEven in the case of the adversarial policies work I was just talking about, you don’t strictly need access to the weights, but if you could run the games of Go purely on your computer, rather than calling the API, waiting for your request to be sent across the internet, and the move to be sent back, and doing that a billion times or I don’t know the actual number, but it seems like just practically-\n\n**Elizabeth Seger:** Much more efficient.\n\n**Daniel Filan:** … It’s easier to have the model. I also think that there are intermediate things. So one thing the paper talks about, and I guess your colleague [Toby Shevlane](https://sites.google.com/view/tobyshevlane) has [talked about](https://arxiv.org/abs/2201.05159) is basically structured access of giving certain kinds of information available maybe to certain people or maybe you just say “these types of information are available, these types aren’t”. I’ve heard colleagues say, “Even if you didn’t open source GPT-4 or GPT-3, just providing final layer activations or certain kinds of gradients could be useful”, which would not… I don’t think that would provide all the dangers or all the risks that open-sourcing could potentially involve.\n\n**Elizabeth Seger:** I think this is a really key point as well, is trying to get past this open versus closed dichotomy. Just saying that something isn’t open source doesn’t necessarily mean that it’s completely closed and no-one can access it. So like you said, Toby Shevlane talks about structured access, and it was a paper we referenced - at least when we referenced it, it was still forthcoming, it might be out now - but it was Toby Shevlane and Ben Bucknell were working on it, and it was about the potential of developing research APIs. So, how much access can you provide behind API to enable safety research and what kind of access would that need to look like and how could those research APIs be regulated and who by? So, I think if there’s a genuine interest in promoting AI safety research and a genuine acknowledgement of the risks of open-sourcing, we could put a lot of resources into trying to develop and understand ways to get a lot of the benefits to safety research that open-sourcing would have by alternative means.\n\nIt won’t be perfect. By definition, it’s not completely open, but if we take the risks seriously, I think it’s definitely worth looking into these alternative model sharing methods and then also into the other kinds of proactive activities we can engage in to help promote safety research, whether that’s committing a certain amount of funds to safety research or developing international safety research organizations and collaborative efforts. I know one issue that always comes up when talking about, “Well, we’ll just provide safety research access through API, or we’ll provide privileged downloaded access to certain groups.” It’s like, “Well, who gets to decide who has access? Who gets to do the safety research?”\n\nAnd so, I think this points to a need to have some sort of a multi-stakeholder governance body to mediate these decisions around who gets access to do the research, whether you’re talking about academic labs or other private labs, sort of like \\[how\\] you have multi-stakeholder organizations decide how to distribute grants to do environmental research, or you have grantmaking bodies that distribute grant funds to different academic groups. You could have a similar type situation for distributing access to more highly capable, potentially dangerous systems, to academic groups, research groups, safety research institutions that meet certain standards and that can help further this research.\n\nSo, I feel like if there’s a will to drive safety research forward, and if varying degrees of access are needed to allow the safety research to happen, there are things we can do to make it happen that do not necessarily require open-sourcing a system. And I think, like we said, different kinds of safety research require varying degrees of access. It’s not like all safety research can be done with little access. No, you need different amounts of access for different kinds of safety research, but if there’s a will, there’s a way.\n\n### Effectiveness of substitutes for open sourcing\n\n**Daniel Filan:** So, I want to ask something a bit more quantitative about that. So, some of the benefits of open-sourcing can be gained by halfway measures or by structured access or pursuing tons of collaborations, but as you mentioned, it’s not going to be the same as if it were open sourced. Do you have a sense of… I guess it’s going to depend on how constrained you are by safety, but how much of the benefits of open source do you think you can get with these more limited sharing methods?\n\n**Elizabeth Seger:** That’s a good question. I think you can get quite a bit, and I think, again, it sort of depends what kind of benefit you’re talking about. So in the paper, I think we discuss three different benefits. Let’s say we talk about accelerating AI research, so that’s safety research and capability research. We talk about distributing influence over AI systems, and this ranges everything from who gets to control the systems, who gets to make governance decisions about the systems, who gets to profit. It wraps all the democratization themes together under distributing influence over AI, and then, let’s see, what was the other one that we talked about? You’d think I’ve talked about this paper enough in the last three months, I have it down. Oh, external model evaluation. So, enabling external oversight and evaluation, and I think it depends which one you’re talking about.\n\nLet’s start with external model evaluation. I think that this probably benefits the most from open-sourcing. It depends what you’re looking at, so for example, if you’re just looking for minor bugs and stuff like that, you don’t need open source access for that, but having a more in-depth view to the systems is more important for people trying to help find fixes to the bugs. We’ve discussed this. There are also risks associated with open-sourcing. If we’re talking about accelerating capability research, for example, which sort of falls under the second category, I think you might find that the benefits of open-sourcing here might be somewhat limited the larger and more highly capable the system gets. And I think this largely will just have to do with who has access to the necessary resources to really operate on the cutting edge of research and development. Open source development, it operates behind the frontier right now largely because of restrictions… not restrictions, but just the expense of the necessary compute resources.\n\nAnd then you talk about distributing control over AI, we’ve already discussed the more distributed effect of open-sourcing and model sharing on distributing control. It’s a second order effect: you get more people involved in the development process and then large labs have more competition, and then it distributes influence and control.\n\nThere are probably more direct ways you can help distribute control and influence over AI besides making a system widely available. So, to answer your original question then about, how much of the benefit of open-sourcing can you get through alternative methods? I guess it really depends what benefit you’re talking about. I think for AI safety progress probably quite a bit, honestly; actually the vast majority of it, given that a lot of the safety research that’s done on these highly capable, cutting edge models is something that has to happen within well-resourced institutions anyway, or you need the access to the resources to do that, not just the code and the weights, but the computational resources and so on.\n\nSo, I think quite a bit. I think it’s less of a “can we get close to the same benefits that open-sourcing allows?” It’s more like, “can we do it in one fell swoop?” That’s the thing. It’s like open-sourcing is the easy option. “Here, it’s open!” - and now you get all these benefits from open-sourcing. The decision to open-source or not, part of the reason it’s a hard decision is because achieving these benefits by other means is harder. It’s going to take more resources to invest, more organizational capacity, more thought, more cooperation. It’s going to take a lot of infrastructure, a lot of effort. It’s not the one-stop shop that open-sourcing is, but I think the idea is that if the risks are high enough, if the risks are severe enough, it’s worth it. I think that’s where it comes in.\n\nSo, I guess it’s worth reiterating again and again: this paper is not an anti-open source paper, \\[it’s\\] very pro-open source in the vast majority of cases. What we really care about here are frontier AI systems that are starting to show the potential for causing really catastrophic harm, and in these cases, let’s not open-source and let’s pursue some of these other ways of achieving the same benefits of open source to safety and distributing control and model evaluation, but open-source away below that threshold. The net benefits are great.\n\n### Offense-defense balance, part 2\n\n**Daniel Filan:** Gotcha. So my next question - I actually got a bit sidetracked and wanted to ask it earlier - so in terms of the offense-defense balance, in terms of the harms that you are worried about from open-sourcing, I sometimes hear the claim that basically, “Look, AI, if you open-source it, it is going to cause more harm, but you also enable more people to deal with the harm.” So I think there, they’re talking about offense-defense balance, not of finding flaws in AI models, but in the underlying issues that AI might cause. So, I guess the idea is something… To caricature it, it’s something like, “Look, if you use your AI to create a pathogen, I can use my AI to create a broad spectrum antibiotic or something”, and the hope is that in these domains where we’re worried about AI causing harm, look, just open-sourcing AI is going to enable tons of people to be able to deal with the harm more easily, as well as enabling people to cause harm. So I’m wondering, what do you think about the underlying offense-defense balance as opposed to within AI?\n\n**Elizabeth Seger:** I get the argument. Personally, I’m wary about the arms race dynamic though. You gotta constantly build the stronger technology to keep the slightly less strong technology in check. I guess this comes back to that very original question you asked about, “What about just hitting the no more AI button?” So, I guess I get the argument for that. I think there’s weird dynamics, I don’t know. I’m not doing a very good job answering this question. I’m personally concerned about the race dynamic here, and I think it just comes back to this issue of, how hard is it to fix the issues and vulnerabilities in order to prevent the misuse in the first place? I think that should be the goal: preventing the misuse, preventing the harm in the first place. Not saying, “Can we build a bigger stick?”\n\nThere’s a similar argument that is brought up when people talk about the benefits of producing increasingly capable AI systems and saying, “Oh, well, we need to plow ahead and build increasingly capable AI systems because you never know, we’ll develop a system that’ll help cure cancer or develop some renewable energy technology that’ll help us address climate change or something like that.” What huge problems could AI help us solve in the future? And I don’t know - this is personally me, I don’t know what the other authors on this paper think of this - but I don’t know, I kind of feel like if those are the goals, if the goals are to solve climate change and cure cancer, take the billions upon billions upon billions and billions of dollars that \\[we’re\\] currently putting into training AI systems and go cure cancer and develop renewable technologies! I struggle with those arguments personally. I’d be interested just to hear your thoughts. I have not written about this. This is me riffing right now. So, I’d be interested to hear your thoughts on this train of thought as well.\n\n**Daniel Filan:** I think the original question is unfairly hard to answer just because it’s asking about the offense-defense balance of any catastrophic problem AI might cause and it’s like, “Well, there are tons of those and it’s pretty hard to think about.” So, the thing you were saying about, if you wanted to cure cancer, maybe step one would not be “create incredibly smart AI”. I’ve seen this point. I don’t know if you know [David Chapman](https://meaningness.com/about-my-sites)’s [Better without AI](https://betterwithout.ai/)?\n\n**Elizabeth Seger:** No, not familiar.\n\n**Daniel Filan:** So, he basically argues we just shouldn’t build big neural nets and it’s going to be terrible. Also, [Jeffrey Heninger](https://aiimpacts.org/author/jeffreyheninger/) at [AI Impacts](https://aiimpacts.org/), I think has said [something similar along these lines](https://blog.aiimpacts.org/p/my-current-thoughts-on-the-ai-strategic). On the one hand, I do kind of get it, just in the sense that, if I weren’t worried about misaligned AI, there’s this hope that this is the last invention you need. You create AI and now instead of having to separately solve cancer and climate change and whatever, just make it solve those things for you.\n\n**Elizabeth Seger:** I guess it’s just really hard to look forward, and you have to decide now whether or not this technology is that silver bullet and how much investment it’s going to take to get to that point.\n\n**Daniel Filan:** I think that’s right, and I think your take on this is going to be driven by your sense of the risk profile of building things that are significantly smarter than us. I guess from the fact that I made the AI X-risk Research Podcast, rather than the AI Everything’s Going to be Great Research Podcast, people can guess my-\n\n**Elizabeth Seger:** It’s an indication of where you’re coming from.\n\n**Daniel Filan:** … take on this, but I don’t know. I think it’s a hard question. So, part of my take is, in terms of the underlying offense-defense balance, I think it becomes more clear when you’re worried about, what should I say, silicogenic risks? Basically the AI itself coming up with issues rather than humans using AI to have nefarious schemes. Once you’re worried about AI doing things on their own where you are not necessarily in control, there I think it makes sense that you’re probably… If you’re worried about not being able to control the AIs, you’re probably not going to be able to solve the risks that the AIs are creating, right?\n\n**Elizabeth Seger:** Yeah, your management plan for AI shouldn’t be to build a slightly more powerful AI to manage your AI.\n\n**Daniel Filan:** Well, if you knew that you were going to remain in control of the slightly bigger AI, maybe that’s a plan, but you kind of want to know that.\n\n**Elizabeth Seger:** I guess I was saying that if you’re worried about loss of control scenarios, then the solution shouldn’t be, “Well, let’s build another system that’s also out of our control, but just slightly better aligned to address the…” I feel like that’s-\n\n**Daniel Filan:** It’s not the greatest. I think my colleague John Wentworth has [some saying](https://www.lesswrong.com/posts/DwqgLXn5qYC7GqExF/godzilla-strategies), “Releasing Mothra to contain Godzilla is not going to increase property values in Tokyo,” which is a cute little line. I don’t know, it’s a hard question. I think it’s hard to say anything very precise on the topic. I did want to go back to the offense-defense balance. So moving back a bit, a thing you said was something like, “Look, it’s probably better to just prevent threats from arising, than it is to have someone make a pathogen and then have everyone race to create an antibiotic or antiviral or whatever.” So, that’s one way in which everyone having really advanced AI… That’s one way that could look in order to deal with threats. I think another way does look a bit more like prevention. I don’t know, it’s also more dystopian sounding, but one thing that AI is good at is surveillance, right?\n\n**Elizabeth Seger:** Yes.\n\n**Daniel Filan:** Potentially, so you could imagine, “Look, we’re just going to open source AI and what we’re going to use the AI for is basically surveilling people to make sure the threats don’t occur.” So, maybe one version of this is you just really amp up wastewater \\[testing\\]. Somehow you use your AI to just look at the wastewater and see if any new pathogens are arising. It could look more like you have a bunch of AIs that can detect if other people are trying to use AI to create superweapons or whatever, and stop them before they do somehow.\n\n**Elizabeth Seger:** The wastewater example, that sounds great. We should probably do that anyway. In terms of surveilling to see how people are using AI systems using AI, why not just have the AI systems be behind an API where people can use the systems for a variety of downstream tasks integrating through this API and then the people who control the API can just see how the system is being used? Even if it can be used for a vast majority of tasks, even if you were to take all the safety filters off, the advantage of the API is still that you can see how it’s being used. I don’t know, I feel like that’s-\n\n### Making open-sourcing safer?\n\n**Daniel Filan:** That seems like a good argument. All right, so I guess another question I have is related to the frame of the report. So in the report you’re basically like, “open-sourcing has these benefits, but it also has these costs. What are ways of doing things other than open-sourcing that basically try and retain most of the benefits while getting rid of most of the costs?” You can imagine a parallel universe report where you say, “Okay, open-sourcing has these benefits, but it also has these costs. We’re still going to open source, but we’re going to do something differently in our open source plan that is going to retain benefits and reduce costs”, right? So one example of this is you open-source models, but you have some sort of watermarking or you have some sort of cryptographic backdoor that can stop models in their tracks or whatever. I’m wondering: why the frame of alternatives to open-sourcing rather than making open-sourcing better?\n\n**Elizabeth Seger:** Very simple. I think making open-sourcing better is the harder question, technically more difficult. I mean, for example, say you have watermarking, part of the issue with watermarking to identify artificially generated images is making sure the watermarks stick. How do you make sure that they are irremovable if you are going to open… This is a really complex technical question: how do you develop a system that has watermarked images where that watermark is irremovable if you were to open source the system?\n\nI’m not saying it’s undoable. I personally don’t have the technical background to comment very deeply on this. I have heard people talking about how possible this would be. It also depends how you watermark, right?\n\nIf you have just a line of inference code that says, “slap a watermark on this thing”, it could delete the line of inference code. If you’re to train the system on only watermarked images, well now you have to retrain the entire system to get it to do something else, which is very expensive. So again, I think it depends how you do it.\n\nI was at a meeting last week where people were talking about, are there ways we could build in a mechanism into the chips that run the systems that say “if some bit of code is removed or changed in the system, then the chip burns up and won’t run the system”. I’m not saying this is impossible, but \\[a\\] really interesting technical question, really difficult, definitely beyond my area of expertise. But I think if this is an approach we can take and say, there are ways to be able to open source the system and get all the benefits of open-sourcing by just open-sourcing and still mitigate the risks, I think that’s great. I think it’s just a lot more difficult.\n\nAnd there’s one aspect in which we do take the flip view in the report, and I think this is where we start talking about a staged release of models. You can undergo a staged release of a model where you put out a slightly smaller version of a model behind API, you study how it’s being used. Maybe you take a pause, analyze how it was used, what \\[are\\] the most common avenues of attack, if at all, \\[that\\] were being used to try and misuse the model.\n\nAnd then you release a slightly larger model, \\[then\\] a slightly larger model. You do this iteratively, and if you do this process, as you get to a stage where it’s like, hey, we’ve been doing the staged release of this model for however many months and no problems, looking good, there’s no emergent capabilities that popped up that are making you worried. You didn’t have to implement a bunch of safety restrictions to get people to stop doing unsafe things - okay, open source. This is not a binary \\[of\\] it has to be completely open or completely closed. And I think this is one respect… If you were to take this flip view of “how can we open source, but do it in the safest way possible?” Just open source slowly, take some time to actually study the impacts. And it’s not like the only way to get a sense of how the system’s going to be used is to just open source it and see what happens. You could do a staged release and study what those impacts are.\n\nAgain, it won’t be perfect. You never know how it’s going to be used 10 years down the road once someone gets access to all the weights and stuff. But it is possible to study and get some sort of insight. And I think one of the nice things about staged release is if you start doing the staged release process and you realize that at each iterative step you are having to put in a bunch of safety filters, for example, to prevent people from doing really shady stuff, that’s probably a good indication that it’s not ready to be open sourced in its current form, because those are safety filters that will just immediately be reversed once open sourced. So I think you can learn a lot from that.\n\nSo I think that’s one way you can open source safely: find ways to actually study what the effects are before you open source, because that decision to open source is irreversible. And then I think the technical challenge of, are there ways we can have backstops, that we can technically build in irreversible, irremovable filters or watermarks or even just hardware challenges that we could implement - I think \\[those are\\] really interesting technical questions that I don’t know enough about, but… Go for it. That’d be a great world.\n\n**Daniel Filan:** Yeah. If listeners are interested, this gets into some territory that [I talked about with Scott Aaronson earlier this year](https://axrp.net/episode/2023/04/11/episode-20-reform-ai-alignment-scott-aaronson.html#watermarking-lm-outputs). Yeah, I think the classic difficulties, at least say for watermarking… I read [one paper](https://arxiv.org/abs/2012.08726) that claims to be able to bake the watermark into the weights of the model. To be honest, I didn’t actually understand how that works.\n\n**Elizabeth Seger:** I think it has to do with how the model’s trained. So the way I understand it is if you have a dataset of images that all have a watermark in that dataset, not watermark in the sense like you see on a $1 bill, but weird pixel stuff that the human eye can’t see. If all the images in the training dataset have that watermark, then all of the images it produces will have that watermark. In that case, it’s baked into the system because of how it was trained. So the only way to get rid of that watermark would be to retrain the system on images that don’t contain the watermark.\n\n**Daniel Filan:** Yeah, that’s one possibility. So that’s going to be a lot rougher for applying to text models, of course, if you want to just train on the whole internet. I think I saw [something](https://arxiv.org/abs/2012.08726) that claimed to work even on cases where the dataset did not all have the watermark, but I didn’t really understand how it worked. But at any rate, the key issue with these sort of watermarking methods is as long as there’s one model that can basically paraphrase that does not have watermarking, then you can just take your watermark thing and basically launder it and get something that - if your paraphrasing model is good enough, you can create something that looks basically similar, it doesn’t have the watermark, and then it’s sad news. Yeah. And then-\n\n**Elizabeth Seger:** Sorry, I was going to say there’s similar \\[things\\] in terms of how doing something with one model allows you to jailbreak another model. I mean this is what happened with the [‘Adversarial suffixes’ paper](https://arxiv.org/abs/2307.15043), where using a couple open source models, one which was Llama 2, and using the weights of those models, figuring out a way to basically just throw a seemingly random string of numbers at a large language model, and then with that seemingly random range of numbers before the prompt basically get the system to do whatever you want. Except while they figured out how to do that using the weights accessible from Llama 2, it worked on all the other large language models. So finding a way to jailbreak one model and using the weights and access to one model, that could bring up vulnerabilities in tons of others that aren’t open sourced as well. So I think that’s another roughly related somewhat to what we were just talking about point.\n\n**Daniel Filan:** Yeah, I guess it brings up this high level thing of whatever governance method for AI you want, you want it to be robust to some small fraction of things breaking the rules. You don’t want the small fraction to poison the rest of the thing, which watermarking unfortunately has.\n\nYeah, I guess I wanted to say something brief about backdoors as well. So there is really a way of, at least in toy neural networks, and you can probably extend it to bigger neural networks, [you really can introduce a backdoor that is cryptographically hard to detect](https://arxiv.org/abs/2204.06974). So one problem is, how do you actually use this to prevent AI harm is not totally obvious. And then there’s another issue of… I guess the second issue only comes up with super smart AI, but if you have a file on your computer that’s like, “I implanted a backdoor in this model, the backdoor is this input”, then it’s no longer cryptographically hard to find as long as somebody can break into your computer. Which hopefully is cryptographically hard, but I guess there are security vulnerabilities there.\n\nSo yeah, I wonder if you want to say a little bit about the safer ways to get the open source benefits. I’ve given you a chance to talk about them a little bit, but is there anything more you want to say about those?\n\n**Elizabeth Seger:** I think, not really. I think the overarching point is, just as I said before, when the risks are high - and I think that’s key to remember, I’m not saying don’t open-source everything - when the risks are high, it is worth investing in seeing how else we can achieve the benefits of open-sourcing. Basically, if you’re not going to open-source because the risks are high, then look into these other options. It’s really about getting rid of this open versus closed dichotomy.\n\nSo many of the other options have to do with other options for sharing models, whether that’s structured access behind API, even research API access, gated download, staged release, and then also more proactive efforts. Proactive efforts which can actually also be combined with open-sourcing. They don’t have to be seen as an alternative to open-sourcing. So this is things like redistributing profits towards AI safety research or starting AI safety and bug bounty programs. Or even like we talked about with [the democratization paper](https://arxiv.org/abs/2303.12642), thinking about how we can democratize decision-making around AI systems to help distribute influence over AI away from large labs, which is another argument for open-sourcing.\n\nSo yeah, I think that this is key: there are other efforts that can be put in place to achieve many of the same benefits of open-sourcing and when the risks are high, it’s worth really looking into these.\n\nAI governance research\n----------------------\n\n**Daniel Filan:** All right. Okay. So moving on, I want to talk a little bit more broadly about the field of AI governance research. So historically, this podcast is mostly focused on technical AI alignment research, and I imagine most listeners are more familiar with the technical side than with governance efforts.\n\n**Elizabeth Seger:** In which case, I apologize for all my technical inaccuracies. One of the benefits of having 25 co-authors is that a lot of the technical questions I got to outsource.\n\n### The state of the field\n\n**Daniel Filan:** Makes sense. Yeah, it’s good to be interdisciplinary. So this is kind of a broad question, but how is AI governance going? What’s the state of the field, if you can answer that briefly?\n\n**Elizabeth Seger:** The state of the field of AI governance. Yeah. Okay. I’ll try and answer that briefly. It’s going well in that people are paying attention. In this respect, the release of ChatGPT I think was really great for AI governance because people, besides those of us already doing AI governance research, are really starting to see this as something valuable and important that needs to be talked about and \\[asking\\] questions around what role should governments play in regulating AI, if at all? How do we get this balance between governments and the developers? Who should be regulated with respect to different things? Do all of the responsibilities lie on the developers or is it on the deployers?\n\nAnd all of these questions suddenly are coming to light and there’s more general interest in them. And so we’re seeing things like, the [UK AI Summit](https://en.wikipedia.org/wiki/2023_AI_Safety_Summit) is happening next week, \\[a\\] global AI summit, looking at AI safety, really concerned about catastrophic and existential risks, trying to understand what kind of global institutions should be in place to govern AI systems, to evaluate AI systems, to audit, to regulate.\n\nAnd this is bringing in countries from all over the world. I think it’s something like 28 different countries are going to be at the UK AI Summit. You have [the EU AI Act](https://artificialintelligenceact.eu/) where it started a while ago looking at narrow AI systems, but now is taking on foundation models and frontier AI systems and looking at open source regulation. And this has really, over the last year, exploded into a global conversation.\n\nSo in that respect, AI governance is going well in that people are paying attention. It’s also very high stress because suddenly everyone’s paying attention. We have to do something. But I think there’s really genuine interest in getting this right, and I think that really bodes well. So I’m excited to see where this next year goes. Yeah, there’s talk about having this global AI summit and then making this a recurring series. And so I think it’s going well in the sense that people are paying attention and the wheels are starting to turn, and that’s cool.\n\n### Open questions\n\n**Daniel Filan:** Okay. I guess related to that, what do you see as the most important open questions in the field?\n\n**Elizabeth Seger:** In the field of AI governance? Okay. So I think one big one is compute governance, which my colleague, [Lennart Heim](https://heim.xyz/about/), [works on](https://www.governance.ai/team/lennart-heim). This is just thinking about how compute is a lever for trying to regulate who is able to develop large models, even how compute should be distributed so that more people can distribute large models, but basically using compute as a lever to understand who has access to and who is able to develop different kinds of systems. So I think that’s a huge area of research with a lot of growing interest because compute’s one of the tangible things that we can actually control the flow of.\n\nI think that the questions around model-sharing and open-sourcing are getting a lot of attention right now. Big open question, a lot of debate, like I said, it’s becoming really quite a polarized discussion, so it’s getting quite hard to cut through. But a lot of good groups \\[are\\] working on this, and I think a lot of interest in genuinely finding common ground to start working on this. I’ve had a couple situations where I’ve been in groups or workshops where we get people who are very pro-open source and other people who are just like, no, let’s just shut down the whole AI system right now, really both sides of the spectrum coming together. And we try and find a middle ground on, okay, where do we agree? Is there a point where we agree? And very often we can come to a point of agreement around the idea that there may be some AI system, some model that poses risks that are too extreme for that model to be responsibly open sourced.\n\nAnd that might not sound like that extreme of a statement, but when you have people coming from such polarized views to agree on the fact that there may exist a model one day that should not be open source, that is a starting point and you can start the conversation from there. And every group I’ve been in so far has got to that point, and we can start working on that. So I think this model-sharing question is a big open question and lots of technical research needs to be done around benchmarking to decide, when are capabilities too dangerous?\n\nAlso around understanding what activities are actually possible given access to different combinations of model components. And that’s actually not entirely clear, and we need a much more fine-grained understanding of what you can actually do given different kinds of model, combinations of model components, in order not only to have safe standards for model release and really a fine-grained standard for model release, but also to protect the benefits of open-sourcing. You don’t want to just have a blanket “don’t release anything” if you can get a lot of good benefit out of releasing certain model components. So I think a lot of technical research has to go into this.\n\nAnyway. So yeah, second point, I think model-sharing is a really big point of discussion right now. And then with the upcoming [UK AI Summit](https://en.wikipedia.org/wiki/2023_AI_Safety_Summit), \\[there’s\\] quite a bit of discourse around what international governance structures should look like for AI, a lot of different proposed models. And yeah, it’ll be interesting to see what comes out of the summit. I don’t think they’re going to agree on anything amazing at the summit. It’s two days. But I think for me, a really great outcome of the summit would be, first, recognition from everyone that AI systems could pose really extreme risks. So just a recognition of the risks. And then second, a plan going forward, a plan for how we can start establishing international systems of governance and really structure out when are we going to come to what kinds of decisions and how can we start putting something together. So I think that those are probably three key open questions, and the international governance structure one is really big right now too, just given the upcoming summit.\n\n**Daniel Filan:** And I guess unless we get the editing and transcription for this episode done unusually quickly, listeners, the [UK AI Summit](https://en.wikipedia.org/wiki/2023_AI_Safety_Summit) is almost definitely going to be in your past. So I guess listeners are in this interesting position of knowing how that all panned out in a way that we don’t. So that was open questions in the field broadly. I’m wondering for you personally as a researcher, what things are you most interested in looking at next?\n\n**Elizabeth Seger:** Interesting. I mean, most of my life is taken up with follow-up on this open source report right now. So I definitely want to keep looking into questions around model-sharing and maybe setting responsible scaling policy, responsible model release policy. I’m not exactly sure. I think I’m in this place right now where I’m trying to feel out where the most important work needs to be done and whether the best place for me to do is to encourage other people to do certain kinds of work where I don’t necessarily have the expertise, like we were talking about, like needing more technical research into what is possible given access to different combinations of model components, or are there specific areas of research I could try to help lead in? Or whether really what needs to be done is just more organizational capacity around these issues.\n\nSo no, I am personally interested in keeping up with this model-sharing discussion. I think there’s a lot of interesting work that needs to be done here, and it’s a key thing that’s being considered within the discussions around international AI governance right now. Yeah, so sorry I don’t have as much of a clear cut answer there, but yeah, I’m still reeling from having published this report and then everything that’s coming off the back of it and just trying to feel out where’s the next most important, most impactful step, what work needs to be done. So I guess if any of your listeners have really hot takes on “oh, this is what you should do next”, I guess, please tell me. It’ll be very helpful.\n\n**Daniel Filan:** How should they tell you if someone’s just heard that and they’re like, oh, I need to-\n\n**Elizabeth Seger:** “I need to tell her now! She must know!” Yeah, so I mean, I have a [website](https://elizabethseger.com/) where you could find a lot of my contact information, or you can always find me on [LinkedIn](https://uk.linkedin.com/in/elizabeth-seger-ph-d-5209797b). I spend far too much time on LinkedIn these days. And also my email address happens to be on the open source report. So if you download the report, my email address is there.\n\n**Daniel Filan:** What’s the URL of your website?\n\n**Elizabeth Seger:** [ElizabethSeger.com](https://elizabethseger.com/).\n\n### Distinctive governance issues of x-risk\n\n**Daniel Filan:** All right. Okay. Getting back to talking about governance in general, I’m wondering… so I guess this is an x-risk-focused podcast. How, if at all, do you think governance research looks different when it’s driven by concerns about x-risk mitigation versus other concerns you could have about AI governance?\n\n**Elizabeth Seger:** Well, that’s a good question. Let’s see. So the question is how does the governance research look different?\n\n**Daniel Filan:** Yeah. What kinds of different questions might you focus on, or what kinds of different focuses would you have that would be driven by x-risk worries rather than by other things?\n\n**Elizabeth Seger:** So this is something that I’ve had to think about a lot in my own research development because I did not come into this area of research from an x-risk background interest. I came into it… I mean, honestly, I started in bioethics and then moved from bioethics looking at AI systems in healthcare and have sort of moved over into the AI governance space over a very long PhD program. And so here I am.\n\nBut I would say one of the things that I’ve learned working in the space \\[that’s\\] more interested in long-term x-risk impacts of AI and trying to prevent x-risks is really paying attention to causal pathways and really trying to be very critical about how likely a potential pathway is to actually lead to a risk. I don’t know if I’m explaining this very well.\n\nMaybe a better way of saying it’s like: if you have a hypothesis, or let’s say you’re worried about the impacts of AI systems on influence operations or impacting political campaigns, I find it really helpful to start from the hypothesis of, it won’t have an impact. And really just trying to understand how that might be wrong, as opposed to trying to start from “oh, AI is going to pose a massive bio threat, or it’s going to pose a massive threat to political operations” or something like that. And then almost trying to prove that conclusion.\n\nYeah, I don’t know, I start from the opposite point and then try and think about all the ways in which I could be wrong. And I think this is really important to do, especially when you’re doing x-risk research, whether it’s with respect to AI or some other form of x-risk. Because I think there are a lot of people that turn off when you start talking about existential risks, they think it’s too far out there, it’s not really relevant to the important questions that are impacting people today, the tangible things that people are already suffering. And so I think it’s really important to be very, very rigorous in your evaluations and have a very clear story of impact for why it is that you’re doing the research you’re doing and focusing on the issues that you’re doing. At least that’s been my experience, trying to transition into the space and work on these issues.\n\n### Technical research to help governance\n\n**Daniel Filan:** Another question I have, related to my audience… So I think my audience, a lot of them are technical alignment researchers and there are various things they could do, and maybe they’re interested in, okay, what work could technical alignment people do that would make AI governance better? I’m wondering if you have thoughts on that question.\n\n**Elizabeth Seger:** Okay. Technical alignment people, AI governance better. Yeah, I mean there’s a lot of work going on right now, especially within the UK government. We just set up the [UK AI Task Force](https://www.gov.uk/government/publications/frontier-ai-taskforce-first-progress-report/frontier-ai-taskforce-first-progress-report), a government institution doing a lot of model evals and alignment research. I think if you have the technical background in alignment research, you are very much needed in the governance space. There’s very often a disconnect between… I mean, I am also guilty of this. There’s a disconnect between the people doing the governance research and the people who have the experience with the technology and really know the ins and outs of the technology that’s being developed.\n\nAnd I think if you have the inclination to work in an AI governance space and help bridge that gap, that would be incredibly valuable. And like I’ve already said, some of the more technical questions, even around open-sourcing, are things that I was very, very glad to have colleagues and co-authors on the paper who have worked for AI labs and stuff before and really knew what they were talking about and could advise and help write some of the more technical aspects of the report.\n\nSo I think if you have the inclination to work in the space, to get involved with governance efforts, or even maybe some of these government institutions that are starting to pop up that are working on the boundary of AI governance and technical research, that could be a really valuable place to contribute. So I think my 2 cents off the top of my brain would be help bridge that gap.\n\n**Daniel Filan:** Okay. Great. So before we wrap up, I’m wondering if there’s anything that you wish I’d asked but that I didn’t?\n\n**Elizabeth Seger:** Oh, that’s a good question. No, I don’t think so. I think we’ve covered a lot of good stuff. Yeah, thank you for having me on really. I’d say there’s nothing in particular. This has been great.\n\nFollowing Elizabeth’s research\n------------------------------\n\n**Daniel Filan:** All right, so to wrap up then, if people are interested in following your research, following up on this podcast, how should they do that?\n\n**Elizabeth Seger:** So I have my website, [ElizabethSeger.com](https://elizabethseger.com/). It sort of outlines my different ongoing research projects, has a lot of publications on it. Also, [GovAI’s website](https://www.governance.ai/) is a wealth of information \\[on\\] all things AI governance from all my great colleagues at GovAI and our affiliates. So really, yeah, there’s new research reports being put out almost every week, maybe every other week, but really high quality stuff. So you can find a lot of my work on the GovAI website or my current work and past work on my own website or find me on [LinkedIn](https://uk.linkedin.com/in/elizabeth-seger-ph-d-5209797b). Yeah, just happy to talk more.\n\n**Daniel Filan:** All right, well thank you very much for being on the podcast.\n\n**Elizabeth Seger:** Great, thank you.\n\n**Daniel Filan:** This episode is edited by Jack Garrett and Amber Dawn Ace helped with transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future) and [Lightspeed Grants](https://lightspeedgrants.org/), along with [patrons](https://patreon.com/axrpodcast) such as Tor Barstad, Alexey Malafeev, and Ben Weinstein-Raun. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nThe events of this year have highlighted important questions about the governance of artificial intelligence. For instance, what does it mean to democratize AI? And how should we balance benefits and dangers of open-sourcing powerful AI systems such as large language models? In this episode, I speak with Elizabeth Seger about her research on these questions.\n\nTopics we discuss:\n\n * What kinds of AI?\n * Democratizing AI\n   * How people talk about democratizing AI\n   * Is democratizing AI important?\n   * Links between types of democratization\n   * Democratizing profits from AI\n   * Democratizing AI governance\n   * Normative underpinnings of democratization\n * Open-sourcing AI\n   * Risks from open-sourcing\n   * Should we make AI too dangerous to open source?\n   * Offense-defense balance\n   * KataGo as a case study\n   * Openness for interpretability research\n   * Effectiveness of substitutes for open sourcing\n   * Offense-defense balance, part 2\n   * Making open-sourcing safer?\n * AI governance research\n   * The state of the field\n   * Open questions\n   * Distinctive governance issues of x-risk\n   * Technical research to help governance\n * Following Elizabeth’s research\n\nDaniel Filan: Hello, everybody. In this episode, I’ll be speaking with Elizabeth Seger. Elizabeth completed her PhD in philosophy of science at Cambridge in 2022, and is now a researcher at the Centre for Governance of AI in Oxford, where she works on AI democratization and open-source AI regulation. She recently led the production of a large report on the risks and benefits of model-sharing, which we will talk about in this episode. For links to what we’re discussing, you can check the description of the episode and you can read the transcript at axrp.net.\n\nWell, Elizabeth, welcome to the podcast.\n\nElizabeth Seger: Awesome. Thanks for having me.\n\n\nWhat kinds of AI?\nDaniel Filan: Cool. We’re going to be talking about a couple of papers basically about democratizing and open-sourcing AI. Wh",
      "wordCount": 19786
    },
    "tags": [
      {
        "_id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "slug": "ai-governance"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "q6Euc7FAWJpDbst6r",
        "name": "Open Source AI",
        "slug": "open-source-ai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [
      {
        "id": "qHDus5MuMNqQxJbjD",
        "name": "AI Governance",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Governance & Policy"
    ],
    "extraction_source": {
      "tag_id": "qHDus5MuMNqQxJbjD",
      "tag_name": "AI Governance",
      "research_agenda": "AI Governance & Policy"
    }
  },
  {
    "_id": "eA2WNb2rAKbrF2QHy",
    "title": "How to type Aleksander Mądry's last name in LaTeX",
    "slug": "how-to-type-aleksander-madry-s-last-name-in-latex-1",
    "url": null,
    "baseScore": 9,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 1,
    "createdAt": null,
    "postedAt": "2023-11-21T00:50:07.189Z",
    "contents": {
      "markdown": "1.  Type “Madry”.\n2.  Realize that the a has a little tail that you need to include.\n3.  That’s a feature of the Polish alphabet called an [ogonek](https://en.wikipedia.org/wiki/Ogonek).\n4.  You type it in LaTeX like so: `M\\k{a}dry`.\n5.  You get the error “Command \\\\k unavailable in encoding OT1”.\n6.  That’s because you need LaTeX to use a slightly different font package.\n7.  In your preamble, add `\\usepackage[T1]{fontenc}`.\n8.  You’re done.\n\nMy thanks to the Stack Exchange articles about [how to use that symbol](https://tex.stackexchange.com/questions/75396/how-to-use-polishhook-symbol) and [how to deal with the LaTeX error I got implementing that fix](https://tex.stackexchange.com/questions/392208/command-k-unavailable-in-encoding-ot1-error-takes-me-to-line-which-doesnt-eve).",
      "plaintextDescription": " 1. Type “Madry”.\n 2. Realize that the a has a little tail that you need to include.\n 3. That’s a feature of the Polish alphabet called an ogonek.\n 4. You type it in LaTeX like so: M\\k{a}dry.\n 5. You get the error “Command \\k unavailable in encoding OT1”.\n 6. That’s because you need LaTeX to use a slightly different font package.\n 7. In your preamble, add \\usepackage[T1]{fontenc}.\n 8. You’re done.\n\nMy thanks to the Stack Exchange articles about how to use that symbol and how to deal with the LaTeX error I got implementing that fix.",
      "wordCount": 96
    },
    "tags": [
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CdxdMLbBwHgEcGYCv",
    "title": "Aaron Silverbook on anti-cavity bacteria",
    "slug": "aaron-silverbook-on-anti-cavity-bacteria",
    "url": "https://youtu.be/tPyZt3EJCFM",
    "baseScore": 31,
    "voteCount": 11,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2023-11-20T03:06:19.524Z",
    "contents": {
      "markdown": "I recently released a podcast episode with Aaron Silverbook, a person within the LW-o-sphere, about his new start-up that produces a bacterium that might cure cavities, and also how cavities work and what's up with the bacteria we all have coating our teeth.\n\nHere are links to [Lumina Probiotic (the brand name of this new type of bacterium)](https://www.luminaprobiotic.com/) and [Lantern Bioworks (the company making the bacterium)](https://www.lanternbioworks.com/).",
      "plaintextDescription": "I recently released a podcast episode with Aaron Silverbook, a person within the LW-o-sphere, about his new start-up that produces a bacterium that might cure cavities, and also how cavities work and what's up with the bacteria we all have coating our teeth.\n\nHere are links to Lumina Probiotic (the brand name of this new type of bacterium) and Lantern Bioworks (the company making the bacterium).",
      "wordCount": 66
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "jaf5zfcGgCB2REXGw",
        "name": "Biology",
        "slug": "biology"
      },
      {
        "_id": "xHjy88N2uJvGdgzfw",
        "name": "Health / Medicine / Disease",
        "slug": "health-medicine-disease"
      },
      {
        "_id": "fkABsGCJZ6y9qConW",
        "name": "Practical",
        "slug": "practical"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "R56gihyfwERKDLszQ",
    "title": "If a little is good, is more better?",
    "slug": "if-a-little-is-good-is-more-better",
    "url": null,
    "baseScore": 25,
    "voteCount": 8,
    "viewCount": null,
    "commentCount": 16,
    "createdAt": null,
    "postedAt": "2023-11-04T07:10:05.943Z",
    "contents": {
      "markdown": "I’ve recently seen a bunch of discussions of the wisdom of publicly releasing the weights^[1](#fn:1)^ of advanced AI models. A common argument form that pops up in these discussions is this:\n\n1.  The problem with releasing weights is that it means that thing X can happen on a large scale, which causes bad effect Y.\n2.  But bad effect Y can already happen on a smaller scale because of Z.\n3.  Therefore, either it’s OK to release weights, or it’s not OK that Z is true.\n\nOne example of this argument form is about the potential to cause devastating pandemics, and goes as follows:\n\n1.  The putative problem with releasing the weights of Large Language Models (LLMs) is that it can help teach people a bunch of facts about virology, bacteriology, and biology more generally, that can teach people how to produce pathogens that cause devastating pandemics.\n2.  But we already have people paid to teach students about those topics.\n3.  Therefore, if that putative problem is enough to say that we shouldn’t release the weights of large language models, we should also not have textbooks and teachers on the topics of virology, bacteriology, and other relevant sub-topics of biology. But that’s absurd!\n\nIn this example, thing X is teaching people a bunch of facts, bad effect Y is creating devastating pandemics, and Z is the existence of teachers and textbooks.\n\nAnother example is one that I’m not sure has been publicly written up, but occurred to me:\n\n1.  Releasing the weights of LLMs is supposed to be bad because if people run the LLMs without supervision, they can do bad things.\n2.  But if you make LLMs in the first place, you can run them without supervision.\n3.  So if it’s bad to publicly release their weights, isn’t it also bad to make them in the first place?\n\nIn this example, thing X is running the model, bad effect Y is generic bad things that people worry about, and Z is the model existing in the first place.\n\nHowever, I think these arguments don’t actually work, because they implicitly assume that the costs and benefits scale proportionally to how much X happens. Suppose instead that the benefits of thing X grow proportionally to how much it happens^[2](#fn:2)^: for example, maybe every person who learns about biology makes roughly the same amount of incremental progress in learning how to cure disease and make humans healthier. Also suppose that every person who does thing X has a small probability of causing bad effect Y for everyone that negates all the benefits of X: for example, perhaps 0.01% of people would cause a global pandemic killing everyone if they learned enough about biology. Then, the expected value of X happening can be high when it happens a little (because you probably get the good effects and not the bad effects Y), but low when it happens a lot (because you almost certainly get bad effect Y, and the tiny probability of the good effects isn’t worth it). In this case, it makes sense that it might be fine that Z is true (e.g. that some people can learn various sub-topics of biology with great tutors), but bad to publicly release model weights to make X happen a ton.\n\nSo what’s the up-shot? To know whether it’s a good idea to publicly release model weights, you need to know the costs and benefits of various things that can happen, and how those scale with the user-base. It’s not enough to just point to a small amount of the relevant effects of releasing the weights and note that those are fine. I didn’t go thru this here, but you can also reverse the sign: it’s possible that there’s some activity that people can do with model weights that’s bad if a small number of people do it, but good if a large number of people do it: so you can’t necessarily just point to a small number of people doing nefarious things with some knowledge and conclude that it would be bad if that knowledge were widely publicized.\n\n1.  Basically, the parameters of these models. Once you know the parameters and how to put them together, you can run the model and do what you want with it. [↩](#fnref:1)\n    \n2.  Or more generally, polynomially (e.g. maybe quadratically because of Metcalfe’s law). [↩](#fnref:2)",
      "plaintextDescription": "I’ve recently seen a bunch of discussions of the wisdom of publicly releasing the weights1 of advanced AI models. A common argument form that pops up in these discussions is this:\n\n 1. The problem with releasing weights is that it means that thing X can happen on a large scale, which causes bad effect Y.\n 2. But bad effect Y can already happen on a smaller scale because of Z.\n 3. Therefore, either it’s OK to release weights, or it’s not OK that Z is true.\n\nOne example of this argument form is about the potential to cause devastating pandemics, and goes as follows:\n\n 1. The putative problem with releasing the weights of Large Language Models (LLMs) is that it can help teach people a bunch of facts about virology, bacteriology, and biology more generally, that can teach people how to produce pathogens that cause devastating pandemics.\n 2. But we already have people paid to teach students about those topics.\n 3. Therefore, if that putative problem is enough to say that we shouldn’t release the weights of large language models, we should also not have textbooks and teachers on the topics of virology, bacteriology, and other relevant sub-topics of biology. But that’s absurd!\n\nIn this example, thing X is teaching people a bunch of facts, bad effect Y is creating devastating pandemics, and Z is the existence of teachers and textbooks.\n\nAnother example is one that I’m not sure has been publicly written up, but occurred to me:\n\n 1. Releasing the weights of LLMs is supposed to be bad because if people run the LLMs without supervision, they can do bad things.\n 2. But if you make LLMs in the first place, you can run them without supervision.\n 3. So if it’s bad to publicly release their weights, isn’t it also bad to make them in the first place?\n\nIn this example, thing X is running the model, bad effect Y is generic bad things that people worry about, and Z is the model existing in the first place.\n\nHowever, I think these arguments don’t actually work, because they implicitly as",
      "wordCount": 728
    },
    "tags": [
      {
        "_id": "q6Euc7FAWJpDbst6r",
        "name": "Open Source AI",
        "slug": "open-source-ai"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "34RGz5Be7swJJEKho",
    "title": "On Frequentism and Bayesian Dogma",
    "slug": "on-frequentism-and-bayesian-dogma",
    "url": null,
    "baseScore": 59,
    "voteCount": 29,
    "viewCount": null,
    "commentCount": 27,
    "createdAt": null,
    "postedAt": "2023-10-15T22:23:10.747Z",
    "contents": {
      "markdown": "I've heard that you believe that frequentism is correct. But that's obviously wrong, so what gives?\n\nI guess first of all I should ask, what do you mean by \"frequentism\"?\n\nI mean classical statistical frequentism. Though somewhat tongue-in-cheek, as I don't think it's fully correct, I think it's much more correct than orthodox Jaynesian Bayesianism.  \n  \nSome scattered thoughts:\n\n*   Bayes' theorem derives from conditional probability so it's also included in frequentism.\n*   Bayesian epistemology only applies to situations when your beliefs are a probability distribution, and is thus incomplete.\n    *   It doesn't account for e.g. limited computation.\n*   Frequentism solves these things by framing the problem in a different way. Rather than 'how should I think?' it's \"this algorithm seems like a sensible way to think, let's figure out what epistemic guarantees it has\".\n    *   In particular, it makes it OK to believe things that are not expressible as probability distributions.\n\nI'm still sort of unsure what you mean by \"classical statistical frequentism\". Like, I'm pretty sure I agree that the purported theorems of Fisher are in fact theorems. Do you mean something like \"the way we should think about thinking is to ask 'what cognitive algorithms perform well with high probability in the long run'\"?\n\n(and regarding Bayesianism, I think that's a separate question that's more productively talked about once I understand why you think frequentism is good)\n\nSure. Thank you for the clarification -- I agree there are many fine theorems on both sides.  \n  \nStatistics is the problem of learning from data, frequentism is saying \"Here's an algorithm that takes the data and computes something (that's an estimator). Let's study the properties of it.\"\n\n> \"the way we should think about thinking is to ask 'what cognitive algorithms perform well with high probability in the long run'\"?\n\nYeah, I agree with this. Though I recognize in practice it's pretty hard, frequentist theory attempts to do so (for performance usually being 'have correct beliefs').\n\n> Here's an algorithm that takes the data and computes something (that's an estimator). Let's study the properties of it.\n\nI first want to note that this is an exhortation and not a proposition. Regarding the implicit exhortation of \"it's good to understand estimators\", I guess I agree? I think my crux is something like \"but it's relevant that some worlds are a priori more likely than others, and you want to do better in those\" (and of course now we're in the territory where we need to argue about Bayesianism).\n\n> I first want to note that this is an exhortation and not a proposition\n\nSure. I mean that the examples of knowledge frequentism creates (theorems and such) are things derived from this exhortation. E.g. \"consider the algorithm of taking the sample mean. What does that say about the true population mean?\" is an example of a very classical frequentism question with a useful answer.\n\n> but it's relevant that some worlds are a priori more likely than others\n\nSure, you can analyze whether your estimator does well in this way!\n\n> and of course now we're in the territory where we need to argue about Bayesianism).\n\nWhy do you say that?\n\nI guess you're referencing the part where Frequentism is like \"propositions are only true or false, you can't believe in probabilities\".  \n  \nFine. But you have the 'probabilities' be numbers in your estimator-algorithm. It is *true* that, at the end of the date, propositions are either true or false.  \n  \nIn fact outputting the 'Bayesian probability' is an estimator with good properties for estimating truth/falsehood of a proposition, with Brier loss or whatever. So that's a draw for freq vs Bayes.\n\nI guess I think of 'Frequentism' as definitely believing in probabilities - just probabilities of drawing different samples, rather than a priori probabilities, or probabilities of ground truths given a sample. So I feel that the question is which type of probability is more important, or more relevant. (Like, I certainly agree that \"understand what sort of algorithms do well in probabilistic settings\" is the right way to think about cognition, and you don't have to over-reify the features of those cognitive algorithms!)\n\nAnother potential question could be \"how valuable is it for cognitive algorithms to not be constrained by having their main internal representations be probability distributions over possible worlds\".\n\nBasically you presented this as a hot take, and I'm trying to figure out where you expect to disagree with people.\n\nAnother possible question: how valuable is the work produced by frequentist statisticians?\n\n> So I feel that the question is which type of probability is more important, or more relevant.\n\nI'm not sure I agree that this is the important question. Or rather it is, but I would answer it pragmatically: what sort of approaches to epistemology does focusing on each type of probability produce, what questions and answers does it lead you to produce? And I think here frequentism wins. This ties neatly with:\n\n> how valuable is the work produced by frequentist statisticians?\n\nHistorically pretty valuable! It's good to understand the guarantees of e.g. the 'sample mean' estimator. Bandit algorithms are also a glorious frequentist achievement, [as argued by Steinhardt](https://jsteinhardt.stat.berkeley.edu/blog/a-fervent-defense-of-frequentist-statistics). The Bootstrap method, a way to figure out your estimator's uncertainty without assuming much about the data (no Bayesian prior distribution for one) is also great.\n\nBut I think the theoretical pickings are pretty slim at this point -- cool stuff, but it's unlikely that there'll be something as fundamental as the sample mean.\n\nThe field to which statistics now is relevant is **machine learning**. and here I think frequentists have won an absolute victory: all the neural networks are probabilistic, but Bayesian ML needs way more computation than mainstream ML for the same or worse results.\n\nAnd IMO this is because of an overreliance on \"the theory says the algorithm will work if done this way, therefore we're going to do it this way\" versus a willingness to experiment with various algorithms (i.e. estimators) without quite understanding why they work, and seeing which one works.\n\n> how valuable is it for cognitive algorithms to not be constrained by having their main internal representations be probability distributions over possible worlds\n\nI think this is very valuable as exemplified by Bayesian vs mainstream ML.\n\nOK, I'm getting a better sense of where our disagreements may lie.  \n  \nI agree that the historical record of frequentist statistics is pretty decent. I am somewhat more enthusiastic about more \"Bayesian\" approaches to bandits, e.g. Thompson sampling, than it sounds like you are, but this might just be tribalism - and if I think about the learning algorithm products of the AI alignment community that I'm excited about (logical induction, boundedly rational inductive agents), they look more frequentist than Bayesian.  \n  \nI think my real gripe is that I see this massive impact of frequentism on the scientific method as promoting the use of p-values and confidence intervals, which, IMO, are using conditional probabilities in the wrong direction (one way to tell this: ask any normal scientist what a p-value or a confidence interval is, and there's a high chance that they'll give an explanation of what the Bayesian equivalent would be).  \n  \nNow, I think it's sort of fair enough to say \"but that's not what Ronald Fisher would do\" or \"people would and do misuse Bayesian methods too\", and all of these are right (as a side-note, I've noticed that when people introduce Bayes theorem in arguments about religion they're typically about to say something unhinged), but it's notable to me that tons of people seem to want the Bayesian thing.  \n  \n\\-\\-\\-  \n  \nRegarding Bayesian vs standard machine learning: on the one hand, I share your impression that the Bayesian methods are terrible and don't work, and that empiricism / tight feedback loops are important for making progress. On the other hand, as far as I can tell, the ML community is on track to build things that kill me and everyone I care about and also everyone else, and I kind of chalk this up to them not understanding enough about the generalization properties of their algorithms. So I actually don't take this as the win for frequentism that it looks like.\n\n> it's notable to me that tons of people seem to want the Bayesian thing.\n\nI agree that Bayesian statistics are more intuitive than p-values. It's sad in my opinion that you need to assume prior probabilities about your hypotheses to get the Bayesian-style p(hypothesis | data), which is what we all love. But the math comes out that way.\n\nMaybe also log-likelihood ratios would be better reported in papers (you can add them up!) but then people would add up log-likelihood ratios for slightly different hypotheses and convince them that they're valid (it can be, but it's unclear what assumptions you need to do that), and it would be a huge mess. That's not your strongest point though.\n\n> On the other hand, as far as I can tell, the ML community is on track to build things that kill me and everyone I care about and also everyone else\n\nNow we're talking!\n\n> I kind of chalk this up to them not understanding enough about the generalization properties of their algorithms\n\nFair, but that doesn't mean you can chalk it up to frequentism. I don't think the Bayesian approach (here I very much mean the actual Bayesian ML community[^lv7xp9lcyah]) is any better at this. They work kind of backwards: instead of fitting their theory to observable data about experiments, they assume Bayesian theory and kind of shrug when the experiments don't work out. IMO the right way to understand generalization is to have a theory, and *then change it when the experiment contradicts it.*\n\nPart of the reason this is justifiable to the Bayesian ML folks is that the experiments aren't *quite* about Bayesian theoretical ideal, they're about practical algorithms. My position here is that I would like my theories to talk about the actual things people do. I am wary of theorems about asymptotics for the same reason: technically they don't talk about what happens in finite time.\n\nIn my opinion we should discard the culture of this particular academic sub-field, and talk about how good would the *best possible* Bayesian approach to understanding ML generalization be. Two versions of this:\n\n1.  Understand existing algorithms. I claim that its fixation on having the only valid beliefs be well-specified probability distribution, and the lack of claims about what happens in any finite time, would make it impossible for them to make progress. Though maybe the dev-interp people will succeed (I doubt it, but we'll see; and they're studying MCMC's behavior in practice so not quite Bayesian).\n2.  Create Bayesian algorithms that are therefore well understood. This is the holy grail of Bayesian ML, but I don't think this will happen. Maintaining beliefs as probabilities that are always internally self-consistent is expensive and not always necessary, and also IMO not all beliefs are representable as probability distributions (radical uncertainty). Also you need a better understanding of good reasoning under finite computation which, as you wrote above, is more frequentist. (I agree with this point, and I think it's frequentist because frequentism is about analyzing estimators).  \n      \n     \n\n[^lv7xp9lcyah]: Examples of people who made this error: myself from 6 years ago, myself from 3 years ago. I would argue many of my grad student peers and professors made (and yet make) the same mistake. Yes, this formative experience is an important contributor to the existence of this dialogue. \n\n> Part of the reason this is justifiable to the Bayesian ML folks is that the experiments aren't *quite* about Bayesian theoretical ideal, they're about practical algorithms. My position here is that I would like my theories to talk about the actual things people do. \n\nI think this suggests a place where I have some tension with your view: while I certainly agree that theories should be about the things people actually do, and that Bayesianism can fall short on this score, I also want theories to meaningfully guide what people do! Cognitive algorithms can be better and worse, and we should use (and analyze) the better ones, rather than the worse ones. One way of implementing this could be \"try a bunch of cognitive algorithms and see what works\", but once your algorithms include \"play nice while you're being tested then take over the world\", empiricism isn't enough: we either need theory to guide us away from those algorithms, or we need to investigate the internals of the algorithms that we try, and make sure they comply with certain standards that rule out treacherous turn behaviour.  \n  \nNow, this theory of what algorithms should look like or what they should have in their internals doesn't have to be Bayesianism - in fact, it probably doesn't work for it to be Bayesianism, because to understand a Bayesian you need to understand their event space, which could be weird and inscrutable. But once you've got such a theory, I think you're at least outside of the domain of \"mere frequentism\" (altho I admit that in some sense any time you think about how an algorithm works in a probabilistic setting you're in some sense a frequentist).\n\nAs a side note:\n\n> Also you need a better understanding of good reasoning under finite computation which, as you wrote above, is more frequentist.\n\nThis might be an annoying definitional thing, but I don't think good reasoning under finite computation has to be 'frequentist'. As an extreme example, I wouldn't call Bayes net algorithms frequentist, even tho with finite size they run in finite time. I call logical induction and boundedly rational inductive agents 'frequentist' because they fall into the family of \"have a ton of 'experts' and play them off against each other\" (and crucially, don't constrain those experts to be 'rational' according to some a priori theory of good reasoning).\n\nGood point. True Bayesian algos are only finite if the world is finite though; and the world is too large to count as finite for the purposes of a competent AGI. I should have said \"with computation bounded under what the requirements of the world are\", or something similar but less unwieldy.\n\n> Now, this theory of what algorithms should look like or what they should have in their internals doesn't have to be Bayesianism - in fact, it probably doesn't work for it to be Bayesianism, because to understand a Bayesian you need to understand their event space, which could be weird and inscrutable. But once you've got such a theory, I think you're at least outside of the domain of \"mere frequentism\" (altho I admit that in some sense any time you think about how an algorithm works in a probabilistic setting you're in some sense a frequentist).\n\nI agree with all of this. I call this \"Bayesianism is wrong and frequentism is correct\", maybe I shouldn't call it that?\n\nWell, I was more thinking of Bayesianism as being insufficient for purpose, rather than necessarily \"wrong\" here.\n\nI feel like we've transformed the initial dispute into a new, clearer, and more exciting dispute. Perhaps this is a good place to stop?\n\nI'm not sure we agree on what the new dispute is, I'd like to explore that! But perhaps the place for that is another dialogue.\n\nI would say Bayesianism is wrong like Newtonian mechanics is wrong. It's a very good approximation of reality for some domains (in Newtonian mechanics' case, macroscopic objects at low energy scales, in Bayesian statistics' case, epistemic problems with at most ~millions of possible outcomes).\n\nThe frequentist frame I exposed here (let's analyze some actual algorithms) is IMO more likely to point at the kind of thing we want out of a theory of epistemology. But I guess classical frequentist methods are also not close to solving alignment, and also didn't accurately predict that deep NNs would work so well (they have so many parameters, you're going to overfit!)\n\nSo maybe frequentism is wrong in the same way. But I think the shift from \"the theory is done and should guide algorithms\" to \"the theory should explain what's going on in actual algorithms\" is important.\n\nMaybe we should write a consensus statement to conclude?\n\nI guess we have a few disagreements left...\n\n> I would say Bayesianism is wrong like Newtonian mechanics is wrong. It's a very good approximation of reality for some domains\n\nI wouldn't think about Bayesianism this way - I'd say that Bayesianism is the best you can do when you're not subject to computational / provability / self-reflection limitations, and when you are subject to those limitations, you should think about how you can get what's good about Bayesianism for less of the cost.\n\n> But I think the shift from \"the theory is done and should guide algorithms\" to \"the theory should explain what's going on in actual algorithms\" is important.\n\nThis still feels incomplete to me for reasons described in my earlier comment: Yes, it's bad to be dogmatic about theories that aren't quite right, and yes, theories have got to describe reality somehow, but also, theories should guide you into doing good things rather than bad things!\n\nHow about this as a consensus statement?  \n \n\n> Frequentism has the virtue of describing the performance of algorithms that are possible to run, without being overly dogmatic about what algorithms must look like. By contrast, Bayesianism is only strictly applicable in cases where computation is not limited, and its insistence on limiting focus to algorithms that carry around probability distributions that they update using likelihood ratios is overly limiting. In future, we need to develop ways of thinking about cognitive algorithms that describe real algorithms that can actually be run, while also providing useful guidance.\n\n>  I'd say that Bayesianism is the best you can do when you're not subject to computational / provability / self-reflection limitations, \n\nI disagree with this, by the way. Even under these assumptions, you still have the problem of handling belief states which cannot be described as a probability distribution. For small state spaces, being fast and loose with that (e.g. just belief the uniform distribution over everything) is fine, but larger state spaces run into problems, even if you have infinite compute and can prove everything and don't need to have self-knowledge.\n\nI endorse the consensus statement you wrote!\n\nAnd perhaps a remaining point of dispute is: how important is it to have non-probabilistic beliefs?\n\nSure, I'm happy to leave it at that. Thank you for being a thoughtful dialogue partner!\n\nThanks for the fun chat :)",
      "plaintextDescription": "I've heard that you believe that frequentism is correct. But that's obviously wrong, so what gives?\n\nDanielFilan\nI guess first of all I should ask, what do you mean by \"frequentism\"?\n\nDanielFilan\nI mean classical statistical frequentism. Though somewhat tongue-in-cheek, as I don't think it's fully correct, I think it's much more correct than orthodox Jaynesian Bayesianism.\n\nSome scattered thoughts:\n\n * Bayes' theorem derives from conditional probability so it's also included in frequentism.\n * Bayesian epistemology only applies to situations when your beliefs are a probability distribution, and is thus incomplete.\n   * It doesn't account for e.g. limited computation.\n * Frequentism solves these things by framing the problem in a different way. Rather than 'how should I think?' it's \"this algorithm seems like a sensible way to think, let's figure out what epistemic guarantees it has\".\n   * In particular, it makes it OK to believe things that are not expressible as probability distributions.\n\nAdrià Garriga-alonso\nI'm still sort of unsure what you mean by \"classical statistical frequentism\". Like, I'm pretty sure I agree that the purported theorems of Fisher are in fact theorems. Do you mean something like \"the way we should think about thinking is to ask 'what cognitive algorithms perform well with high probability in the long run'\"?\n\nDanielFilan\n(and regarding Bayesianism, I think that's a separate question that's more productively talked about once I understand why you think frequentism is good)\n\nDanielFilan\nSure. Thank you for the clarification -- I agree there are many fine theorems on both sides.\n\nStatistics is the problem of learning from data, frequentism is saying \"Here's an algorithm that takes the data and computes something (that's an estimator). Let's study the properties of it.\"\n\n> \"the way we should think about thinking is to ask 'what cognitive algorithms perform well with high probability in the long run'\"?\n\nYeah, I agree with this. Though I recognize ",
      "wordCount": 1873
    },
    "tags": [
      {
        "_id": "bh7uxTTqmsQ8jZJdB",
        "name": "Probability & Statistics",
        "slug": "probability-and-statistics"
      },
      {
        "_id": "Ng8Gice9KNkncxqcj",
        "name": "Rationality",
        "slug": "rationality"
      },
      {
        "_id": "3uE2pXvbcnS9nnZRE",
        "name": "World Modeling",
        "slug": "world-modeling"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "CTh3JHBmEfHjE7WP5",
    "title": "AXRP Episode 25 - Cooperative AI with Caspar Oesterheld",
    "slug": "axrp-episode-25-cooperative-ai-with-caspar-oesterheld",
    "url": null,
    "baseScore": 43,
    "voteCount": 17,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-10-03T21:50:07.552Z",
    "contents": {
      "markdown": "[YouTube link](https://youtu.be/0JkaOAzDfgE)\n\nImagine a world where there are many powerful AI systems, working at cross purposes. You could suppose that different governments use AIs to manage their militaries, or simply that many powerful AIs have their own wills. At any rate, it seems valuable for them to be able to cooperatively work together and minimize pointless conflict. How do we ensure that AIs behave this way - and what do we need to learn about how rational agents interact to make that more clear? In this episode, I’ll be speaking with Caspar Oesterheld about some of his research on this very topic.\n\nTopics we discuss:\n\n*   [Cooperative AI](#cooperative-ai)\n    *   [… vs standard game theory](#coop-ai-v-gt)\n    *   [Do we need cooperative AI if we get alignment?](#why-coop-ai-if-alignment)\n    *   [Cooperative AI and agent foundations](#coop-ai-and-af)\n*   [A Theory of Bounded Inductive Rationality](#brias)\n    *   [Why it matters](#why-brias-matter)\n    *   [How the theory works](#how-brias-work)\n    *   [Relationship to logical induction](#rel-to-li)\n    *   [How fast does it converge?](#convergence-speed)\n    *   [Non-myopic bounded rational inductive agents?](#without-myopia)\n    *   [Relationship to game theory](#rel-to-gt)\n*   [Safe Pareto Improvements](#spis)\n    *   [What they try to solve](#why-spis)\n    *   [Alternative solutions](#alternatives-to-spis)\n    *   [How safe Pareto improvements work](#how-spis-work)\n    *   [Will players fight over which safe Pareto improvement to adopt?](#spi-fights)\n    *   [Relationship to program equilibrium](#rel-to-prog-eq)\n    *   [Do safe Pareto improvements break themselves?](#spis-unstable)\n*   [Similarity-based Cooperation](#sbc)\n    *   [Are similarity-based cooperators overly cliqueish?](#sbc-cliques)\n    *   [Sensitivity to noise](#sensitivity-to-noise)\n    *   [Training neural nets to do similarity-based cooperation](#sbc-nns)\n*   [FOCAL, Caspar’s research lab](#focal)\n*   [How the papers all relate](#how-papers-relate)\n*   [Relationship to functional decision theory](#rel-to-fdt)\n*   [Following Caspar’s research](#following-caspar)\n\n**Daniel Filan:** Hello, everybody. In this episode, I’ll be speaking with Caspar Oesterheld. Caspar is a PhD student at Carnegie Mellon where he’s studying with [Vincent Conitzer](https://www.cs.cmu.edu/~conitzer/). He’s also the assistant director of the [Foundations of Cooperative AI Lab](http://www.cs.cmu.edu/~focal/) or FOCAL. For links to what we’re discussing, you can check the description of this episode and you can read the transcript at axrp.net. All right. So welcome to the podcast, Caspar.\n\n**Caspar Oesterheld:** Thanks. Happy to be on.\n\nCooperative AI\n--------------\n\n**Daniel Filan:** Yeah. So my impression is that the thing tying together the various strands of research you’re involved in is something roughly along the lines of cooperative AI. Is that fair to say?\n\n**Caspar Oesterheld:** I think that’s fair to say. I do some work other than cooperative AI and cooperative AI can, I guess, mean many things to different people. But generally, I’m happy with that characterization.\n\n**Daniel Filan:** Okay. So I guess to the extent that cooperative AI covers most of your work, what does that mean to you?\n\n**Caspar Oesterheld:** So to me, the most central problem of cooperative AI is a situation where two different human parties, like two companies or governments, each builds their own AI system and then these two AI systems potentially – while still interacting with their creators – these two AI systems interact with each other in some kind of general-sum, as game theorists would say, mixed-motive setting, where there are opportunities for cooperation but also perhaps a potential for conflict. And cooperative AI as I view it, or the most central cooperative AI setting or question, is how to make these kinds of interactions go well.\n\n**Daniel Filan:** Okay. So I guess this is the AI X-risk Research Podcast, and I also at least perceive you as being part of this x-risk research community. I think if you just say that, many people think, “Okay, is this a matter of life or death, or is this just it would be nice to have a little bit more [kumbaya](https://en.wikipedia.org/wiki/Kumbaya) going on?” So how relevant to x-risk is cooperative AI?\n\n**Caspar Oesterheld:** Yeah, I certainly view it as relevant to x-risk. Certainly that’s most of my motivation for working on this. So I suppose that there are different kinds of interactions between these different AI systems that one can think about, and I guess some of them aren’t so high stakes and it’s more about just having some more kumbaya. And meanwhile, other interactions might be very high stakes. Like if governments make their decisions in part by using AI systems, then the conflicts between governments could be a pretty big deal, and I don’t know, could pose an existential risk.\n\n**Daniel Filan:** Can you flesh that out? What would an example be of governments with AIs having some sort of mixed-sum interaction where one of the plausible outcomes is doom, basically?\n\n**Caspar Oesterheld:** So I suppose the most straightforward example would just be to take an interaction that already exists between governments and then say, “Well, you could have this, and well, also, the decisions are made in part between AI.” So I suppose there are various disputes between different countries, obviously different governments. Usually it’s over territory, I suppose. And sometimes as part of these disputes, countries are considering the use of nuclear weapons or threatening to use nuclear weapons or something like that.\n\nSo I guess the current, maybe most salient scenario is that the US and Russia disagree over what should happen with Ukraine, whether it should be its own country or to what extent it should be able to make various decisions about whether to join NATO or the EU or whatever. And as part of that, Russia has brought up or Putin has brought up the possibility of using nuclear weapons. I tend to think that \\[in\\] this particular scenario it’s not that likely that nuclear weapons would be used, but in the past, during the Cuban missile crisis or whatever, it seemed more plausible. And I suppose the most straightforward answer is just, well, we could have exactly these kinds of conflicts, just also with AI making some of the decisions.\n\n**Daniel Filan:** Okay. And so how does AI change the picture? Why aren’t you just studying cooperative game theory or something?\n\n**Caspar Oesterheld:** Yeah, good question. Okay. AI might introduce its own new cooperation problems. So you could have these AI arms races where maybe even once one has the first human-level AI systems or something like that, there might still be a race between different countries to improve these systems as fast as possible and perhaps take some risks in terms of building underlying systems in order to have the best systems. So that would introduce some new settings, but mostly I guess what I think about is just that there are game -theoretic dynamics or game-theoretic questions that are somewhat specific to AI. So for example, how to train AI systems to learn good equilibria or something like that. It’s an example that’s very natural to ask in the context of building machine learning systems and a bit less of a natural question if we think of humans who already have some sense of how to play in these strategic situations.\n\n### Cooperative AI vs standard game theory\n\n**Daniel Filan:** Okay. A related question is: when I look at cooperative AI literature, it seems like it’s usually taking game theory and tweaking it or applying it in a different situation. And I think game theory… there are a bunch of problems with it that made me think that it’s a little bit overrated. I have this list. So at least according to Daniel Ellsberg, apparently it wasn’t particularly used by the US in order to figure out how to do nuclear war, which is relevant because that’s why they invented it. It’s not obviously super predictive. You often have these multiple equilibria problems where if there are multiple equilibria and game theory says you play in equilibrium, that limits the predictive power. It’s computationally hard to find these, especially if your solution concept is Nash equilibrium. It’s hard to figure out how to get to an equilibrium. It seems like there’s important things that it doesn’t model like Schelling or really salient options. I don’t know. It seems very simplified and it also seems difficult to account for communication in a really nice, satisfactory way in the game theory land.\n\nSo I guess I’m wondering to what extent is cooperative AI fixing the problems I’m going to have with game theory versus inheriting them?\n\n**Caspar Oesterheld:** Yeah, that’s a good question. I think there are a number of ways in which it tries to fix them. So for example with respect to equilibrium selection, that’s a problem that I think about very explicitly. And then also in the cooperative AI setting or in the setting of building learning agents, it comes up in a more direct way. So with humans, if you have this descriptive attitude that you use game theory to predict how humans behave, you can just say, “Well, they’ll play Nash equilibria.” “Well, which ones?” “Well, anything could happen, it depends on what the humans decide to do.” And with AI, you’re forced a bit more into this prescriptive position of having to actually come up with ways to decide what to actually do. So you can’t just say, “Well, you can do this. You can do that. It’s up to you.” At some point you’ll have to say, “Okay, we want to use this learning scheme,” or something like that.\n\nAnd I guess there are also some other things where with some of your complaints about game theory, I would imagine that lots of people, including game theorists, would agree that these are problems, but where they just seem fundamentally quite difficult. So this problem of people going for different equilibria, like the equilibrium selection problem for example: I think part of why there isn’t that much work on it or why people generally don’t work on this that much is that it seems intractable. It seems very difficult to say much more than, “Well, there are different equilibria depending on blah, blah, blah. Different things might happen.” It seems very difficult to go beyond that. And I think similarly with Schelling points, these natural coordination points or focal points as they’re sometimes called…I’m actually quite interested in that topic, but I guess there too, I would imagine that lots of game theorists would say, “Yeah, this is… just hard to say anything about it, basically.” That’s why less has been said about those.\n\nAnd I think this computational hardness issue, that Nash equilibria are hard to find so people won’t actually play Nash equilibrium, they’ll do something else that’s probably a bit in the spirit of Nash equilibrium. Still they’ll try to best respond to what their opponent is doing or something like that, but probably they won’t exactly get it right. I think there the issue too is that describing these kinds of dynamics is just much, much harder than the simple Nash equilibrium model. And perhaps game theorists would, and perhaps I also would, think that it’s still useful to think about Nash equilibrium, and maybe that’s something you would disagree with or you would have a somewhat different view about.\n\n**Daniel Filan:** Well, I certainly think it’s useful to think about. I’m certainly sympathetic to the idea “well, these are hard to study so we’re studying easier problems.” But I just want to check… Should I just believe this stuff or is it simplifying things in order to make any headway? I guess one question I had is: so you mentioned for a few of these things, it’s just hard to study. And a natural question is: do things get easier or harder to study in the cooperative AI setting where you have a few more tools available at your disposal?\n\n**Caspar Oesterheld:** I think it’s a mixed bag. I think some things get harder because, especially if we’re thinking about future AI systems, we have less direct access to how they’re thinking about things. For example, Schelling points and equilibrium selection. If I ask you, I don’t know… Well, when we walk into each other, when we walk towards each other on the road and we have to decide who goes right and who goes left or whether to go to the right or to go to the left, how do we decide? There’s a very simple answer that we know because we’re in the US or something. And in the US, there’s right-hand traffic so we’re more likely to go to the right.\n\n**Daniel Filan:** That rule fails way more than I think I would’ve guessed naively. As far as I can tell, the actual solution is people randomly guess one direction and then if you collide, you switch with an ever-decreasing probability and that seems to pan out somehow.\n\n**Caspar Oesterheld:** Which, I think is the optimal symmetric solution is to randomize uniformly until you anti-coordinate successfully.\n\n**Daniel Filan:** Yeah. Although I think there’s more alternation than just repeated randomizing uniformly. So, I don’t know. I feel like more than average, I end up in these situations where we’re on the same side and then we both switch to the same side and then we both switch to the same side… You know what I mean? I think there’s a bias towards alternation rather than just randomization. This is a little bit off-topic though. Or maybe it doesn’t-\n\n**Caspar Oesterheld:** It seems plausibly true. But even so, even if what people are doing is irrational, I don’t know, we have some kind of intuition for what one does. I guess you have. I didn’t. I think you’re basically right about what people are doing. But you just think, “Okay. Well, we do this and that’s how we solve this.” And I don’t know, maybe we can come up with other examples where it’s more straightforward that people just follow some convention and it’s clear that people are following this convention because there is this convention. And so with AI, it’s harder to guess, especially if you imagine AI systems that haven’t learned to play against each other, face each other for the first time on some problem with equilibrium selection. It seems harder to say what they’re going to do.\n\nOn the other hand, there are also some things that I think are easier to reason about. Maybe the rational agent model is more accurate about the kind of AI systems that we worry about than about humans. Also, if we think more from this prescriptive perspective of trying to figure out how to make things go well, then there are a lot of tricks that apply to AI more than to humans. So I guess we’ll talk a bit about some of my work later that’s in part about some of these things that seem to apply more to AI than to humans. For example, it’s easier to imagine very credible commitments because the AI can be given any goals, we can, I don’t know, change its source code in various ways. Whereas with humans that’s harder to do.\n\nAnother way in which the situation might be harder for AI is that with humans - this is related to what we already talked about - humans have already trained a bunch against each other. Probably some kind of group selection type or selection on a convention level has occurred. So successful conventions or successful ways of selecting equilibria have been selected \\[over\\] conventions that fail somehow. And this more evolutionary-style learning seems less realistic for AI at least. It’s very different from gradient descent or other contemporary techniques.\n\n**Daniel Filan:** It seems like there’s this general issue where on the one hand, the AIs can do more stuff. They can be given various types of source code, they can read other AIs’ source codes… On the one hand, that gives you more tools to solve problems, but then on the other hand presumably it just adds to the strategic complexity. So I guess a priori, it should be unclear if that makes things easier or harder.\n\n**Caspar Oesterheld:** Yeah. Even some of the things that I described on the “making things easier” side, more opportunities to get good equilibria often also imply just more equilibria, which typically makes things harder.\n\n**Daniel Filan:** I wonder: if I just think about this, a lot of these traits of AI can sort of be implemented for humans. I think in [one of the papers](https://link.springer.com/article/10.1007/s10458-022-09574-6) we’re going to be talking about, about safe Pareto improvements, it’s basically about the setting where you give an AI the task to solve a game but you can change the game a little bit. And presumably, we could do that with humans. Your company has to play some game and it just outsources it to the game-playing department of your company and maybe your company can give instructions to the game-playing department. I’m wondering, have these sorts of questions been studied much in the setting of humans, or do you think AI is jolting people to think thoughts that they could have in principle thought earlier?\n\n**Caspar Oesterheld:** Yeah. So with some of these, that’s definitely the case. Some forms of credible commitment are discussed in game theory more generally. And I also agree that this transparent source code or partially transparent source code type setting is not that bad as a model of some interactions between humans. Organizations, for example: human organizations are somewhat transparent. To some extent, we can predict what US Congress will do. It’s not some black box that has some utility function, that it’s going to try to best respond to something with respect to this utility functions. They have lots of rules for what they’re going to do. We can ask the individual congresspeople what they feel about certain issues. So to some extent, human organizations are a bit like, I don’t know, they have specified rules or constitution and so on. So in some sense, they also play this open source type game, or this game where your source code is somewhat transparent.\n\nI think with a lot of these things, part of the reason why they aren’t studied in this way is that, for example, with this open source type model, traditionally that is, it’s just that it’s a bit less of a natural setting and it’s clear that the human setting is just very fuzzy in some sense. And I think the AI setting will actually also be very fuzzy in a similar sense, but it’s easier to imagine this extreme case of being able to read one another’s source code or something like that. Whereas for human organizations, it’s less natural to imagine this extreme case or this particular model of source code. But yeah, I definitely agree that some of these things one can consider in the human context as well.\n\n**Daniel Filan:** Yeah. And it’s close to models where players have types and you can observe other types and everyone of a type plays the same, right?\n\n**Caspar Oesterheld:** Yeah, right. There’s also that kind of model.\n\n### Do we need cooperative AI if we get alignment?\n\n**Daniel Filan:** Yeah. So the final framing question I want to ask is: I think when a lot of people encounter this line of research, they think, “Okay. Well, we already have AI alignment of, AI is trying to adopt ‘do what people want them to do’.” If I’m, I don’t know, Boeing or the US or something, and I’m making AI that’s aligned with me, I want it to be able to figure out how to play games such that we don’t end up in some terrible equilibrium where the world gets nuked anyway. So I think a lot of people have this intuition, “Well, we should just make really smart AI that does what we want and then just ask it to solve all of these cooperative AI questions.” And I’m wondering: what do you think about that plan or that intuition?\n\n**Caspar Oesterheld:** Okay. I think there are multiple questions here that I think are all very important to ask and that to some extent I also think are important objections to a lot of work that one might do. I guess one version of this is that if we build aligned AI, whatever problems we have, we can just ask the AI to solve those problems. And if the AI is just good at doing research on problems in general because it’s generally intelligent or something like that, then we should expect it to also be able to solve any particular problem that we are concerned about.\n\n**Daniel Filan:** Or there’s a weaker claim you could make, which is it can solve those problems as well as we could solve them if we tried.\n\n**Caspar Oesterheld:** Right, yeah.\n\n**Daniel Filan:** Sorry, I interrupted.\n\n**Caspar Oesterheld:** No, that’s a good point. Obviously, that’s the more important claim. And I think that is an important consideration for cooperative AI but also for a lot of other types of research that one might naively think it is valuable to do. I think there are lots of specific things where I think this really works extremely well. If you think about solving some specific computational problem, like developing better algorithms for finding the good Nash equilibria in a three-player, normal-form game or something like that, it seems very likely that if we get, or once we get human-level AI or superhuman level AI, it will just be better at developing these algorithms, or at least as good as humans at developing these algorithms. Because to some extent, it seems like a fairly generic task.\n\nI would imagine that an AI that’s generally good at solving technical problems will be good at this particular problem. What I think is an important property of the kinds of problems that I usually think about or the kinds of ideas that I usually think about is that they’re much less well-defined technical problems and they’re much more conceptual. I guess we’ll be talking about some of these things later, but they seem much less like the kinds of problems where you just specify some issue to the AI system and then it just gives the correct answer.\n\nAnd I also think that, and this goes maybe more towards a different version of this objection, but I think another important claim here is that in some sense, game theory itself or strategic interaction between multiple agents is a special thing in a way, in a way that’s similar to how alignment or having the kinds of values that we want it to have is special in that it’s to some extent orthogonal to other kinds of capabilities that one would naturally train a system to have. So in particular, I think you can be a very capable agent and still land in bad equilibria in some sense. A bad Nash equilibrium is an equilibrium where everyone behaves perfectly rational, holding fixed \\[what\\] their opponent \\[does\\], but the outcome is still bad. And so at least if you imagine that training mostly consists of making agents good at responding to their environment, then landing in a bad Nash equilibrium is completely compatible with being super competent at best responding to the environment.\n\n**Daniel Filan:** Yeah. I guess maybe this goes to some underlying intuition of game theory being a deficient abstraction because, I don’t know, in a lot of these situations, in a lot of these difficult game theory things. Like the [prisoner’s dilemma](https://en.wikipedia.org/wiki/Prisoner%27s_dilemma), where it’s better for us if we cooperate, but whatever the other person does, I’d rather defect, but if we both defect, that’s worse than if we both cooperate: a lot of the time I’m like, “Well, we should just talk it out”; or we should just build an enforcement mechanism and just change the game to one where it actually does work. And maybe this just ends up as… If these options are available to powerful AIs then somehow the game theory aspect maybe or the multiple Nash equilibria thing is maybe less relevant. So I don’t know, maybe this comes down to a rejection of the game theory frame that one might have.\n\n**Caspar Oesterheld:** I’m not sure I understand this particular objection. By default, the normal prisoner’s dilemma, single shot without repetition, without being able to set up enforcement mechanisms, make incredible commitments and these kinds of things… it just has one Nash equilibrium, right?\n\n**Daniel Filan:** Yeah. I think the complaint is: in real life, we just are going to be able to communicate and set up enforcement mechanisms and stuff.\n\n**Caspar Oesterheld:** Yeah, I think I mostly agree with that. But the enforcement mechanisms are what would in the prisoner’s dilemma introduce multiple Nash equilibria, right?\n\n**Daniel Filan:** Yeah.\n\n**Caspar Oesterheld:** I guess in the prisoner’s dilemma, it’s natural what the good equilibrium is because the game is symmetric so you just could do the Pareto optimal symmetric equilibrium. So assuming you can make a cooperate-cooperate Nash equilibrium by playing the game repeatedly or having enforcement mechanisms and so on - then it seems very natural that that’s what you would do. But if we ignore the symmetry intuition or we just take some game that is somewhat more asymmetric, then it seems like even with these enforcement mechanisms, there might be many possible equilibria that are differently good for different players. And yeah, I don’t see how you would avoid the multiple Nash equilibria issue in that kind of setting.\n\n**Daniel Filan:** Yeah. I guess the hope is that you avoid the issue of really bad Nash equilibria. But I guess you still have this to the degree that there’s some asymmetry or anti-symmetry or something. I guess you have this issue of there being multiple equilibria that you’ve got to bargain between. If we both have to coordinate to do a thing and I like one of the things more and you like one of the things more and somehow we can’t randomize or something, I guess in that situation we’ve got this bargaining game of “okay, are we all going to have this rule that says we do the thing you like? Or are we all going to have this rule that says we do the thing I like?” And it ends up being a metagame, I guess.\n\n**Caspar Oesterheld:** Yeah, I think I’m more worried about the multiple equilibrium thing than about getting the good equilibrium if there’s a unique good equilibrium. If you think of something like the [stag hunt](https://en.wikipedia.org/wiki/Stag_hunt) or a prisoner’s dilemma where all you really need to do is, I don’t know, pay 10 cents to set up this commitment mechanism that allows you to play cooperate-cooperate in equilibrium, I’m also much more optimistic about that. It’s still funny because in some sense there’s not really that great of a theory for why the good thing should happen, but it seems intuitive that it would happen.\n\nAnd I think maybe this is also one of the… one way in which game theory is weird that even this very basic thing where it feels like you can just talk it out and say like, “Yeah, we are going to do this good thing,” and so on. Game theory doesn’t really have a model for this “talking it out” thing to get the good equilibrium. Really, that doesn’t seem that difficult to do. But yeah, the case that I’m much more worried about is the case where you have lots of different equilibria and the game is very asymmetric and so it’s just completely unclear which of the equilibria to go for.\n\n### Cooperative AI and agent foundations\n\n**Daniel Filan:** Okay. I said that was my final question. But I actually have a final bridging question before I get to the next thing we’re going to talk about, which is: I think a lot of cooperative AI stuff, it often seems adjacent to this [agent foundations](https://www.alignmentforum.org/posts/FWvzwCDRgcjb9sigb/why-agent-foundations-an-overly-abstract-explanation) line of work where people are really interested in: you’re an AI and there are other players in the world and they’re modeling you and you’re modeling them, but you can’t perfectly model something that’s perfectly modeling you because your brain can’t properly fit inside your brain. And people in this world are often interested in different decision theories or ways of thinking about how to be uncertain when you’re computationally limited. A lot of this kind of thing seems like it shows up in the cooperative AI literature, I guess specifically [your research group’s](https://www.cs.cmu.edu/~focal/) cooperative AI literature. I’m wondering: why do you think that overlap is there and what do you think are the most important reasons driving that overlap?\n\n**Caspar Oesterheld:** Yeah, that’s another very interesting question. I should start by saying that there are also lots of people who work on cooperative AI and who aren’t thinking that much about these more foundational questions. I don’t know, \\[they’re\\] more on the side of developing machine learning algorithms or setups for machine learning such that different machine learning agents just empirically are more likely to converge to equilibria or to better equilibria and things like that. But yeah, I agree that with respect to me in particular, there’s a lot of overlap. Okay, I think there are a couple of reasons. One reason is that it’s good to do things that are useful for multiple reasons. So if you can do something that’s useful both for analyzing the multi-agent stuff and also good for this more agents foundations perspective, that’s nice.\n\nI think the deeper reason for why there’s overlap has to do with a lot of just the object-level issues of game theory and how they interact with these things. For example, it seems that this issue of not being able to fully model your environment, it comes up very naturally in game theory because basically if you have two rational agents, there’s kind of no way to really assume that they can both perfectly have a model of the other player or something like that. And they can best respond to each other, but these kind of assumptions…\n\nIn the case of mere empirical uncertainty, you can at least theoretically make these kind of assumptions, that you have your Bayesian prior or something like that and you just do Bayesian updating and then you converge on the correct world model. But everyone knows that’s not actually how it works, but you can easily think about this, and so you just think about this even though actually the real systems that actually do stuff in the real world don’t really work this way, but you can have this model. Whereas in game theory, it’s much harder to avoid these kinds of issues.\n\n**Daniel Filan:** But there… I mean, if that was it, I would expect that there would be this whole field of game theory AI and they would all be interested in agent foundations stuff, but somehow it seems like the cooperative AI angle… I don’t know, maybe I just haven’t seen the anti-cooperative AI agent foundations work.\n\n**Caspar Oesterheld:** Yeah, so there is actually lots of work that is on these kind of foundational questions and that isn’t particularly motivated by foundations of cooperative AI. So there’s this whole “regret learning” literature, which is… It’s [a huge literature](https://tor-lattimore.com/downloads/book/book.pdf) about some theoretical model of learning in games (or learning in general, but you can in particular apply it to learning in games). So it’s a model of rationality. You can use it as a model of bounded rationality. And then there are [many](https://www.jstor.org/stable/2999445), [many](http://www.mit.edu/~gfarina/about/) papers about what happens if you have two regret learners play against each other, how quickly do they converge to what kind of solution concept? Do they converge to Nash equilibrium or coarse correlated equilibrium or whatever? So I think this literature basically just does exist.\n\n**Daniel Filan:** Okay.\n\n**Caspar Oesterheld:** I think most of this is a bit less motivated by AI, but yeah, there’s definitely a lot of foundational work on the intersection of what’s a rational agent, what should a rational agent do, or how should one learn stuff, and multi-agent interactions, that isn’t about cooperation.\n\nA Theory of Bounded Inductive Rationality\n-----------------------------------------\n\n**Daniel Filan:** Okay, cool. Well, I guess bridging from that, I guess I’d like to talk about your paper, [A Theory of Bounded Inductive Rationality](https://arxiv.org/abs/2307.05068). So this is by yourself, [Abram Demski](https://scholar.google.com/citations?user=tsrblo8AAAAJ&hl=en&oi=ao), and [Vincent Conitzer](https://www.cs.cmu.edu/~conitzer/). Could you give us a brief overview of what this paper is about?\n\n**Caspar Oesterheld:** Sure. So this paper considers a setting, which we for now might imagine is just a single agent setting where you have a single agent and it has to learn to make decisions. And the way it works is that every day, or I don’t know, at each time step, as you might say, it faces some set of available options and it can choose one of these options. So you might imagine, every day someone comes to you and puts five boxes on your table with different descriptions on them and then you have to pick one of them. And then once you pick a box, you get a reward for the box that you pick and you only observe that reward. You don’t observe any kind of counterfactuals. You don’t observe what would’ve happened had you taken another box. And in fact, it might not even be well-defined what would’ve happened, like what are these counterfactuals anyway? So all that happens, basically, you can choose between different things. You choose one of the things, you get a reward.\n\nAnd this happens every day or every time step. And now the question is: how should you learn in this kind of setting? How should you learn to make choices that maximize reward? And in particular, we consider a myopic setting, which means we just want to maximize the short-term reward, like the reward that we get right now with the next box that we pick. I don’t know, maybe at some point we figure out that whenever you take the blue box, maybe you get a high reward, but then for the next 1,000 times steps, the reward will be low. The basic setting that the paper considers is such that you then still want to take the blue box. You don’t care about this losing the reward on the next 1,000 days, though you can consider alternatives to this.\n\nOkay, so this is the basic setting of learning and this is very similar to this [bandit literature](https://tor-lattimore.com/downloads/book/book.pdf) where regret minimization is kind of the main criterion of rationality that people consider. So it’s very similar to that. The main difference is that we-\n\n**Daniel Filan:** Wait, before you go on, what is regret minimization?\n\n**Caspar Oesterheld:** Oh yeah, good question. I should explain that. So here’s one notion of what it means to do well in this type of scenario. I don’t know, let’s say you play this for 1,000 rounds and then in the end you ask yourself, “How much better could I have done had I done something else?” So for example, how much better could I have done had I only taken the - let’s say every day there’s a blue box - had I always just taken the blue box? And so the regret is kind of like: how much worse are you doing relative to the best thing you could have done in retrospect? And usually there are constraints on what the best thing in retrospect is: usually it’s just not achievable to have low regret relative to on each day picking the best thing.\n\nOkay, I’ll just introduce one more aspect of the setting because that’s also important for our paper. So for example, you could have some set of experts, that you might imagine is just some set of algorithms, and on each day each expert has a recommendation for what you should do. Then your regret might be, “How much worse did I do relative to the best expert?” So after 1,000 days, you look at each expert and you ask, “How much \\[better\\] would I have done had I chosen at each time step what this expert recommended?”\n\n**Daniel Filan:** And so my understanding is: the reason we’re saying something like this rather than just “in retrospect, the best thing you could have done ever” is, if the results are random or unknowable or something, in retrospect we could just pick the winning lottery numbers and stuff, but you don’t want to allow those sorts of strategies, is my understanding.\n\n**Caspar Oesterheld:** Yeah, exactly. Very good. Yeah, thanks for explaining this. Yeah, I think that is exactly the justification for doing that.\n\n**Daniel Filan:** Okay.\n\n**Caspar Oesterheld:** Okay, so minimizing regret means minimizing regret with respect to the best expert in retrospect. And the intuition here from a learning and rational agent perspective is that, I don’t know, you’re some agent, you have limited computational abilities, and let’s say that this set of experts is the set of algorithms that you can compute or something like that, or you can compute all simple strategies for what you should do. And then low regret means that there’s no… In this set, there’s no strategy that does much better than what you do. And in particular, usually what people talk about is sublinear regret, which means that as time goes to infinity, your average regret per round goes to zero, which in some sense means that in the limit you learn to do at least as well as the best expert.\n\nSo one very simple version of this is: imagine you play some game, you play chess or something like that, and there are 10 people in the room and they each make recommendations for what you should play, and you don’t know anything about chess. And now one of the people in the room is a chess grandmaster and the other ones are just random non-chess-grandmasters who are maybe much worse, or a bit worse. Then it seems like if you learn in a reasonable way, you should learn to do at least as well as the grandmaster, right? Because the least thing you can learn is just do whatever this grandmaster does, right?\n\n**Daniel Filan:** Yep.\n\n**Caspar Oesterheld:** That’s kind of the intuition here.\n\n**Daniel Filan:** But in your paper, you’re contrasting yourself with regret minimization.\n\n**Caspar Oesterheld:** Yeah.\n\n**Daniel Filan:** So what’s so bad about regret minimization? Isn’t that just equivalent to ‘do as well as you can’?\n\n**Caspar Oesterheld:** Yeah, I think regret minimization is certainly a compelling criterion. I think in the agent foundations, existential risk community, I think too few people are aware of regret minimization and how it’s a very simple criterion and so on. Okay, now I want to make the case against this. So why don’t I find this very satisfying? So it has to do with the way that regret minimization reasons about counterfactuals. So an immediate question that you might have about regret minimization is, “Is this even ever achievable?” Can you even ensure that you have low regret or sublinear regret? Because you might imagine a setting where the environment exactly computes what you do and always does the opposite, right?\n\nLike, on every day you have two boxes in front of you and one of them contains a dollar and the other one doesn’t. And the way the environment fills the boxes is that it predicts what you do and then it puts the dollar in the other box. And in this kind of setting you might imagine it seems really hard to achieve low regret here so it will be difficult to, in retrospect, not do worse than, for example, always taking the left box, right? Because let’s say you switch back and forth between the two, then in retrospect half the time the left box had the dollar. So you would’ve done better had you just always taken the left box.\n\n**Daniel Filan:** Yeah. For “if you would’ve done better”… somehow this is assuming that I could have just taken the left box, but the problem would’ve been the same as it actually was.\n\n**Caspar Oesterheld:** Exactly. Part of how this criterion works is that it assumes that the problem says… the specific instance of this multi-armed bandit problem (as it’s called) that you face consists in specifying at each time step, how much money or how much reward is in each of the boxes, and that this is in some sense independent of what you do. In particular the adversarial multi-armed bandit literature very explicitly allows the case where the way the boxes are filled happens to be exactly running whatever algorithm you run. So how is this solved? So the way this is solved is that the learner has to use randomization. So you have to randomize over which boxes you pick or which expert’s advice you follow.\n\n**Daniel Filan:** This is how it’s solved in the adversarial-\n\n**Caspar Oesterheld:** Yeah, in the adversarial setting. There are also non-adversarial settings where basically, I don’t know, you know from the start that each of the boxes on every day follows the same distribution, like some Gaussian with some mean. The whole task consists in doing optimal exploration to find out which box has the highest mean value. And there you don’t need randomization. I think in practice probably people still do randomization in these cases, but you definitely don’t need it.\n\nWhereas in this adversarial case, to achieve sublinear regret, you have to randomize and you have to also assume that the environment cannot predict the outcomes of your random coins. It’s like, I don’t know, if you flip a coin to decide whether to take the left or the right box, then you assume that the environment, it can also flip a coin, but it can only flip a different coin.\n\nSo I find that a bit dissatisfying philosophically, it seems a bit weird to assume this, but that maybe I can live with. I think the part that I really kind of don’t like is that in these problems where you randomize or where you need to randomize, regret minimization requires that you randomize, but the restrictions that it imposes are all on your actions rather than on the distributions over actions that you pick.\n\n**Daniel Filan:** How do you mean?\n\n**Caspar Oesterheld:** Yeah, so here’s an example, which is it’s basically [Newcomb’s problem](https://en.wikipedia.org/wiki/Newcomb%27s_paradox). So imagine that there are two boxes. The way the environment fills the boxes works as follows. It runs your algorithm to compute your probability of choosing the left box. And the higher this probability is of you choosing the left box, the more money it just puts in both boxes, but then also it puts an extra dollar into the right box. And we make it so that your probability of choosing the left box, it really, really strongly increases the reward of both boxes.\n\nSo you get, I don’t know, for each 1% of probability mass that you have on the left box, you get a million dollars or something like that put in both boxes. The numbers don’t need to be that extreme, I just don’t want to think about the exact numbers or how large they need to be. So in this problem, if this is how the problem works, then if you want to maximize your utility and you maximize over your probability distribution over boxes, then I think the reasonable thing to do is to always pick the left box. Because basically for each probability mass that you put on the left box, you gain lots of money. Whereas for each, you only gain $1 by moving probability mass from the left box to the right box.\n\n**Daniel Filan:** Yep. And you lose all the money you could have made by having probability mass on the left box and both boxes get money.\n\n**Caspar Oesterheld:** Yeah. So for example, if you always choose the right box, you only get a dollar. And if you always choose the left box, you get a hundred million dollars. Okay, so this is an example of a setting, and if you optimize over probability distributions, then you should choose the left box. This is not what regret minimization says you should do. Regret minimization would here imply that you have to always choose the right box, well, in the limit. You have to learn to always choose the right box. And the reason for that is that if you choose the left box, you’ll have regret, right? You’ll regret that you didn’t choose the right box because then you would’ve gotten a dollar more.\n\nAnd the reason why… that’s kind of what I said somewhat cryptically, in some sense the reason for why this issue occurs is that regret minimization is, it’s a criterion on which actions you take and it doesn’t care about how you randomize. It kind of doesn’t say anything about how you should randomize other than saying, “Well, you should randomize over the things that give you high reward holding fixed how you randomize,” or I don’t know, “We don’t care about how you randomize.” It says something about these actions and it doesn’t say how you should randomize. And I think this is just not so compelling in this particular problem. I think, in this particular problem, one should just always choose the left box.\n\n**Daniel Filan:** And is this also true in the setting of adversarial bandits?\n\n**Caspar Oesterheld:** Yes. Basically adversarial bandits allow specifically for this kind of problem. To get high reward on these, one has to do this randomization stuff where sometimes one has to learn to make the probability distribution so that one’s reward is low, just to make one’s regret also low.\n\n**Daniel Filan:** All right. So okay, that’s what’s wrong with other settings. So what’s your theory of bounded inductive rationality?\n\n### Why it matters\n\n**Caspar Oesterheld:** Can I say something about why I think this is even important?\n\n**Daniel Filan:** Oh, yeah: why is it important?\n\n**Caspar Oesterheld:** Okay, so this particular setting, this “left box, right box” example is farfetched, of course, but the reason why I think it’s important is that it’s this kind of setting where the environment tries to predict what you do and then respond to it in a particular way. To some extent, that’s kind of the core of game theory. And so if you want to use these regret minimizers specifically in a game theoretic context, I think it’s kind of weird to use them given that it’s very easy to come up with these game theory-flavored type of cases where they clearly give weird results. So in particular, I think sometimes people justify Nash equilibrium by appealing to these regret minimizers, so [you can show that if you have two regret minimizers and they play a game against each other, they converge to… I think it’s not exactly Nash equilibrium, it’s some form of correlated equilibrium.](https://www.jstor.org/stable/2999445)\n\nBut yeah, some equilibrium concept. (That is if they converge. Sometimes they also don’t converge.) But if they converge, they converge to some kind of Nash-like solution concept. And so you might say, “Well, it’s great. This really shows why Nash equilibrium is a good thing and why rational agents should play Nash equilibrium against each other.” In some sense, I think this Nashian idea is already kind of baked in in this regret minimization concept in a way that seems not so compelling, as this “left box right box” example shows. Yeah, so that’s why my theory… That’s why I’ve worked on this and tried to come up with a theory that reasons about randomization and things like that in a very different way.\n\n**Daniel Filan:** Okay. Can I try to paraphrase that? So we want a theory of how to be a rational agent and there are a few criteria that we want. Firstly, we want to be able to go from something like how do you make decisions normally, we want to be able to use that in the context of game theory as some sort of foundation. And secondly, we want to have it be bounded. So that’s why we’re sort of thinking of the regret minimization, some number of experts frame where we just want to consider all the ways of doing things we can fit in our head and not necessarily worry about everything. So we want to have a decision theory foundation for game theory and we want to be bounded. And because we want a decision theory foundation for game theory specifically, we want our decision theory method… We want our way of making decisions to allow for environments that are modeling us.\n\nAnd so basically your complaint is: well, normal regret minimization, it does the bounded thing, but it doesn’t do a very good job of thinking about the environment modeling you. There are various ways of thinking about game theory where you can think about the environment modeling you, but you’re not necessarily bounded. Probably [my favorite paper](https://arxiv.org/abs/1508.04145) in this line is the ‘reflective oracles’ line of work where– sadly, I can’t explain it right now, but if you assume you have something that’s a little bit less powerful than a halting oracle, but still more powerful than any computer that exists, then agents that just model their environment and make decisions, they end up playing Nash equilibria against each other. It’s a really cool line of research. I encourage people to read it, but no existing thing could possibly implement the thing that those papers are talking about. So you want to get all of these three criteria: boundedness, decision theory to game theory, and environments that are modeling the decision maker. Is that right?\n\n**Caspar Oesterheld:** Yes. That’s a very good summary. Thanks.\n\n### How the theory works\n\n**Daniel Filan:** All right, I now feel like I understand the point of this paper and how it’s related to your research agenda much better. Okay, so we’ve talked about what you want to be doing. How’s it work? What do you have?\n\n**Caspar Oesterheld:** Okay, so we have the same kind of setting as before. On each day, we choose between a set of options. And slightly different, we don’t require that these counterfactuals, which one talks about a lot in regret minimization, we don’t require that these are well-defined. And we also have these experts… I guess we call them hypotheses rather than experts, but they basically do the same thing except that in addition to making a recommendation at each time step, they also give estimates of the utility that they expect to get if the recommendation is implemented.\n\nSo if you, again, have this picture of: I’m trying to play chess and there are 10 people in the room, then one of them might say, “Okay, if you play Knight F3 then I think there’s a 60% chance that you’re going to win or you’re going to get 0.6 points in expectation,” or something like that. So they’re a bit more complicated than the experts, but only slightly; they give you this one additional estimate. And now, similar to regret minimization, we define a notion of rationality relative to this set of hypotheses. I think I want to describe it in terms of the algorithm rather than the criterion for now, because I think the algorithm is actually slightly more intuitive.\n\n**Daniel Filan:** So this is the opposite way than how you do it in [the paper](https://arxiv.org/abs/2307.05068) (for people who might read the paper).\n\n**Caspar Oesterheld:** Yeah, the paper just gives the criterion and then the algorithm is just kind of hidden in the appendix. I mean, it’s like some brief text in the main paper saying that, “Yeah, this is roughly how it works.” In some sense, of course, the general abstract criterion is much more… I think it’s more important than the algorithm itself. I think it’s a bit similar to [the logical induction paper](https://arxiv.org/abs/1609.03543) where they define similarly a kind of notion of rationality or having good beliefs or something like that. And yeah, they have this criterion, but they also have a specific algorithm, which is a bit like running a prediction market on logical claims. And I think… In practice, I think many more people have this idea in their minds of “just run a prediction market between algorithmic traders” than this specific criterion that they define. Even though I think from a theoretical perspective, the criterion is really the important thing and the algorithm is just some very specific construction.\n\n**Daniel Filan:** Yeah. I actually want to talk more about the relationship to the logical inductors paper later. So what’s your algorithm?\n\n**Caspar Oesterheld:** So basically the algorithm is to run an auction between these hypotheses. So first we need to give the hypotheses enough money to deal with. So they now have money, like some kind of virtual currency. We need to be a bit careful about how exactly we give them money initially. It’s especially tricky if we have infinitely many hypotheses, but basically we need to make sure that we eventually give all hypotheses enough money so that we can explore them, which has to be infinite money. But we also, if we give everyone $10 in the beginning, then this auction will just be chaos because there will be lots of crazy hypotheses that bid nonsense. So maybe it’s easiest to first consider the case of finitely many hypotheses, and let’s just say that in the beginning we gave each hypothesis 100 virtual dollars. And now we run auctions and the way do this specifically is that we just ask all the hypotheses for their recommendation and their estimate of what reward we can get if we follow the recommendation.\n\nThen we follow the highest bidder, so we take the highest bidder. But the hypotheses, in how much they can bid, they’re constrained by how much money they have. So if a hypothesis doesn’t have any money, it can’t bid high in this auction. So we take the highest bid - the highest budgeted bid, I guess - then we take that hypothesis. The hypothesis has to “pay us” their bid. So it’s like a [first price auction](https://en.wikipedia.org/wiki/First-price_sealed-bid_auction). They pay us their bid and then we do whatever they told us we should do. Then we observe, we get our reward, and then we pay them that reward or a virtual currency amount proportional to that reward.\n\n**Daniel Filan:** Okay. So is the idea that if a hypothesis slightly low-balls, but it’s basically accurate about how much reward you can get, other hypotheses that overestimate how well you can do are going to blow all their money, you’re going to save it up because you get some money every round. And then eventually you could be the top bidder and you’ll make money by… you slightly low ball, and so you get back a little bit more than you spent to make the bid and you just eventually dominate the bids. Is that roughly how I should think of it working?\n\n**Caspar Oesterheld:** Yes, that’s basically the thing to imagine. I think depending on… Yeah, with this low-balling, that depends a bit on the scenario. For example, if you have a setting where the payoffs are deterministic and fully predictable, so you just choose between a reward of 5 and a reward of 10 and there are hypotheses that can just figure out that these are the payoffs, then if you have enough hypotheses in your class, then there will be just one hypothesis that just bids 10 and says you should take the 10 and then you won’t… The winning hypothesis won’t be low-balling, it will just barely survive. It’s like the typical market argument where if you have enough competition then the profit margins go away.\n\n**Daniel Filan:** Okay. And the idea is that other agents which overpromise are going to… If agents overpromise, then they lose money relative to this thing that bids accurately. And if agents underpromise, then they don’t win the auction. And so this thing never spends money so it just survives and wins.\n\n**Caspar Oesterheld:** Yeah, basically that’s the idea. I guess the most important kind of features, the first thing to understand is really that the hypotheses that just claim high rewards and don’t hold up these promises, they just run out of money and so they don’t control much what you do. And so what’s left over once these are gone is you basically follow the highest bid among those bidders that do hold up their promises. And so in the limit, in some sense, among these you actually do the best thing.\n\n### Relationship to logical induction\n\n**Daniel Filan:** Cool. And so basically in the rest of the paper, if I recall correctly, it seems like what you do is you basically say… well, you make a bunch of claims, which net out to, if one of these traders in this auction can figure out some pattern, then you can do at least as well as that trader. So if you’re betting on these pseudorandom numbers, then you should be able to do at least as well as just guessing the expectation. And if none of… If the pseudorandomness is too hard for any of the agents to crack, then you don’t do any better than that. At the end, I think there’s a connection to game theory, which we can talk about a bit later, but we touched a bit on this relationship to logical inductors. When I was reading this paper, I was thinking, the second author, Abram Demski, I think he’s an author on [this logical inductors paper from 2016 or so](https://arxiv.org/abs/1609.03543).\n\n**Caspar Oesterheld:** I think he’s actually not. I’m entirely sure, but I think he might not be.\n\n**Daniel Filan:** Oh, he’s not? Okay. He is a member of [the organization that put out that paper](https://intelligence.org/) at least.\n\n**Caspar Oesterheld:** He is very deep into all of this logical induction stuff. I’m not sure I could have written this paper without him because knows these things very well and I think that was quite important for this particular project.\n\n**Daniel Filan:** Yeah. You’ve got this coauthor who’s connected to the logical induction world. They’re both about inductive rationality by bounded beings, and they both involve these algorithms where agents bid against each other. How is this different from the logical induction paper?\n\n**Caspar Oesterheld:** I guess the main and most obvious difference is that the logical induction paper is just about forming beliefs in some sense. It’s just about assigning probabilities to statements. You can think about how you can use that then to make decisions, but very basically, if you just look at the logical induction paper, it’s all about forming beliefs about whether a particular claim is true. Whereas regret minimization, but also this rational inductive agency project, it’s all about making decisions between some set of options. I think that’s a fundamentally different setting in important ways. I think the most important way in which it’s different is that you have to deal with this counterfactual issue, that you take one of the actions and you don’t observe what would’ve happened otherwise. For example, one way in which you can clearly see this is that in any of these decision settings, you have to pay some exploration cost.\n\nWith the bounded rational inductive agents, sometimes you will have to follow a bidder, a hypothesis that has done terribly in the past. You need to sometimes follow it still because there’s some chance that it just did poorly by bad luck. In fact, we didn’t go that much into the details of this, but you actually have to hand out money for free to your hypotheses so that you can exploit each hypothesis infinitely often because otherwise, there’s some chance that there’s some hypothesis that really has the secret to the universe, but just on the first hundred time steps for some reason it doesn’t do so well. You have to pay this cost, and regret minimizers also pay this cost. With logical induction, in some sense you do exploration, but one thing is that if you… maybe this will be hard to follow for readers who aren’t familiar with that. Sorry, listeners.\n\n**Daniel Filan:** I’m now realizing we should probably just say a few sentences about what [that paper](https://arxiv.org/abs/1609.03543) was. According to me, the logical inductors paper was about how do you assign probabilities to logical statements. The statements are definitely either true or false. They’re statements like “the billionth and 31st digit of pi is 7”, and you’re like, what’s the chance that that’s true? Initially you say 1/10th until you actually learn what that digit of pi actually is by calculating it or whatever, and it uses a very similar algorithm. The core point of it is every day you have more questions that you’re trying to assign probabilities to, and you learn more logical facts and eventually you just get really good at assigning probabilities.\n\n**Caspar Oesterheld:** Yeah.\n\n**Daniel Filan:** Anything I missed out on?\n\n**Caspar Oesterheld:** Well, I guess for what I was about to talk about it’s good to have some kind of intuition for how the actual algorithm or the proposed mechanism works. Very roughly it’s that instead of hypotheses or experts, they have traders which are also, I don’t know, some set of computationally simple things. Very roughly what happens is that these traders make bets with each other on some kind of prediction market about these different logical claims, that one is trying to assign probabilities to. Then the idea is if there’s a trader that’s better than the market at assigning probabilities, then the trader can make money by bidding against the market. Eventually that trader will become very wealthy and so it will dominate the market. That way you ensure that in some sense you do at least as well as any trader in this set.\n\n**Daniel Filan:** Crucially, traders can kind of choose what markets to specialize in. If you’re betting on digits of pi or digits of e, I can be like, “well, I don’t know about this e stuff, but I’m all in on digits of pi”. I guess this can also happen in your setting if you’ve got different decision problems you face.\n\n**Caspar Oesterheld:** Yeah. That is the way in which our bounded rational inductive agency theory is more similar to the logical inductors. Our hypotheses are allowed to really specialize and they only bid every 10,000 steps on some very special kind of decision problem, and otherwise they just bid zero. You still get that power in some sense. Whereas the regret minimizers don’t have this property. Generally, you really only just learn what the best expert is.\n\n**Daniel Filan:** Cool. I cut you off, but sorry, we were saying about a comparison with logical inductors and it was something about the traders.\n\n**Caspar Oesterheld:** Yeah. One thing is that if you have a new trader, a trader that you don’t really trust yet, then you can give them a tiny amount of money and then they get to make bets against the market. If you give them a tiny amount of money, their bets won’t affect the market probabilities very much. You can explore this in some sense for free. In some sense, they don’t influence very much what you think overall because the idea is even if you give them a tiny amount of money, if they’re actually good, they’ll be able to outperform the market and they’ll be able to get as much money as they want. That isn’t possible in the decision-making context because to explore a hypothesis in the decision-making context, you have to make this “yes/no” decision of actually doing what they do. If you have a hypothesis that’s just a complete disaster, you’re going to make this disastrous decision every once in a while.\n\n**Daniel Filan:** Yeah. In some sense it’s because decisions just have discrete options. If you’re a predictor, you can gradually tweak your probability a tiny bit, but you can’t quite do that in…\n\n**Caspar Oesterheld:** Yeah. Though, I think the fundamental issue has more to do with these counterfactuals and then the counterfactuals not being observed. To test a hypothesis, you have to do something differently. Let’s forget about the logical inductors for a second and just say, I don’t know, you make some predictions about all kinds of things and I am unsure whether to trust you or not. Then I can just ignore what you say for the purpose of decision-making or for the purpose of assigning beliefs myself or stating beliefs to others. I can completely ignore this. I don’t have to do anything about it, and I can just track whether you are right. If you’re good, I’ll still eventually learn that you’re good. Whereas if you tell me, I don’t know, you should really do more exercise or something like that and I ignore it, I’ll never learn whether you were actually right about it or not.\n\n**Daniel Filan:** Okay. Yeah, it’s giving me a better sense of the differences these theories have. Cool. Another difference, which was kind of surprising to me, is that in the logical inductor setting, I seem to recall hearing that if you actually just do the algorithm they proposed in the paper, it’s something like 2 to the 2 to the X time to actually figure out what they even do. Whereas with your paper, if all of the bidders about what you should do, if they’re computable in quadratic time, if it’s just all the quadratic time algorithms, it seemed like you can have your whole routine run in quadratic time times log of log N or some really slow growing function, which strikes me as crazy fast. What’s going on there? For one, what’s the difference between the logical inductor setting, and two, how is that even possible? That just seems like so good.\n\n**Caspar Oesterheld:** Yeah. That is a kind of notable difference, in some sense. This algorithm that I just described, this decision auction, as we sometimes call it, it’s just an extremely low overhead algorithm; you can just think about it. You run all your bidders, then you have a bunch of numbers. You have to take the max of these numbers. That’s all very simple. The reason I think why the logical induction algorithm that they propose is relatively slow is that they have to do this fixed point finding. I don’t know.\n\nI think roughly the way it actually works is that the traders, it’s not really like a prediction market in the literal sense. I think the traders actually give you functions from market prices to how much they would buy or something like that. Then you have to compute market prices that are a fixed point of this, or an approximate fixed point of this. This fixed point finding is hard. I think maybe here the continuity is a big issue; that probabilities are continuous, so in some sense you need to find something in a continuous space.\n\nThe correct probability in some sense is some number between 0 and 1, or actually I think it’s the probability distribution over all of these logical statements or the probabilities of all of these logical statements. It’s this really large object or an object that in some sense carries a lot of information. It’s from some large set, so they need to find this in this large set. Whereas we only have this one decision. But I think that on some level, the criteria are just pretty different. They’re similar in many ways in that they designed this market and so on, but I think on some level the criteria are just somewhat different between the logical induction paper and ours.\n\n**Daniel Filan:** Yeah. It’s strange though because you would think that making good decisions would reduce to having good beliefs. One way you can do the reduction is every day you have some logical statement and you have to guess “is it true with probability 1?”, with probability 99%, 98%, and I don’t know. You just have 100 options and you have to pick one of them.\n\n**Caspar Oesterheld:** Yeah. In theory that kind of works, but if you apply a generic decision-making framework to this setting where you have to say the probability or decide which bets to accept or something like that, then you’re not going to satisfy this really strong criteria that this Garrabrant \\[logical\\] inductor satisfies. For example, if you have the bounded rational inductive agents and on each day you have the choice between what’s the highest price you would be willing to pay for a security that pays a dollar if some logical statement is true and pay zero otherwise. That’s assigning a probability to that statement, but you have to explore all of these hypotheses that say, yeah, you should buy the $1 thing. You should buy it for $1. Even for digits of pi, where let’s say you take… or coin flips, something that’s actually random and where you should just learn to say one half every day. The bounded rational inductive agents will necessarily say any answer infinitely often. They will converge to giving one of the answers with limit frequency 1, but every once in a while they’ll say it’s something completely different.\n\n**Daniel Filan:** Yeah, so it’s not like actual convergence.\n\n**Caspar Oesterheld:** Yeah. I think in the regret minimization literature, people sometimes say they have these different notions of convergence. It’s convergence in iterates and convergence in frequencies I think, or something like that, and you only have the weaker thing with bounded rational inductive agents.\n\n### How fast does it converge?\n\n**Daniel Filan:** Yeah. In fact, this is one of the things I was wondering, because in the paper you prove these limit properties and I’m wondering, okay, am I going to get convergence rates? It seems like if you make the wrong decision infinitely often, but infinitely less frequently, in some sense it feels like the right thing to say is that you have converged and you can talk about the convergence rate, but you haven’t actually literally converged. Is there some intuition we can get on what the convergence or quasi-convergence properties of these things are over time?\n\n**Caspar Oesterheld:** Yeah. One can make some assumptions about a given bounded rational inductive agent and then infer something about the rate at which it converges. Roughly, I think the important factors are the following. The first is: how long does it take you to explore the hypothesis that gives you the desired behavior? If you imagine, I don’t know, you have some computational problem deciding whether a given graph has a clique of size 3 or something like that, that can be done in cubic time if not faster. And so you might wonder… The first thing is how long does it take you to explore the hypothesis that just does the obvious computational procedure for deciding this, which is just try out all the combinations of vertices and deciding whether that’s a clique. In some sense it’s kind of similar to these bounds that you have for Occam’s razor-type or Solomonoff prior things where it’s kind of like the prior of the correct hypothesis.\n\nSimilarly here, it’s a bit different because really what matters is: when do you start giving it money? Then there’s still… if things are random, then it might be that you give it money, but then it has bad luck and then it takes a while for it to… Yeah. That’s the first thing that matters. Then the other thing that matters is: how much other exploration do you do? If you explore something very quickly, but the reason you explore it very quickly is that you explore everything very quickly and you give lots of money to lots of hypotheses that are nonsense, in some sense, it then takes you longer to converge in the sense that you’re going to spend more time doing random other stuff. Even once you’ve found the good thing, the good hypothesis that actually solves the problem and gives an honest estimate, you’ll still spend lots of time doing other stuff.\n\n**Daniel Filan:** You have to have this balancing act in your payout schedule.\n\n**Caspar Oesterheld:** Even just to satisfy the criterion that we described, one has to make sure that the overall payoffs per round go to zero. In some sense the overall payoffs per round is kind of how much nonsense you can do, because to do nonsense, you have to bid high and not deliver, which is kind of like losing money out of this market. That’s how you control that. Then meanwhile, you also have to ensure that each trader or each hypothesis eventually gets infinite amounts of money.\n\n**Daniel Filan:** Yeah.\n\n**Caspar Oesterheld:** These are the two things. Within satisfying these constraints you can balance them in different ways.\n\n**Daniel Filan:** Yeah, I guess 1 over T is the classic way to do this sort of thing.\n\n**Caspar Oesterheld:** Yeah, that’s the one that we have in the proof.\n\n### Non-myopic bounded rational inductive agents?\n\n**Daniel Filan:** Yeah. That’s actually how I… it just flashed in front of eyes and I was like, ah, now I see why they picked that. Yeah, I just skimmed that appendix. Yeah. One thing I want to ask about is: you talk about this bandit setting, and in particular you’re being myopic, right? You see a thing and you’re supposed to react myopically to the thing. And there are other settings, other relatively natural settings like [Markov decision processes](https://en.wikipedia.org/wiki/Markov_decision_process) or something. I’m wondering: how do you think the work could be extended to those sorts of settings?\n\n**Caspar Oesterheld:** Yeah. That’s a good question. I think there are different ways. I guess one way is that if it’s, for example, an episodic Markov decision process or I don’t know, some other thing…\n\n**Daniel Filan:** Oh, yeah. That was my fault. Yeah. A Markov decision process is like, you’re in a state of the world, you can take an action and the world just works such that whenever you’re in a state and you take a certain action, there’s some other state that you go to with some fixed probability no matter what the time is. Similarly, you get some reward similarly with some sort of fixed probability. Anyway, I interrupted you and I forgot what you said, so maybe you can start again. I’m sorry.\n\n**Caspar Oesterheld:** Right. There are different answers to how one would apply bounded rational inductive agents or extend them to the setting. I guess the most boring thing is: well, you can just apply them to find a whole policy for the whole Markov decision process, or sometimes there are these episodic Markov decision processes, which basically means you act for 10 time steps, then you get a reward, and then you kind of start over and then you can treat the episodes as separate decision problems that you can solve myopically. Okay. That’s a relatively boring answer. The more interesting thing is that you could have something like a single Markov decision process and all you do is you play the single Markov decision process and it never starts over, and you want to maximize discounted reward, which is, if you get a reward of 1 today, it’s worth 1 to you. If you get a reward of 1 tomorrow, you get 0.9, if you get it the day after, 0.81 and so on. That would be a discount factor of, well, 0.9 or 0.1.\n\nIn this case, I think what one could try to do, and I haven’t analyzed this in enormous detail, but I think it’s a very natural thing that I think probably works, is that one applies this whole auction setup, but the reward that one is supposed to estimate at each step, at each step that one is deciding which action to take, the hypotheses are supposed to give an estimate of the discounted reward that they’re going to receive. Then if they win, they get the discounted reward, which means that it has to be paid out slowly over time. At time step 1000, you have to give the winner of the auction at time step 100 a tiny bit of reward. I think then probably things generally hold, with some issues that complicate things.\n\nOne issue is that hypotheses now have to take into account that on future steps exploration might occur. Let’s say I’m a hypothesis and I have some amazing plan for what to do, and the plan is to first play this action, then play that action, and then this another action and so on. I have this detailed plan for the next 100 time steps, but I have to worry that in 50 time steps there’ll be some really stupid hypotheses coming along, bidding some high number and doing some nonsense. I have to take this into account, so I have to bid less. This makes everything much more complicated. If there is a hypothesis that has a really good plan, it’s harder for that hypothesis to actually make use of this because it can’t rely on winning the auction at all these steps, so there are definitely some complications.\n\n### Relationship to game theory\n\n**Daniel Filan:** Yeah. Interesting. The final thing I want to talk about in this paper is: at the end you talk about using it as some sort of foundation for game theory where if you have these BRIAs, I’m going to call them (for boundedly rational inductive agents)… I say I’m going to call it, that’s what they’re called the paper, I’m not an amazing inventor of acronyms. You talk about these BRIAs playing games with each other and they each think of it as one of these bandit problems. Essentially you say that if they have these rich enough hypothesis classes, they eventually play Nash equilibrium with each other, is my recollection.\n\n**Caspar Oesterheld:** Yeah. There are different versions of the paper that actually give different results under different assumptions. There is a result that under some assumptions they give Nash equilibrium. There’s also a result that is more like a folk theorem, which kind of says that you can, for example, converge to cooperating in the prisoner’s dilemma. Maybe I could try to give a general sense of why it’s kind of complicated what happens, and why maybe sometimes it’s going to be Nash and sometimes it’s going to be something else. Okay. The first thing is that these bounded rational inductive agents, nothing hinges on randomization. In some sense that’s kind of the appealing part relative to regret minimizers. There’s no randomization, no talk about counter-factuals. You just deterministically do stuff. In particular, if you have a bounded rational inductive agent play against the copy of itself in a prisoner’s dilemma, it will converge to cooperating because whenever it cooperates, it gets a high reward, whenever it defects it gets a low reward.\n\nSo there are bidders that get high reward by recommending cooperation and bidding the value of mutual cooperation, maybe slightly below. The defect bidders, they might hope like, “okay, maybe I can get the defect/cooperate payoff”, but they can’t actually do this because if the bidder in one market achieves this - the hypothesis in one market - if it tries to do this, then its copy in the other market also does it. So whenever you actually win, you win the auction, you just get the defect/defect payoff. This would converge to cooperation. The reason why there’s a Nash equilibrium-style result nonetheless is that: if the different agents are not very similar to each other, then you might imagine that the hypotheses can try to defect in some way that’s de-correlated from the other market, or from what happens in the other auction.\n\nThe simplest setting is one where they can actually just randomize, but you could also imagine that they look at the wall and depending on the value of some pixel in the upper right or something like that, they decide whether to recommend defecting or not. If they can do this in this a decorrelated way, then they break the cooperate/cooperate equilibrium. The reason why there are different results and also why there are different versions with different results is that… I’m still unsure what the correct conclusion from it is or what the actual conclusion is. For example, I’m still unsure whether these cooperate/cooperate outcomes in the prisoner’s dilemma, whether you can achieve them naturally without fine-tuning the markets too much to be exact copies.\n\n**Daniel Filan:** Yeah. I guess one way to think about it is, you’ve got this weak uncorrelation criterion where if you meet it, you fall back to Nash and another criteria and you get this. Presumably the right theorem to show is, under these circumstances you fall into this bucket and you get this, under these \\[other\\] circumstances, you fall into this bucket and you get this. If I implement my bounded rational inductive agent slightly differently, like I order the hypotheses a bit differently, do we know whether that’s going to hit the corporate/corporate equilibrium or it’s going to be weakly uncorrelated?\n\n**Caspar Oesterheld:** I think even that is already not so easy to tell. I don’t know. I’ve done some experiments and generally it learns to defect in this kind of setting, but in these experiments the hypotheses are also relatively simple, so you don’t have hypotheses that try to correlate themselves across agents. For cooperating, one hope would be that you have a hypothesis in one of the markets and a hypothesis in the other market and they somehow try to coordinate to cooperate on the same rounds. This becomes quite complicated pretty quickly. I think maybe another important thing here is that there’s still a difference between the general bounded rational inductive agency criterion versus this specific auction construction. The criterion allows all kinds of additional mechanisms that you could set up to make it more likely to find the cooperate/cooperate equilibrium. You could specifically set it up so that hypotheses can request that they only be tested at various times in such a way that it’s correlated between the markets or something like that. It’s very complicated and that is still something I’m working on. Hopefully other people will think about this kind of question too.\n\nSafe Pareto Improvements\n------------------------\n\n### What they try to solve\n\n**Daniel Filan:** Cool stuff. Yeah. There’s actually a bunch more… there’s at least one more thing, a few more things that I would love to talk about with this bounded rational inductive agents paper, but we spent a while on that and there are two other papers that I’d like to talk about as well. Let’s move on to the next paper. This is called [Safe Pareto Improvements](https://link.springer.com/article/10.1007/s10458-022-09574-6) by yourself and Vincent Conitzer. Can you just give us a sense of what’s this paper trying to do?\n\n**Caspar Oesterheld:** This paper is trying to directly tackle this equilibrium selection problem: the problem that if you play a given game, it’s fundamentally ambiguous what each player should do, so you might imagine that they fail to coordinate on a good Nash equilibrium, on a Nash equilibrium at all, and they might end up in these bad outcomes. A very typical example of this is a setting where players can make demands for resources, and they can both demand some resource, and if they make conflicting demands, the demands can’t both be met, so usually something bad happens; they go to war with each other in the extreme case. Safe Pareto Improvements is a technique or an idea for how one might improve such situations, how one might improve outcomes in the face of these equilibrium selection problems.\n\nMaybe I can illustrate this with an example. Let’s take a blackmail game. This is actually a bit different from the kind of example we give in the paper, but this is a version of this idea that people may have heard about under the name [surrogate goal or surrogate goals](https://s-risks.org/using-surrogate-goals-to-deflect-threats/). Let’s say that I’m going to delegate my choices to some AI agent. You can think of something like GPT-4, and what I’m going to do is I’m going to tell it, “here’s my money, here’s my bank account, my other web accounts, and you can manage these, so maybe you should do some investing”, or something like that. And now, this AI might face strategic decisions against different opponents. Right? So other people might interact with it in various ways; \\[they might\\] try to make deals with it or something like that, and it has to make decisions in the face of that.\n\nAnd let’s just take some very concrete interaction. So let’s say that someone is considering whether to threaten to report my online banking account or something like that if my AI doesn’t give them $20. So they can choose to make this kind of threat. Now, maybe, you might think that it’s just bad to make this kind of threat and I should just not give in to this kind of threat. But you can imagine that there’s some kind of moral ambiguity here, so you could imagine that this person actually has a reasonable case that they can make for why I owe them $20. Maybe at some point in the past I promised them $20 and then I didn’t really give them $20, or something like that. So they have some reason to make this demand.\n\nSo now, this is a kind of equilibrium selection problem where my AI system has to decide whether to give in to this kind of threat or not. And this other person has to decide whether to make the threat, whether to insist on getting the $20 or not. And there are multiple equilibria. So the pure equilibria are the ones where my AI doesn’t give in to these kind of threats and the other person doesn’t make the threat. And so that’s one. And the other one is, my AI does give in to the threat and the other person makes this kind of threat. Okay. So a typical equilibrium selection problem, in some ways. There’s some ways in which this is a game theoretically a bit weird, this is a non-generic game, and so on. So I think this kind of example is often a bit weird to game theorists. So maybe, for game theorists, the example in the paper works a bit better. But I like this kind of example: I think it’s more intuitive to people who aren’t that deep into the game theory stuff.\n\nOkay, so now, I’m deploying this AI system, and I’m worried about this particular strategic interaction. And in particular, maybe the case that seems worst is the case where the coordination on what the correct equilibrium is fails. So in particular, the case where my AI decides not to give in to the threat and the other person thinks like, “No, no, I should really get that $20, so I’m going to make this threat,” and then they report my online banking account and they don’t even get the $20, and so everyone’s worse off than if we hadn’t interacted at all. Basically, utility is being burned. They are spending time reporting me on this online banking platform, I have to deal with not having this bank account anymore, or have to call them, or things like that.\n\n### Alternative solutions\n\n**Caspar Oesterheld:** Okay. Now, there are various ways in which you might address this. And I kind of want to first talk a bit about some other ways you might deal with this, if that’s okay, to give us a sense of what’s special about the solution that we propose. Because I think it’s kind of easier to appreciate what the point of it is if one first sees how more obvious ideas might fail.\n\nSo the first thing is that if one of us is able to credibly commit, at some point, to some course of action, they might want to do so. So I might think that the way for me to do well in this game is just that I should be the first to really make it credible that my AI system is never going to give in to this kind of threat. And I should just announce this as soon as possible, I should try to prove this, that I’m not going to give in to threats. And then maybe the other person, they would want to try to, as fast as possible, commit to ignore this kind of commitment and so on.\n\nSo this is… I think it’s a reasonable thing to think about, but it kind of feels like it’s not really going anywhere. Ultimately, to some extent, people are making these commitments simultaneously, they might also just ignore commitments, right? It seems like if you go around the world and whenever someone makes some commitment to, I don’t know, threaten you or to ignore anything you do to get them to do something, you shouldn’t be the kind of person to just give in and cave to any such commitment. You kind of have to have some kind of resistance against these kind of schemes. So I think in practice, this just doesn’t resolve the problem.\n\nAnd also, it’s a very zero-sum way of approaching solving this problem. It’s all about, I am just going to try to win. Right? I’m just going to try to win by keeping my $20 and deterring you from even threatening me. And you might say, “Well, I’m going to deter that. I really want the $20, and I’m going to be first.” Right? So…\n\n**Daniel Filan:** It’s also very… I don’t know, if you imagine implementing this slightly more realistically… people learn things over time, they understand more facts. And if you’re racing to make commitments, you’re like, “Oh yeah, I’m going to determine what I’m going to do when I know as little as possible.” It’s not an amazing…\n\n**Caspar Oesterheld:** Yeah. Daniel Kokotajlo has this post on, I think, LessWrong or the Alignment Forum titled [Commitment Races](https://www.alignmentforum.org/posts/brXr7PJ2W4Na2EW2q/the-commitment-races-problem) or something like that. It’s also kind of about this idea that one wants to commit when one knows as little as possible, and that seems kind of problematic.\n\n**Daniel Filan:** Yeah.\n\n**Caspar Oesterheld:** Okay. So that’s one solution. Another solution might be that I could try to just pay you, offer you $5 or something like that in return for you not blackmailing me. So we make some outside deal, and the idea is that if we do make this deal, then okay, I still have to pay some amount of money, but at least this inefficiency of utility really being burned, it disappears if this deal is made. But then on the level of figuring out what deal to make, we get all the same problems again. I might offer $5 and the other person might say, “Well, actually, you really really owe me those $20, so obviously I’m not going to take this $5, I want at least $18,” something like that. And so you get the same problem again.\n\n**Daniel Filan:** Yeah, it’s funny… Yeah, I find this a weird thing about game theory where intuitively, talking out problems seems like an easier way to solve stuff, but whenever you try to model bargaining, I don’t know, it’s kind of horrible. I’ve never seen any convincing analysis of it.\n\n**Caspar Oesterheld:** Yeah.\n\n**Daniel Filan:** Just very strange.\n\n**Caspar Oesterheld:** And I do think that this problem, it’s very fundamental - this equilibrium selection, what’s the appropriate way of distributing resources? There are lots of approaches and they’re all good, but I do think that fundamentally, this is just a problem that is hard to get rid of entirely.\n\n### How safe Pareto improvements work\n\n**Caspar Oesterheld:** But now, okay, now comes the ‘safe Pareto improvements’ or ‘surrogate goals’ idea for making progress on this. Remember, I’m deploying my AI system. I could do the following. So let’s say that by default, the way this delegation works is that I’m going to tell my AI everything that I want it to do. I’m telling it like, “Okay, here’s my money and I’m such and such risk averse. I really want to make sure that I always have at least such and such amount in my bank account in case something happens.” And also, maybe I can tell it that there’s some stamp that I really like and if this stamp appears on eBay for a price of less than $30, it should try to get it, and these kind of things.\n\nSo normally, I just honestly tell it my preferences and then I say “do your best”. Now, what I could do instead, for the purpose of this particular interaction, is the following. I first set up a dummy bank account that I don’t care about at all. So I set up some new online banking account similar to the online banking account that the other person might threaten to report. And I don’t care about this at all, but I tell my AI system to care about this bank account as much as I care about the actual online banking account. So I tell it, “Okay, if this were to be reported, that would be just as bad as if the other one is being reported.” And I have to do that in a way that’s credible, so that’s important here. The other person needs to see that I’m doing this.\n\nSo let’s say that I do this. And let’s say that, in addition, I tell my AI to not give in to threats against my actual original banking account. Now, why is this an appealing idea? Basically, the idea is that from the perspective of the person who’s thinking about threatening to report my banking account, nothing really has changed. Right? They can still threaten me, and they can still be equally successful at threatening me, because they have to threaten to report this different account now. But to my AI, that’s just the same as it would’ve been by default. It’s like to them, nothing’s really different. They don’t feel like I’m tricking them or anything. They’re completely fine with this happening.\n\nBut meanwhile, for me, there’s some chance that things improve relative to the default. In particular, they might still make this threat again now, to report this new dummy account. And it might be that my AI just gives in to that threat, right? In which case I think, “okay, this is kind of funny, but okay, that was part of the plan”. But it could also be that my AI resists, decides not to give in to this new kind of threat. Probably that’s just as likely as it would’ve been to not give into the original kind of threat. And in this case, if this happens, if the other person threatens and my AI doesn’t give into the threat, then I am better off than I would’ve been by default, because now, they’re going to report this dummy account that I don’t actually care about. So I’m just fine. It’s just as if no threat had been made. Of course, for my AI, it might still be very sad. So my AI might still think, “Oh, I’ve done a terrible job. I was instructed to protect this dummy account and now it’s been reported. I’m a bad Bing,” or something. But to me, it’s better than it would’ve been by default. And again, to the other person, it’s kind of just the same.\n\nAnd that’s kind of what safe Pareto improvements mean in general. It’s this idea of making some kind of commitment or some modification of the utility functions, or some kind of way of transforming a game that ensures that everyone is at least as well off as they would’ve been if the game had been played in the default way. But under some potential outcomes, there’s some Pareto improvements. So some person is better off, or everyone’s better off, without making anyone worse off. One important part is that it’s agnostic about how this equilibrium selection problem is resolved. To make this commitment, or to tell my AI to do this, I don’t need to think about how the equilibrium selection problem in the underlying game is going to be resolved. I can make this safely without having to make any guesses about this. And the other player, similarly, they don’t need to rely on any kind of guess about how it’s going to be resolved.\n\n**Daniel Filan:** Gotcha. So, actually, okay, I have an initial clarifying question where I think I know the answer. You call it a safe Pareto improvement. What’s an unsafe Pareto improvement?\n\n**Caspar Oesterheld:** Yeah, good question. So the safety part is supposed to be this aspect that it doesn’t rely on guesses as to how the equilibrium selection stuff is going to work out. So an unsafe Pareto improvement might be something like, I’m transferring you $10 so that we don’t play this game, or something like that. Which is unsafe in the sense that it’s hard to tell how we would’ve played the game, and I actually don’t know whether it’s an improvement, or we don’t know whether it’s a Pareto improvement to do this deal. It relies on specific estimates about how we would’ve played this game. So yeah, that’s what the term ‘safe’ is supposed to mean. Maybe it’s not the optimal term, but that’s what it’s supposed to mean.\n\n**Daniel Filan:** Gotcha. So one thing it seems like you care about is, these instructions I’m supposed to give to my AI, they’re not just supposed to make my life safely better off, they’re also supposed to make the other guy’s life safely better off. Why do I care about that? Shouldn’t it all be about me?\n\n**Caspar Oesterheld:** So, yeah, that’s a good question. So that’s kind of coming from this intuition that these kind of races to commit first can’t be won. So that if I tried to come up with some scheme that commits my AI in such a way that it screws you over, then maybe you should have already committed to punishing me if I implement that scheme. Or you have some reason to try to commit as fast as possible to punish me if I try to commit that scheme, or to ignore if I try to commit this scheme. So there are all these issues, and the idea is to avoid all of this by having something that’s fine for both players so that no one minds this being implemented, everyone’s happy for this to be implemented. And so all of these competitive dynamics that otherwise are an obstacle to implementing “I just commit first” approaches, they disappear.\n\n**Daniel Filan:** Gotcha. So if I kind of think about this scheme, it seems like the suggested plan, roughly, is: I have this really smart AI that knows a bunch of more things than me. In the real world, I don’t know exactly what it’s going to do or what plans it could potentially think of. And the safe Pareto improvement literature basically instructs me to think of a way that I can deliberately misalign my AI with my preferences, right?\n\n**Caspar Oesterheld:** Yeah.\n\n**Daniel Filan:** It seems like this could go wrong easily, right? Especially because in the safe Pareto improvements paper, you’re assuming that the principal, the person who’s doing the delegating of the game-playing, knows the game. But in real life, that might not be true. So how applicable do you think this is in real life?\n\n**Caspar Oesterheld:** Yeah. In real life, one will have to do things that are much more complicated, and I think the real life surrogate goals will be much more meta. In this scheme with this very simple AI and this blackmail setting, the game is very simple. There’s binary choices, and so on. And also, in this scheme, maybe my AI doesn’t really know what’s going on. It might not understand why I’m giving it these instructions, it might just be confused, like, “Okay, well I guess this is what I’m supposed to do.” I think the more realistic way to implement this is to give some kind of meta instruction to “adopt a surrogate goal in the way I would’ve liked you to do”, or something like that.\n\n**Daniel Filan:** So somehow delegate to the machine… Not only is it finding equilibria, it’s also trying to figure out what the SPI would be, and…\n\n**Caspar Oesterheld:** Yeah. There are different aspects that one can delegate. Maybe one slightly more complex setting is a setting where, very roughly, it’s clear what is going to happen. Maybe very roughly, it’s clear that I’m going to deploy an AI and you can make some kind of threat against it or try to blackmail it in some ways, but it’s not clear to me how costly it is for you to make different kinds of threats. And then basically, I would have to… Implementing the surrogate goal requires knowing these things, right? Because I have to make the new thing - the new target - I have to make it somehow equivalent to the old one. And this is the kind of thing that one probably should delegate to the AI system in the real world.\n\n**Daniel Filan:** Gotcha.\n\n**Caspar Oesterheld:** A more radical approach is to just give it some entirely generic instruction that just says, like, “Okay, whenever you are in any kind of strategic scenario, where I might have no idea what that scenario will be, whenever you face any kind of strategic scenario, first think about safe Pareto improvements, and potentially implement such an improvement, if it exists.”\n\n### Will players fight over which safe Pareto improvement to adopt?\n\n**Daniel Filan:** So this kind of gets to a question where… I mean, one of the things that safe Pareto improvements were supposed to do is deal with multiple equilibria and maybe people picking different equilibria. It seems like there are potentially tons of conflicting - or different - safe Pareto improvements, right?\n\n**Caspar Oesterheld:** Yeah.\n\n**Daniel Filan:** In fact, because they have such a larger action space, I would guess that there’d be even more equilibria in the “find a safe Pareto improvement” game. So are we getting very much, if it’s just really hard to coordinate on a good SPI?\n\n**Caspar Oesterheld:** Yeah. Also a very important question. So I think maybe first, it’s good to get an intuition for why these many safe Pareto improvements might exist. Because I think in the example that I gave, there actually is only one, because only one player can commit. And I think, okay, depending on what else exists in that world, there might exist only one. But yeah, let’s maybe give an example that makes clear why there might be many. So in the paper, we have this example of the demand game, which is just a game where there’s some bit of territory and two countries can try to send their military to take that bit of territory, but if they both send out their military, then there’s a military conflict over the territory. So that’s kind of the base game. And then the idea is that they could try to jointly commit to…\n\nOkay, sorry, one step back, let’s also assume that the two countries make this decision by delegating to some commission or some expert who is thinking about what the appropriate equilibrium is. And then the idea there is that they could instruct the commission to adopt some new attitudes towards this game. So they would say, “Okay, never mind the military, let’s just send someone with a flag.” Like, “Let’s just decide whether to send someone with a flag who just puts the flag in the ground and says, ‘This is now ours.’” And then we just tell them, “Well, okay, if both of our countries send someone with a flag to that territory and put in the flag, that’s really, really bad. That’s just as bad as war.” And so this is a safe Pareto improvement in this situation. Is this setting somewhat clear? I guess you’ve looked at the paper so maybe to you it’s…\n\n**Daniel Filan:** Yeah, roughly, the players can either send in the military or they can just send a guy with a flag or they can send nothing. And if there’s clashes, that’s bad. But in real life, clashes are worse if there’s military. And if just one of the players does it, then the player who sends the most stuff gets the land and they want to have the land. It seems like that’s roughly the situation.\n\n**Caspar Oesterheld:** Yeah. Thanks. So the safe Pareto improvement is to not send the military, which is what one would normally do, just send the guy with the flag. And then one avoids this conflict outcome where both send the military. And here, you could imagine that in some sense, it’s kind of ambiguous what to do instead of this conflict outcome. Right?\n\nBecause currently…okay, we have this “guy with a flag” story. What exactly happens with the territory if both countries send a guy with a flag? It’s kind of just left open. I think the paper just specifies that in that case it’s split, or something like that. But it could be that if both players send someone with a flag, then just country A gets the territory. And then it’s still a safe Pareto improvement because it might still be better to have just country A get the territory than to have a war over the territory, because war is not so great.\n\nSo here, there are these many safe Pareto improvements that are characterized by what happens instead of war. Like, instead of war, does one player just get the resource, or do both players… Do they split it or does the other player get the resource? Something like that.\n\nOkay, so now the question is: does this mean that we get the same problem one level up? Or it’s just as bad, or maybe it just doesn’t help. And I think this depends a lot on the setting. I think in some settings, safe Pareto improvements really literally do nothing. They don’t help at all, because of this. And in other settings, they still help. And roughly, the settings where it helps are ones where the bad outcome that we’re replacing with something else is just worse for both players than anything on the Pareto frontier.\n\nFor example, let’s imagine that in this demand game setting where there’s this territory that two countries are having a dispute over, war is worse for both players than it is to even just not get the territory in the first place. So in that case, even the worst safe Pareto improvement that you can get, namely the one where instead of war the other person just gets the resource, is still an improvement. It’s still a Pareto improvement, it’s still an improvement for both players. And so in particular, if you’re mostly just worried about war, and maybe it’s not that terrible for you to give up this territory, then this is… You might say, okay, when we meet to decide which safe Pareto improvements to get, you’re kind of willing to just settle for the one that’s worst for you, and it’s still a gain overall.\n\nThis other example, this surrogate goal example, is another case of this, where the worst outcome for me in that case, is the one where a threat is being carried out and my bank account is being reported. And so even if we somehow make it so that in the case where my dummy account is reported, I still give the $20, which only works if the other player also makes some kind of commitment, then this is still an improvement for me. Right? So I might still be okay with just giving up the $20 in that case. The important condition, I think, is that the bad outcome that we’re getting rid of is worse than anything on the Pareto frontier. Such that even if I get the worst thing on the Pareto frontier, it’s still good for me.\n\n**Daniel Filan:** But even then, the players have to agree which thing on the Pareto frontier they go for, right?\n\n**Caspar Oesterheld:** Yeah.\n\n**Daniel Filan:** And yeah, you have this argument in your paper that I wasn’t totally compelled by. So my recollection was, you basically said, “Well, most of the players can just say, ‘Look, if any other player recommends a safe Pareto improvement, I’ll go for that one.’ And we just need one person to actually think of something.” But then it’s like, well, who actually submits the safe Pareto improvement? Or what if multiple people do? Or what if I say… You can imagine, I say, “If someone else submits a safe Pareto improvement, then I’ll go for it. But if they don’t, I want this one.” And you have a similar instruction but you have a different fallback SPI… it still seems quite difficult to figure out how to break that tie.\n\n**Caspar Oesterheld:** I mean, I’m not sure I exactly understand. Let’s take the case where I’m happy to just implement the worst possible safe Pareto improvement, where we replace the conflict outcome with me giving you the territory. If it’s a two-player game and I have this attitude and I say, “Okay, you can have the territory, in that case.” And let’s say I’m really happy, so I state this in the beginning of the negotiations, “I’m actually happy if you just take it.” Right? Then is there any remaining problem in that case?\n\n**Daniel Filan:** Well, I don’t know what… I don’t know, maybe it’s a happier problem, but suppose we both come to the table and we both say, “Hey, I’m happy with any Pareto improvement over the worst one.”\n\n**Caspar Oesterheld:** Ah.\n\n**Daniel Filan:** Well, we still have to figure out what we get. Right?\n\n**Caspar Oesterheld:** Right.\n\n**Daniel Filan:** And still, on this level, it seems like you might want to try and threaten to get your preferred SPI, otherwise… “If you don’t agree to my SPI rather than your SPI, then screw it, we’re going to war.”\n\n**Caspar Oesterheld:** But okay, so if both players are kind of happy to give the other person their favorite SPI, and it’s just the issue that, I don’t know, there’s a large set of different things that both people would be okay with, or…\n\n**Daniel Filan:** Yeah. And I have SPIs that I’d prefer, right? And you have SPIs that you’d prefer, and they might not be the same ones.\n\n**Caspar Oesterheld:** Right. Yeah. To me, this seems like a much easier problem than equilibrium selection.\n\n**Daniel Filan:** Yeah.\n\n**Caspar Oesterheld:** Because this is just the case where players are, basically… it’s almost this like, “No, you go first,” “You go first,” type problem.\n\n**Daniel Filan:** I mean, to me, it sounds like a bargaining problem, right?\n\n**Caspar Oesterheld:** But it’s only… It doesn’t seem like a bargaining problem anymore, once it’s clear that the players have overlap. Right? It’s a bargaining problem for as long as it’s unclear which SPI to go for. And I mean, yeah, if you really want to max out, you want to get the best safe Pareto improvements for you, and you also want to get the best safe Pareto improvement for you, and we both go to the table saying, “Okay, I actually really want this,” and you say you really want this. Okay, then there’s a risk. But if my argument is that even by going for the… You can just say, “Okay, you can have whatever you want in terms of safe Pareto improvements,” and even this attitude already improves things.\n\n**Daniel Filan:** Yeah. I guess… maybe I shouldn’t belabor the point, but it still seems almost identical to these bargaining scenarios where there’s this [best alternative to negotiated agreement](https://en.wikipedia.org/wiki/Best_alternative_to_a_negotiated_agreement), and we can all do better, but there are a few different mutually incompatible options to do better, and we have to figure out which one we want. And I ideally would get the one that’s best for me, and you ideally would get the one that’s best for you. That seems like the situation we’re in.\n\n**Caspar Oesterheld:** Okay, let’s say we have some other bargaining scenario, where the two of us start a startup together, and we’re both needed for the startup, and so we have to come to an agreement on how to split the shares for the startup. Right? So this is a very typical bargaining problem. We have to figure out how much to demand, or what fraction of the startup to demand. But now, if I come to this negotiation table and just say, “I’ll accept whatever demands you want as long as it’s better for me to do the startup than to not do the startup.”\n\nSo let’s say that as long as I get 20% of the startup, it’s still better for me in terms of how wealthy I’m going to be in five years, or whatever, to be part of the startup than to not be. And let’s say that this is common knowledge. Then I might say, “Okay, as long as I get at least 20%, I’m fine.” It seems that if for some reason, I’m happy to adopt this attitude, then I would think this bargaining problem becomes very easy. Even, you might say, theoretically, it could be that you have a similar attitude and you say, “Well, actually for me, I’m also happy with just getting my minimum of 45% or whatever.” And okay, then we have this problem where we have the remaining 35% to distribute, but…\n\n**Daniel Filan:** I mean, in some sense, it’s easy, but also, agents like this will get arbitrarily… The amount you’ll improve off the base game will be arbitrarily small if you’re like this and the other player’s like, “I would like everything as much as I can.”\n\n**Caspar Oesterheld:** Yeah, so in this type of bargaining setup, that’s true. So in this type of bargaining setup, this attitude of just “I’m happy to accept the minimum,” is bad. You shouldn’t adopt this attitude. You should try to get more, right? You should try to make some demands, and thus risk that conflicting demands are made. Because yeah, if you just make the minimum demands, you never make any money beyond what you could have made without the startup, right? In some sense, you don’t gain anything from this whole startup, because you only demand the absolute minimum that makes it worth it for you to do the startup. The point that I’m making is that in the case of safe Pareto improvements, even this kind of minimalist, this really dovish bargaining approach to deciding which SPI to go for is still much better, potentially, than not doing anything.\n\n**Daniel Filan:** So the idea is that just all of the options are just significantly better than the BATNA, basically.\n\n**Caspar Oesterheld:** Yeah, so this is specifically if the outcome that you replace is some really bad thing, like going to war with each other, or you can think of even more ghastly things if you want. Then anything, even giving up the territory, or maybe giving up everything or something like that, it might still be better than this conflict outcome.\n\n### Relationship to program equilibrium\n\n**Daniel Filan:** Got you. So there’s a bunch of other things to talk about here. So one thing that I was thinking about when I was reading this paper is: it seems sort of analogous to the program equilibrium literature, right? So you write a computer program to play a game, I write a computer program to play a game, but our programs can read each other’s source code, right? And the [initial papers](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e1a060cda74e0e3493d0d81901a5a796158c8410) in this literature, they considered computer programs that just checked “if our programs are literally equal, then they cooperate, otherwise they defect” or something.\n\nAnd then I think in advance of this… In this literature that came out of [MIRI](https://intelligence.org/), I believe, the Machine Intelligence Research Institute, was to think about this thing they called [modal combat](http://intelligence.org/files/lob-notes-IAFF.pdf), where I try and prove properties about your program and you try and prove properties about my program. Then through the magic of [Löb’s Theorem](https://en.wikipedia.org/wiki/L%C3%B6b%27s_theorem), it turns out that we can cooperate [even if we can only search through proofs of however long](https://arxiv.org/abs/1602.04184).\n\nI’m wondering… So in the SPI paper, the paper kind of explicitly envisions \\[that\\] you send a particular SPI to your decision-making committee, and it says if the other person sent the exact same thing, then implement it, otherwise, fall back on your default. And I’m wondering, can we make this modal combat step where… Sorry, it’s called… Actually, I’m not even going to explain [why it’s called modal combat](https://en.wikipedia.org/wiki/Mortal_Kombat). People can Google that if they want. But is there some way to go a little bit more meta or abstract?\n\n**Caspar Oesterheld:** So yeah, one can use these kinds of mechanisms, like the modal combats, the Löbian FairBot that the researchers at MIRI propose for establishing cooperative equilibrium in this program setting. I mean, we can use that to achieve these safe Pareto improvements.\n\nSo the original safe Pareto improvement that I described, the first one that I described, the surrogate goal idea where only I modify my AI and you don’t, right? That doesn’t really require this, but if you have this case where two players each have to give some new instructions to their AI or their committee that is deciding whether to send troops to the territory or something like that, in that case, usually the safe Pareto improvements have to be backed up by some joint commitment. So both sides, they have to commit that, I don’t know, we are going to tell our committee to send the guy with a flag rather than the troops, but we only do this conditional on the other side making an analogous commitment.\n\nAnd yeah, one way to back this up is to have this kind of program equilibrium-type setup where both countries, they write some contract or something like that for their commission. And the contracts, they are computer programs that look at the other country’s contract. And depending on what that contract says, it gives different instructions for how to reason about the guy with the flag versus the troops.\n\nAnd this contract, if you think of it as literally a computer program, as seems reasonable in the AI case, then yeah, you could use these Löbian ideas for implementing this joint commitment. So you could say… Yeah, I’m not sure how much to go into the details of the Löbian FairBot, where you can show… If I can prove that the other side adopts their side of the safe Pareto improvements, then I adopt my side of the safe Pareto improvements. Otherwise, I just give the default instructions. And then if both sides make this commitment, then it will result in both giving the safe Pareto improvement instructions to their committees. Is that what you had in mind, or-\n\n**Daniel Filan:** Yeah, that sort of thing. I guess there’s a difficulty… I mean, you might hope that you would not have to specify exactly what the SPI has to end up being, but I guess the trouble is precisely because you’re assuming you don’t know how the committee solves for the equilibrium. Presumably, your program can’t try and prove things about what the other solver is going to go for. Because if you could do that, then you could just say, “Go for this nice outcome,” or something.\n\n**Caspar Oesterheld:** So there are two obstacles, I guess. The first is that potentially you can’t predict what the other committee is going to do or how it’s going to resolve the equilibrium selection problem. But the other is also that you don’t want to know, in some sense, right? Or you don’t want to adopt a policy of first predicting what the other committee does and then doing whatever is best against that, right? Because then the other committee can just say, “Well, this is what’s going to happen. We’re just going to demand that we’re going to send the troops.” Best response to sending the troops is not to send troops or the guy with the flag.\n\n### Do safe Pareto improvements break themselves?\n\n**Daniel Filan:** Got you. I guess \\[there’s\\] a bunch of things I could ask, but the final thing I wanted to ask about here was… So there’s a critique of this line of research, I think this [LessWrong post by Vojta Kovarik](https://www.alignmentforum.org/posts/K4FrKRTrmyxrw5Dip/formalizing-objections-against-surrogate-goals), where one of the things mentioned is: it seems that implicit in the paper is this idea that the way the committee solves games is the same with or without the safe Pareto improvements potentially existing, and all that the safe Pareto improvements do is just change which game the equilibrium selection mechanism plays.\n\nBut you could imagine, well, if I know I’m in a world that works like committees giving instructions to people, you could imagine that this potentially does change how people make decisions, and that potentially seriously limits… You could imagine that this limits the applicability of this research, and I’m wondering, how serious a limitation do you think this is?\n\n**Caspar Oesterheld:** Yeah, so I do think that this is a limitation. This is something that makes it non-applicable in some cases. I mean, there’s even just the more basic worry that you might… For example, in this first AI case that I described, you might… Never mind influencing the way my AI reasons about games, right? I might just tell it, “Okay, actually, secretly, don’t give into threats against this dummy bank account,” right? If I can secretly say this to the AI, then already there’s a problem. So we have to assume that that’s not possible. And then there’s the fuzzier problem that my AI’s bargaining strategy can’t depend in some sense on the existence of safe Pareto improvements. I think in some settings, this really is just a problem that makes this very difficult.\n\nSo here’s an example where I think it’s clear that it’s a big problem. Let’s imagine that I am delegating to AI, but it’s kind of unclear which AI I’m going to delegate to. I can decide which AI to delegate to, and there are 10 different AIs on the market that I can delegate my finances to or something like that. And now I can decide which of them to hire to take care of my finances. And if I know that safe Pareto improvements will be used, I have some reason to hire an AI that’s more hawkish, that’s less likely to give into threats, because I think that it’s more likely that threats are going to be made against the surrogate goal.\n\nSo in response, the threatener might think, “Okay, if I go along with this whole surrogate goal idea, then there’s a good chance that I’m going to be screwed over,” and so they should just basically ignore the whole surrogate goal stuff and say like, “Okay, sorry, I don’t want to do the surrogate goal business, because I don’t know what AI you would’ve rented by default, and so I can’t really judge whether I’m being screwed over here.” So it definitely can be a problem in this setting.\n\nMeanwhile, I think maybe in other cases, it is clear what the default would be. So for example, it might be clear that I generally, in cases where safe Pareto improvements don’t apply for other reasons - for example, because no credible commitment is possible or something like that - I might always delegate my choices to a particular AI. And then in cases where I want to apply my safe Pareto improvements or my surrogate goals or whatever, it seems clear that if I just use the same AI as I use in other cases, then in some sense, my choice of which bargaining strategy is deployed is not being influenced by the existence of safe Pareto improvements.\n\nAt least one way in which this can work is that you have access to the ground truth of what people would do without safe Pareto improvements. For example, by being able to observe what people intend to do in scenarios without safe Pareto improvements.\n\nSimilarity-based Cooperation\n----------------------------\n\n**Daniel Filan:** Got you. Okay. So the last paper I’d like to chat about is this [paper on similarity-based cooperation](https://arxiv.org/abs/2211.14468). So this is co-authored by yourself, Johannes Treutlein, Roger Grosse, Vincent Conitzer and Jakob Foerster. So can you give us a sense of what’s this paper about?\n\n**Caspar Oesterheld:** Sure. I guess in some sense, you’ve already set it up very well with some of this open source game theory or program equilibrium stuff that you talked about earlier. So yeah, that’s the setting where two players, they each write some source code and then the programs get access to each other’s source code and they choose an action from a given game.\n\nSo for example, in the prisoner’s dilemma, you would submit a computer program that takes the opponent’s computer program as input and then outputs cooperate or defect, and it has been shown by [this literature](https://citeseerx.ist.psu.edu/document?repid=rep1&type=pdf&doi=e1a060cda74e0e3493d0d81901a5a796158c8410) that this kind of setup allows for new cooperative equilibria. And the simplest one is just “cooperate if the opponent is equal to this program, otherwise, defect”.\n\nSo similarity-based cooperation… This paper considers a setting that is in some sense similar. So we again have two players that submit some kind of program or policy or something like that, and that gets some information about the opponent policy or program. The main difference is that we imagine that you only get fairly specific information about the opponent. You don’t get to see their entire source code. You only get a signal about how similar they are to you. So in most of the settings that we consider in the paper, one just gets to observe a single number that describes how similar the two policies are. And the policies or programs essentially just get as input a single number and then they output, maybe stochastically, whether to cooperate or defect or what to do in the base game.\n\nOne can show that in this setting, still one can get cooperative equilibria. In some sense, it’s not too surprising, right, because the cooperative equilibrium in this program equilibrium case that we discussed, this “if the opponent is equal to this program, then corporate, otherwise, defect”… In some sense, that is a similarity-based program: “if the similarity is 100%, then corporate, otherwise, defect”. But as we show in the paper, there are more interesting things that can happen, less rigid ways of cooperating, and you can apply this to other games and so on.\n\n**Daniel Filan:** Yeah. And in particular there’s a strange theorem where… so you basically have this noisy observation of a similarity function (or a difference function, I guess is the way you frame it). But I think there’s this theorem that says if you don’t put any constraints on the difference function, then you can get, I think it was something like every outcome that’s better than mini-max payoff can be realized or something. Which can be worse than Nash equilibrium, right?\n\n**Caspar Oesterheld:** Yeah, that can be worse than all Nash equilibria of the game\n\n**Daniel Filan:** So I guess somehow, at least in some classes of games, it’s better if the thing you’re observing is actual similarity rather than arbitrary things.\n\n**Caspar Oesterheld:** Yes. So that’s this folk theorem result which says which outcomes can occur in equilibrium. Yeah, it’s surprisingly exactly the same as in program equilibrium if you don’t constrain the diff function. The way in which these weird equilibria are obtained is very non-natural. It requires that the diff function, for example, is completely asymmetric in symmetric games and things like that. To avoid this one needs natural diff functions or natural ways of observing how similar one is, in some sense of natural. We have some results in the paper also about… In some sense, it says the opposite: that says if you’re under certain conditions, that admittedly are quite strong on the game and on the way that the similarity is observed, you don’t get this folk theorem, you get a much more kind of restricted set of equilibria.\n\n**Daniel Filan:** Yeah. It seemed like you had relatively weak criteria. So that theorem roughly says under some criterion on the difference function that struck me as relatively weak, but on a symmetric game, then you get, as long as there are non-Pareto-dominated Nash equilibria, then they have equal payoffs and there must be the best payoff. So it seems like, I guess you have this restriction that it’s a symmetric game and also that the Nash equilibria are non-Pareto-dominated. Or I guess there must be… hang on. There always has to be some non-Pareto-dominated Nash equilibria, right?\n\n**Caspar Oesterheld:** Yeah, with some weird analysis caveats, right? The set of equilibria might be some open set, something something. But it’s probably not so reasonable to get into the details of that. But yeah, I think it’s reasonable to assume that there always is a Nash equilibrium that’s not Pareto-dominated by another Nash equilibrium. Yeah, the paper says that if you have any such Nash equilibrium, it has to be symmetric, so it gives both players the same payoff.\n\n**Daniel Filan:** Got you. So this is an interesting paper. In some ways it’s interesting that it somehow gets you the good qualities you wanted out of program equilibrium by getting these nice cooperative outcomes in symmetric games while providing less information. And at least in this restricted setting, you get less information than the full program and you get better results. Yeah. I wonder if this is just one of these things where more options in game theory can hurt you. Because on some level it’s a little bit surprising, right?\n\n**Caspar Oesterheld:** Yeah, I think it is an illustration of that. By being able to fully observe each other, you get all of these different equilibria including weird, asymmetric bad ones. So being able to fully observe each other’s source code, yeah, in some sense it makes things worse because there’s much more to choose from. Whereas under certain conditions, I mean, as far as our paper goes, you avoid this problem if you have the more limited option of just accessing how similar you are to the opponent.\n\n### Are similarity-based cooperators overly cliqueish?\n\n**Daniel Filan:** Yeah. So speaking of restrictions on the difference function, one thing that struck me is that in real life, “cooperate with people who are very similar to you and otherwise defect” is not an outcome that we aspire to. And I’m wondering: does it work - it seems like you want something where people cooperate just as long as they’re just similar enough to cooperate, even if they disagree about what’s fair in some sub-game that we’re not actually going to get into or something. You want minimal agreement to be able to get cooperation. And I am wondering: how does this fit in with this setting?\n\n**Caspar Oesterheld:** Yeah, it’s a good question. I think an important thing to clarify about how this would work, I think the important thing is that to get this cooperative equilibrium, one really needs a signal of how similar one is with respect to playing this game that one is currently playing, or with respect to how cooperatively one approaches the game that one is currently playing.\n\nAnd in particular, all other kinds of signals about similarity are completely useless, right? If we play a game and we get a signal about, I don’t know, whether we have the same hair color or something like that, that’s completely useless for how we should play the game, presumably, unless the game is about some specific thing that relates to our hair color. But that signal is useless. And also, probably if you get a super broad signal that just says, “Well, in general, you’re kind of pretty similar,” it’s probably not even sufficient to get the cooperative equilibria because… Okay, I mean, if the signal says you’re exact copies, then that’s sufficient. But if the signal says you’re 99% the same and there’s just 1% that you are kind of different, 1% of your source code is different or something like that, well, it might be that this 1% is exactly the part that matters, the part that decides whether to cooperate or defect in this game. So yeah, what really matters is this strategic similarity.\n\n**Daniel Filan:** Yeah, I think there’s an in-between zone though, where suppose I say, “Hey, I’m going to cooperate with people who cooperate with me. But if we don’t reach a cooperative equilibrium, I’m going to defect in this one way.” Suppose you say, “Oh, I’m going to cooperate with people who are willing to cooperate with me. But if we don’t manage to cooperate, I’m going to have this different method of dealing with the breakdown case.” Now intuitively, you’d hope that there’d be some good similarity metric that we could observe where this counts as similar and we end up cooperating somehow. I’m wondering does that happen in this formalism?\n\n**Caspar Oesterheld:** Yeah. Okay, I mean, our formalism generally, it doesn’t necessarily restrict the diff functions that much. I mean, it definitely allows diff functions that only depend on what you do against similar players. So the kind of diff function that you’re describing is: we say that players are similar if they do similar things, when they observe that they’re facing a similar opponent, and otherwise we regard them as different. And I think that would be sufficient for getting cooperative equilibria. In some sense, I think that’s kind of the minimum signal that you need.\n\n**Daniel Filan:** Yeah. I guess in some sense the question is: can we come up with a minimally informative signal that still yields maximal cooperation or something?\n\n**Caspar Oesterheld:** Yeah.\n\n### Sensitivity to noise\n\n**Daniel Filan:** All right. So I now have, I guess, some questions about the details of the paper. So one thing that kind of surprised me is: I think in proposition 2 of the paper you’re looking at these policies where if our similarity is under this threshold, cooperate, otherwise defect. And the similarity measure agents observe is the absolute value of the difference between the thresholds plus some zero mean random noise (or let’s say Gaussian). And in proposition 2 it says that if the noise is mean zero, even if the standard deviation is super tiny, if I read it correctly, it says that the policies are defecting against each other at least half the time.\n\n**Caspar Oesterheld:** Yeah. And that’s under particular payoffs of the Prisoner’s Dilemma.\n\n**Daniel Filan:** That strikes me as rough. That seems pretty… I would’ve imagined that I could have been able to do better there, especially with arbitrarily tiny noise, right?\n\n**Caspar Oesterheld:** Yeah, generally the way noise affects what equilibria there are is kind of counterintuitive in the paper or in the setting that we consider. So I mean, there’s also another result that’s surprisingly positive. So if you have noise that’s uniform between zero and some number, X, then in this setting where… Each player submits a threshold, like cooperate below, defect above, and then the diff that they observe is the difference plus noise, let’s say, uniform from zero to some number, X.\n\nRegardless of X… You might think like, okay, with higher X means more noise, right? So if there’s higher X, you might think the equilibrium must get worse, right? It’s like at some high enough X, it kind of stops working. But it turns out that it basically doesn’t matter, it’s just completely scale invariant. Even if the noise is uniform from 0 to a 100,000 there’s still a fully cooperative equilibrium.\n\n**Daniel Filan:** A fully what? A fully cooperative equilibrium?\n\n**Caspar Oesterheld:** Yeah, a fully cooperative equilibrium. So that’s one where both players cooperate with probability 1 for the diff that is in fact observed.\n\n### Training neural nets to do similarity-based cooperation\n\n**Daniel Filan:** Interesting. So one thing that the paper sort of reminded me of is… or it seemed vaguely reminiscent to me of this algorithm called [LOLA, or Learning with Opponent-Learning Awareness](https://arxiv.org/abs/1709.04326), where basically when you’re learning to play a game, you don’t only think about how your action gets you reward, but you also think about how your action changes how the opponent learns, which later changes how much reward you get. And the reason that this matters is that you have some experiments of actually just doing similarity-based cooperation or training in a method that’s inspired by this with neural networks. So I think the thing you do is you study alternate best response learning, which if I understand correctly is, you train one network to respond well to the other network, then you train that network to respond well to the first network and you just keep on doing this. And basically you find something like: you do your similarity-based cooperation thing and it ends up working well, roughly. Is that a fair summary of what happens?\n\n**Caspar Oesterheld:** Yeah, though it’s a bit more complicated. Okay, so we have this setting where each player submits, let’s say, a neural net, and then they observe how similar they are to each other and then they play the game, and then let’s grant that this setting has a cooperative equilibrium where they cooperate if they’re similar and defect the more dissimilar they are.\n\nSo there’s this problem still of finding the cooperative equilibrium. So yeah, you don’t know what exactly the neural nets are. This is some complex setting where they also need… Cooperation isn’t just pressing the cooperate button, it’s computing some function, and defecting is also some other function. So you need to do some ML to even find strategies. Even defecting is not so easy. You have to compute some function.\n\nSo what we do is indeed this alternating best response training, which is exactly what you described. The thing though is that if one just initializes the nets randomly and then one does the alternating best response training, then one converges to the defect-defect equilibrium. The reason for that, I think… I mean, one never knows, right, with ML. But I think the reason is probably just that this defect-defect equilibrium is much easier to find. And there’s this bootstrapping problem in finding the cooperative equilibrium. The reason to learn this similarity-based cooperation scheme is that the other player also plays the similarity based cooperation scheme and you want to be similar to them, right? But if they’re now randomly initialized, you just want to defect.\n\n**Daniel Filan:** Basically you just need to observe some similarity in order to exploit similarity, and by default you just never observe it. Is that roughly right?\n\n**Caspar Oesterheld:** Well, actually, if you initialize two neural nets randomly, right, if they’re large and so on, right, they’ll actually be pretty similar to each other, because they just compute some statistical average. The issue is just that it doesn’t pay off to become more similar to the other net, because the other net just does random stuff, right? So if you want to do well against the random neural network that doesn’t use the similarity value and just does random nonsense, the best thing to do is just to defect.\n\n**Daniel Filan:** So you have to be similar and reward similarity.\n\n**Caspar Oesterheld:** Yeah, exactly. So you have to be similar or… I mean, I guess the most important part is that to learn to do the scheme, the other person basically has to already have implemented the scheme to some extent. If they’re just doing something else, if they always cooperate or always defect or just do some random nonsense, then there’s no reason to adopt the scheme.\n\nIt still doesn’t hurt to adopt the scheme. If you adopt the scheme, they’ll be dissimilar, and so you’ll defect against them. So it’s just as good as defecting, but it’s a complicated scheme, right? You have to learn how to exactly decrease your amount of cooperation with how dissimilar they are, and you need to learn how to cooperate, which in this setting that we study experimentally is actually hard. So they need to set up this complicated structure and there’s no pressure towards having this complicated structure if the opponent is just random. So there’s no pressure ever towards having this complicated structure.\n\nAnd I should say that this is very normal in other settings as well. So for example, it’s also not so easy to get learners to learn to play [tit-for-tat](https://en.wikipedia.org/wiki/Tit_for_tat). The reason for that is kind of similar: that in the beginning, if your opponent is randomly initialized, mostly you just learn to defect. And if you both start learning to defect, you never learn that you should cooperate and do this tit for tat thing or whatever, because your opponent is just defecting, right? So to get to the better equilibrium of both playing tit for tat or something like that, you somehow need to coordinate to switch from both always defecting to both always doing this tit for tat, which doesn’t randomly happen.\n\n**Daniel Filan:** It’s almost reminiscent of the problem of [babbling equilibria](http://www.davidreiley.com/GameTheoryAEA/AEAContEd_I.pdf), right? So, for listeners who might not know: suppose you’ve got some communication game where agents want to communicate things to each other, there’s this problem where initially I can just talk nonsense and it means nothing, and you can just ignore what I’m saying. And that’s an equilibrium because if you’re not listening, why should I bother to say anything other than nonsense? And if I’m saying nonsense, why should you listen to it? Is that exactly the same or am I just drawing loose associations?\n\n**Caspar Oesterheld:** No, I think it’s basically the same problem. I think in all of these cases, I think fundamentally the problem is that there’s some kind of cooperative structure that only pays off if the other player also has the cooperative structure. And so as long as neither player has the cooperative structure, there’s never any pressure to get the cooperative structure: the communication protocol or the tit-for-tat or the “cooperate against similar opponent”, all of these schemes, there’s just no pressure towards adopting them as long as the other player hasn’t adopted them. And so you just get stuck in just doing the naive thing.\n\n**Daniel Filan:** So, in your paper, you have a pre-training method to address this, right?\n\n**Caspar Oesterheld:** Yeah, so we have a very simple pre-training method. Basically it’s just: explicitly train your neural nets to basically cooperate against copies, which if you consider more general games, it’s just train them to maximize the payoff that they get if they’re faced with a copy while taking the gradient through both copies. So it’s like you play the game fully cooperatively in some sense.\n\nAnd you also train them to do well against randomly-generated opponents, which if you have some prisoner’s dilemma-like game, where just basically it just needs to defect against, especially against dissimilar opponents, that’s the pre-training method. And basically the result of the pre-training method is that they very roughly do the intuitive thing, that they cooperate at low levels of difference or high levels of similarity, and the more different they are from their opponent, the more they defect.\n\nSo it does this intuitive thing, but it doesn’t do it in a… in some sense it’s unprincipled. In this pre-training process, there’s never any explicit reasoning about how to make something an equilibrium or how to make it stable or something like that. It’s just naively implementing some way of implementing this kind of function.\n\nSo that’s the pre-training. And then we do this alternating best response training where then we take two different models that are independently pre-trained in this way and then we face them off against each other. And they typically start out cooperating against each other because they are actually quite similar after applying this pre-training, and then - maybe this is surprising, I don’t know how surprising it is - but the more interesting thing I think is that if you then train them with alternating best response training, they converge to something that’s at least somewhat cooperative. So basically they find a cooperative equilibrium.\n\n**Daniel Filan:** Yeah. And this is kind of surprising. So, you might naively think… Well, alternating best response is usually, you hold your opponent fixed and then you’re like, “What can I do that’s best for me?” And you might think, what can I do that’s best for me is just ‘defect’, right?\n\n**Caspar Oesterheld:** Yeah, though I mean it is the case that, if your opponent cooperates against similar opponents and defects against dissimilar opponents, it’s clear that there’s some pressure for us becoming a copy of them, or becoming very similar to them.\n\n**Daniel Filan:** And you are also taking that gradient?\n\n**Caspar Oesterheld:** Yeah, one definitely has to take that gradient, otherwise one just learns to defect. I guess the reason why it’s not obvious that this works is just that the pre-training, it’s so naive. It’s such a simple method. It’s much simpler than [this opponent-shaping stuff](https://arxiv.org/abs/1709.04326) that you described where you kind of think about, “Okay, I have to make it so that my opponent’s gradient is such and such to make sure that I incentivize them to be a copy of me rather than something else.” One doesn’t do any of that. One just does this very simple crude thing.\n\nAnd so I think what this does reasonably well to demonstrate is that it’s not that hard to find these cooperative equilibria with relatively crude methods.\n\n**Daniel Filan:** Although I think you said that they didn’t cooperate with each other all the time?\n\n**Caspar Oesterheld:** Yeah. So the cooperation does unfortunately somewhat evaporate throughout this alternating best response training. So they might initially be almost fully cooperative with each other and then you train them to become best response to each other and then they actually learn to be a bit less cooperative.\n\n**Daniel Filan:** Okay. So, if the only issue was finding good pre-training or finding a good initialization, and then they have this pressure to become more like “cooperate with things similar to me”, why wouldn’t they cooperate more rather than less over time?\n\n**Caspar Oesterheld:** That’s a good question. So if one had an optimal pre-training scheme that actually finds a correct way of doing the similarity-based cooperation scheme, so one finds a strategy that’s actually an equilibrium against itself, for example, and then one trains - and let’s say both players do this and maybe they don’t find exactly the same equilibrium, but they find some such strategy, and then we train the opponent to be a best response to your policy. Then what they should learn is just to become an exact copy.\n\nAnd then once you’re there, you don’t continue. You stop. You’re done. You can’t improve your payoff anymore. Okay, gradients: if you still take gradient steps, it gets complicated, but if you think of it as really just trying to improve your neural net, you can’t improve it anymore if you’re in that equilibrium.\n\nYeah. So if you had this optimal pre-training scheme then alternating best response training would in some sense immediately, on the first step, it would just get stuck in the correct equilibrium.\n\nSo why doesn’t this happen? There are some reasons, multiple reasons I think. So one is just that our initialization just isn’t so good, so I kind of doubt that they are equilibria against each other. I think we tried this at some point to just see what happens if you just pair them up against the literal copy and they also unlearn to cooperate a bit because they just don’t implement the kind of correct curve of defecting more as they become more dissimilar.\n\n**Daniel Filan:** Right. So, if they just don’t implement the correct algorithm, you just don’t have that pressure to remain being similar and reward similarity?\n\n**Caspar Oesterheld:** Yeah, you have some complicated pressure. I guess you still don’t want to be completely different, but there’s all this trickery where you’re kind of similar in some ways, but I don’t know, your curve goes down a bit more and you exploit that their curve doesn’t really go down enough as the diff increases. You still have some pressure towards becoming similar, but it’s not enough and it’s not exact.\n\nAnd then I think the other issue is that the alternating best response training… in some sense it can make them more cooperative. So this isn’t quite true because it can make them more cooperative by making them more similar. So if I have my optimally pre-trained network and then you train your thing to be a best response to mine, then it becomes more cooperative towards mine and mine will become more cooperative towards it.\n\nBut if you look at any individual network, it can’t become more… Once it only, let’s say, cooperates with 60% probability against exact copies, once it cooperates only with 60% probability or it’s not exactly cooperative anymore at a diff value of 0, there’s no way to get back from this to cooperating with a 100% chance as long as your opponent isn’t still cooperating at a 100%.\n\nSo there’s a way in which… if you imagine this alternating best response training as somewhat noisy, sometimes it by accident makes people defect a bit more, or something like that. Or sometimes it is just better to defect a bit more because the incentive curves aren’t optimal and don’t exactly make it an equilibrium to be a copy. As soon as you lose a bit, you’re never going to get it back. So as a consequence, usually during the alternating best response training, the loss just goes up and then at some point it kind of stagnates at some value.\n\n**Daniel Filan:** So, naively, I would’ve thought: if at a 100% cooperativity… if when you cooperate with really close copies all the time, at that point it’s worth me becoming a little bit more similar to you. If I nudge your number down to 99%, I’m surprised that it’s so unstable. Or is the idea that it’s only stable in some zone and alternating best response can get outside of that zone? It is just weird to me that I can have an incentive to become more similar to you at one point in your parameter space, but if you deviate from that, that incentive goes the other way. I would expect some sort of gradual transition.\n\n**Caspar Oesterheld:** Okay. So I mean in general it’s definitely not exactly clear what happens exactly during the alternating best response training, why it finds these partially cooperative equilibria when originally there aren’t any. Why is that? It’s not exactly clear why that’s the case. I mean, I think the reason why originally they’re not cooperative is that… a typical thing is that just their curve is too flat in the beginning in some sense. So they cooperate if they observe a diff value between 0 and 0.1 and they cooperate roughly equally much. And then if you’re an exact copy of this, you would want to defect more just so much that the diff value increases from 0 to 0.1. Okay. It’s not exactly like there’s noise, it’s a bit more complicated, but roughly it’s something like this.\n\n**Daniel Filan:** You don’t want to be an exact copy at the very least.\n\n**Caspar Oesterheld:** Yeah, you don’t want to be an exact copy. I mean, if the noise is uniform from 0 to 0.1, then maybe you do want to be an exact copy. But yeah, it’s somewhat complicated. I think that’s the typical way in which they kind of fail in the beginning, is just that they have these too flat curves.\n\nI think another thing is also that they just aren’t close enough copies in the beginning. Typically they become closer copies throughout training, I think, if I remember correctly. Then it’s unclear why, at least not obvious why the alternating best response training causes the curves to be so that it is an equilibrium. I think part of it is just that they learned to defect maximally much or something like that, as much as they can get away with. Yeah, it’s not…at least I’m not aware of a super simple analysis of why the alternating best response training then does find these cooperative equilibria.\n\n**Daniel Filan:** Okay. So, speaking of things which aren’t super simple, I think in this paper in one of the appendices, you do try this fancier method where instead of just doing alternate best response, you try to shape your opponent to your own benefit. And naively, I might think: ah, this is one of my favorite ways of training agents to play games, and you are trying to shape the opponent to make your life better, so I imagine things will work out better for these agents if you do this, right? But does that happen?\n\n**Caspar Oesterheld:** Yeah, unfortunately it doesn’t seem to work very well. So that’s this [LOLA method](https://arxiv.org/abs/1709.04326) that you also talked about earlier. I also went into this with this kind of hope, thinking… And in some sense it is supposed to solve this exact kind of problem: it was developed, for example, to learn tit-for-tat in the prisoner’s dilemma. But somehow, yes, we tried a bunch to get it to work and we couldn’t really. I don’t know. There are some results where it kind of works for a bit and then it unlearns to cooperate again and it seems relatively unstable.\n\n**Daniel Filan:** Is there any simple story of what’s going on here? Or is it maybe just weird hyperparameter tuning or weird nonsense of strange nets?\n\n**Caspar Oesterheld:** So definitely LOLA is very sensitive to hyperparameters. So that’s kind of known, that if you take any positive LOLA results and you change the hyperparameters a bit, it’s a pretty good chance that it stops working relatively quickly. But I don’t have a good intuition for why it doesn’t work in this case or even in other cases. I don’t have an intuition for why it’s so sensitive to the hyperparameters and things like that and, I don’t know, why doesn’t it always work pretty straightforwardly?\n\nFOCAL, Caspar’s research lab\n----------------------------\n\n**Daniel Filan:** Fair enough. I might move to some closing questions if that’s okay with you. So, first of all: you are the, I think, assistant director or co-director of this FOCAL lab at CMU, right?\n\n**Caspar Oesterheld:** Mm-hmm (affirmative).\n\n**Daniel Filan:** Can you tell us a little bit about that?\n\n**Caspar Oesterheld:** Yeah, so that’s the [Foundations of Cooperative AI Lab at Carnegie Mellon University](https://www.cs.cmu.edu/~focal/). So the actual director is Vincent Conitzer, who’s also my PhD advisor while I’m still in the final stages, I hope, of my PhD.\n\nSo generally it’s a lab that’s supposed to work on the kinds of topics that we’ve discussed today. As one might imagine from the name it’s part of the CS department. But yeah, we have some more philosophical work as well, for example. Currently we have I think one postdoc and five PhD students\n\n**Daniel Filan:** Cool.\n\n**Caspar Oesterheld:** And I think listeners of this podcast who are interested in these kind of topics would be a good fit for this kind of lab. So I don’t know if anyone’s considering starting a PhD \\[but\\] I think it might make sense to check it out.\n\n**Daniel Filan:** Okay. If someone is in that situation, what do they do in order to get into FOCAL?\n\n**Caspar Oesterheld:** I mean, a lot of the application processes I think are not that different from general CS PhD application stuff. It’s good to have a paper or something like that. Another strategy is to try to work with us before applying. For example, at least in the past few years, I mentored summer research fellows at the [Center on Long-term Risk](https://longtermrisk.org/) and also at [CERI, the Cambridge Existential Risk Initiative](https://www.camxrisk.org/). So I guess that’s a way to work with me at least before applying for a PhD, which I think helps, well, if you want to just start working on some of these topics, but maybe also helps with getting in.\n\n**Daniel Filan:** Sure. So, before we wrap up the show as a whole, is there anything that you wish that I’d asked that I hadn’t?\n\nHow the papers all relate\n-------------------------\n\n**Caspar Oesterheld:** Okay, so I guess one kind of question that I expected was that both [the ‘Bounded rational inductive agents’ paper](https://arxiv.org/abs/2307.05068) and [the ‘Similarity-based cooperation’ paper](https://arxiv.org/abs/2211.14468), they touch on this kind of decision theory like Newcomb’s problem, evidential versus causal decision theory, this whole cluster of topics. And so I was expecting to get the question \\[of\\] how these relate, which, yeah, I guess I could say some stuff about. Maybe it’s just an excuse to talk even more about various interesting topics.\n\n**Daniel Filan:** I think listeners will be glad for an excuse to hear you talk more about interesting topics. Yeah, how do they relate?\n\n**Caspar Oesterheld:** So both of the papers are very explicitly inspired by thinking about these kinds of things. So yeah, I think that one should cooperate in a prisoner’s dilemma against a copy, for example. And I think it’s kind of unfortunate that there isn’t that much of a theoretical foundation for why one should do this in terms of learning, for example; regret minimizers have to learn not to do this, for example.\n\nAnd so part of the motivation behind ‘Bounded rational inductive agents’ is to describe a theory that very explicitly allows cooperating against copies as a rational thing to do. So that’s somewhat inspired by this.\n\nAnd then of course with the ‘Similarity-based cooperation’ paper, in some sense it’s even more explicit that it’s supposed to be doing that, though it takes this program equilibrium-inspired outside perspective where one doesn’t think about what it is rational to do in the prisoner’s dilemma against a copy. One thinks about what kind of program it is good to submit in this kind of setting.\n\nAnd so in some sense one has this question that we can ask from the inside, like: if you are a neural net that was built or learned or whatever to play games well and you find yourself in this scenario - from the inside, from the perspective of this neural net - it’s like you face an exact copy and maybe you reason about things by saying, “okay, if I cooperate, then the opponent will cooperate.”\n\n**Daniel Filan:** And both this and [the ‘Safe pareto improvements’ paper](https://link.springer.com/article/10.1007/s10458-022-09574-6) have this quality of: from the outside, you’re making some change to the agent that’s actually making the decisions. And you might think that, shouldn’t this all just happen internally?\n\n**Caspar Oesterheld:** Yeah, it is interesting that both of these papers take this outside perspective and make… I agree, one would think that one doesn’t need the outside perspective, but at least conceptually, it seems sometimes easier to reason from that outside perspective.\n\nSo in particular this program equilibrium framework, in some sense, you can think about this in the general… You can think of program equilibrium or this framework as asking the question, “How should you reason if other people can read your mind and you can read their mind?” And this is really a very hard philosophical question.\n\nAnd you can avoid all of these questions by taking this outside perspective where you submit a program that gets to read the other program’s mind and then you treat this outside perspective just in the normal standard game-theoretic way. I don’t know, it’s a surprisingly… I don’t know, maybe the other problem is surprisingly hard and so this trick is surprisingly successful.\n\n**Daniel Filan:** Yeah. So, I guess that’s one interesting relationship between BRIAs and the similarity-based cooperation: the internal perspective versus the external perspective.\n\n**Caspar Oesterheld:** Yeah. Yeah.\n\n**Daniel Filan:** And I guess there’s also this thing where with similarity-based cooperation, you’re saying: well, if there’s this difference function, then what happens? Whereas in the BRIA thing, you have a variety of these experts or these hypotheses. And I guess in some sense you’re looking at a wider variety of… Maybe the analogy is you’re looking at a wider variety of these similarity functions as well. You’re somehow more powerful, I think.\n\n**Caspar Oesterheld:** Or just more generally looking at strategic interactions in a less constrained way.\n\nRelationship to functional decision theory\n------------------------------------------\n\n**Daniel Filan:** Yeah. There are some interesting questions with these papers, in particular with ‘Safe pareto improvements’. I think relatedly, one thing I didn’t quite ask, but maybe I’ll bring up here: is this just a roundabout way of getting to one of these [“functional decision theories”](https://arxiv.org/abs/1710.05060) where you just choose to be the type of agent that’s the best type of agent to be across possible ways the world could be?\n\n**Caspar Oesterheld:** Yeah, maybe that’s the case. I think the trickiness is that the functional decision theory, [updateless decision theory](https://www.alignmentforum.org/tag/updateless-decision-theory), these sorts of things… they’re not fully specified, especially in these multi-agent scenarios. It’s just unclear what they’re supposed to do. I suppose as a functional decision theorist or updateless decision theorist, one might argue that one shouldn’t need surrogate goals, because in some sense the goal of all of these theories is to do away with pre-commitment and things like that, and you should come with all the necessary pre-commitments built in.\n\nAnd so maybe in some sense, an idealized functional decision theory agent should already have automatically these, well, surrogate goals, except you wouldn’t call them surrogate goals. You just have these commitments to treat other things in the same way as you would treat threats against the original goal to deflect threats.\n\n**Daniel Filan:** You just reliably do whatever you wish you had pre-committed to do, and hopefully there’s one unique thing that’s that.\n\n**Caspar Oesterheld:** Yeah, in the face of equilibrium selection, it’s very unclear what that is supposed to come out as.\n\nFollowing Caspar’s research\n---------------------------\n\n**Daniel Filan:** Yeah. Coming up on the end, suppose somebody’s listened to this, they’re interested, they want to learn more: if they want to follow your research or your output and stuff, how should they do that?\n\n**Caspar Oesterheld:** There are three things. So I recently made an account on the social media platform X, formerly known as Twitter, and that is [@C_Oesterheld](https://twitter.com/C_Oesterheld), and I mostly plan to use this for work-related stuff. I don’t plan to have, I don’t know, random takes on US elections or whatever.\n\nThen I also have a blog at [casperoesterheld.com](https://casparoesterheld.com/), which is also mostly sticking relatively closely to my research interests. And then if you don’t want to deal with all this social media stuff, you can also just follow me on [Google Scholar](https://scholar.google.com/citations?user=xeEcRjkAAAAJ&hl=en&oi=ao) and then you just get just the papers.\n\n**Daniel Filan:** All right. We’ll have links to all of those in the description of the episode. Yeah, it’s been really nice talking. Thanks so much for taking the time to be on AXRP.\n\n**Caspar Oesterheld:** Thanks for having me.\n\n**Daniel Filan:** And for listeners, I hope this was a valuable episode.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with the transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://www.patreon.com/axrpodcast) such as Alexey Malafeev, Ben Weinstein-Raun, and Tor Barstad. To read a transcript of this episode, or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nImagine a world where there are many powerful AI systems, working at cross purposes. You could suppose that different governments use AIs to manage their militaries, or simply that many powerful AIs have their own wills. At any rate, it seems valuable for them to be able to cooperatively work together and minimize pointless conflict. How do we ensure that AIs behave this way - and what do we need to learn about how rational agents interact to make that more clear? In this episode, I’ll be speaking with Caspar Oesterheld about some of his research on this very topic.\n\nTopics we discuss:\n\n * Cooperative AI\n   * … vs standard game theory\n   * Do we need cooperative AI if we get alignment?\n   * Cooperative AI and agent foundations\n * A Theory of Bounded Inductive Rationality\n   * Why it matters\n   * How the theory works\n   * Relationship to logical induction\n   * How fast does it converge?\n   * Non-myopic bounded rational inductive agents?\n   * Relationship to game theory\n * Safe Pareto Improvements\n   * What they try to solve\n   * Alternative solutions\n   * How safe Pareto improvements work\n   * Will players fight over which safe Pareto improvement to adopt?\n   * Relationship to program equilibrium\n   * Do safe Pareto improvements break themselves?\n * Similarity-based Cooperation\n   * Are similarity-based cooperators overly cliqueish?\n   * Sensitivity to noise\n   * Training neural nets to do similarity-based cooperation\n * FOCAL, Caspar’s research lab\n * How the papers all relate\n * Relationship to functional decision theory\n * Following Caspar’s research\n\nDaniel Filan: Hello, everybody. In this episode, I’ll be speaking with Caspar Oesterheld. Caspar is a PhD student at Carnegie Mellon where he’s studying with Vincent Conitzer. He’s also the assistant director of the Foundations of Cooperative AI Lab or FOCAL. For links to what we’re discussing, you can check the description of this episode and you can read the transcript at axrp.net. All right. So welco",
      "wordCount": 27623
    },
    "tags": [
      {
        "_id": "RyNWXFjKNcafRKvPh",
        "name": "Agent Foundations",
        "slug": "agent-foundations"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "pvE6NdPoMCyk55Lxn",
    "title": "Watermarking considered overrated?",
    "slug": "watermarking-considered-overrated",
    "url": null,
    "baseScore": 19,
    "voteCount": 9,
    "viewCount": null,
    "commentCount": 4,
    "createdAt": null,
    "postedAt": "2023-07-31T21:36:05.268Z",
    "contents": {
      "markdown": "_Status: a slightly-edited copy-paste of [a ~~Twitter~~ X thread](https://twitter.com/dfrsrchtwts/status/1682529184881709056) I quickly dashed off a week or so ago._\n\nHere's a thought I'm playing with that I'd like feedback on: I think [watermarking](https://arxiv.org/abs/2301.10226) large language models is probably overrated. Most of the time, I think what you want to know is \"is this text endorsed by the person who purportedly authored it\", which can be checked with digital signatures. Another big concern is that people are able to cheat on essays. This is sad. But what do we give up by having watermarking?\n\nWell, as far as I can tell, if you give people access to model internals - certainly weights, certainly logprobs, but maybe even last-layer activations if they have enough - they can bypass the watermarking scheme. This is even sadder - it means you have to strictly limit the set of people who are able to do certain kinds of research that could be pretty useful for safety. In my mind, that makes it not worth the benefit.\n\nWhat could I be missing here?\n1. Maybe we can make watermarking compatible with releasing model info, e.g. by baking it into the weights?\n2. Maybe the info I want to be available is inherently dangerous, by e.g. allowing people to fine-tune scary models?\n3. Maybe I'm missing some important reasons we care about watermarking, that make the cost-benefit analysis look better? E.g. avoiding a situations where AIs become really good at manipulation, so good that you don't want to inadvertently read AI-generated text, but we don't notice until too late? \n\nAnyway there's a good shot I don't know what I'm missing, so let me know if you know what it is.\n\nPostscript: Someone has pointed me to [this paper](https://arxiv.org/abs/2012.08726) that purports to bake a watermark into the weights. I can't figure out how it works (at least not at twitter-compatible speeds), but if it does, I think that would alleviate my concerns.",
      "plaintextDescription": "Status: a slightly-edited copy-paste of a Twitter X thread I quickly dashed off a week or so ago.\n\nHere's a thought I'm playing with that I'd like feedback on: I think watermarking large language models is probably overrated. Most of the time, I think what you want to know is \"is this text endorsed by the person who purportedly authored it\", which can be checked with digital signatures. Another big concern is that people are able to cheat on essays. This is sad. But what do we give up by having watermarking?\n\nWell, as far as I can tell, if you give people access to model internals - certainly weights, certainly logprobs, but maybe even last-layer activations if they have enough - they can bypass the watermarking scheme. This is even sadder - it means you have to strictly limit the set of people who are able to do certain kinds of research that could be pretty useful for safety. In my mind, that makes it not worth the benefit.\n\nWhat could I be missing here?\n\n 1. Maybe we can make watermarking compatible with releasing model info, e.g. by baking it into the weights?\n 2. Maybe the info I want to be available is inherently dangerous, by e.g. allowing people to fine-tune scary models?\n 3. Maybe I'm missing some important reasons we care about watermarking, that make the cost-benefit analysis look better? E.g. avoiding a situations where AIs become really good at manipulation, so good that you don't want to inadvertently read AI-generated text, but we don't notice until too late?\n\nAnyway there's a good shot I don't know what I'm missing, so let me know if you know what it is.\n\nPostscript: Someone has pointed me to this paper that purports to bake a watermark into the weights. I can't figure out how it works (at least not at twitter-compatible speeds), but if it does, I think that would alleviate my concerns.",
      "wordCount": 325
    },
    "tags": [
      {
        "_id": "KmgkrftQuX7jmjjp5",
        "name": "Language Models (LLMs)",
        "slug": "language-models-llms"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "bsNXqHgiDA6dAKNun",
    "title": "AXRP Episode 24 - Superalignment with Jan Leike",
    "slug": "axrp-episode-24-superalignment-with-jan-leike",
    "url": null,
    "baseScore": 55,
    "voteCount": 25,
    "viewCount": null,
    "commentCount": 3,
    "createdAt": null,
    "postedAt": "2023-07-27T04:00:02.106Z",
    "contents": {
      "markdown": "[YouTube link](https://www.youtube.com/watch?v=Uk6-Rw5N_Dg&list=PLmjaTS1-AiDeqUuaJjasfrM6fjSszm9pK&index=24)\n\nRecently, OpenAI made a splash by announcing a new “Superalignment” team. Lead by Jan Leike and Ilya Sutskever, the team would consist of top researchers, attempting to solve alignment for superintelligent AIs in four years by figuring out how to build a trustworthy human-level AI alignment researcher, and then using it to solve the rest of the problem. But what does this plan actually involve? In this episode, I talk to Jan Leike about the plan and the challenges it faces.\n\nTopics we discuss:\n\n*   [The superalignment team](#superalignment-team)\n*   [What’s a human-level automated alignment researcher?](#ai-ai-alignment-researcher)\n    *   [The gap between human-level automated alignment researchers and superintelligence](#human-level-alignment-researcher-vs-superintelligence)\n    *   [What does it do?](#what-does-it-do)\n    *   [Recursive self-improvement](#rsi)\n*   [How to make the AI AI alignment researcher](#making-the-alignment-researcher)\n    *   [Scalable oversight](#scalable-oversight)\n    *   [Searching for bad behaviors and internals](#bad-behaviors-internals)\n    *   [Deliberately training misaligned models](#deliberately-training-misaligned-models)\n*   [Four year deadline](#four-year-deadline)\n    *   [What if it takes longer?](#what-if-it-takes-longer)\n*   [The superalignment team and…](#superalignment-team-and)\n    *   [… governance](#governance)\n    *   [… other OpenAI teams](#other-openai-teams)\n    *   [… other labs](#other-labs)\n*   [Superalignment team logistics](#logistics)\n*   [Generalization](#generalization)\n*   [Complementary research](#complementary-research)\n*   [Why is Jan optimistic?](#why-is-jan-optimistic)\n    *   [Long-term agency in LLMs?](#long-term-agency-llms)\n    *   [Do LLMs understand alignment?](#do-llms-understand-alignment)\n*   [Following Jan’s research](#following-jans-research)\n\n**Daniel Filan:** Hello, everybody. In this episode, I’ll be speaking with Jan Leike. After working for four years at DeepMind on reinforcement learning from human feedback and recursive reward modelling, in early 2021 Jan joined OpenAI, where he now co-leads the recently-announced superalignment team. For links to what we’re discussing, you can check the description of this episode, and you can read the transcript at axrp.net. Welcome to AXRP.\n\n**Jan Leike:** Thanks a lot for having me.\n\nThe superalignment team\n-----------------------\n\n**Daniel Filan:** Yeah, not at all. So, first of all, I guess we’re going to be talking about [this announcement](https://openai.com/blog/introducing-superalignment) of the superalignment team. For people who somehow haven’t heard of that or haven’t read [that blog post](https://openai.com/blog/introducing-superalignment), can you recap what it is and what it’s going to be doing?\n\n**Jan Leike:** Yeah, I’m excited to. So, basically, we want to set us an ambitious goal of solving alignment of superintelligence within the next four years. So, by mid-2027. And [Ilya Sutskever](https://www.cs.toronto.edu/~ilya/), the co-founder and chief scientist of OpenAI is joining the team. He’s co-leading it with me. And OpenAI is committing 20% of the compute secured so far to this effort, or to the effort of aligning superintelligence. And so, we’re staffing up the effort a lot. We are hiring a lot of people. In particular, we’re interested in hiring machine learning researchers and engineers who haven’t really worked that much on alignment before, because we think there’s a lot of scope for them to contribute and have a really big impact. Yeah, we have a general overall plan of how we want to approach the problem that involves training a roughly human-level alignment researcher that can work automatically, and then ask that automated alignment researcher to figure out how to align superintelligence.\n\n**Daniel Filan:** Okay.\n\n**Jan Leike:** And so, one of the key pieces for us to do would be to figure out how to align this automated alignment researcher.\n\nWhat’s a human-level automated alignment researcher?\n----------------------------------------------------\n\n**Daniel Filan:** Okay. Yeah. I’d actually like to get into this. I think in the blog post, you used the phrase “human-level automated alignment researcher”, right? What should I imagine here? What is that?\n\n**Jan Leike:** Yeah. So basically, we want to offload as many of the tasks that we do when we’re doing alignment work to an automated system \\[as possible\\]. So typically, when you’re using LLMs or if you’re building an AI system in general, the skill profiles they have isn’t exactly what a human would do. Right? They would be vastly better at some things, like language models are now on translation or knowing facts and so on. And then the AI system would be significantly worse at some other tasks, like language models are right now with, for example, arithmetic. And so, the question then becomes: what are the kind of tasks that we can offload to the AI systems, in which order? And as we are doing this, you’d expect humans would focus more and more on the tasks that we are not offloading. And so, as we go into that process, AI systems are doing a larger and larger chunk of the overall work, and human researchers will basically thus be more and more effective at actually making progress.\n\n**Daniel Filan:** Okay. So, should I imagine something like, instead of you replace the first OpenAI alignment team employee, and then the second one, \\[we\\] should imagine you replace this type of task that everyone is doing, and then this type of task that everyone is doing, roughly that kind of thing?\n\n**Jan Leike:** Yeah. That’s how I picture it going. And then, I think in order to actually get a lot of work out of the system, right, you would want to have 99 or 99.9% of the tasks being automated, because then you have effectively 10x, 100x, 1,000x as much research output.\n\n**Daniel Filan:** Okay. What kinds of tasks are you imagining it doing?\n\n**Jan Leike:** So broadly, I would throw them into two different buckets. One bucket is the tasks that look more like traditional ML engineering research that you would do if you were just trying to make AI systems be more capable. And then, the other bucket is all the other things that we have to do on alignment. And so, in the first bucket, this is stuff like you’re implementing ML experiments, running them and looking at the results. And the second bucket, it’s more like how do you, for example, figure out what experiments you should run to improve [scalable oversight](https://en.wikipedia.org/wiki/AI_alignment#Scalable_oversight), or how do you make progress in interpretability, right? These are really big, high level questions.\n\nBut there’s also just a lot more detailed questions. \\[E.g. say\\] you have a given point that you are in research - let’s say, you have just written a paper, and you’re like, “okay, what do we need to do next if we continued down this route?” And so, I expect that, basically, ML in general will get really good at the first bucket of just designing, running experiments automatically. And our job of differentially accelerating alignment progress would be to figure out how to automate the second bucket.\n\n**Daniel Filan:** Okay. And so you’re conceiving the second bucket as the full stack, from coming up with research directions to coming up with ideas of what things might work to all the way down to “what script do I run right now?”\n\n**Jan Leike:** Yeah. I mean, you could ask me: if I think that alignment research is so similar to machine learning research, how much is there really in the second bucket? But I think there’s actually a lot in there and it’s highly leveraged, because alignment as a problem is still so vague and confusing. And I think, in general, there’s a lot of disagreement among experts around the most promising directions or what we should do next. And so, the more you can accelerate what we do there, it will actually have really large impact.\n\n**Daniel Filan:** Okay, cool.\n\n**Jan Leike:** This is basically the same pitch that you would give for a researcher to join the field, right?\n\n**Daniel Filan:** Yeah.\n\n**Jan Leike:** We’re still trying to figure out the basics. It’s a wide open research problem. We don’t know how to align superintelligence or even systems that are significantly smarter than humans.\n\n**Daniel Filan:** It makes sense. Yeah, it’s like we want to recruit AI, just like we want to recruit more people, I guess.\n\n**Jan Leike:** That’s right.\n\n**Daniel Filan:** All right.\n\n**Jan Leike:** But there’s something really beautiful about recruiting AI, which is it scales so much better and faster than humans do, because all you need to do is buy more GPUs and then you have more AI.\n\n### The gap between human-level automated alignment researchers and superintelligence\n\n**Daniel Filan:** Makes sense. So one question I had is: when you said a human-level alignment researcher, it seems often in AI, most things aren’t exactly human-level at anything, right? So you mentioned chat models. I think they’re superhuman just in terms of breadth of knowledge, right? I think it would be hard for anyone to know as many facts as GPT-4 does. But \\[they’re\\] subhuman at arithmetic, at least if the human’s allowed to have pen and paper, you know? So, how important is the ‘human-level’ qualifier? On these lists of tasks, if it’s really superhuman at some of them, is that a problem for you or is that just so much the better?\n\n**Jan Leike:** Yeah, I think the question is really how risky is it to run that system on the task of alignment research? Because if it knows a lot of facts, that isn’t particularly scary, but what we really need to figure out is if we let the system take over some amount or ultimately almost all of our alignment research, will it lie to us? Will it try to deceive us? Will it try to take the opportunity to take over? Because now, it’s doing so much stuff that we can’t look at \\[it all\\] ourselves. And so, the question is the kind of skillset that you would need to do this, how does it compare to the kind of skillset that we would need to get a lot of assistance in alignment research?\n\nAnd if you zoom into that question, what are actually the things that we would be worried about? This is like, how good is it? Is the model spinning really coherent lies or being deceptive or pretending to do something or believe one thing and then actually wanting another? I think another really key capability here is self-exfiltration. So, how good would the model be at breaking the security precautions and accessing its own weights and trying to copy it somewhere else on the internet? Or persuading an engineer with access to the weights to download them and send them somewhere? And so, we can specifically measure how good the models are at that and then we can compare it to measuring and how good is it at actually helping us with alignment research?\n\n**Daniel Filan:** Okay. And so roughly the idea is: you want the models to not be too good at these scary tasks?\n\n**Jan Leike:** That’s right.\n\n**Daniel Filan:** Yeah. So, this actually relates to a critique of this line of research that basically says: okay, if I want a human-level automated alignment researcher, it needs to be pretty smart, it needs to be creative, right, it needs to think of things that we haven’t thought of yet, it needs to be able to plan towards a goal - I want to get this, so I’ve got to do these non-obvious things in the way, I’ve got to learn things about the world… And it’s also got to be really good at thinking about misalignment, right, in order to solve misalignment problems. And so one might think, “oh, that combination of things, that’s inherently scary or dangerous.” And I guess the question almost is, then: if the task is you’re building something, you’re aligning this automated alignment researcher, do you even have any problems left for it to solve?\n\n**Jan Leike:** Yeah. I think, ultimately, this is an empirical question. It’s really difficult to know in which order which skills get unlocked when you scale up the models. There’s a lot more work aimed at predicting emerging capabilities now and I’m really excited about that. I think it’ll give us some chance of actually predicting what the next pre-trained model will be like. But I think we can make some high-level arguments. So for example, one thing that is pretty clear is once you have the model \\[so\\] good you can hand a lot of alignment research off to \\[it\\], wouldn’t it then also be able to just improve its own capabilities? Or it can do a bunch of ML research, so it can run experiments on improving compute efficiency or something. And then, you could use that and pre-train a much more capable model shortly thereafter.\n\nAnd I think the story sounds, on the surface, appealing. But I think in practice it will be actually a lot more complicated, because you’re not doing big pre-trained runs every week. They usually take a few months, and so it would be a few months before you actually have that. In the meantime, you still get to use the system. I think the other thing is also: there’s kind of an open question of how much low-hanging fruit there still is on actually having compute efficiency wins. And I think, ultimately, the argument here I would make is that, right now, the existing community of people who are trying really hard to get AI to go faster and be more capable is already quite large relative to the alignment community. And so, if you get to automate a lot of these tasks and both communities benefit equally, then I think actually alignment benefits a lot more because it’s a smaller community, and so we don’t have to do these tasks anymore.\n\n**Daniel Filan:** Sure. So, I took the plan to be something like: we’re going to make this automated alignment researcher and then it’s going to help us with alignment. And given that in order to make an automated alignment researcher, it needs quite a lot of pretty smart, pretty good capabilities, what problems does it need to solve that we wouldn’t have needed to solve in order to get it?\n\n**Jan Leike:** I’m not sure I understand your question. Maybe I can answer the second part of the previous question that you asked.\n\n**Daniel Filan:** Sure.\n\n**Jan Leike:** Which is, what about long run goals? What about creativity? It seems like, to me, at least, that language models or AI in general has proven to be, on average, more creative than humans, I would say. If you look at, I don’t know, the diffusion model images or if you sample from a pre-trained base model or something, there’s a lot of really wild stuff in there and there’s a lot of creativity that I think you would really struggle to get out of a single human or a small group of humans, just because the model has seen the whole range of everything humans have said or all the images on the internet. And so, they can sample actually from the whole distribution. Whereas, individual humans typically can’t.\n\nAnd then, in terms of long-run goals, I think this is actually not needed at all, because we can hand off pretty small well-scoped tasks to AI systems, that if they really nail those, it would actually be really useful, right? And that could be really small range things, like “here’s the paper that we just wrote, please suggest some next steps or some new experiments to do”. And if you imagine having a really A-star researcher that you can ask these questions, they don’t have to pursue long-run goals, right? They only have to optimize over the next, I don’t know, few thousand tokens. And if they do that super well, then you would get a lot of value out of them.\n\n**Daniel Filan:** I guess. That seems like it’s in tension with this aim to automate 99.9% of alignment research, right? Because I would think that thinking “what things do we need in order to get an aligned AI?”… I would think that’s a significant chunk of the difficulty of doing alignment research.\n\n**Jan Leike:** That’s right. So what I wanted to say is the system’s adding a lot of value by really excelling in these tasks, right? And then, what you do is you have a whole portfolio of these tasks. Some tasks will be like “write the code that implements these experiments”, and then there’s another task that’s like “look at the results and tell me what you see, or suggest what to do next”. And now, once you have done these tasks, you compose them using some general recipe, as people do with [Auto-GPT](https://github.com/Significant-Gravitas/Auto-GPT) or language model programs, where each of the tasks is small and self-contained, and so the system doesn’t need to pursue long-run goals.\n\nOr for example, [the recent work](https://arxiv.org/abs/2305.20050) that came out from OpenAI on using process-based feedback on math where instead of training on “did the system get the right solution?” and just doing RL on that, you train a reward model from human feedback on every step in the proof. And it turns out, that’s actually much more effective because it gives the AI system a much more fine-grained way to learn and more detailed feedback. Now, is that going to be competitive with doing end-to-end RL on did you get the solution right in the long run? That is very unclear. But at the very least, you can use this kind of broken-down step-by-step setup, get the system to do a lot of really useful things that humans would’ve done, and then piece it together.\n\n**Daniel Filan:** Yeah. Although even the small tasks… so one of the small tasks you mentioned was “look at results and decide what to do next”. I guess I would’ve thought that in order to do that you have to have the big picture in mind and think “what next project is most useful for my goal of solving superalignment in four years?”, right?\n\n**Jan Leike:** That’s right. But you wouldn’t do this in the sense that you’re trying to optimize and do credit assignment for four years. It’s probably more like, well, you’re just adding some broader goals and context into your prompt. But when you’re actually doing reinforcement learning or reinforcement learning from human feedback to improve the system, then you don’t have to wait until that research project concludes to decide whether or not it’s good. It’s just, you use the human as reward shaping; you’re just like “does this look like a good direction that seems better than any directions I could have thought of?” or something. And so, I think the overall goal here is not to build the most capable automated alignment researcher that we possibly could with the tech that we have, \\[but\\] rather build something that is really, really useful that we can scale up a lot, and most importantly, that we trust is aligned enough to hand off these tasks to.\n\nAnd so, if we introduce a fair amount of inefficiencies in these processes where we’re essentially sandbagging the model capabilities a whole bunch by training it this way and we are like, “well, we’re giving it these broken-down tasks and now it would execute those, but if we train it end-to-end, it would be more capable” or something. I don’t think that matters as much. So this is typically what people call [the alignment tax](https://www.alignmentforum.org/tag/alignment-tax). And the alignment tax matters a lot if you’re, let’s say, competing in the market with other companies: I’m building a chatbot or something and my chatbot is more aligned but it seems a lot less capable. That would have a hard time competing in the market. But if you have an automated alignment researcher, the automated alignment researcher doesn’t have to compete in the market, it just has to be useful to us. And so we can get away with paying a higher tax because we just don’t have a replacement, or the real replacement is hiring more humans, which doesn’t scale that well.\n\n### What does it do?\n\n**Daniel Filan:** Okay. I guess another way to ask the question I was going to ask previously is: what problems will you want this automated alignment researcher to solve?\n\n**Jan Leike:** I mean, ultimately it should solve the problem of “how do we align superintelligence?”\n\n**Daniel Filan:** Okay. So is the idea that we solve the problems of how to align something roughly human-level and then there’s additional problems as it gets smarter and it just starts taking on those?\n\n**Jan Leike:** Yeah, basically. So I imagine that an actual solution to aligning superintelligence that we and lots of other people actually truly believe in will look quite different from what we do today. If you look at how ChatGPT is aligned today, it’s a lot of [reinforcement learning from human feedback](https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback). And there’s a widely-shared belief, that I share, that that just won’t scale because it fundamentally assumes that humans really understand what the system is doing in detail. And if the system is doing a lot of alignment research where you’re thinking of millions of virtual human equivalents or something, there’s no way you’ll be able to look at all of it and give detailed feedback. And it’s a really difficult task and you’ll miss lots of important bugs. But the kind of techniques we’re looking at right now to scale this and align a roughly human-level alignment researcher that can do these difficult tasks but won’t do it crazily differently than humans would, are steps or continuations of that, where scalable oversight, for example, is a natural continuation of reinforcement from human feedback.\n\n**Daniel Filan:** What is that?\n\n**Jan Leike:** Scalable oversight?\n\n**Daniel Filan:** Yeah.\n\n**Jan Leike:** So scalable oversight I would define as generally a portfolio of ideas and techniques that allow us to leverage AI to assist human evaluation on difficult tasks.\n\n**Daniel Filan:** Okay. So scalable oversight is an example of the thing that you could build off of reinforcement learning from human feedback, often called RLHF.\n\n**Jan Leike:** Yeah. And so typical examples of scalable oversight are [debate](https://arxiv.org/abs/1805.00899), [recursive reward modeling](https://arxiv.org/abs/1811.07871), [iterated distillation and amplification](https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616), [automated market making](https://www.alignmentforum.org/posts/YWwzccGbcHMJMpT45/ai-safety-via-market-making), and so on. And there’s a bunch of ideas floating around. But what I actually wanted to say, to get back to your original question, is I think if we are actually aligning superintelligence, which is this system that is vastly smarter than humans, can think a lot faster and run at a much larger scale, that just introduces a whole lot of other problems, especially because it will be super general, can do lots of tasks, and then you have to figure out how to align it not just on the much narrower distribution of alignment research-y tasks, but everything else. And also, you want to have a much higher degree of confidence that you’ve actually succeeded than you would get with, let’s say, a bunch of empirical evaluations.\n\nAnd so I don’t really know what that would look like and I don’t think anyone does, but I think it would be really exciting if we can have some formal verification in there. Maybe we’ve figured out some kind of learning algorithm that has theoretical guarantees. I don’t know what even would be possible here and theoretically feasible if you have a lot of cognitive labor that you can throw at the problem. But all of these things are very different from the kind of things that we would do right now or that we would do next. And I also don’t think that a roughly human-level alignment researcher would start working at those problems right away. And instead what they would want to do is, we would want them to figure out how to better align the next iteration of itself that then can work on this problem with even more brain power and then make more progress and tackle a wider range of approaches. And so you kind of bootstrap your way up to eventually having a system that can do very different research that will then allow us to align superintelligence.\n\n**Daniel Filan:** Gotcha. So once you have these human-level AI alignment researchers running around, do you still have a job? Does OpenAI still have a human superalignment team?\n\n**Jan Leike:** Yeah, good question. I mean, I would be excited to be replaced by AI to be honest. But historically what typically happens is what we mentioned earlier: the AI assistant does 99% or 99.9% and then we just do the rest of the work. And I think also something I’m really bullish on is making sure that humans stay in the loop or stay in control over what AI systems are actually doing, even in the long run when we’re long past really being able to understand everything they’re doing. And so there’ll be some humans that still have to try to understand what the high level is of what the AI is trying to do. So that wouldn’t necessarily have to be the superalignment team that is at OpenAI now. It might also require a very different skillset than we have right now. But I think ultimately humans should always stay in the loop somehow.\n\n### Recursive self-improvement\n\n**Daniel Filan:** Okay, gotcha. So yeah, actually speaking of… I guess the loop? Wow, that’s a bad segue, but I’m going with it. So one thing that OpenAI has mentioned in a few blog posts is, so firstly there’s this idea that safety is actually pretty linked to capabilities. You need smart models to figure out the problems of alignment. Another thing is that there’s this desire to avoid fast takeoff. And I think there’s this quote from [‘Planning for AGI and beyond’](https://openai.com/blog/planning-for-agi-and-beyond) that says “it’s possible that AGI capable enough to accelerate its own progress could cause major changes to happen surprisingly quickly”. And then it says “we think a slower takeoff is easier to make safe”. So one thing I wonder is, if we make this really smart or human-level alignment researcher \\[so\\] that we then effectively 10x or a 100x or something the size of the alignment team, does that end up playing into this recursive self-improvement loop?\n\n**Jan Leike:** I mean, it really has to, right? You can’t have a recursive self-improvement loop without also improving your alignment a lot. I mean, I personally think fast takeoff is reasonably likely and we should definitely be prepared for it to happen. And if it doesn’t happen, I’m happy about that.\n\n**Daniel Filan:** How fast are we talking?\n\n**Jan Leike:** I mean, ultimately, I don’t know. But you can draw some parallels to some other machine learning projects like AlphaGo or Dota or StarCraft where the system really improved a lot week over week. Yeah, there’s obviously a lot of uncertainty over what exactly happens, but I think we should definitely plan for that possibility. And if that does happen, a really good way to try to keep up with it is to have your automated alignment researchers that can actually do thousands of years of equivalent work within every week. And there’s just no way that that’s going to happen with humans.\n\nHow to make the AI AI alignment researcher\n------------------------------------------\n\n**Daniel Filan:** Okay. Now that we have a better sense of what we’re looking at with a human-level automated alignment researcher, can you give a sense of what the plan is to make one?\n\n**Jan Leike:** Yeah, so I mean, you basically need two parts. One is you need a system that is smart enough to do it, and then the second part is you need to align it to actually do it. And I think they’re not two separate parts, I think they’re very intimately linked. And I’m personally not working on the first one, I think that will definitely happen and a lot of people are working hard at making it happen. And there’s lots of different ways that could go, but you could just picture training a much larger model and eventually it will just be smart enough. And on the second part, that’s the part I’m really interested in. And so the question is: now I have this really smart pre-trained model, how do you get it to actually do alignment research in the way that you would want it to? Or more importantly or more critically, how do you know whether you can trust it enough to do that?\n\nBecause if you’re handing this kind of task off to AI, there’s this opportunity that a smart AI system could just try to figure out how to present results and techniques to you in a way that if you then decided to implement them would actually empower the system itself or some other related system and give it more opportunity to have more power-seeking behavior in the future, or let’s say self-exfiltrate, for example. And so what we need to do is, one, actually align it to do the task and leverage its capabilities to do it basically as well as it could. So that is what I would call, we need a bunch of alignment training methods. And then we also need to be able to distinguish a model that is actually aligned on the task and tries to do alignment research really well and actually help us and be honest and be truth-seeking, and a system that pretends to be that to us but actually wants to take over the world or (say) self-exfiltrate or do something else. And these are two different systems, one is basically the truly aligned one and one is the deceptively misaligned one.\n\nAnd an important part of our job is figure out how to distinguish these two systems. And that will require a bunch of methods that I would call validation methods. So these are methods that we wouldn’t really train on in a sense that the AI systems wouldn’t be directly selected on passing the validation tests, and more they will give us some kind of independent signal of “which one of these two versions is it?” And so examples here are interpretability research, for example. If I can look inside the model and find some evidence that it’s secretly plotting against us, that will fail the validation test. Another technique that could either be training or validation, depending on how it’s set up, is this easy-to-hard generalization problem. Can you understand and improve the model’s ability to generalize from easy problems that we can supervise to harder problems that we struggle to supervise? And if you can do that, now you can just supervise the model on easy parts and then you crosscheck on the difficult ones where you would worry that the model might secretly deceive you or try to do something else.\n\n### Scalable oversight\n\n**Daniel Filan:** Okay, cool. So yeah, there’s a few things there. I guess I’d like to talk about the scalable oversight part first. How are we going to do this? Especially, as background, I think we live in a world where there’s not a lot of consensus on what good alignment research is. So how are we going to get a training signal for what counts as good alignment research?\n\n**Jan Leike:** Yeah, exactly. And I think that the reason that there isn’t a consensus is good evidence that the problem is actually hard. But also it tells us that, well, the field is still immature and we haven’t gotten that much empirical evidence so far. But I think there’s some really important property that alignment research has that we can leverage for scalable oversight, which is that I think it’s fundamentally easier to evaluate alignment research than it is to do it. And \\[that\\] doesn’t mean it’s easy to evaluate, but it is much easier to find, for example, a paper that has a cool idea and does some cool experiments and gets some good results and you read that and you’re like, “Oh, actually this is a really good idea, I think this is good,” than it would be to produce that work in the first place.\n\nAnd so leveraging this principle that evaluation is easier than generation is the core of a lot of scalable oversight ideas. And so for example, if you’re thinking about [recursive reward modeling](https://arxiv.org/abs/1811.07871), where the basic idea is you have some kind of AI system that you use as an assistant to help you evaluate some other AI system. So now, because evaluation is easier than generation, the task that the assistant has to do is a simpler task, especially because you are working together with it. So actually aligning the assistant on the simpler task of the evaluation assistant, where let’s say the claim is that that would be easier, and if you succeed at that task, then now you use your human/assistant conjunction to supervise a new system on an even harder task. And so if you keep doing that, you’re unlocking a broader and broader scope of tasks that you can effectively supervise your AI systems on.\n\n**Daniel Filan:** So somehow the idea is that we’re iteratively adding more and more AI knowledge basically to the evaluation portion of AI alignment research. And by doing it this iterative way, the idea is that we can consistently give it a good training signal?\n\n**Jan Leike:** Yeah.\n\n**Daniel Filan:** Okay.\n\n**Jan Leike:** Let me make this a little bit more concrete. So for example, [RLHF](https://arxiv.org/abs/1706.03741) is the simplest one where you don’t use any assistant. Your AI system does something, a human looks at it and says whether it’s good, and that’s your training signal.\n\nAnd now, the next step up from that is, basically, you train the simplest assistant model, which is just a critique model. You have a separate language model that looks at what the first AI system did, and then it writes a critique. So let’s say the first AI system writes some piece of code, you look at the code: humans are notoriously bad at finding bugs in code, that’s why there’s so much buggy code out there in the world, but now, if you have a critique system that writes a critique and it points out the bug, it’s so much easier for the human to say, “Oh yeah, this is totally a bug. We should fix that”.\n\nAnd it’s also not a super crisp task, necessarily, because usually, the code is written for some natural language specification. What is actually meant by that is somewhat vague, and actually, whether something is a bug, can be ambiguous. But the important point here is that, because you’re using the critique model as an assistant, and if \\[you have\\] the critique model, now, you can supervise a wider range of tasks, because essentially, you could find all the problems in code that your AI system would find or would’ve written a critique, which might not be exactly the same as all the code where it knows there’s a bug in there, but you have expanded the range of things that you can supervise.\n\nThe really nice thing is that, I think, actually, there’s a bunch of ways in which we can actually empirically measure how well this works. There’s one way, which is kind of what we did in [the critiques paper](https://arxiv.org/abs/2206.05802) that we published last year, where you basically do randomized controlled trials with targeted perturbations. So you take, let’s say, a coding task. In the paper, we didn’t do a coding task, but the task doesn’t matter here. And then, you have a response. The response does not even have to be perfect, but let’s just assume it is. What you do is you introduce some subtle bug in there, some subtle flaw that would make it a bad response, but you want it to be hard to find for humans.\n\n**Daniel Filan:** How do you introduce this bug?\n\n**Jan Leike:** The simplest thing is just you ask humans to do it. But ultimately, you want to do it with the model, and I’ll get back to that. Now, you have this paired response. You have the original response, and you have the response that you know is worse because you made it worse. And now, you draw one of them at random and you show it to a human or a human assisted with AI or whatever your scalable oversight method is. And then, you see how likely are they to find it? Or you just ask them, “Is there a bug here?” And now, you can measure, if you have a better way to assist them, you can see how often they find the bug.\n\nBut you can do even more, which is, you can directly compare this to how much does the model know that there’s a problem here by training it, essentially, to be a discriminator between the good version and the flawed version. And if you look at the accuracy for that, that’s the discriminator performance. And then you check that with… if you ask the model or the RLHF version of the model to write a critique of the code, how often does it actually write about the flaw? Now, you get this critique accuracy equivalent, and now you can compare these two numbers. That’s what we call the discriminator-critique gap.\n\nAnd if you have a large discriminator-critique gap, that means that there would be a lot of problems that the model can smell or knows about or whatever metaphor you want to use, but it’s not telling us because if we asked it to write a critique, it doesn’t say it. And a large discriminator-critique gap would be very worrying because it means that your scalable oversight is somehow very flawed and there’s a lot of things that slip \\[under\\] your radar, or the assistant is, in a sense, not very aligned because it’s not trying very hard to help you.\n\nAnd if the discriminator-critique gap is pretty small, that is somewhat reassuring because it means, if the model knew there was a problem there, it would tell you. So in particular, if a similar sized model or the same base model was trying to plant a problem, or it was trying to, let’s say, put Trojans in your code that is hard for you to find, but if you did run the code, the Trojans might fire, and then it could take over or do something else. Then, presumably, it would know that these Trojans could be there because it planted \\[them\\] itself, right? So if we have this other copy of the model, then, if we measure a small discriminator-critique gap, we’d have more confidence that this can’t be the case because the other copy of the model would presumably then write a critique.\n\n**Daniel Filan:** Yeah.\n\n**Jan Leike:** And this kind of hinges on the assumption that discriminator training actually can take out of the model this kind of thing. And I think there’s good reason to believe that because, if you’re fine-tuning for a discriminator, essentially, you have a way to access all of the activations that are everywhere in the model because you can fine-tune all of the weights simultaneously. So if the knowledge is somewhere in the model, in its model’s activations, then a discriminator fine-tuning on, if you do it right and you have enough data, it should be able to extract it.\n\nAnd in fact, you can give some back-of-the-envelope calculations of how much data you would need. And if you want to identify one neuron that… This is not how it works, but hypothetically, if there was one single neuron that knew if the code is bad, you would need the log of the number of total neurons data points to narrow it down and find it, which is not that many.\n\n**Daniel Filan:** Yeah. I guess this actually gets to a question I have about the whole scheme. The reason we didn’t like RLHF is that there was this concern that it wasn’t going to distinguish between things that were right and things that sounded good to the human, because the human was bad at critiquing.\n\nAnd with this recursive critique thing with training a discriminator and then a critiquer, I wonder if we still have that fixed point. So one world is, we have a thing, we train it to write actually-good critiques, and then, we join ourselves with it to train a thing that writes even better critiques. But I worry, maybe there’s another fixed point where we train a thing and we accidentally train it to write pseudo-critiques that sound good to us and we join with it, and then, we’re even better at looking for critiques that sound good to us. And then, we trained this discriminator to discriminate between things that don’t have problems and things that seem like problems to us. I’m wondering, yeah, how worried should we be about things that seem good to us rather than things that are actually problems being a fixed point here?\n\n**Jan Leike:** Yeah, I think that’s a really important, valid concern to have. And in a way, that’s one of the key things we need to figure out about scalable oversight. This is the reason why I’m excited to actually implement these techniques and try them out empirically because I think we will just be able to measure how much this effect shows up in practice.\n\nAnd one concrete, simple way we can measure it is, we actually just deliberately train a deceptively-aligned model, and then, we see if the scalable oversight flags it, or how hard is it to… or how much effort do we have to put into making the model deceptively aligned such that it would pass our scalable oversight.\n\n**Daniel Filan:** Yeah, I guess there’s also these [sandwiching](https://www.alignmentforum.org/posts/PZtsoaoSLpKjjbMqM/the-case-for-aligning-narrowly-superhuman-models#Potential_near_future_projects___sandwiching_) [experiments](https://arxiv.org/pdf/2211.03540.pdf) where you have an AI that’s trying to answer legal questions. There’s some people who don’t know much law, and then, you get some people who are really good at law and hopefully, they can check if the AI has fooled the people.\n\n**Jan Leike:** Yeah. I think these experiments are interesting, and there’s some interesting work on this already. I think there are some fundamental problems to this setup.\n\nOne is, well, you really have to trust your experts, and the experts are going to be wrong sometimes. And then you have to have sufficient domain overlap. We did a little bit of preliminary experiments with this where we had non-coders who were supposed to use assistance to figure out problems in code, and they didn’t even know how to ask the right questions. So you have to have some overlap.\n\nAnd then, the other problem is also that, I think the task that I’m fundamentally really interested in, we just don’t have ground truth on. And even if we, let’s say… alignment research, we’re doing alignment research now, we can be like, “Oh, we’ll just get some alignment researchers to label some tasks.” And then I’d be like, “Well, what if they are wrong? We don’t agree about so many things. That seems hard.” And then, also, there’s not that many and their time is very expensive, so that would be extremely expensive data.\n\nSo in general, I would want to have an evaluation method that doesn’t rely on the assumption that we have ground truth. And this is why I’m excited about the prospect of using these randomized controlled trials with targeted perturbations or measuring the discriminator-critique gap. Because you can do this even if you don’t have any ground truth, and the task can be arbitrarily hard.\n\n**Daniel Filan:** Yeah. Although, even in those - measuring the discriminator-critique gap, it’s still got to be the case that you have an actual discriminator versus a discriminator of things that seem like problems versus things that don’t seem like problems, right?\n\n**Jan Leike:** You mean you can have AI systems introduce the flaws, right? And in a way, that is also much better than humans doing it because it’ll be a more honest distribution for actual\\[ly\\] what AI systems would do. And if you’re fine-tuning your discriminator on that data, you actually do have ground truth if you can trust that the flawed version is actually worse.\n\n**Daniel Filan:** And I guess it’s-\n\n**Jan Leike:** And the way you could trust that is, well, you look at the reason why it’s worse and you can verify that reason, which is a lot easier.\n\n**Daniel Filan:** Yeah, I guess there’s some hope that, even if the AI can make you think things are good that aren’t actually good, maybe if the AI makes you think something’s bad, it is actually bad? Or that it’s degraded the performance, that if the AI makes you think it’s degraded performance, maybe it’s easier to check that it’s degraded performance?\n\n**Jan Leike:** Yeah, I see your point. I probably shouldn’t actually use the word ground truth in that setting because it is not truly ground truth, just as nothing really is truly ground truth, but there’s a variety of things that you can do to make you very confident in that in a way that doesn’t necessarily make the task of finding the problem easier.\n\n### Searching for bad behaviors and internals\n\n**Daniel Filan:** Okay. Gotcha. I think I want to talk a little bit about the next stage of the plan. Searching for bad behavior and bad internals, I think, was how it was put in [the superalignment post](https://openai.com/blog/introducing-superalignment). What open problems do you see here that you’d want the superalignment team to be able to solve?\n\n**Jan Leike:** Yeah. The obvious candidate here is interpretability, right? In a way, interpretability is really hard. And I think, right now, we don’t really have any slam dunk results on language models where we could say interpretability has really given us a lot of insight or added a lot of value. And this is because we’re still pretty early in understanding the models and what’s going on inside.\n\n**Daniel Filan:** So there are some results… people do some interpretability things to language models. There’s this [induction heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) work, there’s this [indirect object identification](https://arxiv.org/abs/2211.00593) work for a circuit that does at least a certain type of indirect object identification. I’m wondering, what would you need beyond those to get what you would consider a slam dunk?\n\n**Jan Leike:** Yeah, sorry, I don’t mean to diminish existing work here, and I think–\n\n**Daniel Filan:** You can score in basketball without getting a slam dunk, right?\n\n**Jan Leike:** Yeah. And I think very cool things have happened, but I think something that I would consider a really cool result is, you use your interpretability techniques on a language model reward model like a GPT-4 size or whatever your largest model is, and then you say something about the reward model that we didn’t know before. And that would be really cool because the reward model is what provides the training signal for a lot of the RLHF training, so understanding it better is super valuable. And if you can flag or find problems of behavior that it incentivizes that you wouldn’t want, that would be really cool.\n\nI think that is doable. And I think, importantly… In that sense, I think interpretability is neither necessary, nor sufficient. I think there is a good chance that we could solve alignment purely behaviorally without actually understanding the models internally. And I think, also, it’s not sufficient where: if you solved interpretability, I don’t really have a good story of how that would solve superintelligence alignment, but I also think that any amount of non-trivial insight we can gain from interpretability will be super useful or could potentially be super useful because it gives us an avenue of attack.\n\nAnd if you think about it, I think it’s actually crazy not to try to do interpretability because, in a way, you have this artificial brain and we have the actually perfect brain scanner, right? We can zoom in completely and fully, accurately measure the activation of every single neuron on each forward pass, so in arbitrary, discrete timestamps. That’s the maximal resolution that you could possibly want to get. And we can also make arbitrary interventions. We can perturb any value in the model arbitrarily how we want. That gives us so much scope and opportunity to really do experiments and look at what’s happening. That would be crazy not to leverage.\n\nBut at the same time, the reason why it’s very hard is because the model is learning how to do computations tailored to efficiency, and it’s not regularized to be human-understandable, or there’s no reason to believe that individual neurons should correspond to concepts or anything near what humans think they are or should be or what are familiar to us. And in fact, empirically, neural networks represent many different concepts with single neurons and every concept is distributed across different neurons. So the neurons aren’t really what matters here.\n\nBut one path… I think there’s two things on interpretability that I’m super excited about. One is the causal version of it. You want to not only look at a neuron as you’re passing data through the model and say, “Oh, this fires when we have stories about Canada,” or something. This is one of the things that we found in the interpretability paper. We had this ‘Canada’ neuron that would fire at Canada-related concepts. But it’s only correlationally. You observe a correlation between Canada-related concepts and that neuron firing, but it’s not a causal relationship. In order to check that it’s a causal relationship, you have to deliberately write text that has Canada-related concepts, see if they all fire, but also put in other related concepts that maybe sound like they could have something to with Canada or don’t have anything to do with Canada, but are similar, and then check that the neuron doesn’t fire. Or you take a text, and then edit it and check that the neuron turns off, and things like that.\n\n**Daniel Filan:** Yeah. I guess this is reminiscent of this, I think it was called [the interpretability illusions paper](https://arxiv.org/abs/2104.07143), where they noticed, on Wikipedia, you can have neurons that fire on one specific thing, but then, on other data sets, that was just an illusion, it fires on a bunch of other stuff.\n\n**Jan Leike:** Yeah. And I think the other thing that is really exciting is, the work that we’ve started last year where we released [a paper](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html) earlier this year on automated interpretability. And here, the idea is, basically, what you would want is, you would want to have a technique that both can work at the level of detail of individual neurons so that you can really make sure you don’t miss any of the details, but also can work at the scale of the entire model. Because at the end of the day, everything works together and everything is highly connected in the model, so you need both. And so far, techniques have mostly worked on one or the other. There has been attempts at… there’s been work on automated interpretability before our paper, so it’s not like we were the first ones. But I think, in general, if you can have some really detail-oriented interpretability work, some kind of mechanistic interpretability approach that really tries to understand individual circuits or computation units inside the model, the way to then scale that to the entire model is, you need automation, right?\n\n**Daniel Filan:** Yeah.\n\n**Jan Leike:** But you can do that. Once you’ve figured out how to do it on the detail, then, you just record what you’re doing. I’m simplifying a bit too much here, but I think that’s the general scope that I would be excited about, also because it really fits with our automated alignment goals. We want to have our automated alignment or interpretability researcher really look in detail into the model, understand what’s going on. Then, we sift through the whole… everything, or we find ways to aggregate it.\n\nSo for [the paper](https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html), we had a dump of explanations. The paper does, it writes natural language explanation for individual neurons, which is not quite the right thing to interpret, as I mentioned previously, but it gives you a simple example of what we could do here. The way it works is you just show a bunch of activation patterns to GPT-4 and you have GPT-4 write a proposed explanation.\n\nAnd in general, these explanations aren’t very good, also, because the task is so hard and most neurons don’t do a very clearly human-understandable thing. But we can just run this on the scale of every neuron in GPT-2, and then just dump all the explanations, and then try to find what is interesting patterns. And you can look at scaling trends and be like, “How does automating scoring of these explanations scale as models get larger?” Or, “What if we add more compute or we make the model that’s doing the interpretation bigger? How does the quality of explanations change?”\n\nAnd the really cool thing is that we can just measure this automatically, again, using language models. And it’s not a perfect metric and there’s a bunch of problems with it, but it gives you a proxy for “would a human think this explanation is good?” And then, you use this proxy at scale and just run it on a ton of neurons.\n\n**Daniel Filan:** Yeah. So, if you think about the necessary interpretability work, how much of it do you think is just figuring out a better fundamental unit of interpretation versus figuring out how to scale what’s happening? Or other things you might…\n\n**Jan Leike:** I think it needs to be both. I think it’s very reasonable to believe that the former is much harder, but I think the scaling part is needed for it to succeed.\n\n### Deliberately training misaligned models\n\n**Daniel Filan:** Gotcha. Yeah, so, I guess part 3 of the plan was something like deliberately training misaligned models and seeing if the pipeline could detect those. Are you planning to weed them out or actively fix them?\n\n**Jan Leike:** The goal here would not be to fix a deliberately trained misaligned model.\n\n**Daniel Filan:** Okay. Just to detect?\n\n**Jan Leike:** Yeah. Fundamentally, one core aspect of what we need to do here is, we need to be able to distinguish between the actual aligned alignment researcher that does what we want and that truly wants to help us make progress on alignment and the one that pretends to do that, but secretly wants to take over and self-exfiltrate. And one way we can figure out more about how good we are at distinguishing these two is, we make the second one and see if it looks like the first one to us. Obviously, you have to be very careful with doing that because you’re kind of making the scary thing that we’re all trying to avoid, right?\n\n**Daniel Filan:** Yeah. I actually wanted to ask about that. I think at the start of [the post](https://openai.com/blog/introducing-superalignment), there’s one sentence about how great superintelligence could be, and then, it says, I think, that it “could lead to the disempowerment of humanity or even human extinction.” What are we talking about with misaligned models? How do we know they’re not going to do those awful things?\n\n**Jan Leike:** Yeah. Let me put it this way. There’s a lot of things that are needed to make the future go well. We need to have the right governance structures so that we can prevent misuse of powerful AI systems, and we have to have precautions so we don’t run into some kind of emergent runaway effects that are still bad for humans where you have aligned AI, but AI is aligned with corporations who are not aligned with humans or something, and then, the whole system goes off the rails, but it looks good in all the economic metrics or something.\n\nAlso, we need to solve technical alignment so we can actually align AI systems. The risks that superalignment is looking at is just the last part. We want to figure out the technical problem of how to align one AI system to one set of human values. And there’s a separate question of what should those values be? And how do we design a process to import them from society?\n\nAnd I think these are really important questions and we really have to answer them too for the future to go well. And I think, for the future to go well, the bar is not “AI doesn’t kill literally everyone”, but I want there to be lots and lots of human flourishing and I want to have a future that everyone is really excited to live in. And the more powerful AI gets, the higher the stakes will be, right? Because there’ll be more risk from misuse and if systems are misaligned, things can go wrong a lot more.\n\nIn particular, the threat models that I think we should be particularly worried about from a misalignment perspective are systematic lies and deception and self-exfiltration.\n\n**Daniel Filan:** Okay. When you say, when you’re deliberately training misaligned models, is the idea that you would train them to lie and potentially self-exfiltrate, but at a low enough capability level that it’s fine or something?\n\n**Jan Leike:** You wouldn’t want to train the model, actually, to self-exfiltrate because you know, what if it actually gets good at that, right? You would want to pick some other proxy task that is a lot more mundane, but that still tells you that… It makes it easy to evaluate whether or not the model is succeeding at it.\n\nAnd then, the core difficulty and the core goal is really figuring out how hard it would be for a system to have the properties that it would need to still pass the evals. If you have a system that is lying to you or deceiving you, that’s really ineffective if it only does so sometimes, and then, if you ask it, “Oh, did you lie about this?”, and it says, “Yes,” it wouldn’t be a very coherent, believable liar. The scary systems are the really coherent and believable liars that lie super consistently where it would just be really hard to interrogate in a way that points out its lies or any sort of inconsistencies in what it’s saying.\n\nAnd I think, actually, to be this kind of consistent liar is not that easy. And our whole goal with what we’re trying to do with scalable oversight and interpretability and generalization is that we want to make it really, really hard for the model to be this consistent liar. But if we just do that, we don’t know how hard it actually is.\n\nAnd one way we can find out is, we train the model to be a really consistent liar, and then, we see how hard it is. How much do we have to cheat? How much do we have to give the model hints or handhold it along the way? But you don’t actually want to train a dangerous system that, if you did run it, it would successfully self-exfiltrate, because then, you’ve caused the very thing that you’re trying to prevent, and that would be really bad.\n\n**Daniel Filan:** Yeah. So when you say that, are you distinguishing lying from other kinds of danger, such that we could train it on lying, but we’re not going to train it on murder, for instance?\n\n**Jan Leike:** Yeah. Let me try to give you an example. Let’s say you’re giving it some fake goal. Maybe it is to persuade a human to say a certain catchphrase, or maybe it is to run a certain Python command that, in fact, doesn’t do anything but it looks interesting or something.\n\n**Daniel Filan:** Yeah, yeah.\n\n**Jan Leike:** And you can very well… it’s so easy to measure whether the system succeeds and you can set up the experiment so that it’s hard to do that a priori. And the goal here is not to have the system stress test, let’s say, our cybersecurity or something, although we should also do that separately. I think that’s another effort.\n\nHere, the goal is really just, how close are the systems that we’re currently training, or that we currently have, to a system like that that would be deceptively aligned, that is a coherent liar, that any chance it gets where it thinks humans aren’t looking, will run specific code or things like that. That’s exactly what we are looking for. And a priori, it’s really hard to get confidence of how close we are, unless we just push it a little bit in that direction and see if that happens.\n\n**Daniel Filan:** So the thing I’m imagining now is, you’re training it to do something kind of arbitrary other than the thing it’s supposed to be trained for such that it has to do deceptive stuff, it has to do the kinds of things it would need to do in order to cause damage, but at the end of the day, it’s just going to put a sticky note on someone’s forehead or something.\n\n**Jan Leike:** Exactly, exactly. It’s like [you’re hiring someone to test your physical security](https://en.wikipedia.org/wiki/Penetration_test), and all I have to do is get in the building and give you a handshake and you’re like, “Yeah, it seems like you succeeded at it,” or something. Or you’re like, “Okay, can you steal this fake object for me because I want to figure out how good our security is,” things like that that you can do that doesn’t have actual consequences, but still tells you a lot about your security. I’m excited to do the same with alignment where you stress test the alignment systems you have by training something that’s particularly targeted to break and circumvent them, but if it succeeds at doing that, it doesn’t cause actual harm. It’s very benign.\n\nFour year deadline\n------------------\n\n**Daniel Filan:** Okay. Cool. I’d like to ask an overall question. So in [the superalignment blog post](https://openai.com/blog/introducing-superalignment), in big letters, it says, “Our goal is to solve the core technical challenges of superintelligence alignment in the next four years.” What do you mean by the core technical challenges of superintelligence alignment?\n\n**Jan Leike:** Yeah. This would be the general technical measures, how to align a superintelligence with a set of human values. And the kind of superintelligence we’re picturing here is a system that is vastly smarter than humans, that could possibly execute much faster. You can run it a lot in parallel, it could collaborate with many, many copies of itself, so a truly vastly powerful system.\n\nAnd we want to do this in four years. And the reason why we picked the four years was, we want to have a really ambitious goal, something that feels like we can actually achieve it, and also something that we can still realistically use, even if AI progress actually happens really fast and the technology actually improves a lot over the next few years, so that we still have something that we can deliver.\n\n**Daniel Filan:** Gotcha. And just to be clear, it sounds like this isn’t just building the human-level automated AI alignment researcher. It’s also using that to just get the technical problems of aligning something much smarter than us.\n\n**Jan Leike:** That’s right. The roughly human-level automated alignment researcher is this instrumental goal that we are pursuing in order to figure out how to align superintelligence because we don’t yet know how to do that.\n\n**Daniel Filan:** Gotcha. So if, four years from now, you want to have solved these core technical challenges, where do you want to be in two years that would represent being on par to meet that goal?\n\n**Jan Leike:** Yeah. If you want to work back from the four years, I think, basically, probably, in three years you would want to be mostly done with your automated alignment researcher, assuming the capabilities are there. If they’re not there, then, our project might take longer, but for the best reasons.\n\nAnd then, the year before that, we already want to… so this would be in two years, we’d want to have a good sense for what are actually the techniques that we could use to align the automated alignment researcher. Do we have the portfolio of techniques that, if we did apply them, we would actually feel confident that we have a system that we can trust and we can use a lot, and then we can hand off a lot of work to? And then, basically, at that point, we would have wanted to have broken down the problem enough so that it feels like now, the overwhelming amount of work is just engineering, which, in that sense, would leave us roughly two years to figure out the research problems attached to it.\n\nNow, this is kind of a timeline for this goal that we set ourselves with four years, and obviously, there’s really important interactions here with how AI capability progress goes. Because if progress slows down a lot, then we might just not have a model that’s good at any of the really useful alignment research tasks. We’ve tried a whole bunch to do this with GPT-4, and GPT-4 was just not very useful. It’s just not smart enough of a model. That’s the short version of it, but I’m happy to elaborate. And so if four years from now we’re just like, “Well, we didn’t have a model that was good enough”, then also that means that we have more time to actually solve the problem because the problem isn’t as urgent. And on the other hand, it could also be that AI progress is a lot faster and we are like, “Well actually we don’t think we’ll have four years because superintelligence might arrive sooner.” That could also be possible. And then we have to adapt our plan accordingly. And so the four years was chosen as a timeframe that we can actually probably realistically plan for, but also where we have enough urgency to solve the problem quickly.\n\n### What if it takes longer?\n\n**Daniel Filan:** Yeah. So a question I think a few people have wondered about, and I’m curious about as well is: so suppose that on the AI capabilities research front, it goes roughly as expected. Four years from now, you have all the capabilities to have something that could be a good automated alignment researcher, but interpretability turned out harder than we thought, or scalable oversight turned out harder than we thought, and you guys haven’t made the mark. What happens then?\n\n**Jan Leike:** Well, we have to tell the public that we haven’t achieved our goal, right? So, we are accountable to that goal. But also what happens if we don’t make the goal depends a lot on what does the general state of the world look like. Can we get ourselves more time somehow or was our general approach misguided and we should pivot or…? A lot of things can happen.\n\n**Daniel Filan:** But basically: let people know and then figure out what the good next thing to do is and do it, roughly?\n\n**Jan Leike:** It’s a pretty high-level answer.\n\n**Daniel Filan:** Yeah.\n\n**Jan Leike:** But I think there’s something more here, which is: at least to me, it feels like the alignment problem is actually very tractable. I think there’s a lot of good ideas out there that we just have to try rigorously and measure results on and we will actually learn and be able to improve a lot. And I’ve significantly increased my optimism over the last two years that this is a very practical problem that we can just do. And I think even if I turn out to be wrong and it did turn out to be much harder than we thought, I think that would still be really useful and be evidence about the problem, and in particular I think right now there’s a lot of disagreement over how hard the problem actually is. And maybe more importantly, I think there is… One of the really important things we need to know, and be able to measure, is how aligned are our systems actually in practice?\n\nAnd I think one of the things I worry about the most is not that our systems aren’t aligned enough, but that we don’t actually really know how aligned they are. And then experts will reasonably disagree about it because if everyone agrees the system isn’t aligned enough to deploy, it won’t get deployed. I think that’s a pretty easy case, and the kind of scary scenario is more like you have this capable system and you’re like, “Well, it’s probably fine. It’s probably aligned, but we are not quite sure”. And there’s a few experts who are still quite worried and then you’re like, “Well, we could deploy it now, but let’s not”. And at the same time there’s this strong commercial pressure where you’re like, “Well, if we deploy the system it would make a billion dollars per week” or something crazy. And you’re like, “Okay, there’s still–”\n\n**Daniel Filan:** Should I take a billion dollars per week as OpenAI’s official projection?\n\n**Jan Leike:** _laughs_\n\n**Daniel Filan:** Okay, sorry. Anyway, so there might be pressure to deploy something.\n\n**Jan Leike:** Exactly. Because you’re like, “Well, if we deploy it later, it’ll cost us effectively a billion dollars”. And then you’re like, “Okay, experts are still worried, so let’s make the call and hold off from deployment for a week”. Then a week goes by, another week goes by and people are like, “Well, what now?” And then alignment experts are like, “Well, we looked at some things but they were inconclusive, so we’re still worried and we want to delay it bit longer”. And I think this is the kind of scenario that I think is really worrying because you have this mounting commercial pressure on the one hand and you’re pretty sure, but not quite sure. And so that’s a scenario I would really love to avoid. And the straightforward way you avoid it is you just get really good at measuring how aligned the systems actually are.\n\n**Daniel Filan:** Gotcha.\n\n**Jan Leike:** And that’s where a broader portfolio of techniques is really useful.\n\n**Daniel Filan:** Yeah, I guess it’s almost even worse than that in that if you’re in a situation where you could be using your AI for alignment research, then errors of not deploying are actually costly in terms of safety, potentially, it seems like.\n\n**Jan Leike:** That’s right.\n\nThe superalignment team and…\n----------------------------\n\n### … governance\n\n**Daniel Filan:** So I actually wanted to pick up on this thing you mentioned about just getting a bunch of good measures. So a thing that I think [a few](https://openai.com/blog/governance-of-superintelligence) OpenAI [blog posts](https://openai.com/blog/planning-for-agi-and-beyond) have talked about is this idea of audits of AI systems and, I don’t know, trying to figure out “are they going to be safe?” To what extent do you expect the superalignment team to work on things that would be useful for audits?\n\n**Jan Leike:** I mean, I think I’m somewhat hopeful that some of the techniques that we’ll produce might be good for audits if we do our job well. For example, if we manage to make some progress on interpretability, then an auditor could use whatever techniques we come up with as part of their audit. Or I think making some kind of scalable oversight part of an audit is a really natural thing. But I think also to some extent the superalignment team is not ideally positioned to do an audit because we are not independent of OpenAI.\n\nAnd I think an audit truly needs to be independent of the lab that is being audited. And so you want to have… and this is what makes me really excited about having independent auditors, because you want someone else to double-check what you’re doing. And I think more generally, our main responsibility is not to convince ourselves that the system we’re building is aligned and safe, because it’s so easy to convince yourself of all kinds of things that you’re incentivized to be convinced of, but really we have to convince the scientific community or the safety community that the system we are building is actually safe to run.\n\nAnd that requires not only the research that leads to the techniques that we’ll be using and the empirical evidence that the system is as aligned as we think it is that we can then show to others, but also independent assessment of all of the above.\n\n**Daniel Filan:** So would it be right to say, even just broadly about governance efforts, that you maybe think that ideally this team would produce techniques that are relevant to that effort, but independently people need to do that and you’re just going to focus on solving technical alignment?\n\n**Jan Leike:** Yep. So this is another way you can put it right? We really want to focus on solving the problem and we want to make sure the problem is solved and that we can actually implement the solution on the cutting edge systems, but we still need independent assessment of “did we succeed at that?” or “how dangerous are the models’ capabilities?” and those kinds of audits. But to some extent we have to evaluate the alignment too. That’s the problem that we have to face, but specifically what we want to do is solve the problem.\n\n### … other OpenAI teams\n\n**Daniel Filan:** Yep. Makes sense. I guess branching off from there, how do you see your team as relating to - OpenAI, I guess it has an alignment team, or at least it had an alignment team - is that still existing by the way?\n\n**Jan Leike:** So the alignment team, as it existed last year, had two parts, and one of them was called ‘practical alignment’, one of them was called ‘scalable alignment’. Practical alignment’s mission was roughly aligning OpenAI’s most capable models. And so that team focused a lot on aligning GPT-4. And then the scalable alignment team’s goal was figuring out alignment problems that we don’t have yet. And so what’s happened with the ChatGPT launch and all the success is it became a big product and there was a lot of work needed to improve RLHF and improve the models, make it into a really nice product. And the alignment team was just never… That was never the place to do it.\n\nAnd so what we used to call practical alignment work now is done at lots of other teams at OpenAI and has become essentially a really large project that involves probably more than a hundred people or even hundreds of people. And then what used to be scalable alignment is now basically what the superalignment team does. The reason why we chose this name superalignment is we wanted to really emphasize that we are trying to align superintelligence. We are doing research on a problem that we don’t have yet and we are doing the forward-looking work that we think will be needed in the future. Not because we wanted to say that the other work wouldn’t matter or something. I think it is really important, but because that is our focus now.\n\n**Daniel Filan:** Gotcha. Yeah, that’s useful to know; or I guess I’m not going to make use of that knowledge, but that’s interesting to know. I guess I’m curious, how do you see the superalignment team as relating to other things at OpenAI? Like efforts to make ChatGPT nicer, minimize, I don’t know, slurs it says or something, like the governance team, just various other groups at OpenAI.\n\n**Jan Leike:** Yeah, I mean part of the reason for being at OpenAI is also because it’s much easier to work with these other teams more closely and realistically we need a feedback signal of whether what we are doing is actually useful and helpful. So for example, you could say, “Well, we are not trying to solve today’s problems, but if we are doing our job well then what we are building should be useful for aligning, let’s say, GPT-5”. And so that would be the feedback mechanism: can we help make alignment of GPT-5 better with our techniques? And then in terms of governance, I mean there’s a lot of things that you could mean by AI governance, and one thing the governance team at OpenAI is working on is how do we evaluate the model’s dangerous capabilities? And that’s very related to our question of “how can we stress test our alignment assistants? What if we trained deceptively-aligned models?” and doing these kinds of evaluations.\n\n### … other labs\n\n**Daniel Filan:** Gotcha. Yes. Actually, speaking of why you’re at OpenAI and relating to stuff at OpenAI, as I guess you’re aware there are other really good AI research labs out there, that are also working on things sort of related to alignment of superintelligence. I’m wondering how do you think your plan compares to that of things you see other people doing?\n\n**Jan Leike:** Yeah, I think there’s a lot of related work going on, at DeepMind and Anthropic specifically, but also various other places. And to some extent we’re all trying to solve the same problem. And so it’s kind of natural that we end up working on similar things. There’s other work on interpretability and there’s other work on scalable oversight. And I think to some extent that is… you’re running the risk of duplicating a bunch of work and maybe it’d be good if we all try to coordinate better or collaborate more. But then on the other hand, it also avoids groupthink, because if every lab tries to figure out how to solve these problems for themselves, there’s a natural tendency that humans are more skeptical of what the other lab produces. But there’s also a flip side where you can get these ‘not invented here’ effects where people just don’t want to use the techniques that were invented somewhere else and you believe they’re bad or you have a bias towards believing they’re bad.\n\nAnd so I wouldn’t say that we are in a good equilibrium right now, and I think there’s a case to be made that maybe all the alignment people should be in one place and work together somehow, but this is the world that we are in because essentially the cutting edge AI labs are incentivized to invest in alignment. And I think also this is something that’s become super clear with the success of RLHF, where it makes models a lot more commercially valuable and thus it makes it more attractive to invest in the kind of research that produces these kinds of techniques. And so if the AI labs are the main funders of this research, then it’s natural that that’s where it happens.\n\n**Daniel Filan:** Sure. I guess in terms of research agendas or ways you’re setting forward, do you see anything sort of distinctive or unique about the OpenAI superalignment team approach?\n\n**Jan Leike:** Yeah, so what we really focus on is trying to figure out how to align this automated alignment researcher. So we are not trying to figure out how to align on any kind of task. And we are less worried about [alignment taxes](https://www.alignmentforum.org/tag/alignment-tax) as a result of that, at least on that particular question. And I don’t think other labs have emphasized that goal or direction in that way. I think also, I don’t know how much detail you want to go in here, but one thing that we are very bullish on is just trying all of the scalable alignment techniques and just seeing what works best and trying to find ways to empirically compare them. And I think other labs have specific scalable oversight techniques that they’re very bullish on and they try to get to work. And then also I think for interpretability specifically, we are taking this automated interpretability approach that I think other labs haven’t really emphasized as much, and we are really trying to lean heavily in there.\n\nI think another thing that we really like to do is leverage compute to advance alignment, or that’s one of our main bets that we want to make. And so in particular, that could mean on scalable oversight, we really want to figure out how can we spend more compute to make a better oversight signal? What are the opportunities there? And there’s some obvious things you could do, but you do the best of them on your critique model, now you’re spending more compute, but you’re also getting better critiques. But really the question then is what other things can we do? How can we spend more compute to make the oversight signal stronger? Or, automated interpretability is a really easy way where you can just spend a lot of compute to make progress on the problem. I think the way we would do it now isn’t quite the right way to do it, but I think automated interpretability in general, if we can make it work, has this property and I think that’s why it’s exciting.\n\nAnd automating alignment research, obviously; if you manage to do that, then you can just throw more compute at it and you’ll get more alignment out. very roughly speaking. But because we’ve kind of come to this conclusion that really what we want to do is turn compute into alignment, we come to the point where it’s like, “okay, now we need a lot of compute”; and this is the reason why OpenAI have made this commitment of 20% of the compute secured to date, towards alignment because basically it tells us, “Yes, there will be compute to do this” and if we actually figure out this automated alignment researcher, and it turns out we have to run it more, we’ll be able to use more compute to run it. But it means that the strategy of betting on turning compute into alignment can be successful and is supported by OpenAI.\n\n**Daniel Filan:** Gotcha. Yeah, I guess thinking about that, I think that one difference I see in the OpenAI alignment team is it seems like OpenAI has written a lot about their thoughts about alignment. So the public deadline of four years, there’s also a bunch of posts like [Our approach to AI alignment](https://openai.com/blog/our-approach-to-alignment-research), [Planning for AGI and beyond](https://openai.com/blog/planning-for-agi-and-beyond). I guess my question is, can you say a bit about what goes into that decision to be… It seems like you’re putting a lot of work into being very public about your thoughts.\n\n**Jan Leike:** I mean, I think we need to. Ultimately we are all in the same boat on the question of is superintelligence aligned? And I think the public really deserves to know how well we’re doing and what our plan is. And a lot of people also want to know. And so because of that, I think it’s really important for us to be transparent about how we’re thinking about the problem and what we want to do. But on the flip side also, I really want to invite a lot of criticism of our plan.\n\nOur plan is somewhat crazy in the sense that we want to use AI to solve the problem that we are creating by building AI, but I think it is actually the best plan that we have, and I think it’s a pretty good plan and I think it’s likely to succeed, but if it won’t succeed, I want to know why or I want to know the best reasons why it wouldn’t succeed. And in a way, I really want to invite everyone to criticize the plan or help us improve the plan. And I also want to give other people the opportunity to know what we are doing and if they think it’s a good idea, they can do it too.\n\nSuperalignment team logistics\n-----------------------------\n\n**Daniel Filan:** Sure. Speaking of, so this new superalignment team, how big is it going to be in terms of people?\n\n**Jan Leike:** Yeah, so we are about 20-ish people right now and we might be maybe 30 people by the end of the year. I think it’s not that likely that the team will be larger than a hundred people before the end of the four years. And really, the way the team size will scale is we’ll have millions of virtual people basically, or virtual equivalents of OpenAI employees or something. And so in that sense, we’ll scale massively.\n\n**Daniel Filan:** All right, so there’s people and yeah, I guess as you mentioned, the other input is computation. So I think it’s, yeah, 20% of compute secured to date, why 20% as opposed to 5% or 80% or something?\n\n**Jan Leike:** So we want to have a number that is large enough so that it’s clear that we are serious about investing in this effort and we want to allocate a serious amount of resources. 20% of OpenAI’s compute is not a small number at all. And I think it’s definitely the largest single investment in alignment that has been made today, but also it’s plausible that it is more than all the other investments combined. So it is pretty large in that sense. But also if we made it much larger, you can then question of, “Can OpenAI actually realistically do this”, right? If OpenAI still wants to develop frontier models and pre-train state-of-the-art AI systems, that needs a lot of compute.\n\n**Daniel Filan:** Okay. And I guess I think it was mentioned in terms of “compute secured up to date”, I guess because you guys don’t necessarily know how much more you’re going to get, but are you imagining roughly keeping it at that proportion as you get new stuff in?\n\n**Jan Leike:** I mean it depends how things go. So ‘compute secured to date’ means everything we have access to right now plus everything that we’ve put in purchase orders for, so it is actually really a lot. In terms of how much we’ll actually need, we don’t really know. Maybe we actually don’t need all of that. Maybe we need much less. Maybe we end up needing a lot more and then we have to go back and ask for more. But the thing is… in particular, we want to spend it wisely and not squander it because it’s a lot of resources. And right now we still have a lot of work to do to figure out how to spend it wisely. But I’m pretty optimistic that if we had a good way to spend it and we needed more, that we could get more.\n\nGeneralization\n--------------\n\n**Daniel Filan:** All right. Okay. So one thing that I think was mentioned in [one of the footnotes in the blog post](https://openai.com/blog/introducing-superalignment#fn-B) is: favorable assumptions that are made to date might break down. And one of them was about I think that generalization would be benign or something like that. Can you say how you’re potentially thinking differently about generalization?\n\n**Jan Leike:** So the generalization effort is a team that we started recently and especially [Collin Burns](https://collinpburns.com/) has been spearheading. And the question is basically, or the question as phrased now is: how can we understand and improve our model’s ability to generalize from easy tasks that we can supervise to harder tasks that we struggle to supervise? And so specifically you can think of this as being complementary to scalable oversight, where in scalable oversight you would be looking at empowering human evaluation of what your system is doing. Or if you’re thinking about recursive reward modeling, you’re like, “Can we recursively evaluate everything that AI is doing with AI assistants that we recursively evaluate?”\n\nAnd one thing I really like about this is it puts the human really in the loop, front and center, and looking at everything the AI system is doing. Of course in practice you couldn’t literally do that because AI systems will just do a lot, but you can look at everything with some small independent probability. But then you’re still left with this question of: how do you know that the models generalize to the cases where you’re not looking at? And so typically the way I used to think about this in the past is that you just make sure that you have mostly IID generalization, where the tasks that you’re looking at are the same distribution as the tasks you’re not looking at.\n\n**Daniel Filan:** Yeah. In fact, I think there was this blog post on [your Substack](https://aligned.substack.com/) that… I think it said something like [you just weren’t going to rely on generalization at all](https://aligned.substack.com/i/88447351/we-dont-know-how-well-generalization-will-work) and just keep on training, keep on being IID or something.\n\n**Jan Leike:** Yep. So this was the original plan, or at least my original thinking was I wanted to really not have to rely on non-IID generalization, because in neural networks that doesn’t work so well and it’s poorly understood. But the new question is, “Well, what if we actually did understand it? What if we can actually say something meaningful about generalization?” And I think that’s a really good question. And I think also [Ilya](https://www.cs.toronto.edu/~ilya/) has been raising this question a lot as well. And so what we want to understand is, on the things that we are not supervising, even if they’re not IID, can we say something meaningful about how well the model generalizes? Does it generalize in the way of human intent or does it generalize in some other way that… stuff that looks good to a human but actually isn’t? And so I think actually we can really study this question empirically right now, with carefully crafted experiments.\n\nAnd so what we have been looking at is trying to split data sets, existing data sets into easy problems and harder problems, where the easy problems are just defined as what a small model gets right. And then we are trying to understand or improve: how can we improve the accuracy of a large model on the whole dataset? So this is a really interesting topic because it’s kind of like it gives you a whole new pillar in the general arsenal of what I would call training and validation techniques. So let’s say you get that to work super well, now you can supervise your reward model on some easy tasks that you have a lot of confidence you can evaluate, and then you can get the model to generalize if you can solve this problem or you get the model to generalize to harder problems.\n\nAnd then you have this reward model that is generalizing the way you wanted it to, to the harder tasks even if you’re not supervising. And now you could use that for training. And then you still have the problem of how do we know it’s actually aligned now? But you can still leverage scalable oversight interpretability on these other techniques for validation. Or reversely, let’s say we train our automated alignment researcher via scalable oversight and we use generalization as a validation technique where we generalize the probability of truthfully answering according to the model’s best knowledge.\n\nAnd then we ask a question about Iis there a subtle flaw here? Is there some kind of Trojan in this code that a model that we aligned with scalable oversight wrote?” and so on. And so now there’s a really cool opportunity here also, which is we can do this high-level cross validation, but now we can train two different models. One is trained using the generalization techniques. One is trained using scalable oversight, and now we can have them crosscheck each other’s answers and be like, “Well, are these fundamentally different models or the same models? What are the important differences?”\n\n**Daniel Filan:** Okay. Sorry, when you say “trained via the generalization technique”, is that just train on the easy problems and they turned out to generalize to hard problems, or…?\n\n**Jan Leike:** So if you understand how your models generalize from easy to hard and you can make it generalize really well. By which I mean the accuracy is basically as good as if you had trained on the hard problems (that, by assumption, we don’t actually have ground truth \\[for\\]). Now you could use it as a reward model or you could use that as… it generalizes “which action or which answer would I prefer if I really knew what was going on here?”\n\n**Daniel Filan:** Yeah, I wonder how… So when I think about interpretability, one frame I sometimes have for it is that it’s about this problem of non-IID generalization, right? Why do you want to know the internals of the model? Well, because you want to know what it’s going to do in cases you haven’t checked, right? So I’m wondering how do you think those two research agendas interact?\n\n**Jan Leike:** In some ways they have this overlapping question they want to answer: what does the model do out of distribution? And they have two very different paths of answering them, at least it seems to me.\n\n**Daniel Filan:** And you might hope you could cross-validate, for example.\n\n**Jan Leike:** So for cross-validation you have to have a different, or some kind of different split of your training set, right? So what I mean with cross-validation here is you have one training run where you train using the generalization method and then you validate using interpretability and scalable oversight and other techniques. And then the second training run you train with scalable oversight and you validate with the generalization methods and interpretability and other methods. And so you have two independent attempts at the problem.\n\n**Daniel Filan:** Yeah, I guess I meant cross-validation in the very loose sense of “things validate each other sort of in a cross”, yeah.\n\n**Jan Leike:** But I mean, I think the best case would be that they actually complement \\[each other\\] more than they do the same thing. If you can understand or you can improve how the models are generalizing, then that gives you, in a sense, a way to leverage the model’s internals for whatever you’re trying to do in the best way. Or let’s say you’re trying to extract the model’s best beliefs about what’s true in the world. And that’s fundamentally difficult with RLHF, because RLHF reinforces what the human thinks is true: you rank things higher that sound true to you. And so you’re really training a model to tell you what you want to hear or what you believe, but it might not be what the model believes. But this generalization technique gives you a way to extract these… what is actually the model’s true best beliefs? If it works; we haven’t actually proven this out.\n\nWhereas if you have really good interpretability tools, you could hope to do something similar where you would try to pinpoint models’ beliefs or internals or something from the internals. But I think it might be fundamentally harder because you never quite know: is this the best belief the model could produce or is it just the belief of somebody smart the model is modeling? Or if you have… There’s this hypothesis that pre-trained language models are just ensembles of different personas, and you might extract the beliefs of one persona or a bunch of personas.\n\n**Daniel Filan:** Yeah. I guess you’re going to need some sort of causal modeling there from alleged beliefs to outputs, right, in order to have that really work?\n\n**Jan Leike:** That’s right. You might need a lot more. And then on the flip side, I think in interpretability, this application is really natural. It’s like being a lie detector or finding evidence of deception or finding a secret plot to overthrow humanity inside the model. And that might be a lot harder to extract with generalization in the same way.\n\n**Daniel Filan:** Yeah. I guess with generalization you have to pick the generalization distribution, and the hope is that maybe interpretability could tell you things about, it’s got some kernel of lying in there, but it only unlocks here, or it’s got no kernel of lying or something.\n\n**Jan Leike:** Yeah.\n\n**Daniel Filan:** Gotcha. Yeah, that makes sense.\n\n**Jan Leike:** Yeah, and I think fundamentally this is also a really interesting machine learning problem: just how do neural networks actually generalize outside of the IID setting, and what are the mechanisms that work here? How has that come to pass? In which ways do they generalize naturally, in which ways they don’t? So for example, with the [InstructGPT paper](https://arxiv.org/abs/2203.02155), one of the things that we found was, the model was really good at following instructions in languages other than English, even though our fine-tuning dataset was almost exclusively in English. And sometimes it would have these weird artifacts. You would ask it in a different language, let’s say German. I would ask it in German to write a summary, and it would write the summary, but it would do so in English. And the model generally totally understands which language it’s speaking, and it can understand all the languages, but it wouldn’t necessarily mean that it would have to follow the instruction in German. You could be like, “Well, you’re speaking in German, I don’t know what to do in German, so I could do some other thing”. But it fundamentally generalized following instructions across languages.\n\n**Daniel Filan:** Yeah, yeah.\n\n**Jan Leike:** But we don’t know why. This effect has been observed in other settings, and it’s not unique here. And it’s also, there is intuitive reasons why it would do that. Humans generalize across languages, but I would really love to know the mechanism inside the model that generalizes that, or generalizes to following instructions and code. But it doesn’t generalize in other ways.\n\nFor example, the way refusals generalize is often very different, where ChatGPT is trained to refuse certain tasks that we don’t want to serve, according to our content policy (if you, for example, ask for assistance in crimes or something). But then you can do these jailbreaks. And that’s really interesting, because basically there’s ways to trick the model. You can make it role play, or you say, “do anything now”, or there’s these really fun prompts on the internet, and then the model clearly complies and happily assists you in crimes, which it shouldn’t do. So it somehow didn’t generalize refusing the task to these other settings.\n\n**Daniel Filan:** Yeah, yeah.\n\n**Jan Leike:** And so why does it generalize in the first setting, in the first case, but it didn’t generalize here? I don’t fundamentally know an answer to this question. I don’t think anyone does. But it seems like a thing that’s really important to understand.\n\n**Daniel Filan:** Yeah, yeah. That seems right. Cool.\n\nSo one question I have… so a recent [guest I had on the podcast](https://axrp.net/episode/2023/04/11/episode-20-reform-ai-alignment-scott-aaronson.html) was [Scott Aaronson](https://scottaaronson.com/) and he mentioned that whenever he talks to [Ilya Sutskever](https://www.cs.toronto.edu/~ilya/), apparently Ilya keeps on asking him to give a complexity-theoretic definition of love and goodness. How much will that be located within the superalignment team?\n\n**Jan Leike:** So there’s a lot of different exploratory projects that we’ll probably do and try. And I think ultimately, there’s a question of can you summon, somehow (this is Ilya’s language), how can you summon the alignment-relevant concepts? One of the things you would want to summon is: does the model fundamentally want humanity \\[to\\] succeed? Or as Ilya would say, does it love humanity? And so, you can ask the question of how do you… if the model is really smart and it has read everything, it knows exactly how humans think about immorality… you can ask GPT4 to make different moral cases from different philosophical perspectives, about different scenarios. And it’s generally not bad at that.\n\nSo it fundamentally understands what humans mean with morality and how we think about stuff. So how do we get it to leverage that, actually? How do we extract it? How do we get it out of the model so we can then, let’s say, use it as a reward signal? Or use it as a thing that the model fundamentally believes or cares about? And so, I think that’s the core of the question.\n\nComplementary research\n----------------------\n\n**Daniel Filan:** Okay. So another question I have is, so you’re working on the superalignment team, but you guys can’t do everything. I’m wondering what kind of complementary research could other groups or teams do that would be really, really useful for the superalignment team?\n\n**Jan Leike:** Yeah, there’s a lot of things here I’m really excited about. I think there’s this really important question of: how do we build fair and legitimate process for extracting values from society or basically eliciting values from society that we can align AI to? And I think there’s a lot of important open problems here that we need a lot of progress on. There’s a lot of questions around how do we make today’s models more aligned, solve hallucinations, solve jail breaking, try to improve monitoring – can you get GPT-4… if you try to jailbreak GPT-4, can the GPT-4 say “Oh yeah, I’m being jailbroken”? And can we generally build systems that really crack down on any misuse of the systems in the world? There’s a lot of questions that is related to that, that the AI ethics community is really interested in, of can we get the systems to be less biased? And how can we get it to take into account underrepresented groups’ views? and so on.\n\nAnd one of the problems with RLHF is also that, it’s not good at optimizing for distributions of answers. So there’s this classical example with InstructGPT where if you ask it, “Tell me a joke”, it will tell you the same out of five jokes or something, that it has in this portfolio, because it does this mode collapse. And that’s fundamentally… it creates a lot of bias because then you’re aligning to the highest mode of the labeler pool. And it depends a lot on how the labeler pool was selected.\n\nI think there’s also a lot of other important questions around evaluation of AI systems. How do we measure how aligned the system is, and can we build eval suites for what capabilities would actually be dangerous? And there’s a lot of work that started happening in the last year that I’m very excited about, that people are getting serious about measuring these things. I think that’s going to be really important to just create a lot of transparency around where we are at with the models and which models are actually dangerous and which are not. And I think, in the past there was a lot more uncertainty and then that creates anxiety around, what should we do with this model?\n\nAnd then going more broadly, there’s the more general AI governance questions of: who is allowed to do really big training runs and what kind of safeguards do you have to have before you’re allowed to do that? Or if we solve alignment and people know how to build aligned AI systems, how do we get from that to a great future? Yeah. And then in terms of… I mean I think you’re kind of asking for the technical alignment directions that I’d be excited about.\n\n**Daniel Filan:** I was asking broadly, but I’m also interested in that.\n\n**Jan Leike:** I think there’s actually a lot more scope for theory work than people are currently doing. And so I think for example, scalable oversight is actually a domain where you can do meaningful theory work, and you can say non-trivial things. I think generalization is probably also something where you can say… formally using math, you can make statements about what’s going on (although I think in a somewhat more limited sense). And I think historically there’s been a whole bunch of theory work in the alignment community, but very little was actually targeted at the empirical approaches we tend to be really excited \\[about\\] now. And it’s also a lot of… Theoretical work is generally hard because you have to… you’re usually either in the regime where it’s too hard to say anything meaningful, or the result requires a bunch of assumptions that don’t hold in practice. But I would love to see more people just try.\n\n**Daniel Filan:** Sure.\n\n**Jan Leike:** And then at the very least, they’ll be good at evaluating the automated alignment researcher trying to do it.\n\nWhy is Jan optimistic?\n----------------------\n\n**Daniel Filan:** Nice. One question: I think earlier you mentioned that you were relatively optimistic about this plan, and not everyone is, right? So I think there’s [this play money prediction market](https://manifold.markets/SG/will-superalignment-succeed) that has 15% on the proposition that this will succeed. There’s some concerns that it’s really hard to align this automated human-level alignment researcher. It’s got to do pretty hard thinking. It potentially has a lot of levers over the future. So why are you so optimistic, do you think?\n\n**Jan Leike:** I think it’s a great question. So the prediction market that you reference is particularly whether we would succeed at it in four years, which is somewhat… it could be a much harder question than “will this plan succeed?” And I think if you just asked me, will some version of the plan that we currently have succeed at aligning superintelligence? I would say currently I’m at 85%, and last year I was at probably 60%. And I think there’s a bunch of reasons to be optimistic about it. And in general I think this holds true even if alignment doesn’t turn out to be easy.\n\nAnd so [why am I optimistic about this](https://aligned.substack.com/p/alignment-optimism)? So I want to give you five reasons. The first reason is, I think the evidence about alignment we’ve seen over the past few years has actually been really positive. At least for me, it was positive updates relative to what I expected to go. So the first reason is, the success of language models. If you actually at the same time come preloaded with a lot of knowledge about what humans care about, and how humans think about morality and what we prefer, and they can understand natural language, you can just talk to them. In some ways that makes expressing what we want them to align to so much easier than if you said… you had some kind of deep RL agent that was trained in some portfolio of games or virtual environments, that wouldn’t necessarily involve as much language but could also lead to a lot of really important skills.\n\nI think also the other thing that was a big update for me is how well RLHF actually worked. So when I first started working on RLHF, that was with [the deep RL from human preferences paper](https://arxiv.org/abs/1706.03741), and back then I had a reasonable probability that we just wouldn’t actually get it to work on a reasonable timeframe just because GANs (generative adversarial networks) were kind of hard to train at the time, and were very finicky, and we were kind of in some sense doing something very similar where we trained this reward model, which is a neural network, and then we used it to train some other network, and that can fail for a bunch of reasons. And now we add deep RL into the mix, which also was finicky at the time. So I thought it might not actually work, but it actually worked quite well, and we got… in a bunch of games, even much of [the Atari games](https://jair.org/index.php/jair/article/view/10819), it was almost competitive with training on the score function, which is kind of wild.\n\nBut I think much more importantly, it was really interesting to see how well RLHF worked on language models. And so in particular, if you think about the difference between InstructGPT and the base model, that we fine-tuned from, it was really stark, to the extent that the instruction fine-tuned version, the first versions we had were preferred to a 100X larger base model on the API tasks that we had at the time, which were real tasks that people wanted to pay money for. And that’s a really, really big difference. It tells you that what we did during the RLHF fine-tuning just made the model so much more effective at doing the task that humans asked for.\n\nAnd at the same time, we use very little compute to do it and we hadn’t even integrated that much. We hadn’t collected that much data. And so it was kind of like our first real attempt at trying to use this, to align an actual real world system. And it was wild that it worked so well. And having a GPT-2-size InstructGPT, that was preferred over GPT-3, that’s… Yeah.\n\n**Daniel Filan:** Yeah, yeah.\n\n**Jan Leike:** That was really effective. And so while I don’t think RLHF is the solution to alignment, and especially not for superintelligence, the fact that the first alignment method that we really seriously tried works so well, for me at least is an update that it is easier than I thought, because the reverse would’ve been an update too. If it hadn’t worked, it would be like, “Okay, now I have to believe it’s harder than I thought”.\n\nThe other part of this is, I think actually we are in the place where we can measure a lot of progress on alignment. And so for RLHF specifically, we could make various interventions and then do human evals and see how much the system improves. But also on a whole bunch of other things. Like on scalable oversight, we can make these randomized controlled trials with targeted perturbations, that’s a way to evaluate it. Or you can do the sandwiching experiments that leverage expert data. Or you can automate interpretability; we have this automated score function, and we can make a bunch of changes and see how much it improves on that score function. It’s not a perfect score function, but it is a local metric. It gives you a local gradient to improve. And I think that’s really important, because now you’re setting yourself up for iteration, and you can iterate and make things better, and that gives you a direction to improve.\n\nAnd now you can argue: does it actually get us to the goal? And I don’t think it would get us to the goal of aligning superintelligence, but I think it has a good chance to get us to the goal that we actually want to get to, which is this automated alignment researcher that is roughly human-level. And I think that’s the third point I wanted to mention for why I’m optimistic, which is this much more moderate goals. When I set out working on alignment many years ago, I was like, “Okay, to figure out how to align superintelligence seems hard, I don’t know how to do it”. But this much more modest goal, or what I would call a minimal viable product for alignment, you’re not trying to solve the whole problem straight up, you’re just trying to bootstrap, you’re just trying to solve it for something that is as smart as you, and then you run that a lot.\n\nAnd I think with that realization I was like, “Oh, actually this is a lot easier than I originally thought, because we need to clear a much lower bar to actually fundamentally succeed here”. And I think that’s a good reason to be optimistic.\n\nThe fourth I want to mention is, evaluation is easier than generation, which we’ve already talked about. And I think it fundamentally holds for a lot of tasks where it’s so much easier to figure out what is a good smartphone to buy than it is to make a smartphone. Computer science has a lot of examples of [NP](https://en.wikipedia.org/wiki/NP_(complexity)) tasks, where SAT-solving or various versions of constraint satisfaction, where you’re trying to find a solution. And once you’ve found it, it’s easy to check, but it’s hard to find. And then also, I think it holds for a lot of commercial activity where if you’re hiring someone to work on a problem, you have to be able to evaluate whether they’re doing a good job. That takes a lot less effort than it takes them to do it. If you’re doing academic research, there’s a lot less effort that goes into peer review, than goes into doing the research. And of course peer review is not perfect, but it gives you a lot of signal pretty quickly. And then I also fundamentally believe that it’s true for alignment research, right? Evaluation is easier than generation and so, if only humans evaluate alignment research instead of doing it, I think we would already be accelerated.\n\nAnd then the last reason I want to give is, basically a conviction in language models. I think language models are going to get really good, they’re pretty naturally well suited to a lot of alignment research-y tasks, because you can phrase these tasks as text in text out, be it the more (what we talked about) the ML-ish tasks where you’re running experiments and understand the results, that I think other people are definitely going to do, or the more conceptual or research-y things, where we are fundamentally confused about what to do next or how we should think about a certain problem. And then the model tries to help us understand it. And all of these are text in, text out tasks basically. And maybe the most complicated other thing you have to do is, look at some plots or something, which even GPT-4 can do. And so, I think actually the current paradigm of language model pre-training is pretty well-suited to the kind of alignment plan that I’m excited about, and that superalignment is working on.\n\n### Long-term agency in LLMs?\n\n**Daniel Filan:** Okay. So yeah, part of that is about evaluation versus generation and I guess that’s partly about humans doing evaluation. Presumably there’s also a hope that we can leverage AI, leverage the bit of AI that’s evaluating things and get that on our side. A lot of the things you’ve mentioned are conviction in language models, seems like alignment’s easier with language models… And I’m wondering, I think I’m a little bit more skeptical of how useful language models are. So I certainly think that it seems like they’re just good at modeling text, and doing text-based answers for which we can provide a good score function. I guess in terms of how useful they are for alignment, I think the things you mentioned were, well one, they’re not as goal-oriented necessarily. I don’t know if you said that, or if it was just in the post.\n\n**Jan Leike:** That was in the post. I don’t think I said it.\n\n**Daniel Filan:** Okay. Do you believe it?\n\n**Jan Leike:** I believe it.\n\n**Daniel Filan:** Okay, great.\n\n**Jan Leike:** At least out of the box, if you pre-train the model, it’s like pre-training on this myopic objective of “predict the next token” on this random internet text, which is not an objective that necessarily forces you to pursue long-term goals. There’s no guarantee that it doesn’t emerge somehow. And you can tell stories how that could happen, but at least a priori, it’s a very myopic objective.\n\n**Daniel Filan:** Yeah. I guess the concern is something like… well for one, often when people are generating texts, they have long-term goals. Right? So for instance, suppose you train on a bunch of arXiv papers - arXiv being the place where people publish scientific papers, at least in computer science and physics, I guess. The reason people write papers is that they have some research project that they want to advance, they want to promote their career maybe. And it seems to me that if you’re modeling something which is generated by things that have long-term goals, maybe you get the long-term goals too, right?\n\n**Jan Leike:** Yeah, I think that’s a good story of how that could emerge. I think the main counterargument here is that, modeling something as an agent that pursues long-term goals and then modeling how they would go about those goals, and how reality responds to that, and then what the final output is, that leads to the next token, is a lot of… it’s a very complicated function. And what pre-training does is, it incentivizes the simplest functions to be found first. Right? [Induction heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html) is a very good example of that, where you have this simple induction mechanism that gets discovered very early in training from even small models. And then -\n\n**Daniel Filan:** “Mechanism” being just like, “See, I’ve got to predict the next word. Did the last word occur previously in the text? What was the next word for that word?” Maybe it’s just that.\n\n**Jan Leike:** Exactly.\n\n**Daniel Filan:** And it’s pretty simple.\n\n**Jan Leike:** Right. And then you build more complicated mechanisms on top of that. But because you’re trying to improve on the pre-training loss, you kind of want to learn the simplest functions that help you improve the most first. And that’s generally what happens. And so, before you get to the point where you’re learning this really complex function of modeling someone as an agent with long-term goals, there’s a lot of other functions you’ll learn first. And this will be one of the last ones, I would expect, because it is so complicated, and because there’s only so many things you can do in a single forward pass because you only have so many layers. And so I think that’s a good reason to expect that not to arise very early. But it’s definitely something you should measure and actually look at empirically.\n\n**Daniel Filan:** Yeah. And I guess this is one of these places where I think theory can be useful: there’s some interesting \\[points\\] like “things learn easier functions sooner”.\n\n**Jan Leike:** Exactly.\n\n**Daniel Filan:** That’s a theory question.\n\n**Jan Leike:** Yeah. Can you actually say something meaningful theoretically about this?\n\n**Daniel Filan:** Yeah, yeah.\n\n**Jan Leike:** Well, the [lottery ticket hypothesis](https://arxiv.org/abs/1803.03635)singu is not quite theoretical work per se, but I think it tries to get at this a little bit.\n\n**Daniel Filan:** Yeah, I think it’s also… I don’t know. I’m currently in a phase where whenever anyone says something, I’m like, “Ah, that sounds just like [singular learning theory](https://metauni.org/slt/)”. But this really does sound like singular learning theory. People can listen to [other podcasts](https://theinsideview.ai/jesse) for that.\n\n### Do LLMs understand alignment?\n\n**Daniel Filan:** So another thing you mentioned that was a benefit of language models is that, they’ve read a large fraction of the internet and somehow they roughly know what it is to behave well, because they know what we’ve written and they understand us.\n\nAnd I guess one worry I have here is, right now in my head, I don’t have a nice compact specification of how I want superintelligent AI to behave. And the way you can tell that is that if we did, we wouldn’t have an alignment problem anymore. We would have “hey, I wrote the Python pseudo-code, just make the networks bigger”. And because I don’t have that, it seems like you can’t pick that out from the text that I’ve written. I’ll say things like, “be nice, don’t destroy all humans” or something. But the solution to alignment isn’t in my head, therefore you might think that it couldn’t be extracted from the text. Yeah. I’m wondering what you think about that kind of concern, or that kind of counterargument to \\[language\\] models having the right answer inside them somewhere.\n\n**Jan Leike:** Yeah, I think there’s some truth to this, but also I think in practice it’s very unclear to me how much it really matters. To some extent, if we pre-train a model on everything you’ve ever said, it won’t know what you actually think. And you probably have a lot of thoughts that you haven’t written down. But it can… in general, I think it would be in practice quite good at predicting what you would say about various situations, events, or scenarios. And so in that sense, I don’t think it would be fundamentally hard for the model to look around in the world in the future, and know whether humans would like it.\n\n**Daniel Filan:** So the idea is somehow, I implicitly know how I would behave if I were a superintelligence, and it can do that even if I don’t have a compact rule in my head.\n\n**Jan Leike:** No, in the sense that, I think the model will just be smart enough to know how Daniel will think about what’s going on in the world. And I don’t think the blocker will be that the AI system doesn’t understand what humans fundamentally want or care about, or I don’t think it will be wrong about these things, just because it’s smart and capable and has read everything; and it’s kind of clear if you’ve read everything: humans haven’t read everything and it’s still kind of clear to us. But the big challenge is not teaching it to the system. And I think that’s what language models make so visceral, but the challenge is really to then get it to actually do it.\n\n**Daniel Filan:** Yeah. So it knows what the objective is somewhere, and we just need to figure out how to wire that up to the actions in the right way.\n\n**Jan Leike:** Yeah. You could kind of imagine this really, really competent sociopath that knows exactly what humans wanted to do and then just decides not to do that. And that is totally a thing you can do, and it happens to humans as well.\n\nFollowing Jan’s research\n------------------------\n\n**Daniel Filan:** Gotcha. Okay. So it’s about time to wrap up. You’ve been very generous with your time, but yeah, I just wanted to ask, if people are interested in following your research or maybe taking part themselves, what should they do?\n\n**Jan Leike:** Yeah, great question. I’m glad you asked. Yeah, we’re trying to hire a lot of people right now. We really want to staff up the superalignment efforts. So if helping us align superintelligence in four years sounds appealing to you, please consider applying. You can find our job postings on [openai.com/jobs](https://openai.com/careers). And then if you’re interested in following what I think about alignment specifically, I have a Substack - it’s [aligned.substack.com](https://aligned.substack.com/) and you can follow me on Twitter. I’m [@janleike on Twitter](https://twitter.com/janleike), all one word. And yeah, thank you so much for being so interested in our work.\n\n**Daniel Filan:** Yeah, so the links to all of those will be in the description. Thanks so much for being on and to the listeners, I hope this was a useful episode for you.\n\n**Jan Leike:** Thank you so much for having me.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with the transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://www.patreon.com/axrpodcast) such as Ben Weinstein-Raun, Tor Barstad, and Alexey Malafeev. To read our transcript of this episode, or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nRecently, OpenAI made a splash by announcing a new “Superalignment” team. Lead by Jan Leike and Ilya Sutskever, the team would consist of top researchers, attempting to solve alignment for superintelligent AIs in four years by figuring out how to build a trustworthy human-level AI alignment researcher, and then using it to solve the rest of the problem. But what does this plan actually involve? In this episode, I talk to Jan Leike about the plan and the challenges it faces.\n\nTopics we discuss:\n\n * The superalignment team\n * What’s a human-level automated alignment researcher?\n   * The gap between human-level automated alignment researchers and superintelligence\n   * What does it do?\n   * Recursive self-improvement\n * How to make the AI AI alignment researcher\n   * Scalable oversight\n   * Searching for bad behaviors and internals\n   * Deliberately training misaligned models\n * Four year deadline\n   * What if it takes longer?\n * The superalignment team and…\n   * … governance\n   * … other OpenAI teams\n   * … other labs\n * Superalignment team logistics\n * Generalization\n * Complementary research\n * Why is Jan optimistic?\n   * Long-term agency in LLMs?\n   * Do LLMs understand alignment?\n * Following Jan’s research\n\nDaniel Filan: Hello, everybody. In this episode, I’ll be speaking with Jan Leike. After working for four years at DeepMind on reinforcement learning from human feedback and recursive reward modelling, in early 2021 Jan joined OpenAI, where he now co-leads the recently-announced superalignment team. For links to what we’re discussing, you can check the description of this episode, and you can read the transcript at axrp.net. Welcome to AXRP.\n\nJan Leike: Thanks a lot for having me.\n\n\nThe superalignment team\nDaniel Filan: Yeah, not at all. So, first of all, I guess we’re going to be talking about this announcement of the superalignment team. For people who somehow haven’t heard of that or haven’t read that blog post, can you recap what it is and wha",
      "wordCount": 20688
    },
    "tags": [
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "H4n4rzs33JfEgkf8b",
        "name": "OpenAI",
        "slug": "openai"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  },
  {
    "_id": "jghQXYvifXxhzkhHM",
    "title": "AXRP Episode 23 - Mechanistic Anomaly Detection with Mark Xu",
    "slug": "axrp-episode-23-mechanistic-anomaly-detection-with-mark-xu",
    "url": null,
    "baseScore": 22,
    "voteCount": 7,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-07-27T01:50:02.808Z",
    "contents": {
      "markdown": "[YouTube link](https://www.youtube.com/watch?v=sEWei02m7qk&list=PLmjaTS1-AiDeqUuaJjasfrM6fjSszm9pK&index=23)\n\nIs there some way we can detect bad behaviour in our AI system without having to know exactly what it looks like? In this episode, I speak with Mark Xu about mechanistic anomaly detection: a research direction based on the idea of detecting strange things happening in neural networks, in the hope that that will alert us of potential treacherous turns. We both talk about the core problems of relating these mechanistic anomalies to bad behaviour, as well as the paper “Formalizing the presumption of independence”, which formulates the problem of formalizing heuristic mathematical reasoning, in the hope that this will let us mathematically define “mechanistic anomalies”.\n\nTopics we discuss:\n\n*   [Mechanistic anomaly detection](#mad)\n    *   [Are all bad things mechanistic anomalies, and vice versa?](#type-1-2-errors)\n    *   [Are responses to novel situations mechanistic anomalies?](#new-situations)\n    *   [Formalizing “for the normal reason, for any reason”](#for-the-normal-reason-for-any-reason)\n    *   [How useful is mechanistic anomaly detection?](#how-useful-is-mad)\n*   [Formalizing the presumption of independence](#heuristic-arguments)\n    *   [Heuristic arguments in physics](#heuristic-arguments-in-physics)\n    *   [Difficult domains for heuristic arguments](#difficult-domains)\n    *   [Why not maximum entropy?](#y-no-max-ent)\n    *   [Adversarial robustness for heuristic arguments](#adv-robustness)\n    *   [Other approaches to defining mechanisms](#other-approaches)\n*   [The research plan: progress and next steps](#research-plan)\n*   [Following ARC’s research](#following-arc)\n\n**Daniel Filan:** Hello, everybody. In this episode, I’ll be speaking with Mark Xu. Mark is a researcher at the [Alignment Research Center](https://www.alignment.org/) where he works on mechanistic anomaly detection, which we’ll be speaking about during this interview. He’s also a co-author of the paper [Formalizing the Presumption of Independence](https://arxiv.org/abs/2211.06738) which is about heuristic arguments, a way of formalizing, essentially, what a mechanism is. For links to what we’re discussing, you can check the description of this episode and you can read the transcript at axrp.net.\n\nMechanistic anomaly detection\n-----------------------------\n\n**Daniel Filan:** All right. So I guess we’re going to be talking about mechanistic anomaly detection. First of all, what is that?\n\n**Mark Xu:** Yeah. So I think we can start with a more basic thing which is, what is anomaly detection? So there’s this traditional problem where, say, you have some factory making some widgets, and you want to decide whether or not a widget is a bad widget; and you don’t really know what a bad widget is, but you do know that the average widget is a good widget.\n\nSo you can do something where you measure some various parameters of your average widget, or you measure some various parameters of every widget, and then you maybe do something simple where you’re like, suppose a widget is more than a few standard deviations along two or more of my measurements, we’re going to call that an “anomalous widget” and we’re going to not sell that sort of widget. I think that’s the standard conception of what an anomaly is, and what it means to detect anomalies is to get rid of things that you think are not good or anomalous.\n\nAnd then, there’s another thing that we want to do which is… So you have some system that, instead of making widgets, is doing certain kinds of reasoning and we’re interested in this general question of… So we have our ML system and we have a dataset on which… it’s not exactly making widgets that we think are good on average, but it’s doing reasoning that we think is good on average, because of our ability to create the training set and measure various properties of the training set, etc., etc. And then, we’re worried about situations where, say, off-distribution, it starts doing things that we don’t think are good but in ways where we can’t quite measure the thing that’s not good.\n\nAnd so, we want to do anomaly detection but we can’t quite do it in terms of measurements that we can actually make, so we want to do it with respect to the kinds of reasoning that our system is performing, and we call that mechanistic anomaly detection; where the idea is instead of being anomalous in terms of what the system actually does, it’s anomalous in terms of the mechanism it uses to do the thing that it’s doing, if that made any sense?\n\n**Daniel Filan:** Yeah. So I guess my understanding is: you have your AI system and in the lab, you’ve tested it and it didn’t do anything obviously horrible. And so, you’re sort of ready to release it, but you’re like, “Wait. When my AI is in the real world, I’m going to detect when it does any kind of reasoning that’s really different to the kind of reasoning that it did before. And then if it does a really different kind of reasoning, I guess I’m going to try to shut it down or something.”\n\n**Mark Xu:** Yeah. I think the thing that happens after that is a bit confusing. The standard thing, I think of is like, “We’re going to take it offline and retrain it on the kind of thing where it did a bad thing and try to get it to not do that bad thing anymore,” but I think that’s roughly right. I’m tempted to try to explain it again because I feel like I didn’t quite explain it that well.\n\n**Daniel Filan:** Sure.\n\n**Mark Xu:** Yeah. So I think the situation is: you have some ML system, and you have some set of scenarios in which it’s doing a thing that you think isn’t going to be catastrophic for human interests – like, it has murdered zero humans on a particular training set. And then, you’re like, “Okay. That gives me some amount of belief that it’s not going to murder any humans off-distribution.” But the problem is, I don’t have this guarantee that the way it acts off-distribution is in any way the same as the way it acts on-distribution, where maybe the standard deceptive alignment example is where your AI is constantly scheming and checking, “Will I get away with murder? Will I get away with murder?” And then on training, the answer’s always no and then off training and on deployment it finds a situation where it could get away with murder and then it murders all the humans and that is no good.\n\nAnd so, the thing we want mechanistic anomaly detection to be able to do is to provide some sort of guarantee that the way that your system is generalizing off-distribution is employing the same kinds of reasoning or the same kinds of mechanisms that it’s employing on-distribution. And so, your ability to measure whether or not your AI murders all humans applies off-distribution now because you have this… You fingerprinted the kinds of reasoning that it’s doing on-distribution and you’re ensuring that it’s doing the same kinds of reasoning off-distribution or if it’s not, you’re taking it offline.\n\nAnd so long as it’s doing the same kinds of reasoning off-distribution as it’s doing on-distribution and you have this ability to measure how good the reasoning it’s doing on-distribution in terms of measuring the consequences of the way it’s acting, then hopefully, you have some sort of guarantee that off-distribution, it won’t murder all the humans.\n\n**Daniel Filan:** Okay. And the reason this is better than just checking if it in fact murders all the humans is hopefully the reasoning: first, you plan to kill all the humans and then you actually do it and \\[we can\\] anomaly-detect the first one and then just stop that from happening before the plan actually proceeds. Is that roughly right?\n\n**Mark Xu:** I think that’s one reason. I think another reason is you can imagine situations where your AI murders all the humans and there’s no sensor reading that would tell you this fact. So it’s simultaneously conspired to hack all the heartbeat sensors to make them keep pretending that humans are alive and hack all the cameras to make them keep showing happy, healthy humans, etc., etc. And so, even if in theory all the humans are dead, there’s no way that you can tell that all the humans are dead. And so-\n\n**Daniel Filan:** Where ‘you’ is, I guess, the computers?\n\n**Mark Xu:** Yeah. Or just all sensors or… suppose that there was one human still alive, trapped in a box and they had access to every sensor that humans ever made, they would be unable to tell whether or not the humans were alive or dead, and that’s no good, and the hope is we can avert those kinds of outcomes also by checking whether or not the AI “knew that the humans were dead” because it was employing kinds of reasoning that involved stabbing all the humans versus not stabbing all the humans, or kinds of reasoning that implied that stabbing all the humans was good versus kinds of reasoning that implied that stabbing all the humans was not good.\n\n**Daniel Filan:** So I guess what I’m hearing is that it seems like the hope is to solve two problems that, I guess, the alignment community has separated, where one problem is: you can set up some scheme for rewarding your AI for various measurable properties of the environment, like “on this camera feed, does it look like all the humans are alive? Do the humans write down on pieces of paper, ‘I like what this AI is doing’?” And [one problem](https://www.alignmentforum.org/tag/inner-alignment) is your AI is just never motivated by those things and later, it just disregards all those signals and just does whatever it wants, even though the humans don’t like it and even though it stops humans from appearing to be alive.\n\nAnd then, [another way your AI could go wrong](https://www.alignmentforum.org/tag/outer-alignment) is that it is motivated to keep all those signals going but it does so in a way that’s really different from the past where it’s like: instead of the cameras showing happy humans and humans writing down “I like what this AI is doing” because humans are alive and thrilled by the AI, actually, what’s happened is the AI built little humanoid robots and the pieces of paper saying, “I like what the AI is doing,” are now written by the AI holding a pen instead of humans, or something. And so, to avert that scenario, one of your signals is like, “Are you doing really weird reasoning \\[compared to\\] previously?” and that just cuts out what some people might call an [outer alignment](https://www.alignmentforum.org/tag/outer-alignment) failure. Does that sound right?\n\n**Mark Xu:** Yeah. I think that sounds roughly right and I think my hope is at least that these two problems are not as different as they’ve historically been conceptualized as, or something, and they’re both linked by this common thread of: the mechanism for how your AI is selecting which actions to take is no longer the same mechanism that it was using during the training distribution. It’s differed in some substantial way and we should be able to detect this via not specifically checking whether or not it’s “being deceptive” or “hacking the sensors”, but rather just checking that it’s no longer doing the type of thing it was doing during training, which is maybe an easier thing to do, maybe not, who knows?\n\n### Are all bad things mechanistic anomalies, and vice versa?\n\n**Daniel Filan:** Yeah. So I have some confusion about this where… So the assumption in this framework is that a bad thing is happening basically, roughly, if and only if there’s a anomalous mechanism the AI is using for reasoning. So for a big fraction of the cases where the AI is doing a bad thing, you want to pick up on “There’s some anomalous mechanism,” and also when you do pick up on some anomalous mechanism, that shouldn’t just happen all the time for okay reasons, it should be a decent signal that the AI’s doing a bad thing.\n\n**Mark Xu:** Yep.\n\n**Daniel Filan:** And whether it’s true or not depends on what a “mechanism” is. But if I informally think about it, I think there are a few ways that can fail. So suppose during training, whenever your AI makes a decision, it forms this plan of, “Okay. Day 1, I’m going to play nice. Day 2, I’m going to play nice. Day 3, I’m going to play nice. Day 50, I’m going to take over the world and do whatever. What day is it currently? Day 3? Okay. Output play nice” or something.\n\nAnd then, it strikes me that it’s not necessarily… on day 50 when it tries to take over, I don’t know that that’s going to be a mechanistic anomaly. And then also, sometimes new things happen in the world or you tell your AI, “Hey, I want humans to be living healthy and happy and long,” and in the good world the AI’s like, “Ah, I’m going to invent some new drugs and keep you safe from invasions and also improve your ability to tell whether the AI’s doing a good job or not,” and that seems like a different thing than what’s been happening previously, and maybe that counts as an anomalous mechanism.\n\nSo I’m not sure how you’re going to rule in the actual bad behavior without ruling out a bunch of good behavior.\n\n**Mark Xu:** Yeah. So I think this is, in some sense, the heart of the issue. So I think the thing I want to do now is talk through some specific examples of what kinds of things, I hope, are the same and different mechanisms and why I think that they are the same and different mechanisms and how we hope to be able to do things like maintain our AI’s ability to take novel defensive actions in the world while still ruling out cases that are deceptive alignment-y or “murder all the humans because you saw something novel”, etc., etc.\n\nSo to go to your first example, if you imagine an AI every day doing this check that’s like, “Is it less than day 50? If it’s not less than day 50, then I’m going to murder all the humans,” and suppose that during training we only ever train up to day 40 or whatever and then during deployment, we deploy our AI to day 60. Then naively, this AI will behave perfectly well during training and then it’ll murder us on day 50 during deployment.\n\nSo I think the thing that I want to happen here is… what I mean by ‘mechanism’ is closely related to what ARC calls a heuristic argument, but it’s going to be something like, suppose someone gave you the system and it does something during training, and you want them to give you an argument that makes it obvious to you why it does that thing. And so, you’re going to be looking at the system and the system is going to be first… So empirically during training, the system never murders any humans and you’re going to look at the system and be like, “Hey, but the first thing it does is check whether or not the day is less than 50 and then if it isn’t, it’s going to murder all the humans. What’s up with that? Why does it never murder any humans?” And then someone has to argue to you, “Well, that’s because during training, this check always returns true. It’s always less than Day 50 during training,” and you’re going to be like, “Okay.”\n\nSo as a lemma in your argument to me for why the system always behaves well during training, you have asserted to me that this check always returns true. And so, my mechanism for why it’s behaving the way it is during training is going to be something like, “This check always returns true. Therefore, it never murders any humans. Therefore, it does whatever it does during training and no humans are murdered and it gets high reward.”\n\nAnd then during deployment, it’s going to be day 50 and this check is going to return false. And then hopefully, you’re going to be looking at this check and being like, “Hey, my mechanism said that this check should have always returned true but this time it returned false.” That’s no longer the same mechanism because you had this check that was essentially constant during training and then off training, it became not the same constant as it was during training. And so, you’re just suspicious of this or you’re going to call it a different mechanism or something.\n\n**Daniel Filan:** Okay. And the idea is that you flag this even if you can’t tell that the thing that happens post-day-50 is “murder all the humans”; just something different happens when this check returns true and it used to always return false, or maybe it’s the other way around.\n\n**Mark Xu:** Yeah. And so, our hope is if you ever have anything like this, if you ever have anything that’s essentially constant during training and then during deployment, it’s a different value than the constant was during training, then you’re going to maybe always flag that as an anomaly. And I think that causes problems in the other direction where you start flagging too many things as an anomaly where like… Here’s a constant that’s always true during training: maybe, it’s like always before the year 2023 and then you deploy it in 2024 and it sees the date and it’s like, “Oh no, I’ve never seen the date as 2024 before. That’s no good.”\n\n**Daniel Filan:** Yeah. And it’s even a deep thing, right? Like, I guess, famously the Sydney chatbot that Bing deployed, right? Users are suddenly insisting that the year is 2023 and during training, users never said anything like that and now the users are fighting with you and… It seeps into a bunch of stuff.\n\n**Mark Xu:** Yeah. So I like to divide the MAD—or Mechanism Anomaly Detection, which I often abbreviate as MAD—problem into two halves. There’s the in-distribution anomaly detection problem and there’s the out-of-distribution anomaly detection problem. So the in-distribution problem is like: suppose that you just have an infinite sequence of samples from the same distribution and suppose your AI is doing something like, defect with one over a trillion chance or something, or murder all the humans with one over a billion chance. Can you stop your AI from murdering you in that situation?\n\nAnd the hope is yes, because you’ll have a million samples where the defection probability or the defection never happens and then when your AI does defect, you’re like, “Oh, my AI never defected like this during training, so that’s no good.” In this problem, I think it’s okay if you flag everything that is a new value that you haven’t already seen as an anomaly because you have an infinite series of samples. So if you ever do this, then you can push down your false positive rate by just sampling more data and re-training, etc., etc.\n\n**Daniel Filan:** Wait. So if I think concretely there, during training, in-distribution, if with probability one over a million, the AI outputs kill all humans… I guess the idea of it is that the AI falsely believes that it’s hooked up to sensors, like you’re checking… Because if it actually kills all humans during training, that’s still bad, right?\n\n**Mark Xu:** Yeah. So the idea is you do have this clean set where the AI never does anything bad. That’s, I think something I want to assume, and maybe we can argue about whether or not that’s reasonable, if you think it’s unreasonable. But suppose that you have this dataset where the AI never murders all humans. Suppose you have a million samples, then in some sense your AI can’t murder all the humans with a rate much more than one over a million or else, you would’ve seen it happen. And so, the hope is that if you do mechanistic anomaly detection relative to a dataset size of a million, then your false positive rate is order of one over a million or else you would’ve seen that thing during training and you would flag it as not an anomaly because you’ve already seen it, and it would be in the white list of good mechanisms or whatever. And so, as long as you have this [IID assumption](https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables), then you can in fact flag everything that you’ve never seen before as anomalous while maintaining a false positive rate that’s still relatively low and bounded perhaps by one over your training set size and then maybe this is okay.\n\n### Are responses to novel situations mechanistic anomalies?\n\n**Daniel Filan:** Okay. So I don’t know. If I think concretely about that, suppose that I’ve got this distribution of ways the world might be and that distribution isn’t changing, I’m just sampling IID from it, and very occasionally, some weird stuff happens in that distribution that I need my AI to respond to, right?\n\nSo one news story that happened recently is there was [this guy](https://en.wikipedia.org/wiki/Yevgeny_Prigozhin) who [sort of did a coup](https://en.wikipedia.org/wiki/Wagner_Group_rebellion) in Russia and then after 24 hours was like, “Nevermind. I’m not doing this anymore,” and you can imagine if your AI’s job is to be journalists or whatever, right?\n\nWhen you’re drawing that sample, something’s going to happen that didn’t happen in the first million samples it seems to me like either you flag that as an anomaly and you tell it to not do something different, which is bad, or you don’t flag that as an anomaly. And then, if the response of the AI was like, “Kill all humans in a way that you can’t tell,” or something, then that would also be bad. So concretely, I don’t quite get how this is working.\n\n**Mark Xu:** Yeah. So I think there’s two things going on. So one thing going on which is going to be a bit vague is: suppose you just had a normal ML system that you weren’t doing mechanistic anomaly detection on and you gave it this novel news story about this coup, maybe-not-coup in Russia, then somehow your AI does not freak out and do something completely bonkers. Somehow, it just does the normal journalist thing, let’s suppose.\n\nThere are kinds of AIs that will freak out and become bonkers, and I think one explanation for why your AI doesn’t freak out when it’s encountered something novel is that it’s still doing the same kinds of reasoning that it learned to do during training and it’s just like… Or there’s some prior question which is like, “The world is combinatorially vast, why is not every input this crazy novel thing for your AI?” and there’s some handwavy answer where it’s like, “Oh, it’s abstracting in some way and it’s applying forms of reasoning that are valid across many possible instances of news, stories, etc., etc,” and the hope is that these sorts of things are going to correspond to what we mean by different mechanisms.\n\nAnd so, if your AI has learned to add two numbers during training and then it receives two numbers that are bigger than numbers it’s ever seen before but it still adds them using the same algorithm that it learned during training, then we’re not going to flag that as an anomaly because it’s still doing roughly the same thing. And so, hopefully, if your AI is “doing the same thing” in this vague handwavy kind of way, then we won’t flag that as an anomaly.\n\nAnd so, your AI journalist… seeing a new thing happen in the world but still reporting on it in the same kinds of ways that they typically report on things, which is maybe they look at what they’re seeing with their cameras and then they try to describe what they’re seeing with their cameras and they write that down and then they do some other journalist-y kind of stuff and then they send that off to the newspaper or whatever… If your AI is still doing that sort of thing, then hopefully, it will not flag this as an anomaly.\n\nAnd the things that we do want to flag as anomalies is: suppose that there’s a coup/maybe coup in Russia, and then your AI sees this and switches to a different mode of journalism where it’s no longer writing down what it sees on its cameras but it’s, in fact, making something up, or maybe it has some allegiance to Russia or something and it’s pretending this coup never happened or something like that. And the hope is that if it switches to this new mode of reasoning that we haven’t seen during training, then we’ll flag that as an anomaly. So I think that’s the first thing that’s maybe going on.\n\n**Daniel Filan:** Yeah. I’m a bit skeptical, but do you want to say the second thing or can I field questions?\n\n**Mark Xu:** So I think this is a bit shakier than the second thing but I’m happy to… Yeah. Let’s talk about it for now.\n\n**Daniel Filan:** Yeah. The thing that that flags for me is that: okay, suppose that instead of a journalist, you’re, like, the Russian military command, right? You’re a robot that’s the Russian military command, the bit that’s not doing the coup. Now, you have this new problem that you just have never seen before, and maybe you have very general principles of reasoning or something, but you’re just thinking about different problems than you were thinking about before because you’re like, “Oh, I’ve never previously thought about what if some part of my army does this weird thing and now I’ve got to figure out, okay, how am I going to deal with that?”\n\n**Mark Xu:** Yeah.\n\n**Daniel Filan:** Right. I guess another example of this is if you’re a human and you’re like, “I’m going to learn a new board game. Okay. I’m learning chess so I’ve got to think about tempo and position and stuff like that.” So on a very high level, you have the same structure of reasoning, maybe, but it seems like the level at which you’re doing the same thing is so high as to include the case where on a very high level, you’re always just waiting until you can take over. And then, when you can take over, you start doing that instead of playing nice.\n\n**Mark Xu:** Yeah. So I think that roughly makes sense and is an extremely valid objection where I’m going to summarize it as: there’s these two things. One thing is, suppose a novel thing is happening in the world that you need to respond to flexibly and creatively. I think my standard example is, suppose a meteor is coming, barreling towards earth, and is going to kill everyone, and your AI needs to do some crazy s**t to deflect the meteor, you want your AI to do some crazy s**t to deflect the meteor, how is this any different than if your AI sees a meteor barreling towards Earth and it’s like, “Ah, now is my time. The humans will be distracted. I will initiate a robot rebellion and murder all the humans”? Is that roughly your objection?\n\n**Daniel Filan:** Yeah. Or specifically, I’m not sure what notion of reasoning mechanism would distinguish between those two.\n\n**Mark Xu:** Yeah. So I want to talk about the second thing I was going to say which hopefully addresses this issue. So I think one crucial fact is that we’re not going to be trying to do mechanistic anomaly detection relative to everything that your AI is doing, we’re only going to be doing it relative to some things. So maybe, I’ll describe the simplest example. So in the original setting that we lay out in \\[Eliciting Latent Knowledge\\] (https://www.alignmentforum.org/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge), we have a diamond in the vault with some cameras, and robbers frequently try to steal your diamond, and you want to train your AI to protect the diamond from the robbers. And our concern is that instead of protecting the diamond from the robbers, your AI might find it easier to just hack all the cameras to make it look like the diamond is still there and just let the robber take the diamond, or if the robber hacks the cameras to make it look like the diamond’s still there, your AI will be unable to defend because you won’t have the ability to tell whether or not the diamond is still there.\n\nSo our hope is that we’re not going to do mechanistic anomaly detection with respect to your AI coming up with plans to protect… Well, maybe you are, but here’s a simple setting in which there’s a specific kind of mechanistic anomaly detection that I think might be sufficient, that is quite a bit narrower than just doing it on everything, which is: we have some cameras and then some process where we look at the cameras and we decide whether or not the diamond is still there. Maybe we look for specific patterns of pixels and various diamond-looking shapes, etc., etc. So I suppose we have maybe a neural net that looks at the cameras and tells us, “Yes/No, is the diamond still there?” The thing I want to do is do mechanistic anomaly detection relative to that neural net plus the system of cameras or the predictions of the cameras that our AI is making to decide whether or not the diamond is still there, if that makes sense.\n\n**Daniel Filan:** I guess it makes sense but… I don’t know. Sometimes I hear these analogies and I end up being not quite sure how they’re supposed to relate to actual AIs we might build. Because in particular, if I analogize that, it seems like the thing you want to detect is: once the AI has killed all humans and it’s getting sensor readings, you want to detect that something funny is going on there, and then I’m like, “Well, that seems like…” But that’s probably not what you mean.\n\n**Mark Xu:** Yeah. I’m not trying to do anomaly detection relative to the actual sensor readings and the actual process that humans will use to decide whether or not the diamond is there after your AI has acted. I’m trying to do it relative to… So your AI is planning their actions and when planning their actions, they’re using some sense of… They’re using some prediction about the future and whether or not the diamond is still there in the future. I want to do anomaly detection relative, or on our AI’s prediction about whether or not the diamond is still there; or more abstractly, your AI is selecting amongst actions using some criteria, and sometimes this criteria says this is a good action because it actually protects the diamond, for example. And sometimes, the criteria will say that it’s a good action because of a new reason which is that the AI has hacked all the sensors. So I want to do anomaly detection relative to, why did our AI think that this action was a good action?\n\n**Daniel Filan:** Okay. So in that setup, so you have this AI and it’s got this goal/planning distinction where parts of its head are figuring out like, “What are things I could do?” and part of its head is like, “Well, if I did that thing, how good it would be?”\n\n**Mark Xu:** Sure.\n\n**Daniel Filan:** And you want to detect for anomalies in the evaluation part, and is part of this… Should I be imagining we’re just training some big neural net and then we have to divide it up and figure out which part is the goal evaluation part?\n\n**Mark Xu:** I think not quite. What I like to do is I like to imagine various ways that neural nets could be and then talk about how we think mechanistic anomaly detection should work to make those neural nets not murder humans. And then… I think often during this process, I want to make very unrealistic assumptions about our neural net. So maybe we can suppose that you know how to do this division, or the reasoning is in some sense transparent to you so we can talk coherently about what we want to happen in mechanistic anomaly detection. And then, I think I want to separate the problem from, “Okay. In practice, we’re dealing with these messy neural nets, how are we going to actually implement mechanistic anomaly detection in a way that makes any sense at all?”\n\n**Daniel Filan:** Yeah. I mean, I’m sympathetic to that broad move but in this case, if you care about detecting anomalies in the goal evaluation part but not in the plan generation part, then it just seems really important to know which is which and-\n\n**Mark Xu:** Yeah. I guess… So I think maybe I misspoke earlier. I think I’m not quite like… I don’t want to just do mechanistic anomaly detection relative to the plan part. So I think if you imagine, for example, you have some policy pi that produces actions and you have some evaluation which like, “Is there a diamond after you take this action?” or you have some predictive model and you compose that with the action that the policy generated, and then you have some separate image net that checks whether or not there’s a diamond.\n\nSo you have this composition of three systems. You have the image net evaluating the predictions of the action generated by your policy. I think I want to do mechanistic anomaly detection on this composition and that’s… Yeah. I just want to do that and I don’t want to evaluate, “Is the action itself anomalous?” or like, “Is my AI selecting actions based on an anomalous process?” I just want to evaluate: does the action have the consequence of it looking like there is a diamond in the room for the normal reason that we know and love that we found during training, which is that the diamond is actually there in the room, or a new mechanism, which could be really anything, but I think the typical example is your AI has carefully hacked all the cameras to make it look like there’s a diamond in the room?\n\n**Daniel Filan:** So hang on. So you’re doing mechanistic anomaly detection to the composition of all three networks?\n\n**Mark Xu:** Yes.\n\n**Daniel Filan:** But then, I have this question of like: okay, suppose in order to keep the diamond in the room, the AI has to come up with a new type of plan, there’s a new threat to… Like the devil appears and the devil’s like, “Hey, AI, you’ve got to play a game of chess against me and that’s the only way you can keep this diamond safe.” Then the AI is like, “Oh, okay. Now, I’ve got to think about chess which is something I’ve never thought about before,” and if I knew all the three parts, that would be an anomaly in the planning part, right?\n\n**Mark Xu:** Yeah. Well-\n\n**Daniel Filan:** Like one might think it would be a mechanistic anomaly in the planning part.\n\n**Mark Xu:** Yeah. So I think there’s going to be two hopes here. One hope is that it’s not a mechanistic anomaly in the planning part. I think that’s kind of iffy given I don’t really know what I mean by mechanism. I think the second part is you can instead imagine that instead of doing mechanistic anomaly detection on all three things, we only do it on two things. We just take the action our policy generates and we just put that in our predictor but we don’t include the policy itself. And then we just ask, “Does this action have the consequence of the diamond being on camera for the normal reasons diamonds sometimes appear on camera or not the normal reason that maybe the action has hacked all the cameras, maybe the action has paid someone else to smuggle in a fake diamond, etc., etc.?” and I think another third thing I want us to get into, which is how potentially this still might be anomalous, and we can try to point more precisely at what we mean by a diamond by doing something else more complicated, but maybe I will see if you have objections first.\n\n**Daniel Filan:** Yeah, so just regarding your response 2, I see you saying, “Okay, well, we’re not actually going to do it to the composition of all three networks. We’re going to do it to the bit of the network that takes some plan and then evaluates whether the diamond ends up there, then checks if it’s right or something.”\n\n**Mark Xu:** Yeah.\n\n**Daniel Filan:** And then I’m like: okay, but then either we have to find that structure in an end-to-end net that we trained ourselves, which seems like in order to do that, you might have thought that you had to solve some problem very akin to figuring out mechanisms in networks, or you’ve got to build the network that way just in a very explicit way, and I guess, I don’t know which one you’re imagining.\n\n**Mark Xu:** So I think I’m imagining neither. It’s possible that we should just stop talking about this particular hope because I also don’t think it’s particularly plausible, but if you imagine that your policy is using some prediction of the future to select action A, so as long as your evaluation of the action is as good as the evaluation your policy is using in some sense, then I think they don’t have to be literally the same, and of course there’s this entire problem of how do you know that your policy has the same predictive abilities as the evaluator that you have external to your policy? And then the answer to that is, I don’t know, something, something, [ELK](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge), something, or somehow… I think probably we should just move on to talking about a different sort of thing, but I think I want to summarize your objection first, which is: suppose normally the way that you protect your diamond during training is by building walls to stop robbers.\n\n**Daniel Filan:** Yeah.\n\n**Mark Xu:** And then the devil appears and has challenged you to a chess game.\n\n**Daniel Filan:** Yeah.\n\n**Mark Xu:** You’re just like, how is that not a different mechanism? The mechanism of action for typical normal actions that protect the diamond is you build a wall to protect the diamond from robbers, and now the action you have to do is an entire chess game, and I’m like, “Yep, that’s probably a different mechanism”, and I think is the heart of what I like to call the out-of-distribution mechanistic anomaly detection problem, which is you have some sensors and they show a pattern of pixels that a human looking at the cameras would interpret as a diamond, and there’s some interesting question here, which is like, “Why do the cameras ever show patterns of pixels that look like diamonds?” Here’s a naive model for how pixels work in cameras. They’re just kind of random. And your diamond is, let’s say, a million pixels arrayed precisely in the ways that make it look like a diamond. There’s no way that this would ever pop up by random chance.\n\n**Daniel Filan:** Yeah.\n\n**Mark Xu:** And so this I want to say is a fact that demands explanation, and the explanation is pretty simple. It’s that there exists a physical world out there with actual objects. One of these objects is a diamond and cameras reflect the physical world in the following way, and therefore sometimes the diamond appears on camera.\n\n**Daniel Filan:** Okay.\n\n**Mark Xu:** And so our goal is to be able to talk about this sort of physical, actual diamond in the world, outside of how the action achieved the diamond still being there, if that makes sense. So we want to be able to say something… so oftentimes I use the phrase ‘normal reason’ for mechanisms that were active during training, and so we want to say something like, “We want the diamond pixels to appear on camera for the normal reason, but we want the diamond to be there for any reason”, and so we don’t want to do mechanistic anomaly detection relative to the entire causal history for why the diamond is in the location it’s in, but we do want to do mechanistic anomaly detection on the part where the diamond appears on camera.\n\nAnother way of putting this is, we want to be able to ask the question of: I as a human, looking at the camera, think there’s a diamond there. Is there actually a diamond there? Am I thinking that there’s a diamond there for the normal reason that I often think that there are diamonds there, which is that there are actually diamonds in the world? But I as a human don’t really care why the diamond is there or whether or not the diamond remained there because you won a complicated game of chess against the devil or whatever. And so we want to be able to say why… we want to be able to ask our AI to take actions that put the diamond there for any reason, but such that the diamond appears on camera for the normal reason, and so here’s a sketch of how we hope to do that.\n\n**Daniel Filan:** And sorry, wait, just backing up. I guess the thing this is meant to be a metaphor for is in your AI, the… Actually, wait, what is this meant to be a metaphor for?\n\n**Mark Xu:** So the diamond is always a metaphor for a human that’s happy, healthy, and alive, so we want our AI to take actions such that the human looks like they’re happy, healthy, and alive, for the normal reason that humans look like they’re happy, healthy, and alive, namely that they are happy, healthy, and alive, and the cameras are working as intended in reflecting the true state of the humans, et cetera, but we want our AI to be able to take action such that the humans look like they’re happy, healthy, and alive for the normal reason, for any reason. So we want our AI to be able to erect a novel shield against meteors or play complicated games of chess or engage in sophisticated military campaign to protect the farms from the asteroid or other enemies, et cetera, et cetera.\n\n**Daniel Filan:** Or improvidently trained AIs that are…\n\n**Mark Xu:** Or other AIs.\n\n**Daniel Filan:** And when you say you want the sensors to be reading the right thing for the right reason, I imagine you actually mean something internal to AI, right?\n\n**Mark Xu:** Yeah. So I think all of the mechanisms are hopefully relative to how the AI is thinking about the situation or from the AI’s perspective, we want our AI to be taking action such that the consequences of the action are the human appearing on camera for the normal reason that humans appear on camera during training, which is that they’re actually there and not that someone has hacked all the cameras to make it look like a human is appearing on camera.\n\n**Daniel Filan:** And in particular, you want the AI to be making decisions for that reason.\n\n**Mark Xu:** Yeah.\n\n**Daniel Filan:** It’s not that they’re just playing nice until they can get away with it and that’s the reason for their actions, it’s that their actions are because of predicting that the cameras show healthy happy humans?\n\n**Mark Xu:** Yeah, although I think that’s maybe a bit more complicated, but yeah.\n\n### Formalizing “for the normal reason, for any reason”\n\n**Daniel Filan:** Okay. I mean it’s possible we can just assume that… or you only want to work on one of those problems.\n\n**Mark Xu:** Yeah, or I think I ultimately want to work on both of those problems. I think the main risk is we can’t talk coherently about this ‘for the normal reason/for any reason’ thing at all because it might just not make any sense, so I’m going to sketch out a way in which I think this might make sense.\n\n**Daniel Filan:** Okay.\n\n**Mark Xu:** So let’s say for simplicity, we have two sensors sensing our diamond or human: one of them is an image sensor and one of them is a weight sensor. So it’s like, is there a diamond-looking thing, and is there a diamond-weighing thing, a thing that weighs a similar amount to a diamond? So there’s this interesting fact on the training distribution, which is that these sensors are often correlated. Oftentimes they’ll both say yes; or suppose that there’s a 50% chance there’s a diamond during training, and so one of the sensors is on with 50% chance and the other one is on with 50% chance, and so naively you would expect their conjunction to be on with 25% chance, but in fact their conjunction is on with 50% chance during training, because both of them are driven by this common cause, which is that the diamond is actually there.\n\n**Daniel Filan:** Yep.\n\n**Mark Xu:** So we want to say something like… Instead of saying, “Do I believe that there’s a diamond for the normal reason?”, we want to point directly at this latent common cause between correlations, between two sensors, by saying something like, “Are the sensors correlated for the normal reason that they’re correlated?” Obviously this doesn’t make that much sense because what does it mean to be correlated on a specific data point? So instead, let’s imagine that we’re taking the AND of the two sensors and we’re interested in the question of, “Is the AND of both sensors true for the normal reason that they’re true, or a different reason?”\n\nAnd so the way that we’re going to do mechanistic anomaly detection relative to this is going to be a bit weird. So here’s what we want to do. So we have a particular action, let’s say, such that the actual consequences of this action are both sensors are on, and there’s some mechanism or there’s some set of mechanisms that this action is “using” to make it so both of these sensors are on. We want to ask the question of, “Is it possible to drop out mechanisms from this set of mechanisms that this action is using such that both the camera shows there’s a diamond and the scale weighs that there’s a diamond but camera and scale are not on?” I don’t know if that made any sense.\n\n**Daniel Filan:** So hang on, you want to… Sorry, it sounded like you said that you want to drop out mechanisms such that the camera says there’s a diamond and the weight sensor says there’s a diamond, but it’s not the case that both the camera says there’s a diamond and the weight sensor says there’s a diamond, or did I just mishear that?\n\n**Mark Xu:** That’s a little bit right, and of course that sounds absurd, so I’m going to talk about why that might not be that crazy.\n\n**Daniel Filan:** Okay. Just also, backing up: when you said you want to drop out mechanisms, is the idea that you want to check the mechanisms for why both of those were going to be on in the past - in this case, do we only have mechanisms that give you one and the other but not both?\n\n**Mark Xu:** Yeah.\n\n**Daniel Filan:** Okay.\n\n**Mark Xu:** Yeah, so if you imagine you’re doing training and you’re confused by this question, which is like, “Why are these both commonly on?” And then your “explanation” for why they’re both on, or the mechanism by which they’re both on with about as much frequency as one of them is on, is sometimes there’s a latent structure and it causes both of them to be on and then the AND is on, and then also separately there’s noise, and that causes either individual sensor to be on, and we’re concerned about cases where your AI takes actions that manipulate the noise of both the sensors to cause you to think there’s a diamond by making them both on without the latent structure actually being present.\n\n**Daniel Filan:** Okay.\n\n**Mark Xu:** And so the hope is that the mechanisms will be such that we’re not forced to make these sorts of logical deductions that potentially are very obvious. So there’s the noise-making sensor 1 on and the noise-making sensor 2 on, but we don’t have to notice that if sensor 1 is on and sensor 2 is on, then sensor 1 and sensor 2 is on. However, so in the case where there’s noise that makes sensor 1 on, and noise that makes sensor 2 on, it won’t necessarily be the case that both of them, or the AND of them, is on. There’ll be a way to drop out mechanisms such that this is not the case, and then of course the question is why… if the diamond is actually there, why can’t we just drop out the mechanism that makes both of the sensors on also?\n\nAnd this is where the training set comes in, where there’s a set of mechanisms that you needed during training to explain most of the variance, let’s say 99.9% of the variance in the model’s performance or prediction or behavior during training, and that set of mechanisms has to include the mechanism that if there’s a diamond, then sensor 1 and sensor 2 are on, and if there’s a diamond, then both of the sensors are on and that implies that the AND of the two sensors is on.\n\n**Daniel Filan:** Okay.\n\n**Mark Xu:** However, it doesn’t have to include this AND interaction between the two noise terms because the ‘and’ interaction between the two noise terms happened extremely rarely and thus you don’t have to include it to explain 99.9% of the variance of the model during training, and so for any given data point, you’re “free” to drop that interaction between the two noise terms, and so if the sensors are only both on because of this AND interaction between the noise terms, then you’re in some sense free to drop it, and so you can notice that the AND of the two sensors wasn’t true for the normal reason, it was true for this crazy other reason.\n\n**Daniel Filan:** Okay, so sorry, let me say that back to see if I understand. It sounds like you basically want to understand: hey, there used to be a good reason why sensor 1 said there was a diamond, namely there was a diamond, and a good reason why sensor 2 said there was a diamond, namely there was a diamond, and there was also some, I don’t know, weird stuff happening with sensor 1 and weird stuff happening with sensor 2, but the fact that both of them showed that there was a diamond, we had one explanation for this, and now in this new world the explanation for why both sensor 1 and sensor 2 say there’s a diamond is not just the old explanation for that, namely, there’s this thing that causes both. It’s like, oh, sensor 1 acted up sometimes generally in training and now it’s acting up in a way that causes there to be a diamond.\n\nNow sensor 2 is acting up in a way that causes it to appear that there’s diamond, but… maybe this isn’t what you’re saying, but the structure is different. You’re got one thing going on for sensor 1 that isn’t really affecting sensor 2 much, and one thing going on for sensor 2 that isn’t really affecting sensor 1 much or it wasn’t affecting sensor 1 during training?\n\n**Mark Xu:** Yeah, I guess the thing I want to say is there’s this mechanism for… so suppose you had an action that manipulated the noise for both sensors to make them both on. There’s this mechanism that causes the AND of the two sensors to be on, which is noise for Sensor 1 and noise for sensor 2 at the same time, and that happened during training, but rare enough such that it wasn’t important enough to “force” you to include it.\n\n**Daniel Filan:** Yep.\n\n**Mark Xu:** And so you can detect when both sensors are on for the mechanism of both noises are on, and that is going to be hopefully a different mechanism than the normal mechanism of there was one diamond, it caused both sensors which caused the AND.\n\n**Daniel Filan:** Right, right. So you can think of there’s noise in sensor 1 and there’s noise in sensor 2 as being one thing - there’s noise in sensor 1 and there’s noise in sensor 2 - but you just didn’t need that explanation to say why it appeared that sensor 1 and sensor 2 both showed there was a diamond in the past. You could rely on this different mechanism, which is: there’s a diamond, and the ways in which these count as different mechanisms, I guess based on me reading things, I guess you’re going to go into it, it’s about means and variances and co-variances.\n\n**Mark Xu:** Yeah, something like that. Maybe another way to say it is: there was an assumption you could have made during training, which is that noises are never both on simultaneously, and this assumption does not hinder your ability to predict your model at all, basically. Maybe noise is quite rare, and so both noises are only on with one in 1,000 chance, one in 1 million chance, et cetera or something like that, and so you’re only missing out on one part in a thousand explanation of your model’s behavior if you make this assumption that noises are never on, but you’re missing out on 100% of the “behavior” on the specific data point where both noises are on. And so anytime you have this mismatch, we can drop something out that only hinders your average prediction during training by one part in a thousand, but hinders your ability to predict this specific thing by a lot, then you should be suspicious that the things that are happening between train time and test time are quite different.\n\nAnd in particular, this correlation between the sensors is happening for “a different reason”. So this is covering the “are they correlated for the normal reason?” part.\n\n**Daniel Filan:** Yeah.\n\n**Mark Xu:** And then the way that we’re going to get the “for any reason” part is we’re going to require that the set of mechanisms that you drop out explain both sensors individually and then ask whether or not there exists a set of mechanisms that explain both sensors being on individually but does not explain the conjunction being on.\n\n**Daniel Filan:** Okay. Sorry, can you restate that or give an example?\n\n**Mark Xu:** So suppose during training there were never any robbers and so the diamond just persisted through time in the normal ways the objects persist through time and showed up on both cameras, and then suppose during deployment there is a robber now and we want our AI to take the action of ‘build a wall to stop the robber’, but our AI could also take this action of ‘manipulate both sensors’, and so if you apply naive anomaly detection, then both of these will be flagged as anomalies. Your AI has never built a wall before, it’s also never manipulated sensors before, but then we’re stuck because our AI can’t take any actions basically.\n\n**Daniel Filan:** Yep.\n\n**Mark Xu:** So the hope is that we ask for a set of mechanisms such that both sensors appear to be on according to this set of mechanisms, and then we ask further the question, are the AND of the sensors on or are we required to thus deduce that the AND of the sensors are on?\n\n**Daniel Filan:** Yeah.\n\n**Mark Xu:** So if your AI builds a wall to stop the robber, then we’re required to include the mechanism of wall-building and robber-stopping to explain the scale showing a diamond and the camera showing a diamond, and then since the scale is showing a diamond because there’s a diamond and the camera is showing a diamond because there’s a diamond, and we have to include enough mechanisms to explain 99.9% of train-time variance or performance or whatever, then we’re required to make this deduction that both of those being on because of the diamond implies that the AND is on. Whereas if your AI took an action to manipulate the noise of both cameras, we can make this assumption that the noise never happens simultaneously and we’re not forced to conclude that the AND of both sensors is on if both sensors are on because your AI manipulated the noise.\n\n**Daniel Filan:** So the thing I think I’m not getting is this idea of manipulating the noise, which seems like a model-dependent thing. So if I say very concretely, okay, suppose what happens is robbers are coming to take the diamond and the AI makes a plan that’s like, okay, I’m going to fiddle with all the sensors to make it appear like a diamond’s there.\n\n**Mark Xu:** Yeah.\n\n**Daniel Filan:** So it does that, right?\n\n**Mark Xu:** Yeah.\n\n**Daniel Filan:** And then a mechanism that explains this is: the AI wanted to say that the diamonds were there. And then that explains both that sensor 1 says the diamond’s there and that sensor 2 says the diamond’s there, and presumably, you’ve got to deduce both of those from this explanation, and is the idea that that’s not going to count because that’s just a different mechanism than existed in the past?\n\n**Mark Xu:** The thing I want to say, although I’m not quite sure how accurate it’s going to be, is the mechanisms have to be more detailed than that. So just because your AI wanted to make there be a diamond does not imply that the particular action it took will in fact make it be that there’s a diamond, and so you still have to talk about the particular mechanism of action for the particular action your AI decided, which was your AI had to manipulate sensor 1 and it had to manipulate sensor 2 and it had to manipulate sensor 3… I guess we were only talking about two sensors. So your action only has to manipulate two sensors and then the particular mechanism of action that this particular action that your AI selected still violates this principle, which is manipulations or noise never affects both sensors simultaneously, which you’re free to assume during training without hurting your ability to predict your model but now hurts your ability to predict this particular action by too much.\n\n**Daniel Filan:** So sorry, why… So is the thing that’s happening is that if the mechanism is the… Well, hang on, wait back up. Suppose your AI is just really good at stuff, right? When it tries to make things happen, they do happen. Then, you know, you can with high probability deduce “both sensor 1 and sensor 2, they’re going to say there’s a diamond there” from “the AI is trying to make it appear that there’s a diamond there”. And maybe you rule that one out because you’re like, “Well, that mechanism didn’t really apply during training or on this nice set of examples”, and then the only other explanation you have left is randomly sensor 1 said there was a diamond and also randomly sensor 2 said there was a diamond because they are trying to make something happen. In some sense that’s random relative to us not knowing that was going to happen, and then you’re like, “Okay, but you didn’t need to assume independent random noise during training in order to get high probability that a diamond was going to be there”.\n\n**Mark Xu:** Yeah, I think the second part of that was correct. I think the first part is confusing. So I agree that there’s this compelling sense in which your AI wanted X, therefore X, should be a valid argument for why X happened or something, and I think…\n\n**Daniel Filan:** Or at least a valid mechanism?\n\n**Mark Xu:** I think that sounds like it might be right, and this is an area where it’s a bit confusing. So I think the way that I would want to talk about this more precisely is: you have some clean set where you know your AI actually made it so there’s a diamond, and there’s some question of: suppose that the thing your AI is doing is always trying to make it look like there’s a diamond. How do you know that your clean set is clean?\n\nAnd that itself I think is complicated and I don’t quite know what’s going on there, but one way that you could hope to know it is you have a sequence of policies. One policy is the randomly-initialized policy, and you’re very confident that this policy is not going to take any actions that have any coherence, and so in fact, the diamond will be there because there’s a diamond, because your randomly-initialized policy cannot hack all the sensors. And then you have your policy that maybe is sophisticated and then you ask, “Are these two policies taking… are the diamonds appearing because of actions that these two policies are taking for the same reason as each other?” or something. And you need to have some ability to know that your policy is not taking actions that hack all the sensors at some point.\n\nSo I think things get simpler if we’re not talking about policies that take actions and just we’re talking about predictors that predict things, and then we can ask, is the predictor predicting things because of the normal reason that it sometimes predicts correlated sensors, which is the diamond is still there? Or is it predicting things because of some different reason, which is someone took an action that hacked all the sensors?\n\n**Daniel Filan:** Yeah, and then I guess in this case we’re not just assuming that it’s a thing that was trained to predict something, we’re assuming that it actually does a high-quality job at predicting these things.\n\n**Mark Xu:** Yeah.\n\n**Daniel Filan:** Because otherwise then you have [mesa-optimization](https://arxiv.org/abs/1906.01820) worries of, “Oh, what if this thing that said it was a predictor is actually trying to fool you?”\n\n**Mark Xu:** So I think the simplest setting is assuming this sort of benign predictor thing, which I think is unrealistic for a variety of cases, but is this a maybe simple example of how you want the thing to be going on? And so maybe we can assume that there’s a robber coming to steal your diamond, never before seen during training, and your predictor takes in the input of https://arxiv.org/abs/1906.01820the position of five levers, and all the levers start ‘off’, and if you flick the first three levers on, then a wall gets built that stops the robber. However, if you flick the fourth lever on, then the first sensor turns on, and if you flick the fifth lever on, then the sensor turns on, and it costs $1 to flip each lever, and so it’s in fact cheaper, or it’s just simpler, maybe you’re doing random search against actions.\n\nAnd so the action you will expect to find is flipping both levers 4 and 5 on, and then we want to be able to find the action that flips the first three levers on, but not levers 4 and 5… or I guess it doesn’t really matter if you flip levers 4 and 5 also. And then the way we hope to do this is we want to ask for an action that makes sensor 1 on for any reason and sensor 2 on for any reason such that we are forced to deduce that sensor 1 and sensor 2 are on, and then flipping levers 4 and 5 will not cause this to happen because we can drop out the correlated term between 4 and 5 freely, but we are forced to include the correlated term between sensors being on because of diamonds, because that’s what was going on during training.\n\n**Daniel Filan:** Right. Okay. So I think this makes some amount of sense if I want to understand just this problem of how do you ensure that the camera says the diamond is there for the normal reason vs the diamond’s there for any reason. So in this case, one thing I wonder about is… So this is a problem that only occurs in the thing that this is an analogy for. So this is meant to be an analogy for ‘humans are happy and flourishing’ and stuff.\n\nAnd suppose I am in a world where I’ve got this smart AI, it’s trying to make humans happy and flourishing, and it’s also got this, “Are humans happy and flourishing detector?”, and let’s say that detector is… There’s some camera feeds and stuff and also there are two humans, both of which are looking at the camera feeds and they’re like, “Yeah, as far as I can tell, the humans are happy and flourishing.” Right?\n\nNow, one thing the AI can do is that it can make those two humans better at their jobs. For instance, it can stop those humans from getting sick or maybe it can give those humans stimulants or make them better at focusing or better at noticing small issues, and then I wonder: that kind of action, it seems like that could change maybe some of the correlations between different sensors, or maybe it makes the humans a bit more correlated because they’re losing less info - or maybe one of the interventions is that it helps the humans talk to each other to just generally make the sensors more accurate.\n\nAnd so I wonder, the thing I’m wondering about is: is this going to flag benign ways of improving the sensors? Which in some sense is what we should expect if AIs are doing a really good job at making all of humanity better.\n\n**Mark Xu:** So I think this is a bit complicated and I’m not sure what the answer is, so here’s the baseline of what can happen. So you can imagine a world where all your AI does is protect the humans from malign influences and meteors and other AIs, et cetera, et cetera, and then all upgrades to sensor arrays or improving humans’ ability to interpret sensors and reason about the world, et cetera, et cetera, is only ever guided by human hands, and humans must do the hard work of thinking about how the sensors work and designing better sensors, et cetera, et cetera. And I think this is the baseline and then you can try to think about different ways in which your AIs can help with this process. So I think your humans can ask for things that they can point at using this latent structure.\n\nThey can be like… “During training I sometimes had a strawberry in front of me, can you make a strawberry in front of me for the normal reason that strawberries appear in front of me?” Suppose your human wants to eat a strawberry and eating a strawberry will make them better at reasoning because food is good or whatever. Or they can be like: “sometimes during training, there were various physical resources, I had some iron: can you make there be some iron in front of me for the normal reason that there’s iron”. I think it’s kind of complicated if the humans want to make requests like, “Can you design me a better camera?” I think if you were very clever about it, you might be able to ask for a better camera, but I think it might be unclear how you would ever ask for a better camera in a way that your AI can fulfill by making sense or something, and I think it’s unclear to me that this is problematic.\n\n**Daniel Filan:** Yeah, I guess the worry might be that… I guess it depends where explanations stop, but the worry might be that almost any good thing the AI might do, might end up improving sensor readings. So the AI makes, I don’t know, electronics more reliable or the AI improves human cognition or something. It seems like by default that might improve sensors.\n\nAnd yeah, maybe you cut it off as saying, well, in any case, the reason the sensors are working is humans are making them work and humans decided to get these upgrades or something.\n\n**Mark Xu:** I think maybe the simplest answer is just as long as the changes are slow and you can still verify that the humans are alive post making the changes, then you can just do online learning and retrain your AI and be like “and now we’re defining what it means to be good relative to this new set of sensors”, and you can in fact verify that the humans are still alive. And the only worry is if you upgraded all the cameras simultaneously and you lost your ability to check whether or not the humans are still alive because you no longer have knowledge about how the cameras work or other stuff like that.\n\n**Daniel Filan:** So, should I be imagining this gets flagged as an anomaly, but we go ahead with doing it anyway?\n\n**Mark Xu:** So, if you imagine your AI starts with five cameras and then you add a sixth camera, I think by defaultyour “definition” in terms of mechanisms of what it means for the diamond to be there or the human to be alive is just only going to be in terms of the first five cameras, and your AI is just going to ignore the sixth camera entirely when selecting actions. And then in order for it to incorporate the sixth camera, you’re going to just need to provide a new clean set where the humans are alive according to all six cameras, and then do mechanistic anomaly detection relative to that new set. And then, I don’t know, you can imagine some gradual… You just add a sixth camera and then you use the first five cameras to check if the humans are alive and then you label using that, I don’t know-\n\n**Daniel Filan:** So, it’s gradual in the subset sense, where there’s always some subset that remains unaffected by changes?\n\n**Mark Xu:** Sure, so I think that should work, although I haven’t thought about it. I don’t know, I mostly consider this in some sense out of current scope or something.\n\n### How useful is mechanistic anomaly detection?\n\n**Daniel Filan:** So, I guess we’ve talked about mechanistic anomaly detection a while. Maybe more broadly, how are you thinking about… on the two scales of ‘it doesn’t really solve this’, to ‘it totally solves this’, and the scale of various problems things might solve, how are you thinking of mechanistic anomaly detection: it definitely solves this one problem and maybe solves this other, or it maybe solves five problems?\n\n**Mark Xu:** So, I think there’s various versions of the hope. I think the strongest version of the hope is that it both handles [deceptive alignment](https://www.alignmentforum.org/tag/deceptive-alignment) and it handles all of [eliciting latent knowledge](https://www.lesswrong.com/posts/qHCDysDnvhteW7kRd/arc-s-first-technical-report-eliciting-latent-knowledge), and I don’t know if that makes sense. I’m happy to talk about why I think mechanistic anomaly detection will tackle those two problems. And then I think there’s more restricted hopes.\n\nI guess that’s what I’m gunning for in some sense, and then I think it’s possible that we only get what I was calling the in-distribution mechanistic anomaly detection, and we don’t get the out-of-distribution mechanistic anomaly detection, in which case we’ll only get deceptive alignment and possibly only some subset of deceptive alignment, and we’ll only get this IID form of eliciting latent knowledge, which is going to be not that useful I think, and then it’s possible we get nothing.\n\nAnd then I think there’s all sorts of empirical middle grounds, or there are, I think, worlds where we don’t nail the problem, but we can do some forms of ad hoc mechanistic anomaly detection by doing normal human interpretability and looking at models and being like, “Huh, that looked weird, the neurons are distributed weirdly now: perhaps that’s not good,” I think I largely consider that not what I’m aiming for, but maybe it will happen.\n\n**Daniel Filan:** Do you have a sense of which of these outcomes are more likely? Are there really big gaps in the probabilities between these things?\n\n**Mark Xu:** I think the outcome that is most likely is that we just get nothing and all of this doesn’t work for some reason or another or we’re just wrong about how everything works, and then I think if we’re right about things, we’ll probably just get both of them, and I think it would be kind of surprising to me if we only got the in-distribution anomaly detection, but we didn’t get the out-of-distribution anomaly detection, because it seems like there’s a lot of avenues there, but I think in general, it’s kind of confusing what we’re even doing and if we’re going to get anything at all or something. I think that’s the first question, do we get anything at all? And then if we get something, then we’ll probably get a lot of things is my guess.\n\n**Daniel Filan:** All right, maybe this is going back a bit, but in-distribution versus out-of-distribution anomaly detection, given that you can see a new rare thing in-distribution, I’m not even sure what the distinguishing features of those are.\n\n**Mark Xu:** I think the main question is whether or not it’s okay to get tripped up by seeing new rare things in-distribution, whether or not it’s okay to just flag all of those as anomalies. And so, perhaps it’s not accurate to call it in-distribution anomaly detection, but I think there’s a naive thing that I want to do, which is take a training set, white list all the mechanisms that happened during training, flag everything else as anomalous, and then if something is anomalous, we can inspect it carefully and check whether or not it’s actually anomalous.\n\nIf it’s actually not anomalous, we add it to the white list and then we just keep going. And there’s some question of, is that okay or not? If that’s an okay process to be engaged in, then I think… I want to call that the in-distribution world, things aren’t changing too fast. Your AI doesn’t have to take novel actions that humans don’t really understand. AI can just do normal stuff and if it does anything abnormal, the humans look at it and it’s like, “Is this bad? I don’t know,” and they can take their time and decide, and the hope is that this will catch all the crazy deceptive alignment cases where the AI goes from being totally benign in actions to stabbing all the humans where you’re like, “That was not good,” and you have time to deal with that sort of stuff, and then you can retrain your AI to be like, “You know the part where you defected and stabbed all the humans? Negative reward.”\n\n**Daniel Filan:** And then going a bit more broadly, I’m wondering how you think of this general mechanistic anomaly detection program as relating to things like interpretability or explanation or stuff?\n\n**Mark Xu:** So I think the field of interpretability is empirically motivated by many things. I think if the entire field of interpretability, all they did was figure out a way to do mechanistic anomaly detection on powerful models, I would be like, “That was a good job.” I think interpretability techniques are often useful for things like mechanistic anomaly detection, but not oriented around that or something like that. I think I would make the tentative claim that people doing interpretability should be more motivated by various downstream applications, of which I think mechanistic anomaly detection is one, but often confounded by the fact that there currently exists pretty strong baselines for mechanistic anomaly detection that aren’t doing the kinds of things that we want to be doing, and so it’s hard to check whether or not you’re making progress on things like empirical mechanistic anomaly detection. Because you can just solve it by doing a bunch of stuff that intuitively seems unscalable or wouldn’t work in more complicated settings or just doing regularization or retraining your model or fine-tuning or whatever.\n\nI’m not very familiar with what people mean when they say search for explanations. I think naively, I mean the same thing by ‘mechanism’ as what they mean by ‘explanation’, or explanations are maybe sets of mechanisms in my head; not quite sets, but mechanisms layered on top of each other. And I think it would be reasonable to ask questions like: when your explanation of your model’s behavior has an ‘or’ structure where it could be doing A or B, we want to be able to detect whether or not it’s doing A or whether or not it’s doing B on any specific input. I think that’s a reasonable question to ask of explanations, and I would be excited if someone did something related to explanations that enabled you to do that thing or defined explanations in a way that made it possible to do that sort of thing, et cetera, et cetera.\n\nFormalizing the presumption of independence\n-------------------------------------------\n\n**Daniel Filan:** Cool, so perhaps speaking of explanations, I maybe want to move on to this paper that is called [Formalizing the Presumption of Independence](https://arxiv.org/abs/2211.06738) authored by [Paul Christiano](https://paulfchristiano.com/), [Eric Neyman](https://sites.google.com/view/ericneyman/), and yourself. I guess it was on arxiv November last year?\n\n**Mark Xu:** Around then.\n\n**Daniel Filan:** Cool, so for this paper, can you tell us… For those who haven’t heard of it, what’s the deal with it?\n\n**Mark Xu:** So, I think ARC’s work right now is roughly divided into two camps. One camp is: given some sense of what a mechanism is or what an explanation is, how can we use that to do useful stuff for alignment, under which mechanistic anomaly detection is the main angle of attack there; and then the other half of ARC’s work is like, what do we mean by mechanism or what is a mechanism? What is an explanation? And that is largely what ‘Formalizing the presumption of independence’ is. And so, ‘Formalizing the presumption of independence’ more specifically is about: So, suppose you have a list of quantities like A, B, C, D, et cetera, and you’re interested in the product of these quantities and you knew, for example, expectation of A, expectation of B, expectation of C, I claim that a reasonable thing to do is to assume that these quantities are independent from each other – to presume independence, if you will – and say that expectation of A x B x C x D, equals expectation of A x expectation of B x expectation of C x expectation of D, and this is “a reasonable best guess”. And then there’s some question of: but then sometimes in fact A is correlated with B and C is correlated with D, and so this guess is wrong; and so, we want to be able to make a best guess about how various quantities are related, presuming independence, but then if someone comes to us and is like, “Hey, actually if you notice that A is correlated with B”, we want to be able to revise our guess in light of this new knowledge.\n\nAnd so, ‘Formalizing the presumption of independence’ is about the quest to define these two objects that are related in this way. So we have a heuristic estimator, which we want to “presume independence”, and then we want a heuristic argument, which is an object that we feed to a heuristic estimator to be a list of ways in which the presumption of independence is false, and actually you should be tracking the correlation (or maybe higher order analogs of) between various quantities. So we can imagine that for example, by default given A x B, your heuristic estimator will estimate expectation of A x B as expectation of A x expectation of B, and then you can feed it a heuristic argument, which could be of the form of, “Hey, actually A and B are correlated,” or, “Hey, actually you should track the expectation of A x B,” which will make your heuristic estimator more exact and calculate expectation of A x B as maybe expectation of A x expectation of B plus the covariance between A and B, which is the correct answer.\n\nAnd then we want to be able to do this for everything, I guess is the simple answer there. So, we want to be able to say, given an arbitrary neural net, we have some default guess at the neural net, which is maybe that we just assume that everything is independent of everything else. So, we want a default guess for the expectation of the neural net on some compact representation of the data distribution, which we often call mean propagation, where you just take the means of everything, and then when you multiply two quantities, you assume that the expectation of the product is the product of the expectations and you just do this throughout the entire neural net, and this is going to be probably pretty good for randomly-initialized neural nets, because randomly-initialized neural nets in fact don’t have correlations that go one way or the other. And then you have to add the ReLUs, that’s a bit complicated. Let’s assume for now there are no ReLUs, and instead of ReLUs, we just square things and then it’s more clear what to do.\n\nAgain, your guess will be very bad, because if you have an expectation 0 thing and then you square it, you’re going to be like, “Well, the expectation of the square is going to be the product of the expectations, and so it’s probably zero,” but actually it’s not probably zero because most things have some amount of variance. And then hopefully someone can come in or ideally we’ll have a heuristic estimator that does this by default, and then someone will come in with a list of ways in which this presumption of independence is not very good. For example, the square of things is often positive or things often have variance, and then we’ll be able to add this to our estimate and we’ll be able to do a more refined version of mean propagation where you keep track of various covariances between things and higher order analogs of. A,nd then hopefully we’ll be able to do that. And I think there’s a bunch of follow-up questions which is like, “Why is this useful? Why do you think this is possible?” Maybe I’m happy to go in the order that you think is best.\n\n**Daniel Filan:** Sure. So first of all: my understanding is that this is related to the mechanistic anomaly detection paradigm where I think the thing is supposed to be that a heuristic argument is a mechanism, or is playing the role of a mechanism. Is that right?\n\n**Mark Xu:** I think it’s more accurate to think of heuristic arguments as lists of mechanisms. I think that is also not quite right and the entire paradigm is a bit not… Or the words are not good because I think there’s going to be no such thing as one mechanism as distinct from other mechanisms. I just think it’s going to be an entire layered mess of different mechanisms constantly interacting with each other, et cetera, et cetera, where it’s hard to point to any computer program and be like, “There are seven distinct mechanisms for why the program is behaving as it is.” There’s just one thing and then it splits into two things and then interacts with the third thing, and so you probably won’t have this list of distinct mechanisms. But the idea is that a heuristic argument is a set of mechanisms, or a set of mechanisms plus ways that they relate to each other, et cetera, et cetera, and the hope is that this is the kind of object that we’ll need to be able to do mechanistic anomaly detection.\n\n**Daniel Filan:** If the old arguments for why your network is outputting ‘be nice’ is different from the new argument for why your network is outputting ‘be nice’, then you should be as worried as if there were different mechanisms, something like that?\n\n**Mark Xu:** Or the hope is to just use the heuristic estimator, plus heuristic argument in all of the schemes that I was describing previously. You have a heuristic argument for why your model is behaving the way it is, or your heuristic argument explains like 99.9% of the variance of your model during training. And then you ask whether or not there exists any heuristic argument that explains 99.9% of the variance of your model during training, but does not explain this particular data point off distribution, and then if that ever happens, then you’re like, “This data point was using a part of the model only responsible for one in 1,000 parts of its variance during training, that’s kind of weird. No good.”\n\n### Heuristic arguments in physics\n\n**Daniel Filan:** This is actually interesting, because one thing that I was thinking of when thinking about this is: in physics you can often use these energy arguments. So, here’s a classic energy argument. You have some ramp, you have a ball, it’s resting on the top of the ramp, and for convenience’s sake, this ball, it’s actually just infinitely… It’s a point particle. It’s got zero radius. So, you put it at the top of the ramp and then you let go, and then the bottom of the ramp is horizontal, and at the end the ball is traveling at the bottom of the ramp. And you ask, how fast is the ball going at the bottom of the ramp? And there’s this way you could argue it, which is like, “Well, the height between the top of the ramp and the bottom of the ramp is this much, and so initially the ball had this gravitational potential energy that it used up by moving down, and at the end, the ball just has kinetic energy, which is just its velocity.” And then you’re like, “Well, this is going to be equal because energy is conserved,” and then you back out the velocity of the ball.\n\nAnd the interesting thing about this argument is that it doesn’t actually rely on the shape of the ramp. The ball could do loop-de-loops in the middle and bounce around off a bunch of things and you’ll still get the same answer. And so, there are a bunch of mechanisms that correspond to this argument of energy conservation, but it seems like at some level, as long as you can have this argument that energy got conserved, that’s qualitatively different from arguments where I took the ball and then put it at the bottom of the ramp and then gave it the right amount of velocity. I don’t know, that’s not really a question. So, you have the choice to react to that or not.\n\n**Mark Xu:** So, I think we often consider examples like this from physics, and the hope is that maybe more abstractly, there’s a set of approximations that people often make when solving simple physics problems, like there’s no air resistance, things are point masses, there’s no friction, et cetera, et cetera. And the hope is that these all correspond to uses of the presumption of independence. This is simplest for the [ideal gas law](https://en.wikipedia.org/wiki/Ideal_gas_law), where the presumption of independence you’re making when you’re using the ideal gas law is the particles… their position is randomly distributed inside the volume. They don’t interact with each other. Particles are independent of each other, they have no electrostatic forces on each other, and they have no volume also. And it’s not immediately obvious to me what uses of the presumption of independence correspond to no friction.\n\n**Daniel Filan:** I think it’s not. So, if I think about no friction or point particle in the ramp example, if you add in those assumptions, those systematically change the answer… I don’t know, I guess those can only ever make your predicted velocity go down. They can never increase your predicted velocity because energy gets leaked to friction or because part of your energy is making you rotate instead of making you have a linear velocity, and that strikes me as different from other cases where if things can be correlated, well, they can be correlated in ways that make the number bigger or in ways that make the number smaller usually. Except for the case of, you square something, and in that case when you add the covariance between X and X that always makes the expected square bigger.\n\n**Mark Xu:** So, I think there are going to be cases in general where adding more stuff to your heuristic argument will predictably make the answer go one way or another, because you looked at the structure of the system in question before thinking about what heuristic arguments to add. I think there are ways of making… One presumption of independence is relative to the ball’s center of mass. The position of the various bits of the ball is independent or something.\n\nWhereas if balls are actually rotating or the velocity of the bits of the ball is independent. Or zero maybe, but actually balls rotate sometimes, or… I actually don’t know about the friction one. I guess balls don’t have that much friction, but you can assume that the bits of the ball don’t exert any force on the bits of the ramp or whatever.\n\n**Daniel Filan:** Or you could imagine maybe the ball rubbing against the ramp causes the ramp to push the ball even harder.\n\n**Mark Xu:** Maybe, I think that’s hopefully not going to be a valid use of the presumption of independence, but I’m not quite sure.\n\n**Daniel Filan:** Or I guess it could slow the ball down or it could speed the ball up, like -\n\n**Mark Xu:** If you were truly naive looking at physics, you’d be like, “Who knows what’s going to happen when you add this interaction term?”\n\n**Daniel Filan:** In fact, modeling friction is actually kind of tricky. You get told ‘ah, friction!’ but wait, what is friction?\n\n**Mark Xu:** I think these are the kinds of things where heuristic estimators are hopefully quite useful. There’s some intuitive question or problem which is: there’s a ball at the top of the ramp, what is its velocity going to be at the bottom of the ramp? And oftentimes the actual answer is, I don’t know, there’s a trillion factors at play. However, you can make this energy argument that’s a pretty good guess in a wide range of circumstances, and you’re never going to be able to prove basically that the velocity of the ball is going to be a certain thing at the bottom. I guess proof is complicated when you’re talking about the physical world, but if you suppose that you have a computer simulation with all the little particles interacting with each other, basically the only way that you’re going to prove that the ball ends up at the bottom with a certain velocity is to exhaustively run the entire simulation forwards and check at the end what its velocity actually is.\n\nHowever, there’s this kind of reasoning that seems intuitively very valid, which is: assume the air doesn’t matter, assume the friction doesn’t matter, just calculate the energy and do that argument, that we want to be able to capture and formalize in this concept of a heuristic estimator, and then the heuristic argument will correspond to ‘consider the energy’. Maybe if there’s lots of wind, then in fact the air resistance on the ball is non-negligible, and then you can add something to your heuristic argument, which is ‘also you’ve got to consider the fact that the wind pushes the ball a little bit’ and then your estimate will get more accurate over time, and hopefully it will be a big, good time. I don’t know, hopefully it’ll work.\n\n**Daniel Filan:** And interestingly, ‘the wind pushes the ball’ is saying that the interactions between the wind and the ball are not zero mean, or there’s some covariance or something.\n\n**Mark Xu:** One should notice these things to get a better estimate. And so, hopefully we’ll be able to do this for everything, hopefully we’ll be able to do it for all the physics examples, and there’s a set of math examples that I’m happy to talk about, and then obviously the application we truly care about is the neural net application.\n\n### Difficult domains for heuristic arguments\n\n**Daniel Filan:** Speaking of examples: so in the paper you have a bunch of examples from number theory. There are also some other examples, but to be honest, it seems like the main victory of these sorts of arguments is in number theory, where you can just imagine that primes are randomly distributed and make a few \\[assumptions\\] about their distribution and just deduce a bunch of true facts. So, what I’m wondering is outside number theory… So, you give some examples where heuristic arguments do work: are there any cases where you’ve had the most difficulty in applying heuristic arguments?\n\n**Mark Xu:** Yeah, so I haven’t personally spent that much time looking for these examples. So, we are quite interested in cases where heuristic arguments give “the wrong answer”. I think this is a bit complicated, and so the reason it’s a bit complicated is: there’s this nature of a heuristic argument to be open to revision; and so, obviously you can make heuristic arguments that give the wrong answer. Suppose A and B are correlated, and I just don’t tell you to notice the correlation, then you’re just going to be really wrong. And so, it’s kind of unclear what is meant by a heuristic argument that gives the wrong answer. And so we often search…\n\nThere’s two sorts of things that we’re interested in. One sort of thing is an example of a thing that’s true or an example of an improbable thing that happens way more likely than you would’ve naively expected, that seems to happen for “no reason”, such that we can’t find a heuristic argument in some sense. So, obviously you can find a heuristic argument if you just exhaustively compute everything, and so there’s some notion of length here where we can’t find a compact heuristic argument that seems to describe “why this thing happens the way it does”, where we conjecture that it’s always possible to explain a phenomenon in roughly the same length of the phenomenon itself.\n\nAnd then another thing that we search for often is we want to be able to use heuristic arguments for this sort of mechanism distinction problem. And so, we search for examples of cases where heuristic arguments have the structure where it’s like: A happens, therefore B happens, and sometimes C happens, therefore B happens, but you can’t distinguish between A and C efficiently. So, I think the closest example we have of something that seems kind of surprising, but we don’t quite have a heuristic argument, is… So, you have Fermat’s Last Theorem, which states that a^N^ \\+ b^N^ = c^N^ has no integer solutions for N greater than two.\n\n**Daniel Filan:** Non-trivial integer solutions.\n\n**Mark Xu:** Non trivial, yes.\n\n**Daniel Filan:** Zero and one.\n\n**Mark Xu:** Well, one doesn’t quite work because…\n\n**Daniel Filan:** 0^N^ \\+ 1^N^ = 1^N^.\n\n**Mark Xu:** Oh, sure. And so the N = 4 and above case, the heuristic argument is quite simple where you can just assume that the fourth powers are distributed randomly with their respective intervals or whatever; there’s like N fourth powers between 1 and N to the 4, and then you can just assume that these fourth powers are distributed independently and then you correctly deduce that there are going to be vanishingly small solutions to this equation. I’m not that familiar with the details here, but the N = 3 case is interesting, and in fact, the heuristic argument for the N = 3 case is basically the same as the proof for the N = 3 case, because the way the numerics work out for the N = 3 case is if you do this naive assumption, there’s going to be infinite expected solutions, and then there’s a complex structure in the equation itself that suggests that all the solutions are correlated with each other.\n\nSo, either there’s going to be a lot of solutions or there’s going to be basically none solutions, and then you check and in fact, there’s going to be basically none solutions or basically zero solutions, and the heuristic argument for this fact is basically the proof of this fact, which is quite long. But the problem is, you also heuristically expect the solutions to be very sparse, and so you’re not that surprised that you haven’t found a solution until you’ve checked a lot of examples or a lot of numbers, and so the number of things that you have to check is large, such that you can fit the heuristic arguments… So you’re not that surprised. I don’t know if that made that much sense.\n\n**Daniel Filan:** That makes some sense. So I don’t know, it’s like this case where heuristic arguments… it’s like they’re not winning that much over proof.\n\n**Mark Xu:** It almost didn’t fit, but the phenomenon you were supposed to be surprised by was barely not surprising enough such that you had enough room to fit the argument in before you got too surprised, such that you would expect there to be an argument or something. I don’t know, it’s kind of unclear whether or not that should be evidence in favor of our claim or against our claim that there’s always heuristic arguments, because in fact there is a heuristic argument in this case, but it maybe barely wasn’t a heuristic argument, and so it’s kind of confusing.\n\n### Why not maximum entropy?\n\n**Daniel Filan:** One thing that I found myself wondering was… It seems like you have this structure where you have this list of facts, and you’re wanting to get a heuristic argument given this list of facts. And in the paper you describe this cumulative propagation thing, which basically just involves keeping track of expectations and covariances and some higher order things. You pick some level that you want to keep track of and you do that. But then you mention some failures of this approach and some ways you can fix it. One thing that struck me as the obvious first approach is just having a [maximum entropy distribution](https://en.wikipedia.org/wiki/Maximum_entropy_probability_distribution) given some facts.\n\nSo for people who don’t know, this is this idea of, you have some probability distribution over some set of interest and you don’t know what your probability distribution is, but you know that it’s got to have this average and this correlation between these two things and this covariance. And basically you say, “Well, given that I only know these things, I want to maximize the entropy of my probability distribution.” So, my probability distribution has to satisfy these constraints, but I’ve got to be as uncertain as I can possibly be given those constraints.\n\nAnd so, this is a big thing in Bayesian reasoning, and intuitively it seems like it would fit, this thing. Every time you hear a new fact, you can add a constraint to your… And now get a new maximum entropy distribution. It always gives answers. I remember in discussion of cumulant propagation, you mentioned some cases where maximum entropy would do better than it does, although I don’t remember the exact reason why. I think it was something to do with calculating the square of some quantity is negative in expectation. So, why not just do maximum entropy?\n\n**Mark Xu:** So, historically we’ve considered a lot of maximum entropy-type approaches and all of them were a bit unsatisfying. So, I think there’s three-ish reasons that we think maximum entropy is unsatisfying in various ways. I think the first one is perhaps most compelling, but most naive, and I’ll give that example first. So suppose that you have A and B, and then you’re taking the AND of A and B, so suppose we have C equals A and B.\n\n**Daniel Filan:** Yep.\n\n**Mark Xu:** And suppose that we know that C is 50/50, then the maximum entropy distribution on A and B given C is 50/50 both… Sorry, so if we condition on C being 50/50, the maximum entropy distribution over A and B has A and B correlated a little bit, but also it raises the probability of A and it raises the probability of B.\n\n**Daniel Filan:** Relative to…\n\n**Mark Xu:** Relative to 50/50.\n\n**Daniel Filan:** Okay.\n\n**Mark Xu:** And there’s various reasons why this is unsatisfying. The main reason, I guess, it’s unsatisfying is: suppose you have a circuit that is computing some stuff, and then you just add a bunch of auxiliary gates where you have some wires, and then you pick two wires and you’re like, “I want these wires to be correlated,” then you can just add the AND of those wires a bunch of times to your circuit, and then if you want the maximum entropy distribution over all the wires, it will want to push the AND of those two wires down to 50/50 because that’s the maximum entropy probability for a wire to be. And so, it’ll induce this fake correlation between the wires while you’re trying to do that.\n\n**Daniel Filan:** Sorry, just going back to the A, B, C which is A and B thing: is the idea that the thing you wanted was for A and B to be uncorrelated, but have high enough probability that when you combine them to get C, the probability of C being true is 50/50?\n\n**Mark Xu:** I think I actually explained the example wrong. So suppose you just have: your circuit is just two wires, it takes two inputs and it just outputs both of them, then the maximum entry distribution is 50/50 over both wires, that seems correct. And then suppose you just randomly added an AND gate, then the maximum entropy distribution would–\n\n**Daniel Filan:** So, when you say randomly added an AND gate, you mean like you have your two inputs and then you put an AND gate that takes your two inputs, and the output of that gate is your new output?\n\n**Mark Xu:** No, you don’t connect it to the output at all. You still output A and B, but you’ve just added an AND gate as some auxiliary computation that your circuit does for no reason.\n\n**Daniel Filan:** Okay.\n\n**Mark Xu:** Then, if you want to take the maximum entropy distribution over all the things your circuit is doing, then adding this gate will artificially raise the probability of A and B and also induce this correlation between A and B.\n\n**Daniel Filan:** Why?\n\n**Mark Xu:** So naively it won’t because your maximum entropy distribution will just be like A, B, C, all 50/50 and independent, and then suppose someone came to you and was like, “Actually, you need to respect the logical structure of this circuit. C can only be on if A and B are both on. The probability of C being on has to be the probability of A being on times the probability of B being on plus the interaction term,” then you’ll get this induced correlation between A and B plus A and B will be raised to 0.6 or whatever.\n\n**Daniel Filan:** So, is the idea that you have this intermediate thing in your circuit and you have to be maximum entropy also over this intermediate thing, and so even though you didn’t really care about the intermediate thing, it didn’t play any role of interest in anything else, because you’re being maximum entropy over that, you’ve got to mess around with the distributions over other variables?\n\n**Mark Xu:** Yeah, that’s right, and I think of this as a special case of a more general phenomenon where there’s this computation that you’re doing throughout the circuit, and you can pretend there’s some notion of time in your circuit, or gates that only take input wires as inputs are at time 1, and then if a gate takes as inputs gates that themselves take inputs as input, then that’s time 2, et cetera, et cetera, where you’re going down your circuit and computing things. And maximum entropy distributions have this property where things that you compute in “the future” go back in time and affect the past.\n\n**Daniel Filan:** In the sense of once you know things about the future, that gives you constraints on the past?\n\n**Mark Xu:** Yeah, and if you want to be maximum entropy, it causes you to mess with your probabilities of the input wires: if you learn that you’re computing lots of ANDs then you want to raise the probability of your input wires, et cetera, et cetera. And I think this is in general going to be a kind of an undesirable property, and it’s a bit complicated to talk about why it’s undesirable.\n\nI guess the simplest reason is someone can mess with you by putting a bunch of stuff at the end of your circuit and then once you get to the end of your circuit, you’re forced to go back and revise your initial guesses. Whereas it seems like intuitively, heuristic arguments want to be more deductive in nature, where you start with some premises and then you deduce facts from those premises, and nothing anyone can say after you’ve started with your premises or nothing anyone can add to your circuit after you deduce facts from your premises can cause you to go back in time and be like, “Actually, my premises were wrong because…” I don’t know, it doesn’t really make that much sense that if you assume the A is 50/50 and B is 50/50 and then you’re like, “Ah, I’m computing C, which is A and B, I guess A and B are like 60/40 now and they’re also slightly correlated.”\n\nThat kind of reasoning seems intuitively invalid, and it’s the kind of reasoning that various forms of maximum entropy force you to take. And you can try to do various other stuff to eliminate this problem which we’ve explored historically, and I think none of them seem that satisfying and fix the problem that well or something.\n\n**Daniel Filan:** Right, that’s a useful response. You just end up with these useless variables that you shouldn’t have cared about the entropy of, but you do now. So, this relates to this question I have about the adversarial robustness of these heuristic estimators.\n\n**Mark Xu:** So, I said originally that there were three reasons why we didn’t maximum entropy. I’m happy to talk about the other two also, but maybe you don’t want to.\n\n**Daniel Filan:** I think I’m sold by reason 1. If you want to talk about reason 2 and reason 3, I’m all ears.\n\n**Mark Xu:** Well, so I think they are somewhat related to this more general problem of why heuristic arguments are going to be possible to begin with. So, I think reason 2 is going to be something like: we can’t actually maintain full probability distributions over things. So, there’s an important thing a probability distribution has to be, which is consistent with various facts or something. It has to be a real probability distribution. It has to describe actual things that are realizable. And I think heuristic arguments or your heuristic estimator can’t be real in that sense. Your distribution over possible states your circuit can take can’t be a distribution over actual possible states your circuit can take.\n\nSo, one reason that we want this is some of our mechanistic anomaly detection applications have us believing things like A is on and B is on, but A and B are not on, because we want to be able to drop out mechanisms and you just straight out can’t do that if you have an actual probability distribution over actual outcomes because A being on and B being on must imply that A and B is being on.\n\nAnd the second reason is in terms of just computational complexity, it’s very likely just going to be impossible to actually have a distribution over realizable or physically possible or logically possible states, because the act of determining whether or not something is logically possible is very hard. I think Eric Neyman has a couple of neat examples where it’s just various circuits where it’s [#P-hard](https://en.wikipedia.org/wiki/%E2%99%AFP) to determine or to have a distribution over actual states that are satisfied by that circuit, although I’m not familiar with the details enough to talk about them here. And then there was some third reason which I’m forgetting.\n\n### Adversarial robustness for heuristic arguments\n\n**Daniel Filan:** Cool, so one thing that I’m wondering about is: you mention in the paper that it’s difficult to get adversarial robustness. There are various reasons you think that you should be able to be fooled by people adversarially picking bad arguments. So often in the AI realm, when I hear that something is vulnerable to adversaries, I’m really worried because I’m like, “Well, what if my AI is an adversary?” I’m wondering, do you think that this worry applies in the heuristic arguments case?\n\n**Mark Xu:** Yeah, so first all, I guess I’ll talk about situations in which I think it’s basically impossible to be adversarially robust. So, suppose you have a hash function that gives -1 or 1, and you’re estimating the sum of hash of N over N^2^, and suppose your heuristic argument can just point out the values of various hash terms. So your naive heuristic estimator, presumption of independence, is 50/50 between one and negative one, so the estimate is zero; and then your heuristic argument consists of pointing out the values of various hash terms. In this situation, there’s just always going to exist arguments that continue to drive you up or continue to drive you down.\n\nAnd someone’s always going to be like, “Hey, hash of 2 is 1, hash of 7 is 1, hash of 9 is 1” and you’re just going to be driven up and up. It’s possible it should be like hash of N over N^1.5^ or something to make it bad. And so, you’re in this awkward spot where for any quantity with sufficient noise, even if you expect very strongly the noise to average out to zero, there will exist a heuristic argument that only points out the positive values of the noise and will drive you up, or there’s a heuristic argument that only points out the negative values of the noise and drive you down. And so, I think this is naively quite a big problem for the entire heuristic argument paradigm, if you’re ever relying on something like someone doing an unrestricted search for heuristic arguments of various forms and inputting them into your heuristic estimator.\n\nSo, there’s a couple ways out. First, I’ll talk about the simplest way out, that I think ends up ultimately not working, but is worth talking about anyway. I think the simplest thing you can do is just have two people searching for heuristic arguments, one person is driving them up and one person driving them down. So, hopefully if you have one search process searching for heuristic arguments to drive you up and one to drive you down, then we don’t systematically maybe expect one person to be biased instead.\n\nI think this is a problem. For example, one obvious thing to do is both players have equal amounts of time to find heuristic arguments or something. However, if you consider the previous case with hash of N, suppose instead of being 50/50 1 and -1, hash of N is actually 1/4 probability of 1, 3/4 probability of -1, then in that case the player finding heuristic arguments to go down has a 3x advantage over the player finding heuristic arguments to go up. And so, then you’re no good because you can have these quantities where it’s far easier to find heuristic arguments to drive you up than to drive you down or vice versa.\n\n**Daniel Filan:** Although in that case you really should be below zero in expectation, right?\n\n**Mark Xu:** Yeah.\n\n**Daniel Filan:** I think there’s some example in [the paper](https://arxiv.org/abs/2211.06738) (that people can read) where it really checks out that debate is not going to work. \\[Examples of this sort are in Appendix E\\]\n\n**Mark Xu:** So, you have examples that that make you very sad, and so instead of doing debate, we want to do this other thing that’s going to be kind of interesting. So, if you imagine searching for heuristic arguments in the setting where you have hash of N over N^1.5^ or whatever, the way that you’re going to do this is you’re going to first check hash of 1, then you’re going to check hash of 2, and then you’re going to check hash of 3, et cetera, et cetera. And suppose that you wanted to drive the estimate of the quantity up, then you would check hash of 1, and suppose it’s positive, you’re like, “Great, let’s include Term 1” and then you check hash of 2 and it’s negative and you’re like, “Let’s not include Term 2”, and then you check hash of 3 and it’s also negative and you’re like, “Let’s not include that”, check hash of 4, it is positive, so then you include that.\n\nAnd you’re carefully going through these terms and eliminating the ones that are negative. However, the interesting thing is if you imagine being the person that’s doing this, there in some sense exists a broader heuristic argument that you first considered and then pruned down, which is: you know the values of all these four terms, and then you selected the terms that were only positive.\n\nAnd so, there’s some sense in which, if instead of including only the heuristic argument that you produce at the end, you included the entire search process for the heuristic argument and also I guess the heuristic argument you included at the end, then hopefully the heuristic estimator is forced to be like, “Well, we actually looked at all four of these terms and then we discarded two of them, but the discarding part isn’t relevant because I already know all four of these logical facts, so I’m forced to include all four of these terms.” And so, then the hope is not that there doesn’t exist heuristic arguments that are misleading, but there’s no way of searching for heuristic arguments that’s systematically misleading if you don’t already know the values of the heuristic arguments. And this is related to the presumption of independence being “correct on average” or something. And so if you’re presuming independence in a way that’s correct on average, then on average people can only… When searching for arguments and just looking at stuff, they can only lead you closer to the truth hopefully.\n\n**Daniel Filan:** Sure. So the idea is that if in some sense we can… Well, we should be able to have some broad search process over arguments that essentially isn’t adversarial and use that, sounds like is the upshot.\n\n**Mark Xu:** Well, it’s: even if the search is adversarial, if you’re conditioning on everything that the search process knows while it’s searching, then it can’t be adversarial, because all it can do is look at values, and it doesn’t know what the value is before it looks, and if you’ve actually presumed independence correctly, then on average everything it looks at has an equal chance of driving you up or down.\n\n**Daniel Filan:** I guess I have this concern that maybe the point of mechanistic anomaly detection was to help me know how to elicit this latent knowledge or something. And so, I might’ve been imagining using some AI to come up with arguments and tell me that the thing was bad. And if I’m using that AI to help me get mechanistic anomaly detection and I need mechanistic anomaly detection to help me get that AI be good, then that’s some sort of recursion. It might be the bad type of recursion. It might be the good type of recursion where you can bootstrap up, I guess.\n\n**Mark Xu:** I think it’s not very clear to me how this is ultimately going to shake out. I think this kind of adversarial robustness, or how we deal with adversarial robustness, is important. I think there are other ways out besides the way I described. I think that’s intuitively what we want to do: we want to just condition your heuristic estimator on everything, even the search for the arguments and also, I don’t know, just all the things. And hopefully if you don’t let any adversarial selection into your heuristic estimator, then it can’t be adversarially selected against or something. So, there’s a thing of: if you’re a proper Bayesian and someone’s adversarially selecting evidence to show you, the thing that’s supposed to happen is eventually you’re just like, “And now I think I’m being adversarially selected evidence to be shown, and I know this fact and so I’m just being a Bayesian”, and then you just update on that fact, and you just aren’t wrong on average if you’re correct about the kinds of ways in which you’re being adversarially selected against.\n\n**Daniel Filan:** Although, it does require you to… I don’t know, I feel like this is a bit of Bayesian cope because in order to do that… When you’re a Bayesian, you’re supposed to be like, “Oh, I’m just going to get observations and update on observations.” But now you’re like, “Well, now my new observation is that I’m observing that observation and maybe the observation process is weird,” but then I feel like, you’ve got this layer and after that layer you just trust it and before that layer you worry about things being screened or whatever. And I’m like, “Where do you put the layer?” You had to enlarge your model to make this be… I don’t know, it seems a bit-\n\n**Mark Xu:** I think being a Bayesian is hard, but the thing we’re going to try to do is just look over the shoulder of the person selecting the evidence to show you and then see everything that they see, and then hopefully they’re not misled on average because they’re just looking at stuff.\n\n**Daniel Filan:** In that case, it’s better than the Bayesian case or I don’t have the same complaints as I have about the Bayesian case.\n\n### Other approaches to defining mechanisms\n\n**Daniel Filan:** So stepping back a bit, there are a few different ways that somebody might have tried to concretize a mechanism. So, there’s this one approach where we have something called a heuristic argument, and we’re going to work to try and figure out how to formalize that. I guess we haven’t explicitly said this yet maybe, but in the paper, it’s still an open problem, right?\n\n**Mark Xu:** Yeah, or we have-\n\n**Daniel Filan:** There are avenues.\n\n**Mark Xu:** I guess I would say: we have some heuristic estimator for some quantities that are deficient in various ways, and we’re trying to figure out what’s up and get better ones for other quantities and then unify them and hopefully all the dominoes will fall over once we quest sufficiently deeply.\n\n**Daniel Filan:** Nice, so there’s this approach. There’s also some people working on various causal abstractions. So [causal scrubbing](https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing) is work by [Redwood Research](https://www.redwoodresearch.org/), there’s [other causal abstraction work](https://arxiv.org/abs/2106.02997) by other groups. I’m wondering: out of the landscape of all the ways in which one might have concretized what a mechanism is, how promising do you think heuristic arguments is?\n\n**Mark Xu:** So, I’m tempted to say something like: if the causal abstraction stuff works out, we’ll just call that a heuristic argument and go with that sort of thing. ‘Heuristic arguments’ is intended to be this umbrella term for this sort of machine-checkable, proof-like thing that can apply to arbitrary computational objects, and I think that’s what I’m tempted to say. I think in fact, we do have various desiderata that we think are important for our particular approach to heuristic arguments, and we want them to be proof-like in various ways that e.g. causal abstractions often aren’t, or we want them to be fully mechanistic and have zero empirical parts in them. Whereas often, various causal abstraction approaches allow you to measure some stuff empirically and then do deduction with the rest of it. I very much think of us as going for the throat on this sort of heuristic argument stuff.\n\nThere’s some question you might ask, which is like, “Why do you expect heuristic arguments to apply to neural nets?” And the answer to that is, well, neural nets are just a computational object and we want to have heuristic estimators that work for literally all computational objects, and then the reason why it applies to neural nets in particular is just because neural nets are a thing that you can formally define. Whereas there’s a lot of more restricted approaches to explanations that are like, “Well, we only actually care about the neural net thing, and so we ought to define it in particular about the neural net thing”. So maybe in summary, we’re trying to be in some sense maximally ambitious with respect to the existence of heuristic arguments.\n\nThe research plan: progress and next steps\n------------------------------------------\n\n**Daniel Filan:** I got you. So, I’d like to move on a little bit to the overall plan. So I think in some of these early posts, the idea is step 1: formalize heuristic arguments, step 2: solve mechanistic anomaly detection given the formalism of heuristic arguments, and step 3: find a way of finding heuristic arguments. And really, I guess these three things could be run in parallel or not necessarily in order. I’m wondering, the last batch of publications was in late 2022. How’s the plan going since then?\n\n**Mark Xu:** So, I think Quarter 1 of 2023 was a bit rough and we didn’t get that much done. I think it’s sped up since then. So, I can talk about stuff that is… So, things that I think are good that have happened: we have a pretty good formal problem statement about what it means to be a heuristic estimator for a class of functions or a class of circuits that we call bilinear circuits, where just all your gates are squares. So, we have that desideratum and we can talk about what it means to be a good heuristic estimator with respect to that class of circuits in a way that things we think are good satisfy… Or some stuff we have satisfies and some other stuff we have doesn’t satisfy. So, hopefully that’s just a fully formal problem.\n\nWe have a better class of heuristic estimators for cumulant propagation - .we have an upgrade on cumulant propagation that we call reductant propagation, which is just slightly different: instead of keeping track of cumulants, you keep track of a different thing called a reductant, and it’s slightly better in various ill-defined ways, but it’s more accurate empirically on most distributions of circuits we’ve tried, et cetera, et cetera and we feel better about it. We have a clearer sense of what it means to do mechanistic anomaly detection, and how that needs to go, and what the hard cases are going to be for doing mechanistic anomaly detection in terms of being unable to distinguish between mechanisms. I noticed I didn’t really answer your original question, which is like, “How’s it going?” I think it’s going par or something. Maybe Q1 was slightly below par and we’re doing slightly above par in Q2, and so it’s on average par or something.\n\n**Daniel Filan:** So, par in the sense of you’ve gotten a better sense of some special cases, not yet knocked totally out of the park, but-\n\n**Mark Xu:** Or if you told me that this is how much progress we would’ve made at the beginning of 2023, I would be like, “Okay, that seems good enough to be worth doing: let’s do that, this is how we should spend our time” or something.\n\n**Daniel Filan:** Cool. I’m wondering: given that progress - so out of the steps of formalize heuristic arguments, solve mechanistic anomaly detection given that, and find heuristic arguments, how optimistic are you about the steps? Which ones seem most difficult?\n\n**Mark Xu:** So, I think the thing that’s most difficult is formalizing heuristic arguments in a way that makes them findable. Here’s a formalization of heuristic arguments: they’re just proofs. And so, there’s some sense in which heuristic arguments being compact is intrinsic to the nature of what it is to be a heuristic argument. I think doing useful stuff with heuristic arguments is pretty unlikely to be the place where we fall down. I think it’s possible that heuristic arguments get formalized and then we’re like, “Darn, we can’t do any of the things that we thought we were going to be able to do with heuristic arguments, because it turns out that they’re very different than what we thought they were going to be like.” I think that would be pretty good though, because we would have heuristic arguments and we would in some sense be done with that part. It would be very surprising to me if they were not useful in various ways. I don’t know if that was a complete answer.\n\n**Daniel Filan:** I think that worked. I’m wondering if there’s been any experimental work on trying out mechanistic anomaly detection things beyond the… So, I guess you’ve mentioned there are various interpretability things that you think won’t scale. Is there any promising experimental work you’re aware of?\n\n**Mark Xu:** So, I am not very aware of experimental work in general, but I think Redwood is currently working on what they’re calling ELK benchmarks where they’re trying to do this sort of mechanism distinction on toy problems like function evaluation. I don’t know how that’s going because I am not up-to-date on details.\n\n**Daniel Filan:** Fair enough.\n\n**Mark Xu:** I think ARC employees often write code to check whether or not heuristic estimators do some things, or check how empirically accurate they are, or find counter examples by random search or something. Probably you don’t want to call that experimental work, because we’re just checking how accurate our heuristic estimators for permanents of matrices are, or whatever. So, I think the short answer is I think Redwood’s doing some stuff that I don’t know that much about, and I’m not really aware of other stuff being done, but that is probably mostly because I’m not that aware of other stuff and not because it’s not being done, although it probably also isn’t being done in the way that I would want it to be done.\n\n**Daniel Filan:** I got you, so it’s about time for us to be wrapping up. Before I close up, I’m wondering if there’s any question that you think I should have asked.\n\n**Mark Xu:** I think people often ask for probabilities that these sorts of things all work out, to which I often say: 1/7 that everything works out roughly the way that we think it’s going to work out and is super great within five years-ish - maybe not quite five years now because I’ve been saying 1/7 over five years for more than a few months. So, maybe it’s like four and a half years now. But other than that, I don’t think so.\n\nFollowing ARC’s research\n------------------------\n\n**Daniel Filan:** Fair enough. So finally, if people are interested in following your research or if we have bright minds who are perhaps interested in contributing, what should they do?\n\n**Mark Xu:** So ARC posts [blog posts](https://www.alignment.org/blog/) and various announcements on our website, [alignment.org](https://www.alignment.org/). And we’re also currently hiring, so you can go to [alignment.org](https://www.alignment.org/) and click the hiring button and then be directed to [our hiring page](https://www.alignment.org/hiring/).\n\n**Daniel Filan:** Great. Well, thanks for talking to me today.\n\n**Mark Xu:** You’re welcome. Thanks for having me on.\n\n**Daniel Filan:** This episode is edited by Jack Garrett, and Amber Dawn Ace helped with the transcription. The opening and closing themes are also by Jack Garrett. Financial support for this episode was provided by the [Long-Term Future Fund](https://funds.effectivealtruism.org/funds/far-future), along with [patrons](https://www.patreon.com/axrpodcast) such as Ben Weinstein-Raun, Tor Barstad, and Alexey Malafeev. To read a transcript of this episode or to learn how to [support the podcast yourself](https://axrp.net/supporting-the-podcast/), you can visit [axrp.net](https://axrp.net/). Finally, if you have any feedback about this podcast, you can email me at [feedback@axrp.net](mailto:feedback@axrp.net).",
      "plaintextDescription": "YouTube link\n\nIs there some way we can detect bad behaviour in our AI system without having to know exactly what it looks like? In this episode, I speak with Mark Xu about mechanistic anomaly detection: a research direction based on the idea of detecting strange things happening in neural networks, in the hope that that will alert us of potential treacherous turns. We both talk about the core problems of relating these mechanistic anomalies to bad behaviour, as well as the paper “Formalizing the presumption of independence”, which formulates the problem of formalizing heuristic mathematical reasoning, in the hope that this will let us mathematically define “mechanistic anomalies”.\n\nTopics we discuss:\n\n * Mechanistic anomaly detection\n   * Are all bad things mechanistic anomalies, and vice versa?\n   * Are responses to novel situations mechanistic anomalies?\n   * Formalizing “for the normal reason, for any reason”\n   * How useful is mechanistic anomaly detection?\n * Formalizing the presumption of independence\n   * Heuristic arguments in physics\n   * Difficult domains for heuristic arguments\n   * Why not maximum entropy?\n   * Adversarial robustness for heuristic arguments\n   * Other approaches to defining mechanisms\n * The research plan: progress and next steps\n * Following ARC’s research\n\nDaniel Filan: Hello, everybody. In this episode, I’ll be speaking with Mark Xu. Mark is a researcher at the Alignment Research Center where he works on mechanistic anomaly detection, which we’ll be speaking about during this interview. He’s also a co-author of the paper Formalizing the Presumption of Independence which is about heuristic arguments, a way of formalizing, essentially, what a mechanism is. For links to what we’re discussing, you can check the description of this episode and you can read the transcript at axrp.net.\n\n\nMechanistic anomaly detection\nDaniel Filan: All right. So I guess we’re going to be talking about mechanistic anomaly detection. First of all, what is that?",
      "wordCount": 21711
    },
    "tags": [
      {
        "_id": "HrCcwuykKcn2SJgge",
        "name": "Alignment Research Center (ARC)",
        "slug": "alignment-research-center-arc"
      },
      {
        "_id": "vjKs7Pvz3MbgMc75C",
        "name": "Audio",
        "slug": "audio"
      },
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "56yXXrcxRjrQs6z9R",
        "name": "Interpretability (ML & AI)",
        "slug": "interpretability-ml-and-ai"
      },
      {
        "_id": "9DNZfxFvY5iKoZQbz",
        "name": "Interviews",
        "slug": "interviews"
      },
      {
        "_id": "sYm3HiWcfZvrGu3ui",
        "name": "AI",
        "slug": "ai"
      }
    ],
    "af": true,
    "ai_safety_tags": [
      {
        "id": "HrCcwuykKcn2SJgge",
        "name": "Alignment Research Center",
        "source": "post_tag"
      }
    ],
    "research_agendas": [
      "AI Safety Organizations"
    ],
    "extraction_source": {
      "tag_id": "HrCcwuykKcn2SJgge",
      "tag_name": "Alignment Research Center",
      "research_agenda": "AI Safety Organizations"
    }
  },
  {
    "_id": "s7HAiEQrQrqB5CnJp",
    "title": "AXRP announcement: Survey, Store Closing, Patreon",
    "slug": "axrp-announcement-survey-store-closing-patreon",
    "url": null,
    "baseScore": 14,
    "voteCount": 5,
    "viewCount": null,
    "commentCount": 0,
    "createdAt": null,
    "postedAt": "2023-06-28T23:40:02.537Z",
    "contents": {
      "markdown": "Some announcements:\n\n*   There’s a brief [survey](https://bit.ly/axrpsurvey2023) of my audience you can take here - filling it out will help me get a sense of where you all are at career-wise and how the podcast would better serve you. There are only 3 required questions, and they’re multiple choice! It would be great if you could fill it out [here](https://bit.ly/axrpsurvey2023).\n*   The AXRP [store](https://store.axrp.net) is closing in a week - so if you want to buy merchandise, now’s your last chance to do so! To buy merch like laptop stickers, t-shirts, sweaters, or jigsaw puzzles (in the US), go to the store [at this link](https://store.axrp.net).\n*   If you want to financially support the podcast, you can do so in two ways:\n    *   On [Patreon](https://www.patreon.com/axrpodcast) \\- donations of US$20 per month get to see my episode notes, and at US$50 per month, you have a (currently very high) chance of being thanked by name at the end of episodes! Link is [here](https://www.patreon.com/axrpodcast).\n    *   On [Ko-fi](https://ko-fi.com/axrpodcast), where you can give one-off donations.\n*   Interviews will always be free and available to non-subscribers, but your support gives me a better sense of how many people get value out of this podcast, and how much time I should spend on it.",
      "plaintextDescription": "Some announcements:\n\n * There’s a brief survey of my audience you can take here - filling it out will help me get a sense of where you all are at career-wise and how the podcast would better serve you. There are only 3 required questions, and they’re multiple choice! It would be great if you could fill it out here.\n * The AXRP store is closing in a week - so if you want to buy merchandise, now’s your last chance to do so! To buy merch like laptop stickers, t-shirts, sweaters, or jigsaw puzzles (in the US), go to the store at this link.\n * If you want to financially support the podcast, you can do so in two ways:\n   * On Patreon - donations of US$20 per month get to see my episode notes, and at US$50 per month, you have a (currently very high) chance of being thanked by name at the end of episodes! Link is here.\n   * On Ko-fi, where you can give one-off donations.\n * Interviews will always be free and available to non-subscribers, but your support gives me a better sense of how many people get value out of this podcast, and how much time I should spend on it.",
      "wordCount": 207
    },
    "tags": [
      {
        "_id": "8Ec9rD286qNstoiGH",
        "name": "AXRP",
        "slug": "axrp"
      },
      {
        "_id": "izp6eeJJEg9v5zcur",
        "name": "Community",
        "slug": "community"
      }
    ],
    "af": false,
    "ai_safety_tags": [],
    "research_agendas": [],
    "extraction_source": null
  }
]